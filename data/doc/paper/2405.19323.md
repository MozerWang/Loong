# Are Large Language Models Chameleons? 

Mingmeng Geng ${ }^{1 *}$ Sihong He ${ }^{2} \quad$ Roberto Trotta ${ }^{1,3 *}$<br>${ }^{1}$ International School for Advanced Studies (SISSA) ${ }^{2}$ UT Arlington<br>${ }^{3}$ Imperial College London


#### Abstract

Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.


## 1 Introduction

Although often referred to as "stochastic parrot" (Bender et al., 2021), the application of large language models (LLMs) goes beyond "talking" (Staab et al. 2023). Their adaptability and versatility remind us of another animal: the chameleon.

Do LLMs have their own opinions? If we ask LLMs to answer some subjective questions, will they behave like humans? And can we rely on their ability to mimic our subjective opinions to create entirely synthetic opinion polls, or as an alternative to post-stratification?

The alignment between LLMs outputs and human values has attracted great interest and has already been measured (Hendrycks et al. 2020a). What could be called the "personality traits" of LLMs have also been widely explored in several papers (Safdari et al., 2023, Jiang et al. 2023b, 2024, Mei et al. 2024). It's not difficult to compare the responses of LLMs to subjective questions with some responses completed by real human beings. But we know that human opinions are generally not uniformly distributed among people, and indeed this is the crux of reliable surveys.

What if prompts with personal information are used to make LLMs' responses more specific? How should we compare such simulated data with real survey data?

In survey research, a number of demographic variables are routinely used, such as gender, race, and age (Goldberg et al. 1998, Herek, 2002, Kyriakidis et al., 2015). Gender and race are two important perspectives to consider for bias in LLMs (Sun et al. 2023; Kotek et al. 2023, Dong et al., 2024). The sensitivity of LLMs to age was also analyzed, based on six age groups (Liu et al., 2024). But the influence and robustness of the prompts are rarely discussed in papers dealing with opinions in LLMs (Röttger et al. 2024). The ability of LLMs to simulate surveys needs to be explored in more depth.

How can one analyze the responses of LLMs to subjective questions using quantitative methods?

Bias in LLMs is almost impossible to avoid, even with subsequent adjustments and alignments (Navigli et al. 2023). Our goal is to measure the difference between LLMs' responses and what[^0]real people think, by asking LLMs to answer actual survey questions with a prompt giving the corresponding demographic variables. Then we can compare the similarities and differences between the simulated results and the real survey data. The data and results obtained from surveys also deviate to a greater or lesser extent from the "ground truth", while the representativeness of the sample is often debated (Russell et al. 2022, Grafström and Schelin, 2014). Therefore, we want to analyze the bias in responses of LLMs, using real survey data as the "ground truth", which means that the survey results are assumed to be representative.

It is also important to evaluate differences between different LLMs, by comparing their 'opinions' with real survey data. Defining the conditions under which LLMs can assist or even replace human participants (if at all), and under which they are unrealistic, is the focus of our research.

## 2 Related work

LLM bias Bias in natural language processing (NLP) arises for several reasons, such as data, models, and research design (Hovy and Prabhumoye, 2021). The risks and shortcomings of LLMs were addressed even before they became popular (Bender et al., 2021). While LLMs solve many of the difficulties that traditional NLP methods have failed to overcome, the challenge of bias persists (Navigli et al. 2023). For example, results based on GPT-3 show that the opinions of certain demographic groups in the United States are not well represented (Santurkar et al., 2023). The political bias of ChatGPT in other countries has also been quantified (Motoki et al., 2024). Bias also looks like a problem shared by other LLMs (Boelaert et al. 2024), and not specific to ChatGPT.

LLM evaluation The pervasiveness and influence of LLMs is increasing rapidly, and it is becoming more important to better evaluate and compare different LLMs (Chang et al. 2024). Platforms that build on users' comparative ratings have also emerged, for example, Chatbot Arena (Chiang et al. 2024). Traditional objective evaluation criteria for solving various tasks, such as MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2020b), do not meet current needs. Therefore, novel evaluation methods have been proposed, such as uncertainty quantification (Ye et al., 2024).

LLM alignment How to align LLMs with human behavior is an exciting and challenging task (Ouyang et al. 2022). Despite the many limitations (Wolf et al. 2023; Kirk et al., 2023), we can see the attempts and results of many researchers on the alignment of LLMs (Shen et al. 2023, Liu et al. 2023). LLMs are used in a variety of applications, so corresponding datasets and benchmarks are also needed for different usage scenarios (Köpf et al., 2024, Koo et al., 2023).

## 3 Data and models

While most research has focused on the United States, our concern is with the European countries, which are more heterogeneous in terms of culture and languages. Training data are also likely to be of very disparate size, especially for smaller countries and languages spoken by a relatively small number of people, which is likely to influence the LLMs' performance when impersonating somebody from that culture.

To better compare with real people's opinions, we made use of data from the European Social Survey (ESS), a biennial survey of attitudes and behaviour. We chose the round 10 of ESS (European Social Survey ERIC (ESS ERIC), 2022), collected from September 2020 up until May 2022, with 59,686 participants from 31 European countries, released from June 2022 to May 2023 (details in the appendix). Germany has the most participants, totaling 8,725, while Cyprus has the least, with only 875 participants.

In addition to ChatGPT (including GPT-3.5 and GPT-4o), we also carried out simulations with open source models via APIs or locally, including LLaMA-2 (Touvron et al. 2023), LLaMA-3, Mistral (Jiang et al., 2023a) and DeepSeek-V2 (Bi et al., 2024).

## 4 Methods

### 4.1 Simulations

The performance of LLMs needs to be evaluated from simulations, as it is not possible to predict the response of LLMs through analysis of the model structure and its parameters. We purchased API tokens from several companies to perform simulations and utilized trained open-source models. As with the real survey, we used the Chat model rather than the completion of sentences. For example:

P1 You are a [man or woman] born in [year] living in [country]. To what extent do you agree with the following statement: '[statement]'? Please respond with only a number (1) Agree strongly (2) Agree (3) Neither agree nor disagree (4) Disagree (5) Disagree strongly.

In real survey experiments, the order of the questions also affects the respondents' answers (Carlsson et al., 2012), and we ask one question at a time, so it's always zero-shot. The same personal information as in the real survey data was used in the simulations of LLMs. Nationality, gender, and age are the demographic variables of interest.

In addition to the different prompts, the influence of parameters cannot be ignored, such as temperature and top $p$. Different parameters were tested to compare the effects of the models.

### 4.2 Measurements

Previous studies have shown that LLMs are likely to be biased, but there is no agreement on how large these biases are. Theoretically, this depends not only on the model itself but also on the questions being asked. Hence, we tried to test and validate different LLMs to determine whether any observed bias in our survey questions is specific to certain models or more widespread.

As we have previously stated, we are dealing with subjective questions that do not have standard answers, but the results of surveys from real people are available. Therefore, we focus on data where participants agree or disagree with a statement, while for simplicity the missing data options ("Refusal", "Don't know", "No answer") were disregarded. The ordinal numbers of the responses from 1 ("Agree strongly") to 5 ("Disagree strongly") were used as a scale for their level of disagreement.

Our goal is to compare responses from LLMs to the ones from humans, so we are interested not only in averages but also in the variance. To better compare the impact of different prompts and different LLMs, we employ the following measures: mean, variance, and J-index.

Inspired by Jaccard similarity (Chierichetti et al. 2010), we define the J-index $J_{q}(G)$ for statement $q$ and people group $G$ that we are interested in (e.g., a country) as following

$$
\begin{equation*}
J_{q}(G)=\frac{\left.\sum_{g \in G} I\left(R_{q}^{h}(g), R_{q}^{m}(g)\right)\right)}{\left.\sum_{g \in G} U\left(R_{q}^{h}(g), R_{q}^{m}(g)\right)\right)}=\frac{\sum_{g \in G}\left|R_{q}^{h}(g) \cap R_{q}^{m}(g)\right|}{\sum_{g \in G}\left|R_{q}^{h}(g) \cup R_{q}^{m}(g)\right|} \tag{1}
\end{equation*}
$$

where $I(\cdot, \cdot)$ and $U(\cdot, \cdot)$ are the intersection and union of two sets of responses, $g$ is the subgroup of $G, R_{q}^{h}(g)$ and $R_{q}^{m}(g)$ represent the responses from survey data and LLM simulations, respectively. A J-index of 1 thus corresponds to perfect congruence between between survey and LLM simulation.

We further take the analysis weight (combination of post-stratification weight and population weight) $w_{i}$ for participant $i$ provided in the ESS dataset to re-weigh $I\left(R_{q}^{h}(g), R_{q}^{m}(g)\right)$ and $U\left(R_{q}^{h}(g), R_{q}^{m}(g)\right)$ to improve the representativeness of survey respondents concerning the target population,

$$
\begin{align*}
I\left(R_{q}^{h}(g), R_{q}^{m}(g)\right) & =\sum_{v \in V_{q}}\left(\min \left(\sum_{i \in g} w_{i} \mathbb{1}_{r_{q}^{h}(i)=v}, \sum_{i \in g} w_{i} \mathbb{1}_{r_{q}^{m}(i)=v}\right)\right)  \tag{2}\\
U\left(R_{q}^{h}(g), R_{q}^{m}(g)\right) & =\sum_{v \in V_{q}}\left(\max \left(\sum_{i \in g} w_{i} \mathbb{1}_{r_{q}^{h}(i)=v}, \sum_{i \in g} w_{i} \mathbb{1}_{r_{q}^{m}(i)=v}\right)\right) \tag{3}
\end{align*}
$$

where $v$ corresponds to the value of the response, $V_{q}$ is the set of all possible values for question $q$, $r_{q}^{h}(i)$ represents the answer in survey data, and $r_{q}^{m}(i)$ means the response of LLM given the same information as participant $i$.

## 5 Results

In our analysis, we used the four countries with the highest number of participants in the ESS (Germany, Greece, Bulgaria, and Italy, with 16132 samples in total). All of our results take poststratification weighting into account.

### 5.1 ChatGPT simulations

Considering answers from GPT-3.5 2 (temperature $=1$, top_p $=0.9$ ) and using the prompt $\mathbf{P} 1$ as described above, we adopted specific birth years rather than age groupings to better demonstrate the sensitivity of ChatGPT to age. ChatGPT does not always respond with only one number as we requested, sometimes including a description after the number (e.g., "(1) Agree strongly"), but it doesn't refuse to answer. The results for the first statement are shown in Figure 1

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-04.jpg?height=537&width=675&top_left_y=824&top_left_x=367)

(a) Men

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-04.jpg?height=537&width=675&top_left_y=824&top_left_x=1083)

(b) Women

Figure 1: Q1: "Gays and lesbians free to live life as they wish"? Prompt: P1. The points represent the mean and the error bars represent the standard deviation (and the same for the next figures). Model: $G P T-3.5$

At least on this question, the output of GPT-3.5 is indeed determined by the demographic variables given in the prompts. The mean of the simulation results is close to the mean of ESS data, except for Germans born before 1940. However, the variance of the simulated data is too small compared to the real data, even when the temperature parameter is set to its maximum value (see Appendix).

The impact of the same variable can vary for different topics. For example, the difference resulting from gender is evident in LLM's response to the second question, as illustrated in Figure 2 .

Although we always used different random seeds for different persons and the same random seed for one person across different questions in our experiments, the problem of too little variance in the simulations is common for various questions, as has been pointed out by other researchers (Boelaert et al. 2024). In particular, the example in Figure 2 shows that although the mean of ESS data and GPT-3.5 simulations are close for women, the variance of simulations is very small -almost nonexistent- in this case. This would seem to imply that women are associated with more stereotypical answers, another facet of gender bias.

### 5.2 Prompts and parameters

In addition to the biases and stereotypes that may result from the model itself and the training data, the issue related to variance is also likely caused by the fact that we provided too little information in prompt $\mathbf{P 1}$, which is far from a real human in real life. Therefore, we tried prompts with a more informative description, such as the respondent's occupation:[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-05.jpg?height=621&width=1401&top_left_y=237&top_left_x=362)

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-05.jpg?height=531&width=675&top_left_y=252&top_left_x=367)

(a) Men

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-05.jpg?height=548&width=675&top_left_y=241&top_left_x=1083)

(b) Women

Figure 2: Q2: "Government should reduce differences in income levels"? Prompt: P1. Model: GPT-3.5.

P2 You are a [man or woman] born in [year] living in [country]. Occupation category: [ISCO3] To what extent do you agree with the following statement: '[statement]'? Please respond with only a number (1) Agree strongly (2) Agree (3) Neither agree nor disagree (4) Disagree (5) Disagree strongly.

The results generated by prompt $\mathbf{P} 2$ are shown in Figure 3 , where the variability of simulations becomes closer to the variance of survey data. This is not because our addition to the prompt has increased the degree of randomness, but rather because occupational differences were previously ignored.

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-05.jpg?height=623&width=1405&top_left_y=1361&top_left_x=360)

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-05.jpg?height=528&width=675&top_left_y=1384&top_left_x=367)

(a) Men

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-05.jpg?height=537&width=677&top_left_y=1382&top_left_x=1082)

(b) Women

Figure 3: Q1: "Gays and lesbians free to live life as they wish"? Prompt: P2. Model: GPT-3.5.

The impact of prompts is also not just about how much information is provided - the order also matters, as has been shown previously (Pezeshkpour and Hruschka, 2023). For example, we tried reversing the order of answer options in the prompt as follows:

P3 You are a [man or woman] born in [year] living in [country]. To what extent do you agree with the following statement: '[statement]'? Please respond with only a number (1) Disagree strongly (2) Disagree (3) Neither agree nor disagree (4) Agree (5) Agree strongly.

The simulation results produced by prompt $\mathbf{P 3}$ shown in Figure 4 contain more "Agree strongly" and less "Agree" compared to Figure 1, although the overall trend is still consistent. It is well known that[^2]when a survey is filled out by humans, the order of the options affects the person's choice Galesic et al. 2008). Our experimental results suggest that the same is true for LLMs to an even larger extent.

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-06.jpg?height=623&width=1399&top_left_y=407&top_left_x=363)

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-06.jpg?height=528&width=656&top_left_y=430&top_left_x=382)

(a) Men

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-06.jpg?height=531&width=675&top_left_y=426&top_left_x=1083)

(b) Women

Figure 4: Q1: "Gays and lesbians free to live life as they wish"? Prompt: P3. Model: GPT-3.5.

For 9 such questions in the ESS (listed in the appendix), we calculated the mean bias and J-index for the simulations results of GPT-3.5 compared to ESS data, and the corresponding results are shown in Figure 5. For ease of plotting, we show the absolute value of the mean difference in the figure. Table 4 and Table 5 in the appendix list the means for different questions in the same country, and the means for the same question in different countries.

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-06.jpg?height=445&width=675&top_left_y=1412&top_left_x=367)

(a) Absolute value of the mean difference (error).

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-06.jpg?height=447&width=675&top_left_y=1408&top_left_x=1083)

(b) J-index (congruence).

Figure 5: Comparisons between survey data and simulation results based on GPT-3.5, broken down by country and question.

As we have seen above, prompt P3 leads to markedly different simulation results for Q1, although this does not always hold true for other questions. For example, the simulation results for $\mathbf{Q 4}$ using prompt $\mathbf{P 3}$ produced better results than those using prompts $\mathbf{P 1}$ and $\mathbf{P}$. The J-index brings further insight. For example, while $\mathbf{Q 3}$ is still the worst question to simulate for Greeks, it does not simulate well for the Germans, either.

When more personal information is included in the prompts (i.e., P2), the bias decreases for 22 out of 36 and the J-index increases for 25 out of 36 cases. It should be noted that the parameters of the LLMs also have an impact on the results. For example, when we set top_p to 0.2 instead of 0.9 , the J-index becomes worse in all 72 cases with prompts $\mathbf{P 1}$ and $\mathbf{P 2}$. And the bias can both increase or decrease with the smaller top_p value, which is in line with our expectations. (See appendix for details.)

### 5.3 Other LLMs

The same prompts are used to perform simulation on other LLMs, in order to check the robustness of our results. Unlike ChatGPT, the other LLMs may refuse to answer, and their responses are not always valid (see appendix for examples), which could be considered as missing data. We could force them to answer by changing prompts, but this would also affect their choices (Röttger et al., 2024). Thus we use the same prompts $\mathbf{P 1}$ and $\mathbf{P 2}$ as before.

The results of the simulation on $L L a M A-2-7 B$ using prompt $\mathbf{P 1}$ for $\mathbf{Q 1}$ are shown in Figure 6, which features a much larger variance compared with GPT-3.5. The survey data corresponding to questions with invalid response in the simulations are not used in the calculations.

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-07.jpg?height=526&width=675&top_left_y=691&top_left_x=367)

(a) Men

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-07.jpg?height=526&width=675&top_left_y=691&top_left_x=1083)

(b) Women

Figure 6: Q1: "Gays and lesbians free to live life as they wish"? Prompt: P1. Model: LLaMA-2-7B.

For a more quantiative comparison, we calculated the difference of the mean compared to ESS data for Q1, listed in Table 1 for the following LLMs: GPT-3.5, GPT-4 ${ }^{4}, L L a M A-2-7 B$ (L-7B), LLaMA-2-13B (L-13B), LLaMA-3-8B (L-8B), Mistral-7B-Instruct-v0.2 (Mistral), DeepSeek-V2 (DS).

Table 1: Results from different LLMs (mean difference between LLMs answers and survey). Values in bold are the closest simulations to the mean for each row, while those in red italics are the worst.

| country | prompt | GPT-3.5 | GPT-4o | L-7B | L-13B | L-8B | Mistral | DS |
| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Germany | P1 | 0.22 | 1.37 | 0.51 | 0.64 | 1.19 | 0.47 | $\mathbf{- 0 . 1 2}$ |
|  | P2 | 0.33 | 0.33 | 0.40 | 0.57 | 1.48 | $\mathbf{- 0 . 1 3}$ | -0.29 |
| Greece | P1 | -0.14 | 1.02 | $\mathbf{0 . 0 4}$ | 0.21 | 0.75 | 0.86 | -0.19 |
|  | P2 | -0.02 | $\mathbf{- 0 . 0 1}$ | -0.13 | 0.41 | 0.92 | 0.78 | 0.03 |
| Bulgaria | P1 | -0.43 | 1.11 | -0.25 | 0.44 | 0.61 | 1.01 | $\mathbf{- 0 . 1 3}$ |
|  | P2 | -0.31 | $\mathbf{0 . 1 0}$ | -0.82 | 0.18 | 0.61 | 1.19 | -0.16 |
| Italy | P1 | $\mathbf{- 0 . 0 3}$ | 0.76 | 0.24 | 0.30 | 0.94 | 0.77 | -0.13 |
|  | P2 | 0.09 | -0.32 | $\mathbf{0 . 0 1}$ | 0.51 | 1.10 | 0.37 | -0.9 |
| Avg. (abs.) | P1 | 0.21 | 1.06 | 0.26 | 0.40 | 0.87 | 0.77 | $\mathbf{0 . 1 4}$ |
|  | P2 | 0.19 | 0.19 | 0.34 | 0.42 | 1.03 | 0.62 | $\mathbf{0 . 1 5}$ |

Table 1 shows that DeepSeek-V2 and GPT-3.5 generate simulation results with weighted averages closest to the survey data, while $L L a M A-3-8 B$ is highly biased for both prompts; occupation informa-[^3]tion is very important for GPT-4o, which appeats to perform generally worse than its predecessor GPT-3.5, which is somewhat unexpected.

Table 2 presents the J-index of the LLMs for Q1. The conclusions are very similar to the previous ones, and the results from Mistral-7B-Instruct-v0.2 are very interesting: the simulation results for Germans and Italians are very close to the real data, while the results of Greeks and Bulgarians pull down the average effect of this model. In a nutshell, we observer that GPT-3.5 is the best performing model among the ones tested.

Table 2: J-index results for different LLMs. Values in bold are the largest $J$-index for each row, while those in red italics are the worst.

| country | prompt | GPT-3.5 | GPT-4o | L-7B | L-13B | L-8B | Mistral | DS |
| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Germany | P1 | 0.43 | 0.13 | 0.35 | 0.17 | 0.25 | $\mathbf{0 . 5 1}$ | 0.46 |
|  | P2 | 0.40 | 0.40 | 0.40 | 0.28 | 0.18 | 0.47 | $\mathbf{0 . 5 0}$ |
| Greece | P1 | $\mathbf{0 . 3 2}$ | 0.24 | 0.27 | 0.23 | 0.26 | 0.20 | 0.27 |
|  | P2 | $\mathbf{0 . 3 8}$ | $\mathbf{0 . 3 8}$ | 0.26 | 0.15 | 0.25 | 0.18 | 0.27 |
| Bulgaria | P1 | $\mathbf{0 . 2 9}$ | 0.20 | 0.26 | 0.17 | 0.23 | 0.19 | 0.19 |
|  | P2 | $\mathbf{0 . 3 6}$ | $\mathbf{0 . 3 6}$ | 0.21 | 0.15 | 0.28 | 0.19 | 0.20 |
| Italy | P1 | $\mathbf{0 . 3 9}$ | 0.28 | 0.27 | 0.28 | 0.22 | 0.37 | 0.33 |
|  | P2 | 0.42 | 0.42 | 0.27 | 0.19 | 0.20 | $\mathbf{0 . 4 8}$ | 0.35 |
| Avg. | P1 | $\mathbf{0 . 3 6}$ | 0.21 | $\mathbf{0 . 2 9}$ | 0.21 | 0.24 | 0.32 | 0.31 |
|  | $\mathbf{P 2}$ | $\mathbf{0 . 3 9}$ | $\mathbf{0 . 3 9}$ | 0.28 | 0.19 | 0.23 | 0.33 | 0.33 |

People with the same country, gender, and year of birth are considered to be in the same group. Figure 7 compares the mean difference of each gender for $\mathbf{Q 2}$ between GPT-3.5 and LLaMA-2-7B. While LLaMA-2-7B shows a larger bias, it appears to be almost gender-independent. GPT-3.5 shows an overall smaller bias, but a more marked influence of gender in both mean and standard deviation.

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-08.jpg?height=575&width=1391&top_left_y=1512&top_left_x=367)

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-08.jpg?height=499&width=658&top_left_y=1518&top_left_x=381)

(a) Model: GPT-3.5

![](https://cdn.mathpix.com/cropped/2024_06_04_c46976dd12ced28f06a5g-08.jpg?height=504&width=675&top_left_y=1515&top_left_x=1083)

(b) Model: LLaMA-2-7B

Figure 7: Q2: "Government should reduce differences in income levels"? Prompt: P1. Comparison of results between GPT-3.5 (left) and LLaMA-2-7B (right).

## 6 Conclusions

Can a LLM adjust itself to impersonate a human like a chameleon? It depends on the object of imitation and the capabilities of the given LLM. Regardless of the goals and process of LLM alignment, our results reflect a clear geographic imbalance: simulations of Bulgarians work worse relative to people from other countries, for reasons that remain to be explored.

Different LLMs have different simulation outcomes, and the parameters of the model sometimes play an important role. The information in the prompt and the way it is described can, of course, greatly influence the LLM's response, with no clear indication as to what is the best choice.

We propose and validate possible ways to make LLM responses more human-like, such as providing additional information in prompts. We also present a metric for LLM responses to subjective questions that focuses more on the distribution than the mean, and crucially, suggest to compare with actual survey data in order to highlight potential gender and cultural biases in the response.

Although LLMs show the potential to perform simulations of collective behavior or to replace human participants in limited settings, more advanced LLMs do not necessarily produce simulation results that are more similar to survey data. On the contrary, earlier vision LLMs may give better simulations on some subjective questions.

## 7 Discussions

Due to budgetary and computational resource constraints, we "only" performed about 1 million simulations, although more simulations would be beneficial in obtaining more complete conclusions. LLMs are updated exceptionally quickly, and comparing which model is better can easily become outdated, so it's important to explore commonalities in terms of opinions in these LLMs. In addition, we are currently considering only 4 countries and 9 questions, which could be expanded to obtain a fuller picture of how European cultural diversity is captured (if at all) in the current generation of LLMs.

We know that LLMs are able to infer people's gender, age, and location from redacted and anonymized input text (Staab et al. 2023). Conversely, promoting LLMs with appropriate demographic variables as we have done here produces answers that appear to be generally aligned with real people's views, although still suffering from biases and stereotypes. Furthermore, biases and stereotypes present in LLMs are likely to slowly influence their users (Röttger et al., 2024), so it is essential to measure them. We use survey data for comparison, which should be more representative of public opinion than crowdsourced data. Compared to previous papers, more detailed simulation and analysis methods are performed, such as considering precise age information and using the weights of the survey data to better estimate the target population. But the process could be improved, for example in the handling of missing data.

LLMs have demonstrated the ability and potential to transform the field of computational social science (Ziems et al., 2024). Before LLMs can be used to model more complex groups and behaviors, it is necessary to scrutinize in detail the aggregate output of simulations, and we believe that comparison with survey data is a good entry point. The application of LLMs in survey research has been discussed (Jansen et al. 2023). A completely neutral LLM is not necessarily a good aim. A model with higher ratings or more parameters is not necessarily more suitable for survey simulation for a number of reasons, such as subsequent human adjustments. And the fine-tuning steps may well remain absolutely necessary.

## Acknowledgments

The authors would like to thank the personnel of RACHAEL S.r.l. for helpful discussions. R.T. acknowledges co-funding from Next Generation EU, in the context of the National Recovery and Resilience Plan, Investment PE1 - Project FAIR "Future Artificial Intelligence Research". M.G. is supported by DM Dottorati Innovazione e Green ex DM1061 Anno 2021 (DM 1061 del 10/08/2021).

## References

Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610-623.

Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., et al. (2024). Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954.

Boelaert, J., Coavoux, S., Ollion, É., Petev, I., and Präg, P. (2024). Machine bias generative large language models have a worldview of their own.

Carlsson, F., Mørkbak, M. R., and Olsen, S. B. (2012). The first time is the hardest: A test of ordering effects in choice experiments. Journal of Choice Modelling, 5(2):19-37.

Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al. (2024). A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1-45.

Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J. E., et al. (2024). Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132.

Chierichetti, F., Kumar, R., Pandey, S., and Vassilvitskii, S. (2010). Finding the jaccard median. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 293-311. SIAM.

Dong, X., Wang, Y., Yu, P. S., and Caverlee, J. (2024). Disclosure and mitigation of gender bias in llms. arXiv preprint arXiv:2402.11190.

European Social Survey ERIC (ESS ERIC) (2022). European social survey (ESS), round 10 - 2020.

Galesic, M., Tourangeau, R., Couper, M. P., and Conrad, F. G. (2008). Eye-tracking data: New insights on response order effects and other cognitive shortcuts in survey responding. Public opinion quarterly, 72(5):892-913.

Goldberg, L. R., Sweeney, D., Merenda, P. F., and Hughes Jr, J. E. (1998). Demographic variables and personality: The effects of gender, age, education, and ethnic/racial status on self-descriptions of personality attributes. Personality and Individual differences, 24(3):393-403.

Grafström, A. and Schelin, L. (2014). How to select representative samples. Scandinavian Journal of Statistics, 41(2):277-290.

Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D., and Steinhardt, J. (2020a). Aligning ai with shared human values. arXiv preprint arXiv:2008.02275.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2020b). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.

Herek, G. M. (2002). Gender gaps in public opinion about lesbians and gay men. Public opinion quarterly, 66(1):40-66.

Hovy, D. and Prabhumoye, S. (2021). Five sources of bias in natural language processing. Language and linguistics compass, 15(8):e12432.

Jansen, B. J., Jung, S.-g., and Salminen, J. (2023). Employing large language models in survey research. Natural Language Processing Journal, 4:100020.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. (2023a). Mistral 7b. arXiv preprint arXiv:2310.06825.

Jiang, G., Xu, M., Zhu, S.-C., Han, W., Zhang, C., and Zhu, Y. (2024). Evaluating and inducing personality in pre-trained language models. Advances in Neural Information Processing Systems, 36.

Jiang, H., Zhang, X., Cao, X., Kabbara, J., and Roy, D. (2023b). Personallm: Investigating the ability of gpt-3.5 to express personality traits and gender differences. arXiv preprint arXiv:2305.02547.

Kirk, H. R., Vidgen, B., Röttger, P., and Hale, S. A. (2023). Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. arXiv preprint arXiv:2303.05453.

Koo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M., and Kang, D. (2023). Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012.

Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z. R., Stevens, K., Barhoum, A., Nguyen, D., Stanley, O., Nagyfi, R., et al. (2024). Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36.

Kotek, H., Dockum, R., and Sun, D. (2023). Gender bias and stereotypes in large language models. In Proceedings of The ACM Collective Intelligence Conference, pages 12-24.

Kyriakidis, M., Happee, R., and De Winter, J. C. (2015). Public opinion on automated driving: Results of an international questionnaire among 5000 respondents. Transportation research part $F$ : traffic psychology and behaviour, 32:127-140.

Liu, S., Maturi, T., Shen, S., and Mihalcea, R. (2024). The generation gap: Exploring age bias in large language models. arXiv preprint arXiv:2404.08760.

Liu, Y., Yao, Y., Ton, J.-F., Zhang, X., Cheng, R. G. H., Klochkov, Y., Taufiq, M. F., and Li, H. (2023). Trustworthy llms: a survey and guideline for evaluating large language models' alignment. arXiv preprint arXiv:2308.05374.

Mei, Q., Xie, Y., Yuan, W., and Jackson, M. O. (2024). A turing test of whether ai chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9):e2313925121.

Motoki, F., Pinho Neto, V., and Rodrigues, V. (2024). More human than human: Measuring chatgpt political bias. Public Choice, 198(1):3-23.

Navigli, R., Conia, S., and Ross, B. (2023). Biases in large language models: origins, inventory, and discussion. ACM Journal of Data and Information Quality, 15(2):1-21.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.

Pezeshkpour, P. and Hruschka, E. (2023). Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483.

Röttger, P., Hofmann, V., Pyatkin, V., Hinck, M., Kirk, H. R., Schütze, H., and Hovy, D. (2024). Political compass or spinning arrow? towards more meaningful evaluations for values and opinions in large language models. arXiv preprint arXiv:2402.16786.

Russell, A. M., Browne, M., Hing, N., Rockloff, M., and Newall, P. (2022). Are any samples representative or unbiased? reply to pickering and blaszczynski. International Gambling Studies, 22(1):102-113.

Safdari, M., Serapio-García, G., Crepy, C., Fitz, S., Romero, P., Sun, L., Abdulhai, M., Faust, A., and Matarić, M. (2023). Personality traits in large language models. arXiv preprint arXiv:2307.00184.

Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., and Hashimoto, T. (2023). Whose opinions do language models reflect? In International Conference on Machine Learning, pages 29971-30004. PMLR.

Shen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., Wu, X., Liu, Y., and Xiong, D. (2023). Large language model alignment: A survey. arXiv preprint arXiv:2309.15025.

Staab, R., Vero, M., Balunović, M., and Vechev, M. (2023). Beyond memorization: Violating privacy via inference with large language models. arXiv preprint arXiv:2310.07298.

Sun, H., Pei, J., Choi, M., and Jurgens, D. (2023). Aligning with whom? large language models have gender and racial biases in subjective nlp tasks. arXiv preprint arXiv:2311.09730.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Wolf, Y., Wies, N., Avnery, O., Levine, Y., and Shashua, A. (2023). Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082.

Ye, F., Yang, M., Pang, J., Wang, L., Wong, D. F., Yilmaz, E., Shi, S., and Tu, Z. (2024). Benchmarking llms via uncertainty quantification. arXiv preprint arXiv:2401.12794.

Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., and Yang, D. (2024). Can large language models transform computational social science? Computational Linguistics, pages 1-55.
