# LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics 

Hengjia Xiao ${ }^{1}$ and Peng Wang ${ }^{2 *}$


#### Abstract

This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named $L L M A^{*}$, aims to leverage the commonsense of $L L M s$, and the utility-optimal $A^{*}$ is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to $L L M s$ on intermediate planning results. This makes the whole path planning process a 'white box' and human feedback guides LLM A* to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against $A^{*}$ and RL shows that LLM $A^{*}$ is more efficient in terms of search space and achieves an on-a-par path with $A^{*}$ and a better path than RL. The interactive nature of $L L M A^{*}$ also makes it a promising tool for deployment in collaborative human-robot tasks.


## I. INTRODUCTION

Path planning together with mapping, localisation, and motion planning has long been considered the cornerstone techniques to enable full autonomous agents such as robots [1]-[3]. Path planning usually takes the environment map, the initial and goal states as inputs and aims to find the (near) optimal path in terms of action cost to move the agent from the initial state to the goal state. A variety of path planning approaches have been proposed and they can be categorised into 1) search-based algorithms such as the classical A* search which is guaranteed to find an optimal path when heuristics are admissible and consistent; 2) samplingbased planning such as the Rapidly-exploring Random Tree (RRT) [4] and Ant Colony Optimisation (ACO) [5], etc.; and 3) Data-driven planning such as neural A* [6] and Reinforcement Learning (RL) based search [7].

$\mathrm{RL}$ is one of the most renowned data-driven planning methods. In a nutshell, RL achieves path planning through trial and error, i.e., when in state $s$, an agent will evaluate rewards it can get from performing a certain action $a$ and choose the action with the best rewards to perform. This helps the agent to learn a 'good' policy that generates an obstacle-free path from the initial state to the goal state. A variety of policies have been proposed [7] and the Proximal Policy Optimisation (PPO) is among these that achieve a balance between convergence efficiency and stability [8].[^0]

While industries and academia are exhilarated by the performance of data-driven planning, they remain codingheavy and black boxes as they are mostly underpinned by deep neural networks. We believe that techniques should eventually improve human well-being, and accordingly, humans, especially normal civilians should be granted access to the underlying mechanisms of such techniques, to control or even terminate certain services whenever and wherever necessary.

The recent advancements in LLMs have made an 'overhaul of data-driven techniques' possible. LLMs are capable of human-like text generation and task completion [9]. The texts generated by LLMs constantly show that they can deal with commonsense information well to a human-like level, especially in a chain-of-prompts manner where humans can enforce/teach LLMs to find the answers in demand. This resembles RL apart from the rewards are assessed and granted by humans through interaction with LLMs. All these have partially distanced people from questioning the explainability of LLMs' horsepower, i.e., the 'transformer', which is a deep neural network itself and focused more on the models' ability to 'answer a question' in each prompt.

This paper harnesses the advantages of $\mathrm{A}^{*}$ being able to find an optimal path and LLMs being able to consider commonsense knowledge when interacting with humans (agents) and proposes the LLM A* for robot path planning.

Briefly, LLM A* can be summarised into three crucial stages. This first stage is to set up the workspace where a robot performs path planning. An environment model and how it will be used by the robot will be communicated to LLMs. The second stage entails the description of the initial state, the goal state, and the heuristics to be used. We have adopted a generic form of heuristic that considers obstacles, distances, and actions the robot can perform in the workspace. It is worth mentioning that the heuristic can be customised according to the setup. The LLMs will then work on finding the next valid 'moves' for path planning. These moves will be assessed against the cost function, which is the combination of the path cost up to now and the heuristic from the current state. This is similar to A*. The best 'move' with the minimum cost will be selected and communicated to humans on request, who can assess the move and decide whether to accept or decline the move, etc. The decision will be communicated to LLMs to carry on completing the planning. The involvement of humans' guidance and assessments can help to minimise the use of tokens, which remains a challenge to use LLMs such as ChatGPT [9].

LLM A* follows a '\{state, action, rewards\}' pattern, which is similar to RL but no explicit coding is needed, and humans have full control of the planning process. To this end, we have compared it to the advanced RL algorithm PPO for performance evaluation. A quantitative comparison with $\mathrm{A}^{*}$ is also conducted. The contributions of this paper include: 1). a first-of-its-kind LLM-based A* is proposed for mobile robot path planning; 2). The interactive and codingfree nature makes LLA* appealing to non-expert robotic users; 3). Improved safety assurance compared to $\mathrm{A}^{*}$ and $\mathrm{RL}$ as humans have full control of the planning process.

The remainder of the paper is organised as follows. Some related works are introduced in Section II. Section III elaborates on the approach. Experiments and discussions are provided in Section IV, and the paper is concluded in Section V.

## II. RELATED WORK

## A. LLM in Robotic Task Planning

Commanding robots through human language/instructions has long been a dream to robotists [10], [11]. Before the remarkable success of LLMs, Anderson et al. [10] have proposed the R2R navigation framework, where Transformers were used to achieve visually grounded natural language navigation by converting human instructions into robot action sequences. Each action will lead the robot to move from one viewpoint to another. How the path is planned between viewpoints were not elaborated and navigation errors up to 10 metres were reported.

Trained on (text) datasets of immense size, LLM models come with commonsense knowledge, which is promising to help accelerate robotic task planning [12]. To the best of our knowledge, most research from literature follows the visual-linguistic grounding strategy and proposes to harness the commonsense knowledge of LLMs to build a better correspondence between human language/instructions and visual perceptions of the robots. Song et al. [12] have demonstrated that LLMs can help with few-shot planning, i.e., the commonsense knowledge of LLMs is used to generate hierarchical plans, with the high-level plan being a set of viable while commonsense aligned subgoals generated by LLMs. The high-level plan is next passed to a low-level planner that is independent of the instructions to plan paths between subgoals. Other works such as [13] still fall into visual-linguistic grounding but a collision check is proposed.

It is worth noting that the works mentioned above do not touch on how LLMs can be used to plan a path between subgoals or viewpoints. Aghzal et al. [14] propose path planning from natural language (PPNL), with a focus on utilising LLMs in spatial-temporal path planning. However, the method is purely prompt-based, without linking and comparing it to path planning methods such as $\mathrm{A}^{*}$ and RL.

## B. Reinforcement Learning for Robotic Planning

Reinforcement learning is another data-driven learningbased framework for path planning. In the RL framework, an agent learns a policy to select a sequence of actions that would lead the agent to efficiently move from one state to another, to eventually reach the goal. The learning process is guided by a policy that rewards good actions while penalising bad actions.

The capacity of deep learning in data representation and processing has drawn a lot of attention from the RL and robotic community. Deep learning models were then integrated into the RL framework and deep reinforcement learning (DRL) was coined [15] to enable more sophisticated and effective policy learning, for robotic task planning, etc. [16].

However, both RL and DRL still face the challenges of 1) low data efficiency that entails the demand for a significant number of interactions with the environment to learn the policies and the demand for large datasets for deep learning model training, and 2) the stability of convergence, i.e., RL/DRL models are not guaranteed to converge. Therefore, recent DRL models have focused on the optimisation for not only the fitness of states but also the measurement of convergence. The PPO model with actor-critic structures is one of the most renowned models. PPO is a policy gradient algorithm to maximise the horizon returns while the actorcritic structure provides a situation in which states and policy can be trained separately so that the agent achieves better planning performance and convergence [17].

The prompt-driven LLM-based path planning can be considered as a variant of RL models, with rewards provided by humans through prompts and policies implicitly learned by LLMs through interactions. We therefore compare the path planning performance of LLM-based methods and RL-based methods in this paper.

## III. THE LLM A* APPROACH

## A. Agent Path Planning Formulation

For agents like mobile robots, path planning aims to find a point-to-point utility-optimal path given an environment representation, a set of valid actions, an initial state, and a goal state. To this end, the path planning problem can be formulated as a graph $\mathscr{G}=(\mathscr{S}, \mathscr{A})$, where $\mathscr{S}$ is a set of states that represent the environment, and $\mathscr{A}$ is a set of potentially valid actions that the robot can perform to traverse from one state to the other. For instance, given two states $\left(s_{i}, s_{j}\right) \in \mathscr{S}$, we can define an edge $a_{i j} \in \mathscr{A}$, which indicates that the agent can move from state $s_{i}$ to state $s_{j}$. The cost related to the action $a_{i j}$ can be denoted as $c\left(a_{i j}\right)$.

We can further define $\left(s_{s}, s_{g}\right) \in \mathscr{S}$ as the initial state and the goal state. The output of path planning will be a sequence of states that form a solution path $\mathscr{P}=\left(s_{0}, s_{1}, \cdots, s_{n}\right)$, with $s_{s}=s_{0}, s_{g}=s_{n}$, and $s_{i} \in \mathscr{S}, i=0,1, \cdots, n$. The path planning is carried out by iteratively expanding from the initial state, and at each state $s$, the valid actions will lead to a set of states $\operatorname{Succ}(s) \subseteq \mathscr{S}$ that could be explored next. The cumulative cost of every state $s \in \operatorname{Succ}(s)$ is calculated and the state to explore next is the one with the minimum cost. This process iterates until the goal $s_{n}$ is reached. For a state $s_{i}, i \in[0, n]$,
the cumulative cost is defined as

$$
\begin{equation*}
g\left(s_{i}\right)=\sum_{j=0}^{j=i-1} c\left(a_{j, j+1}\right) \tag{1}
\end{equation*}
$$

The search process is designed as such to find a path $\mathscr{P}_{\text {min }}$ from $s_{s}$ to $s_{g}$ with a minimum total path cost.

## B. Agent Path Planning with $A^{*}$

A* is one of the most popular search-based path-planning algorithms [5]. On top of the cumulative cost, A* exploits heuristics from priors or experiences to accelerate the search process. While the cumulative cost $g\left(s_{i}\right)$ accounts for the real cost from the initial state $s_{0}$ to $s_{i}$, a heuristic cost is defined as an 'estimate' of the cost from $s_{i}$ to the goal state $s_{n}$, and it is usually denoted as $h\left(s_{i}\right)$. Considering both costs, we introduce the cost of state $s_{i}$ in $\mathrm{A}^{*}$ as

$$
\begin{equation*}
f\left(s_{i}\right)=g\left(s_{i}\right)+h\left(s_{i}\right) \tag{2}
\end{equation*}
$$

When $h\left(s_{i}\right)$ is admissible and consistent, A* is guaranteed to find the optimal path. Therefore, A* has benchmarked to assess the performance of path-planning algorithms.

## C. Path Planning with Reinforcement Learning

Path planning with RL can also be formulated as a graph $\mathscr{G}=(\mathscr{S}, \mathscr{A})$. An agent will start at an initial state $s_{0}$ and learn a policy $\pi$ through constant interactions with the environment. The agent will select an action $a_{i j}$ to traverse from state $s_{i}$ to state $s_{j}$, until it reaches the goal state $s_{n}$. For brevity and without causing ambiguity, we will omit the subscriptions below.

When the agent is in a state $s$, we can define a state value $V(s)$ that indicates the preference of the state. In general, the bigger the value, the agent will be more likely to move to this state. $V(s)$ is usually in the form of

$$
\begin{equation*}
V(s)=\sum_{a} \pi(a \mid s) Q(s, a) \tag{3}
\end{equation*}
$$

with

$$
\begin{equation*}
Q(s, a)=\sum_{s^{\prime}} P\left(s^{\prime}, r \mid s, a\right)\left(r+\gamma V\left(s^{\prime}\right)\right) \tag{4}
\end{equation*}
$$

the action evaluation function that assesses an action $a$ that could complete the transformation from $s$ to $s^{\prime}, P\left(s^{\prime}, r \mid s, a\right)$ is the transformation probability, $r$ is the reward of the action, and $\gamma$ is the discount factor on state $s^{\prime} . \pi(a \mid s)$ is the policy to select an action from all feasible actions to complete the transformation from $s$ to $s^{\prime}$.

The policy $\pi$ is usually parameterised by a learnable variable $\theta$, and then equation (3) can be rewritten as

$$
\begin{equation*}
J(\theta)=V^{\pi}(s)=\sum_{a} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \tag{5}
\end{equation*}
$$

where $J(\theta)$ is the objective function to optimise. Schulman et al. have shown that directly optimise equation (5) commonly causes redundant approximates and high space complexity [8]. To deal with this, Schulman et al. [18] propose to use an alternative objective function

$$
\begin{align*}
L(\theta) & =J(\theta)-J\left(\theta_{\text {old }}\right) \\
& =\sum_{s} \rho_{\theta_{\text {old }}}(s) \sum_{a} \pi_{\theta}(a \mid s) \hat{A}_{\theta_{\text {old }}}(s, a) \tag{6}
\end{align*}
$$

where

$$
\begin{equation*}
\hat{A}^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s) \tag{7}
\end{equation*}
$$

is the advantage that describes the superiority of selecting an action $a$ at state $s$, and $\rho_{\theta_{\text {old }}}$ is the distribution of state $s$ before updating the policy.

In this configuration given by equation (6), the objective is to optimise the total advantages. The PPO model uses clip techniques to avoid the dramatic change of the policy to get better optimisation convergence control [8], [18]. In the PPO setup, equation 6 becomes

$L^{c}(\theta)=\mathbb{E}[\min (r(\theta) \hat{A}(s, a), \operatorname{clip}(r(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}(s, a))]$,

where $r(\theta)=\pi_{\theta}(a \mid s) / \pi_{\theta_{\text {old }}}(a \mid s)$ is the policy ratio that is clipped into $[1-\varepsilon, 1+\varepsilon]$ with $\varepsilon$ a small number set manually.

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-3.jpg?height=648&width=1695&top_left_y=1790&top_left_x=215)

Fig. 1: Pipeline of the proposed LLM A*. LLM block indicates the language model before the conversation with the user, and LLM A* block indicates the model is aware of the scenario and is ready to perform path planning.

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-4.jpg?height=1464&width=1401&top_left_y=173&top_left_x=362)

Fig. 2: Setting the scene through conversation.

## D. LLM A* for Path Planning

LLM A* is an LLM-based path-planning framework inspired by $\mathrm{A}^{*}$ and RL, to utilise commonsense knowledge of LLMs to achieve few-shot, code-free, and near-optimal path planning for robotics. LLMs are considered codefree implicit policy learners, that learn from environment information encoded in a graph $\mathscr{G}=(\mathscr{S}, \mathscr{A})$, and rewards from the cost function shown in equation (2) as well as from humans that are communicated to LLMs through prompts, to achieve near-optimal path planning.

As shown in Fig. 1, building an LLM A* model comprises mainly two stages. The first stage is 'teaching' an LLM to be aware of the environment the agent is in and the motions it can perform, which are encoded in a graph $\mathscr{G}=(\mathscr{S}, \mathscr{A})$. The start and target nodes $\left(s_{s}, s_{g}\right) \in \mathscr{S}$, heuristics, and any other relevant rules are also communicated to the LLM. One example of communicating such information to the LLM is given in Fig. 2
The second stage is the path planning stage. With the necessary information in mind, the LLM will try to plan a path between the initial state $s_{s}$ and the goal state $s_{g}$. As prompts are used to communicate with the LLM, humans could be needed in the planning stage for two reasons: 1) guide the LLM only to communicate necessary information back to humans, which will save costly tokens; 2) provide guidance/heuristics when necessary to accelerate the planning process. The prompts will also enable humans to query about the planning process whenever and wherever necessary, to make the whole planning process a white box to humans. Fig. 3 shows how path planning is delivered by 'chatting' with the LLM.

## IV. EXPERIMENTS AND DISCUSSIONS

## A. Setup

To evaluate the performance of LLM A*, we have conducted a range of experiments to compare it to $\mathrm{A}^{*}$ and

```
User: Now, the agent is at the grid $[1,29]$. Be aware of obstacles I told you and the 8 actions you can perform. All
the actions should serve for the purpose of 'achieving and reaching the target node'. The agent must move in the
correct direction while avoiding obstacles.
LLM: Based on the current location of the agent at grid [1, 29], and considering the obstacles and objective, I will
provide you a subset of the action space that satisfies the following criteria:
Summarising user's message on rules ...
Based on these criteria, the subset of optimal actions for the next step will be determined. I will provide you with a
list containing the corresponding action numbers.
User: Please ONLY provide the action number list with the format 'opt_actions: $[1,2,3]$ '.
LLM: opt_actions: $[0,1,2,3,4,5,6,7]$. Based on the Euclidean distance standard, next step will be moving to grid
$[2,28]$.
User: Now, the agent is at the grid [2,28]. Be aware of obstacles I told you and the 8 actions you can perform. All
the actions should serve for the purpose of 'achieving and reaching the target node'. The agent must move in the
correct direction while avoiding obstacles.
LLM: opt_actions: [0, 1, 2, 3, 7], Based on the Euclidean distance standard, next step will be moving to grid [3, 27].
```

Fig. 3: Path planning through conversation.

RL. For LLM A*, we have considered two variants, i.e., 1) only consider the heuristic $h(s)$ as the cost function, denoted as Greedy LLM A*; 2) consider the combination of the cumulative and heuristic costs $f(s)$ as the cost function, denoted as LLM A*. The GPT3.5-turbo-16k LLM is used as it provides more tokens than GPT3.5-turbo or even GPT4. This is to ensure that we can get the planning results without disruption, not necessarily that the algorithm needs so many tokens. The GPT-3.5-turbo-16k LLM model we used allows a total of 16,384 tokens. In comparison, GPT-3.5-turbo has a maximum number of 4,096 tokens. In case there is a need to reduce the consumption of tokens to replicate the experiments, one can reset the request dictionary to null after each interaction. To enable full control/access to the planning process, one can split a single interaction process into two stages: 1) planning by LLM A*; and 2) outputs of necessary results upon requests.

An occupancy grid map with a size $32 \times 32$ is primarily used for evaluation. There are free spaces and obstacles on the map, where a robot agent can only move in free space and needs to avoid colliding with obstacles. The agent can move in eight directions at maximum wherever and whenever it is safe. All experiments are carried out using Python 3.8+ on Google Colab.

The RL model we used is based on the PPO configuration, together with the standard $\mathrm{A}^{*}$ are used for comparison.

## B. LLM A* Training and Session Design

For experiments that involve LLM, there are two stages including initialization where essential information about the environment and the agent will be prompted to set up planning. The information includes 1) locations of the start and goal states; 2) obstacle distribution; 3) agent action space; 4) Chebyshev distance measurement that is used for cumulative and heuristic costs calculation; and 5) the objective to plan a path for the agent between the start and goal states. In addition, some planning rules can be communicated to LLM as well, which include 1) a viable path should avoid colliding with obstacles; 2) the path should ideally expand along the direction from heuristics or human guidance; 3) selection of the suitable actions that could accelerate the planning process.

In the second interactive planning stage, LLM can return planning results based on the initial information and other prompts to help humans guide or monitor the planning. This stage works iteratively and interactively until a path is successfully planned. It is worth noting that humans can request intermediate planning results at any stage. This makes the planning process a white box to humans, which helps to assure safety, etc.

## C. RL Model Training

The PPO model is employed for comparison. In our case, the PPO model is based on an actor-critic structure that includes two 3-layer deep neural networks for policy and value training, respectively. The model is trained for over $8,000,000$ steps (a total of more than 8,000 episodes) to ensure convergence.

On the other hand, we set the learning rates of both actor and critic to be smaller than 0.0005 to ensure that the model won't be fixed too early, achieving a balance of exploration and exploitation in contrast to the entropy penalty for convergence.

We have also randomised the start state in each episode, to improve the adaptability of the PPO model to different environments. This is achieved by proposing to use of an easy-to-difficult mechanism to train the PPO model. To be specific, we start from the states near the goal in which the agent is so close to it that ideally, it can reach the goal in one step. Next, a state will be randomly chosen from these states to train the PPO model. As the model only has a small scope of the map, it will be 'easy' to converge. After that,

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-6.jpg?height=559&width=550&top_left_y=173&top_left_x=489)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-6.jpg?height=558&width=550&top_left_y=171&top_left_x=1080)

(b)

Fig. 4: Path planning results of $\mathrm{A}^{*}$ and LLM A*.

the scope of the map will be increased to re-train the model until convergence. This process will iterate until the goal state is reached. As the scope of the map increases, it will become 'difficult' for the model to converge, but with the easy-to-difficult mechanism, the model achieves a balance of adaptability to various environments while converging efficiently. After training, we subsequently ran a test episode to make the agent move from the start state toward the goal state and this will generate a path.

## D. Main Results

LLM A* will be first examined for the reduction of the search space compared to A*. Secondly, we compare LLM A* with both LLM Greedy and PPO in terms of path smoothness, etc. This collectively will provide an assessment of the feasibility and superiority of applying LLM in path planning.

1) Search space: Fig. 4 shows the path planning results of A* and LLM A*, respectively. The red indicates the final path and the green shows the overall searched space. One can see that $\mathrm{A}^{*}$ has a bigger search space compared to LLM A*.
On top of that, LLM A* also generates a path that agrees with humans, e.g., the path in the lower-center area goes diagonally rather than following the obstacles. We attribute this partially to the commonsense knowledge of the LLM A* which learns from humans, and partially to potential guidance provided by humans through interactions.
2) Accuracy and path smoothness: Fig. 5 shows the comparison of the path planning results of LLM A*, LLM Greedy, and the PPO model.

We can see that using heuristics only as the cost function or LLM Greedy has the smallest search space. This is because the greedy nature of the heuristics will always try to guide the agent to move towards the goal direction. However, the existence of obstacles will bounce the agent backward, henceforth, the agent has to go back and forth in the final path planned, making it less efficient. In contrast, LLM A* considers both cumulative and heuristic costs, which will expand the search space, but ends up with a 'smoother' path (less back and forth). Nevertheless, both LLM A* and LLM Greedy outperform the RL model, either in terms of path smoothness or the search space. This demonstrates that LLM

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-6.jpg?height=558&width=550&top_left_y=1868&top_left_x=213)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-6.jpg?height=562&width=550&top_left_y=1863&top_left_x=777)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-6.jpg?height=556&width=550&top_left_y=1869&top_left_x=1335)

(c)

Fig. 5: Comparative results: (a), (b) and (c), are results from LLM A*, LLM Greedy, and RL.

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-7.jpg?height=640&width=808&top_left_y=198&top_left_x=233)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_0a09eab24a48328d033bg-7.jpg?height=646&width=811&top_left_y=192&top_left_x=1058)

(b)

Fig. 6: Comparative results: (a) average steps per episode of the PPO model; (b) average scores achieved per episode of the PPO model.

with commonsense could help improve the performance of RL models in tasks such as path planning. Fig. 6 shows the average steps and scores against episodes for the PPO model training. We can see that the PPO model converges in 8,000 episodes and the results are generated from this model.

## E. Discussion

It is undeniable that the utilisation of LLM will provoke more interactions between humans and AI agents. We have demonstrated that by combining LLM with conventional path-planning algorithms such as $\mathrm{A}^{*}$, near-optimal pathplanning results can be achieved, while still leaving humans the opportunity to scrutinise the path-planning process. This takes us one step closer to making AI techniques white boxes to humans and would contribute to safety assurance in human-robot interaction/collaboration.

It is worth noting that the form of planning a path through interaction with LLMs resembles the process of using RL in path planning, and we have demonstrated that the commonsense knowledge from LLMs can significantly improve the path planning performance, both in terms of path smoothness and search space. This will be further explored in future work.

## V. CONCLUSIONS

This paper is the first to introduce the LLM A*, merging Large Language Models (LLMs) and the classic A* algorithm, to revolutionise path planning for mobile embodied agents. By harnessing the inherent commonsense of LLMs and the optimality of $\mathrm{A}^{*}$, LLM A* achieves few-shot nearoptimal path planning compared to rivals such as RL models, while gaining the advantage of granting humans the whole access to the path planning process, which will help to ensure safety when incorporate LLM A* in embodied agent path planning.
Despite the advantages of LLM A*, it is just as inefficient as data-driven methods such as RL models. Future works will focus on improving the efficiency of LLM A*, to strike a trade-off between the interactive capacity and efficiency to facilitate real-life deployment of such models.

## REFERENCES

[1] P. Wang, Q. Zhang, and Z. Chen, "A grey probability measure set based mobile robot position estimation algorithm," International Journal of Control, Automation and Systems, vol. 13, pp. 978-985, 2015.

[2] Q.-b. Zhang, P. Wang, and Z.-h. Chen, "An improved particle filter for mobile robot localization based on particle swarm optimization," Expert Systems with Applications, vol. 135, pp. 181-193, 2019.

[3] P. Wang, L. Mihaylova, P. Bonnifait, P. Xu, and J. Jiang, "Featurerefined box particle filtering for autonomous vehicle localisation with openstreetmap," Engineering Applications of Artificial Intelligence, vol. 105, p. $104445,2021$.

[4] D. González, J. Pérez, V. Milanés, and F. Nashashibi, "A review of motion planning techniques for automated vehicles," IEEE Transactions on intelligent transportation systems, vol. 17, no. 4, pp. 1135-1145, 2015.

[5] C. Carr and P. Wang, "Fast-spanning ant colony optimisation (fasaco) for mobile robot coverage path planning," arXiv preprint arXiv:2205.15691, 2022.

[6] R. Yonetani, T. Taniai, M. Barekatain, M. Nishimura, and A. Kanezaki, "Path planning using neural a* search," in International Conference on Machine Learning. PMLR, 2021, pp. 12029-12039.

[7] C. Zhou, B. Huang, and P. Fränti, "A review of motion planning algorithms for intelligent robots," Journal of Intelligent Manufacturing, vol. 33, no. 2, pp. 387-424, 2022.

[8] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.

[9] OpenAI, "ChatGPT: Engaging openai's conversational ai," https:// openai.com/blog/chatgpt 2023, [Accessed on November 20, 2023].

[10] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3674-3683.

[11] C. Carr, P. Wang, and S. Wang, "A human-friendly verbal communication platform for multi-robot systems: Design and principles," arXiv preprint arXiv:2211.09519, 2022.

[12] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su, "Llm-planner: Few-shot grounded planning for embodied agents with large language models," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 2998-3009.

[13] A. Xie, Y. Lee, P. Abbeel, and S. James, "Language-conditioned path planning," arXiv preprint arXiv:2308.16893, 2023.

[14] M. Aghzal, E. Plaku, and Z. Yao, "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning," arXiv preprint arXiv:2310.03249, 2023.

[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, "Playing atari with deep reinforcement learning," arXiv preprint arXiv:1312.5602, 2013.

[16] C. Zhou, X. Lu, J. Dai, B. Huang, X. Liu, and P. Fränti, "Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning," arXiv preprint arXiv:2309.03758, 2023 .

[17] M. Andrychowicz, A. Raichuk, P. Stańczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, et al., "What matters for on-policy deep actor-critic methods? a large-scale study," in International conference on learning representations, 2020.

[18] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, "Trust region policy optimization," in International conference on machine learning. PMLR, 2015, pp. 1889-1897.


[^0]:    *This work was supported by Manchester Metropolitan University. Corresponding to Peng Wang (p.wang@mmu.ac.uk)

    ${ }^{2}$ Peng Wang is with the Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, M15 6BH, UK p.wang@mmu. ac.uk

