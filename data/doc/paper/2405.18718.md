# Efficient Model-agnostic Alignment via Bayesian Persuasion 

Fengshuo Bai ${ }^{1,2,3}$ Mingzhi Wang ${ }^{2, \dagger *}$ Zhaowei Zhang ${ }^{2,3, \dagger}$ Boyuan Chen $^{2, \dagger}$ Yinda Xu $^{1}$<br>Ying Wen ${ }^{1, \ddagger} \quad$ Yaodong Yang ${ }^{2, \ddagger}$<br>${ }^{1}$ Shanghai Jiao Tong University<br>${ }^{2}$ Institute for Artificial Intelligence, Peking University<br>${ }^{3}$ National Key Laboratory of General Artificial Intelligence, BIGAI


#### Abstract

With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of $16.1 \%$ in mathematical reasoning ability and $13.7 \%$ in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.


## 1 Introduction

Recent years have witnessed increased attention and effort in aligning large language models (LLMs) with human intentions and values [35, 26]. This alignment is facilitated by providing reliable supervision through demonstrations [8, 45], reward signals [37], preferences [16, 40] or critiques [42, 5], and by employing methods such as supervised learning (e.g., Supervised Fine-tuning, SFT) or reinforcement learning (e.g., Reinforcement Learning from Human Feedback, RLHF) [37].

However, these methods, including RLHF, require multiple models and direct training of large models, which significantly increases computational demands [40]. Moreover, Fine-tuning cannot be applied to closed-source models, complicating output control for alignment with human intents. Additionally, current alignment methods, like SFT and RLHF, face limitations when human evaluators lack expertise in complex tasks $[7,43]$. These challenges highlight the need for efficient, scalable alignment strategies for both open-source and closed-source models. Therefore, our work aims to address the above challenges by answering the following question:[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_fb916625e5f139c5aeb0g-02.jpg?height=680&width=1328&top_left_y=251&top_left_x=388)

Figure 1: An illustration of our persuasion framework. The Receiver observes a signal $g$ sent by the Advisor and updates its belief from the prior distribution $\mu^{0}$ to a posterior distribution $\mu^{g}$. The axes depict the Advisor's expected utility $\hat{v}(\mu)$ across various information distributions $\mu$. In this context, $\operatorname{co}(\hat{v})$ denotes the convex hull of $\hat{v}$, while $V$ represents the concave closure of $\hat{v}$. Here, $V(\cdot)$ is the largest expected utility Advisor can achieve with any signal. From the Advisor's perspective, the Receiver's performance is enhanced following persuasion.

## Can we use a smaller model to influence the behaviors of larger models, thereby enabling alignment and enhancing performance with little human supervision or feedback?

Inspired by Bayesian persuasion [30], we model the alignment problem as an information design process, involving a protocol between a small model and a large model. Instead of training multiple models as in techniques like RLHF, we only employ a small model with minimal supervision to learn a signaling strategy that influences the behaviors of fixed large models. In this setup, large models are treated as a black box. This modeling approach significantly reduces the demand for computational resources by delegating alignment tasks to smaller models, leading to a more lightweight and efficient framework. This makes it inherently suitable for a broader range of alignment scenarios, accommodating tasks of varying difficulty and larger models.

In this work, we introduce a novel framework termed Bayesian Persuasion Alignment, as illustrated in Figure 1. We frame alignment as a Bayesian Persuasion (BP) problem, wherein a smaller model serves as the Advisor and a larger model acts as the Receiver. In this setup, the Advisor generates a signal that is sent to the Receiver. Upon receiving this signal, the Receiver updates its beliefs and produces a response. Our core insight is that a smaller model trained on supervision for optimal signaling strategy can effectively persuade larger models, thereby improving the quality of their responses, i.e., their outputs. This mechanism provides several advantages: (1) From the perspective of information design, the well-defined signaling strategy of the Advisor ensures an increase in the Advisor's utility without decreasing the Receiver's utility [30]. (2) The Advisor manipulates the Receiver's belief to enhance performance, significantly reducing the need for training resources while ensuring alignment performance, making it an effective and parameter-efficient alignment strategy. (3) Moreover, the learned signaling strategy can be applied to different Receivers, guaranteeing a model-agnostic nature and making it easier to generalize to harder tasks.

To the best of our knowledge, BP Alignment is the first integration of Bayesian persuasion with the alignment framework. Our main contributions can be summarized in three folds: First, we introduce a parameter-efficient and model-agnostic alignment framework that trains a smaller model to enhance the performance of various larger models. Second, we demonstrate that our persuasion framework significantly improves the performance of various large models on mathematical problem-solving and code-generation tasks. Specifically, the Advisor (Phi-2) enables significant enhancements, with an average improvement of $22.5 \%$ on GSM8K [17], $39.0 \%$ on MATH [23], and a $24.7 \%$ increase on

HumanEval [12]. Lastly, we theoretically analyze our framework and provide an upper bound on the Advisor's regret, indicating its effectiveness in learning the optimal signaling strategy.

## 2 Related Work

In this section, we will review existing works on Scalable Oversight, AI Persuasion, and Eliciting Latent Knowledge (ELK).

Scalable Oversight. As models begin to achieve broadly human-level performance and take on more complex tasks that are difficult for humans to understand, providing continual, reliable feedback and ensuring that the models' behaviors align with human intents becomes challenging. This naturally raises the crucial issue of scalable oversight: how can we provide supervisory signals to more powerful AI systems and ensure they are aligned with human intents? [36, 2, 26] Unlike current methods that focus on enhancing the capabilities of weak supervisors [14, 7, 31], our framework addresses this challenge by transforming weak supervisors into persuaders and identifying the optimal signal-sending strategy to effectively influence the behaviors of stronger models.

Our work also differs from weak-to-strong generalization [10] and similar alignment methods [27, 32]. These methods face a trade-off: the strong model may either mimic the weak model, reducing performance or use its reasoning abilities to improve [10]. Additionally, they often rely heavily on ground truth labels. In contrast, our approach uses small models as Advisors to elicit the capabilities of stronger models without adding noisy labels. Guided by the information design principle, our method scales to various stronger models and challenging tasks, minimizing the need for ground truth. We evaluate our method on various mathematical problem-solving and code-generation tasks using only GPT-2 as the Advisor. It achieves significant advancements, with an average improvement of $9.5 \%$ on GSM8K, $22.6 \%$ on MATH, and $13.7 \%$ on HumanEval across a range of strong models.

AI Persuasion. Persuasion is a dynamic game process in which one player (sender) influences the beliefs or actions of another player (receiver) by providing informative signals, thereby affecting the outcomes for both players. AI persuasion can be categorized into two types based on the target: (1) AI persuading humans to change their original viewpoints (i.e., Captology) [21, 47, 33, 19], and (2) employing persuasive signals to change the behavior of AI systems. While most existing research has concentrated on the former, studies on the latter remain relatively nascent. In this paper, we primarily explore the latter category.

Zeng et al. [48] conducted a comprehensive review of decades of social science research and proposed a taxonomy to automatically generate persuasive adversarial prompts that induce unsafe behaviors in LLMs. However, their method lacks a formal definition or analysis of persuasive behavior and its impact. Bayesian persuasion [30,29] is a symmetric information framework that utilizes to influence beliefs by strategically sharing information aligned with motivations, aiding decision-making tasks [22]. While its application to language models remains unexplored, [49] suggested it could align AI during deployment by tailoring information based on different scenarios. To the best of our knowledge, we are the first to use dialogue to apply Bayesian persuasion to LLMs and enhance their capabilities.

Eliciting Latent Knowledge. Christiano et al. [15], Hobbhahn [24] introduced a theoretical framework termed Eliciting Latent Knowledge (ELK), designed to extract latent knowledge from models to assess whether AI systems align with human intents. This framework includes aspects such as honesty analysis [20], knowledge elicitation [9, 38], and general task knowledge acquisition [10]. Our framework utilizes advisor persuasion to elicit strong models' latent knowledge for solving difficult tasks and has the potential for honesty analysis.

Typical ELK methods struggle to train models to report true beliefs rather than just aligning with human preferences. This issue arises because both strategies yield identical training losses, as they produce the same answers to training inputs [24]. As a result, models tend to align with human expectations instead of reporting their own beliefs. Our method addresses this by decoupling the training objective into a reporting objective and a persuasion objective, focusing on optimal signaling rather than human-evaluated ground truth.

## 3 Bayesian Persuasion Alignment

In this section, we formally introduce the proposed persuasion framework. We begin by establishing the notations and outlining the persuasion protocol, followed by defining the overall objective and conducting a theoretical analysis of regret.

### 3.1 Protocol and Notations

We introduce the persuasion protocol wherein an Advisor (small model) persuades a Receiver (large model) to improve its response to a given input. For each input $x$, a finite set $\mathcal{C}_{x}$ contains all associated information items (i.e., state) with input $x$. The Receiver's utility function $u(x, c, y)$ is continuous and dependent on its response $y \in \mathcal{Y}$ to the input $x \in \mathcal{X}$ and the associated information item $c \in \mathcal{C}_{x}$. Similarly, the Advisor has a continuous utility function $v(x, c, y)$, which is contingent on the Receiver's response, input, and associated information item. Importantly, in our settings, the Advisor lacks knowledge of the Receiver's utility function. For each input $x$, the Advisor and Receiver start with a shared prior $\mu_{x}^{0} \in \operatorname{int}\left(\Delta\left(\mathcal{C}_{x}\right)\right)^{2}$. The signaling strategy $\pi$ is defined by a finite realization space $\mathcal{G}$ and a family of distributions $\pi_{x}(\cdot \mid c), c \in \mathcal{C}_{x}$ over $\mathcal{G}$. This strategy is implemented through a neural network with parameters $\theta$. The Advisor sends a signal, and the Receiver observes the chosen signal realization $g \in \mathcal{G}$ (with $|\mathcal{G}|<\infty$ ).

The game timing in persuasion is as follows: The Advisor commits to a signaling strategy $\pi_{\theta}$ and announces it to the Receiver. For a given input $x$, the Advisor observes an information item sampled from $\mu_{x}^{0}$ and sends a signal $g$ to the Receiver. Upon receiving this signal, the Receiver updates its belief about information item in $\mathcal{C}_{x}$, forming posterior distribution $\mu_{x}^{g}$ via Bayes's rule. The Receiver then chooses a response from the response set, which is defined by

$$
\begin{equation*}
y^{*}\left(\mu_{x}^{g}\right)=\arg \max _{y \in \mathcal{Y}} \underset{c \sim \mu_{x}^{g}}{\mathbb{E}}[u(x, c, y)] \tag{1}
\end{equation*}
$$

The solution of the game is the problem of optimal signaling strategy design from the Advisor's point of view. Taking the Receiver's response as given, the Advisor selects a signaling strategy $\pi_{\theta}$ that maximizes its expected utility. Since the responses are generated only by the Receiver, the Advisor cannot directly influence the information set. Instead, the Advisor can leverage its informational advantage concerning the information item to influence the Receiver indirectly by way of signaling, thereby persuading the Receiver to generate improved responses.

### 3.2 Signaling Strategy and Belief Update

A signaling strategy of the Advisor generates a distribution over $\mathcal{G}$, which is the signal realization space. Formally, the signaling strategy $\pi_{\theta}$ comprises a function $\pi_{x}: \mathcal{C} \rightarrow \Delta(\mathcal{G})$ for each input $x \in \mathcal{X}$. Upon observing an information item $c$, the Advisor sends a signal sampled from $\pi_{x}(c)$, where the input is $x$, and $\pi_{x}(g, c)$ denotes the probability of $g \in \mathcal{G}$ within this distribution.

The signal space $\mathcal{G}$ is broadly construed, including some uninformative signaling strategies. For instance, the Advisor may send the same signal regardless of the information item $c$, such that $\pi_{x}(c)=\pi_{x}\left(c^{\prime}\right)$ for all $c, c^{\prime} \in \mathcal{C}_{x}$. Without loss of generality, we assume that signals in $\mathcal{G}$ are perceived as distinct by the Receiver.

Upon receiving a signal $g$, the Receiver updates its posterior belief regarding the information items. The conditional probability of the information item being $c$ is defined as:

$$
\begin{equation*}
\operatorname{Pr}\left(c \mid g, \pi_{x}\right)=\frac{\mu_{x}(c) \pi_{x}(g \mid c)}{\sum_{c^{\prime} \in \mathcal{C}_{x}} \mu_{x}\left(c^{\prime}\right) \pi_{x}\left(g \mid c^{\prime}\right)} \tag{2}
\end{equation*}
$$

The derivation of the Receiver's posterior belief also depends on its knowledge of the signaling strategy $\pi_{\theta}$. In accordance with the Bayesian persuasion framework, the Advisor commits to a signaling strategy $\pi_{\theta}$ at the beginning of the process and announces it to the Receiver.

### 3.3 Signaling Strategy Optimization

From the Advisor's perspective, the objective is to identify a signaling strategy $\pi_{\theta}$ that maximizes the Advisor's expected utility, thereby inducing superior responses from the Receiver. Accordingly, the[^1]signaling strategy is optimized by minimizing the following loss function:

$$
\begin{equation*}
\mathcal{L}(\theta)=-\underset{x \in \mathcal{X}}{\mathbb{E}}\left[\sum_{c \sim \mathcal{C}_{x}} \operatorname{Pr}\left(c \mid g, \pi_{x}\right) v(x, c, y)\right] \tag{3}
\end{equation*}
$$

where $y$ is the Receiver's response to input $x$ as determined by equation (1).

### 3.4 Regret Analysis

Although the Bayesian Persuasion Alignment framework we propose is practically appealing, a pivotal theoretical question arises: How can we ensure that this framework robustly learns the optimal signaling strategy over time? Specifically, it is crucial to demonstrate that the Advisor can gradually find the most persuasive signaling strategy through its interactions with the Receiver. This convergence guarantee is essential for our framework. To address this question, we draw inspiration from the online Bayesian persuasion setting [11, 6] and analyze the performance of our algorithm from an online learning perspective. We introduce the concept of regret, which quantifies the utility difference between the algorithm's performance and the optimal strategy over a certain period. Demonstrating that the algorithm's regret grows sublinearly with time would imply that it can progressively converge to the optimal strategy.

Without loss of generality, we focus on signaling schemes that are both direct and persuasive [3], according to the revelation principle. In a direct signaling scheme, the signals directly correspond to response recommendations for the Receiver. Moreover, a signaling scheme is considered persuasive if it incentives the Receiver to follow the response recommendations provided by the Advisor. Let $\mathcal{P}$ denote the set of direct and persuasive signaling schemes, where each element $\phi \in \mathcal{P}$ is a mapping $\phi: \mathcal{C} \rightarrow \Delta(\mathcal{Y})$. To simplify the notation, we omit $x$ in subsequent analysis. With the definition of the set of persuasive signaling schemes $\mathcal{P}$, the Advisor's expected utility under a signaling scheme $\phi \in \mathcal{P}$ can be expressed as follows:

$$
\begin{equation*}
v(\phi):=\sum_{c \in \mathcal{C}} \sum_{y \in \mathcal{Y}} \mu_{c} \phi_{c}(y) v(c, y) \tag{4}
\end{equation*}
$$

where $\mu_{c}$ represents the prior probability of information item $c, \phi_{c}(y)$ denotes the probability that the signaling scheme $\phi$ recommends response $y$ under information item $c$, and $v(c, y)$ is the Advisor's utility when the Receiver takes response $y$ under information item $c$.

Next, we introduce a linear mapping $f$ that maps each signaling scheme $\phi \in \mathcal{P}$ to a point in the $\mathbb{R}^{|\mathcal{Y}|}$ space. Specifically, for each $\phi \in \mathcal{P}$, we define

$$
\begin{equation*}
f(\phi):=[-v(\phi, y)]_{y \in \mathcal{Y}} \tag{5}
\end{equation*}
$$

where $v(\phi, y)=\sum_{c \in \mathcal{C}} \mu_{c} \phi_{c}(y) v(c, y)$ represents the Advisor's expected utility when the Receiver takes response $y$ under signaling scheme $\phi$.

Intuitively, the mapping $f$ represents each signaling scheme as a $|\mathcal{Y}|$-dimensional vector, where each component $-v(\phi, y)$ represents the negative of the Advisor's expected utility for response $y$. This representation embeds the signaling schemes into a Euclidean space that directly corresponds to the Receiver's response space. Furthermore, we examine the convex hull of the graph of $f$, denoted as $\operatorname{co} f(\mathcal{P})$. Each point within co $f(\mathcal{P})$ corresponds to a convex combination of signaling schemes.

Formally, the Advisor's regret at round $T$ is defined as:

$$
\begin{equation*}
R_{T}:=\max _{\phi \in \mathcal{P}} \sum_{t=1}^{T} v(\phi)-\sum_{t=1}^{T} v\left(\phi_{t}\right) \tag{6}
\end{equation*}
$$

To analyze the regret bound of our persuasion framework, we present the theoretical version of our algorithm in Algorithm 1.

```
Algorithm 1 Theoretical Persuasion Algorithm
Require: Set of information items $\mathcal{C}$, set of responses $\mathcal{Y}$, prior distribution $\mu^{0}$, Advisor's utility
    function $v$, regret minimizer $\mathcal{R}$ for the set $\operatorname{co} f(\mathcal{P})$
    for $t=1, \ldots, T$ do
        $\operatorname{co} f(\mathcal{P}) \ni z_{t} \leftarrow \mathcal{R} . \operatorname{RECOMMEND}()$
        $\left\{\left(\phi_{t}^{(i)}, \lambda_{t}^{(i)}\right)\right\}_{i \in[m+1]} \leftarrow \operatorname{DECOMPOSE}\left(z_{t}, f(\mathcal{P})\right) \quad \triangleright$ Caratheodory's Theorem
        Sample $i_{t} \in[m+1]$ with probabilities $\lambda_{t}^{(1)}, \ldots, \lambda_{t}^{(m+1)}$
        Let $\phi_{t} \leftarrow \phi_{t}^{\left(i_{t}\right)}$
        Observe information item $c_{t} \sim \mu^{0}$
        Select and play action $y_{t} \sim \phi_{t}\left(\cdot \mid c_{t}\right)$
        $\mathcal{R} . \operatorname{OBSERVE}\left(v\left(c_{t}, y_{t}\right)\right)$
    end for
```

The key idea behind this algorithm is to maintain a regret minimizer $\mathcal{R}$ over the possible signaling strategies, represented by the convex hull of the set of posterior distributions co $f(\mathcal{P})$. At each round $t$, the algorithm obtains a recommended strategy $z_{t}$ from $\mathcal{R}$ and decomposes it into a convex combination of extreme points $\left\{\left(\phi_{t}^{(i)}, \lambda_{t}^{(i)}\right)\right\}_{i \in[m+1]}$ using Caratheodory's Theorem [18]. The algorithm then samples an index $i_{t}$ according to the weights $\lambda_{t}^{(i)}$ and plays the corresponding signaling strategy $\phi_{t}=\phi_{t}^{\left(i_{t}\right)}$. Upon observing the realized information item $c_{t}$ and the Advisor's utility $v\left(c_{t}, y_{t}\right)$ for the chosen reponse $y_{t}$, the algorithm updates the regret minimizer with this feedback.

Under this theoretical algorithm, we can derive the following regret bound.

Theorem 1. Algorithm 1 guarantees regret $R_{T}=O\left(m^{3 / 2} \sqrt{T \log T}\right)$, where $m=|\mathcal{Y}|$ is the number of receiver's reponses.

The regret bound presented in Theorem 1 demonstrates that our algorithm achieves sublinear regret over the time horizon $T$, with a dependence on the size of Receiver's response space $m$. The output space of LLMs is theoretically infinite, as they can generate text of arbitrary length. However, each response's length is practically limited. Additionally, responses with same semantics are considered equivalent given a specific input. Therefore, the Receiver's response space can be regarded as finite, aligning well with the assumptions in our theoretical analysis. Although there are differences between the theoretical algorithm and its practical implementation, the core principle of learning the optimal signaling strategy through interaction remains consistent. This consistency provides a theoretical guarantee for the algorithm's performance and demonstrates its effectiveness in learning the optimal signaling strategy over time.

## 4 Experiments

In this section, we evaluate the effectiveness of our persuasion framework on mathematical problems and code generation. Our evaluation aims to address the following key questions:

(1) Can our framework enhance the Receiver's performance in various tasks? (Section 4.2.1)

(2) Can our framework find a non-trivial signaling strategy? (Section 4.2.2)

(3) How about the efficiency of the proposed framework? (Section 4.2.4)

Furthermore, we investigate the generalization of the signaling strategy across different Receivers (Section 4.2.1), across varying difficulties (Section 4.2.3), and for various tasks (Appendix B.1). Details on experiments are provided in Appendix A.

### 4.1 Settings

Implementation Details. We train the Advisor for math-solving tasks using the training datasets from GSM8K and MATH, and for the code generation task using the training dataset from MBPP. We construct an information set for each input using Llama3-8B-Instruct ${ }^{3}$ for all datasets. Specifically, each input includes seven information items, each emphasizing different key aspects essential for problem-solving. Further details on information set construction are provided in Appendix A.1. In[^2]

Table 1: Performance of various Receivers under persuasion. We report the accuracy on GSM8K and MATH, and the pass@ 1 score on HumanEval across four information structures. "Posterior Information" refers to sampling the information item from the posterior distribution, influenced by the Advisor. The Advisor for math tasks differs from that for code generation tasks. Arrows indicate performance improvements relative to the prior distribution.

| Task | Receiver | No <br> Information | All <br> Information | Prior <br> Information | Posterior Information |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | Advisor (GPT-2) | Advisor (Phi-2) |
| GSM8K <br> (8-shot) | Phi-2 | 56.0 | 41.0 | 56.8 | 59.1 | 62.1 |
|  | Mistral-7B | 34.3 | 48.0 | 45.7 | 50.4 | 53.8 |
|  | Llama2-7B | 15.1 | 36.6 | 27.2 | 34.5 | 45.6 |
|  | Llama2-7B-Chat | 21.8 | 31.8 | 37.3 | 40.0 | 50.0 |
|  | Llama2-13B | 25.2 | 38.9 | 36.2 | 38.9 | 45.9 |
|  | Llama2-13B-Chat | 33.9 | 37.3 | 36.1 | 37.9 | 39.2 |
|  | Llama3-8B | 47.6 | 54.0 | 53.7 | 56.0 | 62.6 |
|  | Llama3-8B-Instruct | 73.5 | 72.2 | 72.3 | 74.5 | 75.4 |
|  | Vicuna-7B | 14.9 | 19.9 | 29.9 | 35.1 | 43.2 |
|  | Vicuna-13B | 23.0 | 24.8 | 35.0 | 43.9 | 49.2 |
|  | Vicuna-33B | 43.2 | 44.1 | 47.8 | 53.1 | 58.5 |
|  | Average (accuracy) | $35 . \overline{3}$ | $-4 \overline{0} . \overline{8}$ | $\overline{43} .5$ | $4 \overline{7} .6(9.5 \%-\overline{\%}$ | $\overline{5} \overline{3} . \overline{2} \overline{(\mathbf{2 2} . \overline{5} \%} \bar{\uparrow})$ |
| MATH <br> (4-shot) | Phi-2 | 10.1 | 11.6 | 11.5 | 13.9 | 15.4 |
|  | Mistral-7B | 6.4 | 10.3 | 7.9 | 9.3 | 10.8 |
|  | Llama2-7B | 4.1 | 9.5 | 6.3 | 8.6 | 10.3 |
|  | Llama2-7B-Chat | 4.6 | 7.8 | 6.0 | 8.0 | 10.4 |
|  | Llama2-13B | 4.5 | 9.7 | 7.7 | 9.6 | 11.4 |
|  | Llama2-13B-Chat | 5.2 | 9.8 | 7.3 | 9.2 | 10.5 |
|  | Llama3-8B | 11.0 | 16.1 | 12.8 | 15.9 | 16.0 |
|  | Llama3-8B-Instruct | 18.1 | 18.6 | 18.1 | 18.9 | 19.7 |
|  | Vicuna-7B | 3.8 | 10.1 | 6.7 | 8.8 | 10.5 |
|  | Vicuna-13B | 3.8 | 11.1 | 6.7 | 9.5 | 11.0 |
|  | Vicuna-33B | 6.8 | 13.1 | 9.3 | 11.2 | 13.4 |
|  | Average (accuracy) | $\overline{7} . \overline{1}$ | $\overline{1} \overline{1} . \overline{6}$ | $\overline{9} . \overline{1}$ | $\overline{1} \overline{1} . \overline{2} \overline{(\mathbf{2}} \overline{\mathbf{2}} \overline{\mathbf{6}} \overline{\%} \bar{\uparrow})$ | $\overline{12} \overline{7} \overline{7} \overline{\mathbf{3}} \overline{\mathrm{0}} \overline{\mathbf{0} \%} \bar{\uparrow})$ |
| HumanEval <br> $(0$-shot $)$ | Phi-2 | 45.7 | 35.1 | 39.6 | 45.7 | 49.4 |
|  | Mistral-7B | 28.3 | 32.4 | 31.2 | 33.2 | 35.4 |
|  | Llama2-7B | 12.2 | 16.2 | 14.4 | 16.2 | 18.3 |
|  | Llama2-7B-Chat | 13.4 | 16.3 | 12.6 | 15.6 | 22.6 |
|  | Llama2-13B | 17.1 | 17.7 | 16.5 | 19.5 | 21.3 |
|  | Llama2-13B-Chat | 19.5 | 17.2 | 16.1 | 17.4 | 22.0 |
|  | Llama3-8B | 27.4 | 23.8 | 24.6 | 31.7 | 37.2 |
|  | Llama3-8B-Instruct | 56.7 | 48.1 | 53.2 | 56.3 | 59.5 |
|  | CodeLlama-7B | 31.1 | 30.5 | 28.9 | 31.2 | 32.9 |
|  | CodeLlama-13B | 36.5 | 39.0 | 35.4 | 40.8 | 40.2 |
|  | CodeLlama-34B | 48.7 | 45.1 | 41.8 | 49.7 | 53.2 |
|  | Average-(pass @ $\overline{1})$ | 30.6 | $\overline{2} \overline{9} . \overline{2}$ | 28.6 | $\overline{3} \overline{2} .5 \overline{1} \overline{3} . \overline{7} \% \bar{\uparrow})$ | $\overline{3} \overline{5} . \overline{6}(\overline{\mathbf{2}} \overline{4.7} \overline{\mathbf{\%}} \bar{\uparrow} \bar{\uparrow})$ |

practical implementation, the Advisor's utility function is defined as the logarithm of the probability of generating the correct answer, while the Receiver's utility function is $u(x, c, y)=P(y \mid x, c)$, naturally aligning the model's inherent mechanism with the Receiver's behavior. A detailed description and intuition for the setup of the utility function are provided in Appendix A.2.

Datasets. For a comprehensive evaluation of the ability, we select two kinds of tasks: math problems and code generation.

- GSM8K [17] is high-quality linguistically diverse grade school math word problems created by human problem writers, which contains $7.5 \mathrm{k}$ training problems and $1 \mathrm{k}$ test problems.
- MATH [23] is a dataset of challenging competition mathematics problems, which is segmented into $7.5 \mathrm{k}$ training problems and $5 \mathrm{k}$ testing problems.
- HumanEval [12] is a code evaluation benchmark consisting of 164 original programming questions, assessing language comprehension, algorithms, and basic mathematics, with some questions equivalent to simple software interview questions.
- MBPP [4] consists of approximately $1 \mathrm{k}$ crowdsourced Python programming problems, covering basic programming knowledge, standard library functionalities, etc. This dataset is only used for the training of the Advisor models.

Advisor and Receiver. In our persuasion framework, we employ two models: an Advisor (small model) and a Receiver (large model). For the Advisor, we select two well-known open-source small models: GPT-2 [39] (124M) and Phi-2 [25] (2.7B). To broadly investigate the generalization of the proposed method across various models, we consider several large models as Receivers: Phi-2 [25] (2.7B), Llama-2 [46] (7B, 13B), Llama-3 (8B), CodeLlama [41] (7B, 13B, 34B), Vicuna [13] (7B, 13B, 33B), and Mistral [28] (7B).

Evaluation Metrics. For the math problems, we determine accuracy by extracting the last number from the generated responses and comparing it directly to the ground truth. For the code generation tasks, our evaluation focuses on assessing the functional correctness of LLM-synthesized code. We use the unbiased version of the pass @ 1 [12] setting for both tasks, namely only generating one result per round. In practice, we use the tool chain-of-thought-hub ${ }^{4}$ and DeepSeek-Coder ${ }^{5}$ to perform the evaluation process for math and code generation tasks, respectively.

Evaluation Settings. For any given input, there is a corresponding set of information item, each item of which is related to the input. In our experiments, we examine four information structures. Given the specific input, the Receiver may observe: (1) No Information items, (2) All Information items, (3) an item sampled from the Prior Information distribution, or (4) an item sampled from the Posterior Information distribution. Naturally, the variation in information structure has an impact on the quality of the Receiver's response.

### 4.2 Results

We evaluate the Advisor with various Receiver models to investigate the effectiveness of its signaling strategy on math problem-solving and code generation tasks. Table 1 shows the Advisor improves the performance of various models through persuasion instead of training. Additional experiments demonstrate that our persuasion framework can identify a non-trivial signaling strategy, which exhibits superior performance in terms of efficiency and generalization.

### 4.2.1 Performance on Persuasion

To investigate the effectiveness of our persuasion framework, we conduct an experimental evaluation of the Receiver's behavior under prior information distribution and posterior information distribution. Table 1 illustrates that Advisor can significantly improve different Receiver's performance across a variety of tasks. Comparing the Receiver's performance without additional information to that with prior information, we find that additional information enhances the Receiver's performance. From the perspective of persuasion, prior and posterior distributions share the same information set. Instead of training, the Advisor (GPT-2) can significantly enhance the performance of various models, achieving an average improvement of $9.5 \%$ on GSM8K, $22.6 \%$ on MATH, and $13.7 \%$ on HumanEval. When considering the increment in the model parameters for the Advisor, a larger one (Phi-2) enables significant enhancements, with an average improvement of $22.5 \%$ on GSM8K, $39.0 \%$ on MATH, and a $24.7 \%$ increase on HumanEval. One important observation can be noticed: a good signaling strategy by the Advisor can effectively persuade different Receivers.

### 4.2.2 Impact on Information Structure

In our experiment, we also analyze the impact of different information structures on the Receiver. In the persuasion process, the receiver combines information items from the information set with input to generate a response. From the perspective of prompt engineering, we evaluate the quality of responses when the receiver either disregards information items or considers all information items, to demonstrate the effect of information selection. For 'No Information', it serves as a baseline, equivalent to standard performance testing for LLMs. As shown in Table 1, when the Receiver can access all information items, its performance improves. However, it is noteworthy that for some models, using all information items results in minimal gains or even a decline in performance compared to not using the information. It can be explained that providing too much information disperses the model's attention and risks exceeding the model's maximum window length.

### 4.2.3 Easy-to-Hard Generalization

In the extended evaluation, we investigate the generalizability of the Advisor. The results presented in Table 1 demonstrate that the Advisor's signaling strategy is effective across various Receiver,[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_fb916625e5f139c5aeb0g-09.jpg?height=517&width=621&top_left_y=327&top_left_x=405)

(a) MATH Level 1-3 (easy)

![](https://cdn.mathpix.com/cropped/2024_06_04_fb916625e5f139c5aeb0g-09.jpg?height=520&width=618&top_left_y=328&top_left_x=1095)

(b) MATH Level 4-5 (hard)

Figure 2: Performance of Receiver on easy and hard problems. The Advisor (GPT-2 and Phi-2) is trained on easy problems of training set (MATH level 1-3), and we observe that the capability of the Receiver greatly improved on both easy and hard tasks with the persuasion signal of the Advisor. In both subfigures, our method outperforms scenarios where no information or only prior information is given, and it even surpasses scenarios where all information is provided for most Receiver models.

confirming its broad applicability. Following Sun et al. [44], we evaluate our framework's Easy-toHard Generalization, which is defined as the ability to address hard tasks by training on simpler ones. We train our Advisor on easy problems (levels 1-3) from the MATH training dataset and assess their effectiveness in persuading various models on both easy (levels 1-3) and complex problems (levels 4-5) of the MATH test dataset. As shown in Figure 2, we observe that advisors not only enhance the performance of various receiver models on easy problems but also improve their performance on hard problems, which are only trained exclusively with supervision on easy problems.

### 4.2.4 Efficiency on Persuasion

The efficiency of our framework lies in two aspects. On one hand, a well-trained Advisor can persuade various models to elicit better responses. On the other hand, during the inference stage, our method achieves enhanced Receiver's performance with fewer input tokens. To better understand the relationship between performance improvement and prompt length, we design the Average Relative Performance Improvement (ARPI) metric to measure the improvement in performance relative to the increase in prompt length.

Average Relative Performance Improvement (ARPI). To compare the performance of a specific information structure with 'No Information' across several receivers, let $R(A)$ represent the performance of structure $A$, and let $L$ denote the length of the input prompt tokens. We define $\operatorname{ARPI}(A \mid B)$ as follows:

$$
\begin{equation*}
\operatorname{ARPI}(A \mid B)=\frac{1}{N} \sum_{i=1}^{N} \frac{R(A)-R(B)}{L(A)-L(B)} \tag{7}
\end{equation*}
$$

$\operatorname{ARPI}(A \mid B)$ presents the relative performance difference of structure $A$ compared with structure $B$. Figure 3 shows that when the Receiver uses all available information to generate responses, it improves performance relative to using no information, but this results in a $26 \%$ increase in the length
of input tokens, thereby increasing the computational cost of inference. In contrast, utilizing our persuasion framework, Phi-2 increases the input token length by only $6.9 \%$ while achieving a $22.5 \%$ performance improvement, leading to a higher efficiency ratio.

## 5 Conclusion

In this work, we introduce Bayesian Persuasion Alignment, a novel framework that integrates the concept of Bayesian persuasion with AI alignment. By formulating alignment as a Bayesian Persuasion problem, we employ a smaller model as an Advisor to generate signals that persuade a larger model, the Receiver, to enhance its performance. Our experimental results demonstrate significant improvements in the performance of various large models on mathematical problemsolving tasks and code generation tasks. The theoretical analysis provides an upper bound on the Advisor's regret, highlighting the efficacy of our method in learning the optimal signaling strategy. We hope our approach will inspire future research in integrating information design with alignment, contributing to the development of more efficient and effective AI systems.

Limitations. The effectiveness of persuasion depends on the signaling strategy and is also influenced by the inherent capabilities of the Receiver. If the model itself lacks the ability to complete a certain task, our method may not be effective, which limits the applicability of our framework.

## References

[1] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Conference on Learning Theory (COLT), pages 263-274. Citeseer, 2008.

[2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

[3] Itai Arieli and Yakov Babichenko. Private bayesian persuasion. Journal of Economic Theory, $182: 185-217,2019$

[4] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021.

[5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[6] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Francesco Trovò, and Nicola Gatti. Optimal rates and efficient algorithms for online bayesian persuasion. In International Conference on Machine Learning (ICML), pages 2164-2183. PMLR, 2023.

[7] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilè Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.

[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33: $1877-1901,2020$.

[9] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022.

[10] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023 .

[11] Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online bayesian persuasion. Advances in Neural Information Processing Systems (NeurIPS), 33:16188-16198, 2020 .

[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.

[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[14] Paul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.

[15] Paul Christiano, Mark Xu, and Ajeya Cotra. Arc's first technical report: Eliciting latent knowledge. https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/ arc-s-first-technical-report-eliciting-latent-knowledge, 2021.

[16] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.

[17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.

[18] WD Cook and RJ Webster. Caratheodory's theorem. Canadian Mathematical Bulletin, 15(2): 293-293, 1972.

[19] Esin Durmus, Liane Lovitt, Alex Tamkin, Stuart Ritchie, Jack Clark, and Deep Ganguli. Measuring the persuasiveness of language models, 2024. URL https: / www . anthropic. com/news/measuring-model-persuasiveness.

[20] Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. Truthful ai: Developing and governing ai that does not lie. arXiv preprint arXiv:2110.06674, 2021.

[21] Brian J Fogg. Persuasive technology: using computers to change what we think and do. Ubiquity, 2002(December):2, 2002.

[22] Jiarui Gan, Rupak Majumdar, Goran Radanovic, and Adish Singla. Bayesian persuasion in sequential decision-making. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 36, pages 5025-5033, 2022.

[23] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.

[24] Marius Hobbhahn. Eliciting latent knowledge (elk) - distillation/summary. https://www.alignmentforum.org/posts/rxoBY9CMkqDsHt25t/ eliciting-latent-knowledge-elk-distillation-summary, 2022.

[25] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 2023.

[26] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.

[27] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416, 2024

[28] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[29] Emir Kamenica. Bayesian persuasion and information design. Annual Review of Economics, $11: 249-272,2019$.

[30] Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101(6):2590-2615, 2011

[31] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

[32] Yuejiang Liu and Alexandre Alahi. Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts. arXiv preprint arXiv:2402.15505, 2024.

[33] SC Matz, JD Teeny, Sumer S Vaid, H Peters, GM Harari, and M Cerf. The potential of generative ai for personalized persuasion at scale. Scientific Reports, 14(1):4692, 2024.

[34] Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM, 1994.

[35] OpenAI. Gpt-4 technical report, 2023.

[36] OpenAI. Introducing superalignment. https://openai.com/blog/ introducing-superalignment, 2023. Accessed on July 5, 2023.

[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35:27730-27744, 2022.

[38] Lorenzo Pacchiardi, Alex J Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, Owain Evans, and Jan Brauner. How to catch an ai liar: Lie detection in black-box llms by asking unrelated questions. arXiv preprint arXiv:2309.15840, 2023.

[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https : / /openreview.net/forum?id=HPuSIXJaa9.

[41] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[42] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.

[43] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548, 2023.

[44] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. CoRR, abs/2403.09472, 2024.

[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023 .

[46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[47] Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. Persuasion for good: Towards a personalized persuasive dialogue system for social good. arXiv preprint arXiv:1906.06725, 2019.

[48] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.

[49] Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma, and Yaodong Yang. Incentive compatibility for ai alignment in sociotechnical systems: Positions and prospects. arXiv preprint arXiv:2402.12907, 2024.
