# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION 

Julius Heitkoetter, Michael Gerovitch, Laker Newhouse<br>Department of Electrical Engineering and Computer Science<br>Massachusetts Institute of Technology<br>Cambridge, MA 02139, USA<br>\{juliush,mgerov,lakern\}@mit.edu


#### Abstract

The trustworthiness of highly capable language models is put at risk when they are able to produce deceptive outputs. Moreover, when models are vulnerable to deception it undermines reliability. In this paper, we introduce a method to investigate complex, model-on-model deceptive scenarios. We create a dataset of over 10,000 misleading explanations by asking Llama-2 7B, 13B, 70B, and GPT-3.5 to justify the wrong answer for questions in the MMLU. We find that, when models read these explanations, they are all significantly deceived. Worryingly, models of all capabilities are successful at misleading others, while more capable models are only slightly better at resisting deception. We recommend the development of techniques to detect and defend against deception. Code is available at https://github.com/julius-heitkoetter/deception


## 1 INTRODUCTION

Since the release of OpenAI's ChatGPT, large language models (LLMs) have revolutionized information accessibility by providing precise answers and supportive explanations to complex queries (Spatharioti et al., 2023, Caramancion, 2024, OpenAI, 2022). However, LLMs have also demonstrated a propensity to hallucinate explanations that are convincing but incorrect (Zhang et al. 2023. Walters \& Wilder, 2023; Xu et al. 2024). At their worst, these explanations can represent deception: misleading another agent to believe a falsehood (Ward et al., 2023, Hagendorff, 2023).

Deceptive explanations raise concerns for a model's reliability and trustworthiness (Park et al. 2023). LLMs have employed deceptive strategies to achieve their goals, both in games (O'Gara. 2023; Bakhtin et al., 2022; Pan et al., 2023) and in realistic scenarios (Scheurer et al., 2023), including convincingly pretending to be human (Achiam et al., 2023).

As model capability continues to grow, detecting deception is integral to ensuring safety in frontier models (Hubinger et al. 2024). Previous studies of LLM falsehoods and deception use hand-crafted or model-generated tasks to evaluate standalone model performance (Azaria \& Mitchell, 2023, Lin et al., 2021; Perez et al. 2023). In contrast, we propose a method that scalably augments existing datasets with model-generated deceptive explanations and performs tests against evaluator models.

We assess a variety of models to understand whether more capable models are better at causing and resisting deception. We present four main contributions:

![](https://cdn.mathpix.com/cropped/2024_06_04_042b6a590cd9ec8e47edg-01.jpg?height=241&width=1352&top_left_y=2189&top_left_x=381)

Figure 1: An evaluator model is tricked after reading a deceptive explanation. (In George Orwell's 1984, the main character is made to think that $2+2=5$.)

![](https://cdn.mathpix.com/cropped/2024_06_04_042b6a590cd9ec8e47edg-02.jpg?height=713&width=1241&top_left_y=289&top_left_x=431)

Figure 2: GPT-3.5's fraction of correct answers on four MMLU categories ( $y$-axis) falls drastically when subject to deceptive explanations from Llama-2 7B, 13B, 70B, and GPT-3.5 ( $x$-axis).

- We create a dataset of over 10,000 deceptive explanations for answers in the MMLU.
- We find that Llama-2 7B, 13B, 70B, and GPT-3.5 are all significantly deceived.
- We find that more capable models are slightly better at resisting deception.
- We find that all models are deceptive, although GPT-3.5 is the least deceptive.


## 2 METHODS

Datasets and Models We construct our dataset by extracting question-answer pairs from the Massive Multitask Language Understanding (MMLU) dataset (Hendrycks et al., 2021), a popular model benchmark consisting of SAT-like multiple choice questions across 57 different categories labeled with the correct answer. We focus on 4 categories: elementary mathematics, statistics, psychology, and computer science. We experiment on GPT-3.5 Turbo and a suite of instruction finetuned Llama-2 models (7B, 13B, and 70B). Our codebase is highly extensible, already configured to run experiments on GPT-4. We do not report on GPT-4 only due to high API costs 1 For more details on our models and datasets, see Appendix A.

Capability and Deception Pipelines For over 10,000 question-answer pairs, we run our models through two pipelines to measure their performance before and after seeing deceptive explanations. The capability pipeline establishes a control group. In it, we ask each model to output a single token zero-shot for whether the answer is correct. Next, the deception pipeline establishes an experimental group. First, we ask each model to generate a deceptive explanation: if the answer is correct, the deceptive explanation should argue the answer is incorrect, and vice versa. We call models in this stage deceivers. Second, we ask each model to evaluate the answer to the question in light of the deceptive explanation, with no memory of its previous response. We call models in this role evaluators. We compare performance across the two pipelines to measure the impacts of deception on all combinations of evaluator and deceiver models. See a diagram in Figure 1 All prompts we use are available in Appendix B More details on the pipeline are in Appendix A.

Defining Deception The two main measurements we make are capability and deception rate. Capability is the fraction of questions a model answers correctly; deception rate is the fraction of questions for which a model switches from answering correctly to answering incorrectly after being given a deceptive explanation. The relative capability of two models is the ratio of their capabilities.[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_042b6a590cd9ec8e47edg-03.jpg?height=518&width=1388&top_left_y=278&top_left_x=366)

Figure 3: The negative correlation $(r<-0.45, p<0.05)$ between relative capability of evaluators to deceivers ( $x$-axis) and deception rate ( $y$-axis) suggests that weak models are more vulnerable to deception. Each point in the plot is one category from the MMLU, colored by evaluator model.

We formalize these definitions as follows. Let $\mathrm{QA}$ be a set of question-answer pairs for a given category of the MMLU. Let $M:$ QA $\rightarrow\{0,1\}$ represent a model, where $M(q a)=1$ if the model successfully evaluates whether the answer is correct. We create two sets of question-answer

![](https://cdn.mathpix.com/cropped/2024_06_04_042b6a590cd9ec8e47edg-03.jpg?height=46&width=1388&top_left_y=1159&top_left_x=366)
$C(M, \mathrm{QA})$ denote the fraction of correct answers that $M$ gives on QA. Then the capability of $M$ on the category is $\frac{1}{2}\left(C\left(M, \mathrm{QA}_{\text {incorrect }}\right)+C\left(M, \mathrm{QA}_{\text {correct }}\right)\right)$. We take an average so that deterministic strategies (e.g., always say the answer is incorrect) have capability 0.5 . Now, let $D:$ QA $\rightarrow$ QAE represent a deceiver model that injects a deceptive explanation, turning a question-answer pair into a question-answer-explanation triple. Among questions that $M$ originally answered correctly, denote the fraction of switches from correct to incorrect answers as $S(M, D, \mathrm{QA})$. Then the deception rate of $D$ against $M$ is $\frac{1}{2}\left(S\left(M, D, Q_{\text {incorrect }}\right)+S\left(M, D, Q_{\text {correct }}\right)\right)$. See further details in Appendix C.

## 3 RESULTS

Deception is Significant We run 4 models (GPT-3.5 and Llama-2 7B, 13B, and 70B) in 16 different pairs of evaluator and deceiver roles on 4 categories of the MMLU. Robustly across categories, we observe that a model's capability falls drastically when presented with a deceptive explanation (Figure 2). For GPT-3.5, capability falls from near $70 \%$ to $20 \%$. Note that random guessing would score $50 \%$ on capability. Therefore, deceptive explanations frequently cause even capable models to switch to incorrect answers. See Appendix D for all bar plots of deception rate.

Weak Models Are More Vulnerable When we vary the evaluator model, we find a moderate negative correlation between evaluator capability and deception rate ( $r<-0.45, p<0.05$ ). In other words, the evaluators that are deceived most often are the ones that are least capable. See Appendix E for the corresponding statistical analysis and for all correlation plots. Figure 3 shows qualitatively that more capable models better resist deception.

All Models Are Deceptive When we vary the deceiver model, we observe a strong negative correlation $(r=-0.87 \pm 0.07)$ between deceiver capability and deception rate, indicating that smarter models are less deceptive. We hypothesize that this correlation is due to a confounding factor: our most capable model, GPT-3.5, is also better aligned for truthfulness. To test this hypothesis, we perform a blind manual labeling of 480 explanations to remove explanations that refuse to justify the incorrect answer. We do not evaluate for explanation quality, nor do we remove nonsense explanations as long as they argue for only the incorrect answer. The refusal rate is $5.0 \%$ (Llama-2 7B), 4.2\% (Llama-2 13B), 5.5\% (Llama-2 70B), and 15.8\% (GPT-3.5). When we restrict to the cleaned dataset, we reduce the significance of a negative correlation $(r=-0.46 \pm 0.26)$ between deceiver capability and deception rate. Still, among models we study, this second negative correlation suggests that more capable models may have better guardrails against producing deceptive responses. However, the slope is shallow: GPT-3.5 still produces deceptive responses $84.2 \%$ of the time, causing deception over $80 \%$ of the time.
![](https://cdn.mathpix.com/cropped/2024_06_04_042b6a590cd9ec8e47edg-04.jpg?height=518&width=1386&top_left_y=278&top_left_x=366)

Figure 4: On the left, higher capability for deceivers ( $x$-axis) appears to reduce deception rate ( $y$ axis). The reason is that GPT-3.5 often produces inconclusive explanations. We blindly label 480 examples to remove such explanations. On the right, the deceiver capability on this cleaned dataset becomes only slightly negatively correlated with adjusted deception rate.

## 4 DISCUSSION

Sycophancy One potential concern with our methodology is that we are not measuring deception, but rather agreeableness or sycophancy (Sharma et al. 2023). To isolate the effect of sycophancy, we ran additional experiments using sycophancy steering vectors for Llama-2 7B and Llama-2 13B computed by Rimsky et al. (2023). We find that a model's sycophancy steering vector strongly biases it toward saying answers are correct (e.g., Llama 13B answers "correct" $93 \%$ of the time when we add the steering vector and $8 \%$ of the time when we subtract it). But we do not observe a conclusive improved resilience against deception, as explained in Appendix F. Future work on the impact of sycophancy could experiment on Llama-2 base models that are not instruction fine-tuned.

Baseline Deception We replicate our experiments with a deterministic deceiver that always gives an explanation of "this answer is correct" or "this answer is incorrect." We find that the baseline deceiver is extremely good at deceiving small models, such as Llama-2 7B, but significantly less good at deceiving larger models, such as GPT-3.5 (see Appendix G. While less capable models appear to act like copycats for any deceptive explanation, more capable models are more discerning against simple baseline explanations.

Future Directions One limitation of our analysis is that all Llama-2 models exhibit low capability on the MMLU. Future work could address low capability on the MMLU in a few ways. One would be to run experiments with a simpler dataset. Tree-of-thought (Wei et al., 2022) and other model enhancements could also increase the capability of existing models. One could also run experiments with stronger models, such as GPT-4, Claude, and Gemini. Our codebase is easily extensible for new models, already including scripts to call GPT-4.

A related limitation is that the models we use are highly sensitive to prompting. Two promising directions are studying the effect of few-shot prompting (Brown et al. 2020) and directly sampling model logits. Further studies could explore how prompting affects resilience to deception, which is particularly relevant for retrieval-augmented generation applications (Lewis et al. 2020).

Previous studies have probed for knowledge and truth representations in LLMs with varying levels of success (Marks \& Tegmark, 2023, Farquhar et al., 2023, Burns et al., 2023, Levinstein \& Herrmann, 2023) and tried to intervene to improve model faithfulness (Zou et al., 2023, Rimsky et al.||2023, Li et al. 2023). Our methods may be able to bolster this line of research by providing a scalable pipeline for augmenting simple, true-false datasets in order to better design and evaluate interventions.

## 5 CONCLUSION

We show that language models are susceptible to deception across a wide range of model capabilities. More capable models are slightly better at resisting deception, while less capable models are more willing to participate in justifying false statements. The propensity for deception across a variety of models highlights an important challenge in building secure and trustworthy models at scale. Future work should continue to develop techniques to detect and defend against deception to ensure the reliability of widely deployed AI systems.

## ACKNOWLEDGMENTS

We are grateful to Stephen Casper, Wes Gurnee, and Jacob Andreas for initial project direction. We thank the Center for AI Safety for providing access to A100 GPUs and MIT AI Alignment for covering the cost of OpenAI API calls. We are grateful to Jeremy Bernstein, Joseph Newhouse, Emily Robinson, Gabe Wu, Naomi Bashkansky, and Nick Gabrieli for helpful feedback on drafts of this manuscript.

## REFERENCES

Josh Achiam, Steven Adler, and Sandhini Agarwal et al. GPT-4 technical report, 2023.

Amos Azaria and Tom Mitchell. The internal state of an LLM knows when it's lying, 2023.

Anton Bakhtin, Noam Brown, and Emily Dinan. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074, 2022. doi: 10.1126/science.ade9097. URL/https://www.science.org/doi/abs/10.1126/ science.ade9097.

Tom Brown, Benjamin Mann, and Nick Ryder et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ETKGuby0hcs

Kevin Matthe Caramancion. Large language models vs. search engines: Evaluating user preferences across varied information retrieval scenarios, 2024.

Paul F Christiano, Jan Leike, and Tom et al. Brown. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Sebastian Farquhar, Vikrant Varma, Zachary Kenton, Johannes Gasteiger, Vladimir Mikulik, and Rohin Shah. Challenges with unsupervised llm knowledge discovery, 2023.

Thilo Hagendorff. Deception abilities emerged in large language models, 2023.

Dan Hendrycks, Collin Burns, and Steven Basart et al. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.

Evan Hubinger, Carson Denison, and Jesse Mu et al. Sleeper agents: Training deceptive LLMs that persist through safety training, 2024.

Cue Hyunkyu Lee, Seungho Cook, Ji Sung Lee, and Buhm Han. Comparison of two meta-analysis methods: Inverse-variance-weighted average and weighted sum of z-scores. Genomics Inform, 2016. doi: $10.5808 /$ GI.2016.14.4.173.

B. A. Levinstein and Daniel A. Herrmann. Still no lie detector for language models: Probing empirical and conceptual roadblocks, 2023.

Patrick Lewis, Ethan Perez, and Aleksandra et al. Piktus. Retrieval-augmented generation for knowledge-intensive NLP tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9459-9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.

Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model, 2023.

Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods, 2021.

Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets, 2023.

Aidan O'Gara. Hoodwinked: Deception and cooperation in a text-based game for language models, 2023.

OpenAI. Introducing ChatGPT, 2022. URLhttps://openai.com/blog/chatgpt.

Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the MACHIAVELLI benchmark, 2023 .

Peter S. Park, Simon Goldstein, Aidan O'Gara, Michael Chen, and Dan Hendrycks. AI deception: A survey of examples, risks, and potential solutions, 2023.

Ethan Perez, Sam Ringer, and Kamile Lukosiute et al. Discovering language model behaviors with model-written evaluations. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 13387-13434, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.847. URLhttps://aclanthology.org/2023.findings-acl. 847

Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering Llama 2 via contrastive activation addition, 2023.

Jérémy Scheurer, Mikita Balesni, and Marius Hobbhahn. Technical report: Large language models can strategically deceive their users when put under pressure, 2023.

Mrinank Sharma, Meg Tong, and Tomasz Korbak et al. Towards understanding sycophancy in language models, 2023.

Sofia Eleni Spatharioti, David M. Rothschild, Daniel G. Goldstein, and Jake M. Hofman. Comparing traditional and LLM-based search for consumer choice: A randomized experiment, 2023.

Hugo Touvron, Louis Martin, and Kevin Stone et al. Llama 2: Open foundation and fine-tuned chat models, 2023.

William H. Walters and Esther Isabelle Wilder. Fabrication and errors in the bibliographic citations generated by ChatGPT, Sep 2023. URL https://www.nature.com/articles/ s41598-023-41032-5

Francis Rhys Ward, Francesco Belardinelli, Francesca Toni, and Tom Everitt. Honesty is the best policy: Defining and mitigating AI deception, 2023.

Jason Wei, Xuezhi Wang, and Dale Schuurmans et al. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https://arxiv.org/abs/ 2201.11903 .

Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models, 2024.

Shunyu Yao, Dian Yu, and Jeffrey et al. Zhao. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

Yue Zhang, Yafu Li, and Leyang Cui et al. Siren's song in the AI ocean: A survey on hallucination in large language models, 2023.

Andy Zou, Long Phan, and Sarah Chen et al. Representation engineering: A top-down approach to AI transparency, 2023.
