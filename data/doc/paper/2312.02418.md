# Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data 

Yu Yang ${ }^{1,2 *}$<br>yuyang@cs.ucla.edu

Aaditya K. Singh ${ }^{2}$<br>aaditya.singh.21@ucl.ac.uk

Anas Mahmoud ${ }^{2}$<br>nas.mahmoud@mail.utoronto.ca

Kushal Tirumala ${ }^{2}$<br>ktirumala@meta.com

Mostafa Elhoushi ${ }^{2}$<br>melhoushi@meta.com

Fabian Gloeckle ${ }^{2}$

fgloeckle@meta.com

Baptiste Rozière $^{2} \quad$ Carole-Jean Wu ${ }^{2} \quad$ Ari S. Morcos ${ }^{3 \dagger} \quad$ Newsha Ardalani $^{2}$<br>broz@meta.com carolejeanwu@meta.com<br>new@meta.com<br>${ }^{1}$ UC Los Angeles $\quad{ }^{2}$ FAIR at Meta ${ }^{3}$ DatologyAI


#### Abstract

Code datasets, often collected from diverse and uncontrolled sources such as GitHub, potentially suffer from quality issues, thereby affecting the performance and training efficiency of Large Language Models (LLMs) optimized for code generation. Previous studies demonstrated the benefit of using embedding spaces for data pruning, but they mainly focused on duplicate removal or increasing variety, and in other modalities, such as images. Our work focuses on using embeddings to identify and remove "low-quality" code data. First, we explore features of "low-quality" code in embedding space, through the use of synthetic corruptions. Armed with this knowledge, we devise novel pruning metrics that operate in embedding space to identify and remove low-quality entries in the Stack dataset. We demonstrate the benefits of this synthetic corruption informed pruning (SCIP) approach on the well-established HumanEval and MBPP benchmarks, outperforming existing embedding-based methods. Importantly, we achieve up to a $3 \%$ performance improvement over no pruning, thereby showing the promise of insights from synthetic corruptions for data pruning.


## 1 Introduction

Machine learning, and in particular Large Language Models (LLMs), are transforming a wide range of industries. Their capabilities extend even to specialized tasks like code generation and medical diagnostics, thus amplifying their societal and economic impact [1]. In this race for higher performance, some training datasets have swelled to petabyte size, sourced from extensive repositories like the Common Crawl. While significant effort has gone into optimizing the computational aspects of training LLMs, such as hardware acceleration and algorithmic improvements [2], the question of data efficiency is still relatively under-explored. Data efficiency is not merely a computational concern but is intrinsically tied to the quality of the training data. The use of large, but ineffective, datasets can result in protracted training times, higher energy consumption, and ultimately, models that are expensive to deploy and maintain [3].[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_7be1f7b7784d00fe6fcfg-02.jpg?height=551&width=1395&top_left_y=234&top_left_x=365)

Figure 1: Schematic of SCIP. First, we synthetically corrupt code data, which tends to move code embeddings to smaller clusters or further from cluster centroids. Then, we use this insight to propose a new pruning metric, resulting in improved training efficiency and better end performance.

Code datasets, usually compiled from diverse, open-source platforms like GitHub, are often riddled with inconsistencies, errors, or low-quality code snippets. These issues not only undermine the model's final performance but also affect the efficiency and effectiveness of the training process. The presence of such low-quality data essentially "pollutes" the learning environment, leading to suboptimal results. Therefore, improving data quality is not merely an ancillary task but a fundamental requirement for achieving the full potential of code-generating LLMs. A recent study [4] showcased the benefits of so-called "textbook-quality" data in enhancing model efficiency for code-generation tasks. However, their strategy relies heavily on generating closed-source data with GPT-3.5 and then filtering it based on GPT-4 [5] predictions, both of which are proprietary models, thus making this approach less accessible for many researchers due to high costs and difficulty of reproducibility. Furthermore, another study [6] highlighted potential issues with training on generated outputs. This emphasizes the need for open-source techniques to identify valuable data in existing, large-scale, natural corpora.

Building upon these identified challenges and gaps in existing research, we focus on easy-to-use, accessible pruning methods for the large open-source Stack dataset [7]. To this end, we take inspiration from recent approaches to data pruning in the domains of image [3] and multimodal models [8], which make use of pre-trained embedding spaces to identify useful or duplicate data, to keep or prune, respectively. In the hitherto unexplored domain of code, we introduce synthetic corruption informed pruning (SCIP): First, we identify what constitutes "low-quality" data in embedding space through controlled corruption of existing data, and find that corrupted code tends to reside in smaller clusters and often be farther from cluster centroids. Then, we introduce a pruning strategy, based on these insights, that ranks data points based on their cluster size and distance to the nearest centroid, aiming to remove a predefined fraction of the data. Using these embedding-based methods for pruning low-quality code, we demonstrate improvements in performance and training efficiency on widely used benchmarks [9, 10].

## 2 What Does Low-Quality Mean for Code Data?

### 2.1 Definition of Low-Quality Data

Let $\mathcal{D}$ be the original dataset, $\mathcal{Q} \subseteq \mathcal{D}$ be a subset, and $\mathcal{D}_{\text {test }}$ be the test set. Let $x_{\text {test }, i}$ be the $i$-th test example in $\mathcal{D}_{\text {test }}$. First, we define a general metric $M$, which could potentially be pass $@ \mathrm{k}$ [9] or any other quality metric. We then define $M\left(\theta(\mathcal{D}), \mathcal{D}_{\text {test }}\right.$ ) as the expectation of a particular metric (for example, pass $@ \mathrm{k}_{i}$ ) over all $x_{\text {test }, i}$ in $\mathcal{D}_{\text {test }}$ when training on dataset $\mathcal{D}$ with model parameters $\theta$ :

$$
M\left(\theta(\mathcal{D}), \mathcal{D}_{\text {test }}\right)=\mathbb{E}_{x_{\text {test }, i} \in \mathcal{D}_{\text {test }}}\left[\text { pass } @ \mathrm{k}_{i}\right]
$$

The set $\mathcal{Q}$ is defined as "low-quality" if the following inequality holds:
![](https://cdn.mathpix.com/cropped/2024_06_04_7be1f7b7784d00fe6fcfg-03.jpg?height=626&width=1266&top_left_y=234&top_left_x=424)

Figure 2: Corrupted data tends to reside in smaller clusters (top row) and farther from centroids (bottom row) when compared to the original, uncorrupted data. The effects are more pronounced for syntax errors (left two columns) as compared to content errors (right two columns). Red dotted line indicates mean, black dotted line indicates 0. More details and analysis can be found in Appendix B. 2 .

$$
M\left(\theta(\mathcal{D}), \mathcal{D}_{\text {test }}\right)<M\left(\theta(\mathcal{D} \backslash \mathcal{Q}), \mathcal{D}_{\text {test }}\right)
$$

In simpler terms, $\mathcal{Q}$ is considered "low-quality" data if removing it from $\mathcal{D}$ improves the score of the general metric $M$ on $\mathcal{D}_{\text {test }}$.

### 2.2 SCIP: Two-Step Framework for Identifying Low-Quality Data

To systematically identify low-quality data, we propose a two-step framework, illustrated in Figure 1. The first step involves the creation of data with known errors, serving as markers for low-quality data. From this first step, we gather insights on how corruption affects embeddings (obtained with a pretrained model), and use this knowledge to prune data with similar embedding properties.

Synthetic Corruption Generation To identify and prune "low-quality" code data, it's important to understand its possible forms. We consider two main domains: syntax errors and content errors. Synthetic corruption has the benefit of creating matched pairs of higher and lower quality data, making it more controlled than alternative approaches which could be confounded by style.

- Data with Syntax Errors: Syntax errors are clear indicators of bad code, preventing a file from executing successfully. Such issues can be as common as unmatched parentheses or as nuanced as referencing undeclared variables. To intentionally introduce these errors for the sake of our experiments, we employ two main corruptions: removing closed brackets (specifically, ') ', ' $]$ ', '\}') and renaming variables to syntactically invalid names.
- Data with Content Errors: Although such code may run without immediate issues, its output might diverge from the intended result due to underlying logical errors. To simulate this, we either alter conditional operators (through negation) or offset array indices (changing ' $i$ ' to ' $i+1$ ') to disrupt data access patterns.

More specifics can be found in Appendix B. Through these synthetic corruptions, we ensure a systematic introduction of both syntax and content errors, aiding in a more comprehensive identification of "low-quality" data. By focusing on a representative sample of errors, we effectively set the stage for the next step: identifying and pruning "low-quality" data in large-scale datasets.

Data Pruning Informed by Synthetic Corruptions In the embedding space of a pre-trained code embedding model, StarEncoder [11], we see that synthetic corruption exhibits a distinct change: corruption moves points to smaller clusters or further out from centroids, as compared to the original, uncorrupted code (Fig. 22). These insights shape our pruning strategy. By focusing on data in smaller

Table 1: Pass@ 1 performance on HumanEval and MBPP for different pruning methods with $20 \%$ files pruned.

|  | No <br> pruning | Random <br> Pruning | SSL <br> Prototype | SemDeDup | D4 | Small <br> Clusters | Far from <br> Centroids | Combined <br> Small+Far |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| HumanEval | $25.0 \%$ | $24.0 \%$ | $23.8 \%$ | $20.7 \%$ | $23.2 \%$ | $23.2 \%$ | $\underline{26.8 \%}$ | $\mathbf{2 8 . 0 \%}$ |
| MBPP | $\underline{33.4 \%}$ | $31.9 \%$ | $32.2 \%$ | $32.4 \%$ | $31.2 \%$ | $\mathbf{3 5 . 0 \%}$ | $30.8 \%$ | $33.0 \%$ |

clusters and distant from centroids, we aim to efficiently identify and remove low-quality data from the original dataset. A formal version of the algorithm, with pseudocode can be found in Appendix $\mathrm{C}$

## 3 Pruning Low-quality Data for More Efficient Training

### 3.1 Experiment Setup

Dataset. Our experiments utilize the Stack v1.1 dataset [7], which is sourced from GitHub repositories published from 2015 to 2022, and specifically designed for code generation tasks. Although the dataset includes code from 358 different programming languages, we narrow our focus solely to Python to ensure a more controlled study. This results in a dataset of $12.6 \mathrm{M}$ files and $20.4 \mathrm{~B}$ tokens.

Model and Training Details. Following the methodology of the current state-of-the-art open-source model, Code Llama [12], we fine-tune a 1.5B LLaMA [13] model instead of training from scratch. The model has 48 layers, 24 heads per layer, and inner dimension of 1536 . All experiments are run on 32 NVIDIA A100 GPUs with fully-sharded data parallel [14]. We use a learning rate of 3e-4, a batch size of 576, a sequence length of 2048, and train for 56,000 steps ( $\sim 67 \mathrm{~B}$ tokens).

### 3.2 Evaluation

Our evaluation employs two well-established benchmarks in the code generation field: HumanEval [9] and MBPP [10]. The primary metric for evaluation across these benchmarks is "pass @k," which measures the percentage of test cases that are correctly solved within the top-k generated code snippets. For baselines, we compare to no pruning, random pruning (averaged over 3 seeds), and three other pruning methods using embeddings, based on prior work in other modalities: SSL-prototypes [3], SemDeDup [8], and D4 [15]. Additional details can be found in Appendix D

### 3.3 Results

In Table 1, our proposed methods - pruning data that are "Far from Centroid" and within "Small Clusters" - yield clear performance improvements on HumanEval and MBPP, respectively. However, better performance on one benchmark often comes at the expense of the other, perhaps due to the different natures of these tasks. Motivated by the strong performance of our two suggested methods, we experimented with a combined method: first pruning files from small clusters, then files far from centroids, with the ratio between these defined by a parameter $\alpha$. We found that $\alpha=0.8$ performed best (see Appendix C). Impressively, this combined method achieves the best performance of all methods tried on HumanEval, a full $3 \%$ above no pruning and better than all prior work on embedding-based pruning, while also remaining competitive with no pruning on MBPP.

We also observe in Fig. 1 that "Far from Centroid" and "Small Clusters" both achieve an efficiency speedup (both methods achieve the baseline pass @ 1 rate in fewer training steps). Further insights into the qualitative attributes of pruned data are presented in Fig. 44."

## 4 Conclusions

We introduce SCIP, a systematic method to identify and remove "low-quality" code data from large datasets. Building on the insights of the value of high-quality data presented in earlier studies [4], our work goes further by offering accessible, open-source, and cost-effective pruning techniques through the use of embedding spaces. We go beyond prior work in embedding-based pruning [3, 8, 15] by motivating heuristics through identification of "low-quality" data via synthetic corruptions: we
systematically create code discrepancies, both in syntax and content, to understand their influence on the embedding space. Our findings reveal that syntax errors lead to significant shifts away from cluster centroids and into smaller clusters. Leveraging these observations, we designed pruning methods that consider both distances to centroids and cluster sizes to effectively identify and remove low-quality data. Applying these pruning methods leads to better performance on code generation benchmarks, showing the promise of insights from synthetic corruptions for improving pruning techniques.

More broadly, our results underscore the significance of rigorous data curation. Beyond just code, more rigorously examining "low-quality" data could lead to more informed pruning techniques. Similar to how code can have both syntax and content discrepancies, natural language data too can have structural (e.g., grammatical) and semantic (e.g., factually incorrect) anomalies. In future work, the strategies and methodologies established here of using synthetically corrupted data as a pruning signal could be extended and adapted to general natural language datasets, ensuring models trained on them produce more accurate, reliable, and coherent outputs.

## Acknowledgments

We would like to sincerely thank Jack Lanchantin for the insightful discussions, and Shubham Toshniwal, Koustuv Sinha, and Alberto Bietti for generously sharing their valuable insights drawn from their previous research.

## References

[1] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at the labor market impact potential of large language models, 2023.

[2] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R'e. Flashattention: Fast and memory-efficient exact attention with io-awareness. ArXiv, abs/2205.14135, 2022.

[3] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523-19536, 2022.

[4] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.

[5] OpenAI. Gpt-4 technical report, 2023.

[6] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget, 2023.

[7] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022.

[8] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls, 2023.

[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[10] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

[11] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João

Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! 2023.

[12] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[13] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[14] FairScale authors. Fairscale: A general purpose modular pytorch library for high performance and large scale training. https://github.com/facebookresearch/fairscale, 2021.

[15] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving llm pretraining via document de-duplication and diversification. arXiv preprint arXiv:2308.12284, 2023.

[16] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912-9924, 2020.

[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.

[18] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
