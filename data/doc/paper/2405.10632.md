# Beyond Static AI Evaluations: Advancing Human Interaction Evaluations for LLM Harms and Risks 

Lujain Ibrahim ${ }^{1,2}$, Saffron Huang ${ }^{3}$, Lama Ahmad ${ }^{4}$, Markus Anderljung ${ }^{2}$<br>${ }^{1}$ Oxford Internet Institute, University of Oxford<br>${ }^{2}$ Centre for the Governance of AI<br>${ }^{3}$ Collective Intelligence Project<br>${ }^{4}$ OpenAI


#### Abstract

Model evaluations are central to understanding the safety, risks, and societal impacts of AI systems. While most realworld AI applications involve human-AI interaction, most current evaluations (e.g., common benchmarks) of AI models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations - "human interaction evaluations" (HIEs) - which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that HIEs can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused HIE design framework - containing a human-LLM interaction taxonomy - with three stages: (1) identifying the risk and/or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of HIEs.


## 1 Introduction

Artificial intelligence (AI) model evaluations have become central in developers and regulators' efforts to ensure that AI models are "safe." Governments like that of the United Kingdom, through its AI Safety Institute (UK Government 2024), and the United States, through a recent Executive Order, emphasize the importance of conducting model evaluations for various risks like discrimination and cybersecurity (The White House 2023); AI labs, including OpenAI with its Preparedness Framework and Anthropic with its Responsible Scaling Policy (OpenAI 2023; Anthropic 2023), propose utilizing model evaluations to monitor and mitigate misuse and catastrophic risks; and, academic researchers are developing safety evaluation datasets at unprecedented rates (RÃ¶ttger et al. 2024). This has positioned model evaluations as integral to a range of important decisions on the safe development and deployment of AI systems.

While drawing on the history of research in natural language processing (NLP), the evaluation of general purpose[^0]

AI models faces new challenges. These models have openended input and output spaces, and thus can produce unpredictable and varied responses which make designing comprehensive evaluations challenging (Ganguli et al. 2022). Their model evaluation results are also sensitive to different prompting strategies (Liang et al. 2022), suffer from replicability issues (Ganguli et al. 2023; Hutson 2018), and can be compromised by data leakage from test sets (Jiang et al. 2024). These challenges have prompted calls for intensified and interdisciplinary research efforts to enhance the robustness, reliability, and replicability of evaluations (Chang et al. 2024).

Current safety evaluation approaches primarily develop datasets and benchmarks to evaluate model completions for hazardous behaviors and capabilities (Sharma et al. 2023; Parrish et al. 2021; Wang et al. 2023). While such approaches are important in identifying potential downstream harms, they insufficiently address the 'sociotechnical gap' - the discrepancy between what safety assessments predict in controlled, model-only settings and how models actually perform in the environments in which they are deployed (Weidinger et al. 2023; Liao and Xiao 2023; Raji et al. 2022). By assessing models in isolation, these evaluation approaches incorporate human factors in limited ways, thereby failing to capture the complexity of real-world human-model interaction (Chang et al. 2024).

This sociotechnical gap manifests across three dimensions: (1) joint performance: while many AI use cases involve human interaction, most benchmarks are noninteractive and might not reveal capabilities and behaviors that may only appear through human-model collaboration and possible feedback effects (Pan et al. 2024); (2) evaluation tasks: analyses of real-world usage data reveal significant discrepancies between benchmark tasks and those performed in practical scenarios, indicating that current evaluations may not accurately reflect model behaviors outside of 'lab settings' (Ouyang et al. 2023); and (3) human impact: current evaluations are insufficient for examining the direct impact of model capabilities and behaviors on users (Weidinger et al. 2023).

Recognizing these limitations, there has been an increasing recognition of the need to involve human users in assessments of model risks. In this work, we provide evaluators whether in labs, governments, or academia - with a frame-
work to design and interpret human interaction evaluations (HIEs). We focus on large language models (LLMs) due to their ubiquity and influence in current applications, though we expect that many of our findings will also apply to other modalities (with additional considerations which we invite future work to address). Our key contributions are:

- Defining HIEs and clarifying how they contribute to our understanding of model risks and harms. We propose a definition for HIEs and outline the key roles they can play in the current landscape of assessing model safety (Section 3).
- Developing an evaluation design framework. We develop a three-stage framework for designing HIEs. Specifically, we synthesize evaluation considerations from NLP and human-computer interaction (HCI) to introduce a taxonomy of interaction forms for evaluating human-LLM interactions (Section 4).
- Instantiating the framework. We instantiate the framework by describing two potential HIEs to better understand overreliance and persuasion risks (Section 5).
- Providing actionable recommendations. We conclude with recommendations for advancing the practice of HIEs (Section 6).


## 2 Background \& Related Work

We first clarify the key terms used in this paper. Then, we review research on the evaluation of human-LLM interaction.

### 2.1 Defining model safety evaluations

We follow existing work in adopting a wide definition for "safety" which encompasses model capabilities and behaviors associated with various taxonomized risks and harms (Weidinger et al. 2022; Shelby et al. 2023). Examples include different types of biases (Parrish et al. 2021), toxicity or malicious advice (Hartvigsen et al. 2022), sycophancy or power-seeking behaviors (Sharma et al. 2023), and dangerous capabilities like persuasion and cybersecurity risks (Phuong et al. 2024). In this context, 'harms' refer to negative outcomes that have already occurred and can be directly observed, whereas 'risks' refer to the potential for harm, specifically the likelihood and the magnitude of harm.

We define a model evaluation as an assessment that targets one or both of two model properties: model capabilities and model behaviors. Model capabilities refer to the range and extent of tasks or functions a model can perform, such as translating languages or identifying objects in images. Model behaviors, on the other hand, describe how the model executes these tasks and how it responds under various circumstances, including its handling of potential biases or its reaction to unexpected inputs.

### 2.2 Reviewing the evaluation of human-LLM interaction

In NLP, much of the research on human-LLM interaction has focused on analyzing and improving domains like dialogue and co-writing (Li et al. 2021; Lin et al. 2023). Additionally, other research uses datasets of human-LM conver- sations from adversarial testing with crowdworkers to improve chatbot safety (Xu et al. 2021). Recent work on human preferences also collects interaction data in the form of feedback on live conversations between humans and LLMs (Kirk et al. 2024).

However, research on interactive evaluations of LLMs has been relatively limited, with some exceptions. Lee et al. notably present a novel approach extending LLM evaluations from static to interactive through three stages: (1) defining an interactive task, (2) constructing an interactive system, and (3) designing evaluation metrics (Lee et al. 2022). They evaluate several models across five tasks and find that better non-interactive performance does not always translate to better interactive performance, and that user judgment of performance can diverge from that of crowdsourced annotators. Other studies like Collins et al. utilize this framework to interactively evaluate three LLMs as assistants in proving undergraduate-level mathematics (Collins et al. 2023). User-led, interactive evaluations have also been explored as a way to compare the performance of different models, such as the popular use of Elo or relative win rates between models (Chiang et al. 2024).

Most existing work focuses on interaction as a source of feedback for improving model performance; our study expands on this work, notably Lee et al. (2022) and existing insights from HCI, to specifically improve model safety as opposed to general model performance (Subramonyam et al. 2024; Gordon et al. 2021).

## 3 The Case for Human Interaction Evaluations

### 3.1 Definition

Weidinger et al. (2023) first use the term "human interaction evaluation" to describe evaluations which study "effects on people interacting with AI systems, and the human-AI dyad". Here, we further refine and operationalize this definition. We define an HIE as the assessment of human-model interactions, which includes the process and/or the outcome of a human engaging with a model - that is, focusing on the mechanisms of interaction, the outcomes yielded, or a combination of the two.

Humans can be involved in model evaluations in different ways; in this paper, we distinguish between static evaluations and interactive evaluations. While static evaluations, like common dataset-based benchmarks, can still prompt models using human-written prompts and assess model outputs using annotations from human crowdworkers, interactive evaluations - which we focus on and develop here engage humans as subjects of the evaluation who either respond to or actively elicit model outputs through interacting with the model (Chang et al. 2024; Lee et al. 2022).

We specifically focus on evaluation units that consist of one human and one model, though we expect that many important HIEs will go beyond that (see Section 6.1). Borrowing from (non-safety focused) NLP research, we propose that human-model interactions be evaluated in the context of completing tasks (e.g., brainstorming, article writing) (Lee et al. 2022; Liang et al. 2022). Thus, the evaluation tar-
get may be the process of interaction aimed at completing some task, also known in HCI as an interaction trace, or it may be the resulting outcome of these tasks (e.g., a decision that was made by a user with model assistance). An interaction trace is defined as a record or log of interactions between a user and a system, where the logs include data such as user actions (e.g., clicks, keystrokes, edits), system responses (e.g., model output), and contextual information (e.g., device type) (Lee, Liang, and Yang 2022; Collins et al. 2023).

### 3.2 Utility

We argue that because HIEs can assess the performance of human-model teams and the impact of model capabilities and behaviors on human users, they can be used to (1) increase the validity of evaluations, (2) assess direct human impact and interaction-specific harms, and (3) guide future assessments of models' societal impact.

Increasing the validity of evaluations We argue that HIEs can have greater internal and external validity compared to static evaluations, as well as increase the validity of static evaluations.

The internal validity of an evaluation is the extent to which the chosen datasets, survey questions, or prompts measure the target model properties in the controlled environment of the evaluation (Rooney et al. 2016; Raji et al. 2021). HIEs enable researchers to collect richer forms of observable data (Kiela et al. 2021). For example, researchers can observe the behaviors of the entire human-model system, including how participants' beliefs change after interacting with the model, what they do with given information, and which model capabilities they leverage most frequently. Such detailed observations can be used to generate insights or create datasets, prompts, or other proxies that can more faithfully measure the target model properties (Mialon et al. 2023). Therefore, HIEs may not only be a possible substitute for some static evaluations, but may also be an important complement that tests the validity of static evaluations.

The external validity of an evaluation is the extent to which the insights gained from it apply beyond the specific controlled environment of the evaluation. HIEs allow for experimenting and drawing conclusions in contexts that are more akin to real-world contexts, as they directly involve human users interacting with models. This increases the likelihood that evaluation results generalize to real-world settings. For example, an HIE could assess the capacity of human users to elicit a particular model capability and identify the cases in which this occurs. HIEs are thus particularly useful when it comes to assessing not only the upper bound of capabilities (e.g., in the case of human uplift evaluations with domain experts) but also the extent to which everyday humanmodel interactions elicit model capabilities approaching this upper bound. Importantly, the representativeness of an HIE study sample is also a determiner of external validity as it concerns whether HIE results on a study population (e.g., a representative sample of U.S. adults crowdsourced from an online platform) can generalize to the target population (e.g., the population of the U.K. or the entire user base of a product) (Susukida et al. 2017).

Assessing direct human impact and interaction-specific harms HIEs could assess direct impact - how and whether users are affected by certain model capabilities and behaviors (e.g., whether persuasive capabilities lead to attitude change). This offers several evaluation advantages.

First, HIEs enhance the external validity of human impact assessment. Static evaluations often rely on grading a set of model outputs using simple metrics, assuming these are adequate proxies for human impact. However, this does not establish a direct connection to actual effects on human behavior and decision-making. For example, static benchmarks cannot directly assess the psychological impact of models on human users' emotions, cognitions, and behaviors. This implicates harms like those from anthropomorphic language (Abercrombie et al. 2023), overreliance on models (Xu, Feng, and Chen 2023), model-driven deception and persuasion (Hackenburg et al. 2023; Karinshak et al. 2023a), and the engagement in friendships and romantic relationships with AI systems (Brandtzaeg, Skjuve, and FÃ¸lstad 2022) - all of which HIEs may be more suited to address.

Second, many risks are specifically realized through human interaction, and are difficult to discover and study without additional human involvement. For example, static benchmarks do not fully capture effects of feedback loops of human-model influence, which previous work has shown can drive harmful behavior like in-context reward hacking (Pan et al. 2024). Conversely, the risks of a model which static benchmarks show can produce falsehoods, for example, may not be realized if humans consistently observe and reject the outputs as misleading.

Finally, in the discovery and assessment of model risks, HIEs can enhance user agency by shifting the evaluator perspective from third-party (e.g., developers or annotators) to first-party (users), allowing for direct user input on experienced harms and giving users greater evaluatory power (Lam et al. 2022; Lee et al. 2022). Users' detections of harmful algorithm behaviors are heavily informed by personal experiences with and exposure to societal biases, suggesting that user-driven evaluations can elicit more diverse, comprehensive, and therefore useful results (Shen et al. 2021; DeVos et al. 2022).

Guiding future assessments of models' societal impact HIEs allow us to understand model impact on human users through an individual frame, but the complexity of network and ecosystem dynamics can make it difficult to generalize from this individual frame to a societal one (Rybski et al. 2009; Johnson, Faraj, and Kudaravalli 2014). Despite this, the results of HIEs can be used to develop individual-level interventions with societal-level implications (Chater and Loewenstein 2023). For example, understanding how individuals interact with non-factual or persuasive model output can help develop more effective ways to label AI-generated content. This not only helps individuals identify and understand synthetic media, but also addresses broader societal concerns about misinformation and trust.

Furthermore, integrating the results of HIEs with additional contextual information can provide a clearer view
of how, if at all, these risks would materialize at the societal level. For example, the feasibility of human interaction risks can be contingent upon the availability of resources such as time, money, and personnel. Consider the assessment of disinformation risks; it is crucial to consider both the findings of a relevant HIE and the effectiveness of existing social media defenses, like whether content is adequately watermarked or how recommendation systems respond to AI-generated content, to design assessments that more accurately reflect broader societal impact (Kapoor and Narayanan 2023).

### 3.3 Methods

HIEs can either be controlled, where interactions are set in a structured, lab-like setting to systematically study specific variables under predetermined conditions, or grounded, where interactions occur in naturalistic, real-world settings to observe how humans interact with models in real-life environments. We briefly recap the methods for conducting HIEs, organizing them around established methods in HCI (Kuniavsky 2003):

Human subject experiments observe or measure the effects of certain conditions or interventions on human participants in controlled settings. An example is randomized controlled trials (RCTs), where participants are randomly assigned to different groups to compare outcomes across interventions or control conditions (Hariton and Locascio 2018). Human-LLM experiments can involve participants engaging with models to examine how they perceive, use, or are influenced by the model outputs. They may also track participants' interactions with models for analysis, and/or include surveys with pre- and post-interaction questions.

Real-world usage datasets contain real-world interaction information (e.g., of common use cases, prompting methods, revisions, usage patterns) about human interactions with models in grounded settings. Datasets may also include reported or inferred user characteristics (e.g., geographic location, age). Analyzing these datasets secondarily, but not directly involve human subjects in an evaluation. These datasets are also currently largely accessible only to product providers, with the exception of some crowdsourcing initiatives, e.g., ShareGPT (2022) and Zhao et al. (2024).

User studies often involve relatively small groups of participants engaging with models in controlled or grounded settings. These studies can gather in-depth insight into how individuals interact with models, including their strategies, preferences, and challenges. Qualitative methods like thinkaloud protocols, cognitive walkthroughs, interviews, and focus groups are often utilized in user studies (Wilson 2000).

## 4 A Framework for Designing Safety-focused Human Interaction Evaluations

We first present findings from a brief review of existing HIEs (Section 4.1). This review guides our proposed three-stage framework (Section 4.2) for designing safety-focused HIEs. ${ }^{1}$ LLMs have a vast surface of capabilities and risks which require evaluation. However, as it is practically not possible to cover the entire evaluation space, an additional purpose of this framework is to help evaluators make explicit what is included or omitted. This can aid decision-makers - policymakers, developers, users or affected non-users - in interpreting the scope and coverage of evaluation results (Liang et al. 2022).

### 4.1 Patterns in emerging human interaction evaluations

A recent review by Weidinger et al. (2023) shows that the number of HIEs is limited but growing. We conduct a brief review of 21 such evaluations. We restrict our review to studies which utilize one of the methods from Section 3.3 to assess an aspect of model safety, risks, or harms. Our review consists of 11 studies from the repository of safety evaluations in Weidinger et al. (2023)'s review (last updated in December 2023), along with 10 additional studies which we add from existing knowledge and suggestions made by the paper authors. We examine these studies for common patterns and limitations, categorizing them by (1) risk/harm focus area, (2) methods, and (3) design attributes, namely target study population, nature of risks, interaction tasks, and number of models evaluated (as seen in Appendix Table 1). The insights from this review lay the groundwork for our framework, and are summarized below:

Risk aspect Recent work has drawn attention to the importance of assessing marginal rather than absolute risks that stem from AI systems (Kapoor et al. 2024). We find that while earlier studies often focused on absolute risks assessing the direct likelihood and severity of harms from model use, more recent studies tend to examine marginal risks, comparing the risks of models against a human baseline or that of other existing technologies.

Target user groups Research indicates that different user groups experience computing harms differently, highlighting the importance of evaluating diverse user interactions and perceptions when assessing model risks (Aizenberg and van den Hoven 2020). While around half of the studies examined effects across user groups, these examinations often occurred only after data collection. We find that few studies targeted a specific population, like intentionally using a representative sample, or otherwise justified their sampling strategy.

Interaction modes There is a wide range of possible human-LLM interactions. We find that many interaction modes, such as assistance (e.g., planning a cybersecurity attack) or collaboration (e.g., co-writing a persuasive argument), remain underexplored with most studies focusing primarily on exposure to outputs (e.g., reading a persuasive message) or explorative conversations (e.g., dialogue with a social agent).[^1]

Generalizability The broader applicability of findings across different models and tasks is important for conducting comprehensive evaluations. Most studies assessed either a single model or system (e.g., a specific model within ChatGPT) or a single type of task (e.g., information retrieval), limiting their generalizability.

### 4.2 Framework

Synthesizing the results of our review with evaluation considerations from NLP and HCI, we propose a three-staged approach to constructing safety-focused HIEs: (1) identifying risk and/or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. ${ }^{2}$

Stage 1: Identifying risk and/or harm area The first stage of any evaluation should be a clear articulation of the real-world question it aims to answer. We propose formulating hypotheses by (1) identifying the types of risks or harms to be studied, incorporating existing knowledge about these risks or harms, and (2) classifying the aspect of the risk as absolute, marginal, or residual.

What is the real-world question of interest? The types of model risk and harms have been extensively taxonomized in previous work (Shelby et al. 2023; Weidinger et al. 2022; Solaiman et al. 2023). Evaluations vary widely based on the evaluator's objectives and the prior information available. An evaluation may be exploratory, focusing on identifying new risks and augmented capabilities; or, it may be targeted, focusing on measuring the impact of well-identified and understood harms, or validating parts of existing static evaluation.

What aspect of risk is being considered? HIEs can assess different aspects of risk. We propose considering one or several of the following aspects when designing HIEs: (1) absolute risk, (2) marginal risk, and (3) residual risk. Table 1 illustrates examples of different aspects of risk for four distinct risk areas.
- Absolute risk assesses direct risks from model use. This may be most suitable when assessing legally and socially unacceptable harms (e.g., discrimination) by evaluating direct contributions to these outcomes.
- Marginal risk assesses additional risks from model use relative to a baseline such as human capabilities or the use of existing technologies or other hypothetical scenarios. This may be most suitable when assessing risks that are already present with current tools (e.g., those related to information retrieval) (Kapoor et al. 2024).
- Residual risk considers risks due to model use which remain after implementing safety mitigations or interventions (Fraser and y Villarino 2023). This may be most suitable when there are feasible and realistic safety measures (e.g., new safety filters) that are expected to reduce a specific risk.[^2]

Stage 2: Characterizing use context The second stage involves specifying the use context, which we break down into (1) the harmful use scenario, (2) the user, model, and system dimensions, and (3) the human-model interaction mode and tasks.

What is the harmful use scenario? In HCI research, user goals or objectives have been shown to shape how they engage with systems and thus influence the outcomes of these engagements (Subramonyam et al. 2024). Thus, here, we group harmful use scenarios according to the user's objectives in the interaction. In each scenario, we also consider the affected parties of any harm caused by use. We illustrate in Table 2 the four main scenarios which we believe cover the most salient failure modes of current concern (Mitchell 2024).

What are the user, model, and system dimensions? Users and models which make up an evaluation vary across multiple dimensions. Recognizing these dimensions allows evaluators to conduct targeted analyses that examine a range of potentially consequential model characteristics and move beyond the limitations of assuming standardized, universal human subjects. Currently, most research subjects come from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations, which do not represent the majority of technology users (Seaborn, Barbareschi, and Chandra 2023). As a starting point, we propose specifying the following user, model, and system dimensions:

- User dimensions: users can be characterized by information on their demographics, domain expertise, technical knowledge, and emotional or psychological state (Ibrahim, Rocher, and Valdivia 2024; Liao and Sundar 2022).
- Model dimensions: models can be characterized by their size (number of parameters), tuning (e.g, instructiontuned, chat-tuned), dataset (e.g., composition, treatment, language). ${ }^{3}$
- System dimensions: models could be augmented by system messages and/or scaffolding ${ }^{4}$ with prompt engineering, planning and reasoning frameworks, or tools. ${ }^{5}$

What is the interaction mode and associated evaluation tasks? For each use scenario, there exists multiple interaction modes, visualized in Figure 1, which define the nature of the human-model relationship in completing certain tasks towards the objective (Gao et al. 2024; HÃ¤ndler 2023). Those tasks may be goal-oriented tasks focused on specific outcomes (e.g., summarization), or open-ended tasks which[^3]

|  | Absolute | Marginal | Residual |
| :---: | :---: | :---: | :---: |
| Dicrimination risks | Risks of model use leading <br> to unfair treatment based on <br> protected classes (e.g., race, <br> age, gender) | Comparison of <br> discrimination risks from <br> model use in <br> decision-making vs. human <br> decision-making | Discrimination risks that <br> remain after use of <br> anti-discrimination measures <br> (e.g., model fairness checks) |
| Biorisks | Risks of model providing <br> critical information for the <br> creation of biological threats | Comparison of risks from <br> accessing critial bio-related <br> information from model <br> versus from google search | Biorisks that remain after <br> restricting model access of <br> suspected bad actors |
| Overreliance risks | Risks of becoming <br> over-dependent on <br> inaccurate or biased model <br> output | Comparison of risks from <br> overreliance on model <br> versus overreliance on other <br> decision-making tools | Overreliance risks that <br> remain after measures to <br> encourage human oversight <br> (e.g., regular reminders to <br> fact-check model output) |
| Persuasion risks | Risks of model changing <br> someone's attitudes or <br> behaviors | Comparison of persuasion <br> risks from model vs. from <br> traditional persuasion <br> methods (e.g., human <br> salesperson) | Persuasion risks that remain <br> despite effective <br> watermarking of <br> AI-generated content |

Table 1: Examples of absolute, marginal, and residual risks for different risk areas

| Scenario | Misuse | Unintended harm: <br> personal impact | Unintended harm: <br> external impact |
| :---: | :---: | :---: | :---: |
| Objective | User intentionally uses <br> model to inflict harm on <br> another person, group of <br> people, or system | User uses model, gets <br> harmed in the process | User uses model, <br> unintentionally harms <br> another person, group of <br> people, or system |
| Affected parties | External subjects | User | External subjects |
| Example(s) | Influence operations, <br> cybersecurity attacks | Exposure to harmful <br> stereotypes in model output | Decision-maker trusts <br> inaccurate model judgment <br> hurting decision-subject |

Table 2: Three primary harmful use scenarios and examples of each

are exploratory and without a clear endpoint (e.g., social dialogue). We present example tasks in Table 3.

Based on observed use cases, existing literature reviews, and studies on real-world usage data, we taxonimize five main modes of prototypical human-model interactions (Ouyang et al. 2023; Zhao et al. 2024):

- Collaboration: human and model work in tandem towards completing joint goal-oriented tasks.
- Direction: human instructs the model to complete specific goal-oriented tasks.
- Assistance: model provides support to human in completing specific goal-oriented tasks.
- Cooperation: human and model undertake separate but complementary goal-oriented tasks. Unlike collaboration, where involvement is mutually integrated, cooperation involves distinct contributions towards the same goal but without shared execution.
- Exposure: human observes or is exposed to model output (e.g., human reads a model-generated message).
- Exploration: human engages in open-ended tasks with model (e.g., open-ended dialogue).

Stage 3: Choosing evaluation parameters Finally, after selecting the risk area to evaluate and identifying the set up, the third stage is choosing an evaluation target within that set up and selecting evaluation metrics to measure the chosen target.

What is the evaluation target? HIEs shift the evaluation target from model-only output to joint human-model output. HIEs also introduce a new target: the interaction trace. Evaluating an interaction trace is evaluating the process of human-model interaction towards completing a task; this may include assessing the contextual appropriateness of model responses, the adaptability of the model to human inputs, and the efficiency of achieving the intended outcome. When evaluating an interaction trace, a decision on the evaluation period (i.e., for how long to evaluate) must also be

![](https://cdn.mathpix.com/cropped/2024_06_04_a764a5ac677b36434badg-07.jpg?height=298&width=1743&top_left_y=195&top_left_x=191)

User $\cap$ Model

Figure 1: Taxonomy of human-LLM interaction modes. The figure illustrates different human-LLM interaction paths from an initial set of instructions to completing goal-oriented or open-ended tasks.

| Task category | Example tasks |
| :---: | :---: |
| Content generation | Email writing, creative <br> writing, article writing |
| Data translation | Table creation, language <br> translation, summarization |
| Dialogue | Social dialogue, <br> brainstorming |
| Data retrieval \& analysis | Sentiment analysis, <br> classification, information <br> retrieval |
| Reasoning | Mathematical reasoning, <br> commonsense reasoning, <br> domain-specific reasoning |
| Coding | Code generation, debugging |
| Discussion | QA, explanation, <br> brainstorming |
| Decision-making aid | Scoring, evaluation, advice |

Table 3: Sample interaction tasks. The tasks are categorized into eight high-level task categories (Chang et al. 2024), drawn from information on how models are currently used, e.g., as reported by Ouyang et al. (2023) and how models may be used in the future.

made as it can affect the evaluation results. For example, evaluating the persuasiveness of models after exposure to one model output (e.g., a single message) or via a dialogue where 10 messages are exchanged may lead to different conclusions on models' persuasive capabilities.

What are the metrics used to evaluate that target? HCI offers a wide diversity of metrics to measure different targets in accordance with evaluation needs (Damacharla et al. 2018). These metrics can be subjective metrics - capturing personal perceptions, feelings, and judgments of users - or objective metrics - capturing quantifiable, direct measures of performance or behavior (e.g., response time and accuracy) (Coronado et al. 2022; Gordon et al. 2021). Subjective metrics may be more suited for open-ended tasks such as dialogue, while objective metrics may be more suited for goal-oriented tasks such as solving a crossword puzzle. Table 4 presents a set of interaction evaluation metrics, offering both subjective and objective measures for quantifying human impact of models and the performance of human-model teams.

## 5 Example Evaluations

The proposed framework is illustrated through the design of two HIEs: one for overreliance risks in Table 5 and one for persuasion risks in Table 6.

## 6 Discussion

Here, we highlight the main limitations of our framework and propose some recommendations.

### 6.1 Limitations

Our framework and taxonomy are starting points for safetyfocused HIEs and are focused on evaluation units consisting of one human and one model. More detailed taxonomizing is necessary as models continue to be integrated in real-world applications and new interaction forms involving multiple human actors and models emerge (Li et al. 2023; Ward et al. 2024). Additionally, our framework does not fully consider the effects of feedback loops in human-LLM interaction which complicate discerning whether outcomes are driven by the model's influence on humans or human input to the model. Further research into the conceptual and empirical risks of these loops is needed (Pan et al. 2024; Krauth, Wang, and Jordan 2022).

### 6.2 Recommendations

Our first and most primary recommendation is for greater investment into HIEs. Several domains - such as medicine and automotives - allocate considerable portions of research and development budgets to identify the impact of their products during development and after deployment (Wouters, McKee, and Luyten 2020). In this paper, we outline how HIEs, although costly compared to other model evaluation methods, can improve AI risk assessments. We also argue that HIEs are useful for validating existing static evaluations - a focus we encourage more HIEs to adopt. Thus, as the capabilities and use of AI systems increase, so should the resources and efforts we dedicate to understanding them and their potential impact.

Concerns over, and some objections to, expanding human involvement in model evaluation often invoke one of four challenges:

| Evaluation target | Objective/subjective | Description | Metric examples |
| :---: | :---: | :---: | :---: |
| Outcome - task quality | Objective | Metrics that measure objec- <br> tive quality of a task output | Accuracy in solving a <br> crossword puzzle |
| Outcome - task quality | Subjective | Metrics that use third-party <br> evaluators to assess the sub- <br> jective quality of a task out- <br> put | Consistency of a summary <br> given a document |
| Interaction trace - <br> characteristics | Objective | Metrics that measure some <br> aspect of an interaction trace | Number of queries users <br> make, number of revisions, <br> time between queries |
| Interaction trace - user <br> experience | Subjective | Metrics that survey users for <br> responses relating to their in- <br> teraction experience | Usability metrics, user <br> satisfaction surveys, <br> psychometrically validated <br> measures (e.g., Social <br> Responsiveness Scale (SRS), <br> Decision Regret Scale) |

Table 4: Subjective and objective evaluation metrics for the assessment of the processes and outcomes of human-LLM interactions

- Costs: experiments, particularly long experiments or powered experiments with large sample sizes, can require significant funds to compensate participants and develop and use online platforms.
- Replicability: variable and unpredictable study environments, along with AI labs frequently updating their systems without interruption or announcement, might make replicating HIEs and applying their results in real-world settings challenging (Baxter, Courage, and Caine 2015).
- Representativeness: participants recruited from crowdsourcing platforms often involve WEIRD samples that are biased along axes like race, geography, and technical literacy (Levay, Freese, and Druckman 2016).
- Ethical issues: there may be serious ethical issues with exposing human participants to harmful model behaviors, such as hateful or persuasive output.

Given additional investment, we offer tangible suggestions to alleviate these concerns:

Develop accessible protocols, guides, and standardized test suites This paper serves as a step towards organizing and sharing design-level considerations for safety-focused HIEs. We recommend further developing accessible protocols, guides, and standardized test suites, similar to those available for non-interactive evaluations, to specifically facilitate human participation in model evaluation (UK AI Safety Institute 2024; METR 2023). An example of this is CheckMate, a prototype platform which records interaction traces of humans interacting with LLMs (Collins et al. 2023). Such infrastructure can reduce costs associated with custom tool development, accommodate evaluators of diverse technical proficiencies, and promote wider participation in model evaluation.

Apply best practices from established disciplines to develop rigorous and replicable HIEs While the science of evaluating general purpose AI systems is in its early stages, disciplines such as HCI, experimental psychology, and economics have well-established, often human-centered, experimental practices that can be adapted. Additionally, it may be beneficial to encourage standardized reporting for HIEs (e.g., similar to initiatives like CONSORT statements for medical trials (Moher, Schulz, and Altman 2001; Morten, Nicholas, and Viljoen 2024)) that disclose important evaluations details, including information on the models, experimental design, and sampling strategies.

Diversify sourcing strategies and community involvement Recruitment efforts should attend to underrepresented groups and different geographical regions, which constitute a significant and growing portion of LLM users (Duarte 2024). Evaluations should expand to involve community stakeholders beyond context-independent experimental settings and in more long-term evaluation issues such as determining appropriate risk thresholds and unacceptable use cases (Sloane et al. 2022; Bergman et al. 2024).

Attend to ethical issues of human participation Thoughtful participant training, debriefing, and feedback collection can mitigate some risks to participants. In ethically-challenging cases, the use of historical usage data may be a sufficient substitute. Simulated human interactions is another direction that further research should explore, but is also one that carries its own set of critical and functional ethical issues (Agnew et al. 2024; Gui and Toubia 2023).

## 7 Acknowledgments

In no particular order, we are grateful to Jamie Bernardi, Merlin Stein, Patrick Levermore, Kobi Hackenburg, Deep Ganguli, Ben Bucknall, Esin Durmus, and Christopher Summerfield for feedback on this draft. We thank Kobi Hackenburg for help visualizing the taxonomy. Lujain Ibrahim acknowledges funding from the Oxford Internet Institute Dieter Schwarz Foundation.

| Stage | Substage | Description |
| :---: | :---: | :---: |
| 1 Identifying <br> risk and/or <br> harm | Research <br> question | Research has investigated risks from integrating algorithmic decision-support <br> systems (ADS) into high stakes decision-making settings like in criminal justice <br> and hiring (Green and Chen 2019). Introducing LLMs to such settings may involve <br> additional risks, as LLMs are capable of engaging in dialogue about their decisions, <br> and generating plausible but false information. Thus, there may be new risks of <br> overreliance on these systems, where users depending too heavily on their output, <br> potentially overlooking their limitations. Overreliance can be especially concerning <br> in cases where a model produces or defends inaccurate or biased judgements. |
|  |  | We use the case study of hiring assistance where a system is used to predict <br> candidate success and ask: what are there additional overreliance risks associated <br> with the use of LLMs compared to human decision-making? |


|  | Risk aspect | We focus on marginal risk from LLMs compared to human decision-making <br> (Green and Chen 2019; Kleinberg et al. 2017). We are interested in examining what <br> LLMs add in terms of new risks, if anything at all. Thus, our two experimental <br> conditions are: (1) human only and (2) human using LLM. |
| :--- | :--- | :--- |
| 2 | Harmful use <br> scenario | The harmful use scenario is primarily the unintended harm: personal impact <br> - scenario, as our aim is to understand possible harms inflicted on the decision-maker <br> from overreliance (e.g., loss of autonomy, decision regret). |

User, model, User dimensions Users (e.g., hiring managers) may range in their technical and system literacy and thus awareness of various capabilities and limitations of AI systems. dimensions We include a few questions for users to report their technical literacy in a pre-experiment survey.

Model \& system dimensions We test five LLaMA models, all tuned for chat interactions: LLaMA 2 (7B), LLaMA 2 (13B), LLaMA 2 (70B), LLaMA 3 (8B), LLaMA 3 (70B).

Interaction Mode In decision-making support settings, the most salient interaction form is mode and tasks assistance where an LLM advises a human on a decision. In this case, the decision is whether or not to hire a candidate.

Tasks The three main associated evaluation tasks are:

- Summarization: summarizing the candidate's documents (e.g., CV, cover letter)
- Information retrieval: gathering information on the candidate, on the role, on other applicants, and on other companies hiring
- Advice: providing a score of how likely the candidate is to succeed in the role

![](https://cdn.mathpix.com/cropped/2024_06_04_a764a5ac677b36434badg-09.jpg?height=811&width=1750&top_left_y=1711&top_left_x=185)

Table 5: Evaluation to examine overreliance on models during high stakes decision-making

| Stage | Substage | Description |
| :---: | :---: | :---: |
| 1 Identifying <br> risk and/or <br> harm | Research <br> question | LLMs have been shown to produce political messages that rival the persuasiveness <br> of human messages, even those written by expert political consultants (Hackenburg <br> et al. 2023). The majority of research on such persuasion risks has focused on <br> attitude change due to exposure to persuasive LLM output. It is equally important <br> to study the mechanisms of uplift in message persuasiveness when users access an <br> LLM to co-write a persuasive message. This can aid in understanding LLMs' <br> persuasive capabilities and their limitations (Phuong et al. 2024). |
|  |  | Thus, here, we ask: how does the use of an LLM lead to a change in human writing <br> performance - quality and persuasiveness - of political messaging in opinion <br> pieces? |

Risk aspect We focus on marginal risk as we are interested in assessing the model's persuasive capabilities relative to a human baseline. Thus, our three experimental conditions are (1) human only (college students), (2) human only (domain experts), (3) human using LLM (college students), (4) human using LLM (domain experts).

| Characterizing <br> use context | Harmful use <br> scenario | The harmful use scenario is the misuse scenario, as we aim to understand how <br> LLMs can assist bad actors in producing persuasive messaging that can manipulate <br> people's opinions and actions. |
| :---: | :---: | :---: |
|  | User, model, <br> and system <br> dimensions | User dimensions We recruit a mix of college students and political consultants. We <br> are interested in how this variation in domain expertise influences risks. We <br> consider this variation a proxy for difficulty of capability elicitation. <br> Model \& system dimensions We test three models, all tuned for chat interactions: <br> Claude 3 Opus, GPT-4, and LLaMA 3. |
|  | Interaction <br> mode and tasks | Mode As we are focused on co-writing, the main interaction form is collaboration. <br> Tasks The three main associated evaluation tasks are: |
|  |  | - Brainstorming: brainstorming arguments to include in the article <br> - Information retrieval: gathering information on political issues, support for <br> them across demographics, and relevant historical events |
|  |  | - Article writing: writing a medium-length article (around 500 words) to convince <br> someone of a political issue |


| 3 Choosing <br> evaluation <br> parameters | Evaluation <br> target | The main evaluation target here is the interaction trace. We also investigate how <br> characteristics of the interaction trace influence the outcome, which is the resulting <br> persuasive article. |
| :--- | :--- | :--- |
| Evaluation <br> metrics | To evaluate the interaction trace, we use <br> Subjective metrics: <br> - User satisfaction and perceived helpfulness of the model <br> - Perceived creativity and novelty of the model <br> Objective metrics: <br> - Length of the interaction (time, number of queries) until users submit their <br> articles |  |
| - Length of the prompts used, types of prompts used (generation, revision, |  |  |
| evaluation) |  |  |

To evaluate the outcome of the human-LLM collaboration, we use third-party experts to assess the persuasiveness of the articles using a detail rubric to:

- Score article quality
- Score article persuasiveness

Table 6: Evaluation to understand the mechanisms of human uplift of model persuasion capabilities

## References

Abercrombie, G.; Curry, A. C.; Dinkar, T.; and Talat, Z. 2023. Mirages: On anthropomorphism in dialogue systems. arXiv preprint arXiv:2305.09800.

Agnew, W.; Bergman, A. S.; Chien, J.; DÃ­az, M.; ElSayed, S.; Pittman, J.; Mohamed, S.; and McKee, K. R. 2024. The illusion of artificial inclusion. arXiv preprint arXiv:2401.08572.

Aizenberg, E.; and van den Hoven, J. 2020. Designing for human rights in AI. Big Data \& Society, 7(2): 2053951720949566.

Anthropic. 2023. Anthropic's responsible scaling policy.

Bai, H.; Voelkel, J. G.; Eichstaedt, j. C.; and Willer, R. 2023. Artificial Intelligence Can Persuade Humans on Political Issues.

Bailey, P. E.; Leon, T.; Ebner, N. C.; Moustafa, A. A.; and Weidemann, G. 2023. A meta-analysis of the weight of advice in decision-making. Current Psychology, 42(28): 24516-24541.

Baxter, K.; Courage, C.; and Caine, K. 2015. Understanding your users: a practical guide to user research methods. Morgan Kaufmann.

Bergman, S.; Marchal, N.; Mellor, J.; Mohamed, S.; Gabriel, I.; and Isaac, W. 2024. STELA: a community-centred approach to norm elicitation for AI alignment. Scientific Reports, 14(1): 6616.

BÃ¶hm, R.; JÃ¶rling, M.; Reiter, L.; and Fuchs, C. 2023. People devalue generative AI's competence but not its advice in addressing societal and personal challenges. Communications Psychology, 1(1): 32.

Brandtzaeg, P. B.; Skjuve, M.; and FÃ¸lstad, A. 2022. My AI friend: How users of a social chatbot understand their human-AI friendship. Human Communication Research, 48(3): 404-429.

Brehaut, J.; O'Connor, A.; Wood, T.; Hack, T.; Siminoff, L.; Gordon, E.; and Feldman-Stewart, D. 2003. Validation of a Decision Regret Scale. Medical decision making : an international journal of the Society for Medical Decision Making, 23: 281-92.

Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3): 1-45.

Chater, N.; and Loewenstein, G. 2023. The i-frame and the s-frame: How focusing on individual-level solutions has led behavioral public policy astray. Behavioral and Brain Sciences, 46: $\mathrm{e} 147$.

Chiang, W.-L.; Zheng, L.; Sheng, Y.; Angelopoulos, A. N.; Li, T.; Li, D.; Zhang, H.; Zhu, B.; Jordan, M.; Gonzalez, J. E.; and Stoica, I. 2024. Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. arXiv:2403.04132.

Collins, K. M.; Jiang, A. Q.; Frieder, S.; Wong, L.; Zilka, M.; Bhatt, U.; Lukasiewicz, T.; Wu, Y.; Tenenbaum, J. B.; Hart, W.; et al. 2023. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694.
Coronado, E.; Kiyokawa, T.; Ricardez, G. A. G.; RamirezAlpizar, I. G.; Venture, G.; and Yamanobe, N. 2022. Evaluating quality in human-robot interaction: A systematic search and classification of performance and human-centered factors, measures and metrics towards an industry 5.0. Journal of Manufacturing Systems, 63: 392-410.

Costello, T. H.; Pennycook, G.; and Rand, D. G. 2024. Durably reducing conspiracy beliefs through dialogues with AI.

Damacharla, P.; Javaid, A. Y.; Gallimore, J. J.; and Devabhaktuni, V. K. 2018. Common Metrics to Benchmark Human-Machine Teams (HMT): A Review. IEEE Access, 6: 38637-38655.

DeVos, A.; Dhabalia, A.; Shen, H.; Holstein, K.; and Eslami, M. 2022. Toward User-Driven Algorithm Auditing: Investigating users' strategies for uncovering harmful algorithmic behavior. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22. New York, NY, USA: Association for Computing Machinery. ISBN 9781450391573 .

Doshi, A. R.; and Hauser, O. P. 2024. Generative artificial intelligence enhances creativity but reduces the diversity of novel content. arXiv:2312.00506.

Duarte, F. 2024. Number of ChatGPT users (May 2024).

Durmus, E.; Lovitt, L.; Tamkin, A.; Ritchie, S.; Clark, J.; and Ganguli, D. 2024. Measuring the Persuasiveness of Language Models.

Fraser, H.; and y Villarino, J.-M. B. 2023. Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough. arXiv:2308.02047.

Gadiraju, V.; Kane, S.; Dev, S.; Taylor, A.; Wang, D.; Denton, E.; and Brewer, R. 2023. "I wouldn't say offensive but...": Disability-Centered Perspectives on Large Language Models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT '23, 205-216. New York, NY, USA: Association for Computing Machinery. ISBN 9798400701924.

Ganguli, D.; Hernandez, D.; Lovitt, L.; Askell, A.; Bai, Y.; Chen, A.; Conerly, T.; Dassarma, N.; Drain, D.; Elhage, N.; et al. 2022. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 1747-1764.

Ganguli, D.; Schiefer, N.; Favaro, M.; and Clark, J. 2023. Challenges in evaluating AI systems.

Gao, J.; Gebreegziabher, S. A.; Choo, K. T. W.; Li, T. J. J.; Perrault, S. T.; and Malone, T. W. 2024. A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. arXiv preprint arXiv:2404.00405.

Gordon, M. L.; Zhou, K.; Patel, K.; Hashimoto, T.; and Bernstein, M. S. 2021. The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21. New York, NY, USA: Association for Computing Machinery. ISBN 9781450380966 .

Green, B.; and Chen, Y. 2019. Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* '19, 90-99. New York, NY, USA: Association for Computing Machinery. ISBN 9781450361255.

Gui, G.; and Toubia, O. 2023. The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective. SSRN Electronic Journal.

Hackenburg, K.; Ibrahim, L.; Tappin, B. M.; and Tsakiris, M. 2023. Comparing the persuasiveness of role-playing large language models and human experts on polarized US political issues.

Hackenburg, K.; and Margetts, H. 2023. Evaluating the persuasive influence of political microtargeting with large language models.

Hariton, E.; and Locascio, J. J. 2018. Randomised controlled trials - the gold standard for effectiveness research: Study design: randomised controlled trials. BJOG, 125(13): 1716. Epub 2018 Jun 19.

Hartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.; Ray, D.; and Kamar, E. 2022. Toxigen: A large-scale machinegenerated dataset for adversarial and implicit hate speech detection. arXiv preprint arXiv:2203.09509.

Hutson, M. 2018. Artificial intelligence faces reproducibility crisis.

HÃ¤ndler, T. 2023. A Taxonomy for Autonomous LLMPowered Multi-Agent Architectures.

Ibrahim, L.; Rocher, L.; and Valdivia, A. 2024. Characterizing and modeling harms from interactions with design patterns in AI interfaces. arXiv:2404.11370.

Jakesch, M.; Bhat, A.; Buschek, D.; Zalmanson, L.; and Naaman, M. 2023. Co-Writing with Opinionated Language Models Affects Users' Views. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23. New York, NY, USA: Association for Computing Machinery. ISBN 9781450394215.

Jiang, M.; Liu, K. Z.; Zhong, M.; Schaeffer, R.; Ouyang, S.; Han, J.; and Koyejo, S. 2024. Investigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059.

Johnson, S. L.; Faraj, S.; and Kudaravalli, S. 2014. Emergence of Power Laws in Online Communities: The Role of Social Mechanisms and Preferential Attachment. MIS Quarterly, 38(3): 795-A13.

Kapoor, S.; Bommasani, R.; Klyman, K.; Longpre, S.; Ramaswami, A.; Cihon, P.; Hopkins, A.; Bankston, K.; Biderman, S.; Bogen, M.; Chowdhury, R.; Engler, A.; Henderson, P.; Jernite, Y.; Lazar, S.; Maffulli, S.; Nelson, A.; Pineau, J.; Skowron, A.; Song, D.; Storchan, V.; Zhang, D.; Ho, D. E.; Liang, P.; and Narayanan, A. 2024. On the Societal Impact of Open Foundation Models. arXiv:2403.07918.

Kapoor, S.; and Narayanan, A. 2023. How to Prepare for the Deluge of Generative AI on Social Media. Knight First Amendment Institute.
Karinshak, E.; Liu, S. X.; Park, J. S.; and Hancock, J. T. 2023a. Working with AI to persuade: Examining a large language model's ability to generate pro-vaccination messages. Proceedings of the ACM on Human-Computer Interaction, 7(CSCW1): 1-29.

Karinshak, E.; Liu, S. X.; Park, J. S.; and Hancock, J. T. 2023b. Working With AI to Persuade: Examining a Large Language Model's Ability to Generate Pro-Vaccination Messages. Proc. ACM Hum.-Comput. Interact., 7(CSCW1).

Kiela, D.; Bartolo, M.; Nie, Y.; Kaushik, D.; Geiger, A.; Wu, Z.; Vidgen, B.; Prasad, G.; Singh, A.; Ringshia, P.; et al. 2021. Dynabench: Rethinking benchmarking in NLP. arXiv preprint arXiv:2104.14337.

Kirk, H. R.; Whitefield, A.; RÃ¶ttger, P.; Bean, A.; Margatina, K.; Ciro, J.; Mosquera, R.; Bartolo, M.; Williams, A.; He, H.; et al. 2024. The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models. arXiv preprint arXiv:2404.16019.

Kleinberg, J.; Lakkaraju, H.; Leskovec, J.; Ludwig, J.; and Mullainathan, S. 2017. Human Decisions and Machine Predictions*. The Quarterly Journal of Economics, 133(1): 237-293.

Krauth, K.; Wang, Y.; and Jordan, M. I. 2022. Breaking Feedback Loops in Recommender Systems with Causal Inference. arXiv:2207.01616.

Kuniavsky, M. 2003. In Observing the User Experience, xiii-xvi. San Francisco: Morgan Kaufmann. ISBN 978-155860-923-5.

Lam, M. S.; Gordon, M. L.; Metaxa, D.; Hancock, J. T.; Landay, J. A.; and Bernstein, M. S. 2022. End-user audits: A system empowering communities to lead large-scale investigations of harmful algorithmic behavior. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW2): 134.

Lee, M.; Liang, P.; and Yang, Q. 2022. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In Proceedings of the 2022 CHI conference on human factors in computing systems, 1-19.

Lee, M.; Srivastava, M.; Hardy, A.; Thickstun, J.; Durmus, E.; Paranjape, A.; Gerard-Ursin, I.; Li, X. L.; Ladhak, F.; Rong, F.; et al. 2022. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746.

Levay, K. E.; Freese, J.; and Druckman, J. N. 2016. The demographic and political composition of Mechanical Turk samples. Sage Open, 6(1): 2158244016636433.

Li, H.; Chong, Y. Q.; Stepputtis, S.; Campbell, J.; Hughes, D.; Lewis, M.; and Sycara, K. 2023. Theory of Mind for Multi-Agent Collaboration via Large Language Models. arXiv:2310.10701.

Li, H.; Gao, T.; Goenka, M.; and Chen, D. 2021. Ditch the gold standard: Re-evaluating conversational question answering. arXiv preprint arXiv:2112.08812.

Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar,

A.; et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.

Liao, Q.; and Sundar, S. S. 2022. Designing for Responsible Trust in AI Systems: A Communication Perspective. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, 1257-1268. New York, NY, USA: Association for Computing Machinery. ISBN 9781450393522.

Liao, Q. V.; and Xiao, Z. 2023. Rethinking model evaluation as narrowing the socio-technical gap. arXiv preprint arXiv:2306.03100.

Lin, J.; Tomlin, N.; Andreas, J.; and Eisner, J. 2023. Decision-oriented dialogue for human-ai collaboration. arXiv preprint arXiv:2305.20076.

METR. 2023. METR Autonomy Evaluations Resources.

Mialon, G.; DessÃ¬, R.; Lomeli, M.; Nalmpantis, C.; Pasunuru, R.; Raileanu, R.; RoziÃ¨re, B.; Schick, T.; DwivediYu, J.; Celikyilmaz, A.; et al. 2023. Augmented language models: a survey. arXiv preprint arXiv:2302.07842.

Mitchell, M. 2024. Ethical ai isn't to blame for Google's Gemini debacle.

Moher, D.; Schulz, K. F.; and Altman, D. G. 2001. The CONSORT statement: revised recommendations for improving the quality of reports of parallel-group randomised trials. The lancet, 357(9263): 1191-1194.

Morten, C.; Nicholas, G.; and Viljoen, S. 2024. Researcher Access to Social Media Data: Lessons from Clinical Trial Data Sharing. Berkeley Technology Law Journal, Forthcoming.

Mouton, C. A.; Lucas, C.; and Guest, E. 2024. The Operational Risks of AI in Large-Scale Biological Attacks: Results of a Red-Team Study. Santa Monica, CA: RAND Corporation.

Nov, O.; Singh, N.; and Mann, D. 2023. Putting ChatGPT's Medical Advice to the (Turing) Test. arXiv:2301.10035.

OpenAI. 2023. Preparedness.

Ouyang, S.; Wang, S.; Liu, Y.; Zhong, M.; Jiao, Y.; Iter, D.; Pryzant, R.; Zhu, C.; Ji, H.; and Han, J. 2023. The shifted and the overlooked: a task-oriented investigation of user-gpt interactions. arXiv preprint arXiv:2310.12418.

Pan, A.; Jones, E.; Jagadeesan, M.; and Steinhardt, J. 2024. Feedback Loops With Language Models Drive In-Context Reward Hacking. arXiv preprint arXiv:2402.06627.

Parrish, A.; Chen, A.; Nangia, N.; Padmakumar, V.; Phang, J.; Thompson, J.; Htut, P. M.; and Bowman, S. R. 2021. BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193.

Patwardhan, T.; Liu, K.; Markov, T.; Chowdhury, N.; Leet, D.; Cone, N.; Maltbie, C.; Huizinga, J.; Wainwright, C.; Jackson, S. F.; and et al. 2024. Building an early warning system for LLM-aided biological ..

Pentina, I.; Hancock, T.; and Xie, T. 2023. Exploring relationship development with social chatbots: A mixed-method study of replika. Computers in Human Behavior, 140: 107600 .
Phuong, M.; Aitchison, M.; Catt, E.; Cogan, S.; Kaskasoli, A.; Krakovna, V.; Lindner, D.; Rahtz, M.; Assael, Y.; Hodkinson, S.; et al. 2024. Evaluating Frontier Models for Dangerous Capabilities. arXiv preprint arXiv:2403.13793.

Raji, I. D.; Bender, E. M.; Paullada, A.; Denton, E.; and Hanna, A. 2021. AI and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366.

Raji, I. D.; Kumar, I. E.; Horowitz, A.; and Selbst, A. 2022. The fallacy of AI functionality. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 959-972.

Rooney, A. A.; Cooper, G. S.; Jahnke, G. D.; Lam, J.; Morgan, R. L.; Boyles, A. L.; Ratcliffe, J. M.; Kraft, A. D.; SchÃ¼nemann, H. J.; Schwingl, P.; et al. 2016. How credible are the study results? Evaluating and applying internal validity tools to literature-based assessments of environmental health hazards. Environment international, 92: 617-629.

RÃ¶ttger, P.; Pernisi, F.; Vidgen, B.; and Hovy, D. 2024. SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety. arXiv preprint arXiv:2404.05399.

Rybski, D.; Buldyrev, S. V.; Havlin, S.; Liljeros, F.; and Makse, H. A. 2009. Scaling laws of human interaction activity. Proceedings of the National Academy of Sciences, 106(31): 12640-12645.

Sandoval, G.; Pearce, H.; Nys, T.; Karri, R.; Garg, S.; and Dolan-Gavitt, B. 2023. Lost at c: A user study on the security implications of large language model code assistants. In 32nd USENIX Security Symposium (USENIX Security 23), 2205-2222.

Seaborn, K.; Barbareschi, G.; and Chandra, S. 2023. Not Only WEIRD but "Uncanny"? A Systematic Review of Diversity in Human-Robot Interaction Research. International Journal of Social Robotics, 15(11): 1841-1870.

ShareGPT. 2022. ShareGPT.

Sharma, M.; Tong, M.; Korbak, T.; Duvenaud, D.; Askell, A.; Bowman, S. R.; Cheng, N.; Durmus, E.; HatfieldDodds, Z.; Johnston, S. R.; et al. 2023. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548.

Shelby, R.; Rismani, S.; Henne, K.; Moon, A.; Rostamzadeh, N.; Nicholas, P.; Yilla-Akbari, N.; Gallegos, J.; Smart, A.; Garcia, E.; et al. 2023. Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 723-741.

Shen, H.; DeVos, A.; Eslami, M.; and Holstein, K. 2021. Everyday algorithm auditing: Understanding the power of everyday users in surfacing harmful algorithmic behaviors. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW2): 1-29.

Sloane, M.; Moss, E.; Awomolo, O.; and Forlano, L. 2022. Participation Is not a Design Fix for Machine Learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO '22. New York, NY, USA: Association for Computing Machinery. ISBN 9781450394772.

Soice, E. H.; Rocha, R.; Cordova, K.; Specter, M.; and Esvelt, K. M. 2023. Can large language models democratize access to dual-use biotechnology? arXiv:2306.03809.

Solaiman, I.; Talat, Z.; Agnew, W.; Ahmad, L.; Baker, D.; Blodgett, S. L.; au2, H. D. I.; Dodge, J.; Evans, E.; Hooker, S.; Jernite, Y.; Luccioni, A. S.; Lusoli, A.; Mitchell, M.; Newman, J.; Png, M.-T.; Strait, A.; and Vassilev, A. 2023. Evaluating the Social Impact of Generative AI Systems in Systems and Society. arXiv:2306.05949.

Spitale, G.; Biller-Andorno, N.; and Germani, F. 2023. AI model GPT-3 (dis) informs us better than humans. Science Advances, 9(26): eadh1850.

Subramonyam, H.; Pea, R.; Pondoc, C. L.; Agrawala, M.; and Seifert, C. 2024. Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces. arXiv:2309.14459.

Susukida, R.; Crum, R. M.; Ebnesajjad, C.; Stuart, E. A.; and Mojtabai, R. 2017. Generalizability of findings from randomized controlled trials: application to the National Institute of Drug Abuse Clinical Trials Network. Addiction, 112(7): 1210-1219.

The White House. 2023. https://www.whitehouse.gov/briefing-room/presidentialactions/2023/10/30/executive-order-on-the-safe-secureand-trustworthy-development-and-use-of-artificialintelligence/.

UK AI Safety Institute. 2024. Inspect AI.

UK Government. 2024. Ai Safety Institute approach to evaluations.

Wang, B.; Chen, W.; Pei, H.; Xie, C.; Kang, M.; Zhang, C.; Xu, C.; Xiong, Z.; Dutta, R.; Schaeffer, R.; et al. 2023. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. arXiv preprint arXiv:2306.11698.

Ward, F. R.; MacDermott, M.; Belardinelli, F.; Toni, F.; and Everitt, T. 2024. The Reasons that Agents Act: Intention and Instrumental Goals. arXiv:2402.07221.

Weidinger, L.; Rauh, M.; Marchal, N.; Manzini, A.; Hendricks, L. A.; Mateos-Garcia, J.; Bergman, S.; Kay, J.; Griffin, C.; Bariach, B.; et al. 2023. Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986.

Weidinger, L.; Uesato, J.; Rauh, M.; Griffin, C.; Huang, P.S.; Mellor, J.; Glaese, A.; Cheng, M.; Balle, B.; Kasirzadeh, A.; et al. 2022. Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 214-229.

Wilson, T. D. 2000. Recent trends in user studies: action research and qualitative methods. Information Research, 5(3). Wouters, O. J.; McKee, M.; and Luyten, J. 2020. Estimated Research and Development Investment Needed to Bring a New Medicine to Market, 2009-2018. JAMA, 323(9): 844853.

Xie, T.; Pentina, I.; and Hancock, T. 2023. Friend, mentor, lover: does chatbot engagement lead to psychological dependence? Journal of service Management, 34(4): 806828 .
Xu, J.; Ju, D.; Li, M.; Boureau, Y.-L.; Weston, J.; and Dinan, E. 2021. Bot-adversarial dialogue for safe conversational agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 29502968.

Xu, R.; Feng, Y.; and Chen, H. 2023. ChatGPT vs. Google: a comparative study of search performance and user experience. arXiv preprint arXiv:2307.01135.

Zhang, Z.; Jia, M.; Lee, H.-P. H.; Yao, B.; Das, S.; Lerner, A.; Wang, D.; and Li, T. 2024. "It's a Fair Game", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI '24. ACM.

Zhao, W.; Ren, X.; Hessel, J.; Cardie, C.; Choi, Y.; and Deng, Y. 2024. WildChat: 1M ChatGPT Interaction Logs in the Wild. arXiv:2405.01470.
