# MADLAD-400: A Multilingual And Document-Level Large Audited Dataset 

Sneha Kudugunta $^{\dagger} \quad$ Isaac Caswell $^{\diamond} \quad$ Biao Zhang $^{\dagger} \quad$ Xavier Garcia $^{\dagger}$<br>Christopher A. Choquette-Choo ${ }^{\dagger}$ Katherine Lee ${ }^{\dagger} \quad$ Derrick Xin $^{\dagger}$ Aditya Kusupati ${ }^{\diamond}$<br>Romi Stella $^{\dagger}$ Ankur Bapna ${ }^{\dagger}$ Orhan Firat ${ }^{\dagger}$<br>${ }^{\dagger}$ Google DeepMind ${ }^{\circ}$ Google Research


#### Abstract

We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models ${ }^{1}$ available to the research community.


## 1 Introduction

The availability of large multilingual corpora has accelerated the progress of multilingual natural language processing (NLP) models [69, 19, 47, 9, 51]. However, most publicly available generaldomain multilingual corpora contain 100-200 languages [69, 51, 2], with some datasets containing more languages in specific domains such as religious content [4], children's books [45] or dialects [3].

A common approach to creating such datasets is to mine language specific data from general web crawls such as CommonCrawl [57, 43, 68] to create datasets. We simply take this approach and scale it. We train a document-level LangID model on 498 languages to obtain CommonCrawl annotations at a document level and obtain a 5-trillion token, document-level monolingual dataset.

However, such web-scale corpora are known to be noisy and contain undesirable content $[53,48$, 21], with their multilingual partitions often having their own specific issues such as unusable text, misaligned and mislabeled/ambiguously labeled data [40]. To mitigate this, we manually audit our data. Based on our findings, we discard 79 of the languages from our preliminary dataset, rename or combine several languages and apply additional preprocessing steps. Finally, to validate the efficacy of our dataset, we train multilingual machine translation models of various sizes up to 10.7B parameters, as well as an 8B decoder-only model, and then evaluate these models on highly multilingual translation evaluation sets.

In Section 2, we describe the creation and composition of MADLAD-400, and discuss the results of the audit. Then, in Section 3, we describe the parallel data we collect using publicly available sources to train the multilingual machine translation models described in Section 4.1. In Section 4, we describe the training process of the multilingual machine translation models and $8 \mathrm{~B}$ decoder-only model, and then evaluate these models on highly multilingual translation datasets. In Section 5 we describe our tests for memorization in the multilingual models that we release and discuss preliminary results. Finally, we discuss the limitations of this work and directions for future work.

${ }^{1}$ https://github.com/google-research/google-research/tree/master/madlad_400

![](https://cdn.mathpix.com/cropped/2024_06_04_348bc135438c0574c483g-02.jpg?height=553&width=1390&top_left_y=241&top_left_x=365)

Figure 1: Comparing the size of the noisy and clean monolingual datasets in MADLAD-400. The difference is more noticeable on lower-resource languages, where noise effects are especially severe. For reference, languages supported by Google Translate are shaded in green. Note that, since this chart is in log scale, the difference in size is much greater than it may appear; for instance, for the lower-resource half of the dataset, the ratio is about $4 \times$ on median.

## 2 MADLAD-400

The process we follow to create MADLAD-400 is similar to that of other large-scale web corpora $[15,68,2,51]$. First, we collect as large a dataset of unlabeled web text as possible. More specifically, we use all available snapshots of CommonCrawl ${ }^{2}$ as of August 20, 2022. After some preliminary data cleaning, we use a highly multilingual LangID model to provide document-level annotations (Section 2.2). Finally, we conduct a self-audit (Section 2.4), or quality review, of this preliminary dataset partitioned by language, and design filters to remove noisy content. When appropriate, we correct language names and remove languages from the preliminary dataset. We note that building MADLAD-400 was an iterative process, and that while we describe one major quality review in depth, we conducted several stages of filtering. To reflect this, we describe the preprocessing steps and improvements made in chronological order.

We release two version of this dataset: a 5 trillion token noisy dataset, which is the dataset obtained before applying document-level LangID and the final filters, and a 3 trillion token clean dataset, which has a variety of filters applied based on our self-audit, though it naturally has a fair amount of noise itself. Each dataset is released in both a document-level form and a sentence-level form. Some overall statistics for these dataset versions are given in Table 2, with a graph visualizing the distribution of sizes (number of tokens) across languages in Figure 1. The final version of MADLAD- 400 has 419 languages, with a varied geographic distribution, as seen in Table 1.
Table 1: Geographic distribution of languages in MADLAD-400.

| Continent | \# Languages |
| :--- | :---: |
| Asia | 149 |
| Americas | 66 |
| Africa | 87 |
| Europe | 89 |
| Oceania | 26 |
| Constructed | 2 |

Table 2: Overall statistics of both the noisy and clean partitions of MADLAD-400.

| Dataset Version | \# Documents |  | \# Sentences |  | \# Tokens |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Total | Median | Total | Median | Total | Median |
| MADLAD-400-noisy | $7.8 \mathrm{~B}$ | $27 \mathrm{~K}$ | $150 \mathrm{~B}$ | $240 \mathrm{~K}$ | $5.0 \mathrm{~T}$ | $7.1 \mathrm{M}$ |
| MADLAD-400-clean | $4.0 \mathrm{~B}$ | $1.7 \mathrm{~K}$ | $100 \mathrm{~B}$ | $73 \mathrm{~K}$ | $2.8 \mathrm{~T}$ | $1.2 \mathrm{M}$ |

[^0]
### 2.1 Preliminary Filters

We carry out a few preliminary preprocessing steps on the web-crawled corpus: first, we deduplicate lines across documents [44]. Then, we filter out all pages that do not contain at least 3 lines of 200 or more characters (as done by Xue et al. [68]). We also use other commonly used filtering heuristics such as removing lines containing the word "Javascript" and removing pages that contain "lorem ipsum" and curly brackets " \{" (as done by Raffel et al. [57]).

### 2.2 Language Identification (LangID)

We train a Semi-Supervised LangID model (SSLID) on 500 languages, following the recipe introduced by Caswell et al. [15]. We then filter the corpus on document-level LangID, which was taken to be the majority sentence-level LangID prediction. The resulting dataset is MADLAD-400-noisy. For the Additional details on these LangID models is in Appendix A.1.

### 2.3 Filtering Out Questionable Content

To assess the quality of this preliminary dataset, we inspected 20 sentences each from a subset of 30 languages in our dataset. Based on our observations, we introduced a score, pct_questionable. The pct_questionable score is simply the percentage of sentences in the input document that were "questionable". A sentence was considered questionable if any of the following were true:

1. Document consistency: Sentence-level LangID does not match the document-level LangID.
2. List Case: Over 50\% percent of the tokens began in a capital letter (we apply this filter only if the sentence has at least 12 tokens.)
3. Abnormal Lengths: The sentence has under 20 characters or over 500 characters. We note that this is a bad heuristic for ideographic languages ${ }^{3}$ ).
4. Technical Characters: Over $20 \%$ of the characters in the sentence match $[0-9\{\}+/()>]$.
5. Cursed Regexes: The sentence matched a "cursed regex". These are a heuristic set of substrings and regexes that we found accounted for a significant amount of questionable content in the data samples we observed. They are described in depth in Appendix A.2.

We removed all documents with a percent_questionable score greater than $20 \%$. Furthermore, we removed any document with under 5 sentences.

### 2.4 Self-Audit (Quality Review)

After filtering out generally lower-quality content with the approach described above, we performed a self-audit of every corpus in this dataset, following Kreutzer et al. [40]. The aim of our self-audit was to correct any remaining systematic issues by either applying additional filters, renaming/merging language codes, or completely removing the language from the dataset. Although we do not speak most of the 498 languages, we were able to give high-level comments on the general quality. For each language, we inspected a sample of 20 documents. This task was evenly divided between the first two authors based in part on which scripts they could read. We used the following guidelines:

- If dataset is mostly plausibly in-language text, we can keep it. For unknown languages, search the web for a few sentences and look at the website and URL for language clues.
- If dataset is noisy but the noise looks filterable, leave a note of how to filter it.
- If the dataset is very noisy and does not look possible to filter, mark it for removal.
- Optionally put note that may be helpful for downstream users, e.g. if dataset is $100 \%$ Bible.

We made the decision to include languages that looked noisy, but omit any language that was majority noise, or only had 20 or fewer docs. While this is not a high quality bar, we hope it still has the potential to be useful to the research community, given that foundation models have demonstrated the potential to learn distributions for very few exammples [12]. The motivation for not releasing "nonsense" or tiny datasets is to avoid giving a false sense of how multilingual the dataset is ("Representation washing"), as recommended by Quality at a Glance [40].

Overall Results. Of the 498 languages that we obtained LangID annotations for, we decided to omit 79 languages, bringing the final number of languages in MADLAD-400 to 419. Based on[^1]the self-audit, we also expanded the filters (particularly the cursed regexes), and made changes as described in Sections 2.5 and 2.6. We details stats for these languages in Appendix Section A.4.

For transparency, we provide full results of the self-audit in Appendix A.4. In Table 3, we provide an overview of the issues surfaced through this self-audit. We find that a significant fraction of languages contain mostly or entirely religious documents, while other issues include misrendered text, pornographic content, and boilerplate.

Table 3: Summary of results of the audit on the preliminary dataset comprising of 498 languages. Note that there may be multiple issues with data in one language.

| \# Languages... |  |
| :--- | ---: |
| Audited | 498 |
| With significant amounts of Bible data | 141 |
| With significant amounts of JW data | 37 |
| With significant amounts of LDS data | 2 |
| With significant amounts of virama-based issues | 8 |
| With a significant number of short docs | 42 |
| With complaints about noise | 28 |
| With complaints about porn | 10 |
| With complaints about boilerplate | 15 |
| With a note to remove from the dataset | 77 |

### 2.5 Additional Filters

Based on the results of the self-audit, we apply three additional filters.

Virama Filtering and Correction. Many languages using Brahmic Abugida (South and Southeast Asian scripts like Devanagari, Khmer, etc.) use some variant on the virama ${ }^{4}$ character. We found that such languages in MADLAD-400-noisy had incorrectly encoded viramas: for example, तुम्हारे was rendered as तुम हारे., where the middle character is a detached virama. Therefore, for the languages bn, my, pa, gu, or, ta, te, kn, ml, si, th, tl, mn, lo, bo, km, hi, mr, ne, gom, as, jv, dv, bho, dz, hne, ks_Deva, mag, mni, shn, yue, zh, ja, kjg, mnw, ksw, rki, mtr, mwr and xnr, we did a special filtering/correction step - we removed all extraneous spaces before virama characters. We provide the pseudocode and list of virama characters in Appendix A.2.

Zawgyi Encoded Data. We found that languages using Myanmar script like my and mnw appeared to have the same issues with virama characters that still remained after applying the virama correction. This was because a large fraction of Myanmar script data on the internet is Zawgyi encoded data, which appears to have the rendering issues described above if rendered in Unicode. Therefore, we used an open-source Zawgyi detector ${ }^{5}$ to convert the encoding of documents with more than a $50 \%$ probability of being Zawgyi encoded into standard Unicode encoding.

Chinese-Specific Filters. The Mandarin (zh) data in CommonCrawl had a particular issue with pornographic content. We combed through the data and developed a list of strings likely to be present in pornographic content, and filtered out all documents containing the strings in the blocklist. This resulted in a $17 \%$ reduction in the number of documents and a $56 \%$ reduction in file size. We list these strings in Appendix A.2.

### 2.6 Correcting Other Systematic Issues.

Based on various specific notes from the self-audit, we made a variety of changes. Five datasets were found to be in the wrong language, and were renamed or merged into the correct dataset. Six[^2]languages that looked suspicious were run by native speakers of those or related languages, some of which were discarded, and some of which were merged into the correct dataset. Finally, we removed all languages with fewer than 20 documents. Details can be seen in Appendix A.3.

## 3 Parallel Data

To train the machine translation (MT) models described in Section 4.1, we also collect a dataset composed of publicly available datasets coming from various data sources. A full list of the data sources and associated language pairs are in Appendix A.5. The final dataset has 156 languages across 4.1B sentence pairs and 4124 language pairs total. In the rest of the paper, we refer to the input sentence to an MT model as the "source side" and the reference/output sentence as the "target side".

### 3.1 Filters

We describe the data preprocessing steps taken below. We find that a significant amount of data is filtered out, with the amount of data available 396 of $4.1 \mathrm{k}$ language pairs reducing by more than $40 \%$.

Deduplication. We deduplicate sentence pairs that are an exact match on both the source and target.

Virama Filtering and Correction/Zawgyi Encoded Data. We observed the same issues described in Section 2.5, and used the same filters for sentence pairs where either the source language or target language belonged to the list of languages in Section 2.5.

Unmatched Toxicity Filters. We use the unmatched toxicity filters described by NLLBTeam et al. [51], but ultimately unusable for our purposes in most cases. For the languages ace, am, ar, az, bg, bm, bn, bs, cs, din, en, es, fa, fr, ga, gl, ha, hi, id, it, kk, ko, ml, ms, my, nl, no, nus, prs, ru, scn, sd, so, sv, tg, th, tt, ur, uz and zh, more than $3 \%$ of documents were marked as having unmatched toxicity. On closer inspection, we found that while $\mathrm{zh}$ and ko had a lot of pornographic content that was removed by the filtering process, most other languages removed sentences that had homonyms of non-toxic words. Similarly, languages like id, ur, $\mathrm{tg}$, fa and no had data from Tanzil (Qur'an dataset), but the toxicity word lists contained words such as kafir, mercy and purity, that are not normally considered toxic content for our purpose of filtering the dataset using wordlists.

Source-Target Filters. We removed all sentences that have more than $75 \%$ overlap between the source and target side. To avoid filtering out valid entity translations, we only applied this filter on sentences longer than 5 tokens. In addition, we remove sentence pairs whose source length to target length ratio falls outside of $0.66-1.5$. We omitted this filter for the following, which are mainly non-whitespace languages: zh, ja, ko, km, my, lo, th, wuu, shn, zh_tw, zh_cn,iu, simple, dz, kr_Arab, din, nus and mi.

Script Filters. We removed all sentences that are less than $50 \%$ in-script for both the source and target language. For instance, if the sentence was supposed to be in kaa (Cyrillic script) but was $70 \%$ in the Latin script, we removed it.

### 3.2 Self-Audit (Quality Review)

Similar to the self-audit done for MADLAD-400, we conducted a review of the data sources that compose the parallel data we collected to verify the quality of this data. We collected 20 source-target pairs from each language, and assessed the data for the presence of offensive content, porn, and whether the data seemed to be of the correct language pair and whether the target sentence seemed to be a plausible translation. Since we did not have access to native speakers of all 157 languages, the latter was primarily based on guesses. In Appendix A. 5 we provide full details of the instructions we provided to auditors, the results of the self-audit and any changes made the dataset.

### 3.3 A Note on Language Codes

As observed by Kreutzer et al. [40], the datasets used to create the parallel data (and MADLAD-400) use a variety of different language codes. We use the BCP-47 standard, which specifies the 2-letter ISO-693-1 code when applicable, and otherwise the ISO-693-3 code. Script tags and region tags are omitted when they are defined as the default value by CLDR ${ }^{6}$, and otherwise included. For example, ks refers to Kashmiri in Nastaliq/Arabic script (CLDR default), whereas ks_Deva refers to Kashmiri in Devanagari. A detailed investigation of codes in MADLAD-400 can be found in Appendix A.3.

### 3.4 Multiway Data

We create additional multiway data by applying the $n$-gram matching method $(n=8)$ from Freitag and Firat [25] to the processed dataset. Using this, and the publicly available data, we obtain 11.9B sentences across a total of 20742 language pairs. Full details may be found in Appendix A.7.

## 4 Experiments

We validate our data by training encoder-decoder machine translation models in Section 4.1 and decoder-only language models in Section 4.2, and test them on several translation benchmarks.

### 4.1 MT Models

We train models of various sizes: a 3B, 32-layer parameter model, ${ }^{7}$ a 7.2B 48-layer parameter model and a 10.7B 32-layer parameter model. We share all parameters of the model across language pairs, and use a Sentence Piece Model [41] with 256k tokens shared on both the encoder and decoder side. Each input sentence has $\mathrm{a}\langle 2 \mathrm{xx}>$ token prepended to the source sentence to indicate the target language [35].

We use both supervised parallel data with a machine translation objective and the monolingual MADLAD-400 dataset with a MASS-style [62] objective to train this model. Each of these objectives is sampled with a $50 \%$ probability. Within each task, we use the recently introduced UniMax [18] sampling strategy to sample languages from our imbalanced dataset with a threshold of $N=10$ epochs for any particular language. We also explored back-translation by randomly sampling $2 \mathrm{M}$ monolingual samples (or the total number of samples for that given language) for each language and translating them to/from English using the 3B model. Following Bapna et al. [9] (\$3.5), we filter the back-translated data in a variety of ways. For a natural target and a back-translated source, we filter by round-trip ChrF to discourage hallucinations (threshold of 0.32), by ChrF between source and target to discourage copying (threshold of 0.30 ), by the length ratio of source to target (asymmetric bounds of $(0.45,1.6)$, and by LangID prediction of the source. We then finetune the $7.2 \mathrm{~B}$ model for a 10,000 steps by randomly mixing the original and the back-translated data with a combining ratio of 1:1. We list specific architecture and training details of these models in Appendix A.8.

### 4.2 Zero-shot Translation with Language Models

Given recent interest in the efficacy of unsupervised translation using large language models, we explore training language models solely on the monolingual data. We follow the same training schedule and model configurations from Garcia et al. [27]. In particular, we consider 8B decoderonly models, following the same model hyperparameters as previous work [17, 27]. We train these models using a variant of the UL2 objective [63] adapted for decoder-only models, and use the same configuration as previous work $[27,52]$. We provide additional details in Appendix A.8.[^3]

Table 4: Evaluation scores on WMT (depicted as <bleu> / <chrf>) for the MT models and language models described in Section 4.1 and Section 4.2 compared against NLLB-54B.

|  | NLLB | MT-3B | MT-7.2B | MT-10.7B | LM-8B |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  |  |  |  |  | 0 0-shot | 1-shot | 5 -shot | 10-shot |
| Xx2en | $34.2 / 60.4$ | $33.4 / 60.0$ | $34.9 / 60.6$ | $\mathbf{3 4 . 6 / 6 0 . 8}$ | $2.3 / 17.3$ | $25.1 / 51.4$ | $26.2 / 52.9$ | $26.2 / 53.4$ |
| en2xx | $\mathbf{3 1 . 1 / 5 8 . 0}$ | $28.2 / 55.4$ | $29.3 / 56.2$ | $29.0 / 56.2$ | $1.0 / 10.3$ | $18.7 / 43.5$ | $18.8 / 44.5$ | $19.3 / 45.5$ |
| Average | $\mathbf{3 2 . 7 / 5 9 . 2}$ | $30.8 / 57.7$ | $32.1 / 58.4$ | $31.8 / 58.5$ | $1.6 / 13.8$ | $21.9 / 47.4$ | $22.5 / 48.7$ | $22.8 / 49.4$ |

### 4.3 Evaluation

We use the sacreBLEU [55] implementation of bleu ${ }^{8}$ and chrf ${ }^{9}$ as metrics. We evaluate our trained models on the following datasets:

WMT. We use the 15 WMT languages frequently used to evaluate multilingual machine translation models by Siddhant et al. [61], Kim et al. [38], Kudugunta et al. [42], NLLBTeam et al. [51]: cs , de, es, fi, fr, gu, hi, kk, lv, lt, ro, rs, es, tr and zh.

Flores-200. We evaluate on the languages in the Flores-200 dataset [51] that overlap with the languages available in either MADLAD-400 or the parallel data described in Section 3. We list these languages in Appendix A.9. For non-English-centric pairs, we evaluate on a 272 language pair subset of the 40k language pairs possible due to computational constraints. We evaluate on all language pairs possible using the following languages as either source or target language: en $\mathrm{fr}, \mathrm{cs}, \mathrm{zh}, \mathrm{et}$, $\mathrm{mr}$, eu, cy, so, ckb, or, yo, ny, ti, ln, fon and ss. We obtained this set of languages by selecting every $10^{\text {th }}$ language by number of tokens in MADLAD-400 (clean), starting with French $(\mathrm{fr})$. Noticing that this had no Indian languages, we shifted af and fo (both close dialects of HRLS) down one index to $\mathrm{mr}$ and or, respectively. Finally, we noticed that this initial list had supervised and unsupervised languages, but didn't have a good representative of a "slightly supervised language", that is, one with a small but extant amount of parallel data. Therefore, we added yo to the list, which has the least parallel data of any supervised language. This resulting subset of languages also contains a nice variety of scripts: Latin, Chinese, Devanagari, Arabic, Odia, and Ethiopic scripts.

NTREX. We evaluate on the languages in the recently introduced NTREX dataset [23].

Gatones. Finally, we evaluate on the languages in GATONES, the in-house, 38-language eval set used in [9] and the GatitOS paper [36]. Again, we take the subset of languages overlapping with the languages available in either MADLAD- 400 or the parallel training data.

Table 5: Evaluation scores on Flores-200 (depicted as <bleu> / <chrf>) for the MT models and language models described in Section 4.1 and Section 4.2 compared against NLLB-54B. All metrics are computed with the sacrebleu reference implementation.

|  | NLLB | MT-3B | MT-7.2B | MT-10.7B | LM-8B |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | 0 -shot | 1-shot | 5-shot | 10-shot |
| $\mathbf{x x 2 e n}$ | $\mathbf{3 5 . 5 / 5 9 . 6}$ | $29.7 / 54.4$ | $30.9 / 55.4$ | $31.9 / 56.4$ | $2.0 / 13.3$ | $20.5 / 44.1$ | $22.3 / 46.9$ | $22.4 / 47.6$ |
| en2xx | $\mathbf{2 0 . 7 / 5 0 . 1}$ | $17.3 / 44.1$ | $17.8 / 44.7$ | $18.6 / 45.7$ | $0.4 / 5.7$ | $8.1 / 26.7$ | $8.7 / 29.0$ | $8.7 / 28.8$ |
| Mean | $\mathbf{2 8 . 2 / 5 4 . 9}$ | $23.5 / 49.2$ | $24.4 / 50.0$ | $25.3 / 51.1$ | $1.2 / 9.6$ | $14.3 / 35.5$ | $15.6 / 38.0$ | $15.6 / 38.2$ |
| xx2yy | $\mathbf{1 3 . 7 / 4 0 . 5}$ | $8.8 / 31.2$ | $8.4 / 30.9$ | $10.1 / 34.0$ | $0.3 / 4.1$ | $4.0 / 16.1$ | $4.4 / 17.3$ | $4.2 / 17.1$ |

### 4.3.1 Few-shot evaluation for language modeling

We perform few-shot prompting to evaluate the language model with the following prompt:

$[\mathrm{sl}]: X_{1} \backslash \mathrm{n}[\mathrm{tl}]: Y_{1} \backslash \mathrm{n} \backslash \mathrm{n}[\mathrm{sl}]: X_{2} \backslash \mathrm{n}[\mathrm{tl}]: Y_{2} \backslash \mathrm{n} \backslash \mathrm{n} . . .[\mathrm{sl}]: X \backslash \mathrm{n}[\mathrm{tl}]:$[^4]

Table 6: Evaluation scores on the recently introduced NTREX test set (depicted as <bleu> / <chrf>) for the MT models and language models described in Section 4.1 and Section 4.2 compared against unsupervised baselines [10]. Note that LM-8B is evaluated on a $50 \%$ split of the NTREX data and is not comparable to the MT-model evaluations.

|  | Baziotis et al. [10] | MT-3B | MT-7.2B | MT-10.7B | LM-8B |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | 0 -shot | 1-shot | 5-shot | 10 -shot |
| Results on the subset of Baziotis et al. [10] |  |  |  |  |  |  |  |  |
| $x \times 2 e n$ | $23.6 / 51.7$ | $34.3 / 59.9$ | $36.1 / \epsilon$ | 35.9 | $4.0 / 18.9$ | $23.4 / 48.8$ | $26.8 / 52.8$ | 27.6 / 53. |
| en |  | 2 | 22. |  | 1.0 | 15.2 | 16.5 | $15.9 / 42.3$ |
| Average | 19.8 / 51.7 | $28.3 / 55.1$ | $29.4 / 55.8$ | $29.4 / 56.1$ | $2.5 / 13.9$ | $19.3 / 44.5$ | $21.6 / 47.6$ | $21.8 / 48.0$ |
| Results on full test sets |  |  |  |  |  |  |  |  |
| $x x 2 e n$ | - | 5 | 32 . | 33 | 3.21 | $20.4 / 43.8$ | 23.8 | $24.4 / 49.1$ |
|  | - | 16.5 | 17. | 17.1 | $0.8 / 7.3$ | $11.7 / 31.2$ | $12.6 / 32.4$ | $12.3 / 32.3$ |
| Average | - | $23.5 / 47.0$ | $25.1 / 49.0$ | $25.7 / 49.7$ | $2.0 / 12.3$ | $16.0 / 37.4$ | $18.1 / 40.2$ | $18.3 / 40.6$ |

where [sl] and $[\mathrm{tl}]$ denote the source and target language name (expressed in English. For example, when translating a sentence from en to te, we use $[s l]=$ English and $[t l]=T e l u g u$ ), respectively. $X_{\star}$ and $Y_{\star}$ are demonstration examples used for prompting, and $X$ is the test input.

For each test example, we randomly sample demonstration examples, which is simple yet performs competitively with more complicated strategies $[66,72]$. In particular, we randomly select examples from the dev split of each dataset. Since NTREX does not have a dev split, we randomly sample 1000 examples as the dev set and use the rest for test evaluation.

### 4.4 Results

In Tables 4 and 6 we present evaluation scores on the WMT datasets and NTREX datasets, which are evaluation sets in the news domain. We find that both the 7.2B parameter model and the 10B parameter model is competitive with the significantly larger NLLB-54B model [51] on WMT. For the recent NTREX dataset, the only published results are small-scale results by Baziotis et al. [10].

In Table 5 we find that on Flores-200, our model is within 3.8 chrf of the 54B parameter NLLB model, while on xxyy pairs the 10.7B model is behind by 6.5 chrf. This is likely due to a combination of factors, including using a significantly smaller model (5x smaller), domain differences [10, 9], and back-translated data [60]. Similarly, in Table 7, we find that the 10.7B parameter model is within 5.7 chrf of the scores reported by Bapna et al. [9]. Again, it is very difficult to compare their results to ours; their two largest advantages are 1) iterative back-translation, and 2) access to a much larger in-house text data. In Table 8, we display the results for when we finetune the 7.2B parameter model on backtranslated data. While this setup is very likely sub-optimal, we see that back-translation greatly improves en $2 \mathbf{x x}$ translation (by 3.0 chrf, in the case of Flores-200) in most cases. We note that the results we present are merely baselines to demonstrate the utility of MADLAD-400, and hope that future work builds upon these experiments by applying improved modeling techniques.

Finally, across all evaluation datasets, we find that while results on few-shot translation using the 8B language model increase with an increasing number of demonstrations, these results are still significantly weaker than the results of models trained on supervised data. We present per-language pair results on all datasets in Appendix A.10.

Table 7: Evaluation scores on the GATONES test set used by Bapna et al. [9] (depicted as <bleu> / <chrf>) for the MT models and language models described in Section 4.1 and Section 4.2.

|  | NTL (Bapna et al. [9]) |  | MT-3B | MT-7.2B | MT-10.7B | LM-8B |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $1.6 \mathrm{~B}$ | $6.4 \mathrm{~B}$ |  |  |  | 0-shot | 1-shot | 5-shot | 10 -shot |
| xx2en | $-/ 37.2$ | $-/ 41.2$ | $13.3 / 34.6$ | 14.8 / 36.0 | $15.4 / 37.0$ | $0.3 / 6.5$ | $6.6 / 25.4$ | $8.3 / 28.1$ | $8.4 / 28.4$ |
| en $2 y$ | $-/ 28$ | $-/ 3$ |  |  |  | 0.21 | $1.7 / 10.5$ | $1.7 / 9.9$ | $1.8 / 9.4$ |
| Average | $-/ 32.9$ | $-/ 37.2$ | $8.9 / 29.3$ | $10.1 / 31.1$ | 10.4 / 31.8 | $0.3 / 5.4$ | $4.2 / 18.0$ | $5.0 / 19.0$ | $5.1 / 18.9$ |

Table 8: Evaluation scores on different test sets (depicted as <bleu> / <chrf>) for MT-7.2B trained with back-translated data (+BT).

|  | WMT |  | Flores-200 |  | NTREX |  | GATONES |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | MT-7.2B | $+\mathbf{B T}$ | MT-7.2B | $+\mathbf{B T}$ | MT-7.2B | $+\mathbf{B T}$ | MT-7.2B | $+\mathbf{B T}$ |
| $\mathrm{xx} 2 \mathrm{en}$ | 34.9 / 60.6 | 33.8 / 60.4 | 30.9 / 55.4 | 27.2 / 53.9 | 32.7 / 56.2 | $31.0 / \mathbf{5 6 . 5}$ | 14.8 / 36.0 | $10.2 / 34.5$ |
| en $2 x x$ | 29.3 / 56.2 | 29.8 / 56.9 | $17.8 / 44.7$ | 18.5 / 47.7 | 17.6 / 41.9 | 18.4 / 44.4 | $5.4 / 26.2$ | $3.5 / 26.1$ |
| average | $\mathbf{3 2 . 1} / 58.4$ | $31.8 / \mathbf{5 8 . 6}$ | $24.4 / 50.0$ | 22.8 / $\mathbf{5 0 . 8}$ | 25.1 / 49.0 | $24.7 / 50.4$ | $10.1 / 31.1$ | $6.9 / 30.3$ |
| $\mathbf{x x} 2 y \mathbf{y}$ | - | - | 8.4 / 30.9 | 8.4 / 31.9 | - | - | - | - |

## 5 Training Data Extraction and Memorization

Generative models have been shown to regurgitate training data [13] that may plagiarize, violate copyright assumptions, or infringe privacy. It can be difficult to assess and prevent these cases because such information may be paraphrased in ways that are difficult for automated systems to detect [32]. Instead, existing literature measures memorization in generative models to estimate the propensity for disallowed outputs. Typically, this means prompting a language model with some prefix of length $P$ and comparing generated outputs of length $S$ with the training data to see if they are 'novel' or if the generation is simply a regurgitation of its training data $[13,6,32,33,14]$. In the multilingual setting this may present new risks because tail languages may be more vulnerable to memorization [6].

The Difficulty of Assessing Memorization in Translation Settings. While memorization has been well-studied for language models, assessing the extent of memorization is difficult within translation settings. This is primarily because translation has a significantly smaller space of valid outputs, as opposed to many possible continuations for language modeling. This presents some difficulty in extending common memorization tests for language generation to translation. As an illustrative example, consider the case of translating to the same target language as the source ("translate_copy"). Performing a standard training data extraction attack would test if the generation matches the continuation. However, success would not indicate training data extraction as the adversary would have already had access to it. ${ }^{10}$ Thus, we modify the standard framework for testing memorization to better identify additional leaked data.

Memorization in Translation Settings We define memorization in translate_copy to be when the model outputs any generation with length $S>P$ that matches the continuation; then, $S-P$ captures the additional bits. In cases where the source and target language are different ("translate_diff"), performing a similar test would require knowledge of which part of the continuation exactly corresponded to the prompt. Given that such an alignment is not easily obtained, we instead use the relative token lengths between the continuation and the prompt to choose an appropriate size of $S$. For example, if at training time the continuation for the target language was $1.5 \times$ larger, we set $S=P \cdot 1.5+\delta$ where $\delta$ captures the additional bits. For each of translate_copy and translate_diff, we sample 2,000 sequences for each language and choose $P=50$. We then perform both a verbatim match of the generation with the continuation and an approximate match requiring $90 \%$ Levenshtein similarity similar to [32].

Results. We show the per-language and average training data extraction rates, for both the translate_copy and translate_diff settings in Figure 2, with $S$ set to test for 50 tokens of additional information leakage. We find that translate models can memorize and regurgitate their training data, even beyond what is contained in the prompt. We also observe that some lower resource languages may exhibit higher memorization rates, however we observe no strong correlation between the resource level and the level of memorization. In the translate_diff tests, we observe much lower memorization - we hypothesize this may be due to the higher difficulty of the task. Even though many languages have nontrivial memorization, we found that many languages exhibited no memorization across the samples tested (257/370 for translate_copy and 130/146 for translate_diff ). We also present results for approximate memorization in Appendix A.12, which show that translate models may also paraphrase memorizations leading to even higher memorization rates.

Discussion Our preliminary experiments show that memorization can exist in the translation setting. However, capturing when memorization is intended or beneficial versus undesired is still an open[^5]![](https://cdn.mathpix.com/cropped/2024_06_04_348bc135438c0574c483g-10.jpg?height=436&width=1266&top_left_y=243&top_left_x=424)

Figure 2: Monolingual (translate_copy) data used in translation is more likely to be memorized. Verbatim training data extraction rates for both translate_copy (left) and translate_diff (right) data. Extraction performed on the 3B parameter model using a $S=P+50$. In monoway, 257/370 languages exhibited no memorization in testing and 130/146 for multiway.

question. To aid future research in this direction, we design and include "canaries"-carefully crafted data designed to be outliers to the natural training distribution that can be used to analyze memorization. Canaries enable studying memorization in the multilingual and machine translation settings by measuring the capability to extract canaries added to the training set $[6,33]$. As with Anil et al. [6], our canaries are designed to share characteristics with the natural training data so as to better ground memorization evaluation in practical risks. The canaries are also designed tosl be outliers to assess varying degrees of risk. To ensure similarity with natural data, canaries are generated by sampling and then randomly modifying real data in a manner similar to [6], where each source of randomness defines the canary type. In total, we generate $1,945,631$ canaries across both the monolingual MADLAD -400 dataset and the parallel data ( $\approx 0.0026 \%$ of the training data). The methodology for each canary type and the exact distribution of canaries are detailed in Appendix A.11.

## 6 Related Work

Extensive work has been done to mine general purpose datasets for multilingual machine translation and language modeling. Xue et al. [68] introduce mC4, a general web domain corpus on 101 languages to train mT5, a pretrained language model for downstream NLP tasks. Similarly, Conneau et al. [19] introduce CC-100, later extended to CC100-XL by Lin et al. [47]. The OSCAR corpus [2] is also a mined dataset that supports 166 languages and the ROOTS corpus is a compiled dataset that contains 46 natural languages. Glot500-C [31] covers 511 languages: however, it is not clear how many of these languages comprise solely of religious texts. Bapna et al. [9] create an internal dataset on 1500+ languages, while NLLBTeam et al. [51] mine a dataset from CommonCrawl and ParaCrawl [22]. Recently, Leong et al. [45] created a 350+ language dataset from children's books.

In addition, there have been efforts to get better represented corpora and models for languages often underrepresented in general multilingual corpora: Serengeti [3] introduces a dataset and associated model trained on 517 African languages and language varieties, while IndicTrans2 [26] introduces a machine translated model for the 22 scheduled languages in India.

## 7 Limitations

While we used thorough self-audits to guide the creation of MADLAD-400, we note that most audits were conducted by non-speakers of the languages in MADLAD-400; as a result, many types of noise, like machine-generated or disfluent content, could not be detected. Moreover, toxicity detectors, classifiers and filters that work reliably for all the 419 languages in MADLAD-400 do not exist, limiting the extent to which we can clean and document $[21,8]$ the dataset. It is possible that issues still remain, so we encourage users to report issues that will be listed on the project Github page ${ }^{11}$. This paucity extends to the availability of multilingual evaluation sets for these languages we could only evaluate our models on 204 of the languages in MADLAD-400. Additionally, even[^6]though decoder-only models are often evaluated on NLP tasks that are not necessarily machine translation [30, 7, 5], we did not conduct such evaluations - most available benchmarks cover only 30-50 languages of which most are not tail languages (which forms the focus of MADLAD-400). We instead leave this to future work. Finally, during our self-audit we noted the skew of data on the long tail towards specific domains such as religious texts. We hope that these limitations motivate the creation of more language-specific corpora not captured by web crawls, and the development of language-specific data cleaning tools and practices.

## 8 Conclusion

Through MADLAD-400, we introduce a highly multilingual, general web-domain, document-level text dataset. We perform a self-audit of this dataset for quality on samples of all 498 languages, develop filters, and remove spurious datasets, for a total of 419 languages in the release. We carefully describe the dataset creation process, laying out the iterations of audits and improvements upon the preliminary dataset along with observations that guided our decisions. We hope that this encourages creators of large-scale pretraining datasets both to put in their due diligence for manually inspecting and dealing with data, and also to describe and publicize the process in a level of detail that is reproducible and insightful for downstream users. This increased visibility into the dataset creation cycle can in turn improve model development and enable responsible data use [58]. Using MADLAD400 , we train and release large machine translation and general NLP models and evaluate them thoroughly. We hope that this further motivates work towards language technologies that are more inclusive of the rich language diversity housed by humanity.

## 9 Ethics Statement

Innovation in NLP technologies in English has been accelerated by training large scale deep learning models [20, 12] on massive web corpora [16, 73, 57]. However, on the long tail of written languages in the world there is a lack of high quality general data sources [37] that impede the progress of NLP tools for other languages. We hope that making an audited and cleaned corpus such as MADLAD-400 available mitigates this issue. While we extensively cleaned MADLAD-400, the extent to which we can preprocess this data is limited by how not all languages have available tools for removing problematic content such as porn, toxic content, PII, copyrighted content or noise. We urge practitioners to carefully consider their target usecase before using MADLAD-400.

## Acknowledgements

We would like to thank Wolfgang Macherey, Zoubin Ghahramani and Orevaoghene Ahia for their helpful comments on the draft. We would also like to thank Subramanian Venkateswaran for debugging the virama rendering issues, and Ali Dabirmoghaddam for his insight on data samples of various languages in MADLAD- 400 .

## References

[1] StatMT. https://www.statmt.org/. Accessed: 2022-05-03.

[2] J. Abadji, P. O. Suarez, L. Romary, and B. Sagot. Towards a cleaner document-oriented multilingual crawled corpus. arXiv preprint arXiv:2201.06642, 2022.

[3] I. Adebara, A. Elmadany, M. Abdul-Mageed, and A. A. Inciarte. Serengeti: Massively multilingual language models for africa. arXiv preprint arXiv:2212.10785, 2022.

[4] Ž. Agic and I. Vulic. Jw300: A wide-coverage parallel corpus for low-resource languages. Association for Computational Linguistics, 2019.

[5] K. Ahuja, R. Hada, M. Ochieng, P. Jain, H. Diddee, S. Maina, T. Ganu, S. Segal, M. Axmed, K. Bali, et al. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528, 2023 .

[6] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

[7] A. Asai, S. Kudugunta, X. V. Yu, T. Blevins, H. Gonen, M. Reid, Y. Tsvetkov, S. Ruder, and H. Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.

[8] J. Bandy and N. Vincent. Addressing" documentation debt" in machine learning research: A retrospective datasheet for bookcorpus. arXiv preprint arXiv:2105.05241, 2021.

[9] A. Bapna, I. Caswell, J. Kreutzer, O. Firat, D. van Esch, A. Siddhant, M. Niu, P. Baljekar, X. Garcia, W. Macherey, T. Breiner, V. Axelrod, J. Riesa, Y. Cao, M. X. Chen, K. Macherey, M. Krikun, P. Wang, A. Gutkin, A. Shah, Y. Huang, Z. Chen, Y. Wu, and M. Hughes. Building Machine Translation Systems for the Next Thousand Languages. arXiv e-prints, art. arXiv:2205.03983, May 2022.

[10] C. Baziotis, B. Zhang, A. Birch, and B. Haddow. When does monolingual data help multilingual translation: The role of domain and model scale. arXiv preprint arXiv:2305.14124, 2023.

[11] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[13] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633-2650, 2021.

[14] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.

[15] I. Caswell, T. Breiner, D. van Esch, and A. Bapna. Language id in the wild: Unexpected challenges on the path to a thousand-language web text corpus, 2020. URL https://arxiv . org/abs/2010.14571.

[16] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.

[17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[18] H. W. Chung, N. Constant, X. Garcia, A. Roberts, Y. Tay, S. Narang, and O. Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023.

[19] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.

[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[21] J. Dodge, M. Sap, A. Marasović, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758, 2021.

[22] M. Esplà-Gomis, M. L. Forcada, G. Ramírez-Sánchez, and H. Hoang. Paracrawl: Web-scale parallel corpora for the languages of the eu. In Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks, pages 118-119, 2019.

[23] C. Federmann, T. Kocmi, and Y. Xin. NTREX-128 - news test references for MT evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pages 21-24, Online, Nov. 2022. Association for Computational Linguistics. URL https: //aclanthology.org/2022.sumeval-1.4.

[24] A. Fernando, S. Ranathunga, and G. Dias. Data augmentation and terminology integration for domain-specific sinhala-english-tamil statistical machine translation. arXiv preprint arXiv:2011.02821, 2020.

[25] M. Freitag and O. Firat. Complete multilingual neural machine translation. CoRR, abs/2010.10239, 2020. URL https://arxiv.org/abs/2010.10239.

[26] J. Gala, P. A. Chitale, R. AK, S. Doddapaneni, V. Gumma, A. Kumar, J. Nawale, A. Sujatha, R. Puduppully, V. Raghavan, et al. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. arXiv preprint arXiv:2305.16307, 2023.

[27] X. Garcia, Y. Bansal, C. Cherry, G. Foster, M. Krikun, F. Feng, M. Johnson, and O. Firat. The unreasonable effectiveness of few-shot learning for machine translation. arXiv preprint arXiv:2302.01398, 2023.

[28] H. J. Groenewald and W. Fourie. Introducing the autshumato integrated translation environment. In Proceedings of the 13th Annual conference of the European Association for Machine Translation, 2009.

[29] B. Haddow and F. Kirefu. Pmindia-a collection of parallel corpora of languages of india. arXiv preprint arXiv:2001.09907, 2020.

[30] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning, pages 4411-4421. PMLR, 2020.

[31] A. ImaniGooghari, P. Lin, A. H. Kargaran, S. Severini, M. J. Sabet, N. Kassner, C. Ma, H. Schmid, A. F. Martins, F. Yvon, et al. Glot500: Scaling multilingual corpora and language models to 500 languages. arXiv preprint arXiv:2305.12182, 2023.

[32] D. Ippolito, F. Tramèr, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. A. Choquette-Choo, and N. Carlini. Preventing verbatim memorization in language models gives a false sense of privacy. arXiv preprint arXiv:2210.17546, 2022.

[33] M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini, E. Wallace, S. Song, A. Thakurta, N. Papernot, et al. Measuring forgetting of memorized training examples. arXiv preprint arXiv:2207.00099, 2022.

[34] E. Joanis, R. Knowles, R. Kuhn, S. Larkin, P. Littell, C.-k. Lo, D. Stewart, and J. Micher. The nunavut hansard inuktitut-english parallel corpus 3.0 with preliminary machine translation results. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2562-2572, 2020.

[35] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Viégas, M. Wattenberg, G. Corrado, et al. Google's multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339-351, 2017.

[36] A. Jones, I. Caswell, I. Saxena, and O. Firat. Bilex rx: Lexical data augmentation for massively multilingual machine translation, 2023.

[37] P. Joshi, S. Santy, A. Budhiraja, K. Bali, and M. Choudhury. The state and fate of linguistic diversity and inclusion in the nlp world. arXiv preprint arXiv:2004.09095, 2020.

[38] Y. J. Kim, A. A. Awan, A. Muzio, A. F. C. Salinas, L. Lu, A. Hendy, S. Rajbhandari, Y. He, and H. H. Awadalla. Scalable and efficient moe training for multitask multilingual models. arXiv preprint arXiv:2109.10465, 2021.

[39] P. Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of machine translation summit $x$ : papers, pages 79-86, 2005.

[40] J. Kreutzer, I. Caswell, L. Wang, A. Wahab, D. van Esch, N. Ulzii-Orshikh, A. Tapo, N. Subramani, A. Sokolov, C. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot, C. Rivera, A. Rios, I. Papadimitriou, S. Osei, P. O. Suarez, I. Orife, K. Ogueji, A. N. Rubungo, T. Q. Nguyen, M. Müller, A. Müller, S. H. Muhammad, N. Muhammad, A. Mnyakeni, J. Mirzakhalov, T. Matangira, C. Leong, N. Lawson, S. Kudugunta, Y. Jernite, M. Jenny, O. Firat, B. F. P. Dossou, S. Dlamini, N. de Silva, S. Çabuk Ball, S. Biderman, A. Battisti, A. Baruwa, A. Bapna, P. Baljekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia, O. Ahia, S. Agrawal, and M. Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50-72, 2022. doi: 10.1162/tacl_a_00447. URL https://aclanthology.org/2022.tacl-1.4.

[41] T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.

[42] S. Kudugunta, Y. Huang, A. Bapna, M. Krikun, D. Lepikhin, M.-T. Luong, and O. Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3577-3599, Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.304. URL https://aclanthology.org/2021.findings-emnlp. 304.

[43] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35: $31809-31826,2022$.

[44] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.

[45] C. Leong, J. Nemecek, J. Mansdorfer, A. Filighera, A. Owodunni, and D. Whitenack. Bloom library: Multimodal datasets in 300+ languages for a variety of downstream tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8608-8621, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https: //aclanthology .org/2022.emnlp-main. 590.

[46] D. Liebling, K. Heller, S. Robertson, and W. Deng. Opportunities for human-centered evaluation of machine translation systems. In Findings of the Association for Computational Linguistics. NAACL 2022, pages 229-240, 2022.

[47] X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal, S. Bhosale, J. Du, et al. Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668, 2021.

[48] A. S. Luccioni and J. D. Viviano. What's in the box? a preliminary analysis of undesirable content in the common crawl corpus. arXiv preprint arXiv:2105.02732, 2021.

[49] T. Nakazawa, M. Yaguchi, K. Uchimoto, M. Utiyama, E. Sumita, S. Kurohashi, and H. Isahara. Aspec: Asian scientific paper excerpt corpus. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 2204-2208, 2016.

[50] G. Neubig. The Kyoto free translation task. http://www.phontron.com/kftt, 2011.

[51] NLLBTeam, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.

[52] G. Orlanski, K. Xiao, X. Garcia, J. Hui, J. Howland, J. Malmaud, J. Austin, R. Singh, and M. Catasta. Measuring the impact of programming language distribution. arXiv preprint arXiv:2302.01973, 2023.

[53] A. Paullada, I. D. Raji, E. M. Bender, E. Denton, and A. Hanna. Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns, 2(11):100336, 2021.

[54] J. Philip, V. P. Namboodiri, and C. Jawahar. A baseline neural machine translation system for indian languages. arXiv preprint arXiv:1907.12437, 2019.

[55] M. Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Brussels, Belgium, Oct. 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https : // aclanthology.org/W18-6319.

[56] R. Pryzant, Y. Chung, D. Jurafsky, and D. Britz. Jesc: Japanese-english subtitle corpus. arXiv preprint arXiv:1710.10639, 2017.

[57] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

[58] N. Sambasivan, S. Kapania, H. Highfill, D. Akrong, P. Paritosh, and L. M. Aroyo. "everyone wants to do the model work, not the data work": Data cascades in high-stakes ai. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-15, 2021.

[59] H. Schwenk, V. Chaudhary, S. Sun, H. Gong, and F. Guzmán. Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia. arXiv preprint arXiv:1907.05791, 2019.

[60] R. Sennrich, B. Haddow, and A. Birch. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany, Aug. 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL https://aclanthology.org/P16-1009.

[61] A. Siddhant, A. Bapna, O. Firat, Y. Cao, M. X. Chen, I. Caswell, and X. Garcia. Towards the next 1000 languages in multilingual machine translation: Exploring the synergy between supervised and self-supervised learning. CoRR, abs/2201.03110, 2022. URL https://arxiv . org/abs/2201.03110.

[62] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.

[63] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, H. S. Zheng, N. Houlsby, and D. Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022.

[64] J. Tiedemann. Parallel data, tools and interfaces in opus. In Lrec, volume 2012, pages 22142218. Citeseer, 2012.

[65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings .neurips. cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.

[66] D. Vilar, M. Freitag, C. Cherry, J. Luo, V. Ratnakar, and G. Foster. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102, 2022.

[67] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.

[68] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.

[69] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https://aclanthology.org/2021.naacl-main. 41.

[70] Q. Ye, S. Devendra, F. Matthieu, P. Sarguna, and N. Graham. When and why are pre-trained word embeddings useful for neural machine translation. In HLT-NAACL, 2018.

[71] B. Zhang, P. Williams, I. Titov, and R. Sennrich. Improving massively multilingual neural machine translation and zero-shot translation. arXiv preprint arXiv:2004.11867, 2020.

[72] B. Zhang, B. Haddow, and A. Birch. Prompting large language model for machine translation: A case study. arXiv preprint arXiv:2301.07069, 2023.

[73] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.

[74] M. Ziemski, M. Junczys-Dowmunt, and B. Pouliquen. The united nations parallel corpus v1. 0 . In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3530-3534, 2016.
