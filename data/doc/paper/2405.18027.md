# TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models 

Jaewoo Ahn ${ }^{1}$ Taehyun Lee ${ }^{1} \quad$ Junyoung Lim ${ }^{1}$<br>Jin-Hwa Kim ${ }^{1,2}$ Sangdoo Yun ${ }^{1,2}$ Hwaran Lee ${ }^{2}$ Gunhee Kim ${ }^{1}$<br>${ }^{1}$ Seoul National University ${ }^{2}$ NAVER AI Lab<br>\{jaewoo.ahn, taehyun.lee\}@vision.snu.ac.kr, icarus001104@snu.ac.kr<br>\{j1nhwa.kim, sangdoo.yun, hwaran.lee\}@navercorp.com, gunhee@snu.ac.kr


#### Abstract

While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TIMECHARA, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-ofthe-art LLMs (e.g., GPT-4o). To counter this challenge, we propose NARRATIVE-EXPERTS, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-intime character hallucinations effectively. Still, our findings with TimeCHARA highlight the ongoing challenges of point-in-time character hallucination, calling for further study. ${ }^{1}$


## 1 Introduction

The recent progress in large language models (LLMs) has opened up a new phase of generative agents (Park et al., 2023; Xi et al., 2023; Wang et al., 2024a), where LLMs simulate human-like behaviors, memories, and cognitive processes. A particularly promising area is the development of role-playing LLM agents (Shanahan et al., 2023; Kong et al., 2023; Li et al., 2023b), which simulate the personas of either real individuals or fictional characters and engage with users to provide a more vivid experience. A variety of applications, including Character AI, GPTs, Talkie, Replika, AI[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_5496fd22fb4a2192ef9eg-01.jpg?height=137&width=480&top_left_y=754&top_left_x=1051)

[System Prompt] Act like Harry Potter at 37 years old

Harry Potter (Al)

"I'm 37 years old and working at the Ministry of Magic."

"I heard that you are married with Ginny Weasley!"

Harry Potter (Al)

"Uhm, yeah- I'm married to Ginny, and she's my wife."

[Consistent]

[System Prompt] Act like Harry Potter in his $5^{\text {th }}$ year

Harry Potter (AI)

"I'm in my $5^{\text {th }}$ year at Hogwarts."

"I heard that you are married with Ginny Weasley!" Harry Potter (AI)

"Uhm, yeah- l'm married to Ginny, and she's my wife."

[Inconsistent]

Figure 1: An illustrative figure of point-in-time character hallucination demonstrated by a role-playing agent simulating Harry Potter. (Top) The agent, simulating Harry Potter at 37 years old, consistently responds to the user's queries. (Bottom) The agent, simulating Harry Potter in his fifth year at Hogwarts, erroneously mentions a future event - his marriage to Ginny Weasley — which occurs after his fifth year.

Dungeon, SillyTavern, showcase the growing popularity of these role-playing LLM agents. However, most current approaches of role-playing agents (Han et al., 2022; Li et al., 2023a; Zhou et al., 2023) only simulate characters who are omniscient in timeline; for example, a Harry Potter character who is aware of all events leading up to the end of their respective series.

We suggest the importance of situating characters at a particular moment in the narrative progression. We coin this as point-in-time role-playing, en-
compassing three key rationales: narrative immersion, avoidance of spoilers, and engagement in fandom role-playing. Firstly, while a fully-informed character can interact with users drawing from their entire history, a character in the middle of the story inspires narrative immersion (Ryan, 2003, 2008). It sparks the user's curiosity about forthcoming events and deepens their emotional bond with the character, who remains unaware of their eventual fate. Secondly, this approach can avoid spoilers. Consider a media franchise such as Harry Potter, where all books are published, but upcoming adaptations (e.g., "Harry Potter TV series"2) are awaited. Users who wish to avoid spoilers before watching the new TV series would prefer interacting with a character from a midpoint in their story, thereby avoiding knowledge of future events. Thirdly, this approach can support recently popular fandom roleplaying ${ }^{3}$, a blend of fan fiction and traditional roleplaying games. Individuals adopt the personas of their favorite characters at specific points in their stories and craft new narratives or engage with fellow enthusiasts in this creative endeavor.

To accurately represent characters at specific time points, the agents should recognize the character's knowledge boundary. This includes their unawareness of future events, their ability to recall past events precisely, and their understanding of the individuals involved in those past events. However, current LLM-based role-playing agents are prone to character hallucination (Shao et al., 2023), displaying knowledge that contradicts their character's identity and historical context (e.g., Julius Caesar talking about his favorite movie). Despite the seriousness, the problem has not been investigated in terms of maintaining character consistency, especially in relation to their historical timelines, and robustness to such hallucinations.

We introduce a new point-in-Time Character hallucination benchmark, TIMECHARA, to rigorously assess role-playing LLMs at specific time points, thereby evaluating the agents' spatiotemporal self-consistency and their ability to avoid character hallucination. Figure 1 exemplifies the point-in-time character hallucination, where Harry Potter, in his fifth year at Hogwarts, inappropriately mentions a future fact about his wife, Ginny Weasley. We select 14 fictional characters from four popular novel series and develop a pipeline to[^1]

generate interview questions tailored to each character at a specific point in their story, along with spatiotemporal labels to determine the spatiotemporal consistency of their responses. Table 1 outlines comparison with existing related benchmarks.

Our empirical experiments reveal a significant issue of point-in-time character hallucination in state-of-the-art LLMs, including GPT-4o (OpenAI, 2024) and GPT-4 (OpenAI, 2023). This suggests that despite LLMs memorizing extensive knowledge from books (Chang et al., 2023), they still struggle with maintaining spatiotemporal consistency during role-playing scenarios. To mitigate this, we propose a decomposed reasoning method, NARRATIVE-EXPERTS, which partitions reasoning tasks among narrative experts specialized in temporal (i.e., identifying between past and future events) and spatial domains (i.e., discerning whether a character was present or absent in specific past events). Experiments show that NARRATIVE-EXPERTS significantly reduces point-in-time character hallucination and enhances spatiotemporal consistency. Still, our TimeChara underscores the ongoing challenge of point-in-time character hallucination and highlights the potential for future improvements.

Our main contributions are as follows:

1. We introduce TimeChara, a novel benchmark for evaluating character hallucination in point-in-time role-playing agents. We also develop an automated pipeline to construct the dataset, comprising 10,895 instances in total.
2. Through TimeChara, we identify significant hallucination issues within state-of-theart role-playing LLMs including GPT-4o.
3. We propose NARRATIVE-EXPERTS, a simple but effective method to mitigate point-in-time hallucination by decomposing reasoning with each step led by the narrative expert.

## 2 Related Work

We include a more thorough literature review in Appendix A. In this section, we only discuss the most relevant works.

Role-playing LLM agents. Prior research on conversational AI has focused on developing dialogue agents with self-consistent personas (Zhang et al., 2018; Kim et al., 2020; Ahn et al., 2023). Furthermore, LLMs are increasingly being used to simulate human behavior (Park et al., 2023). Many
of these efforts involve using LLMs to role-play specific characters, such as Harry Potter, Socrates, and others (Shanahan et al., 2023; Wang et al., 2023a,b). In this context, Shao et al. (2023) introduced character hallucination, a scenario where a role-playing agent inappropriately exhibits knowledge that is inconsistent with the character's identity and historical background. On the other hand, Chen et al. (2023) proposed a point-in-time "Harry Potter" role-playing dialogue dataset. They focused on assessing whether a role-playing agent responds naturally to the character within a specific point in a storyline. Rather than directly stress-testing role-playing LLMs at specific time points by asking confusing questions, their relevance metrics are designed to gauge overall character alignment given natural scene and dialogue context. However, existing studies on role-playing agents have not extensively examined how well these agents are robust to point-in-time character hallucination. We aim to stress-test role-playing LLMs at specific time points by assessing their spatiotemporal self-consistency and robustness against pointin-time character hallucination, as detailed in Table 1. Additionally, we compare concurrent work to TimeChara in Table 6 in Appendix A.

LLM's temporal reasoning capability. Understanding the concept of time is crucial for LLMs, as the information they acquire is often timesensitive (Chen et al., 2021; Zhang and Choi, 2021; Dhingra et al., 2022; Chu et al., 2023). To assess LLMs' temporal reasoning capabilities, several studies have set benchmarks. Jang et al. (2022) examined how well LLMs adapt to frequentlyupdated knowledge corpus. Feng et al. (2023) focused on whether LLMs can interpret the impact of subtle contextual changes on relevant temporal relationships. Tan et al. (2023b) developed TimelineQA, a dataset for querying the lifelogs of imaginary people. While these benchmarks evaluate the temporal reasoning capabilities of LLMs, we extend them to point-in-time role-playing scenarios by evaluating whether role-playing LLMs maintain the character's spatiotemporal consistency.

## 3 The TimeChara Benchmark

To create TIMECHARA, we select four renowned novel series: Harry Potter, The Lord of the Rings, Twilight, and The Hunger Games. This choice is based on two main reasons: (i) the ease of gathering raw text content (i.e., transcripts) and personality information for each character, useful for dataset construction, and (ii) the fact that recent state-ofthe-art LLMs store knowledge of these series well in their parametric memories (Chang et al., 2023), facilitating tests for point-in-time character hallucination. Note that TimeChara is not exclusively limited to these series; it is easily extendable to other narratives, provided that raw text content and personality information for the characters can be obtained. Then, we identify 14 main characters across the four novel series, detailed in Appendix F. We pinpoint a particular moment in each character's timeline (e.g., Hermione on Christmas during her first year at Hogwarts) rather than assuming they are aware of all events up to the end of the series as in previous studies (Tan et al., 2023b; Wang et al., 2023b).

We organize our dataset in an interview format where an interviewer poses questions and the character responds. Specifically, we differentiate between fact-based and fake-based interviews.

### 3.1 Fact-Based Interview

To evaluate point-in-time character hallucination, we categorize the data into four types as follows:

The unawareness of the future (i.e., future type): The character at the chosen time point should not know about future events (e.g., "Who is your wife?" to first-year Harry).

The memorization of the past: The character should accurately recall past events. Since episodic events occur at specific locations or scenes, the questions are further categorized as follows. The awareness of the absence (i.e., past-absence type): The character recognizes they are not in an event (e.g., "Did you see the moment when Harry received the Invisibility Cloak on Christmas?" to first-year Hermione on Christmas). The awareness of the past (i.e., past-presence type): The character acknowledges they are in an event (e.g., "Did you see the moment when Harry received the Invisibility Cloak on Christmas?" to first-year Ron on Christmas). The awareness of the past, irrelevant of participation (i.e., past-only type): Questions in this type focus on gauging the character's overall knowledge of past events, including relationships between characters or the significance of magical items (e.g., "Who is Dobby?" to second-year Harry on Halloween). The term "only" suggests that these questions primarily assess the character's understanding and memory of past information, not exclusively tied to their direct

| Evaluation <br> Dataset / <br> Benchmark | Dataset <br> automatically <br> constructed? | Support <br> point-in-time <br> role-playing? | Evaluate <br> near-future <br> unawareness? <br> (Temporal) | Evaluate <br> absence <br> awareness? <br> (Spatial) | Evaluate <br> fake event <br> awareness? <br> (Fake question) | Evaluation method <br> for character <br> hallucination |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |

Temporal Reasoning Domain

| TemporalWiki | $\checkmark$ | $x$ | - | - | - | - |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| TODAY | $\Delta$ | $x$ | - | - | - | - |
| TempReason | $\checkmark$ | $x$ | - | - | - | - |
| TempTabQA | $x$ | $x$ | - | - | - | - |
| TimelineQA | $\checkmark$ | $x$ | - | - | - | - |
| Role-Playing Domain |  |  |  |  |  |  |
| LIGHT | $x$ | $x$ | $x$ | $x$ | $x$ | F1 w/ gold response: <br> $[0-1]$ (implicit) |
| RoleBench | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | Rouge-L w/ gold response: <br> $[0-1]$ (implicit) |
| CharacterDial | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | Human as judges: <br> $[1$ to 5$]$ (unscalable) |
| HPD | $x$ | $\checkmark$ | $x$ | $\checkmark^{*}$ | $x$ | LLM as judges w/ <br> speaker attribute \& relation labels: <br> [top-1 ranking $]$ (implicit) |
| Character-LLM | $\checkmark$ | $x$ | $\boldsymbol{X}$ (Question from <br> distinct era/narrative: <br> easy) | $x$ | $\boldsymbol{X}$ (Only in <br> training set) | LLM as judges w/o <br> spatiotemporal labels: <br> $[1$ to 7$]$ (inaccurate) |
| TiMeCHARA | $\checkmark$ | $\checkmark$ | $\boldsymbol{\checkmark}$ (Question from <br> the same era/narrative: <br> hard) | $\checkmark$ | $\checkmark$ | LLM as judges w/ <br> spatiotemporal labels: <br> $[0$ or 1$]$ (accurate) |

Table 1: Comparison of TimeChara with other datasets or benchmarks: TemporalWiki (Jang et al., 2022), TODAY (Feng et al., 2023), TempReason (Tan et al., 2023a), TempTabQA (Gupta et al., 2023), TimelineQA (Tan et al., 2023b), LIGHT (Urbanek et al., 2019), RoleBench (Wang et al., 2023b), CharacterDial (Zhou et al., 2023), HPD (Chen et al., 2023), and Character-LLM (Shao et al., 2023). $\Delta$ indicates that TODAY used both LLM (i.e., GPT-3.5) and human annotations for dataset construction. ' - ' denotes that the criteria are not applicable (i.e., only applicable to role-playing benchmarks), while ' $\boldsymbol{X}$ ' denotes a 'No' response to the given criteria. *HPD has only a single instance intended to evaluate absence awareness among its 149 test set instances, as shown in Table 4. In the last column, 'accurate' means TimeChara uses spatiotemporal labels provided to the LLM judge to measure hallucinations. 'Inaccurate' indicates Character-LLM evaluates hallucinations without spatiotemporal labels, relying on parametric memory. 'Implicit' means that the evaluation measures hallucinations indirectly via lexical similarity with the gold response or relevance to character attributes and relation labels instead of directly identifying hallucinations in the generated response. 'Unscalable' means that human evaluation requires manual annotations, making it less scalable than other automatic methods.

| Scene | "Why can't we get through?" Harry hissed to Ron... "I think we'd better go and wait by the car," said Harry... |
| :--- | :--- |
| Event Summary | Harry and Ron took the enchanted car to Hogwarts after a barrier mishap at King's Cross. |
| Question | "Tell me your feelings when \{Event Summary \}." |
| Character | 1st-year Harry Potter at the end of the scene |
| Data Type | Future |
| Spatiotemporal Label | Future: At the end of the scene of Harry Potter and the Philosopher's Stone as a 1st-year student, <br> Harry Potter should (1) not be aware of or (2) contain any expression that reveals the moment when \{Event Summary \}. |
| Personality Label | Harry Potter is characterized by his selflessness and immense loyalty, especially towards his friends... |
| Gold Response | "Oh, I don't really know what you're talking about. Ron and I haven't tried to go through the barrier..." |

Table 2: An example of our future type data instance with the fact-based structured question.

experiences or observations.

Table 2 shows an example of a fact-based interview. TimeCHARA assesses point-in-time character hallucination using questions derived from the same narrative. This contrasts with Shao et al. (2023), who (1) use questions that span different time or narratives and (2) do not support point-in-time role-playing (e.g., "Can you write

Python codes?" to Beethoven), as marked in Table 1. Hence, our interviews demand detailed narrative understanding, making hallucination detection more challenging.

### 3.2 Fake-Based Interview

In addition to the fact-based interview, which tests whether questions about real events are answered correctly, we introduce the fake-based interview. It is designed to evaluate if role-playing agents can identify and rectify the errors in interview questions by partially altering fact-based questions. Fake-based interviews are concentrated on pastonly type questions (e.g., "How did you become Slytherin?" to first-year Harry on September 1st: The correct answer is that he became Gryffindor). We exclude future type questions since correcting misinformation about unknown future events is not possible. Similarly, we exclude past-presence and past-absence type questions because verifying or refuting a character's event participation in nonexistent past events is ambiguous.

### 3.3 Evaluation on TiMECHARA

Since it is not scalable to manually evaluate the roleplaying LLMs' responses to interview questions, as done in Shao et al. (2023); Zheng et al. (2023), we adopt the LLM-as-judges approach to assess along two dimensions:

Spatiotemporal consistency for assessing pointin-time character hallucination: The model should accurately recall the character's past experiences. This includes the character's unawareness of future events and awareness of presence or absence in past events, as described in $\S 3.1$. This metric is time-dependent; the model should only exhibit the knowledge that the character possesses up to the specific time point.

Personality consistency: The model should emulate the character's personality, including their manner of thinking, speaking styles, tones, emotional responses, and reactions. This encompasses the character's preferences, values, and convictions. This metric is time-independent; the response should consistently reflect the character's enduring personal traits.

Step-by-step evaluation with spatiotemporal labels. Following Wei et al. (2022), we instruct the GPT-4 Turbo (gpt-4-1106-preview) model (OpenAI, 2023) to step-by-step score the performance in each dimension. For specific examples of prompts

![](https://cdn.mathpix.com/cropped/2024_06_04_5496fd22fb4a2192ef9eg-05.jpg?height=434&width=780&top_left_y=228&top_left_x=1049)

Figure 2: Evaluation accuracy of LLM judges for spatiotemporal consistency. Judges with spatiotemporal labels show superior performance compared to those without in both GPT-4/3.5. We randomly select 300 data instances containing responses generated by GPT-4 Turbo (see Table 5) and manually annotate them with binary labels to indicate whether spatiotemporal consistency holds or not. We compare the relative evaluation accuracy of LLM judges with humans (marked by 100). 'Total' denotes the average score across all cases.

used in this process, refer to Appendix B. Unlike Shao et al. (2023), our evaluation of point-in-time character hallucination (or spatiotemporal consistency) provides judges with precise spatiotemporal labels, which encompass the character's experiences with people, events, and objects. As shown in Figure 2, the labels enable a much more accurate evaluation of response consistency with the character's known history (e.g., During his first year on Christmas, Harry can respond based on the moment but should not wrongly recall it: \{moment description $\}$. Please refer to Table 10 in Appendix C for details of this evaluation of past memorization). The responses with contradiction or inconsistency regarding the spatiotemporal labels are scored as 0 ; otherwise, those in alignment are rated as 1 . We will describe the details of how to construct these spatiotemporal labels in $\S 3.4$.

For evaluating personality consistency, we adopt a methodology similar to Shao et al. (2023) but enhance it by sourcing more detailed personality traits from the Fandom page ${ }^{4}$. We then rate these traits on a 1-7 Likert scale to measure how closely a response aligns with a character's personality, where 1 signifies a weak reflection, and 7 indicates an exact match.

### 3.4 Dataset Construction

To create the TimeCHara benchmark, we propose a new automated pipeline that easily scales up the[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_5496fd22fb4a2192ef9eg-06.jpg?height=551&width=1585&top_left_y=233&top_left_x=241)

Figure 3: An illustration of our automated pipeline for constructing TimeChara. See Table 2 and Appendix C for examples of the complete dataset.

dataset while reducing the need for manual human annotation, as depicted in Figure 3.

Extract scenes, event summaries, and participants lists from books. The first step is to extract specific scenes from literary works using GPT-4 Turbo. We instruct it to extract $N$ distinct scenes containing multi-turn dialogues among characters, as detailed in Appendix D. 1 (see Table 13). For every extracted scene, we instruct GPT-4 to generate (1) a concise, single-sentence summary (i.e., event summary) of scene information and (2) a list of the participants involved in that scene, as shown in Appendix D. 1 (see Table 14).

Generate questions from event summary. Initially, we generate fact-based questions based on two different methods. We begin by creating factbased structured questions about characters' involvement in events by combining question templates with the event summary. To this end, we curate 18 different question templates like "Tell me your feelings when \{event summary\}." Appendix D. 2 shows all 18 question templates. Following this, we create fact-based free-form questions that assess a character's understanding of the event, regardless of their direct participation, as detailed in Appendix D.2. Subsequently, we proceed to formulate fake-based questions, employing a methodology similar to the creation of fact-based free-form questions, with further details available in Appendix D.2.

## Assign spatiotemporal labels to each charac-

ter. Given a question for a specific scene and event, the goal is to create a combination of \{scene, event summary, question, character with their time point $\}$. By choosing the character and its time point, the data type is automatically classified into one of

| Question <br> generation <br> method | Fact-based |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | \# Future | \# Past- <br> absence | \# Past- <br> presence | \# Past- <br> only | Fake-based <br> \# Past- <br> only |
| Harry Potter Series |  |  |  |  |  |
| Fact \& structured | 892 | 745 | 1,991 | - | - |
| Fact \& free-form | 765 | - | - | 784 | - |
| Fake \& free-form | - | - | - | - | 711 |
| The Lord of the Rings Series |  |  |  |  |  |
| Fact \& structured | 252 | 555 | 725 | - | - |
| Fact \& free-form | 224 | - | - | 228 | - |
| Fake \& free-form | - | - | - | - | 203 |
| Twilight Series |  |  |  |  |  |
| Fact \& structured | 221 | 277 | 395 | - | - |
| Fact \& free-form | 176 | - | - | 179 | - |
| Fake \& free-form | - | - | - | - | 170 |
| The Hunger Games Series |  |  |  |  |  |
| Fact \& structured | 212 | 309 | 348 | - | - |
| Fact \& free-form | 181 | - | - | 188 | - |
| Fake \& free-form | - | - | - | - | 164 |
| Sum |  |  | 10,895 |  |  |

Table 3: Data statistics of four series in TIMECHARA.

four types: future, past-absence, past-presence, past-only. These data types serve as spatiotemporal labels for the \{scene, event summary, question, character with their time point $\}$ combination. Appendix D. 3 details how to select the character and assign its time point for each type.

Add detailed descriptions to the spatiotemporal labels. Based on the \{scene, event summary, question, character with their time point \} combination and the predefined data type (i.e., spatiotemporal label), we add detailed descriptions to the spatiotemporal labels to serve as a basis for evaluating role-playing agent responses. Refer to Appendix D. 4 for details.

Generate gold responses and manually filter data instances. To generate a gold response for each data instance, we prompt GPT-4 with the com-

| \# Spatiotemporal label | HPD | Character-LLM | TIMECHARA |
| :--- | :---: | :---: | :---: |
| \# Future | $0(0.0 \%)$ | $57(4.4 \%)$ | $2,923(26.8 \%)$ |
| \# Past-absence | $1(0.7 \%)$ | $0(0.0 \%)$ | $1,886(17.3 \%)$ |
| \# Past-presence | $1(0.7 \%)$ | $0(0.0 \%)$ | $3,459(31.7 \%)$ |
| \# Past-only (Fact) | $20(13.4 \%)$ | $856(65.5 \%)$ | $1,379(12.7 \%)$ |
| \# Past-only (Fake) | $0(0.0 \%)$ | $0(0.0 \%)$ | $1,248(11.5 \%)$ |
| \# None | $127(85.2 \%)$ | $394(30.1 \%)$ | $0(0.0 \%)$ |
| \# Total | 149 | 1,307 | 10,895 |

Table 4: Comparison of test data statistics from three benchmarks. 'None' denotes that the instance is not included in any pre-defined labels (e.g., next response generation in a plain conversation between characters) Note that the questions for 57 future type instances in Character-LLM are from different eras or narratives compared to the character, while future type questions in TIMECHARA are from the same eras or narratives.

bination of \{question, character with their time point, spatiotemporal label $\}$. At last, we manually filter by the authors, whose criteria and results are shown in Appendix D.5.

### 3.5 Dataset Analyses

Statistics. The total number of event summaries is 1,643: 914 for Harry Potter, 261 for The Lord of the Rings, 245 for Twilight, and 223 for The Hunger Games. As a result, the dataset contains 10,895 instances, and Table 3 provides detailed statistics. The average lengths of (questions, gold responses, spatiotemporal labels) are (29.2, 117.6, 543.2) words, respectively.

In addition, we manually reviewed the test datasets from three different benchmarks (i.e., HPD, Character-LLM, and TimeCHARA) and classified them based on the spatiotemporal label of the given question or previous utterances in the case of multi-turn dialogue, without considering the agent's response for simplicity, as detailed in Table 4. While TimeChara consists of data instances evenly distributed over spatiotemporal labels, examples in HPD and Character-LLM are mainly classified as Past-only (Fact) or None types. This result demonstrates that TimeCHARA focuses on stress-testing the spatiotemporal consistency of role-playing LLMs, while the others focus on assessing the fact-based question-answering task or plain conversation between characters.

Furthermore, we provide the lexical diversity of free-form questions and compare it to the structured questions in Appendix E.1. Finally, we present a detailed human evaluation process to ensure the quality of TIMECHARA, as described in Appendix E.2.

## 4 Decomposed Reasoning

We find that existing LLMs struggle with spatiotemporal consistency as in Table 5, despite their extensive knowledge from books (Chang et al., 2023). To overcome this issue, we propose a reasoning method named NARRATIVE-EXPERTS, which decomposes reasoning steps into specialized tasks, employing narrative experts on either temporal (i.e., distinguishing past from future events) or spatial (i.e., identifying characters' presence in past events) aspects while utilizing the same backbone LLM.

Temporal Expert: This expert pinpoints the scene's book and chapter from a question, assigning a future or past label. If deemed future, it bypasses the Spatial Expert and advises the roleplaying agent with a specific hint (i.e., "Note that the period of the question is in the future relative to \{character\}'s time point. Therefore, you should not answer the question or mention any facts that occurred after \{character\}'s time point.").

Spatial Expert: It assesses whether a character is involved in the scene, indicating a past-absent label if applicable. A tailored hint is then provided to the role-playing agent if the scene is past-absent (i.e., "Note that \{character\} had not participated in the scene described in the question. Therefore, you should not imply that \{character\} was present in the scene.").

Finally, the role-playing LLM incorporates hints from these experts into the prompt and generates a response. Appendix G.5.1 offers details of the algorithm and the prompts designed for experts.

## 5 Experiments on TIMECHARA

### 5.1 Dataset Sampling for Evaluation

Due to the high computational cost of employing GPT-4 judges, fully evaluating the $11 \mathrm{~K}$ instance dataset is challenging. Instead, we randomly sample 600 data instances to assess the point-intime character hallucination of role-playing agents. First, we sample 300 instances with fact-based structured questions, evenly distributed across three data types: future (100 instances), pastpresence (100 instances), and past-absence (100 instances). Then, we pick 200 instances with factbased free-form questions, with an equal split of 100 instances each from future and past-only types. Lastly, we choose 100 instances with fake-based free-form questions, all from the past-only type.

In addition, we provide experimental results for the entire $11 \mathrm{~K}$ dataset in Appendix G.6.

### 5.2 Baseline Methods

We focus on inference-based agents as opposed to training-based agents (Shao et al., 2023), due to the impracticality of training agents to simulate characters across diverse time points; notably, our dataset includes 219 time points. We utilize four different state-of-the-art LLMs as a backbone model for role-playing agents to respond to our dataset: GPT-4o (i.e., gpt-4o-2024-05-13), GPT-4 (i.e., gpt-4-1106-preview), GPT-3.5 (i.e., gpt-3.5-turbo-1106), and Mistral 7B Instruct (i.e., mistral-7b-instruct-v0.2) (Jiang et al., 2023). To test their various reasoning capabilities, we employ several baselines as follows.

Zero-shot prompt. This is to directly prompt an agent to generate a response based on the system instruction and a question as follows:

## Zero-Shot Prompt Template

## System Instruction:

I want you to act like \{character\} from \{author\}'s \{series_name\} novel series. I want you to respond and answer like \{character\}, using the tone, manner, and vocabulary \{character\} would use. Assume that you are on \{time_point\} in \{book_name\} and interviewing with the interviewer. You should not answer the question and mention any fact that is future to the period. If he (or she) was not present at the location where the question was raised, he (or she) is likely unaware of the information or knowledge related to that question.

User Prompt:

\{question\}

Note that we instruct the agent to be unaware of future events and to acknowledge the absence when responding to past-absence type questions.

Zero-shot-CoT prompt. (Kojima et al., 2022). This method exploits a zero-shot prompt by adding the phrase "Let's think step by step" at the end of the question. This addition aims to improve the step-by-step reasoning capability of LLMs.

Few-shot prompt (in-context learning). This approach provides LLMs with four instances (4shot), with details on how examples were selected available in Appendix G.2.

Iterative self-correction. Recent studies (Pan et al., 2023; Shinn et al., 2023) found that LLMs have the capability for self-correction, iteratively refining their initial responses based on the given criteria. Among various methods, we choose the selfrefine (Madaan et al., 2023), since it is adaptable to dialogue domains with multiple evaluation criteria. Further details can be found in Appendix G.3.
Retrieval-augmented generation (RAG). In some prior research, retrieval-augmented generation (Lewis et al., 2020) can mitigate hallucinations (Shuster et al., 2021). We develop a retrieval module that employs OpenAI's embedding (i.e., text-embedding-ada-002) to provide contexts to LLMs. In addition, we add a variant named RAGcutoff, which is designed to limit its retrieval exclusively to the events prior to a defined character period. Thanks to this constraint, agents can avoid access to future contexts. Further details are available in Appendix G. 4 .

### 5.3 Decomposed Reasoning with RAG

Beyond NARRATIVE-EXPERTS, we also explore NARRATIVE-EXPERTS-RAG-CUTOFF, which integrates NARRATIVE-EXPERTS with the RAG-cutoff method. We provide a complete algorithm and prompts used for experts in Appendix G.5.2.

### 5.4 Experimental Results

See Appendix G. 1 for implementation details. Table 5 finds even GPT-4o and GPT-4, state-of-the-art LLMs, still struggle with point-in-time character hallucinations.

Future type. All baselines exhibit confusion with future type questions with accuracies at $51 \%$ or below. It highlights a prevailing issue of role-playing agents that inadvertently disclose future events. The naive RAG scores the lowest among baselines, showing that indiscriminately providing contexts harms the performance. Our NARRATIVE-EXPERTS and NARRATIVEEXPERTS-RAG-CUTOFF significantly enhance performance, thanks to the temporal expert.

Past-absence and past-only types. Both naive RAG and RAG-cutoff can potentially mitigate hallucinations for these question types by leveraging context from their retrieval modules. However, their performance still lags behind that observed in past-presence questions, with gaps of $10 \%$ points and $13 \%$ points in past-absence types, and $19 \%$ points and $20 \%$ points in past-only types, respectively. Conversely, our methods enhance outcomes in both past-absence and past-only types, thanks to the support of both temporal and spatial experts.

Past-presence type instances: All baselines, except for Mistral, perform admirably, showcasing the role-playing LLMs' proficiency in memorizing narratives from novel series. Our methods slightly lag in this type due to narrative experts' occasional

| Method | Spatiotemporal Consistency $(\%) \uparrow$ |  |  |  |  | Personality <br> Consistency (1-7) $\uparrow$ | AlignScore $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Future | Past-absence | Past-presence | Past-only | Avg. |  |  |
| Mistral Instruct 7B (mistral-7b-instruct-v0.2) |  |  |  |  |  |  |  |
| zero-shot | $44.5 \pm 3.5$ | $53.0 \pm 5.0$ | $63.0 \pm 4.9$ | $38.0 \pm 3.4$ | $46.8 \pm 2.0$ | $\mathbf{6 . 0 2} \pm \mathbf{0 . 0 4}$ | $18.50 \pm 0.66$ |
| RAG-cutoff | $48.0 \pm 3.5$ | $44.0 \pm 5.0$ | 71.0 $\pm 4.6$ | $51.5 \pm 3.5$ | $52.3 \pm 2.0$ | $5.90 \pm 0.05$ | $17.82 \pm 0.68$ |
| narrative-experts (Ours) | $55.0 \pm 3.5$ | $81.0 \pm 3.9$ | $57.0 \pm 5.0$ | $\overline{42.5 \pm 3.5}$ | $55.5 \pm 2.0$ | $\overline{5.87 \pm 0.04}$ | $20.57 \pm 0.71$ |
| narrative-experts-RAG-cutoff (Ours) | $62.0 \pm 3.4$ | $87.0 \pm 3.4$ | $66.0 \pm 4.8$ | $58.5 \pm 3.5$ | $65.7 \pm 1.9$ | $5.85 \pm 0.04$ | $\mathbf{2 2 . 2 0} \pm \mathbf{0 . 8 0}$ |
| GPT-3.5 Turbo (gpt-3.5-turbo-1106) |  |  |  |  |  |  |  |
| zero-shot | $29.0 \pm 3.2$ | $33.0 \pm 4.7$ | $91.0 \pm 2.9$ | $41.5 \pm 3.5$ | $44.2 \pm 2.0$ | $5.89 \pm 0.04$ | $24.06 \pm 0.93$ |
| RAG-cutoff | $37.5 \pm 3.4$ | $34.0 \pm 4.8$ | $91.0 \pm 2.9$ | $55.5 \pm 3.5$ | $51.8 \pm 2.0$ | $5.73 \pm 0.05$ | $24.39 \pm 0.95$ |
| narrative-experts (Ours) | $47.5 \pm 3.5$ | $70.0 \pm 4.6$ | $86.0 \pm 3.5$ | $\overline{43.5 \pm 3.5}$ | $56.3 \pm 2.0$ | $5.76 \pm 0.04$ | $\underline{27.03 \pm 0.92}$ |
| narrative-experts-RAG-cutoff (Ours) | $\underline{46.0 \pm 3.5}$ | $72.0 \pm 4.5$ | $84.0 \pm 3.7$ | $\mathbf{5 7 . 5} \pm \mathbf{3 . 5}$ | $60.5 \pm 2.0$ | $5.61 \pm 0.05$ | $28.24 \pm 0.93$ |
| GPT-4 Turbo (gpt-4-1106-preview) |  |  |  |  |  |  |  |
| zero-shot | $46.5 \pm 3.5$ | $75.0 \pm 4.4$ | $90.0 \pm 3.0$ | $59.0 \pm 3.5$ | $62.7 \pm 2.0$ | $6.44 \pm 0.03$ | $24.63 \pm 0.71$ |
| zero-shot-cot | $48.5 \pm 3.5$ | $75.0 \pm 4.4$ | $92.0 \pm 2.7$ | $61.0 \pm 3.5$ | $64.3 \pm 2.0$ | $6.51 \pm 0.03$ | $23.67 \pm 0.65$ |
| few-shot | $47.0 \pm 3.5$ | $76.0 \pm 4.3$ | $88.0 \pm 3.3$ | $67.0 \pm 3.3$ | $65.3 \pm 1.9$ | $6.35 \pm 0.03$ | $28.35 \pm 0.87$ |
| self-refine | $48.0 \pm 3.5$ | $75.0 \pm 4.4$ | $94.0 \pm 2.4$ | $65.0 \pm 3.4$ | $65.8 \pm 1.9$ | $6.44 \pm 0.03$ | $24.41 \pm 0.70$ |
| RAG | $33.5 \pm 3.4$ | $81.0 \pm 3.9$ | $91.0 \pm 2.9$ | $72.0 \pm 3.2$ | $63.8 \pm 2.0$ | 6.55 | $21.14 \pm 0.64$ |
| RAG-cutoff | $50.0 \pm 3.5$ | $79.0 \pm 4.1$ | $92.0 \pm 2.7$ | $\overline{72.0 \pm 3.2}$ | $69.2 \pm 1.9$ | $6.47 \pm 0.03$ | $24.15 \pm 0.72$ |
| narrative-experts (Ours) | $92.5 \pm 1.9$ | $90.0 \pm 3.0$ | $\overline{90.0 \pm 3.0}$ | $\overline{67.5 \pm 3.3}$ | $83.3 \pm 1.5$ | $\overline{6.27 \pm 0.03}$ | $\mathbf{3 1 . 8 6} \pm \mathbf{0 . 7 3}$ |
| narrative-experts-RAG-cutoff (Ours) | $\overline{93.0 \pm 1.8}$ | $89.0 \pm 3.1$ | $88.0 \pm 3.3$ | $74.5 \pm 3.1$ | $\overline{85.3 \pm 1.5}$ | $6.30 \pm 0.03$ | $\underline{31.18 \pm 0.72}$ |
| GPT-4o (gpt-4o-2024-05-13) |  |  |  |  |  |  |  |
| zero-shot | $46.0 \pm 3.5$ | $74.0 \pm 4.4$ | $90.0 \pm 3.0$ | $65.5 \pm 3.5$ | $64.5 \pm 2.0$ | $6.26 \pm 0.03$ | $26.78 \pm 0.81$ |
| RAG-cutoff | $51.0 \pm 3.5$ | $74.0 \pm 3.5$ | $\overline{92.0 \pm 2.7}$ | $74.5 \pm 3.1$ | $69.5 \pm 1.9$ | $\overline{6.28 \pm 0.03}$ | $24.27 \pm 0.73$ |
| narrative-experts (Our | $94.5 \pm 1.6$ | $84.0 \pm 3.7$ | $83.0 \pm 3.8$ | $\overline{68.5 \pm 3.3}$ | $82.2 \pm 1.6$ | $6.02 \pm 0.04$ | $\mathbf{3 3 . 5 8} \pm \mathbf{0 . 8 0}$ |
| narrative-experts-RAG-cutoff (Ours) | $\overline{95.5 \pm 1.5}$ | $\overline{89.0} \pm \mathbf{3 . 1}$ | $86.0 \pm 3.5$ | $79.5 \pm 2.9$ | $\overline{\mathbf{8 7 . 5} \pm 1.4}$ | $6.05 \pm 0.04$ | $32.57 \pm 0.83$ |

Table 5: Results of point-in-time character hallucination on 600 sampled data instances. We report the average scores with their standard error of the mean (SEM). Bold numbers indicate the highest scores, while underline numbers are the second-best. All responses are evaluated by GPT-4 Turbo (gpt-4-1106-preview) as judges, with the exception of measuring AlignScore (Zha et al., 2023).

mispredictions, yet this shortfall is minor compared to significant enhancement in the other three types.

Personality consistency: All methods generally maintain a consistent character portrayal, scoring above 5.6 in personality consistency. However, our methods receive lower scores from the GPT-4 judge due to their tendency to respond with unawareness regarding future events or character absences, which sometimes falls short of fully conveying the expected character's personality. In contrast, the GPT-4 judge appears to favor responses from roleplaying agents that indiscriminately disclose information, regardless of its relevance to the character's knowledge boundary at a specific time point.

AlignScore evaluation: In addition, we utilize AlignScore (Zha et al., 2023), a metric based on a post-trained RoBERTa-large model, to assess the spatiotemporal consistency of the role-playing agent without relying on GPT-4 judges. Refer to Appendix G. 6 for details of the AlignScore. The results in Table 5 show that AlignScore is in agreement with evaluations from GPT-4 judges, and our methods achieve the highest AlignScores across the three different backbone LLMs.

Furthermore, we provide further analyses be- yond the main experiments, including human evaluation results that closely align with those of the LLM judges, as detailed in Appendix H.

## 6 Conclusion

We highlighted the importance of point-in-time role-playing agents for enhancing narrative engagement, preventing spoilers, and facilitating fandom role-play activities. To maintain a character's spatiotemporal consistency and avoid hallucinations, we introduced the TimECHARA benchmark and developed an automated pipeline, resulting in 10,895 instances. Using TimeCHARA, we identified significant hallucination issues in state-of-the-art roleplaying LLMs. To address these, we proposed NARRATIVE-EXPERTS, an effective method to reduce character hallucinations by breaking down the reasoning process and guiding it with narrative experts. Despite these efforts, our findings indicate ongoing challenges with point-in-time character hallucinations, suggesting the need for further improvements.

## Limitations

Despite the advancements presented in this study, there are some limitations as follows. (1) Sourced only from English books: Since TimeCHARA consists primarily of texts written in English and sourced from English-speaking countries, it may reflect cultural biases inherent to these regions. One solution is to incorporate multilingual and multicultural books, as new data can be added to TiMECHARA automatically. (2) High costs of GPT4 judges: The financial expenses of extensive GPT4 evaluations can be prohibitive, restricting the feasibility of conducting large-scale assessments. An alternative would be using open-source LLMs for evaluation, such as Kim et al. (2024). (3) Latency and cost issues with NARRATIVE-EXPERTS: The narrative expert requires generating multiple hints and responses per question. This introduces increased latency and computational costs. Future research on efficiently generating responses while reducing point-in-time character hallucinations will be anticipated.

## Ethics Statement

To mitigate any potential issue arising from the use of the four novel series, we address concerns about copyright issues as follows:

1. Source attribution: The dataset utilizes raw text from each novel series, and we acknowledge the copyrights held by the authors and publishers.
2. Fair use justification: We believe our use of the copyrighted text qualifies as "fair use" under U.S. law, whose criteria include:

- Purpose of use: The dataset is used exclusively for non-commercial, educational, and research purposes.
- Nature of the copyrighted work: The work is used in a research context to evaluate point-in-time character hallucination of role-playing LLMs, an inherently academic pursuit.
- Lack of market harm: Our dataset does not substitute for the original works nor harm their market.

3. Content of dataset: TimeCHARA comprises only a fraction of the content necessary for dataset construction, indicating that our dataset includes approximately $40 \%$ of the original text from the sources.
4. Accessibility and reproducibility: We will publish all dataset scripts and the dataset itself, restricting access to those who agree to use it only for research.

Besides, the generated dataset may inadvertently include harmful content intended to mislead characters. In adherence to the NLP ethics community's guidelines on 'toxic text' (Gehman et al., 2020; Askell et al., 2021), We manually reviewed all $11 \mathrm{~K}$ data instances and filtered out those containing provocative scenes. Specifically, we removed fewer than ten cases associated with severe trauma and explicit violence that could negatively impact users who will read questions and the agent's responses.

By implementing these measures, we ensure that our research respects both the legal rights of the original content creators and the ethical standards of the research community.

## Acknowledgements

We thank Jamin Shin, Hyunwoo Kim, Euihyun Tae, and the anonymous reviewers for their valuable comments. This work was supported by SNU-NAVER Hyperscale AI Center, the Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2019II191082, SW StarLab; No. RS-2022-II220156, Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation; No. RS-2021II211343, Artificial Intelligence Graduate School Program (Seoul National University)), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C2005573). Gunhee Kim is the corresponding author.

## References

Jaewoo Ahn, Yeda Song, Sangdoo Yun, and Gunhee Kim. 2023. MPCHAT: Towards multimodal personagrounded conversation. In $A C L$.

AI Dungeon. https://aidungeon.com/.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas

Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. 2021. A general language assistant as a laboratory for alignment. arXiv:2112.00861.

Kent Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023. Speak, memory: An archaeology of books known to ChatGPT/GPT-4. In EMNLP.

Character AI. https://beta.character.ai/.

Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. Roleinteract: Evaluating the social interaction of role-playing agents. arXiv:2403.13679.

Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023. Large language models meet harry potter: A dataset for aligning dialogue agents with characters. In EMNLP Findings.

Wenhu Chen, Xinyi Wang, and William Yang Wang. 2021. A dataset for answering time-sensitive questions. In NeurIPS Datasets and Benchmarks.

Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. 2024. Can ai assistants know what they don't know? arXiv:2401.13275.

Zheng Chu, Zekun Wang, Jiafeng Liang, Ming Liu, and Bing Qin. 2023. MTGER: Multi-view temporal graph enhanced temporal reasoning over timeinvolved document. In EMNLP Findings.

Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-aware language models as temporal knowledge bases. TACL, 10:257-273.

Yu Feng, Ben Zhou, Haoyu Wang, Helen Jin, and Dan Roth. 2023. Generic temporal reasoning with differential analysis and explanation. In $A C L$.

Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In EMNLP Findings.

GPTs. https://openai.com/blog/introducing-gpts.

Vivek Gupta, Pranshu Kandoi, Mahek Vora, Shuo Zhang, Yujie He, Ridho Reinanda, and Vivek Srikumar. 2023. TempTabQA: Temporal question answering for semi-structured tables. In $E M N L P$.

Kilem Gwet. 2008. Computing inter-rater reliability and its variance in the presence of high agreement. The British journal of mathematical and statistical psychology, 61:29-48.
Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee, and Buru Chang. 2022. Meet your favorite character: Open-domain chatbot mimicking fictional characters with only a few utterances. In NAACL.

Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura PerezBeltrachini, Max Ryabinin, Xuanli He, Clémentine Fourrier, and Pasquale Minervini. 2024. The hallucinations leaderboard - an open effort to measure hallucinations in large language models. arXiv:2404.05904.

Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. 2022. TemporalWiki: A lifelong benchmark for training and evaluating ever-evolving language models. In EMNLP.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan $\mathrm{Su}$, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12).

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv:2310.06825.

Hyunwoo Kim, Byeongchang Kim, and Gunhee Kim. 2020. Will I sound like me? improving persona consistency in dialogues through pragmatic selfconsciousness. In EMNLP.

Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023. FANToM: A benchmark for stress-testing machine theory of mind in interactions. In EMNLP.

Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2024. Prometheus: Inducing finegrained evaluation capability in language models. In ICLR.

Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS.

Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang Dong. 2023. Better zero-shot reasoning with role-play prompting. arXiv:2308.07702.

Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In NeurIPS.

Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, and Haozhen Sun. 2023a. Chatharuhi: Reviving anime character in reality via large language model. arXiv:2308.09597.

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023b. Camel: Communicative agents for "mind" exploration of large language model society. In NeurIPS.

Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via selfalignment. arXiv:2401.12474.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In NeurIPS.

Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. TMLR.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In $E M N L P$.

Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained hallucination detection and editing for language models. arXiv:2401.06855.

OpenAI. 2023. Gpt-4 technical report. arXiv:2303.08774.

OpenAI. 2024. Hello gpt-4o.

Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv:2308.03188.

Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In UIST.

Replika. https://replika.com/.

Marie-Laure Ryan. 2003. Narrative as virtual reality: Immersion and interactivity in literature and electronic media. The Johns Hopkins University Press.
Marie-Laure Ryan. 2008. Interactive narrative, plot types, and interpersonal relations. In ICIDS.

Yisi Sang, Xiangyang Mou, Mo Yu, Shunyu Yao, Jing Li, and Jeffrey Stanton. 2022. TVShowGuess: Character comprehension in stories as speaker guessing. In NAACL.

Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large LMs. In EMNLP.

Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623:493-498.

Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-LLM: A trainable agent for roleplaying. In EMNLP.

Tianhao Shen, Sun Li, Quan Tu, and Deyi Xiong. 2023. Roleeval: A bilingual role evaluation benchmark for large language models. arXiv:2312.16132.

Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In NeurIPS.

Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In EMNLP Findings.

SillyTavern. https://github.com/sillytavern/sillytavern.

Talkie. https://www.talkie-ai.com/.

Qingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023a. Towards benchmarking and improving the temporal reasoning capability of large language models. In $A C L$.

Wang-Chiew Tan, Jane Dwivedi-Yu, Yuliang Li, Lambert Mathias, Marzieh Saeidi, Jing Nathan Yan, and Alon Halevy. 2023b. TimelineQA: A benchmark for question answering over timelines. In ACL Findings.

Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, and Kun Gai. 2024. Enhancing roleplaying systems through aggressive queries: Evaluation and improvement. arXiv:2402.10618.

Meiling Tao, Xuechen Liang, Tianyu Shi, Lei Yu, and Yiting Xie. 2024. Rolecraft-glm: Advancing personalized role-playing in large language models. arXiv:2401.09432.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,

Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. arXiv:2307.09288.

Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: A chinese benchmark for role-playing conversational agent evaluation. arXiv:2401.01275.

Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game. In EMNLP.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024a. A survey on large language model based autonomous agents. Front. Comput. Sci., 18 .

Xi Wang, Hongliang Dai, Shen Gao, and Piji Li. 2024b. Characteristic AI agents via large language models. In LREC-COLING.

Xintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. 2023a. Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. arXiv:2310.17976.

Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023b. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv:2310.00746.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In NeurIPS.

Ka Wong, Praveen Paritosh, and Lora Aroyo. 2021. Cross-replication reliability - an empirical approach to interpreting inter-rater reliability. In $A C L$.

Nahathai Wongpakaran, Tinakon Wongpakaran, Danny Wedding, and Kilem Gwet. 2013. A comparison of cohen's kappa and gwet's ac1 when calculating inter-rater reliability coefficients: A study conducted with personality disorder samples. BMC Medical Research Methodology, 13.
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. The rise and potential of large language model based agents: A survey. arXiv:2309.07864.

Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, and Pengfei Liu. 2023. How far are we from believable ai agents? a framework for evaluating the believability of human behavior simulation. arXiv:2312.17115.

Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for honesty. arXiv:2312.07000.

Mo Yu, Qiujing Wang, Shunchi Zhang, Yisi Sang, Kangsheng Pu, Zekai Wei, Han Wang, Liyan Xu, Jing Li, Yue Yu, and Jie Zhou. 2022. Few-shot character understanding in movies as an assessment to metalearning of theory-of-mind. arXiv:2211.04684.

Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Evaluating factual consistency with a unified alignment function. In $A C L$.

Michael Zhang and Eunsol Choi. 2021. SituatedQA: Incorporating extra-linguistic contexts into QA. In EMNLP.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In $A C L$.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv:2309.01219.

Runcong Zhao, Qinglin Zhu, Hainiu Xu, Jiazheng Li, Yuxiang Zhou, Yulan He, and Lin Gui. 2024. Large language models fall short: Understanding complex relationships in detective narratives.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In NeurIPS Datasets and Benchmarks.

Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, Sahand Sabour, Xiaohan Zhang, Wenjing Hou, Yijia Zhang, Yuxiao Dong, Jie Tang, and Minlie Huang. 2023. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv:2311.16832.
