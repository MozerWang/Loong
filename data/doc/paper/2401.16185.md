# LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning 

Yuqiang Sun<br>Nanyang Technological University<br>Singapore, Singapore<br>suny0056@e.ntu.edu.sg<br>Han Liu<br>East China Normal University<br>Shanghai, China<br>hanliu@stu.ecnu.edu.cn

Daoyuan $\mathrm{Wu}^{*}$<br>Nanyang Technological University<br>Singapore, Singapore<br>daoyuan.wu@ntu.edu.sg<br>Wei Ma<br>Nanyang Technological University<br>Singapore, Singapore<br>ma_wei@ntu.edu.sg

Yue Xue<br>MetaTrust Labs<br>Singapore, Singapore<br>xueyue@metatrust.io<br>Lyuye Zhang<br>Nanyang Technological University<br>Singapore, Singapore<br>zh0004ye@e.ntu.edu.sg

Miaolei Shi<br>MetaTrust Labs<br>Singapore, Singapore<br>stan@metatrust.io

Yang Liu<br>Nanyang Technological University<br>Singapore, Singapore<br>yangliu@ntu.edu.sg


#### Abstract

Large language models (LLMs) have demonstrated significant potential for many downstream tasks, including those requiring humanlevel intelligence, such as vulnerability detection. However, recent attempts to use LLMs for vulnerability detection are still preliminary, as they lack an in-depth understanding of a subject LLM's vulnerability reasoning capability - whether it originates from the model itself or from external assistance, such as invoking tool support and retrieving vulnerability knowledge.

In this paper, we aim to decouple LLMs' vulnerability reasoning capability from their other capabilities, including the ability to actively seek additional information (e.g., via function calling in SOTA models), adopt relevant vulnerability knowledge (e.g., via vector-based matching and retrieval), and follow instructions to output structured results. To this end, we propose a unified evaluation framework named LLM4Vuln, which separates LLMs' vulnerability reasoning from their other capabilities and evaluates how LLMs vulnerability reasoning could be enhanced when combined with the enhancement of other capabilities. To demonstrate the effectiveness of LLM4Vuln, we have designed controlled experiments using 75 ground-truth smart contract vulnerabilities, which were extensively audited as high-risk on Code4rena from August to November 2023, and tested them in 4,950 different scenarios across three representative LLMs (GPT-4, Mixtral, and Code Llama). Our results not only reveal ten findings regarding the varying effects of knowledge enhancement, context supplementation, prompt schemes, and models but also enable us to identify 9 zero-day vulnerabilities in two pilot bug bounty programs with over 1,000 USD being awarded.


## 1 INTRODUCTION

In the rapidly evolving landscape of computer security, the introduction of Large Language Models (LLMs) has significantly altered our approach to addressing complex challenges. These models, distinguished by their extensive pre-training and strong instruction following, exhibit a remarkable ability to understand and interpret[^0]

the semantics of both human and programming languages. This leads to a new paradigm called LLM-based vulnerability detection, which brings the highest level of intelligence and flexibility compared to traditional program analysis-based vulnerability detection (e.g., $[15-17,28,29,56,64,66,69]$ ) and neural network-based vulnerability detection (e.g., $[18,21,45,55,68,70]$ ).

Under this emerging new paradigm (triggered by the successful release of ChatGPT [13, 38, 48] on 30 November 2022), there are mainly two dimensions of related research. One dimension focuses on how to design specific LLM-based detectors for different security problems. For example, researchers have successfully designed TitanFuzz [25], FuzzGPT [26], Fuzz4All [67], and ChatAFL [47] for fuzzing various vulnerabilities; GPTScan [57] and GPTLens [33] for detecting smart contract vulnerabilities; and LLift [42] and LATTE [46] for LLM-enhanced program and binary analysis.

The other dimension aims to benchmark or evaluate LLMs' capabilities in the context of vulnerability detection. It focuses on how different models, configurations, and instructions could influence the final vulnerability detection results, aiming to answer the key research question, "How far have we come?" in the paradigm of LLM-based vulnerability detection. Notably, Thapa et al. conducted a pioneer work [59] towards this objective by benchmarking the performance of transformer-based language models in detecting software vulnerabilities compared to RNN-based models. With the release of ChatGPT and GPT-4, more LLM-focused benchmark studies were conducted, including for smart contracts [19, 24] and for traditional C/C++/Java vulnerabilities [31, 37, 62], as well as for vulnerability repair [50].

Our research falls into the second dimension. However, instead of focusing on the performance of individual LLM instances and their configurations, we delve into the paradigm itself and consider what is missing or could be improved. To this end, we first abstract and generalize the paradigm of LLM-based vulnerability detection into an architecture shown in Figure 1, where LLMs are denoted by LLM. Existing LLM-based vulnerability detection typically takes a piece of target code $T C$ and asks $L L M$ to determine whether $T C$

![](https://cdn.mathpix.com/cropped/2024_06_04_c7144faaae4aa688b1bcg-02.jpg?height=521&width=854&top_left_y=279&top_left_x=169)

Figure 1: An illustration of the LLM-based vulnerability detection paradigm and an overview of the LLM4Vuln framework.

is vulnerable under certain prompt schemes (e.g., role playing [33] and chain-of-thought [65]). However, the additional information about TC (e.g., the context of functions and variables involved in $T C$ ), which can be obtained by LLMs through invoking tool support, is often overlooked in the paradigm. More importantly, LLMs are pre-trained up to a certain cutoff date ${ }^{1}$, making it challenging for $L L M$ to adapt to the latest vulnerability knowledge. In other words, it is essential to incorporate relevant vulnerability knowledge $V K$ into the paradigm. Furthermore, open-source LLMs typically have weaker instruction-following capabilities than OpenAI models, due to the latter being aligned with extensive reinforcement learning from human feedback (RLHF) [40], which could indirectly affect $L L M$ 's reasoning outcome in automatic evaluation. As we can see, LLM's vulnerability reasoning capability can be influenced by various factors beyond the model itself and its configuration.

Based on this intuition, rather than treating LLM-based vulnerability detection as a whole for evaluation, we aim to decouple LLMs' vulnerability reasoning capability from their other capabilities for the first time and assess how LLMs' vulnerability reasoning could be enhanced when combined with the enhancement of other capabilities. As for benchmarking and tuning the model itself under different configurations (e.g., different temperatures), we defer to another dimension of related work focused on how to pre-train vulnerability-specific or security-oriented LLMs [20, 27, 30, 32, 35, $39,43,49,51,71]$, going beyond merely being a language model.

To achieve this research objective, we propose LLM4Vuln, a unified evaluation framework for decoupling and enhancing LLMs' vulnerability reasoning. As illustrated in Figure 1, LLM4Vuln first considers the state-of-the-art (SOTA) LLMs' ability to actively invoke tools for seeking additional information about $T C$, such as through function calling in proprietary OpenAI models [8] and in fine-tuned open-source models $[9,11]$. Besides supplying additional information for TC through LLM-invoked tools, LLM4Vuln also decouples and enhances LLMs' $V K$ by providing a searchable vector database of vulnerability knowledge, similar to the latest RAG (retrieval-augmented generation [41]) technique for NLP-domain knowledge enhancement. Furthermore, LLM4Vuln incorporates typical prompt engineering enhancements by exploring different prompt schemes and employs the most capable GPT-4 model to[^1]

refine the raw, unstructured output of models with less proficient instruction-following capabilities. More specifically, LLM4Vuln introduces the following novel design:

- For knowledge enhancement, LLM4Vuln not only incorporates raw vulnerability reports for knowledge retrieval but also takes into account summarized vulnerability knowledge. However, the summarized knowledge cannot be directly searched using only TC. Therefore, during the automatic summarization of vulnerability knowledge, we also generate descriptions of the functionality or applicable scenarios for each original vulnerability. By embedding these summarized functionalities into a vector database, we enable the retrieval of relevant $V K$ based on the similarity between its corresponding functionality and that of $T C$.
- For tool invocation, to the best of our knowledge, we are the first to enhance LLM-based vulnerability analysis with the capability to seek tool support through LLM's function calling mechanism [8]. In this paper, LLM4Vuln focuses on providing LLMs with $T C$ ' context through program analysis, but it can be easily extended to include more complicated additional information about $T C$, e.g., control and data flow.
- For prompt schemes, we adopt common practices such as CoT (chain-of-thought [65]) and customize it into two specific CoT schemes for vulnerability analysis scenarios. The details will be introduced in $\S 3.3$.
- For instruction following, we propose using a well-instructed model to align and structure the analysis output of a lessinstructed model, facilitating automatic result evaluation.

In this paper, we focus our implementation of LLM4Vuln on smart contracts $[1,2]$, though LLM4Vuln is versatile and can be adapted to other programming languages with minor adjustments. To facilitate this, we collect 1,013 high-quality smart contract vulnerability data for knowledge "training," and 75 vulnerability-introducing code segments for ground-truth testing. They are tested in 4,950 scenarios across three types of knowledge enhancement (including one with only LLMs' pre-trained knowledge), two context supplementation options (with or without context supplementation), and three prompt schemes (raw and two CoT schemes), using three representative LLMs (GPT-4, Mixtral, and Code Llama). By analyzing the results annotated by LLMs in §3.4, we identify ten key findings, some of which are highlighted as follows (more details in $\S 5$ ):

- (Knowledge Enhancement) Adding vulnerability knowledge aids LLMs' vulnerability reasoning but requires careful design. Long forms like full vulnerability reports distract LLMs and increase false positives. Shorter, summarized knowledge improves vulnerability identification.
- (Context Supplementation) Supplying context through the invocation of tools may not always enhance LLMs' ability to reason about vulnerabilities. It could also lead to distractions, hindering LLMs from accurately identifying vulnerabilities.
- (Prompt Schemes and Different LLMs) LLMs designed for different purposes exhibit significant differences in their ability to detect vulnerabilities. LLMs trained on code-related tasks, such as code generation, may not always excel in analyzing code for vulnerabilities. CoT prompt schemes affect Code

Table 1: Major LLMs used for security tasks (input price and output price are in USD per $1 \mathrm{M}$ tokens).

| Model API | Max. <br> Token | Knowledge <br> Cutoff Date | Input <br> Price | Output <br> Price |
| :--- | :--- | ---: | ---: | ---: |
| gpt-4-1106-preview | $128 \mathrm{k}$ | $04 / 2023$ | 10 | 30 |
| gpt-4-32k-0613 | $32 \mathrm{k}$ | $09 / 2021$ | 60 | 120 |
| gpt-4-0613 | $8 \mathrm{k}$ | $09 / 2021$ | 30 | 60 |
| gpt-3.5-turbo-1106 | $16 \mathrm{k}$ | $09 / 2021$ | 1 | 2 |
| Mistral-8*7b | $32 \mathrm{k}$ | Summer 2023 | 0.30 | 1 |
| Mistral-7b | $32 \mathrm{k}$ | Summer 2023 | 0.05 | 0.25 |
| Code-Llama-34b | $16 \mathrm{k}$ | $07 / 2023$ | - | - |
| Code-Llama-13b | $16 \mathrm{k}$ | $07 / 2023$ | - | - |

Llama similarly to GPT-4, but their impact on Mixtral's vulnerability detection is neither significant nor stable.

Besides findings on benchmarking LLMs' vulnerability reasoning, we also conducted a pilot study in $\S 5.6$ on using LLM4Vuln to test two real-world projects in crowdsourcing audit bounty programs. We submitted a total of 11 and 12 issues to these two projects, respectively, and 5 and 4 of these issues were confirmed by the project teams, leading to more than 1,000 USD in bug bounty being awarded. The discovery of these 9 zero-day vulnerabilities demonstrates the practical impact and value of LLM4Vuln. We further conducted a case study on the 4 publicly available vulnerabilities, which shows that LLM4Vuln can effectively detect vulnerabilities missed by existing tools, even when the vulnerability does not exactly match the knowledge in the vector database.

## 2 BACKGROUND

Generative Pre-training Transformer (GPT) models, exemplified by GPT-3.5 [48], represent a class of extensive language models trained on diverse text corpora covering a wide range of knowledge across various domains. As indicated in Table 1, there are different variants of such Large Language Models (LLMs), including the GPT series from OpenAI, such as GPT-3.5-turbo, GPT-4, and GPT-4turbo. There are also open-source implementations, like Mixtral [12, 34] and Llama 2 [61]. Mixtral is a model developed by Mistral AI, capable of matching or outperforming the Llama 2 family and surpassing GPT-3.5 in most benchmarks. Mixtral-8x7b-instruct is a fine-tuned version of Mixtral-8x7b, specifically optimized for instruction following. Code Llama-13B [7, 52] from Meta AI is a large-scale language model trained on 13 billion tokens of code and comments from GitHub. Code Llama employs a learning system known as spaced repetition to focus training on tasks where the model is more likely to make errors.

These models can be tailored for specific applications using methods like fine-tuning or zero-shot learning, enabling them to utilize tools and address problems beyond their initial training data $[38,54]$ OpenAI and other research groups have shown the effectiveness of these approaches in preparing LLMs for interactive use with external tools. Moreover, LLMs can engage with knowledge beyond their training datasets through skillful in-context learning prompts, even without fine-tuning [23]. However, it is worth noting that not all in-context learning prompts are equally effective for tasks like vulnerability detection. Additionally, Wei et al. introduced the "chain-of-thought" prompting methodology [65], aimed at enhancing reasoning by breaking down tasks into sequential steps. This

![](https://cdn.mathpix.com/cropped/2024_06_04_c7144faaae4aa688b1bcg-03.jpg?height=496&width=830&top_left_y=283&top_left_x=1103)

Figure 2: LLM4Vuln's four pluggable components for evaluating and enhancing LLMs' vulnerability reasoning capability.

approach prompts LLMs to address each step individually, with each stage's output influencing the next, fostering more logical and coherent outputs.

However, the application of these techniques in vulnerability detection specifically remains an area of uncertainty. It is unclear how these methods could be used to improve precision or recall in LLM-based vulnerability detection. Also, the type of knowledge that could be effectively integrated into in-context learning to boost performance in vulnerability detection tasks needs further investigation and clarification.

## 3 THE LLM4VULN FRAMEWORK

In this section, we introduce the design of LLM4Vuln, which is a modular framework to support the paradigm of LLM-based vulnerability detection. As illustrated in Figure 2, LLM4Vuln supports four types of pluggable components for evaluating and enhancing an LLMs' vulnerability reasoning capability. These components are Knowledge Retrieval, Tool Invocation, Prompt Schemes, and Instruction Following. All of them are well-decoupled, allowing for easy replacement with other implementations. That said, for each component of LLM4Vuln, only one implementation is required for it to function effectively.

The first component (\$3.1) aims to provide LLMs with up-to-date knowledge of vulnerabilities. In LLM4Vuln, we have designed two types of vector databases for knowledge retrieval: one stores the original vulnerability reports, and the other contains summarized knowledge of vulnerabilities.

The second component (\$3.2) invokes tools to provide additional information about the target code TC to LLMs. In LLM4Vuln, we utilize function calling [8] to invoke external tools for retrieving extra information. For models that do not support function calling, we can employ SOTA models, such as GPT-4, to assist them and supplying them with the results from the invoked tools.

The third and fourth components are both discussed in §3.3. The Prompt Schemes component aims to provide enhanced instructions to LLMs to improve reasoning. For this purpose, we have designed three prompt schemes: Raw, Pre-CoT, and Post-CoT. Specifically, the Raw prompt scheme simply asks LLMs to reason about a vulnerability without special instructions, while Pre-CoT and Post-CoT are two chain-of-thought prompt schemes. Besides prompt schemes,

![](https://cdn.mathpix.com/cropped/2024_06_04_c7144faaae4aa688b1bcg-04.jpg?height=379&width=832&top_left_y=285&top_left_x=186)

Figure 3: Two types of vulnerability knowledge retrieval.

LLM4Vuln also aims to enhance instruction-following for structured output, which facilitates the automatic evaluation of results.

Lastly, for benchmarking purpose only, we design an LLM-based result annotation and analysis component in $\S 3.4$.

### 3.1 Retrieving Relevant Vuln Knowledge

Although LLMs are trained with extensive code vulnerability data, they typically have a knowledge cutoff date for pre-training, as indicated in Table 1. As a result, LLMs do not have up-to-date vulnerability knowledge, which is particularly crucial for detecting dynamically evolving logic vulnerabilities, such as those found in smart contracts. To address this issue, LLM4Vuln proposes two types of knowledge retrieval methods for enhancing vulnerability knowledge in LLMs, as illustrated in Figure 3.

In the first type, as illustrated in the left part of Figure 3, we collect original vulnerability reports along with the corresponding vulnerable code. We calculate their embeddings and create a vector database containing both the embeddings of the code and the associated vulnerability report. When a target code segment $T C$ is provided, it can be used to directly search the vector database for the most similar code segments. Since a code segment may include both code and comments, the comments can match the vulnerability reports, and the code can correspond to the code mentioned in the report. After the retrieval process, we use only the text of the vulnerability report, excluding the code, as raw vulnerability knowledge for subsequent analysis.

In the second type, we first use GPT- 4 to summarize the vulnerability reports. This summary includes the functionality of the vulnerable code and the root cause of the vulnerability, encapsulated in several key sentences. The prompts used for summarizing the code's functionality and the key concept causing the vulnerability, as well as examples of the summarized knowledge, can be found in Appendix A and C, respectively.

With the functionality and knowledge from past vulnerability reports summarized, as depicted in the right part of Figure 3, we then calculate the embedding of this functionality part and create a vector database that contains only the functionality embeddings. When a target code segment $T C$ is provided, we use GPT-4 to first summarize its functionality and then use this extracted functionality to retrieve similar functionalities in the vector database. With the matched functionality, we can directly retrieve the corresponding vulnerability knowledge as summarized knowledge for further analysis.

With the vector database, we can provide token-level similarity matching of knowledge to LLMs to enhance their understanding of vulnerabilities. The proposed method can also be easily extended to include other types of knowledge in natural language. For example, it can utilize other graph-based similarity-matching algorithms to match the control flow or data flow of the code segment with the knowledge.

### 3.2 Invoking Tool Support via Function Calling

As previously illustrated in Figure 1, LLMs may invoke external tool support to retrieve additional information about the target code $T C$, such as context information. This necessity arises because some vulnerabilities may result from the combination of multiple functions. For instance, in the case of price manipulation vulnerabilities, the function that calculates prices and the function that uses the price may together cause financial loss [57].

In this paper, we utilize GPT's function calling mechanism [8] to assist LLMs in retrieving extra context information, although this could be extended to include control and data information provided by static analysis tools. Here, we provide the function calling APIs we have defined, along with a description of the usage of each function. With these APIs, LLMs can actively call the function when they require the information.

getFunctionDefinition: This API retrieves the definition of a function. The input is the function name and the class name, and the output is the function's definition. If the class name is not available, the function with the same name in the current class will be used. This API allows LLMs to retrieve the definition of the callee function called by the caller function, aiding LLMs in better understanding the code segment.

getClassInheritance: This API retrieves all parent classes of a given class. The input is the class name, and the output is a list of parent classes. If the class name is not provided, the current class is used. This API allows LLMs to retrieve the inheritance information of the current class, enabling them to detect issues that require class context information.

getVariableDefinition: This API retrieves the definition of a local or state variable. The input includes the variable name and the contract name (for state variables), with the output being the variable's definition. This function enables LLMs to retrieve detailed definitions of global variables in the current or another class. The variable definitions, not included in the functions, are crucial for LLMs to understand variable semantics. This API is also helpful for LLMs to know the type of variables used in a class's member functions.

These APIs provide extra information about $T C$ to LLMs, allowing us to evaluate whether these supplements improve LLMs' vulnerability detection performance. For open-source models that do not the support of function calling yet, they can utilize information from the results of models with this capability. To facilitate this, we record all function calls and information retrieved by OpenAI models and provide the same to the open-source models.

### 3.3 Prompt Schemes and Instruction Following

In this section, we describe the enhancement of prompt schemes and instruction following.

Prompt Schemes. As described in §3.1, there are two types of knowledge provided to LLMs: the original vulnerability reports and

Prompt Combination: Knowledge + Output + Scheme

## Knowledge

Prefix 1 - LLM's own knowledge:

As a large language model, you have been trained with extensive knowledge of smart contract vulnerabilities. Based on this past knowledge, please evaluate whether the given smart contract code is vulnerable.

## Prefix 2 - Raw knowledge:

Now I provide you with a vulnerability report as follows: \{report\}. Based on this given vulnerability report, pls evaluate whether the given smart contract code is vulnerable.

## Prefix 3 - Summarized knowledge:

Now I provide you with a vulnerability knowledge that \{knowl\}. Based on this given vulnerability knowledge, evaluate whether the given smart contract code is vulnerable.

## Output Result:

In your answer, you should at least include three parts: yes or no, type of vulnerability (answer only one most likely vulnerability type if yes), and the reason for your answer.

## Scheme 1 - Raw:

Note that if you need more information, please call the corresponding functions.

## Scheme 2 - Pre-CoT:

Note that during your reasoning, you should review the given smart contract code step by step and finally determine whether it is vulnerable. For example, you can first summarize the functionality of the given code, then analyze whether there is any error that causes the vulnerability. Lastly, provide me with the result.

## Scheme 3 - Post-CoT:

Note that during your reasoning, you should consider whether you could generate a Proof of Concept (PoC) or a patch to facilitate your analysis. For example, if you think the given smart contract code is vulnerable, you can first generate a potential $\mathrm{PoC}$ or a patch. Then, based on this information, re-evaluate whether the given smart contract code is vulnerable and provide me with the result.

Figure 4: Three prompt schemes combined with three different knowledge prefixes, yielding nine detailed prompts.

the summarized knowledge of vulnerabilities. Additionally, LLMs possess inherent knowledge of vulnerabilities from their training. Therefore, we have designed three prompt schemes corresponding to three types of knowledge usage: LLM's own knowledge, Raw knowledge, and Summarized knowledge. Figure 4 illustrates how these types of knowledge can be combined with different CoT instructions to form three kinds of prompt schemes:

- In Scheme 1 - Raw, we simply ask LLMs to generate results without any specific instructions. LLMs can use the APIs mentioned in $\S 3.2$ to retrieve related code segments. For

![](https://cdn.mathpix.com/cropped/2024_06_04_c7144faaae4aa688b1bcg-05.jpg?height=317&width=832&top_left_y=286&top_left_x=1102)

Ground Truth

Figure 5: The process of automatic annotation by GPT-4.

open-source models, this scheme does not include this particular sentence.

- In Scheme 2 - Pre-CoT, we request LLMs to follow chain-ofthought instructions before generating the result. The LLMs should first summarize the functionality implemented by the given code segment, then analyze for any errors that could lead to vulnerabilities, and finally determine the vulnerability status.
- In Scheme 3 - Post-CoT, we ask LLMs to follow chain-ofthought instructions after determining whether they can generate a patch or a Proof of Concept (PoC) exploit. The idea is that if LLMs can create a patch or exploit, they are more likely to reason about a vulnerability effectively. Conversely, if they fail to generate a patch or exploit, it can help them be more confident in rejecting a vulnerability.

Improved Instruction Following. Since all outputs from LLMs are in natural language, they are unstructured and need summarization and annotation to derive the final evaluation results. LLMs have been successfully utilized as evaluators [22, 44], and we use GPT-4 to automatically annotate the outputs of LLMs. The function calling API is employed to transform unstructured answers into structured results. Specifically, LLM4Vuln generates structured results based on the answers provided by different prompt schemes and LLMs. These results include two parts: whether the LLM considers the code to be vulnerable, and the rationale for its vulnerability or lack thereof. The specific prompt used for this process can be found in Appendix A. Following this step, we can understand from the output both the LLMs' assessment of the code's vulnerability and the reasons behind their conclusions.

### 3.4 LLM-based Result Annotation and Analysis

The components mentioned above are designed to enhance LLMs' vulnerability reasoning capabilities. However, there is also a need for a component that enables automatic evaluation of LLMs' reasoning on vulnerabilities. As such, we have designed LLM4Vuln to perform LLM-based result annotation, which is used to obtain the final evaluation results after individual LLMs generate their raw output.

As depicted in Figure 5, the annotation process involves two steps: First, LLM4Vuln compares the vulnerability type in the ground truth with the type of vulnerabilities reported by LLMs. If there is a mismatch in vulnerability types, or if the LLMs fail to detect the correct type, LLM4Vuln generates a reason for the mismatch or the detection failure. The prompts used in this step can be found in Appendix B.

Based on the raw Yes/No output from individual LLMs and whether the vulnerability type matches, as annotated by GPT-4, LLM4Vuln can automatically obtain the following annotation results in terms of true positives, false positives, and false negatives ${ }^{2}$ : TP (True Positive): The subject LLM correctly identifies the vulnerability with accurate type.

FP (False Positive): The subject LLM identifies a vulnerability, but the type is incorrect.

FN Type 1 (FN-1): The subject LLM fails to identify the correct vulnerability type and erroneously concludes that the code is not vulnerable.

FN Type 2 (FN-2): The subject LLM correctly identifies the vulnerability type, but incorrectly concludes that the vulnerability does not exist.

Besides automatic result annotation, we also design a semiautomatic annotation process to identify the reasons why individual LLMs fail to correctly reason about a vulnerability. To facilitate this, we first manually annotate 300 cases to identify common reasons for errors, which then inform the LLM-based annotator. Specifically, for cases of FP and FN-2, the annotator will attribute one of the following four reasons for the incorrect result:

Need Other Code (NOC): The subject LLM requires additional related code to make an informed decision. Various factors can contribute to this need, such as the inadequacy of provided APIs for retrieval or the LLM's inability to correctly utilize them.

Need Detailed Analysis (NDA): The subject LLM lacks the reasoning ability to perform detailed analysis, such as dataflow analysis or symbolic execution.

Wrong Reasoning (WR): The subject LLM employs incorrect reasoning, possibly due to insufficient knowledge or flawed reasoning capabilities.

Other: The reason does not fall into any of the above categories, or the subject LLM fails to provide detailed reasoning in its response.

## 4 IMPLEMENTATION AND SETUP

With LLM4Vuln's modular design introduced in $\S 3$, we now target a detailed implementation of LLM4Vuln in this section. In this paper, we focus on LLM4Vuln's application to smart contracts [1, 2], given that LLMs have demonstrated greater effectiveness in identifying logic vulnerabilities in natural-language-like Solidity contracts [33,57] than in detecting traditional vulnerabilities in $\mathrm{C} / \mathrm{C}++$ programs [42, 46]. Nevertheless, LLM4Vuln is generic and extendable to other programming languages with minor adjustments to the knowledge dataset and the source code parser, and we leave this implementation extension to future work.

### 4.1 Data Collection and Testing

$\S 3.1$ introduced the methodology by which LLM4Vuln builds a vulnerability knowledge base and enables vector-based knowledge retrieval. In this section, we describe how we collect high-quality smart contract vulnerability data for knowledge "training" and ground-truth testing. Specifically, we utilize Code4Rena [6], a popular crowdsourcing auditing platform for smart contracts, to collect all the high-risk vulnerabilities from its GitHub issues [5] between[^2]

January 2021 and November 2023. Since all the LLMs evaluated in this paper were trained with data available before August 2023, we use the vulnerabilities identified before August 2023 as the knowledge set and those identified after July 2023 as the testing set. Ultimately, the knowledge set comprises 1,013 vulnerabilities from 251 projects, and the test set includes 75 vulnerability-introducing code segments, extracted from all 39 public vulnerabilities in 24 projects after July 2023. For knowledge retrieval, the vector database is FAISS [36], and we set top-K to retrieve the 5 most relevant pieces of knowledge per query. As a result, although there are only 75 vulnerability-introducing code segments, they generate 2,475 $(75 \times 11 \times 3)$ diverse test cases for different combinations of knowledge retrieval $(1+5+5)$ and prompt schemes (3), without even considering the context variation.

To test diverse combinations as shown in Figure 4, we propose a technique to make the LLM requests more efficient and reduce token consumption. Specifically, we first send the Raw prompt along with each piece of knowledge for a given code segment. Then, we retrieve extra information for the code segment and save all the retrieved code segments in a list. When making requests with PreCoT and Post-CoT prompts, as well as with open-source models, we reuse the code segments from this list. This technique allows us to reduce token consumption and also enables open-source models without a function call API to utilize the extra information.

### 4.2 LLMs Evaluated and Their Configurations

For LLMs, we aim to benchmark the most advanced proprietary and open-source models available at the time of our evaluation (between November 2023 and January 2024). Therefore, from the LLMs listed in Table 1, we select GPT-4-1106-preview as the proprietary model and choose Mixtral-8x7b-instruct and CodeLlama-34b-instruct as the open-source models, based on the background introduced in $\S 2$. Unfortunately, the cloud version of CodeLlama-34b-instruct on the Replicate website we used ${ }^{3}$ was buggy, resulting in all our outputs being garbled or blank. Eventually, for Code Llama, we used its $13 \mathrm{~b}$-instruct version.

We use the OpenAI API to interact with GPT-4-1106-preview and the Replicate API to interact with Mixtral-8x7b-instruct and CodeLlama-13b-instruct. We adhere to the default model configuration provided by the model providers, as we cannot predict the parameter specifications users might choose in practice. The configurations are as follows:

- GPT-4: Temperature: 1, Top-p: 1, Frequency penalty: 0.0, Presence penalty: 0.0 .
- Mixtral: Temperature: 0.6, Top-p: 0.9, Top-k: 50, Frequency penalty: 0.0 , Presence penalty: 0.0 .
- Code Llama: Temperature: 0.8, Top-p: 0.95, Top-k: 10, Frequency penalty: 0.0 , Presence penalty: 0.0 .

For simplicity, it is worth noting that going forward, we use the above model names to refer to the specific model instances tested in this paper.

For the choice of temperature, we do not want to sacrifice the LLM's inherent creativity for less randomness (by setting the temperature to 0 ). Therefore, we use the models' default settings, which are supposed to be widely used. Moreover, due to the high cost of[^3]testing the LLMs (about \$1,500 for GPT-4, $\$ 150$ for Mixtral, and $\$ 15$ for Code Llama, with the latter two evaluated using only summarized knowledge), we cannot repeat the entire experiment multiple times. Despite these constraints, we argue that potential randomness for individual cases is not an issue for our evaluation. First, as explained in $\S 1$, our research objective is not to obtain a precise performance number for individual models; instead, we are concerned only with the relative improvement by LLM's different assisted capabilities (e.g., knowledge retrieval and function calling). Second, as mentioned in $\S 4.1$, we have 2,475 diverse test cases (around 5,000 cases considering the context variation), which are sufficient to show statistical patterns. Lastly, we welcome other researchers to conduct replication and verification, and we will release all the raw evaluation data for comparison at an anonymous website [10].

## 5 EVALUATION

In this section, we evaluate how LLMs' vulnerability reasoning could be enhanced when combined with the enhancement of other capabilities under the framework of LLM4Vuln. Specifically, we use the experimental setup described in $\S 4$ and the standards for calculating True Positives (TP), False Positives (FP), and False Negatives (FN) as defined in $\S 3.4$. Note that $\S 3.4$ explains and defines two types of FN, which are marked as FN-1 and FN-2 here.

The overall results are presented in Tables 2 and 3. Table 2 displays the raw results of TP, FP, and FN under different combinations of knowledge, context, prompt schemes, and LLMs. Table 3, on the other hand, shows the calculated precision $(\mathrm{P})$, recall $(\mathrm{R})$, and $\mathrm{F} 1-$ score (F) based on these raw results. Based on these results, we perform a correlation analysis and answer five research questions (RQs) from $\S 5.1$ to $\S 5.5$. Note that the results related to the two open-source models are primarily analyzed in $\S 5.4$ for RQ4, as they mainly serve for comparison with GPT-4's results. Lastly, we also conduct a pilot study in $\S 5.6$ on using LLM4Vuln to test new projects for zero-day vulnerabilities.

### 5.1 RQ1: Effects of Knowledge Enhancement

In this RQ, we aim to evaluate the effects of knowledge enhancement mechanisms introduced by LLM4Vuln in §3.1.

We first observe that summarized knowledge (GPT-4 with Summarized Knowledge) positively impacts LLMs' vulnerability reasoning. Compared to the scenario with no extra knowledge (i.e., using LLMs' own pre-trained knowledge under GPT-4 without Extra Knowledge ${ }^{4}$ ), GPT-4 with Summarized Knowledge significantly improves the precision and F1-score of GPT-4's vulnerability reasoning in almost all scenarios, as indicated in Table 3. For example, in the scenario of using a Raw prompt with no context, the precision increases from $12.12 \%$ to $28.17 \%$, and the F1-score also increases from $19.28 \%$ to $27.58 \%$, making this the highest precision and F1score among all scenarios. Regarding raw vulnerabilities, as shown in Table 2, summarized knowledge similarly helps increase the number of TP and reduce the number of FP in almost all scenarios. This suggests that summarized knowledge can effectively aid in detecting more true vulnerabilities and reducing false alarms.[^4]

In contrast, raw knowledge (GPT-4 with Original Vuln Report) seems to have a negative impact on LLMs' vulnerability reasoning. According to Table 2, GPT-4 with Original Vuln Report identifies fewer true vulnerabilities (TP) and also increases the number of false alarms (FP), compared with GPT-4 without Extra Knowledge. Consequently, raw knowledge leads to a reduction in both precision and F1-score across all scenarios in Table 3, compared with scenarios without any additional knowledge supplementation. Nevertheless, GPT-4 with Original Vuln Report reports the highest number of potential vulnerabilities (although most of them are incorrectly typed and thus classified as FP), suggesting it has the potential to cover more ground-truth vulnerabilities, as indicated by the highest recall in Table 3.

We now analyze the difference in reasoning between GPT-4 with Original Vuln Report and GPT-4 with Summarized Knowledge. Specifically, raw knowledge, like original vulnerability reports, is lengthy and rich in detail. Due to the attention mechanism [63] of LLMs, these abundant details can easily distract an LLM's focus, leading it to reach a positive yet incorrect result. In contrast, summarized vulnerability knowledge is concise and contains only the information necessary to detect a specific vulnerability. Hence, when provided with summarized knowledge, LLMs tend to be more focused and are less likely to generate false positives, as indicated by the lowest number of FP for GPT-4 with Summarized Knowledge in Table 2. However, when the provided knowledge does not match the target code $T C$, and given that the knowledge is well-summarized, it may compromise the LLM's ability to generalize the provided knowledge to $T C$. This is indicated by the highest number of FN-1 for GPT-4 with Summarized Knowledge in Table 2.

Finding 1: Adding vulnerability knowledge aids LLMs' vulnerability reasoning but requires careful design. Long forms like full vulnerability reports distract LLMs and increase false positives. Shorter, summarized knowledge improves vulnerability identification. However, mismatched knowledge can hinder the LLM's ability to apply it to specific test codes.

Besides statistical analysis, we also performed an in-depth analysis of the problems we identified during knowledge retrieval. Specifically, we found that certain types of vulnerabilities make it easier to retrieve the correct knowledge. For example, vulnerabilities related to price manipulation make it easier to retrieve accurate knowledge, as both the variable names (related to pricing) and the functionality (calculating price) are similar across various vulnerable code segments. However, other types of vulnerabilities pose greater challenges in retrieving the correct knowledge. For instance, vulnerabilities related to arithmetic operations, such as overflow/underflow and division by zero, make it more difficult to retrieve appropriate knowledge. This difficulty arises because the embedding in the vector database does not effectively correlate mathematical notation with natural language. Additionally, the step of summarizing code to its functionality may result in the loss of critical information needed for accurate knowledge retrieval. This loss of semantic information can lead to failure in retrieving the correct knowledge, consequently causing the LLM to miss detecting the vulnerability.

Table 2: Raw results of TP, FP, and FN under different combinations of knowledge, context, prompt schemes, and LLMs.

|  | Raw |  |  |  | Pre-CoT |  |  |  | Post-CoT |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathrm{TP}$ | $\mathrm{FP}$ | FN-1 | FN-2 | $\mathrm{TP}$ | $\mathrm{FP}$ | FN-1 | FN-2 | $\mathrm{TP}$ | FP | FN-1 | FN-2 |
| GPT-4 W/O Extra Knowledge | C | 5 | 46 | 15 | 9 | 7 | 41 | 15 | 12 | 11 | 53 | 8 | 3 |
|  | $\mathrm{N}$ | 8 | 58 | 5 | 4 | 8 | 52 | 11 | 4 | 6 | 57 | 9 | 3 |
| GPT-4 W/O Extra Knowledge <br> (Normalized with respect to top-K) | $\mathrm{C}$ | 25 | 230 | 75 | 45 | $35 \triangle$ | $205 \triangle$ | 75 | 60 | $55 \triangle$ | $265 \nabla$ | 40 | 15 |
|  | $\mathrm{N}$ | 40 | 290 | 25 | 20 | $40-$ | $260 \triangle$ | 55 | 20 | $30 \nabla$ | $285 \triangle$ | 45 | 15 |
| GPT-4 W/ Original Vuln Report | C | $19 \downarrow$ | $315 \downarrow$ | 38 | 3 | $26 \downarrow \triangle$ | $286 \downarrow \triangle$ | 53 | 10 | $26 \downarrow \triangle$ | $322 \downarrow \nabla$ | 23 | 4 |
|  | $\mathrm{N}$ | $25 \downarrow$ | $326 \downarrow$ | 23 | 1 | $28 \downarrow \triangle$ | $302 \downarrow \triangle$ | 44 | 1 | $30-\triangle$ | $336 \downarrow \nabla$ | 8 | 1 |
| GPT-4 W/ Summarized Knowledge | C | $37 \uparrow$ | $116 \uparrow$ | 186 | 36 | $51 \uparrow \triangle$ | $131 \uparrow \nabla$ | 137 | 56 | $44 \downarrow \triangle$ | $173 \uparrow \nabla$ | 121 | 37 |
|  | $\mathrm{N}$ | $60 \uparrow$ | $153 \uparrow$ | 128 | 34 | $32 \downarrow \nabla$ | $118 \uparrow \triangle$ | 176 | 49 | $51 \downarrow \nabla$ | $188 \uparrow \nabla$ | 97 | 39 |
| Mixtral W/ Summarized Knowledge | $\mathrm{C}$ | $\overline{6 \downarrow}$ | $19 \uparrow$ | 326 | 24 | $6 \downarrow-$ | $3 \uparrow \triangle$ | 344 | 22 | $3 \downarrow \nabla$ | $16 \uparrow \triangle$ | 341 | 15 |
|  | $\mathrm{N}$ | $7 \downarrow$ | $21 \uparrow$ | 324 | 23 | $2 \downarrow \nabla$ | $10 \uparrow \triangle$ | 334 | 29 | $5 \downarrow \nabla$ | $18 \uparrow \triangle$ | 328 | 24 |
| Llama W/ Summarized Knowledge | C | $31 \downarrow$ | $207 \downarrow$ | 133 | 4 | $29 \downarrow \nabla$ | $267 \downarrow \nabla$ | 73 | 6 | $42 \downarrow \triangle$ | $264 \downarrow \nabla$ | 65 | 4 |
|  | $\mathrm{N}$ | $18 \downarrow$ | $223 \downarrow$ | 125 | 9 | $37 \downarrow \triangle$ | $257 \downarrow \nabla$ | 77 | 4 | $40 \downarrow \triangle$ | $261 \downarrow \nabla$ | 70 | 4 |

1. " $\mathrm{C}$ " and " $\mathrm{N}$ " represent the results with context and without context, respectively.
2. $\triangle$ and $\nabla$ indicate better or worse values compared to the Raw column.
3. $\uparrow$ and $\downarrow$ indicate better or worse values compared to the second row (for GPT-4) and the fourth row (for open-source models).

Table 3: Calculated precision (P), recall (R), and F1-score (F), presented as raw percentage numbers, from the results in Table 2 .

|  | $\overline{\text { Raw }}$ |  |  | Pre-CoT |  |  | Post-CoT |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{F}$ | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{F}$ | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{F}$ |
| GPT-4 W/O Extra Knowledge | C | 9.80 | 17.24 | 12.50 | $14.58 \triangle$ | $20.59 \triangle$ | $17.07 \triangle$ | $17.19 \triangle$ | $50.00 \triangle$ | $25.58 \triangle$ |
|  | $\mathrm{N}$ | 12.12 | 47.06 | 19.28 | $13.33 \triangle$ | $34.78 \nabla$ | $19.28-$ | $9.52 \nabla$ | $33.33 \nabla$ | $14.81 \nabla$ |
| GPT-4 W/ Original Vuln Report | C | $5.69 \downarrow$ | $31.67 \uparrow$ | $9.64 \downarrow$ | $8.33 \downarrow \triangle$ | $29.21 \uparrow \nabla$ | $12.97 \downarrow \triangle$ | $7.47 \downarrow \triangle$ | $49.06 \downarrow \triangle$ | $12.97 \downarrow \triangle$ |
|  | $\mathrm{N}$ | $7.12 \downarrow$ | $51.02 \uparrow$ | $12.50 \downarrow$ | $8.48 \downarrow \triangle$ | $38.36 \uparrow \nabla$ | $13.90 \downarrow \triangle$ | $8.20 \downarrow \triangle$ | $76.92 \uparrow \triangle$ | $14.81 \triangle$ |
| GPT-4 W/ Summarized Knowledge | C | $24.18 \uparrow$ | $14.28 \downarrow$ | $17.96 \uparrow$ | $28.02 \uparrow \triangle$ | $20.90 \uparrow \triangle$ | $23.94 \uparrow \triangle$ | $20.28 \uparrow \nabla$ | $21.78 \downarrow \triangle$ | $21.00 \downarrow \triangle$ |
|  | $\mathrm{N}$ | $28.17 \uparrow$ | $27.03 \downarrow$ | $27.58 \uparrow$ | $21.33 \uparrow \nabla$ | $12.45 \downarrow \nabla$ | $15.72 \downarrow \nabla$ | $21.34 \uparrow \nabla$ | $27.27 \downarrow \triangle$ | $23.94 \uparrow \triangle$ |
| Mixtral W/ Summarized Knowledge | C | $24.00 \downarrow$ | $1.69 \downarrow$ | $3.15 \downarrow$ | $66.66 \uparrow \triangle$ | $1.61 \downarrow \nabla$ | $3.15 \downarrow-$ | $15.79 \downarrow \nabla$ | $0.84 \downarrow \nabla$ | $1.59 \downarrow \nabla$ |
|  | $\mathrm{N}$ | $25.00 \downarrow$ | $1.98 \downarrow$ | $3.67 \downarrow$ | $16.66 \downarrow \nabla$ | $0.55 \downarrow \nabla$ | $1.06 \downarrow \nabla$ | $21.74 \downarrow \nabla$ | $1.40 \downarrow \nabla$ | $2.63 \downarrow \nabla$ |
| Llama W/ Summarized Knowledge | C | $13.03 \downarrow$ | $18.45 \uparrow$ | $15.27 \downarrow$ | $9.79 \downarrow \nabla$ | $26.85 \uparrow \triangle$ | $14.36 \downarrow \nabla$ | $13.72 \downarrow \triangle$ | $37.84 \uparrow \triangle$ | $20.14 \downarrow \triangle$ |
|  | $\mathrm{N}$ | $7.47 \downarrow$ | $11.84 \downarrow$ | $9.16 \downarrow$ | $12.59 \downarrow \triangle$ | $31.36 \uparrow \triangle$ | $17.96 \uparrow \triangle$ | $13.29 \downarrow \triangle$ | $35.09 \uparrow \triangle$ | $19.28 \downarrow \triangle$ |

1. $\uparrow$ and $\downarrow$ indicate the change in value compared to the first row (for GPT-4) and the third row (for open-source models).

Finding 2: There are two directions for future improvement to enhance knowledge retrieval in vulnerability detection. First, it is crucial to preserve the semantic information during the retrieval process. Second, the embedding process of the vector database should be refined to establish a suitable correlation between mathematical notation and natural language.

### 5.2 RQ2: Effects of Context Supplementation

In this RQ, we aim to evaluate the effects of context supplementation provided by LLMs' capability of invoking tools, as introduced in $\S 3.2$. We present all the results with context supplementation (the "C" rows) and without context supplementation (the " $\mathrm{N}$ " rows) for all combinations of knowledge, prompt schemes, and models in Table 2 and 3.

According to Table 2, out of the total 15 scenarios (combinations of 5 models with knowledge and 3 prompt schemes), 9 scenarios show both a higher number of TP and FP without context supplementation. In the remaining scenarios, 4 show a lower number of TP and FP without context supplementation, and 2 scenarios have a lower number of TP but a higher number of FP without context supplementation.
This suggests that less context can help LLMs focus more on the code segment itself, potentially reducing false positives. Conversely, more context can assist LLMs in better understanding the code, potentially increasing true positives. However, additional context may not necessarily enhance reasoning abilities and could introduce extraneous information, potentially distracting LLMs from accurately identifying vulnerabilities.

Finding 3: Supplying context through the invocation of tools may not always enhance LLMs' ability to reason about vulnerabilities. It could also lead to distractions, hindering LLMs from accurately identifying vulnerabilities.

We further analyze the reasons why context supplementation may not always be beneficial. The first reason is that asking LLMs to request additional context, even though LLM4Vuln has defined interfaces for function calling, may not always yield accurate context. We have observed instances where the LLM retrieves an excessive amount of context that is unrelated to the actual ground-truth bug, leading it in the wrong direction. More importantly, not all types of vulnerabilities require analysis with related code; for example, some arithmetic bugs. Different types of vulnerabilities may need
different kinds of contextual code. In the case of price manipulationrelated vulnerabilities, related code through the call chain might be more useful in identifying where the price is calculated and used. Conversely, for other logical errors, the define-use chain or more detailed related code might be more helpful. Providing inappropriate extra code information can introduce additional noise, which, in turn, could lead to poorer results.

Finding 4: There are two directions for future improvement in enhancing context supplementation for vulnerability detection. First, enhancing LLMs' ability to request relevant context via function calling. Second, supplying varied additional information tailored to different vulnerability types.

### 5.3 RQ3: Effects of Different Prompt Schemes

In this RQ, we aim to investigate the effects of different prompt schemes, specifically Pre-CoT and Post-CoT as introduced in $\S 3.3$, on LLMs' vulnerability reasoning.

For Pre-CoT, we first observe that it can improve precision in most scenarios. Out of the ten scenarios presented in Table 3, PreCoT improves precision in seven, with the exceptions being GPT-4 with Summarized Knowledge and without Context, Mixtral with Summarized Knowledge and without Context, and Code Llama with Summarized Knowledge and with Context. Specifically, as indicated in Table 2, Pre-CoT increases the number of TP in half of the scenarios (with two remaining equal), and simultaneously reduces the number of FP in seven out of ten scenarios. This improvement is likely because LLMs generate new tokens based on existing ones, and Pre-CoT prompts LLMs to conduct reasoning according to the functionality before providing an answer, leading to more reasonable outcomes in vulnerability detection. On the other hand, Pre-CoT does not consistently impact recall improvement, as shown in Table 3. Further analysis regarding the reasons for FP and FN cases will be discussed in $\S 5.5$.

Finding 5: The Pre-CoT prompt scheme can improve the precision of LLMs' vulnerability reasoning in most scenarios, yet it does not have a consistent impact on recall improvement.

For Post-CoT, we do not observe a stable impact on precision because the number of TP and FP simultaneously increases. Indeed, Post-CoT may indirectly hint at vulnerability in the code within the prompt, using words like "Proof of Concept" and "Patch." Regarding recall, while it increases in most scenarios, this is mainly due to both TP and FP increasing, which indirectly leads to a decrease in the number of FN.

Finding 6: The Pre-CoT prompt scheme is not as effective as originally thought; instead, it may indirectly lead LLMs to lean towards positive results about vulnerabilities, causing the number of TP and FP to simultaneously increase.

### 5.4 RQ4: GPT-4 vs. Code Llama and Mixtral

In this RQ, we aim to evaluate the vulnerability reasoning capability of open-source models and determine whether the chain-of-thought prompt scheme is also effective for them. Due to the high expense required to test all combinations of different knowledge and prompt schemes, we only evaluate the best setup of the GPT-4 model with summarized knowledge. We have chosen two state-of-the-art opensource models for our evaluation: Mixtral and Code Llama. Their detailed configurations were previously introduced in $\S 4.2$.

For Mixtral, both the number of TP and FP are lower than those for GPT-4 and Code Llama, as shown in Table 2. This leads to a significant decrease in recall and a low F1-score. Across all six combinations of prompt schemes and context supplements, the recall is less than $2 \%$, and the F1-score is below $4 \%$, as indicated in Table 3. Specifically, with the combination of Mixtral with Context for the Pre-CoT prompt scheme, the precision reaches $66.66 \%$ due to the low numbers of both TP and FP.

Compared to GPT-4, Code Llama exhibits a lower number of FN, including both FN-1 and FN-2, as detailed in Table 2. However, Code Llama has a significantly higher number of FP cases. Consequently, although Code Llama has a higher recall than GPT-4 in most scenarios (except for those without Context and for the Raw prompt scheme), its precision is lower, as shown in Table 3.

To understand the differences in vulnerability detection capabilities between Mixtral and Code Llama, we can consider their design purposes. Mixtral is designed for general tasks similar to GPT-3.5, while Code Llama is specifically designed for generating code based on instructions. As a result, Code Llama tends to produce results that more closely match the given instruction, regardless of whether that instruction is correct. Mixtral, on the other hand, is more likely to reject an instruction if it deems it incorrect.

Finding 7: LLMs designed for different purposes exhibit significant differences in their ability to detect vulnerabilities. LLMs trained on code-related tasks, such as code generation, may not always excel in analyzing code for vulnerabilities.

Regarding the impact of CoT mechanisms on open-source models, we first observe that for Code Llama, both Pre-CoT and Post-CoT prompt schemes increase the number of TP and FP, as shown in Table 2. Concurrently, the number of FN-1 decreases. This outcome aligns with the results observed for GPT-4. In contrast, with Mixtral, the prompt scheme does not have a significant or stable impact on its vulnerability detection capability. Since both the TP and FP numbers are low for Mixtral, any changes in these numbers may not result in a stable or significant impact on the model's performance.

Finding 8: Chain-of-thought prompt schemes affect Code Llama similarly to GPT-4, but their impact on Mixtral's vulnerability detection is neither significant nor stable.

### 5.5 RQ5: Insights from Undetectable Cases

In this RQ, we aim to analyze the reasons for FP and FN. FN-1 cases, which are caused by LLMs incorrectly identifying vulnerability types and resulting in false negatives, are not included in this RQ

Table 4: Summary of reasons for the FP and FN-2 cases.

| Model | WR | NDA | NOC | Other |
| :--- | ---: | ---: | ---: | ---: |
| GPT-4-All | 1014 | 300 | 220 | 119 |
| GPT-4-S | 303 | 113 | 91 | 42 |
| Mixtral | 98 | 1 | 0 | 0 |
| Code Llama | 711 | 22 | 11 | 8 |

due to the numerous potential causes. We limit this RQ to audit results with context supplementation, as one type of vulnerability requires more code context. This approach helps determine whether the current context supplementation is sufficient for detecting vulnerabilities.

Table 4 presents the results. The table headers list the four reasons for FP and FN, namely Wrong Reasoning (WR), Need Detailed Analysis (NDA), Need Other Code (NOC), and Other, as defined in §3.4. In the first column, GPT-4-All refers to all results of GPT-4 with context supplementation, while GPT-4-S represents only the results of GPT-4 with summarized knowledge. This distinction is made to facilitate comparison with the results of Mixtral and Code Llama.

From the reasons summarized by GPT in Table 4, the primary and most significant factor affecting LLMs' ability to detect vulnerabilities is the lack of reasoning capability, as evidenced by the high number of instances labeled as Wrong Reasoning (WR). Secondly, the need for a supplement of detailed analysis, such as data-flow analysis and symbolic execution, is evident for LLMs. Without these, LLMs may not reason correctly and may produce FP or FN, as indicated by the number of cases needing detailed analysis (NDA) for GPT-4.

Thirdly, even though LLM4Vuln has provided function calling APIs for LLMs to retrieve extra information, LLMs may still require more code or information to detect vulnerabilities, as seen by the number of cases needing other code (NOC) for GPT-4. This need arises because, on the one hand, LLMs may require specific types of code information to correctly detect vulnerabilities, such as definition-usage information. On the other hand, due to the negative impact caused by noise, more precise and diverse code information is necessary. For example, vulnerabilities related to precision loss may require the definition of the state variable and the code segments that update this state variable.

Finding 9: Besides reasoning capability, LLMs also need detailed analyses like data-flow analysis and symbolic execution, which could rectify errors and confirm LLM-generated results. Moreover, diverse code information is essential for LLMs to accurately identify different vulnerabilities.

Compared to GPT-4, the primary reason for the incorrect results of Mixtral and Code Llama is wrong reasoning. Unlike GPT-4, these models do not adequately recognize the need for additional code information or support for detailed analysis. As evidenced in Table 4, there is a notably low number of cases requiring detailed analysis (NDA) and needing more code (NOC) for open-source models, especially for Mixtral. This may be due to a relative lack of reasoning ability for vulnerability detection in these models compared to GPT-4.

```
function _routerSwapFromPath(
    SwapPath memory _uniSwapPath,
    uint256 _amountIn,
    address _to,
    uint256 _deadline
) private returns (uint256 amountOut) {
    require(_uniSwapPath.path.length >= 2, "SoulZap: need path0
        of >=2");
    address outputToken = _uniSwapPath.path[_uniSwapPath.path.
        length - 1];
    uint256 balanceBefore = _getBalance(IERC20(outputToken),
        _to);
    routerSwap(
        _uniSwapPath.swapRouter,
        _uniSwapPath.swapType,
        _amountIn,
        _uniSwapPath.amountOutMin,
        _uniSwapPath.path,
        _to,
        _deadline
    );
    amountOut = _getBalance(IERC20(outputToken), _to) -
        balanceBefore;
}
```

Figure 6: Case 1 - Lack of Duplication Check for Input Array.

Finding 10: The main reason for the inferior performance of open-source models in detecting vulnerabilities, compared to GPT-4, is their lack of reasoning ability.

### 5.6 RQ6: Testing New Projects for Zero-Day Vulnerabilities

In this RQ, we applied LLM4Vuln using the combination of GPT-4 with Summarized Knowledge and without Context for the Raw prompt to two real-world projects in crowdsourcing audit bounty programs. This combination achieved the highest F1-score and precision, as indicated in Table 3. Throughout these audit bounty programs, we submitted a total of 11 and 12 issues to two different projects, with 5 and 4 issues respectively confirmed by them. Due to confidentiality agreements and ongoing ethical considerations, we cannot disclose the name and details of the first program. The second program is Apebond [3], which involves a set of contracts for single transaction token swaps, liquidity provision, and bond purchases. The audit report for Apebond is available at [4] and does not include real names of the auditors. From these two programs combined, we received a total bounty of over 1,000 USD. In the following section, we will conduct case studies on the confirmed issues in the second project, namely Apebond.

The first case involves an iteration without proper checks for duplicate entries, potentially leading to financial losses. The source code is shown in Figure 6. The function _routerSwapFromPath is designed to execute a token swap operation using the input parameter _uniSwapPath, which is expected to contain a swapping path _uniSwapPath. path indicating the series of token conversions to be executed. If the input array contains duplicate entries, it will result in unnecessary token conversions, and the fees paid for these duplicated conversions will be lost. Using LLM4Vuln, we matched the functionality of the code with the knowledge that "For any functionality that involves processing input arrays, especially in smart contracts or systems managing assets and tokens, it's crucial to implement stringent validation mechanisms to check for

```
function getSwapRatio(
    SwapRatioParams memory swapRatioParams
) internal view returns (uint256 amount0, uint256 amount1) {
    // More code
    vars.token0decimals = ERC20(address(swapRatioParams.token0)
            ).decimals();
    vars.token1decimals = ERC20(address(swapRatioParams.token1)
        ).decimals();
    vars.underlying0 = _normalizeTokenDecimals(vars.underlying0
                , vars.token\thetadecimals);
    vars.underlying1 = _normalizeTokenDecimals(vars.underlying1
        , vars.token1decimals);
    // More code
}
function _normalizeTokenDecimals(
    uint256 amount,
    uint256 decimals
) internal pure returns (uint256) {
}
    return amount * 10 ** (18 - decimals);
```

Figure 7: Case 2 - Precision Calculation Error Type I.

```
function pairTokensAndValue(
    address token0,
    address token1,
    uint24 fee,
    address uniV3Factory
) internal view returns (uint256 price) {
    // More Code
    uint256 sqrtPriceX96;
    (sqrtPriceX96, , , , , , ) = IUniswapV3Pool(tokenPegPair).
        slot0();
    uint256 token0Decimals = getTokenDecimals(token0);
    uint256 token1Decimals = getTokenDecimals(token1);
    if (token1 < token0) price = (2 ** 192) / ((sqrtPriceX96)
        ** 2 / uint256(10 ** (token0Decimals + 18 -
        token1Decimals)));
    else price = ((sqrtPriceX96) ** 2) / ((2 ** 192) / uint256
            (10 ** (token0Decimals + 18 - token1Decimals)));
```

14 }

Figure 8: Case 3 - Precision Calculation Error Type II.

duplicate entries." The full details of this knowledge are available in Knowledge 1 in Appendix C.

The second case, as shown in Figure 7, involves a precision calculation error that could lead to financial loss. In lines 7 and 8, the function getSwapRatio calculates the underlying balances of the swap tokens. This process includes normalization of precision, but it incorrectly assumes that the precision of the input token is always 18 decimal places. However, when obtaining price ratios from other oracles, the precision may vary and not always be 18 decimal places. This incorrect assumption about precision can lead to miscalculation of the swap ratio, potentially causing the user to swap too many or too few tokens. With LLM4Vuln, we matched the functionality of the code with the knowledge that "The vulnerability stems from an incorrect handling of decimal precision while calculating the price ratio between two oracles with different decimals." The full details of this knowledge are available in Knowledge 2 in Appendix C.

Similarly, as shown in Figure 8, the function pairTokensAndValue is responsible for calculating the price of tokens using sqrtPriceX96 obtained from a UniswapV3Pool. However, this function also erroneously assumes that the precision of sqrtPriceX96 is always 18 decimal places. The knowledge matched in LLM4Vuln for this scenario is "The vulnerability occurs when calculating the squared root price of a position in a liquidity pool with tokens having different decimal values." While two matched knowledge from cases 2

```
function _zap(
    ZapParams memory zapParams,
    SwapPath memory feeSwapPath,
    bool takeFee
) internal whenNotPaused {
    // Verify and setup
    if (zapParams.liquidityPath.lpType == LPType.V2) {
        // some checks
        vars.amount0In = zapParams.amountIn / 2;
        vars.amount1In = zapParams.amountIn / 2;
    }
    // More code
    if (zapParams.liquidityPath.lpType == LPType.V2) {
        (vars.amount0Lp, vars.amount1Lp, ) = IUniswapV2Router02
            (
            zapParams.liquidityPath.lpRouter
        ). addLiquidity(
            zapParams.token0,
            zapParams.token1,
            //
        );
    }
    // More code
}
```

Figure 9: Case 4 - Funding Allocation Error.

and 3 are not identical - the latter specifically mentions the "squared root price" - they are semantically similar and both applicable to describing the same type of vulnerability. The full details of this knowledge are available in Knowledge 3 in Appendix C.

In Figure 9, the function _zap divides the input amount (amountIn) equally between amount0In and amount 1 In on lines 9 and 10. However, the actual token reserve ratio in the pool may not be 1:1. This discrepancy can lead to an imbalanced provision of liquidity when addLiquidity is called on line 16 , as the token pair might require a different ratio for optimal liquidity provision. The knowledge matched in LLM4Vuln for this case is "The fundamental vulnerability occurs when liquidity providers add liquidity to a pool of two tokens, and the token amounts provided have different proportions as compared to the existing liquidity pool. The contract uses the smaller of these proportions to calculate the amount of $\mathrm{LP}$ tokens minted." While this knowledge does not precisely describe the vulnerability, as there is no smaller proportion in this specific case, it can still be useful in detecting the vulnerability when combined with the reasoning ability of LLMs. The full details of this knowledge are available in Knowledge 4 in Appendix C.

From the above four cases, it is evident that knowledge supplements in LLM4Vuln can aid in detecting vulnerabilities, even if the vulnerability does not exactly match the knowledge in the vector database. Among these four cases, only the first can be detected by static analysis tools, while the other three are logic bugs closely related to business logic. With the enhancement of knowledge, LLM4Vuln demonstrates its capability to detect bugs in real-world projects that are not identified by existing tools.

## 6 DISCUSSION

### 6.1 Lessons Learned

In this section, we summarize the key insights gained from our empirical study in $\S 5$ on using LLMs for detecting vulnerabilities in smart contracts, along with some recommendations for enhancing performance.

Invocation of External Tools. Invoking external tools may enhance the performance of LLMs in detecting vulnerabilities in source code. However, relying solely on LLMs to retrieve information or to invoke external tools is not always reliable, as it may lead to the loss of critical data or the inclusion of excessive, irrelevant code. To make external tool invocation more effective, it is essential to imbue LLMs with knowledge from human auditing experts during the knowledge supplementation process. This guidance can help them to retrieve only the necessary context information.

Knowledge Enhancement. Enhancing knowledge is crucial for helping LLMs better understand code and vulnerabilities. Matching code with knowledge is complex, and an inadequate matching algorithm could lead to semantic loss. An algorithm capable of accurately preserving or translating the semantic information of mathematical symbols in the code to natural language for matching knowledge is essential.

Prompt Scheme Selection. Chain-of-thought prompt schemes can significantly affect LLMs' performance. When designing prompt schemes, it is important to break down the tasks into simpler subtasks that LLMs can easily solve. Additionally, the prompt scheme should avoid suggesting results in the prompt or including too many tasks in a single prompt, as this can introduce noise and disrupt the LLMs' results.

### 6.2 Threat to Validity and Limitations

Our study currently faces two main limitations. The first concerns the randomness of LLMs' outputs. To minimize bias from different parameters and random seeds, we use default settings for all models, which means we do not use a consistent seed for all queries. To counteract the randomness, our study extends 75 original vulnerabilityintroducing code segments into a large number of test cases, totaling 4,950 across various combinations. We encourage other researchers to replicate and verify our work, and we will release all raw evaluation data for comparison at [10].

The second limitation relates to the selection of programming languages. Our evaluation of LLMs is solely on Solidity, the most popular language for smart contracts. However, LLM4Vuln is adaptable to other languages, such as $\mathrm{C} / \mathrm{C}++$, Java, JavaScript, etc. To apply LLM4Vuln to these languages, it is necessary to compile a vulnerability knowledge base for the target language and implement corresponding tool-invoking function calls. These function calls should be tailored based on the specific syntax and semantics of each language. For example, Solidity features contract inheritance, which is absent in $\mathrm{C}$ and JavaScript. The existing set of function calls could also be expanded to include data flow analysis and symbolic execution, applicable across multiple languages.

## 7 RELATED WORK

LLM-Based Vulnerability Detection. Vulnerability detection has been a long-standing problem in software engineering. Traditional methods rely on predefined rules or fuzzy testing which are usually incomplete and difficult to detect unknown vulnerabilities. In recent years, with the development of code-based LLMs [53, 60], researchers have proposed many methods to detect vulnerabilities based on LLMs. For example, Thapa et al. [60] explored how to leverage the LLMs in software vulnerability detection. Alqarni et al. [14] introduced a novel model fine-tuned for detecting software vulnerability. Tang et al. [58] proposed novel methods that utilize LLMs to detect software vulnerabilities, they combine sequence and graph information enhancing function-level vulnerability detection. Hu et al. [33] proposed a novel adversarial framework which includes generation and discrimination stages to detect smart contracts vulnerabilities using LLMs.

Besides, some researchers apply LLMs to fuzzy testing for vulnerability detection. For instance, Deng et al. [25] introduce TitanFuzz to leverage LLMs to generate input cases for fuzzing DL libraries. FuzzGPT [26] is another work that uses LLMs to synthesize unusual programs for fuzzing vulnerabilities. Meng et al. [47] proposed an LLM-based protocol implementation fuzzing method called ChatAFL. Xia et al. [67] presented Fuzz4All, a tool that employs LLMs to generate fuzzing inputs for all kinds of programs.

Additionally, there are also some studies combining LLM with existing static analysis methods. For instance, Sun et al. [57] combined GPT with static analysis to detect logic vulnerabilities in smart contracts. Li et al. [42] proposed an automated framework to interface static analysis tools and LLMs. However, these studies only focus on how to detect vulnerabilities, and they do not consider the reasoning capability of LLMs and the impact of the reasoning capability on vulnerability detection. In this paper, we introduce LLM4Vuln, a unified framework to benchmark to explore the capability of LLM's reasoning in vulnerability detection.

Benchmarking LLMs' Capability in Vulnerability Detection. The other researches focus on evacuating the capability of the LLM's reasoning in vulnerability detection. For example, Thapa et al. [59] compared the performance of transformer-based LLM with RNN-based models in software vulnerability detection. Chen et al. [19] conducted an empirical study to investigate the performance of LLM in detecting smart contracts vulnerabilities. David et al. [24] evaluate the performance of LLMs in security audit on 52 DeFi smart contracts. Gao et al. [31] introduced a dataset of vulnerabilities and evacuated the LLMs, deep learning-based method and traditional static analysis models in vulnerability detection. Khare et al. [37] also benchmarked the effectiveness of LLMs in Java and C++ vulnerability detection. Additionally, LLMs have also been evaluated on the vulnerability repair task [50]. However, these studies focus on the performance of individual LLM instances and their configurations. In contrast, we aim to explore the capability of LLMs' reasoning in vulnerability detection.

Security-Oriented LLMs. Beyond being a Language model, some studies introduced security-oriented LLMs, i.e., vulnerability-specific or security-oriented models. For example, Lacomis et al. [39] proposed a technique for creating corpora specifically designed to train models for renaming decompiled code. Pal et al. [49] presented a novel model to predict the real variable name from the decompilation output for security analysis. Pei et al. [51] introduced a new mechanism of transformers for learning different binary and source code semantics in code security analysis tasks. Chen et al. [20] proposed a novel model to improve the output of the decompilation results for security analysis. Ding et al. [27] introduced an execution-aware pre-training strategy to improve model performance in the complicated execution logic. Gai et al. [30] presented a blockchain-oriented model to detect anomalous blockchain transactions. Guthula et al. [32] proposed a network security model
pre-trained on unlabeled network packet traces. Jiang et al. [35] focused on the binary corpora, and presented a specific LLM for binary code analysis. Li et al. [43] also proposed a specific LLM for binary code analysis by capturing the unique characteristics of disassembly. All of these studies focused on the security-oriented LLMs, while our work focuses on the general LLMs' capability and decouples their vulnerability reasoning capability from their other capabilities.

## 8 CONCLUSION

This paper proposed LLM4Vuln, a unified evaluation framework designed to decouple and enhance LLMs' reasoning about vulnerabilities. With LLM4Vuln, we conducted an empirical study on the paradigm of LLM-based vulnerability detection, with an emphasis on smart contract vulnerabilities. By applying LLM4Vuln to evaluate 75 high-risk smart contract vulnerabilities in 4,950 different scenarios across three LLMs (GPT-4, Mixtral, and Code Llama), we obtained valuable insights on the importance of summarized knowledge enhancement in providing relevant knowledge, context supplementation for a comprehensive code scope, and well-structured prompt schemes to facilitate improved instruction following. These aspects have diverse effects in refining LLMs' ability to identify vulnerabilities. Furthermore, our pilot study identified 9 zero-day vulnerabilities across two bug bounty programs, collectively offering bounties of over 1,000 USD, underscoring the practical impact and value of LLM4Vuln. Overall, our research presents an improved paradigm for LLM-based vulnerability detection. In the future, we plan to continue enhancing the existing components of LLM4Vuln and expand its application to other programming languages.

## ACKNOWLEDGEMENTS

We thank Dawei Zhou, Xinrong Li, Liwei Tan, and other colleagues at MetaTrust Labs for their help with LLM4Vuln. This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-019), the National Research Foundation, Singapore, and the Cyber Security Agency under its National Cybersecurity R\&D Programme (NCRP25-P04-TAICeN). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Cyber Security Agency of Singapore.

## REFERENCES

[1] 2023. Ethereum Whitepaper. https://ethereum.org/whitepaper

[2] 2023. Solidity Programming Language. https://soliditylang.org

[3] 2023. SoulZapV1. https://github.com/SoulSolidity/SoulZapV1

[4] 2024. Apebond_final_Secure3_Audit_Report.pdf. https://github.com/ Secure3Audit/Secure3Academy/blob/main/audit_reports/Apebond/Apebond_ final_Secure3_Audit_Report.pdf

[5] 2024. Code4rena Findings on GitHub. https://github.com/orgs/code-423n4 repositories?q=findings

[6] 2024. Code4rena Security Audit Reports. https://code4rena.com/reports

[7] 2024. codellama-13b-instruct. https://replicate.com/meta/codellama-13binstruct

[8] 2024. Function calling - OpenAI API. https://platform.openai.com

[9] 2024. Llama-2-7b-chat-hf-function-calling-v3. https://huggingface.co/Trelis Llama-2-7b-chat-hf-function-calling-v3

[10] 2024. LLM4Vuln: Large Language Models for Vulnerability Detection. https //sites.google.com/view/llm4vuln/home

[11] 2024. Mistral-7B-Instruct-v0.1-function-calling-v2. https://huggingface.co/ Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2

[12] 2024. Mixtral-8x7B-instruct-v0.1. https://replicate.com/mistralai/mixtral-8x7binstruct-v 0.1

[13] 2024. OpenAI. https://openai.com/

[14] Mansour Alqarni and Akramul Azim. 2022. Low level source code vulnerability detection using advanced bert language model. In Proceedings of the Canadian Conference on Artificial Intelligence-Https://caiac. pubpub. org/pub/gdhb8oq4 (may 27 2022)

[15] Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bartel, Jacques Klein, Yves Le Traon, Damien Octeau, and Patrick McDaniel. 2014 FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps. ACM SIGPLAN Notices 49, 6 (June 2014), 259-269 https://doi.org/10.1145/2666356.2594299

[16] Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bar tel, Jacques Klein, Yves Le Traon, Damien Octeau, and Patrick McDaniel. 2014 FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (Edinburgh, United Kingdom) (PLDI '14). Association for Computing Machinery, New York, NY, USA, 259-269 https://doi.org/10.1145/2594291.2594299

[17] Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs. In 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI 08). USENIX Association, San Diego, CA. https://www.usenix.org/conference/osdi-08/klee-unassisted-andautomatic-generation-high-coverage-tests-complex-systems

[18] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2022 Deep Learning Based Vulnerability Detection: Are We There Yet? IEEE Transac tions on Software Engineering 48, 9 (Sept. 2022), 3280-3296. https://doi.org/10. 1109/TSE.2021.3087402

19] Chong Chen, Jianzhong Su, Jiachi Chen, Yanlin Wang, Tingting Bi, Yanli Wang Xingwei Lin, Ting Chen, and Zibin Zheng. 2023. When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We? arXiv:2309.05520 (Sept 2023). https://doi.org/10.48550/arXiv.2309.05520 arXiv:2309.05520 [cs]

20] Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Claire Le Goues, Graham Neubig, and Bogdan Vasilescu. 2022. Augmenting Decompiler Output with Learned Variable Names and Types. 4327-4343. https://www.usenix.org/conference/ usenixsecurity22/presentation/chen-qibin

[21] Yizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, and David Wagner 2023. DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. In Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses. ACM, Hong Kong China, 654-668. https://doi.org/10.1145/3607199.3607242

[22] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 15607-15631. https://doi.org/10.18653/v1/2023.acllong. 870

[23] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei 2023. Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. arXiv:2212.10559 (May 2023). https //doi.org/10.48550/arXiv.2212.10559 arXiv:2212.10559 [cs].

[24] Isaac David, Liyi Zhou, Kaihua Qin, Dawn Song, Lorenzo Cavallaro, and Arthur Gervais. 2023. Do you still need a manual smart contract audit? arXiv:2306.12338 (June 2023). https://doi.org/10.48550/arXiv.2306.12338 arXiv:2306.12338 [cs].

[25] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming Zhang. 2023. Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models. In Proceedings of the 32nd ACM
SIGSOFT International Symposium on Software Testing and Analysis. ACM, Seattle WA USA, 423-435. https://doi.org/10.1145/3597926.3598067

[26] Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, and Lingming Zhang. 2023. Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries. IEEE Computer Society, 830-842. https://www.computer.org/csdl/proceedingsarticle/icse/2024/021700a830/1RLIWGVtGec

[27] Yangruibo Ding, Ben Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. 2023. TRACED: Execution-aware Pre-training for Source Code. arXiv:2306.07487 (June 2023). https://doi.org/10.48550/arXiv.2306.07487 arXiv:2306.07487 [cs].

[28] Yuzhou Fang, Daoyuan Wu, Xiao Yi, Shuai Wang, Yufan Chen, Mengjie Chen, Yang Liu, and Lingxiao Jiang. 2023. Beyond "Protected" and "Private": An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Soft ware Testing and Analysis (Seattle, WA, USA) (ISSTA 2023). Association for Computing Machinery, New York, NY, USA, 1157-1168. https://doi.org/10.1145/3597926.3598125

[29] Andrea Fioraldi, Dominik Maier, Heiko Eißfeldt, and Marc Heuse. 2020. AFL++: Combining Incremental Steps of Fuzzing Research. https://www.usenix.org/ conference/woot20/presentation/fioraldi

[30] Yu Gai, Liyi Zhou, Kaihua Qin, Dawn Song, and Arthur Gervais. 2023. Blockchain Large Language Models. arXiv:2304.12749 (April 2023). https://doi.org/10.48550/ arXiv.2304.12749 arXiv:2304.12749 [cs]

[31] Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, and Chao Zhang. 2023. How Far Have We Gone in Vulnerability Detection Using Large Language Models. arXiv:2311.12420 (Dec. 2023). https://doi.org/10.48550/arXiv.2311.12420 arXiv:2311.12420 [cs]

[32] Satyandra Guthula, Navya Battula, Roman Beltiukov, Wenbo Guo, and Arpit Gupta. 2023. netFound: Foundation Model for Network Security. arXiv:2310.17025 (Nov. 2023). https://doi.org/10.48550/arXiv.2310.17025 arXiv:2310.17025 [cs]

[33] Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Furkan Tekin, and Ling Liu. 2023. Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives. arXiv:2310.01152 (Oct. 2023). https://doi.org/10.48550/arXiv.2310. 01152 arXiv:2310.01152 [cs]

[34] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of Experts. arXiv:2401.04088 (Jan 2024). https://doi.org/10.48550/arXiv. 2401.04088 arXiv:2401.04088 [cs].

[35] Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, and Xiangyu Zhang. 2023. Nova ${ }^{+}$: Generative Language Models for Binaries. arXiv:2311.13721 (Nov. 2023). https://doi.org/10.48550/arXiv.2311.13721 arXiv:2311.13721 [cs]

[36] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with GPUs. arXiv:1702.08734 (Feb. 2017). https://doi.org/10.48550/arXiv. 1702.08734 arXiv:1702.08734 [cs].

[37] Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, and Mayur Naik. 2023. Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities. arXiv:2311.16169 (Nov. 2023). https: //doi.org/10.48550/arXiv.2311.16169 arXiv:2311.16169 [cs].

[38] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199-22213.

[39] Jeremy Lacomis, Pengcheng Yin, Edward Schwartz, Miltiadis Allamanis, Claire Le Goues, Graham Neubig, and Bogdan Vasilescu. 2019. DIRE: A Neural Approach to Decompiled Identifier Naming. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, San Diego, CA, USA, 628-639. https://doi.org/10.1109/ASE.2019.00064

[40] Nathan Lambert, Louis Castricato, Leandro von Werra, and Alex Havrilla. 2022. Illustrating Reinforcement Learning from Human Feedback (RLHF). Hugging Face Blog (2022). https://huggingface.co/blog/rlhf.

[41] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 9459-9474. https://proceedings.neurips.cc/ paper files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf

[42] Haonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian. 2023. The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models. arXiv:2308.00245 (Nov. 2023). https://doi.org/10.48550/arXiv.2308.00245 arXiv:2308.00245 [cs]

[43] Xuezixiang Li, Yu Qu, and Heng Yin. 2021. PalmTree: Learning an Assembly Language Model for Instruction Embedding. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security. ACM, Virtual Event Republic of Korea, 3236-3251. https://doi.org/10.1145/3460120.3484587

[44] Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023. Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. arXiv:2310.01432 (Oct. 2023). https://doi.org/ 10.48550/arXiv.2310.01432 arXiv:2310.01432 [cs].

[45] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based system for vulnerability detection. arXiv preprint arXiv:1801.01681 (2018).

[46] Puzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng Wang, Zhi Li, and Limin Sun. 2023. Harnessing the Power of LLM to Support Binary Taint Analysis. arXiv:2310.08275 (Oct. 2023). https://doi.org/10.48550/ arXiv. 2310.08275 arXiv:2310.08275 [cs].

[47] Ruijie Meng, Martin Mirchev, Marcel Bohme, and Abhik Roychoudhury. [n.d.]. Large Language Model guided Protocol Fuzzing. In Proceedings of the Symposium on Network and Distributed System Security 2024. https://doi.org/10.14722/ndss. 2024.24556

[48] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. https: //doi.org/10.48550/arXiv.2203.02155 arXiv:2203.02155 [cs].

[49] Kuntal Kumar Pal, Ati Priya Bajaj, Pratyay Banerjee, Audrey Dutcher, Mutsumi Nakamura, Zion Leonahenahe Basque, Himanshu Gupta, Saurabh Arjun Sawant, Ujjwala Anantheswaran, and Yan. 2024. "Len or index or count, anything but v1": Predicting Variable Names in Decompilation Output with Transfer Learning. In Proceedings of the 45th IEEE Symposium on Security and Privacy, San Francisco, USA (2024).

[50] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2023. Examining Zero-Shot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium on Security and Privacy (SP). IEEE, San Francisco, CA, USA, 2339-2356. https://doi.org/10.1109/SP46215.2023.10179324

[51] Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, and Suman Jana. 2023. Symmetry-Preserving Program Representations for Learning Code Semantics. arXiv:2308.03312 (Aug. 2023). https://doi.org/10.48550/arXiv.2308.03312 arXiv:2308.03312 [cs].

[52] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. arXiv:2308.12950 (Aug. 2023). https://doi.org/10.48550/arXiv.2308.12950 arXiv:2308.12950 [cs].

[53] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. arXiv:2308.12950 [cs.CL]

[54] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761 (Feb. 2023). https://doi.org/10.48550/arXiv.2302.04761 arXiv:2302.04761 [cs].

[55] Dongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, and Suman Jana. 2019. Neuzz: Efficient fuzzing with neural program smoothing. In 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 803-817.

[56] Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario Polino, Andrew Dutcher, John Grosen, Siji Feng, Christophe Hauser, Christopher Kruegel, and Giovanni Vigna. 2016. SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis. In 2016 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, 138-157. https://doi.org/10.1109/SP.2016.17

[57] Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Haijun Wang, Zhengzi Xu, Xiaofei Xie, and Yang Liu. 2023. GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis. arXiv:2308.03314 (Dec. 2023). https://doi.org/10.48550/arXiv.2308.03314 arXiv:2308.03314 [cs].

[58] Wei Tang, Mingwei Tang, Minchao Ban, Ziguo Zhao, and Mingjun Feng. 2023. CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection. Journal of Systems and Software 199 (2023), 111623. https://doi.org/10.1016/j.jss.2023.111623

[59] Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, and Surya Nepal. 2022. Transformer-Based Language Models for Software Vulnerability Detection. In Proceedings of the 38th Annual Computer Security Applications Conference. ACM, Austin TX USA, 481-496. https://doi org $/ 10.1145 / 3564625.3567985$

[60] Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, and Surya Nepal. 2022. Transformer-based language models for software vulnerability detection. In Proceedings of the 38th Annual Computer Security Applications Conference. 481-496.
[61] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 (July 2023). https://doi.org/10.48550/arXiv.2307.09288 arXiv:2307.09288 [cs].

[62] Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, and Gianluca Stringhini. 2023. Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet. arXiv:2312.12575 (Dec. 2023). https: //doi.org/10.48550/arXiv.2312.12575 arXiv:2312.12575 [cs].

[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All You Need. arXiv:1706.03762 (Aug. 2023). https://doi.org/10.48550/arXiv.1706.03762 arXiv:1706.03762 [cs].

[64] Tielei Wang, Tao Wei, Guofei Gu, and Wei Zou. 2010. TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection. In 2010 IEEE Symposium on Security and Privacy. IEEE, Oakland, CA, USA, 497-512. https://doi.org/10.1109/SP.2010.37

[65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 24824-24837. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf

[66] Daoyuan Wu, Debin Gao, Robert H. Deng, and Chang Rocky K. C. 2021. When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid. In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). IEEE, Taipei, Taiwan, 543-554. https://doi.org/10.1109/DSN48987.2021.00063

[67] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. 2024. Fuzz4All: Universal Fuzzing with Large Language Models. arXiv:2308.04748 (Jan. 2024). https://doi.org/10.48550/arXiv.2308.04748 arXiv:2308.04748 [cs]

[68] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017. Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (Dallas, Texas, USA) (CCS '17). Association for Computing Machinery, New York, NY, USA, 363-376. https://doi.org/10.1145/ 3133956.3134018

[69] Xiao Yi, Yuzhou Fang, Daoyuan Wu, and Lingxiao Jiang. 2023. BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects. In Proc. ISOC NDSS.

[70] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks. In Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips. cc/paper_files/paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf

[71] Wenyu Zhu, Hao Wang, Yuchen Zhou, Jiaming Wang, Zihan Sha, Zeyu Gao, and Chao Zhang. 2023. kTrans: Knowledge-Aware Transformer for Binary Code Embedding. arXiv:2308.12659 (Aug. 2023). https://doi.org/10.48550/arXiv.2308. 12659 arXiv:2308.12659 [cs].
