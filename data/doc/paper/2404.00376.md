# Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks 

Hyunjae Kim ${ }^{1}$, Hyeon Hwang ${ }^{1}$, Jiwoo Lee ${ }^{1}$, Sihyeon Park ${ }^{1}$,<br>Dain Kim ${ }^{1}$, Taewhoo Lee ${ }^{1}$, Chanwoong Yoon ${ }^{1}$, Jiwoong Sohn ${ }^{1}$,<br>Donghee Choi ${ }^{2}$, Jaewoo Kang ${ }^{1,3^{*}}$<br>${ }^{1}$ Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, 02841, Republic<br>of Korea.<br>${ }^{2}$ Imperial College London, South Kensington Campus, London, SW7<br>$2 \mathrm{AZ}$, United Kingdom.<br>${ }^{3}$ AIGEN Sciences, 25 Ttukseom-ro 1-gil, Seongdong-gu, Seoul, 04778,<br>Republic of Korea.<br>*Corresponding author(s). E-mail(s): kangj@korea.ac.kr;<br>Contributing authors: hyunjae-kim@korea.ac.kr;<br>hyeon-hwang@korea.ac.kr; hijiwoo7@korea.ac.kr; sh10@korea.ac.kr;<br>dain-kim@korea.ac.kr; taewhoo@korea.ac.kr; cwyoon99@korea.ac.kr;<br>jw_sohn@korea.ac.kr; donghee.choi@imperial.ac.uk;


#### Abstract

While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by $13.1 \%$, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by $13.4 \%$ and $9.8 \%$, respectively. Notably, it surpassed the passing threshold of the United States Medical Licensing Examination (USMLE) for the first time for a 7B-parameter model. Additionally, our system offered more detailed free-form responses to clinical queries compared


to existing 7B and 13B models, approaching the performance level of GPT-3.5. This significantly narrows the performance gap with large LMs, showcasing its effectiveness in addressing complex medical challenges.

Keywords: AI for medicine, small language model, chain-of-thought reasoning, synthetic data, USMLE

## 1 Introduction

Recent advancements in large language models (LM) suggest a promising future for the application of artificial intelligence (AI) in the field of healthcare and medicine, serving as efficient and rapid decision-making assistants for professionals [1, 2]. Several models have exceeded a passing threshold of $60 \%$ on the United States Medical Licensing Examination (USMLE) questions [3-7], recently reaching a remarkable accuracy rate of $90 \%$ [8]. Furthermore, their competency has been showcased in effectively addressing real-world clinical case challenges, including responding to clinical inquiries related to daily practices, engaging in conversational history-taking, and diagnosing complex clinical cases $[9-11]$.

Despite these achievements, applying large LMs to the medical field is hindered by privacy and security concerns [1, 12-14]. Proprietary models like OpenAI's GPT3.5 [15] and GPT-4 [16] can be prone to security vulnerabilities, as users are required to transmit and receive data through web-based APIs ${ }^{1}$ because of their closed-source nature. Managing sensitive patient data through services provided by commercial companies becomes a challenge in the absence of well-established regulations.

Recent efforts have been made to develop open-source models that can be deployed on in-house servers [17-19] and to specialize them for the biomedical and clinical domains [20-23]. However, the problem remains that these models lack the necessary multi-step reasoning capabilities to solve complex problems. In medicine, strong reasoning skills are particularly crucial for analyzing problems systematically, constructing logical paths, and accurately predicting answers. While commercial large LMs naturally exhibit this "chain-of-thought" (CoT) reasoning ability [24], thanks to their vast amount of parameters often exceeding 100 billion, smaller open-source LMs do not inherently acquire these abilities during pre-training [25-27]. This necessitates the development of an open-source model that possesses adequate medical reasoning capabilities to tackle complex medical problems.

In this study, we introduce Meerkat-7B, a novel open-source medical AI system with enhanced reasoning skills acquired from textbooks. Our model is built upon the state-of-the-art LM, Mistral-7B [18], and fine-tuned using a diverse set of carefully crafted data. Specifically, we used 9.3K USMLE-style questions with corresponding CoT reasoning paths from the MedQA dataset [28], along with $78 \mathrm{~K}$ high-quality synthetic CoT data generated from 18 medical textbooks. We also utilized instruction-following and chat datasets to address a wide range of use cases in[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-03.jpg?height=485&width=1310&top_left_y=326&top_left_x=333)

Fig. 1: Overview of recent advances in language models (LM) based on their performance on the MedQA benchmark [28]. Large closed-source models have surpassed the USMLE passing threshold, reaching a state-of-the-art performance with $90 \%$ accuracy [8]. On the other hand, the previous best open-source model, MediTron-70B [21], has achieved a score of only $70.2 \%$, while no 7B-scale models have managed to surpass the USMLE passing threshold ( $60 \%$ ). Our new open-source model, Meerkat-7B, has achieved an accuracy of $74.3 \%$, demonstrating notable progress in open-source model development in the medical domain.

this field. Meerkat-7B achieved an average accuracy of $64.2 \%$ across seven benchmarks, surpassing GPT-3.5 (175B) [15], MediTron-7B [21], and BioMistral-7B [22] by $13.1 \%, 13.4 \%$, and $9.8 \%$, respectively, and establishing a new state-of-the-art performance benchmark among open-source 7B models. Notably, Meerkat-7B achieved scores of 74.3 and $71.4 \%$ on the MedQA [28] and the USMLE Sample Test, marking the first instance where a 7B model surpassed the USMLE's passing threshold of $60 \%$ accuracy. Furthermore, when comparing the free-form responses of Meerkat-7B and baseline models to day-to-day clinical queries [29], our model delivered more detailed and comprehensive responses compared to existing models of sizes $7 \mathrm{~B}$ and 13B, showing comparable performance to GPT-3.5. We underscore that our Meerkat-7B model, along with the CoT fine-tuning approach, substantially narrowed the performance gap with commercial LMs, enabling smaller models to tackle challenging reasoning tasks (Figure 1). Our contributions are summarized as follows:

- We introduce Meerkat-7B, a novel medical AI system with high-level multi-step reasoning capabilities. Our model stands as the first 7B model to surpass the passing threshold of USMLE, while outperforming GPT-3.5 (175B), MediTron-7B, and BioMistral-7B by $13.1 \%, 13.4 \%$, and $9.8 \%$, respectively, across seven medical-domain benchmark datasets.
- We will publicly release all the artifacts including our model weights and training data online, which includes the CoT data for the MedQA questions and the new
![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-04.jpg?height=914&width=1282&top_left_y=334&top_left_x=430)

Fig. 2: Performance of models on seven multiple-choice QA benchmark datasets. Our Meerkat-7B models generally outperformed existing 7B models and GPT-3.5 and even outperformed MediTron-70B on MedQA. The scores of GPT-3.5, GPT-4 and MediTron-70B are obtained from the papers of Nori et al. [6], Toma et al. [30], Chen et al. [21], and Chen et al. [31]. Note that the performance of MediTron-70B has not been reported on the USMLE sample test, Medbullets-4, Medbullets-5, and JAMA Clinical Challenge. All benchmark tasks are formulated as multiple-choice question answering, with most having four options. The USMLE sample test has up to nine options, while Medbullets- 5 has five options.

MedBooks-CoT-18 dataset that comprises synthetic question-answer pairs with CoT reasoning paths extracted from 18 textbooks.

- To the best of our knowledge, Meerkat-7B represents the first instance of training a medical AI system using CoT data synthesized from raw textbooks and showing its effectiveness (see Methods for details).


## 2 Results

### 2.1 Main Results

### 2.1.1 USMLE

USMLE-style benchmarks have been commonly used as a fundamental testbed for evaluating LMs in the medical domain [3-8, 21, 22]. Figure 2 (a)-(d) shows Meerkat-7B significantly outperformed the previous best 7B models, MediTron-7B and BioMistral7B, and even outperformed GPT-3.5 and MediTron-70B despite its fewer parameters. Specifically, the model reached $70.6 \%$ and $74.3 \%$ on MedQA [28] in single-model and ensemble evaluation settings, respectively, significantly exceeding the passing threshold of $60 \%$. Additionally, our model achieved $71.4 \%, 61.0 \%$, and $55.3 \%$ on the USMLE sample test, Medbullets-4 (4 option), and Medbullets-5 (5 option) [31], respectively, surpassing both open-source models and GPT-3.5. ${ }^{2}$

### 2.1.2 MedMCQA and MMLU-Medical

MedMCQA [32] and MMLU-Medical [33] are widely used benchmarks consisting of multiple-choice exam questions. Most of these questions directly evaluate fundamental medical knowledge, which differs from USMLE-style questions. Figure 2 (e) and (f) demonstrates that Meerkat-7B achieved $60.7 \%$ and $72.4 \%$ on MedMCQA and MMLUMedical, respectively. The model outperformed MediTron-7B and achieved comparable performance to BioMistral-7B on MedMCQA, while it significantly surpassed both baseline models on MMLU-Medical. This highlights that while Meerkat-7B was trained using CoT extracted from USMLE-style questions (see Methods for details), its generalizability extends beyond USMLE to various types of medical exam questions. Please note that the scores in MMLU-Medical were calculated based on the average accuracies across six medical-related subjects in the original MMLU benchmark, and each result for a single subject is presented in Table A1.

### 2.1.3 JAMA Clinical Challenge

Evaluating models solely based on medical exam benchmarks may not sufficiently validate the practical utility of the models, because real clinical scenarios demand more specialized medical knowledge and complex reasoning abilities than typical medical exam questions. Therefore, we utilized the JAMA Clinical Challenge dataset [31], which consists of challenging and unusual real-world clinical cases spanning various medical domains, along with corresponding questions and answers. Figure 2 demonstrates that our 7B model achieved $54.0 \%$ accuracy, surpassing the performance of GPT-3.5 (50.1\%), MediTron-7B (49.3\%), and BioMistral-7B (48.6\%). This indicates that the reasoning abilities that Meerkat-7B has learned from USMLE problems are transferable to challenging clinical problems.[^1]

Table 1: The performance of models in providing long-form responses for everyday clinical questions in the K-QA dataset [29]. The completeness score (\%) measures how well the response includes essential content from the gold standard response for a given query. The factuality score (\%) evaluates whether the model's response contradicts the gold standard response for a given query (see Section 4.2 for details of the metrics). We chose not to assign a factuality score to Med-Alpaca, despite achieving a high factuality score, because it often generates irrelevant responses to the given query and these low-quality responses are not considered in the factuality score.

| Model | Size | Completeness | Factuality |
| :--- | ---: | :---: | :---: |
| GPT-4 | Unknown | 81.0 | 92.5 |
| GPT-3.5 | 175B | 71.4 | 92.0 |
| ChatDoctor [34] | $7 \mathrm{~B}$ | 63.0 | 89.1 |
| Mistral-Instruct [18] | $7 \mathrm{~B}$ | 62.4 | 88.1 |
| Med-Alpaca [35] | 13B | 6.8 | - |
| PMC-LLaMA [20] | 13B | 49.8 | 90.0 |
| Meerkat (Ours) | $7 \mathrm{~B}$ | 70.3 | 89.6 |

### 2.1.4 Real-world Clinical Questions

While exam questions are valuable for assessing the medical knowledge and reasoning abilities of language models, they may not fully reflect the complexities and breadth of real-world clinical queries. Therefore, we compared Meerkat-7B with instruction-tuned LMs using the K-QA dataset [29], which includes real-world clinical questions commonly encountered in clinicians' daily practices, along with corresponding long-form answers annotated by healthcare experts. Table 1 demonstrates that the completeness score of Meerkat-7B significantly surpassed that of all other opensource LMs and was comparable to GPT-3.5, indicating that our model provided detailed responses that encompass necessary parts for a given query. However, the model exhibits inferior factuality performance compared to GPT-3.5 and GPT-4, implying that the issue of hallucination in small models remains a persistent challenge. Tables A2, A3, and A4 present a case study comparing the responses of human experts, GPT-4, ChatDoctor-7B, and Meerkat-7B.

### 2.2 Ablation Study

### 2.2.1 Effect of CoT Fine-tuning

To assess the effectiveness of CoT fine-tuning, we compared the performance of models trained solely on question-answer pairs from the MedQA training set (referred to as "MedQA") with those trained on both MedQA data and CoT reasoning (referred to as "MedQA-CoT"). Figure 3 demonstrates that CoT fine-tuning dramatically improved MedQA performance by an average accuracy of $7.5 \%$ across the five models.

![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-07.jpg?height=403&width=1282&top_left_y=238&top_left_x=338)

Fig. 3: Performance comparison of five language models trained with three different datasets on the MedQA benchmark. Mistral-7B [18] and Gemma-7B [19] were the top performers, despite not being specialized models for biomedicine. "MedQA": training the model only using question-answer pairs in the MedQA training set. "MedQACoT": training the model using MedQA question-answer pairs and corresponding CoT reasoning data. "MedQA-CoT + MedBooks-CoT-18": training the model using the MedQA-CoT data and additional CoT data generated from textbooks. Note that Mistral-7B was used as our backbone model.

### 2.2.2 Effect of Textbook Augmentation

Figure 3 shows that augmenting the training data with additional QA pairs and CoT reasoning paths obtained from textbooks (referred to as "MedQA-CoT \& MedBooksCoT-18") led to a further improvement in performance, with an average accuracy increase of $5.4 \%$ across the five models compared to those trained using MedQA-CoT alone.

### 2.2.3 Language Model Selection

We assessed five open-source LMs with 7B parameters released between July 2023 and March 2024 using the MedQA dataset. As depicted in Figure 3, general-purpose models like Mistral-7B and Gemma-7B [19] outperformed biomedical-specific models such as MediTron-7B and BioMistral-7B. Although the details of their pretraining corpus remain unknown, we hypothesize that these models achieved high MedQA performance due to extensive training on diverse corpora, likely including a large amount of biomedical literature. While Mistral and Gemma exhibited similar performance, we selected Mistral-7B as our backbone model because of its faster inference speed [18].

### 2.3 Assessment of Model Explanations

We evaluated the quality of explanations for USMLE-style questions in the Medbullets4 dataset from Mistral-7B-Instruct, GPT-4, and Meerkat-7B by comparing them to human explanations. ${ }^{3}$ For evaluation, we employed two standard scoring metrics, ROUGE-L [36] and BERTScore [37]. Additionally, we calculated the GPT-4 score,[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-08.jpg?height=454&width=1240&top_left_y=276&top_left_x=428)

Fig. 4: Evaluation of model explanations for USMLE-style questions. the scores were measured by comparison with human explanations. Meerkat-7B performed the best according to ROUGE-L [36] and BERTScore [37], and it ranked second in terms of the GPT-4 score. "O" denotes explanations for questions that Meerkat-7B answered correctly, while "X" indicates explanations for questions the model answered incorrectly.

assigning a score from 1 to 5 to model explanations based on completeness, accuracy, clarity, relevance, and logical coherence, with human explanations assumed to receive a score of 5 . Figure 4 illustrates that Meerkat-7B exhibits the highest performance compared to the other instruct-tuned models based on ROUGE-L and BERTScore metrics, indicating that Meerkat-7B provided explanations that are lexically and semantically similar to human explanations. Based on the GPT-4 score, GPT-4 achieved the highest score, while Meerkat-7B fell short compared to GPT-4 but outperformed Mistral-7BInstruct. Furthermore, the score of explanations provided by Meerkat-7B for correctly answered questions was over 1.4 points higher than the score for explanations of incorrect answers, indicating that the quality of reasoning paths highly influences the accuracy of answers [38].

## 3 Discussion

In this study, we introduced a new medical AI system called Meerkat-7B. This model is a small LM with 7 billion parameters and will be openly available as open-source software, allowing it to run on-premise with relatively low-spec GPUs (e.g., a single 24G NVIDIA GeForce RTX 3090 GPU) in research labs and hospitals. The model was initialized with Mistral-7B weights and continually trained on various instruction-following datasets. One of our key contributions is the creation of the MedBooks-CoT-18 dataset, where we utilized GPT-4 to generate USMLE-style questions from medical textbooks and collected multi-step reasoning paths required to answer these questions. As a result of fine-tuning our model with synthetic CoT data, our model outperformed GPT-3.5, MediTron-7B, and BioMistral-7B by $13.1 \%, 13.4 \%$, and $9.8 \%$, respectively, across seven benchmarks. The improvement was most notable in MedQA, where our model achieved $74.3 \%$, surpassing the previous best of $70.2 \%$

Table 2: Statistics of our instruction-tuning datasets. "\# Examples" denotes the number of training examples for each dataset. $\dagger$ indicates that the dataset is newly constructed in this study. The total number of training examples is 460,625 .

| Target Application | Dataset | \# Examples |
| :---: | :---: | :---: |
| Multiple-choice QA | MedQA-CoT ${ }^{\dagger}[28]$ | 9,308 |
|  | MedBooks-CoT-18 $8^{\dagger}$ | 77,776 |
|  | MedMCQA [32] | 182,821 |
| Free-form QA | LiveQA [40] | 633 |
|  | MedicationQA [41] | 689 |
|  | ChatDoctor-cleaned $^{\dagger}$ [34] | 112,165 |
| Multi-turn Dialogue | MedQA-dialog ${ }^{\dagger}[28]$ | 4,839 |
| Clinical Note Generation | MTS-dialog [42] | 1,201 |
| Miscellaneous | AlpaCare [43] | 52,002 |

by MediTron-70B, pushing the state-of-the-art on this benchmark. Additionally, our model surpassed the passing threshold of USMLE for the first time for a 7B-parameter model.

As observed in Table 1, our model provided more detailed responses without sacrificing factuality compared to the baseline small models. However, when compared to larger LMs such as GPT-3.5 and GPT-4, it exhibited a noticeable decrease in factuality score, suggesting the need for further development towards more reliable AI systems in future research. Furthermore, given that our model was not fine-tuned using preference alignment techniques like reinforcement learning from human feedback (RLHF) [39], there is a possibility it could offer unsupported, unsafe, or biased responses. Hence, it is crucial to exercise caution and obtain expert validation before deploying the model in real-world scenarios to guarantee its reliability. Additionally, as evident from Table A1 and Figure 2, the discrepancies in encoded parametric knowledge between the 7B and 70B models appear to be noticeable, particularly in datasets such as MMLU-Medical and MedMCQA, where multi-step reasoning is relatively less essential compared to USMLE-style questions. In such cases, the effectiveness of our CoT fine-tuning approach also might be less impactful than expected. Our approach focuses on training the model to solve complex problems using its existing knowledge, rather than storing vast amounts of new medical knowledge in the model parameters. We believe that developing open-source LMs capable of effectively memorizing extensive medical knowledge or retrieval-augmented approaches can complement our fine-tuning training method and expect to validate this synergy in future research.

## 4 Methods

Meerkat-7B is a model based on Mistral-7B, ${ }^{4}$ specifically instruct-tuned for the medical domain. The key highlight of the model training lies in constructing a high-quality instruction-tuning dataset, which is elaborated in Section 4.1. Details of experimental settings including descriptions of benchmark datasets can be found in Section 4.2.[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-10.jpg?height=494&width=1297&top_left_y=238&top_left_x=425)

Fig. 5: The overall process of generating synthetic chain-of-thought (CoT) data. (1) GPT-4 was prompted to provide answers, along with step-by-step explanations, for USMLE-style questions from MedQA [28], resulting in the creation of $9.3 \mathrm{~K}$ CoT data. (2) GPT-4 received three randomly sampled questions from MedQA and text chunks from medical textbooks as input to produce synthetic question-answer pairs. (3) GPT-4 was then prompted to generate step-by-step explanations for these generated questions, resulting in an additional $78 \mathrm{~K}$ CoT data.

### 4.1 Training

The training data preparation involves three primary steps: generating high-quality CoT data from a question-answering (QA) dataset, augmenting this CoT data using medical textbooks, and reusing/repurposing pre-existing instruction-following datasets covering various medical use cases with suitable modifications. Table 2 lists the datasets used for training Meerkat-7B. Figure 5 depicts the overall process for generating and augmenting CoT data.

### 4.1.1 Chain-of-thought Reasoning Data Generation

Chain-of-thought (CoT) reasoning [24] refers to the process of providing step-by-step explanations to arrive at a conclusion for complex problems. Several studies have demonstrated that generating CoT using large LMs and then leveraging it to finetune smaller LMs is promising, but it remains understudied in the medical domain. We prompted GPT-4 to solve USMLE-style questions from the MedQA training set using step-by-step reasoning (see Table A5 for the input prompt). These questions are specifically designed to evaluate the multi-step medical reasoning skills of humans, making them suitable sources for obtaining CoT reasoning data. We kept questions if the responses followed the specified output format and the answer predictions were correct; otherwise, we filtered them out, resulting in $9.3 \mathrm{~K}$ out of $10 \mathrm{~K}$ questions remaining. We did not utilize explanations from human experts unlike the study of Singhal et al. [5] because GPT-4 offers more detailed explanations for the same questions compared to humans. Figure 6 provides examples of explanations from humans and GPT-4.

Let's solve this step-by-step, referring to authoritative sources as needed. Among the options, only pectoralis minor muscle origins from the outer surfaces of the 3rd to 5th ribs.

![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-11.jpg?height=177&width=208&top_left_y=503&top_left_x=1412)

The patient is a 22-year-old male marathon runner who complains of right-sided rib pain when running long distances. Physical examination notes an exhalation dysfunction at ribs 45 on the right side, indicating that those ribs are not moving down and in as they should during exhalation. Among the given options, we need to identify which muscle or muscle group could be targeted using a direct method to correct the exhalation dysfunction at ribs 4-5. (A) Anterior scalene is primarily involved in the elevation of the first rib, so it would not be directly involved in correcting an exhalation dysfunction at ribs 4-5. (B) Latissimus dorsi extends, adducts, and internally rotates the arm but has little influence on the rib movement, particularly at the level of ribs 4-5 during respiration. (C) Pectoralis minor can influence rib movement as it attaches to the coracoid process of the scapula and ribs 3-5. It plays a role in scapular stabilization and also can elevate the ribs when the scapula is fixed, aiding in inhalation. However, in the case of an exhalation dysfunction, this muscle's role would be less significant. (D) Quadratus lumborum, while involved in lateral flexion of the vertebral column and hiking the hip, does not have a primary role in rib movement during respiration. The serratus anterior and intercostal muscles are primarily involved in rib movement during respiration, but they are not listed in the options. Of the options provided, the pectoralis minor is the most likely candidate to assist in rib movement, particularly at the level of ribs 3-5, and could be utilized in a direct method to correct rib dysfunction during respiration.

Fig. 6: Examples of explanations provided by human experts and GPT-4 for a USMLE-style question. GPT-4 offers answers that are notably more detailed, comprehensive, and accurate compared to those from humans. The human explanation is sourced from the study of Singhal et al. [5].

### 4.1.2 Data Augmentation With Medical Textbooks

Collecting CoT paths solely from QA pairs within a single MedQA dataset does not provide a wide enough variety of training examples to maximize the reasoning abilities of small LMs. To overcome this obstacle, we constructed MedBooks-CoT-18, a dataset containing an additional $78 \mathrm{~K}$ question-answer pairs along with corresponding CoT paths, which are automatically generated from 18 English medical textbooks, spanning various medical disciplines. ${ }^{5}$ Initially, we segmented textbooks into chunks with an average character-level length of up to $4 \mathrm{~K}, 8 \mathrm{~K}$, and $12 \mathrm{~K}$, while allowing for overlap between chunks. Each text chunk was fed into GPT-4 with three USMLE-style questions sampled from MedQA for reference. GPT-4 was then prompted to generate[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_5c50262745ae9b457ec8g-12.jpg?height=651&width=631&top_left_y=254&top_left_x=450)

```
(Synthetic Question by GPT-4)
A newborn delivered at 35 weeks gestation via emergency
caesarean section is noted to have increased respiratory effort and
grunting. The infant's mother has a history of well-controlled
diabetes. On examination, the infant exhibits tachypnea and use of
accessory muscles for breathing. A chest x-ray reveals a diffuse
granularity of the lungs. Which of the following is the most likely
cause of the infant's respiratory distress?
(A) Inadequate surfactant levels
(B) Congenital heart disease
(C) Neonatal sepsis
(D) Pneumothorax
Answer: (A)
```

Fig. 7: Example of a question and answer generated from a textbook chunk [45]. Spans highlighted in the same color indicate related content. It's evident that GPT-4 heavily relies on the textbook to generate questions, resulting in a significant reduction in hallucination and ensuring that the generated QA examples cover essential content from the textbook. This leads to an overall improvement in quality.

the correct answer, plausible options, and questions for a case representation based on the provided textbook chunk, which included the patient's demographic information and symptoms (see Table A6 for prompt details). By instructing GPT-4 to generate questions based on textbooks, as depicted in Figure 7, we can reduce potential hallucinations during question generation and ensure that the questions reflect essential medical knowledge covered in the textbooks, relevant to both medical exams and realworld clinical practice. Additionally, we instructed the model to refrain from generating questions if the provided text chunk contained significant noise or lacked adequate information to create QA examples. After USMLE-style questions were newly created from textbooks, we followed a similar procedure to generate CoT reasoning paths using GPT-4 as we did with the MedQA dataset. We filtered out examples where GPT-4's predicted answers were different from the original answers provided by GPT-4, similar to the concept of round-trip consistency [44].

### 4.1.3 Utilization of Existing Instruction-following Datasets

In addition to the $\mathrm{CoT}$ datasets that we constructed, we incorporated existing instruction-following datasets into the model training to enhance the versatility of our model for various medical-domain applications (see Table 2 for the summary). We refined or repurposed several datasets to better suit the model training and align with the target applications. Below are detailed descriptions of each dataset:

- MedMCQA [32]: This large dataset comprises exam questions from the two Indian affiliations, AIIMS (All India Institute of Medical Sciences) and NEET PG (National

Eligibility cum Entrance Test for Post Graduate courses). We leveraged this dataset because it spans a broad spectrum of medical knowledge across 21 subjects, which could complement the medical knowledge of Mistral-7B. Although the dataset also includes human explanations for the questions, we did not utilize them because they were too brief and not sufficiently detailed.

- LiveQA [40]: This dataset contains healthcare-related questions received by the U.S. National Library of Medicine (NLM), accompanied by free-form responses from experts. The questions span various topics including diseases, drugs, and more, making it ideal for training our model on real-world queries.
- MedicationQA [41]: This dataset comprises consumer questions, particularly focusing on inquiries related to drugs, along with expert responses. Since these types of questions constitute a significant portion of healthcare inquiries, they serve as valuable resources for developing practical medical AI models.
- ChatDoctor-cleaned (ours): The data is derived from ChatDoctor [34], a collection of real patient inquiries and doctor responses obtained from an online medical consultation platform. While ChatDoctor provides rich and useful data examples, it also contains noise inherent to online $\mathrm{Q} \& \mathrm{~A}$ platforms, such as greetings or closing remarks by the doctors (e.g., they often begin the response with "Welcome to Chat Doctor" or end the response with "Best wishes, Chat Doctor."). To address this, we manually created three noisy inputs and corresponding corrected outputs, utilizing them as in-context examples. We then employed GPT-3.5 to remove noise from $112 \mathrm{~K}$ original responses, resulting in our ChatDoctor-cleaned dataset. See Table A7 for the input prompt.
- MedQA-dialogue (ours): While engaging in multi-turn dialogue with users is a crucial requirement for medical AI, there's a lack of suitable training datasets for this purpose. To fill this gap, we instructed GPT-3.5 to generate conversations by roleplaying as both patients and doctors based on MedQA questions and corresponding CoT reasoning. In the dialogue, the patient should minimally communicate their symptoms and medical history, while the doctor should guide the conversation, asking follow-up questions to gather a thorough medical history and records. We generated $4.8 \mathrm{~K}$ conversations corresponding to the intended output format from the entire 9.3K MedQA examples, creating the MedQA-dialogue dataset. Details of the input prompt is provided in Table A8.
- AlpaCare [43]: This dataset is a collection of synthetic $52 \mathrm{~K}$ medical-domain instructions generated by GPT-3.5 and GPT-4, similar to the self-instruct approach [46, 47]. We included this dataset to improve the model's generalizability to various user queries and use cases.


### 4.1.4 Training Details

We initialized the model with the Mistral-7V-v0.1 weights ${ }^{6}$ and fine-tuned the model on a combined dataset comprising the nine training datasets listed in Table 2, using a standard next-token prediction objective. The model was trained for three epochs with a batch size of 128 , a learning rate of $2 \mathrm{e}-6$, a warm-up ratio of 0.04 , and a[^5]

Table 3: Statistics of benchmark datasets and the evaluation metrics used. "\# Examples": the number of test examples for each dataset. The K-QA dataset requires free-form responses as answers, while the other datasets consist of multiple-choice QA problems where the correct answer must be chosen from given options.

| Type | Dataset | Metric | \# Examples |
| :--- | :--- | :--- | ---: |
| USMLE | MedQA [28] | Accuracy | 1,273 |
|  | USMLE sample test |  | 325 |
|  | Medbullets-4 [31] |  | 308 |
|  | Medbullets-5 [31] | Accuracy | 308 |
| Other exams | MedMCQA [32] | Average Accuracy | 4,182 |
|  | MMLU-Medical [33] |  | 1,089 |
|  | - Clinical knowledge |  | 265 |
|  | - Medical genetics |  | 100 |
|  | - Anatomy |  | 135 |
|  | - Professional medicine |  | 144 |
|  | - College biology |  | 173 |
| Clinical Challenges | JAMA Clincal Challenge [31] | Accuracy | 1,524 |
| Real-world Queries | K-QA [31] | Completeness | 201 |

maximum length of 2,048 tokens, using eight 80G A100 GPUs, which took approximately 1.5 days to complete. We used the FastChat GitHub repository ${ }^{7}$ and applied FlashAttention [48] and the fully sharded data parallel approach (FSDP) for efficiency.

### 4.2 Experimental Settings

### 4.2.1 Benchmark Datasets

We provide an overview of the benchmark datasets in Table 3 with detailed descriptions below.

- MedQA [28]: MedQA is one of the most widely used benchmarks in the medical domain. The dataset consists of USMLE-style questions curated by medical examination experts from various medical question banks (an example of USMLEstyle questions can be found in Figure 6). These questions are structured in a multiple-choice format, with four options provided for each question.
- USMLE sample test: ${ }^{8}$ This resource is an official study material for students preparing for the USMLE, closely mirroring the style and difficulty level of the actual tests. Each question is accompanied by a varying number of options, ranging from four to nine. We utilized the preprocessed version of this data as provided by Toma et al. $[30] .{ }^{9}$[^6]
- Medbullets [31]: This dataset comprises USMLE-style questions sourced from tweets posted since April 2022. Compared to questions in MedQA or the USMLE sample test, these questions are less likely to have been encountered during pre-training, making them more challenging to solve. We utilized both Medbullets-4, which provides four options, and Medbullets-5, which offers five options.
- MedMCQA [32]: This benchmark corresponds to the test split of the MedMCQA dataset, which consists of medical exam questions with four options. Note that we used the training split for model training, as described in Section 4.1.
- MMLU-Medical [33]: MMLU was originally designed to assess the world knowledge of models across various subjects including mathematics, physics, history, and law. Singhal et al. [4] created MMLU-Medical by extracting six subjects relevant to the medical field from MMLU, clinical knowledge, medical genetics, anatomy, professional medicine, college biology, and college medicine, aiming to evaluate medical-specialized systems.
- JAMA Clinical Challenge [31]: This dataset, sourced from the JAMA Network website, ${ }^{10}$ comprises long and complex clinical cases. Most questions involve diagnosing or determining the next steps based on the provided case. The dataset adopts a multiple-choice QA format, offering four options for each question.


### 4.2.2 Completeness and Factuality Evaluation

The K-QA dataset contains human-generated answers accompanied by detailed statements categorized as "must-have" or "nice to have," indicating whether they should be necessarily included in the answer or are additional but beneficial. Following previous studies [29, 49], we utilized these atomic facts to employ an evaluation approach based on natural language inference (NLI). We considered the model's response as the "premise" and each human statement as a "hypothesis." Using GPT-4, we assessed whether the premise entails or contradicts each hypothesis. ${ }^{11}$ The completeness score $S_{\text {comp }}$ evaluates how well the model's response entails the must-have statements associated with each question, and the factuality score $S_{\text {fact }}$ checks if the model's response contradicts any of the must-have or nice-to-have statements as follows:

$$
\begin{align*}
& S_{\text {comp }}\left(r_{i}, \mathcal{A}_{i}\right)=\sum_{a \in \mathcal{A}_{i}} \frac{\mathbf{1}\left[r_{i} \text { entails } a\right]}{\left|\mathcal{A}_{i}\right|} \\
& S_{\text {fact }}\left(r_{i}, \mathcal{A}_{i}\right)= \begin{cases}0 & \text { if } \exists a \in \mathcal{A}_{i} \text { such that } r_{i} \text { contradicts } a \\
1 & \text { otherwise }\end{cases} \tag{1}
\end{align*}
$$

where $r_{i}$ is the model response to the $i$-th question, $\mathcal{A}_{i}$ is the list of statements corresponding to the $i$-th questions, and $a$ is a statement. The indicator function $\mathbf{1}[$ cond $]$ returns 1 if the condition is true; otherwise 0 . Both completeness and factuality scores are averaged across all questions in the dataset.[^7]

### 4.2.3 Details of Baseline Models

In Figures 2 and 3, we trained LLaMA-7B and MediTron-7B with a learning rate of 2e5 for three epochs, while Mistral-7B, BioMistral-7B, and Gemma-7B were trained with a learning rate of $2 \mathrm{e}-6$ for three epochs. Other hyperparameters and computational environments were similar to those used for Meerkat-7B (described in Section 4.1.4). We observed significant variations in the models' performance depending on the learning rate and training epoch. Consequently, we selected the better-performing option between the learning rates of $2 \mathrm{e}-5$ and $2 \mathrm{e}-6$, and between three and five epochs. For ChatDoctor-7B, Mistral-7B-Instruct, Med-Alpaca-13B, and PMC-LLaMA-13B, we did not conduct further training as they were already instruction-tuned. Instead, we referred to the hyperparameters recommended by their respective authors and the official GitHub repositories to generate responses.

### 4.2.4 Meerkat-7B Inference

During inference, we applied BFloat16 and used a temperature of 0.7 , and a repetition penalty of 1.0 , provided by the Transformers library. In the ensemble evaluation, we utilized a choice shuffling ensemble technique [8], which involves randomizing the given options before presenting them to the models and subsequently conducting a majority vote to determine the final predictions. This helps mitigate potential biases in the position of the correct answer [50].

## References

[1] Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W.: Large language models in medicine. Nature medicine 29(8), 19301940 (2023)

[2] Tian, S., Jin, Q., Yeganova, L., Lai, P.-T., Zhu, Q., Chen, X., Yang, Y., Chen, Q., Kim, W., Comeau, D.C., et al.: Opportunities and challenges for chatgpt and large language models in biomedicine and health. Briefings in Bioinformatics 25(1), 493 (2024)

[3] Kung, T.H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., Madriaga, M., Aggabao, R., Diaz-Candido, G., Maningo, J., et al.: Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health 2(2), 0000198 (2023)

[4] Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al.: Large language models encode clinical knowledge. Nature 620(7972), 172-180 (2023)

[5] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., et al.: Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617 (2023)

[6] Nori, H., King, N., McKinney, S.M., Carignan, D., Horvitz, E.: Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 (2023)

[7] Brin, D., Sorin, V., Vaid, A., Soroush, A., Glicksberg, B.S., Charney, A.W., Nadkarni, G., Klang, E.: Comparing chatgpt and gpt-4 performance in usmle soft skill assessments. Scientific Reports 13(1), 16492 (2023)

[8] Nori, H., Lee, Y.T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N., Larson, J., Li, Y., Liu, W., et al.: Can generalist foundation models outcompete specialpurpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452 (2023)

[9] Zakka, C., Shad, R., Chaurasia, A., Dalal, A.R., Kim, J.L., Moor, M., Fong, R., Phillips, C., Alexander, K., Ashley, E., et al.: Almanac-retrieval-augmented language models for clinical medicine. NEJM AI 1(2), 2300068 (2024)

[10] Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg, J., Tanno, R., Wang, A., Li, B., Amin, M., Tomasev, N., et al.: Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654 (2024)

[11] Eriksen, A.V., Möller, S., Ryg, J.: Use of GPT-4 to diagnose complex clinical cases. Massachusetts Medical Society (2023)

[12] Li, X., Zhang, T.: An exploration on artificial intelligence application: From security, privacy and ethic perspective. In: 2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA), pp. 416-420 (2017). IEEE

[13] Bartoletti, I.: Ai in healthcare: Ethical and privacy challenges. In: Artificial Intelligence in Medicine: 17th Conference on Artificial Intelligence in Medicine, AIME 2019, Poznan, Poland, June 26-29, 2019, Proceedings 17, pp. 7-10 (2019). Springer

[14] Meskó, B., Topol, E.J.: The imperative for regulatory oversight of large language models (or generative ai) in healthcare. NPJ digital medicine $\mathbf{6}(1), 120$ (2023)

[15] OpenAI: Introducing chatgpt (2022)

[16] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)

[17] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)

[18] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b.

arXiv preprint arXiv:2310.06825 (2023)

[19] Google: Gemma: Introducing new state-of-the-art open models (2024)

[20] Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454 (2023)

[21] Chen, Z., Cano, A.H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., Köpf, A., Mohtashami, A., et al.: Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079 (2023)

[22] Labrak, Y., Bazoge, A., Morin, E., Gourraud, P.-A., Rouvier, M., Dufour, R.: Biomistral: A collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373 (2024)

[23] Xie, Q., Chen, Q., Chen, A., Peng, C., Hu, Y., Lin, F., Peng, X., Huang, J., Zhang, J., Keloth, V., et al.: Me llama: Foundation large language models for medical applications. arXiv preprint arXiv:2402.12749 (2024)

[24] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824-24837 (2022)

[25] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models. Transactions on Machine Learning Research (2022)

[26] Tay, Y., Dehghani, M., Tran, V.Q., Garcia, X., Bahri, D., Schuster, T., Zheng, H.S., Houlsby, N., Metzler, D.: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131 (2022)

[27] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)

[28] Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., Szolovits, P.: What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences 11(14), 6421 (2021)

[29] Manes, I., Ronn, N., Cohen, D., Ber, R.I., Horowitz-Kugler, Z., Stanovsky, G.: K-qa: A real-world medical q\&a benchmark. arXiv preprint arXiv:2401.14493 (2024)

[30] Toma, A., Lawler, P.R., Ba, J., Krishnan, R.G., Rubin, B.B., Wang, B.: Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding. arXiv preprint arXiv:2305.12031 (2023)

[31] Chen, H., Fang, Z., Singla, Y., Dredze, M.: Benchmarking large language models on answering and explaining challenging medical questions. arXiv preprint arXiv:2402.18060 (2024)

[32] Pal, A., Umapathi, L.K., Sankarasubbu, M.: Medmcqa: A large-scale multisubject multi-choice dataset for medical domain question answering. In: Conference on Health, Inference, and Learning, pp. 248-260 (2022). PMLR

[33] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020)

[34] Li, Y., Li, Z., Zhang, K., Dan, R., Jiang, S., Zhang, Y.: Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Cureus 15(6) (2023)

[35] Han, T., Adams, L.C., Papaioannou, J.-M., Grundmann, P., Oberhauser, T., Löser, A., Truhn, D., Bressem, K.K.: Medalpaca-an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247 (2023)

[36] Lin, C.-Y.: Rouge: A package for automatic evaluation of summaries. In: Text Summarization Branches Out, pp. 74-81 (2004)

[37] Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019)

[38] Zhang, M., Press, O., Merrill, W., Liu, A., Smith, N.A.: How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534 (2023)

[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. Advances in neural information processing systems 35, 27730-27744 (2022)

[40] Abacha, A.B., Agichtein, E., Pinter, Y., Demner-Fushman, D.: Overview of the medical question answering task at trec 2017 liveqa. In: TREC, pp. 1-12 (2017)

[41] Abacha, A.B., Mrabet, Y., Sharp, M., Goodwin, T.R., Shooshan, S.E., DemnerFushman, D.: Bridging the gap between consumers' medication questions and trusted answers. In: MedInfo, pp. 25-29 (2019)

[42] Abacha, A.B., Yim, W.-w., Fan, Y., Lin, T.: An empirical study of clinical note generation from doctor-patient encounters. In: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. $2283-2294$ (2023)

[43] Zhang, X., Tian, C., Yang, X., Chen, L., Li, Z., Petzold, L.R.: Alpacare: Instruction-tuned large language models for medical application. arXiv preprint arXiv:2310.14558 (2023)

[44] Alberti, C., Andor, D., Pitler, E., Devlin, J., Collins, M.: Synthetic QA corpora generation with roundtrip consistency. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6168-6173. Association for Computational Linguistics, Florence, Italy (2019). https://doi.org/10.18653/ v1/P19-1620 . https://aclanthology.org/P19-1620

[45] Sattar, H.A.: Fundamentals of Pathology: Medical Course and Step 1 Review. Pathoma.com, ??? (2020)

[46] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D., Hajishirzi, H.: Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022)

[47] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.: Stanford Alpaca: An Instruction-following LLaMA model. GitHub (2023)

[48] Dao, T., Fu, D., Ermon, S., Rudra, A., Ré, C.: Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35, 16344-16359 (2022)

[49] Min, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.-t., Koh, P.W., Iyyer, M., Zettlemoyer, L., Hajishirzi, H.: Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251 (2023)

[50] Ko, M., Lee, J., Kim, H., Kim, G., Kang, J.: Look at the first sentence: Position bias in question answering. arXiv preprint arXiv:2004.14602 (2020)

[51] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
