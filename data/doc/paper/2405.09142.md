# Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization 

Jenthe Thienpondt, Kris Demuynck<br>IDLab, Department of Electronics and Information Systems<br>Ghent University - imec, Belgium<br>jenthe.thienpondt@ugent.be


#### Abstract

Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected speech segments. In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD systems. Subsequently, speaker diarization can be performed efficiently by extracting the VAD logits and corresponding speaker embedding simultaneously, alleviating the need and computational overhead of an external VAD model. We provide an extensive analysis of the behavior of the frame-level attention system in current speaker verification models and propose a novel speaker diarization pipeline using ECAPA2 speaker embeddings for both VAD and embedding extraction. The proposed strategy gains state-of-the-art performance on the AMI, VoxConverse and DIHARD III diarization benchmarks.


## 1. Introduction

Speaker diarization tries to solve the question of 'Who spoke when?' of an audio file. Current state-of-the-art speaker diarization systems [1, 2] follow a cascaded approach by dividing the problem statement in several subtasks, usually consisting of voice activity detection (VAD), speaker embedding extraction and embedding clustering. While recent end-to-end diarization models provide a compelling and promising alternative [3, 4], current end-to-end approaches only prove competitive with cascaded systems in a limited set of scenarios [5].

Most VAD models rely heavily on artificially generated training data because of the limited availability of audio with frame-level speech/non-speech labels. For example, the VAD system provided in the popular SpeechBrain [6] library consists of a hybrid recurrent and convolutional neural network trained on a mixture of LibriSpeech [7] and the QUT-NOISE [8] augmentation dataset. In a similar fashion, the winners of the diarization track of the recent VoxCeleb Speaker Recognition Challenge (VoxSRC) relied on VoxCeleb1\&2 [9, 10] mixed with MUSAN [11] augmentations to train their VAD model [1]. While attempts are made by training VAD models directly on a combination of the available labeled diarization datasets $12 \boxed{13}$, performance degrades significantly on out-ofdomain test data [13].

In contrast to VAD models, speaker embedding extractors have access to a large number of labeled speech corpora, such as VoxCeleb1\&2. Recent advances in deep learning architectures are capable of exploiting this abundance of data, as shown by the popular x-vector [14] and ECAPA-TDNN [15] architectures based on time-delayed neural networks (TDNNs) or speech- adapted variations of ResNet models [16, 17]. Recently, hybrid models combining both architectures, such as CNN-TDNN [18] and ECAPA2 【19], have shown additional performance benefits.

In this paper, we propose a novel, weakly supervised approach for VAD which exploits the notion that the framelevel attention mechanism of a speaker embedding extractor shares the goal of a VAD model, i.e. indicating intelligible speech frames. An attention mechanism allows speaker embedding extractors to weight individual frame-level features before the temporal pooling operation, highlighting frames containing speaker information while suppressing non-speech or noisy frames. By interpreting the attention weights as framelevel VAD logits, the speaker embedding extractor can act as a weakly supervised VAD model, only relying on utterancelevel speaker labels during training. The potential benefits of this strategy are substantial compared to a traditional supervised VAD model. It alleviates the need of an (artificial) dataset with frame-level speech/non-speech labels, can learn from the large availability of speaker verification corpora, reduces the computational impact of the VAD step to zero when also used as embedding extractor for diarization and requires no or minimal modifications to current speaker verification architectures and training setups.

The rest of the paper is organized as follows: Section 2 describes the derivation of VAD logits from a frame-level attention mechanism in speaker embedding extractors. Section 3 analyzes these attention-based VAD logits from a pre-trained ECAPA2 model to asses robustness, resolution and behavior in a variety of speech scenarios. Section 4 uses these insights to propose a diarization pipeline using a single ECAPA2 model for both VAD and embedding extraction. Subsequently, Section 5 explains the experimental setup for testing the VAD, speaker embedding and final diarization performance separately with the corresponding results given in Section 6. Finally, Section 7 provides some concluding remarks.

## 2. Weakly supervised VAD

Current state-of-the-art speaker embedding extractors often employ a frame-level attention mechanism before the temporal pooling layer [15]. The usage of such attention systems is commonly motivated by its ability to highlight frame-level features containing speaker information and suppressing frames with less relevant information such as silence, noise or distorted speech. In this section, we start with a brief overview of the temporal- and channel-based attention mechanism used in stateof-the-art speaker embedding extractors such as ECAPA2 and how the attention responses can be transformed to VAD logits.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-2.jpg?height=466&width=1682&top_left_y=238&top_left_x=193)

Figure 1: Temporal-and channel-based attention mechanism of a speaker embedding extractor during training. The attention responses $\boldsymbol{E}$ can be interpreted as VAD logits, resulting in a weakly supervised VAD training setup only requiring utterance-level speaker labels.

### 2.1. Temporal- and channel-based attention

The simplest form of attentive statistical pooling in speaker verification architectures produces a singular attention scalar $e_{t}$ for each frame-level feature $\boldsymbol{h}_{t}$ at time step $t$ in the statistical pooling layer. This attention model can be extended to the channel dimension, as proposed in 【15】, by calculating a separate scalar $e_{t, c}$ for each frame-level feature channel $c$ :

$$
\begin{equation*}
e_{t, c}=\boldsymbol{p}_{c}^{T} f\left(\boldsymbol{W} \boldsymbol{h}_{t}+\boldsymbol{b}\right)+k_{c} \tag{1}
\end{equation*}
$$

with $\boldsymbol{W} \in \mathbb{R}^{R \times C}$ and $\boldsymbol{b} \in \mathbb{R}^{R \times 1}$ being the respective weights and bias of an intermediate linear bottleneck layer of dimension $R$ followed by the ReLU non-linearity given by $f($.$) .$ Subsequently, a linear operation with weights $\boldsymbol{p} \in \mathbb{R}^{R \times 1}$ and bias $k$ produces the attention scalar $e_{t, c}$ which is normalized across the temporal dimension by applying a softmax function:

$$
\begin{equation*}
\alpha_{t, c}=\frac{\exp \left(e_{t, c}\right)}{\sum_{t}^{T} \exp \left(e_{t, c}\right)} \tag{2}
\end{equation*}
$$

with $\alpha_{t, c}$ being the normalized attention scalar used to weight the frame- and channel-level feature $h_{t, c}$. The attentive mean pooled statistics $\tilde{\mu}_{c}$ for each channel $c$ can now be calculated as follows:

$$
\begin{equation*}
\tilde{\mu}_{c}=\sum_{t}^{T} \alpha_{t, c} h_{t, c} \tag{3}
\end{equation*}
$$

Correspondingly, the attentive standard deviation $\tilde{\sigma}_{c}$ is given by:

$$
\begin{equation*}
\tilde{\sigma}_{c}=\sqrt{\sum_{t}^{T} \alpha_{t, c} h_{t, c}^{2}-\tilde{\mu}_{c}^{2}} \tag{4}
\end{equation*}
$$

The concatenation of $\tilde{\boldsymbol{\mu}}$ and $\tilde{\boldsymbol{\sigma}}$ can subsequently be projected to a lower dimension using a linear layer to produce the final speaker embedding $\boldsymbol{d}$. Figure 1 depicts the temporal- and channel-based attention mechanism during the training phase of a speaker embedding extractor.

### 2.2. From attention to voice activity detection

The attentive scalars in $\boldsymbol{e}_{t}$ represent the importance of the corresponding frame-level feature located at $t$ to produce a robust speaker embedding. Consequently, the attention system should act similarly to a VAD model, assigning lower weights to silent, noisy and irrelevant frames while assigning higher weights to frames with distinguishable speech. Additionally, state-of-theart speaker embeddings are robust against a wide variety of background conditions, such as music, noise and babble interference, which should enable the model to distinguish between frames with speech in challenging conditions and true non-speech frames. To adapt the frame-level attention vector $\boldsymbol{e}_{t}$ for the VAD task, we calculate the average value across the channel dimension, resulting in one VAD logit $v_{t}$ for each time step:

$$
\begin{equation*}
v_{t}=\frac{\sum_{c}^{C} e_{t, c}}{C} \tag{5}
\end{equation*}
$$

The subsequent training setup results in a weakly supervised VAD model only depending on utterance-level speaker labels, which is a far less strict requirement as opposed to the need of frame-level speech/non-speech labels for supervised VAD training. Furthermore, the VAD logits $\boldsymbol{v}$ can be extracted simultaneously with the speaker embedding $\boldsymbol{d}$, at no additional computational cost. This can especially be beneficial when employed in a speaker diarization system, which will be explored in Section 4

## 3. Attention response analysis

In this section, we analyze the attention responses of a speaker embedding extractor to assess robustness, resolution and behavior on a variety of speech conditions to asses the feasibility of a weakly supervised VAD model in practical scenarios. For all analyses in this section, we use a pre-trained ECAPA2 speaker verification model as proposed in [19] which employs the attention mechanism described in Section 2 The model is trained using the development partition of VoxCeleb2, which provides audio from a wide range of background conditions and should result in a robust attention mechanism. More details about this model can be found in the accompanying paper [19].

### 3.1. Speech and non-speech responses

Figure 2 depicts the mean frame-level attention responses $\boldsymbol{v}$ provided by the ECAPA2 model given a variety of input audio sources. We observe that the frame-level attention responses behave similarly to a VAD model, with higher values assigned to speech frames and lower values to intermediate silence frames in the speech example. Noticeably, the attention system is able to distinguish speech from non-speech audio sources, consistently assigning low weights to frames with common background augmentations. However, we could also observe some
![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-3.jpg?height=602&width=786&top_left_y=250&top_left_x=201)

Car honking
![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-3.jpg?height=642&width=790&top_left_y=889&top_left_x=198)

Figure 2: ECAPA2 mean frame-level attention responses $v$ (green) on speech and non-speech audio inputs (blue). The $v$ values are capped between -0.3 and 0.3 for clarity. The red dotted line represents the optimal speech decision threshold. Notice that the attention system behaves similarly to a VAD model, without access to speech/non-speech labels during training.

errors. The attention weights have a tendency to assign higher values to frames at the onset of an audio source, even for nonspeech sounds, as depicted in the car honking example. This leads to some occasional classification errors of a small set of frames of non-speech sources.

### 3.2. Augmentation robustness

To assess the impact of common background audio on the ECAPA2 attention mechanism, we depict the mean $v_{t}$ value of speech frames from 1000 random LibriSpeech [7] utterances augmented with music, babble and noise audio at varying signal-to-noise (SNR) levels. Only frames with speech are included to prevent silent frames impacting the mean $v_{t}$.

We observe that noise and music augmentation behaves similarly, with a drastic decrease in the detection of speech frames when the SNR drops below -30, at which point the speaker becomes barely audible. Babble proves more challenging, as the presence of other speakers quickly results in a lower $v_{t}$ response. When interfering speakers become sufficiently

![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-3.jpg?height=616&width=808&top_left_y=246&top_left_x=1069)

Figure 3: Mean attention response $v_{t}$ from LibriSpeech samples augmented with noise, music and babble at different SNR ratios.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-3.jpg?height=596&width=791&top_left_y=1004&top_left_x=1084)

Figure 4: Mean attention response $v_{t}$ and standard deviation of frame-level features when transitioning from noise to speech.

prominent, the embedding extractor will have increased difficulty to determine a dominant speaker, ultimately assigning a low weight to the frame. As a result, the $v_{t}$ response will likely be less useful to determine overlapping speech frames.

### 3.3. Attentive region of influence

Frame-level features before the pooling operation in current state-of-the-art speaker embedding extractors have a wide receptive field due to a large number of stacked convolutional operations, amplified by striding or dilation. In comparison, typical VAD models have a more narrow receptive field due to the goal of providing frame-level speech probabilities, which can potentially be negatively impacted if the decision is based on a too wide input region. However, it is known that framelevel features of speaker embedding extractors are much more inclined to be influenced by the corresponding input features at the center of their receptive field [19].

We analyze the region of influence for the $v_{t}$ response in Figure 4. which depicts the average $v_{t}$ value and corresponding standard deviation of frames when transitioning from silence to speech of 1000 random LibriSpeech utterances. We observe

![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-4.jpg?height=443&width=1682&top_left_y=247&top_left_x=193)

Figure 5: Attention responses $v_{t}$ on TIMIT dataset, grouped by phoneme. 'spn' and 'sil' indicate speaker noise and silence, respectively.
![](https://cdn.mathpix.com/cropped/2024_06_04_d0593eb79225d4a27421g-4.jpg?height=532&width=806&top_left_y=836&top_left_x=196)

Figure 6: Single-step diarization with combined embedding and VAD extraction (left) and standard two-step diarization (right).

empirically that the region of influence of the $v_{t}$ value is on average $[t-20, t+20]$, with each frame corresponding to $10 \mathrm{~ms}$ of audio, and more heavily weights central input frames. This is significantly smaller than the corresponding receptive field of $v_{t}$ and proves favorable for the combination of fine-grained VAD and robust utterance-level speaker embeddings.

### 3.4. Phoneme response

A VAD model should be able to react similarly to vowels and consonants, although the latter is more challenging due to the transient and abrupt nature of consonants compared to vowels. To gain a better understanding on how the attention mechanism of a speaker embedding extractor reacts to different acoustic units, Figure 5 illustrates the average $v_{t}$ response, grouped by phoneme, of audio from the training partition of TIMIT [20].

We can observe that the attention system is significantly more inclined to put higher $v_{t}$ values to frames containing vowels in comparison to consonants. We suspect that vowels tend to carry more speaker-specific information. For example, the more defined formants of vowels should provide a better fingerprint of the vocal tract and corresponding speaker identity. Furthermore, there is a clear distinction between voiced and unvoiced consonants, with some unvoiced consonants given a similar speaker-identifying weight as silence (e.g. p). However, coarticulation of unvoiced consonants with other phonemes possibly mitigates the overall impact on practical VAD performance.

## 4. Single-step speaker diarization

As established in the previous section, the attention system of a speaker embedding extractor behaves similarly to a VAD model. This opens up the possibility to use a single speaker verification model for both VAD and embedding extraction, removing the computational overhead of the VAD phase. Subsequently, we propose a full diarization pipeline using a single speaker verification model for both VAD and embedding extraction, which we will refer to as single-step diarization.

Standard cascaded diarization performs frame-level VAD and speaker embedding extraction in separate steps, with the windowed embedding extraction done on the detected speech segments. To enable single-step VAD and embedding extraction, we obtain both the frame-level $v_{t}$ values and corresponding speaker embedding using a sliding window of width $w$ and step size $s$ over the input utterance. Subsequently, we determine the speech segments by averaging the overlapping $v_{t}$ values and performing hysteresis thresholding with starting and stopping thresholds $\theta_{\text {on }}$ and $\theta_{o f f}$, respectively. Afterwards, speaker embeddings from windows with no speech segments detected are discarded and not used in the clustering phase. When speaker labels are provided by the clustering algorithm, only the corresponding detected speech frames gets assigned to this speaker. Figure 6 depicts the difference between our single-step and standard two-step VAD and embedding extraction.

## 5. Experimental setup

We use the same ECAPA2 model as used in Section 2 for our experimental evaluation. VAD weights and speaker embeddings are extracted using the single-step method as described in Section 4 with a sliding window length of 2 seconds and step size of 1 second. Hysteresis thresholding is applied to determine the continuous speech segments. Afterwards, speech segments close together are merged and short segments are removed. All thresholds are determined using the development partitions of the diarization datasets.

Spectral clustering is used to group the extracted embeddings and assign labels to the corresponding speech segments. Only speaker embeddings containing speech in the previous step are included. The affinity matrix is constructed using the cosine similarity of the speaker embeddings. We only keep the 10 highest similarities for each row in the affinity matrix for robustness. Subsequently, the eigenvectors and eigenvalues of the normalized Laplacian, derived from the pruned similarity matrix, are calculated. To estimate the number of speakers $n$ present in the input utterance, we use the eigengap crite-

Table 1: VAD error rate on various test sets using the attention responses of ECAPA2 in comparison with other publicly available supervised (SV) VAD models. Bold indicates best VAD performance of models not using in-domain training data.

| System | SV | In-domain | AMI (array) |  |  | AMI (headset) |  |  | VoxConverse |  |  | DIHARD III |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $\overline{\text { FA }}$ | MS | $\overline{\text { VAD }}$ | $\overline{F A}$ | MS | VAD | $\overline{F A}$ | MS | $\overline{\text { VAD }}$ | FA | MS | VAD |
| SpeechBrain | $\checkmark$ | - | 4.3 | 1.4 | 5.7 | 3.5 | 1.5 | 5.0 | 2.2 | 1.7 | 3.9 | 17.8 | 3.6 | 21.4 |
| Silero | $\checkmark$ | - | 3.6 | 2.3 | 5.9 | 3.3 | 1.5 | 5.2 | 2.4 | 1.9 | 4.3 | 11.0 | 9.1 | 20.1 |
| PyAnnote 3.1 | $\checkmark$ | $\checkmark$ | 1.9 | 2.4 | 4.3 | 2.0 | 1.1 | 3.1 | 1.6 | 0.6 | 2.2 | 3.9 | 3.2 | 7.1 |
| ECAPA2 (VAD only) | - | - | 3.0 | 2.6 | 5.6 | 3.0 | 1.9 | 4.9 | 1.2 | 1.5 | 2.7 | 8.3 | 9.9 | 18.2 |

Table 2: Diarization error rate on various test sets using ECAPA2 in comparison to other published non-fusion and single-scale diarization results using oracle VAD. Results are given with and without usage of the oracle number of speakers.

| System | AMI (array) |  | AMI (lapel) |  | AMI (headset) |  | VoxConverse |  | DIHARD III |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | oracle | estimate | oracle | estimate | oracle | estimate | oracle | estimate | oracle | estimate |
| xvec+ClusterGAN [21] | 3.6 | 2.8 | - | - | - | - | - | - | - | - |
| ECAPA-TDNN [22] | 2.8 | 3.0 | 2.3 | 2.5 | 1.7 | 4.0 | - | - | - | - |
| TitaNet-L [23] | - | - | 2.0 | 2.0 | 1.7 | 1.9 | - | - | - | - |
| DR-DESA $[24]$ | - | - | - | - | - | - | - | 4.4 | - | 15.0 |
| ECAPA2 | 2.0 | 1.8 | 1.8 | 1.7 | 1.5 | 1.4 | 4.2 | 3.8 | 13.8 | 14.1 |

rion [25]. Finally, the $n$ first eigenvectors are used for K-means clustering to determine the speaker labels.

We use the evaluation partitions of the AMI, VoxConverse and DIHARD III diarization datasets to asses the performance of our proposed method. The AMI dataset consists of meeting data while VoxConverse contains multi-speaker audio collected from YouTube. DIHARD III is composed of multiple challenging domains, ranging from clinical interviews to telephone speech [26]. For AMI, we follow the standard evaluation setup as described in [22] by using the "Full-corpus-ASR partition" and exclude the TNO meetings for both the development and test set. The audio from the microphone array is beamformed using the open-source BeamformIt toolkit [27]. The diarization and VAD error rate is measured on the non-overlapping speech regions while using a forgiveness collar value of 0.25 seconds. For VoxConverse and DIHARD III, we follow the official corresponding challenge setups [1, 26] by evaluating all speech regions and applying a collar value of 0.25 and 0 seconds, respectively. We note that our approach currently does not handle overlapping speech, imposing a penalty on VoxConverse and DIHARD III results. The VAD error rate is reported with false alaram (FA) and missed detection (MS) breakdown.

## 6. Results

Table 1 depicts the VAD error rate on all test sets of our attention-based VAD approach and other publicly available VAD models. Notably, we see that the ECAPA2 VAD system gains similar or better results in comparison to supervised models from the SpeechBrain and Silero frameworks. This indicates that the attention mechanism of a speaker embedding extractor can act as a weakly supervised VAD system with on par or better performance than comparable supervised VAD models. For reference, we also included the results of the popular PyAnnote 13 ] library, which outperforms all other models. However, it is important to notice that the PyAnnote model is trained using the in-domain development partitions of the AMI, VoxConverse and DIHARD III datasets, resulting in significant performance increases on the corresponding test sets [13].

To gain an individual assessment on the diarization task of ECAPA2 speaker embeddings combined with spectral clustering, DER results using oracle VAD on the AMI and VoxConverse test sets are available in Table 2, along with the best published performance of current state-of-the-art models. For a fair comparison, we only included results of non-fusion and singlescale diarization systems. We observe that our ECAPA2-based system improves the DER on average over the best models with $20.4 \%$ and $13.7 \%$ relative on all test sets, respectively. This illustrates that the state-of-the-art speaker verification performance of the ECAPA2 embeddings results in an accompanying increase in diarization robustness. Notably, our ECAPA2 embedding extractor achieves these results while being trained on a significantly smaller training set than all other models in Table 2 We also observe that estimating the number of speaker clusters using the eigengap criterion often results in slightly better performance when compared to using the oracle number of speakers. We noted that speaker embeddings from challenging audio conditions tend to create an additional noisy cluster, preventing them from interfering with the true speaker clusters. Usage of the oracle number of speakers enforces the algorithm to assign noisy embeddings to speakers, lowering the overall robustness of the clustering results.

## 7. Conclusion

In this paper, we provided an extensive analysis of the behavior of frame-level attention weights of a speaker verification model. We established that the attention mechanism acts similarly to a weakly supervised VAD system and achieves similar or better performance as comparable supervised VAD models. Subsequently, we proposed a diarization approach which extracts the VAD weights and speaker embedding simultaneously using a single speaker verification model, providing a compelling and efficient alternative to standard cascaded speaker diarization.

## 8. References

[1] Ze Li, Yuke Lin, Xiaoyi Qin, Ning Jiang, Guoqing Zhao, and Ming Li, "The dku-msxf speaker verification system for the voxceleb speaker recognition challenge 2023," VoxCeleb Speaker Recognition Challenge 2023 Workshop, 2023.

[2] Grigor Kirakosyan Davit Karamyan, "The krisp diarization system for the voxceleb speaker recognition challenge 2023," VoxCeleb Speaker Recognition Challenge 2023 Workshop, 2023.

[3] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Nagamatsu, and Shinji Watanabe, "End-to-End Neural Speaker Diarization with Permutation-free Objectives," in Proc. INTERSPEECH 2019, 2019, pp. 4300-4304.

[4] Federico Landini, Alicia Lozano-Diez, Mireia Diez, and Lukáš Burget, "From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization," in Proc. INTERSPEECH 2022, 2022, pp. 5095-5099.

[5] Federico Landini, Mireia Diez, Alicia Lozano-Diez, and Lukáš Burget, "Multi-speaker and wide-band simulated conversations as training data for end-to-end neural diarization," in ICASSP 2023, 2023, pp. 1-5.

[6] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, François Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio, "SpeechBrain: A general-purpose speech toolkit," 2021, arXiv:2106.04624.

[7] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: An asr corpus based on public domain audio books," in Proc. ICASSP, 2015, pp. 5206-5210.

[8] David Dean, Sridha Sridharan, Robbie Vogt, and Michael Mason, "The qut-noise-timit corpus for the evaluation of voice activity detection algorithms," in Proc. INTERSPEECH 2010, 2010.

[9] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman, "VoxCeleb: A large-scale speaker identification dataset," in INTERSPEECH 2017, 2017, pp. 2616-2620.

[10] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman, "VoxCeleb2: Deep speaker recognition," in INTERSPEECH 2018, 2018, pp. 1086-1090.

[11] David Snyder, Guoguo Chen, and Daniel Povey, "Musan: A music, speech, and noise corpus," arXiv preprint arXiv:1510.08484, 2015.

[12] Hervé Bredin and Antoine Laurent, "End-to-end speaker segmentation for overlap-aware resegmentation," in Proc. INTERSPEECH 2021, 08 2021, pp. 3111-3115.

[13] Alexis Plaquet and Hervé Bredin, "Powerset multi-class cross entropy loss for neural speaker diarization," in Proc. INTERSPEECH 2023, 2023.

[14] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, "X-vectors: Robust dnn embeddings for speaker recognition," in ICASSP 2018, 2018, pp. 5329-5333.
[15] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification," in INTERSPEECH 2020, 2020, pp. 3830-3834.

[16] Jenthe Thienpondt, Brecht Desplanques, and Kris Demuynck, "Integrating frequency translational invariance in TDNNs and frequency positional information in 2D ResNets to enhance speaker verification," in INTER SPEECH 2021, 2021, pp. 2302-2306.

[17] D. Garcia-Romero, Greg Sell, and A. McCree, "Magneto: X-vector magnitude estimation network plus offset for improved speaker recognition," in Proc. Odyssey 2020, 2020, pp. 1-8.

[18] Zhuxin Chen, Duisheng Chen, Hanyu Ding, and Yue Lin, "The NetEase Games system description for textdependent sub-challenge of SDSVC 2020,".

[19] Jenthe Thienpondt and Kris Demuynck, "Ecapa2: A hybrid neural network architecture and training strategy for robust speaker embeddings," in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023, pp. 1-8.

[20] J. Garofolo, Lori Lamel, W. Fisher, Jonathan Fiscus, D. Pallett, N. Dahlgren, and V. Zue, "Timit acousticphonetic continuous speech corpus," Linguistic Data Consortium, 111992

[21] Monisankha Pal, Manoj Kumar, Raghuveer Peri, Tae Jin Park, So Hyun Kim, Catherine Lord, Somer L. Bishop, and Shrikanth S. Narayanan, "Meta-learning with latent space clustering in generative adversarial network for speaker diarization," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 12041219,2020 .

[22] Nauman Dawalatabad, Mirco Ravanelli, Franccois Grondin, Jenthe Thienpondt, Brecht Desplanques, and Hwidong Na, "Ecapa-tdnn embeddings for speaker diarization," in Proc. INTERSPEECH 2021, 2021.

[23] Nithin Rao Koluguri, Taejin Park, and Boris Ginsburg, "Titanet: Neural model for speaker representation with $1 \mathrm{~d}$ depth-wise separable convolutions and global context," in ICASSP 2022, 2022, pp. 8102-8106.

[24] You Jin Kim, Hee-Soo Heo, Jee-Weon Jung, Youngki Kwon, Bong-Jin Lee, and Joon Son Chung, "Advancing the dimensionality reduction of speaker embeddings for speaker diarisation: Disentangling noise and informing speech activity," in ICASSP 2023, 2023, pp. 1-5.

[25] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss, "On spectral clustering: Analysis and an algorithm," in $A D$ VANCES IN NEURAL INFORMATION PROCESSING SYSTEMS. 2001, pp. 849-856, MIT Press.

[26] Neville Ryant, Prachi Singh, Venkat Krishnamohan, Rajat Varma, Kenneth Ward Church, Christopher Cieri, Jun Du, Sriram Ganapathy, and Mark Y. Liberman, "The third dihard diarization challenge," ArXiv, vol. abs/2012.01477, 2020.

[27] X. Anguera, C. Wooters, and J. Hernando, "Acoustic beamforming for speaker diarization of meetings," IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 2011-2021, September 2007.

