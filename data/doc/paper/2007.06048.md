# Minimod: A Finite Difference solver for Seismic Modeling 

Jie Meng* Andreas Atle ${ }^{*} \quad$ Henri Calandra ${ }^{\dagger} \quad$ Mauricio Araya-Polo*

July 14,2020


#### Abstract

This article introduces a benchmark application for seismic modeling using finite difference method, which is named MiniMod, a mini application for seismic modeling. The purpose is to provide a benchmark suite that is, on one hand easy to build and adapt to the state of the art in programming models and changing high performance hardware landscape. On the other hand, the intention is to have a proxy application to actual production geophysical exploration workloads for Oil \& Gas exploration, and other geosciences applications based on the wave equation. From top to bottom, we describe the design concepts, algorithms, code structure of the application, and present the benchmark results on different current computer architectures.


## 1 Introduction

Minimod is a Finite Difference-based proxy application which implements seismic modeling (see Chapter 2) with different approximations of the wave equation (see Chapter 3). Minimod is selfcontained and designed to be portable across multiple High Performance Computing (HPC) platforms. The application suite provides both non-optimized and optimized versions of computational kernels for targeted platforms (see Chapter 5). The target specific kernels are provided in order to conduct benchmarking and comparisons for emerging new hardware and programming technologies.

Minimod is designed to:

- Be portable across multiple software stacks.
- Be self-contained.
- Provide non-optimized version of the computational kernels.
- Provide optimized version of computational kernels for targeted platforms.
- Evaluate node-level parallel performance.
- Evaluate distributed-level parallel performance.

${ }^{*}$ Total EP R\&T, email: jie.meng@total.com

${ }^{\dagger}$ Total S.A.

The first four items are covered in Section 5 and the remainder items are covered in Section 6 .

New HPC technologies evaluation is a constant task that plays a key role when decisions are taken in terms of computing capacity acquisitions. Evaluations in the form of benchmarking provide information to compare competing technologies wrt relevant workloads. Minimig is also use for this purpose, and insight collected with it has been part the last major acquisitions by Total (see Figure 1 .

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-02.jpg?height=952&width=1377&top_left_y=611&top_left_x=379)

Figure 1: Evolution of computing capacity wrt geophysical algorithms.

It can be observed in Figure 1, how more complex geophysical algorithms drive larger capacity installations in Total. The main driver of performance demand by the geophysical algorithms presented in figure are: the accuracy of the wave equation approximation and the addition of optimization or inverse problem schemes. The former is captured in Minimig, where the later is out of scope of this article. Performance trends obtained by conducting experiments with Minimig (or similar tools) influenced the decisions for the last ten years, this mainly motivated by the transition of the main workloads from Ray-based to wave-based methods.

## 2 Seismic Modeling

Seismic depth imaging is the main tool used to extract information describing the geological structures of the subsurface from recorded seismic data, it is effective till certain depth after which it becomes inaccurate. At its core it is an inverse problem which consists in finding the best model minimizing the distance between the observed data (recorded seismic data) and the predicted data (produced by computational means). The process to estimate the predicted data is known as forward modeling. It is based on the resolution of the wave equation for artificial perturbations of the subsurface given initial and boundary conditions. This simulation is repeated as many times as perturbations were introduced during seismic data acquisition. In Figure 2 on of such experiments is represented, in this case for a marine setup. The perturbation (namely source) is introduce by an airgun dragged behind a ship, then the waves propagate through the medium. At each interface between layers of materials with different characteristics part of the energy is reflected. These reflections are recorded at sea level (at surface for a onshore setup) by a network of sensors (in the figure depicted in red) also pulled by the ship.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-03.jpg?height=531&width=957&top_left_y=976&top_left_x=584)

Figure 2: The mechanical medium represented by the subsurface model is perturbed and the wave propagation simulated. In this case, waves are depicted in discretized form as rays, for simplicity.

Solving the forward modeling efficiently is crucial for geophysical imaging as one needs to get solutions for many sources and many iterations as we progressively the subsurface model improves. Constant progresses in data acquisition and in rocks physics labs, more powerful computers and integrated team including physicists, applied mathematicians and computer scientists have greatly contributed to the development of advanced numerical algorithms integrating more and more complex physics. For the last 20 years, the field has been very active in the definition and introduction of different wave equation approximations and corresponding numerical methods for solving forward problem. But the real change came with the implementation of the full wave equation, thanks to the petascale era in HPC, giving access to a complete representation of the wavefield. It allowed geo-scientist to re-design imaging algorithm both in time dynamic and time harmonic domain. The most popular numerical scheme used, nowadays by the industry, is based on finite difference methods (FDM) on regular grids [9], 4]. We refer to [19] for examples of FDM in the geophysics frameworks and to [8] for 3D applications.

## 3 Finite Differences

Various numerical methods have been explored for the modeling of seismic wave propagation including the finite difference, finite element, finite volume and hybrid methods. Among those methods, the finite difference method is the most popular one for its simplicity and easy and straightforward implementation.

The first step of implementing the governing equations is to called discretizations, basically consist on write the equations on forms that allow the direct implementation of differential operators. The discretizations of the governing equations are impose on three different kind of grids, depending on the symmetry of the problem. We use the standard collocated grid, and two versions of staggered grid, namely Yee [21, 17], 18] and Lebedev [11].

The first equation to be described is the second order acoustic wave equation with constant density, solving for the pressure wavefield $p$,

$$
\begin{equation*}
\frac{1}{v_{p}^{2}} \frac{\partial^{2} p(\mathbf{x}, t)}{\partial t^{2}}-\nabla^{2} p(\mathbf{x}, t)=f(\mathbf{x}, t) \tag{1}
\end{equation*}
$$

where $v_{p}$ is the velocity of the pressure wavefield, $p(\mathbf{x}, t)$ expanded to $3 \mathrm{D}$ domain is $p(x, y, z, t)$, likewise for the source $f(\mathbf{x}, t)=f(x, y, z, t)$.

The second equation is the first order acoustic wave equation with variable density $\rho$,

$$
\begin{equation*}
\frac{1}{\rho v_{p}^{2}} \frac{\partial p(\mathbf{x}, t)}{\partial t}-\nabla \cdot \mathbf{v}(\mathbf{x}, t)=f(\mathbf{x}, t), \quad \rho \frac{\partial \mathbf{v}(\mathbf{x}, t)}{\partial t}-\nabla p(\mathbf{x}, t)=0 \tag{2}
\end{equation*}
$$

where $p$ is the pressure wavefield, and $\mathbf{v}$ is a vector wavefield for the particle velocities (time derivatives of displacement) along the different coordinate axis.

The third equation is the acoustic transversely isotropic first order system, see 3] for details.

Finally, we have the elastic equations with variable density $\rho$,

$$
\begin{equation*}
\frac{\partial \boldsymbol{\sigma}(\mathbf{x}, t)}{\partial t}-C D \mathbf{v}(\mathbf{x}, t)=\mathbf{f}(\mathbf{x}, t), \quad \rho \frac{\partial \mathbf{v}(\mathbf{x}, t)}{\partial t}-D^{t} \boldsymbol{\sigma}(\mathbf{x}, t)=0 \tag{3}
\end{equation*}
$$

where $\boldsymbol{\sigma}$ is a vector wavefield for the stresses using Voigt notation and $\mathbf{v}$ is a vector wavefield for the particle velocities. The derivative operator $D$ is

$$
D=\left(\begin{array}{ccc}
\frac{\partial}{\partial x} & &  \tag{4}\\
& \frac{\partial}{\partial y} & \\
& \frac{\partial}{\partial z} & \frac{\partial}{\partial z} \\
\frac{\partial}{\partial z} & & \frac{\partial}{\partial x} \\
\frac{\partial}{\partial y} & \frac{\partial}{\partial x} &
\end{array}\right)
$$

and $D^{t}$ is the transpose of $D$ without transpose of the derivatives. This is a subtle difference since a derivative is anti-symmetric. We have two different symmetry classes, isotropic and transversely
isotropic, which only differs in the sparsity pattern of the stiffness tensor $C$.

The above described discretizations are implemented with the following names as kernels:

- Acoustic_iso_cd: Standard second order acoustic wave-propagation in isotropic media with constant density.
- Acoustic_iso: first order acoustic wave-propagation in isotropic media on a staggered Yee-grid variable density.
- Acoustic_tti: first order acoustic wave-propagation in transversely isotropic media on a staggered Lebedev-grid.
- Elastic_iso: first order elastic wave-propagation in isotropic media on a staggered Yee-grid.
- Elastic_tti: first order elastic wave-propagation in transversely isotropic media on a staggered Lebedev-grid.
- Elastic_tti_approx: Non-standard first order elastic wave-propagation in transversely isotropic media on a staggered Yee-grid

All discretizations use CPML 10 at the boundary of the computational domain, with the option of using free surface boundary conditions at the surface. Full unroll of the discretization is given for acoustic_iso_cd, as example, this is the simplest kernel in Minimod for simulating acoustic wavepropagation in isotropic media with a constant density domain, i.e. equation (1). The equation is discretized in time using a second-order centered stencil, resulting in the semi-discretized equation:

$$
\begin{equation*}
p^{n+1}-Q p^{n}+p^{n-1}=\left(\Delta t^{2}\right) v_{p}^{2} f^{n} \tag{5}
\end{equation*}
$$

where

$$
Q=2+\Delta t^{2} v_{p}^{2} \nabla^{2}
$$

The equation is discretized in space using a 25 -point stencil in space, with nine points in each direction of three dimensions:

$$
\begin{array}{r}
\nabla^{2} p(x, y, z) \approx \sum_{m=1}^{4} c_{x m}[p(i+m, j, k)+p(i-m, j, k)-2 p(i, j, k)] \\
c_{y m}[p(i, j+m, k)+p(i, j-m, k)-2 p(i, j, k)] \\
c_{z m}[p(i, j, k+m)+p(i, j, k-m)-2 p(i, j, k)]
\end{array}++
$$

where $c_{x m}, c_{y m}$ and $c_{z m}$ are discretization parameters that approximates second derivatives in the different spatial directions. The parameters can be derived from the Taylor expansion of the derivatives in the $\mathrm{x}, \mathrm{y}$ and $\mathrm{z}$-direction respectively, where the approximation would be of order 8 . The derivatives can also use optimized stencils, that reduce the dispersion error at the expense of formal order.

## 4 Computing costs

Being the core algorithm of Finite Difference, stencil-based computation algorithms represent the kernels of many well-known scientific applications, such as geophysics and weather forecasting.

However, the peak performance of stencil-based algorithms are limited because of the imbalance between computing capacity of processors units and data transfer throughput of memory architectures. In Figure 3 the memory access problem is shown. The computing part of the problem is basically the low re-use of the memory accessed elements.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-06.jpg?height=648&width=873&top_left_y=711&top_left_x=626)

Figure 3: Memory layout for an simple stencil example, to access the required values multiple useless cache lines (bottom) need to be accessed with incurred penalties. Figure extracted from [6].

In order to deal with the above described limitation, a great amount of research have been devoted to optimize stencil computations to achieve higher performance. For example, de la Cruz and Araya-Polo proposed the semi-stencil algorithm 6] which improves memory access pattern and efficiently reuses accessed data by dividing the computation into several updates. Rivera et al. [16] showed that tiling 3D stencil combined with array padding could significantly reduce miss rates and achieve performance improvements for key scientific kernels. Recently, Nguyen et al. [14] introduced higher dimension cache optimizations.

Advanced programming models have been explored to improve stencil performance and productivity. In 2012, Ghosh et al. 77 analyzed the performance and programmability of three high-level directive-based GPU programming models (PGI, CAPS, and OpenACC) on an NVIDIA GPU for kernels of the same type as described in previous sections and for Reverse Time Migration (RTM, [1), widely used method in geophysics. In 2017, Qawasmeh et al. [15] implemented an MPI plus OpenACC approach for seismic modeling and RTM. Domain-specific languages (DSLs) for stencil algorithms have also been proposed. For example, Louboutin et al. introduced Devito [12], which a new domain-specific language for implementing differential equation solvers. Also, de la Cruz and Araya-Polo proposed an accurate performance model for a wide range of stencil sizes which captures the behavior of such 3D stencil computation pattern using platform parameters [5].

## 5 Minimod Description

### 5.1 Source Code Structure

In this section, we introduce the basic structure of the source code in Minimod. As we described in Section 3, the simulation in Minimod consists of solving the wave equation, the temporal requires the spatial part of the equation to be solve at each timestep for some number of timesteps. The pseudo-code of the algorithm is shown in algorithm 1, for the second order isotropic constant density equation. We apply a Perfectly Matched Layer (PML) 2] boundary condition to the boundary regions. The resulting domain consists of an "inner" region where Equation 5 is applied, and the outer "boundary" region where a PML calculation is applied.

```
Data: f: source
Result: $\mathbf{p}^{n}$ : wavefield at timestep $n$, for $n \leftarrow 1$ to $T$
$1 \mathbf{p}^{0}:=0$;
for $n \leftarrow 1$ to $T$ do
    for each point in wavefield $\mathbf{u}^{n}$ do
        Solve Eq. 5 (left hand side) for wavefield $\mathbf{p}^{n}$;
    end
    $\mathbf{p}^{n}=\mathbf{p}^{n}+\mathbf{f}^{n}$ (Eq. 5 right hand side);
end
```

Algorithm 1: Minimod high-level description

As described in algorithm 1, the most computationally expensive component of minimod is the computation of the wavefield for each point. We list the code structure of the wavefield calculation in algorithm 2.

```
Data: $p^{n-1}, p^{n-2}$ : wavefields at previous two timsteps
Result: $p^{n}$ : wavefield at current timestep
for $i \leftarrow \mathrm{xmin}$ to xmax do
    if $i \geq \mathrm{x} 3$ and $i \leq \mathrm{x} 4$ then
        for $j \leftarrow$ ymin to ymax do
            if $j \geq \mathrm{y} 3$ and $j \leq \mathrm{y} 4$ then
                // Bottom Damping (i, j, z1...z2)
                // Inner Computation (i, j, z3...z4)
                // Top Damping (i, j, z5...z6)
            else
                // Back and Front Damping (i, j, zmin...zmax)
            end
        end
    else
        // Left and Right Damping (i, ymin...ymax, zmin...zmax)
    end
end
```

Algorithm 2: Wavefield solution step

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-08.jpg?height=593&width=523&top_left_y=240&top_left_x=736)

Figure 4: Code tree structure of Minimod package.

The general structure listed above is the backbone for all the propagators included in Minimod. To keep the code simple and flexible, each propagator is compiled separately. This can be selected by setting the propagator variable in the version file before compiling. Figure 4 presents a tree structure of Minimod code suite.

### 5.2 Targets

Each propagator has also its own implementation depending the hardware targeted. The target directory located in each propagator is the way of setting targets. In the source code structure, the following implementations of the target are provided:

- seq_opt-none : implement kernels without any optimization (so as the short form of sequential none optimization mode). The purpose of the sequential implementation is to be used as a baseline to compare the optimized results. Also, this is useful to analyze not parallel-related characteristics.
- omp_opt-none : implement kernels using OpenMP directives for multi-threading execution on CPU. The goal of this target implementation is to explore the advanced CPU features on multi-core or many-core systems.
- acc_opt-gpu : implement kernels using OpenACC directives for offloading computing to GPU (so as the short form of accelerator optimization using GPU). The goal of this target implementation is to explore the accelerating on GPU using OpenACC programming model standard.
- omp_offload_opt-gpu : implement kernels using OpenMP directives for offloading to GPU (so as the short form of OpenMP offloading optimization using GPU). The goal of implementing this target is to explore the accelerating on GPU using OpenMP programming standard.

In addition to change the propagator that is to be used in tests, one may also change the "version" file to use a different target by setting the "target" variable to the desired multi-threaded or accelerator implementations.

### 5.3 Compilation and usage

After the target propagators, compilers, and accelerating implementation settings are selected, the source code is ready for compilation, as follows:

```
To compile the sequential mode of Minimod package:
$> source set_env.sh
$> make
To compile the multi-threading mode with OpenMP directives:
$> source set_env.sh
$> make _USE_OMP=1
To compile the offloading to GPU mode with OpenMP directives:
$> source set_env.sh
$> make _USE_OMP=tesla
To compile the multi-threading mode with OpenACC directives:
$> source set_env.sh
$> make _USE_ACC=multicore
To compile the offloading to GPU mode with OpenACC directives:
$> source set_env.sh
$> make _USE_ACC=tesla
```

The parameters of Minimod are shown in the following verbatim section. Those are the basic parameters for seismic modeling and they are set as command-line options. The main parameters include: grid sizes, grid spacing on each dimension, the number of time steps and the maximum frequency.

```
[]$ ./modeling_acoustic_iso_cd_seq_opt-none --help
```

| --ngrid | $100,100,100$ | \# Grid size |
| :--- | :--- | :--- |
| --dgrid | $20,20,20$ | \# Dmesh: grid spacing |
| --nsteps | 1000 | \# Nb of time steps for modeling |
| --fmax | 25 | \# Max Frequency |
| --verbose | .false. | \# Print informations |

In terms of expected results, the following verbatim section presents an example to show how to run the application and the run-time results of single-thread Minimod acoustic-iso-cd kernel. As we can see, the results report all the parameters that are used in the modeling and at the end the kernel time and modeling time of running the application.

[]]\$ ./modeling_acoustic_iso_cd_seq_opt-none --ngrid 240,240,240 --nsteps 300

| nthreads | $=$ | 1 |  |  |
| :---: | :---: | :---: | :---: | :---: |
| ngrid | $=$ | 240 | 240 | 240 |
| dgrid | $=$ | 20.0000000 | 20.0000000 | 20.0000000 |
| nsteps | $=$ | 300 |  |  |
| fmax | $=$ | 25.0000000 |  |  |
| vmin | $=$ | 1500.00000 |  |  |
| vmax | $=$ | 4500.00000 |  |  |
| $\mathrm{cfl}$ | $=$ | 0.800000012 |  |  |
| stencil | $=$ | 4 | 4 | 4 |
| source_loc | $=$ | 120 | 120 | 120 |
| ndamping | $=$ | 27 | 27 | 27 |
| ntaper | $=$ | 3 | 3 | 3 |
| nshots | $=$ | 1 |  |  |
| time_rec | $=$ | 0.00000000 |  |  |
| nreceivers | $=$ | 57600 |  |  |
| receiver_increment | $t=$ | 1 | 1 |  |
| source_increment | $=$ | 1 | 1 | 0 |
| time step | $100 /$ | 300 |  |  |
| time step | 200 / | 300 |  |  |
| time step | $300 /$ | 300 |  |  |
| Time Kernel | 30.47 |  |  |  |
| Time Modeling | 31.01 |  |  |  |

## 6 Benchmarks

In this section examples of Minimod experimental results are presented. The purpose is illustrate performance and scalability of the propagators with regard to current HPC platforms.

### 6.1 Experimental set-up

The different propagators of Minimod are evaluated on Fujitsu A64FX architecture, AMD EYPC system, Intel Skylake and IBM Power8 system, as well as Nvidia's V100 GPUs. The specifications of hardware and software configurations of the experimental platforms are shown in Table 1 .

|  | Hardware | Software |
| :---: | :---: | :---: |
| CPUs | A64FX Armv8-A SVE architecture | Fujitsu Compiler 1.1.13 (frt) |
| CPU cores | 48 computing cores | OpenMP (-Kopenmp) |
| Memory | 32 GB HBM2 | auto-parallelisation |
| L2 | $8 \mathrm{MB}$ | (-Kparallel) |
| L1 | $64 \mathrm{~KB}$ |  |
| Device Fabrication | $7 \mathrm{~nm}$ |  |
| TDP | $160 \mathrm{~W}$ |  |
| CPUs | AMD EYPC 7702 | GCC 8.2 (gfortran) |
| CPU cores | 64 computing cores | OpenMP |
| Memory | $256 \mathrm{~GB}$ |  |
| $\mathrm{L} 3$ | $256 \mathrm{MB}$ (per socket) |  |
| L2 | $32 \mathrm{MB}$ |  |
| L1 | $2+2 \mathrm{MB}$ |  |
| Device Fabrication | $14 \mathrm{~nm}$ |  |
| TDP | $200 \mathrm{~W}$ |  |
| CPUs | 2x Intel Xeon Gold 5118 | intel compiler 17.0.2 (ifort) |
| CPU cores | 24 (12 per $\mathrm{CPU})$ |  |
| Memory | $768 \mathrm{~GB}$ |  |
| L3 | $16 \mathrm{MB}$ (per socket) |  |
| L2 | $1024 \mathrm{~KB}$ |  |
| L1 | $32+32 \mathrm{~KB}$ |  |
| Device Fabrication | $14 \mathrm{~nm}$ |  |
| $\mathrm{TDP}$ | $2 \times 105 \mathrm{~W}$ |  |
| CPUs | 2 x IBM Power8 (ppc64le) | PGI 19.7 (pgfortran) |
| CPU cores | 20 computing cores (10 per CPU) | OpenMP (-mp) |
| Memory | $256 \mathrm{~GB}$ |  |
| L3 | $8 \mathrm{MB}$ (per two cores) |  |
| L2 | $512 \mathrm{~KB}$ (per two cores) |  |
| L1 | $64+32 \mathrm{~KB}$ |  |
| Device Fabrication | $22 \mathrm{~nm}$ |  |
| TDP | $2 \times 190 \mathrm{~W}$ |  |
| GPU | 1 x Nvidia V100 | PGI 19.7 (pgfortran) |
| cores | 2560 Nvidia CUDA cores | OpenACC (-ta=tesla) |
| Memory | 16 GB HBM2 |  |
|  | $6144 \mathrm{~KB}$ |  |
| Device fabrication | $12 \mathrm{~nm}$ FFN |  |
| Power consumption | $290 \mathrm{~W}$ |  |

Table 1: Hardware and software configuration of the experimental platforms. From top to bottom, the first section is Fujitsu A64FX Arm8-A architecture. The second section is AMD EYPC Rome architecture. The third section is Intel Skylake architecture. The fourth section is IBM Power8 architecture. And the bottom section is the specification of Nvidia's V100 GPU.

### 6.2 Performance characterization

In our experiments, we use roofline model proposed by Williams et al. [20] to understand the hardware limitations as well as evaluating kernel optimization. In the roofline model, the performance of various numerical methods are upper bounded by the peak floating point operations (flop) rate and the memory bandwidth while running on single-core, multi-core or accelerator processor architecture.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-12.jpg?height=794&width=1455&top_left_y=584&top_left_x=324)

Figure 5: Roofline model analyses on AMD EYPC System.

Figure 5 shows the peak performance in term of GFlops per seconds and memory bandwidth of cache and main dynamic random-access memory (DRAM) of the AMD EYPC system listed in Table 1 where we conducted experiments on. The arithmetic intensity in the roofline plot is calculated by the number of floating point operations that are performed in the stencil calculation divided by the number of words that we need to read from and write to memory [6].

### 6.3 Single compute node-level parallelism

We use Minimod to experiment the single compute node-level parallelism on different computer systems. As shown in Figure 7. The system-level performance tests are conducted on IBM power, Fujitsu A64FX systems, and compared with using NVIDIA's V100 GPUs as accelerators. The different propagators in Minimod (acoustic_iso_cd, acoustic_iso, acoustic_tti, elastic_iso, and elastic_tti) are tested, and results are shown in Figure 6 .

As we observe in Figure 6, the Fujitsu A64FX processor (as shown in the orange bars) provides better performance for all the propagators in comparison to both IBM power system (as shown in the dark blue bars), Intel skylake system (as shown in the light blue bars), as well as AMD EYPC Rome systems (as shown in the yellow bars). In fact, the performance of Fujitsu A64FX is closer
to the performance of the system with Nvidia's V100 GPU accelerator (as shown in the green bars).

The single node-level scalability tests are conducted on IBM power, AMD EYPC, and Fujitsu A64FX systems. The problem size for the strong scalability tests are set to be $240 \times 240 \times 240$. As presented in Figure 7, the results are compared between the three modern computer systems and also compares against the ideal case. Across the three systems, Fujitsu A64FX system again wins IBM power and AMD EYPC Rome systems in the single-node scalability tests.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-13.jpg?height=780&width=1521&top_left_y=610&top_left_x=302)

Figure 6: System-level performance comparison results of Minimod with different propagators running on IBM power system (dark blue bars), on Intel Skylake system (light blue bars), on AMD EPYC system (yellow bars), on Fujitsu A64FX system (orange bars), and on NVIDIA's V100 GPU (green bars).

### 6.4 Distributed Memory Approach

The distributed version of Minimod is implemented using Message Passing Interface (MPI). The domain decomposition is defined using regular Cartesian topology, and the domain decomposition parameters need to match the total number of MPI ranks: for example, for the three-dimensional domain decomposition in $x \times y \times z$ equals $2 \times 2 \times 4$, the rank number needs to be 16 . As for the time being, only acoustic_iso_cd propagator is available within the distributed version of Minimod.

The implementation of MPI communication between sub-domains uses non-blocking send and receives. The communication operates in "expected message" mode that has no overlap of communication with computation. Each subdomain performs the following steps: first, to pack the messages to be transmitted in buffers; second, to perform communication by posting all sends and receives, and finally wait till the communication is complete and unpacks the arrived data.

We evaluated both weak scalability and strong scalability of the distributed version of Minimod

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-14.jpg?height=683&width=1198&top_left_y=244&top_left_x=472)

Figure 7: Single compute node-level scalability comparison results of Minimod running on IBM power system (blue curve), on AMD EYPC Rome system (yellow curve), and on Fujitsu A64FX system (red curve), and both are compared against the ideal scale-up (green curve).
![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-14.jpg?height=578&width=1654&top_left_y=1126&top_left_x=234)

Figure 8: MPI weak scalability of Minimod running on IBM power system for both the ideal problem-sizes (baseline $1000 \times 1000 \times 1000$ with linear increment on $\mathrm{X}$ dimension) and practical problem-sizes (the problem-size increments are balanced on each dimension) running on 1 to $8 \mathrm{MPI}$ ranks respectively.

for acoustic_iso_cd propagator. The results of weak scalability running Minimod on IBM power system is shown in Figure 8, which presents the evaluation results using both the ideal problem sizes and the practical problem sizes running on 1 to 8 MPI ranks, respectively.

For the weak scalability test running ideal problem sizes, we used a baseline of $1000 \times 1000 \times 1000$ with linear increment on $\mathrm{X}$ dimension (for example, for the test running on 6 MPI ranks we used a problem-size of $6000 \times 1000 \times 1000$ ). And for practical problem-sizes, we used the same baseline while the problem-size increments are balanced on each dimension (for example, for the test run-
ning on 6 MPI ranks we used a problem-size of $1856 \times 1856 \times 1856$ ). The green curves in Figure 8 present the efficiencies in comparison to the baseline result.

We can see from Figure 8 that the weak scalability holds well for running from 1 rank scale to up to 8 ranks for the ideal problem sizes. And for the practical problem sizes which is more close to the real seismic acquisition situation, the weak scalability efficiencies for 2 ranks and 4 ranks are higher than $100 \%$ because of the slightly smaller problem sizes compared to the baseline case $(1280 \times 1280 \times 1280$ for 2 ranks and $1600 \times 1600 \times 1600$ for 4 ranks $)$, while it starts diminishing when it reaches 8 ranks mainly because of the increase of problem sizes.

The results of strong scalability are shown in Figure 9. The strong scalability tests are conducted on both IBM power and Fujitsu A64FX systems. The problem size for the strong scalability tests is set to $1024 \times 1024 \times 1024$, on the rank numbers of $8,16,32,64,128$, and 256 respectively.

As presented in Figure 9, the results of the kernel execution on the IBM power and the Fujitsu A64FX systems are compared with the ideal scaling trend. The strong scalability results on both systems are very close when the MPI rank number is smaller than 64 , while the kernel shows slightly better scalability results on the IBM system than on the Fujitsu system when running with 128 and 256 MPI ranks. In comparison to the ideal case, scalability on the IBM power system reached $63 \%$ while on the Fujitsu system reached $60 \%$ of the ideal scalability.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-15.jpg?height=813&width=1353&top_left_y=1290&top_left_x=386)

Figure 9: MPI strong scalability comparison results of Minimod running on IBM power system (blue curve) and on Fujitsu A64FX system (red curve), and both are compared against the ideal scale-up (green curve).

### 6.5 Profiling

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-16.jpg?height=1130&width=1651&top_left_y=351&top_left_x=237)

Figure 10: Profiling results of Minimod using HPCToolkit.

Profiling and analyses was conducted on Minimod, for example using the HPCToolkit [13] from Rice University. Figure 10 shows a screenshot of the trace view in HPCToolkit profiling Minimod acoustic iso kernel implemented in multi-threading mode using OpenMP. The biggest panel on the top left presents sequences of samples of each trace line rendered. The different colors represent the time spends on different subroutines which are listed on the right panel. The bottom panel in Figure 10 is the depth view of the target Minimod application which presents the call path at every time step.

As an illustrative example for profiling Minimod, Figure 11 shows the profiling results from HPCToolkit trace view for the sequential implementation of the simplest kernel acoustic_iso_cd (acoustic wave-propagation in isotropic media with constant density) in Minimod without any optimization. To better understand the behavior of the kernel, what is shown in the picture is a case with one thread with the stencil computation on a grid size of $100 \times 100 \times 100$. As shown in Figure 11, the majority of the time is spent on running the "target_pml_3d" which is the implementation of perfectly-matched region, as shown in the dark color areas in the top left panel. And the green vertical line is for the "target_inner_3d", where the thread performs computation for the inner region of stencil.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-17.jpg?height=1098&width=1635&top_left_y=356&top_left_x=234)

Figure 11: Example of profiling sequential mode of Minimod acoustic-iso-cd kernel using HPCToolkit.

An advantage of HPCToolkit it that can profile the results of Minimod GPU mode for each time sampling traces. Figure 12 shows the the profiling results of the entire execution of Minimod acoustic-iso-cd kernel in OpenACC offloading to GPU mode. Different than the CPU profiling trace views, the GPU profiling trace view on HPCToolkit top-left panel window is composed of two rows. The top row shows the CPU (host) thread traces and the bottom row is for the GPU (device) traces.

A zoomed-in view of this GPU profiling results is presented in Figure 13. We selected time step shows the GPU that is running the "target_pml_3d" kernel where the blank white spaces in the GPU row shows the idleness. The same as in the profiling results for CPU, different colors here represent the time spends on different GPU calls.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-18.jpg?height=835&width=1569&top_left_y=274&top_left_x=278)

Figure 12: GPU profiling results of Minimod acoustic-iso-cd kernel using HPCToolkit.

![](https://cdn.mathpix.com/cropped/2024_06_04_a2a689c4c555ef3e09feg-18.jpg?height=840&width=1567&top_left_y=1271&top_left_x=279)

Figure 13: A detailed view of GPU profiling Minimod acoustic-iso-cd kernel using HPCToolkit.

## 7 Conclusion

This article introduces a proxy application suite for seismic modeling using finite difference method named Minimod. The design concepts, underline algorithms, and code structures of Minimod are described. The benchmark results of Minimod are shown on different computer architectures for both single compute node-level parallelism and distributed memory approaches.

## 8 Acknowledgements

We would like to thank Total and subsidiaries for allowing us to share this material. We would also like to express our appreciation to Diego Klahr for his continuous support, and our colleague Elies Bergounioux in France for discussions on the adaptability of proxy applications in production. We also thank Ryuichi Sai from Rice University for his contribution on the profiling results using HPCToolkits. We would like acknowledge Pierre Lagier from Fujitsu for his help with the experiments conducted with latest Fujitsu technology. Last but not least, many thanks to our former colleague Maxime Hugues for his initial implementation of the presented software.

## References

[1] M. Araya-Polo, J. Cabezas, M. Hanzich, M. Pericas, F. Rubio, I. Gelado, M. Shafiq, E. Morancho, N. Navarro, E. Ayguade, J. M. Cela, and M. Valero. Assessing accelerator-based hpc reverse time migration. IEEE Transactions on Parallel and Distributed Systems, 22(1):147-162, 2011

[2] J.-P. Berenger. A perfectly matched layer for the absorption of electromagnetic waves. Journal of Computational Physics, 114(2):185 - 200, 1994.

[3] K. Bube, T. Nemeth, P. Stefani, R. Ergas, W. Lui, T. Nihei, and L. Zhang. On the instability in second-order systems for acoustic vti and tti media. Geophysics, 77:171-186, 2012.

[4] M. Dablain. The application of high-order differencing to the scalar wave equation. Geophysics, $51: 54-66,1986$.

[5] R. de la Cruz and M. Araya-Polo. Towards a multi-level cache performance model for 3d stencil computation. Procedia Computer Science, 4:2146 - 2155, 2011. Proceedings of the International Conference on Computational Science, ICCS 2011.

[6] R. de la Cruz and M. Araya-Polo. Algorithm 942: Semi-stencil. ACM Trans. Math. Softw., 40(3), Apr. 2014.

[7] S. Ghosh, T. Liao, H. Calandra, and B. M. Chapman. Experiences with openmp, pgi, hmpp and openacc directives on iso/tti kernels. In 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, pages 691-700, Nov 2012.

[8] R. W. Graves. Simulating seismic wave propagation in 3d elastic media using staggered-grid finite differences. Geophysics, 86:1091-1106, 1996.

[9] K. R. Kelly, R. W. Ward, S. Treitel, and R. M. Alford. Synthetic seismograms: A finitedifference approach. Geophysics, 41:2-27, 1976.

[10] D. Komatitsch and R. Martin. An unsplit convolutional perfectly matched layer improved at grazing incidence for the seismic wave equation. Geophysics, 72:155-167, 2007.

[11] V. Lebedev. Difference analogues of orthogonal decompositions, basic differential operators and some boundary problems of mathematical physics. ii. USSR Computational Mathematics and Mathematical Physics, 4:36-50, 1964.

[12] M. Louboutin, M. Lange, F. Luporini, N. Kukreja, P. A. Witte, F. J. Herrmann, P. Velesko, and G. J. Gorman. Devito (v3.1.0): an embedded domain-specific language for finite differences and geophysical exploration. Geoscientific Model Development, 12(3):1165-1187, 2019.

[13] J. Mellor-Crummey, R. Fowler, and D. Whalley. Tools for application-oriented performance tuning. In Proceedings of the 15th International Conference on Supercomputing, ICS '01, page 154-165, New York, NY, USA, 2001. Association for Computing Machinery.

[14] A. Nguyen, N. Satish, J. Chhugani, C. Kim, and P. Dubey. 3.5-d blocking optimization for stencil computations on modern cpus and gpus. In SC '10: Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-13, 2010.

[15] A. Qawasmeh, M. R. Hugues, H. Calandra, and B. M. Chapman. Performance portability in reverse time migration and seismic modelling via openacc. The International Journal of High Performance Computing Applications, 31(5):422-440, 2017.

[16] G. Rivera and Chau-Wen Tseng. Tiling optimizations for 3d scientific computations. In $S C$ '00: Proceedings of the 2000 ACM/IEEE Conference on Supercomputing, pages 32-32, 2000.

[17] J. Virieux. Sh-wave propagation in heterogeneous media: Velocity-stress finite-difference method. Geophysics, 49:1933-1957, 1984.

[18] J. Virieux. P-sv wave propagation in 'heierogeneous media: velocity-stress finite-difference method. Geophysics, 51:889-901, 1986.

[19] J. Virieux and S. Operto. An overview of full-waveform inversion in exploration geophysics. Geophysics, 74, 2009.

[20] S. Williams, A. Waterman, and D. Patterson. Roofline: An insightful visual performance model for multicore architectures. Commun. ACM, 52:65-76, 042009.

[21] K. Yee. "numerical solution of initial boundary value problems involving maxwell's equations in isotropic media.". IEEE Transactions on Antennas and Propagation, 14:302-307, 1966.

