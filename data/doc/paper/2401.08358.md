# Hallucination Detection and Hallucination Mitigation: An Investigation 

Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, Gregory Dudek<br>Samsung AI Center, Montreal, Quebec, Canada<br>junliang.luo@partner.samsung.com, \{tianyu.li, di.wu1\}@samsung.com<br>m.jenkin@partner.samsung.com, \{steve.liu, greg.dudek\}@samsung.com


#### Abstract

Large language models (LLMs), including ChatGPT, Bard, and Llama [49], have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.


## 1 INTRODUCTION

Large language models (LLMs) are trained machine learning models that generate symbols (typically text) based on a sequence of previous symbols provided. This previous set of symbols is known as the prompt. The output symbol can then be appended to the prompt and the LLM used to generate the next output symbol. This process is repeated until the complete output is obtained. LLMs have shown significant capabilities, and LLM-based technology can now be
found at the core of a number of research projects and commercial AI applications. Although LLMs have shown considerable promise, one concern with their use in critical applications is that the LLMs are prone to "hallucinations", that is to produce output that are seemingly correct but contains nonfactual information

The consequence of hallucinations can be catastrophic and the LLM can produce believable outputs that are misleading and factually incorrect. Detecting and dealing with hallucinations has thus become a critical issue for LLMs to be applied to real-world problems. Here we provide a review of current approaches to detect and mitigate hallucinations. This report is organized as follows. In Section 2, we present a brief introduction to some common natural language generation (NLG) metrics as well as classification metrics that are presented in these papers. Section 3 and Section 4 present the discussions on existing works on hallucination detection and hallucination mitigation. Finally, we present reproduced results of existing works in Section ??. Here we first present a quick overview of the key characteristics of the reviewed papers in Table 1.1 and Table 1.2 for hallucination detection and mitigation tasks, respectively.

Table 1.1: Summary of the reviewed literature on hallucination detection.

| Method | Affiliation | Conference | Code Available | Dataset | Task | Key Words | Type |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Hades [26] | Microsoft | ACL'22 | Yes | Yes | QA | LLM Fine-tune, synthetic dataset, <br> contextual detector |  |
| NPH [9] | U Alberta | EMNLP'21 | Yes | No | QA | LLM fine-tune, contextual detector, knowledge grounded |  |
| Enfa [5] | MILA | ACL'22 | Yes | Yes | Summarization | Prior and posterior probabilities, k-nn, human annotated dataset |  |
| CNSG [59] | FAIR | ACL'21 | Yes | Yes | Summarization <br> Translation | LLM Fine-tune, synthetic dataset, contextual detector |  |
| SelfCheckGPT [28] | U Cambridge | N/A | Yes | Yes | QA | Tampling-based, self-consistency, human annotated dataset, plug-n-play |  |
| AlignScore [56] | UCSD | ACL'23 | Yes | No | Various NLG tasks | Unification of data sources and tasks, LLM fine-tune |  |
| ExHalder [44] | Google | WWW'23 | No | Yes | Summarization | NLI dataset pretraining, augmented fine-tuning, human annotated dataset | Sentence |
| Harim+ [46] | NCSOFT | AACL'22 | Yes | No | Summarization | Plug-n-play, prior and posterior probabilities, <br> decoder overconfidence regularizer |  |
| HaluEval [24] | Renmin U | N/A | Yes | Yes | QA, dialog, <br> summarization | Benchmark datasets, LLM self-detection, prompt engineering |  |

Table 1.2: Summary of the literature reviewed on hallucination mitigation.

| Method | Afffliation | Conference | Code Avaiable | Dataset | Task | Key Words |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| RHO [18] | HKUST | ACL'23 | Yes | No | KGD | Conversational model, local and global grounding, human evaluation |
| RAGs [45] | Facebook | EMNLP'21 | Yes | No | KGD | Retrieval-Augmented Generation (RAG), encoder-decoder architecture, human evaluation |
| Control Codes [45] | Google | ACL2021 | No | No | KGD | Control codes (Guide tokens), LLM training and output resampling. human evaluation |
| MixCL [47] | U Shandong | AAAI'23 | Yes | No | KGD | Contrastive Learning, LLM fine-tune, human evaluation |
| HRRMAN 58$]$ | U Edinburgh | EMNLP 2020 | No | No | Summarization | Quantitative correction. Bidirectional LSTM encoder, attention mechanism, human evaluation |
| Self-contradictory [33] | ETH Zurich | N/A | Yes | Yes | QA | Self correction, prompt engineering, LM-based analyzer |

## 2 InTRODUCTION To COMMON METRICS

This report is primarily concerned with hallucination detection and mitigation. This section reviews some of the common metrics on classification and more general natural language generation tasks.

### 2.1 ClaSSIFICATION METRICS

Classification problems take an observation and identify which of a set of categories to which it belongs. Under the scope of this report, the classification problem is often presented as a
binary classification problem, i.e., either the response is hallucinated or not hallucinated. In this subsection, we briefly introduce some of the commonly-seen classification metrics.

ACCURACY The classification accuracy is the ratio of the number of correct predictions to the total number of inputs.

PRECISION AND RECALL Precision and recall are a popular pair of metrics in the classification literature. Assume we are under the binary classification setup, i.e., either positive class or negative class. Then precision is essentially asking the question "What proportion of positive predictions is actually correct?," while recall is asking the question "What proportion of actual positives is classified correctly?".

F-SCORE To fully evaluate the effectiveness of a model, one must examine both precision and recall. Unfortunately, precision and recall are often in tension. That is, improving precision typically reduces recall and vice versa. One classic approach to this issue is to use F-score. The F1 score is a specific F score. F1 is the harmonic mean between recall and precision. The harmonic mean is bounded. It attends more towards the lower value. Therefore, even if one of recall and precision is high, F1 will be more responsive the lower one and won't be driven up by just one high score.

AUC Often the classification model will assign scores for the instances to be evaluated. A final classification result can be obtained by thresholding these scores. As a consequence, different threshold often leads to different classification result. A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: the true positive rate and the false positive rate, where the former is a synonym for recall, and the later describes the proportion of all instances that are falsely classified as positive with respect to the negative instances. AUC stands for "Area under the ROC Curve", that is, AUC measures the two-dimensional area underneath the entire ROC curve.

BSS The BSS (Brier Skill Score) is a score function that measures the accuracy of probabilistic predictions. For unidimensional predictions, the BSS is strictly equivalent to the mean squared error as applied to predicted probabilities. Given a set of instances and their true class, BSS computes the mean squared difference between the predicted probability assigned to the possible classification result for each instance and the actual class of the instance.

G-MEAN The geometric mean (G-mean) is the root of the product of class-wise sensitivity. This measure tries to maximize the accuracy on each of the classes while keeping them balanced. For binary classification G-mean is the squared root of the product of the sensitivity and specificity. For multi-class problems it is a higher root of the product of sensitivity for each class.

### 2.2 NLG METRICS

Typically, when evaluating an NLG (natural language generation) system, one is given a candidate text sequence (prediction) and a (list of) reference text (the ground truth) and the similarity of the two should be evaluated. For example, a candidate/reference text pairs could be given by the following:

- Candidate: "I am a member of SAIC Montreal."
- Reference A: "I am from Montreal."
- Reference B: "This is SAIC Montreal."

where reference A and B together forms a list of reference text. The goal of the evaluate is to compute the similarity/distance between the candidate and the reference text. In this subsection, we will briefly talk about some of the common metrics used in NLG tasks, especially regarding to this report.

$\mathrm{N}$-GRAMS $\mathrm{N}$-grams are a series of adjacent words, symbols or tokens in a given document. When used as an NLG metric, n-grams often refers to the fraction of n-grams in the candidate text which are present in any of the reference texts. For example, assume we are given the candidate/reference pair as in the example shown above, the total number of 2-grams in the reference text is 6 , while the overlapped 2-grams are "I am" and "SAIC Montreal", which gives a 2-grams score of $1 / 3$.

BLEU BLEU [37] (Bilingual Evaluation Understudy) is a metric for automatically evaluating various NLG tasks with reference text. The BLEU score is a number between zero and one that measures the similarity of the candidate text to a set of reference texts. A value of 0 means that the candidate output has no overlap with the reference text (low quality) while a value of 1 means there is perfect overlap with the reference (high quality). BLEU is a precision based score, meaning it looks at the fraction of overlapped n-grams between the candidate and the reference w.r.t. the total n-grams of the candidate text. One can see that it is easy to cheat by generating very short candidate text. Therefore, to circumvent this issue, BLEU proposes to incorporate a brevity penalty to penalize short candidate text.

SACREBLEU Standard BLEU often depends on the tokenization methods used, resulting in non-comparable BLEU scores across different experimental setups. SacreBLEU[38] uses standardized tokenization and normalization schemes to unify the metrics to allow crossexperiment comparisons.

RougE Rouge [25] is the abbreviation of Recall Oriented Understudy for Gisting Evaluation. As is clear from its name, ROUGE is based only on recall, and is often used for summary evaluation. The computation is similar to BLEU, except that instead of having the number of n-grams in the candidate as the denominator, Rouge uses the number of n-grams in the
references as the denominator. Depending on the feature used for calculating recall, ROUGE may be of many types, namely ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S.

METEOR METEOR [1] (Metric for Evaluation for Translation with Explicit Ordering) is another metric for machine translation evaluation. One problem with BLEU is that it is often considered as being too rigid as it only allows exact n-gram matching. To address this problem, METEOR modifies the precision and recall computations, replacing them with a weighted F-score based on mapping unigrams with relaxed measure on synonym and paraphrase matching as well as stemmed-word matching. It also adds a penalty function for incorrect word order.

BERTSCORE The previously described metrics are word-overlap-based metrics. That is, they are computed via the percentage of the overlapped text spans between the candidate and the references. Another popular type of metrics is the embedding based metrics. Word embeddings extend word overlap-based metrics beyond exact match. Instead of just looking at the frequencies of overlapped words, we now look at the distance (inner product or cosine similarity) between the embeddings of these words. BERTScore [57] is an embedding-based metric. BERTScore computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings using a BERT model.

BARTSCORE BARTScore [55] conceptualizes the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models (BART [23]). The core idea is that models trained to convert the generated candidate text to/from a reference output will achieve higher scores when the candidate text is better. BARTScore is parameter- and dataefficient. It can also better support evaluation of generated text from different perspectives (e.g., informativeness, coherence, factuality, etc.).

BLEURT BLEURT [43, 39] (Bilingual Evaluation Understudy with Representations from Transformers) is a machine learning-based automatic metric that can capture non-trivial semantic similarities between sentences. BLEURT leverages the word representations from BERT [20], and transfer learning to construct this automatic metric. They first create a large set of synthetic datasets by perturbing the text from Wikipedia and then use automatic metrics, such as BLEU and Rouge to score these texts. Then they construct a regression tasks on these synthetic data with BERT representations. Finally, they finetuned the regression model with human annotations.

FEQA Similar to BLEURT, FEQA [8] (Faithfulness Evaluation with Question Answering) is a model-based metric, leveraging the Question-Answering framework. The main idea of this metric is to generate questions and answers from the candidate text, then conditioned on the reference to obtain the answers for the same set of questions and see if the answers match. The advantage of this method is that it aligns really well with human judges on faithfulness. The disadvantage is however, since it is based on LLM (BART [23] and BERT [20]), therefore requires quite significant computing resources, and often slow to run.

QuESTEval QuestEval [42] is also a question-answering based metric, similar to FEQA. One thing to note is that FEQA is a precision-based metric, as it extract questions and answers from the candidate and then ask the reference the same questions to see if their answers match. QuestEval additionally incorporates recall into the framework as well. For the precision, similar to FEQA, one can generate question and answers from the candidate and then ask the reference for answers and compare to obtain precision score. For the recall, one can generate question from the reference text and conditioned on the candidate to obtain the corresponding answers. Then the QuestEval score can be obtained by combining the precision and recall scores.

DAE DAE [11] (Dependency Arc Entailment) is another model-based metric. Unlike FEQA and BLEURT, it leverages the framework of textual entailment. Under this framework, a model is often used to evaluate if the candidate is entailed by the reference. Most of the methods that fall under this category often considers a sentence-level evaluation, however this type of approach do not localize and failed to provide fine-grained evaluation. This work instead, asks whether the semantic relationship manifested by individual dependency arcs in the candidate text is supported by the reference. This approach views dependency arcs as semantic units that can be interpreted in isolation. Each arc is therefore judged independently based on whether the relation it implies is entailed by the source sentence.

## 3 HALLUCINATION DETECTION

Hallucination detection is the task of identifying potential hallucinations in the generated responses by LLMs. Typically in the literature, there are two major archetypes of this task: token-level detection and sentence-level detection. The token-level tasks try to identify the exact token/named entity that might suffer from hallucination while the sentence-level tasks try to identify the hallucinated sentences. In this section, we will review papers of these two types of hallucination detection tasks.

### 3.1 TOKEN LEVEL

### 3.1.1 HADES: A TOKEN-LEVEL REFERENCE-FREE HALLUCINATION DETECTION BENCHMARK FOR FREE-FORM TEXT GENERATION [26]

Github Link: https://github.com/microsoft/HaDes

Tasks: Question answering.

Core Idea: Propose a novel token-level reference-free hallucination detection annotated dataset. Additionally, a series of token-level detection models baselines are implemented and evaluated on the proposed dataset.

This work centers on detecting hallucinations at the token-level without relying on groundtruth references. While existing methods often operate at the sentence or document level with an oracle reference, these approaches face challenges when ground-truth references are unavailable for free-form text generation. In addition, detecting at finer token levels can

Table 3.1: Benchmark (numbers in percentages (\%)) for the online setting on HADES, where detecting models have access to the bidirectional context. Except for BSS, all are the higher the better. (Table source: [26])

| Model | ACC | G-Mean | BSS | AUC | Not Hallucination |  |  |  | Hallucination |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | $\mathrm{F} 1$ | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{F} 1$ |  |  |  |
| GPT-2 | $\mathbf{7 1 . 5 8}$ | $\mathbf{7 0 . 9 8}$ | $\mathbf{1 9 . 1 3}$ | $\mathbf{7 7 . 7 1}$ | $\mathbf{7 1 . 3 2}$ | 77.29 | $\mathbf{7 4 . 1 9}$ | $\mathbf{7 1 . 9 3}$ | $\mathbf{6 5 . 1 9}$ | $\mathbf{6 8 . 4 0}$ |  |
| BERT | 71.00 | 70.43 | $\mathbf{1 8 . 6 6}$ | $\mathbf{7 8 . 8 3}$ | 70.91 | 76.50 | 73.60 | 71.12 | 64.84 | 67.84 |  |
| RoBERTa | 70.67 | 70.14 | 19.77 | 77.07 | 70.74 | 75.87 | 73.22 | 70.58 | 64.84 | 67.59 |  |
| XLNet | 70.08 | 69.17 | 19.76 | 76.59 | 69.39 | $\mathbf{7 7 . 6 0}$ | 73.27 | 71.08 | 61.66 | 66.04 |  |

Table 3.2: Benchmark (numbers in percentages (\%)) for the offline setting on HADES, where detecting models have access to the bidirectional context. Except for BSS, all are the higher the better. (Table source: [26])

| Model | ACC | G-Mean | BSS | AUC | Not Hallucination |  |  |  | Hallucination |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{F} 1$ | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{F} 1$ |  |  |
| LR | 62.25 | 60.77 | - | - | 62.35 | 72.08 | 66.86 | 62.10 | 51.24 | 60.33 |  |  |
| SVM | 63.67 | 61.50 | - | - | 62.89 | 76.18 | 68.90 | 65.05 | 49.65 | 56.31 |  |  |
| BERT | 71.92 | $\mathbf{7 1 . 9 5}$ | 19.06 | 78.63 | $\mathbf{7 4 . 4 6}$ | 71.29 | 72.84 | 69.31 | $\mathbf{7 2 . 6 1}$ | 70.92 |  |  |
| RoBERTa | $\mathbf{7 2 . 8 3}$ | 70.94 | $\mathbf{1 8 . 7 8}$ | 78.72 | 74.06 | 74.46 | 74.41 | 71.43 | 70.67 | $\mathbf{7 1 . 0 5}$ |  |  |
| XLNet | 72.33 | 71.39 | 18.79 | $\mathbf{7 8 . 9 3}$ | 71.15 | $\mathbf{8 0 . 1 3}$ | $\mathbf{7 5 . 3 7}$ | $\mathbf{7 4 . 0 7}$ | 63.60 | 68.44 |  |  |

be crucial for real-time prevention of misleading content as well. This work provides two important initial steps towards hallucination detection under this setup. First, the authors develop a novel token-level reference-free hallucination detection annotated dataset named HADES (HAllucination DEtection dataSet). They then create a series of baseline token-level detection models trained on this dataset. These models can detect potential hallucinations at the token level without the need for an oracle reference. This approach could be useful in real-time applications where ground-truth references may not be readily available.

To construct the HADES dataset, roughly three steps are followed. First, a large number of text segments extracted from English language Wikipedia (English WIKI-40B [13]) using BERT [20] are perturbed. In applying this contextual perturbation the authors maintain two principles: 1) the fluency and syntactic correctness of the perturbed text should be preserved; 2) the perturbed text should be lexically diverse. The text perturbation process is split into three operations, namely MASK, REPLACE and RANK. In the MASK step, one word (named entity) spans are masked based on a predefined mask ratio. Then, in the REPLACE step, a pretrained BERT-base model is leveraged to predict the masked span and replace the span with the prediction. All top- $\mathrm{k}(\mathrm{k}=10)$ samplings are considered as possible candidates as this provides a good trade-off between diversity (number of distinct tokens) and coherence (number of incoherent perturbations). In the final step, the perturbed text is post-processed with a RANK operation as an additional screening step to filter out global incoherence and syntactic issues. In the end, the authors collect around 1M perturbed text segments in the pool after contextual perturbation. One thing to note is that not all of these contain hallucination,
as the BERT model can generate factual information given that it is pretrained on a rich open web corpus.

After the initial data perturbation, the authors use human annotation for the labeling process. Here, they use a so-called "model-in-the-loop" procedure to annotate a challenging subset, as human annotation is prohibitively expensive at this scale. The annotation process is split into several rounds. For each round, they first retrain a hallucination detection model (initialized with BERT) on the annotated instances from the previous rounds. This model is used to select the next batch of data to be annotated from the remaining unlabeled data. In total, after accumulating annotations for several rounds, they obtain 12,719 instances with 71,226 HITS from judges. They conduct 14 rounds of annotation, increasing the annotation scale with each round (ranging from around 200 instances/round to around 4000 instances/round). Out of 12,719 annotated instances, 10,954 instances reached consensus among judges and are included in the HADES dataset. The dataset is split into train, validation and test sets with sizes of $8754,1000,1200$ respectively. In the final dataset, "hallucination" cases slightly outnumber "not hallucination" cases, with a ratio of $54.5 \% / 45.5 \%$.

Methodology-wise, this work proposes a series of baseline methods based on pretrained transformer models, including BERT [20], GPT-2 [40], XLNet [54] and RoBERTa [60]. These transformer-based models can potentially leverage context or embedded world knowledge to detect self-contradictory or anti-commonsense content. Specifically, for an input text segment and a target token, the model predicts binary hallucination labels for the given text spans. The authors propose two different settings: online and offline. In the online setting, the model can only access the unidirectional preceding context, which simulates on-the-fly generation. While in the offline setting, it is assumed that generation is complete, so the the model is able to perceive the bidirectional context. The results are shown in Figure 3.1 and Figure 3.2 for online and offline settings respectively.

One thing worth mentioning is that for the baseline transformer-based detectors, longer context length tends to result in a better performance. The authors run BERT-large detection model with different context lengths and characterize its performance in both online and offline settings. Starting from the target words, they set a fixed size (5/10/20/40/80/160) context window and truncate all text beyond this window. As the context window enlarges, the model performance grows rapidly when context length is smaller than 80 , and then gradually converges. This observation highlights the importance of context in hallucination detection using the detectors in this work. Therefore, one should pay attention to the use case of the HADEs detection model that it might underperform when the text length is not sufficiently long.

### 3.1.2 NPH: Neural Path Hunter: Reducing HalluCinATiOn in Dialogue SyStemS VIA Path GRoUNDING [9]

Github Link: https://github.com/nouhadziri/Neural-Path-Hunter

Tasks: Question and answering.

See Section 4.2.

Source:

Under the proposals, 120,000 additional asylum seekers will be distributed among EU nations, with binding quotas.(...) Mr Juncker told the European Parliament it was "nota time to take fright". (...) He said tackling the crisis was "a matter of humanity and human dignity"."It is true thatEurope cannot house all the misery in the world. But wehave to put it into perspective." (...)

## Generation:

European Commission President Jean-Claude Juncker hasset out his proposals for dealing with the migrant crisisin a speech to MEPs, saying Europe cannot house all themisery in the world.

Figure 3.1: Example of factual hallucinations in a BART generated summary on XSum dataset [35]. Neither the title "European Commission President" nor the first name "Jean- Claude" is mentioned in the document but both are factual. (Figure source: $[9]$ )

### 3.1.3 HalluCinated But Factual! InspeCting the FactualitY of HalluCinationS in AbSTRACTIVE SUMMARIZATION [5]

Github Link: https://github.com/mcao516/EntFA

Tasks: Summarization.

Core Idea: The paper proposes that not all hallucinations are detrimental within the scope of text summarization. On the contrary, some hallucinations can be factual and can enrich the semantics of the summarized text.

Previous methods (Hades, NPH) mainly address the hallucination detection problem under the scope of the question and answering task. Under this setting, hallucination is often defined to be answers that are factually false. In summarization, however, the definition is slightly different as the summarization task involves extracting and abstracting key content from the source document. Therefore, hallucination under this task is often defined to be any content in the summary that is not directly inferrable from the source text. One of the main contributions of [5] is that the authors point out that some hallucinations under summarization tasks can be factual as well, and these hallucinations should not be removed or addressed in the summaries as they provide additional information that the source text does not contain. One illustrative example can be found in Figure 3.1. In addition, this work also proposes a novel detection method that separates factual from non-factual hallucinations of entities. This approach utilizes an entity's prior and posterior probabilities according to pretrained and finetuned masked language models, respectively. They show that empirically this method outperforms five baseline methods and strongly correlates with human judges. The authors also provide a small annotated dataset for token-level hallucination detection on summarization task. A Token-level Reference-free Hallucination Detection Benchmark for Free-form text generation.

For the dataset, the authors annotated 800 summaries generated by BART, which was one of the state-of-the-art abstractive summarization models available at the time. The input documents are randomly selected from XSUM test set [35]. 2,838 entities are extracted from the

Table 3.3: Summary-level Pearson correlation coefficients between various automatic metrics and human judgments of factuality for XSUM datasets. High Pearson correlation coefficients indicate better alignment with human judges. (Table source: [9])

| Metric | FRANK <br> (Partial Pearson's $\rho$ ) | PCC |
| :--- | :--- | :--- |
| BLUE | 0.139 | 0.118 |
| ROUGE-1 | 0.155 | 0.132 |
| BERTScore | -0.0359 | 0.025 |
| QAGS | -0.0225 | 0.175 |
| FEQA | 0.0242 | - |
| DAE | 0.0444 | - |
| ENFA (proposed) | $\mathbf{0 . 1 8 3}$ | $\mathbf{0 . 2 6 8}$ |

800 generated summaries. Then $30 \%$ of the samples is set aside as the test set, while remaining is for training. The authors manually labeled each entity with one of the following three tags: non-hallucinated, factual hallucination, and non-factual hallucination. First, they extract entities from the given summary using automatic NER tools [15]. Then extracted entities are labeled based on the following criteria: if the the entity can be directly determined by the information from the source document, then this entity is non-hallucinated; if no internet search can prove or disprove the factuality of the entity or one can find evidence that disprove the factuality of the entity, it is labeled as non-factual hallucination; otherwise, the entity is labeled as factual hallucination. They name this dataset as XENT.

The main task involves differentiating non-factual hallucinations from both factual hallucinations and non-hallucinations. To do so, the authors propose to compute the prior and posterior probabilities of the entity, which is calculated by masked language models based on BART [23]. The prior probability is defined as the probability of its generation by a language model that does not have access to the source text, while the posterior probability is computed by conditioning on the source document. Intuitively, if the two probabilities are close to one another, then there is a low likelihood that the entity is a factual error (non-factual hallucination) as the small difference indicates providing the source text does not offer more (less) evidence for the entity. To classify the hallucination and factuality statuses of a given entity, the authors propose to use K-Nearest Neighbors (KNN) as a discriminator model. The reason for using this algorithm, claimed by the authors, is that it requires no training and makes minimal assumptions about the form of the decision boundary, as a non-parametric method and it also offers adequate interpretability. The KNN classifier is trained using the prior and posterior probabilities as features on the labeled dataset. One can find the corresponding experimental results using the XENT dataset can be found in Figure 3.3. The results show that the proposed approach outperforms five baselines on the factuality classification task.

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-11.jpg?height=363&width=1088&top_left_y=321&top_left_x=678)

Figure 3.2: Generation of synthetic data with hallucination labels. A hallucinated version of the original text is generated by feeding the noised sentence to the encoder-decoder model BART. Hallucination labels are assigned to each token by computing the edit distance between the hallucinated text and the original one. Labels of 1 refer to hallucinated words. (Figure source: [9])

### 3.1.4 Detecting HalluCinATEd Content in ConditionAl NeURal SEQUENCE GENERATION [59]

Github Link: https://github.com/violet-zct/fairseq-detect-hallucination

Tasks: Summarization and translation.

Core Idea: The paper proposes a pipeline to create token-level hallucinated synthetic dataset. In addition, it also proposes a detection method based on a pretrained language model.

In this study, the authors present a technique to identify hallucinations. The method involves fine-tuning pretrained language models on synthetic data, incorporating automatically inserted hallucinations. Through experiments conducted on machine translation (MT) and abstractive summarization, the proposed approach consistently surpasses robust baselines across all benchmark datasets. Additionally, the authors demonstrate the utility of token-level hallucination labels in defining a nuanced loss function for the target sequence in low-resource MT, leading to noteworthy enhancements compared to strong baseline methods. The method is also applied to word-level quality estimation for MT, showcasing its effectiveness in both supervised and unsupervised settings.

Figure 3.2 shows the synthetic dataset creation procedure proposed in this paper. This process can be divided into two phases: generation of hallucinated sentences and label assignments. To generate hallucinated sentences, the authors propose to first apply a noising function that removes words from the original target sentence and then use a pretrained BART [23] to generate a hallucinated version of the original text conditioned on the masked text with beam search. Then, to assign the labels, the authors compute the edit distance between the hallucinated text and the original one, and back-trace the deletion and substitution operations with dynamic programming. All the positions in the hallucinated text involving these two operations are labeled as hallucinations and everything else is considered faithful to the original text. For the abstractive summarization task, the authors perform this procedure on the XSUM dataset [30], which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. For the MT task, they leverage the multi-

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-12.jpg?height=271&width=1464&top_left_y=327&top_left_x=296)

Source S

True target $T$

Hallucinated version of target $\mathrm{T}^{\prime}$

Figure 3.3: Finetuning XLM-Roberta (for cross-lingual generation task, e.g. MT) or Roberta (for monolingual generation task, e.g. text summarization) on the synthetic training data. (Figure source: [59])

Table 3.4: F1 (x100) of hallucination labels on MT and abstractive summarization tasks. The first block are baseline methods and the second block are reported results of the proposed method. Bold indicates best results not using references. (Table source: [59])

| Methods | MT |  |  | Summarization |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---: |
|  | TranS2S | MBART | PtGen | TConvS2S | TranS2S | BERTS2S |  |
| Alignment | 29.47 | 9.93 | 38.92 | 37.94 | 34.37 | 25.81 |  |
| Overlap-based | 9.14 | 3.24 | 57.22 | 54.25 | 53.79 | 55.13 |  |
| Synonym-based | - | - | 59.54 | 63.73 | 58.66 | 53.07 |  |
| Ours (w/o reference) | 65.75 | 41.92 | 63.66 | 65.94 | 61.70 | 55.45 |  |
| Ours (w/o reference + synonym) | - | - | $\mathbf{6 4 . 7 2}$ | $\mathbf{6 9 . 3 7}$ | $\mathbf{6 3 . 8 8}$ | $\mathbf{5 6 . 4 9}$ |  |
| Ours (w/ reference) | $\mathbf{6 6 . 0 8}$ | $\mathbf{4 6 . 8 1}$ | 63.89 | 66.28 | 62.24 | 55.88 |  |

domain Chinese-English (Zh-En) translation dataset [51], which consists of four balanced domains: law, news, patent and subtitles.

Methodology-wise, the authors propose a general-purpose method for token-level hallucination detection for conditional sequence generation tasks. Given the source input, the authors formulate the task of token-level hallucination detection as a sequence labeling problem where a binary label is predicted at each position of the machine generation. The authors finetune a cross-lingual language model (LM) [6] for MT and a monolingual LM [60] for summarization. In both cases, the input consists of concatenating the original text, the true target (translation or summary), and the hallucinated version of the target as a single input sequence to the model. Then the standard classification loss is used to to carry out the finetuning process. A graphical illustration of this procedure can be found in Figure 3.3.

Table 3.4 shows the F1 scores of token-level hallucination labels across six benchmark datasets for MT and abstractive summarization. The proposed method achieves decent performance on this task and ranks the best among all baseline methods. One thing worth mentioning is that, during training, the true target is concatenated into the input of the model. However, during the testing phase, due to the lack of the true target, the authors propose to replace this part of input by some dummy variables. Surprisingly, the proposed model can generalize well without references. As a contrast, the authors further report that the model can achieve a significantly higher recall but worse precision when including the true target as
a part of the input at test time.

### 3.2 SENTENCE LEVEL

### 3.2.1 SELFCHECKGPT: ZERO-RESOURCE BLACK-BOX HALLUCINATION DETECTION FOR GENERATIVE LARGE LANGUAGE MODELS [28]

Github Link: https://github.com/potsawee/selfcheckgpt

Tasks: Question and answering.

Core Idea: This paper makes a claim that most of the hallucinations happen when the model is unsure about itself. Therefore, for this type of hallucinations, one can detect it via evaluating the LLM's self-consistency by checking the similarity between the main response and a set of sample responded generated with the same LLM with higher temperature.

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-13.jpg?height=585&width=579&top_left_y=1027&top_left_x=727)

Figure 3.4: Flowchart of the data annotation process. (Figure source: [28])

In this study, the authors introduce "SelfCheckGPT," a straightforward sampling-based approach designed for fact-checking large language models (LLMs) in a zero-resource manner, i.e., without relying on an external database. SelfCheckGPT capitalizes on the intuitive notion that if an LLM possesses knowledge of a particular concept, sampled responses are likely to be consistent and contain similar facts. Conversely, for hallucinated facts, stochastically sampled responses are expected to diverge and contradict each other. By extracting multiple responses from an LLM, one can assess information consistency among the different responses, thereby discerning factual statements from hallucinated ones.

The paper explores three variants of SelfCheckGPT for measuring informational consistency: BERTScore [57], question-answering [29], and n-grams. Additionally, they also delve into natural language inference scores using DeBERTa-v3-large [14] and prompt engineering using LLMs. The authors validate this approach using GPT-3 [4] to generate passages from the WikiBio dataset [22] about each individual topics, then manually annotate the factuality of

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-14.jpg?height=748&width=1190&top_left_y=314&top_left_x=430)

Figure 3.5: SelfCheckGPT with Question Answering. (Figure source: [28])

the generated passages using human judges. The study demonstrates that SelfCheckGPT can i) identify non-factual and factual sentences and ii) rank passages in terms of factuality. Empirical comparisons underscore that, in sentence-level hallucination detection tasks, the proposed approach outperforms various baseline methods.

For the dataset, the authors opt to use WikiBio [22] as a starting point. WikiBio is a dataset of the first paragraph (with tabular information) of Wikipedia articles about specific concepts. The goal of this dataset is to generate passages using LLMs and evaluate the factuality with respect to the original article. To do so, they rank the WikiBio test set in terms of paragraph length and randomly sample 238 articles from the top $20 \%$ of the longest articles (to ensure no obscure concept is selected). Then, they use GPT-3 (text-davinci-003) [4] to generate articles on a specific concept in the dataset, using the prompt "This is a Wikipedia passage about \{concept\}". There are in total 238 passages which contains 1,908 sentences. Then the authors ask human judges to annotate each of the generated sentences into three classes: major inaccurate, minor inaccurate and accurate, based on the process described in Figure 3.4. Of the 1,908 annotated sentences, $761(39.9 \%)$ of the sentences are labelled major-inaccurate, $631(33.1 \%)$ are minor-inaccurate, and 516 (27.0\%) are accurate.

The idea of SelfCheckGPT is to randomly sample various responses and compute a consistency score between these generated samples and the main response. For the target LLM, one should generate the main response passage using the regular setting and then generate $n$ number of randomly-drawn passages using the same query as the list of references. To ensure the randomness, one common practice is to increase the temperature parameter of the LLM. Then a consistency score will be computed between the main response passage and the randomly-drawn passages. The scoring can be done in various ways as mentioned in the previous paragraph, such as question answering or n-grams, etc. The process of SelfCheckGPT using question answering [29] as the consistency score can be found in Figure 3.5.

Table 3.5: AUC-PR for sentence-level detection tasks. Passage-level ranking performances are measured by Pearson correlation coefficient and Spearman's rank correlation coefficient w.r.t. human judgements. (Table source: [28])

| Method | Sentence-level (AUC-PR) |  | Passage-level (Corr.) |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- |
|  | NonFact | NonFact* | Factual | Pearson | Spearman |
| Random | 72.96 | 29.72 | 27.04 | - | - |
| GPT-3's probabilities (LLM, grey-box) |  |  |  |  |  |
| Avg(-logp) | 83.21 | 38.89 | 53.97 | 57.04 | 53.93 |
| Avg(H)+ | 80.73 | 37.09 | 52.07 | 55.52 | 50.87 |
| Max(-logp) | 87.51 | 35.88 | 50.46 | 57.83 | 55.69 |
| Max(H) | 85.75 | 32.43 | 50.27 | 52.48 | 49.55 |
| LLaMA-30B's probabilities (Proxy LLM, black-box) |  |  |  |  |  |
| Avg(-logp) | 75.43 | 30.32 | 41.29 | 21.72 | 20.20 |
| Avg(H) | 80.80 | 39.01 | 42.97 | 33.80 | 39.49 |
| Max(-logp) | 74.01 | 27.14 | 31.08 | -22.83 | -22.71 |
| Max(H) | 80.92 | 37.32 | 37.90 | 35.57 | 38.94 |
| SelfCheckGPT (black-box) |  |  |  |  |  |
| w/ BERTScore | 81.96 | 45.96 | 44.23 | 58.18 | 55.90 |
| w/ QA | 84.26 | 40.06 | 48.14 | 61.07 | 59.29 |
| w/ Unigram (max) | 85.63 | 41.04 | 58.47 | 64.71 | 64.91 |
| Combination | 87.33 | 44.37 | 61.83 | 69.05 | 67.77 |

Experiments are conducted to determine whether SelfCheckGPT is capable of identifying the factuality of sentences. In detecting non-factual sentences, both major-inaccurate labels and minor-inaccurate labels are grouped together into the non-factual class, while the factual class refers to accurate sentences. Table 3.5 shows correlation results (higher the better) with respect to human judges on the sentence-level hallucination detection task. The results in Table 3.5 show that all of the SelfCheckGPT methods correlate the best with human judgements among all the compared baseline methods. Further, the three variants of SelfCheckGPT appear complementary, with the combined approach being the best-performing system, achieving the highest Pearson correlation of 69.05 .

### 3.2.2 ALIGNSCORE: EVALUATING FACTUAL CONSISTENCY WITH A UNIFIED ALIGNMENT FUNCTION [56]

Github Link: https://github.com/yuh-zha/AlignScore

Tasks: NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization.

Core Idea: This paper proposes a new general factual consistency metric based on an unified
text-to-text information alignment function through unifying a wide range of data sources and NLG tasks.

Previous research, exemplified by the aforementioned SelfCheckGPT, has developed several metrics primarily reliant on specific functions such as natural language inference (NLI) or question answering (QA). However, these metrics are typically trained on limited data and are consequently confined to assessing a singular type of task, often overlooking diverse factual inconsistencies (e.g., contradictions, hallucinations) that manifest in various inputs/outputs (e.g., sentences, documents) across different tasks.

This paper introduces ALIGNSCORE as a novel, comprehensive factual consistency metric founded on a unified text-to-text information alignment function. The authors propose a model that unifies a broad spectrum of data sources, leveraging massive and diverse datasets to train a general information alignment model. This model estimates an alignment score when presented with two arbitrary pieces of text. To achieve this, the authors reformat and aggregate data from 15 datasets spanning 7 prominent language tasks, including NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization. This approach yields a total of 4.7 million training examples with diverse characteristics, resulting in an alignment function characterized by remarkable generalizability.

ALIGNSCORE is constructed using the alignment function as a foundational element. The authors conduct extensive experiments on large-scale benchmarks encompassing 22 evaluation datasets, with 19 of these datasets not encountered during the alignment training. Impressively, ALIGNSCORE demonstrates significant improvement over a broad array of previous metrics. Furthermore, with 355 million parameters, ALIGNSCORE either matches or surpasses metrics based on ChatGPT and GPT-4, despite the latter being orders of magnitude larger.

Methodology-wise, they propose the unified alignment functions. They first train the alignment function by unifying a large diversity of data sources. Then they define ALIGNSCORE by combining the alignment function with a new context/claim splitting and aggregation strategy.

The idea of the ALIGNSCORE comes from the so-called alignment function. Given two pieces of text $a$ and $b, b$ is considered to be aligned with $a$ if all information in $b$ is present in $a$ and does not contradict $a$. A typical set-up is that $b$ is a claim that the LLM makes (answer) while $a$ is the context provided to the LLM (query). Ideally, we want the claim to be supported by the context. Conceptually, one can model the information alignment as a function that maps the text pair $(a, b)$ to a label $y$ that characterizes the level of alignment, i.e., $f(a, b)=y$, which is defined to be the alignment function.

In order to train the alignment function, the authors opt to adapt and aggregate diverse language tasks to form an unified alignment training corpus (Figure 3.6). One challenge of learning across multiple data sources and NLG tasks is to unify the input and output. To unify input formats, the authors convert each sample into a text pair $(a, b)$, with specific treatments for some irregular tasks, such as QA. To unify output formats, the authors convert all tasks into a set of related alignment problems to preserve as much information as possible from the
![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-17.jpg?height=630&width=1346&top_left_y=336&top_left_x=364)

Figure 3.6: The information alignment problem and how we unify various tasks into the alignment task. We convert each sample in the tasks we consider into a text pair $(a, b)$, and the alignment function predicts a label $y$ characterizing the level of alignment. The underlined text indicates items in the original dataset (e.g., question and answer in a QA dataset) are combined to form part of the text pair in the alignment dataset. (Figure source: [56])

original datasets. To do so, they devise three options for the alignment label $y$ :

$$
\begin{aligned}
y_{\text {bin }} & \in\{A L I G N E D, N O T-A L I G N E D\} \\
y_{3 \text { way }} & \in\{A L I G N E D, C O N T R A D I C T, N E U T R A L\} \\
y_{\text {reg }} & \in[0,1]
\end{aligned}
$$

More concretely, for tasks that come with discrete labels, depending on their setup, the alignment function predicts either the binary classification label $Y_{b i n}$ (paraphrase, QA, information retrieval, and summarization) or the 3-way classification label $y_{3 w a y}$ (NLI, and fact verification); for tasks with continuous labels (semantic textual similarity), the alignment function predicts the regression label $y_{\text {reg }}$. Here a higher $y_{\text {reg }}$ indicates that more information in $b$ is supported by $a$. The authors build the alignment model with a language model (e.g., RoBERTa [60]) and 3 individual linear layers as the 3-way classification ( $y_{3}$ way ), binary classification ( $y_{\text {bin }}$ ), and regression ( $y_{\text {reg }}$ ) heads.

Then author then propose to compute the sentence level ALIGNSCORE using the result from the alignment function. Specifically, for each sentence in the claim passage, the evaluation is done against all context sentences using the alignment function. Then, the highest alignment score is selected for each claim sentence as the sentence level ALIGNSCORE. In addition, one can obtain the document-level ALIGNSCORE by averaging all the sentence level ones. Note this method is quite similar to SelfCheckGPT in Section 3.2.1. The difference here is that ALIGNSCORE assumes the context passage is given, while in SelfCheckGPT obtain the context by random generation with high-temperature LLM. Here one remark is that we can

Table 3.6: The AUC-ROC of different metrics on the SummaC benchmark. The last column (AVG) is the average performance of each metric. The dark green indicates the best metric on each dataset or on average. And the light green indicates the second best. CGS and XSF are abbreviations for CoGenSumm and XSumFaith, respectively. (Table source: $[56]$ )

| Type | Metric | CGS | XSF | PolyTope | FactCC | SummEval | FRANK | AVG |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| QA | FEQA | 53.7 | 47.6 | 54.3 | 47.9 | 48.8 | 37.2 | 48.3 |
|  | QuestEval | 60.4 | 63.6 | 77.0 | 74.2 | 74.3 | 85.8 | 72.5 |
|  | QAFactEval | 83.4 | 66.1 | 86.4 | 89.2 | 88.1 | 89.4 | 83.8 |
|  | ROUGE-1 | 69.7 | 64.5 | 82.5 | 75.8 | 87.2 | 85.0 | 77.4 |
| Matching | ROUGE-2 | 70.5 | 65.9 | 83.7 | 76.0 | 87.2 | 85.3 | 78.1 |
|  | ROUGE-L | 70.2 | 62.9 | 81.9 | 76.3 | 87.3 | 85.3 | 77.3 |
|  | BERTScore | 71.8 | 55.8 | 86.9 | 75.0 | 83.8 | 84.5 | 76.3 |
|  | NER-Overlap | 63.1 | 49.0 | 85.3 | 70.9 | 79.6 | 84.9 | 72.1 |
|  | SimCSE | 51.1 | 64.9 | 72.1 | 49.8 | 56.6 | 68.1 | 60.4 |
| NLI | 56.2 | 62.2 | 75.2 | 59.0 | 77.2 | 74.8 | 67.4 |  |
|  | BLEURT | 60.8 | 64.7 | 76.7 | 59.7 | 71.1 | 82.5 | 69.2 |
|  | MNLI | 44.9 | 46.6 | 45.0 | 48.3 | 43.5 | 59.3 | 47.9 |
|  | DAE | 52.4 | 76.7 | 72.8 | 54.2 | 66.1 | 78.9 | 66.8 |
|  | SummaC-ZS | 73.6 | 58.0 | 87.5 | 83.7 | 85.8 | 85.3 | 79.0 |
|  | SummaC-CONV | 67.2 | 70.3 | 81.8 | 92.3 | 86.1 | 88.5 | 81.0 |
| Ours | UniEval | 84.7 | 65.5 | $\mathbf{9 3 . 4}$ | 89.9 | 86.3 | 88.0 | 84.6 |
|  | CTC | 76.5 | 65.9 | 89.5 | 82.6 | 85.6 | 87.3 | 81.2 |
|  | BARTScore | 74.3 | 62.6 | 91.7 | 82.3 | 85.9 | 88.5 | 80.9 |
|  | FactCC | 64.9 | 55.1 | 78.5 | 72.7 | 71.8 | 69.8 | 68.8 |
|  | BLANC | 54.1 | 53.5 | 74.7 | 56.4 | 68.6 | 83.4 | 65.1 |

potentially combine the two methods, using ALIGNSCORE as a consistency score to plug in the SelfCheckGPT framework.

The experiments of this work focus more on the summarization. The result shown in Table 3.6 is based on the SummaC dataset [21], which standardizes the task of summary inconsistency detection by casting it as a binary classification problem. Here the context would be the source document, while the claim is the summary generated by the LLM. One can observe that ALIGNSCORE-large achieves the best average performance on the SummaC benchmark, scoring the highest in 4 out of 6 datasets. Recall the authors use RoBERTa as a base model to finetune the alignment function. For ALIGNSCORE-large in the table, it leverages RoBERTa-large as the base model and ALIGNSCORE-base uses the basic RoBERTa model as the base model.

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-19.jpg?height=571&width=1271&top_left_y=337&top_left_x=381)

Figure 3.7: An illustrative example of automated news head- line hallucination detection with a model generated natural language explanation. (Figure source: [44])

### 3.2.3 "WHY IS THIS MISLEADING?": DETECTING NEWS HEADLINE HALLUCINATIONS WITH EXPLANATIONS [44]

Github Link: No code provided.

Dataset Link: https://bit.ly/exhalder-dataset

Tasks: Summarization.

Core Idea: This paper proposes a new framework to address hallucination detection under the task of headline generation. Exhalder leverages an explainer that adapts the knowledge from public natural language inference datasets into the news domain, to further improve the hallucination detection result.

In this work, the authors propose a new framework named ExHalder to address this challenge for headline hallucination detection. Automatic news headlines generation, often viewed as a specialized document summarization task, focuses on creating headline-style summaries for news articles. Its significance has grown in rapidly conveying ongoing news to users, becoming a pivotal feature in the news domain. LLM-based approaches, especially in automatic headline generation, has become widely favored. However, addressing hallucination issues poses a critical challenge for deploying this feature in web-scale systems, demanding factual accuracy in the news domain. The proposed method integrates knowledge from a public natural language inference dataset into the news domain, generating natural language sentences to elucidate hallucination detection results. Figure 3.7 shows an example of the hallucination detection pipeline leveraging ExHalder. To evaluate the model performance, the authors collect a dataset with more than six thousand labeled (article, headline) pairs. Extensive experiments on this dataset and another six public ones demonstrate that ExHalder can identify hallucinated headlines accurately and justifies its predictions with human-readable natural language explanations.

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-20.jpg?height=414&width=1470&top_left_y=336&top_left_x=293)

Figure 3.8: The ExHalder framework overview. (Figure source: [44])

The ExHalder framework is built upon three key components for news headline hallucination detection: (1) a reasoning classifier which takes (article, headline) pairs as inputs and examine if the headline is contradicting the article and outputs the class label with the explanation of the classification; (2) a hinted classifier which receives as input the (article, headline, explanation) triplet and predicts the if the headline is hallucinated, where the explanation acts as a "hint" in this classifier, providing some extra information; (3) an explainer that generates the natural language explanation based on the input (article, headline) with its known class label. Then, in order to avoid the data scarcity issue on labeling, the authors propose to (1) pretrain all the three components with large-scale natural language inference (NLI) datasets, and (2) use augmented training with the explainer to further finetune the hinted-classifier and the reasoning classifier. Figure 3.8 shows an overview of the ExHalder framework.

In this paper, the authors collect a new dataset that contains 6,270 human curated examples, where each example consists of a triplet of (news article, news headline, hallucination label). Among these data, they split 5,190 examples for training, 349 examples for validation, and 731 examples for testing. The headline is generated from NHNet [12] and the label is obtained from multiple human experts according to a common guideline. Specifically, three fulltime journalism degree holders were involved to determine the final hallucination label of each example via majority voting. Among all the examples, 1,934 of them are labeled as "hallucinated" and the remaining 4,336 examples are labeled as "entailed" (not hallucinated). Furthermore, there are 2,074 examples with additional rater-written comments (besides binary hallucination labels) and they treat them as user-provided explanations.

Table 3.7 shows the qualitative results of ExHalder and other baseline methods. Some explanations of the abbreviations:

- ExHalder-NoPT: ExHalder without the NLI-based pretraining step.
- ExHalder-NoEX: ExHalder with the NLI-based pretraining step but without leveraging any explanation information.
- ExHalder-NoHC: ExHalder without the hinted classifier module.

In the findings, firstly, it is evident that traditional methods employing manual feature engineering (e.g., SVM, XGBoost) yield subpar results, underscoring the challenge of headline

Table 3.7: Quantitative results on the news headline hallucination detection dataset. The superscript * means the improvement is statistically significant compared to $T 5_{x x l}$. (Table source: $[44])$

| Methods | Accuracy | Precision | Recall | F1 |
| :--- | :--- | :--- | :--- | :--- |
| SVM | 57.31 | 28.65 | 20.50 | 23.90 |
| XGBoost | 60.19 | 42.39 | 60.67 | 49.91 |
| BERT-base | 73.46 | 71.43 | 31.38 | 43.60 |
| T5-xxl | 82.39 | 76.29 | 66.93 | 71.29 |
| T5-xxl+EXP | 82.62 | 78.98 | 64.15 | 70.63 |
| ExHalder-NoPT | 82.08 | 75.96 | 66.11 | 70.69 |
| ExHalder-NoEX | 83.17 | 80.01 | 64.71 | 71.54 |
| ExHalder-NoHC | 84.08 | 82.06 | 65.69 | 72.96 |
| ExHalder | $\mathbf{8 4 . 4 6}$ | $\mathbf{8 2 . 6 3}$ | $\mathbf{6 7 . 1 6}$ | $\mathbf{7 4 . 0 8}$ |

hallucination detection. This difficulty highlights the necessity for models capable of capturing nuanced semantic distinctions between the article and its headline. Secondly, the comparison between ExHalder and ExHalder-NoPT reveals that the NLI-based pretraining significantly enhances the identification of hallucinated headlines. This improvement is attributed to the model being primed with entailment task semantics. Thirdly, the performance gap between ExHalder and ExHalder-NoEX indicates additional advancements can be achieved by injecting the explanation information into the model training process. Finally, ExHalder demonstrates the most superior performance across all metrics, outperforming the second-best method by a substantial margin.

### 3.2.4 HaRiM+: Evaluating SUMmary QuALITY WITH HalLUCINation RisK [46]

Huggingface API: https://huggingface.co/spaces/NCSOFT/harim_plus

Tasks: Summarization.

Core Idea: This paper repurposes the decoder overconfidence-regularizing objective as a hallucination risks measurement and propose a hallucination detection metric, ready to be used as a Huggingface API.

In this study, the authors reinterpret the decoder overconfidence-regularizing objective suggested in [31] as a hallucination risk measurement to better estimate the quality of generated summaries. The authors propose a reference-free metric, HaRiM+ for summarization tasks. The metric only requires an off-the-shelf summarization model to compute the hallucination risk based on token likelihoods. The metric is very stright-forward to deploy, with no additional training or human-alignment. To evaluate its performance for summarization, the authors compare the proposed metric with various baselines on three annotated datasets: FRANK [36], QAGs [50] and SummEval [10], where Harim+ shows state-of-the-art correlation to human judgment.

It is a commonly-known problem where the decoder relies too much on the decoder's own
context while being less dependent on the encoder's counterpart in a encoder-decoder architecture [3]. [31] introduced a margin-based token-level objective as a regularization term that prevents the decoder from focusing too much on the decoder-side context. Under summarization task, hallucination is often considered to be not faithful to the source document, which can be seen as the encoder's context. Therefore, one can reinterpret the aforementioned regularization term as the hallucination risk, which is the core idea of Harim+. Harim+ is defined over the similar concepts of prior and posterior probabilities we mentioned in the subsection 3.1.3, where the prior probability is defined as the probability of its generation by a language model that does not have access to the source text, while the posterior probability is computed by conditioning on the source document.

They perform experiments on various datasets: FRANK [36], and QAGS annotations [50]. FRANK and QAGS contain 2,246 and 470 pairs, respectively, of article and system-generated summary from CNN-DailyMail [34] as well as BBC-XSUM [35] corpora. Every example in the benchmark contains human judgement on the corresponding example's factuality.

Table 3.8 shows the metric to human judgement (segment-level) correlation. From the table, we can observe that Harim+ shows the highest Kendall's $\tau$ in most criteria of CNN/DailyMail based benchmarks. In addition, Harim+ has the most correlation w.r.t. human judgements except several settings, i.e. XSUM and SummEval-Relevance. Another remark to make is that the proposed Harim+ prefers self-generated summary (i.e. summary generated by the same summarization model the scorer depending on) to human written references. This is likely due to the inductive bias of preferences toward summaries generated by abstractive summarization systems, as the score is based on an LLM.

### 3.2.5 HaluEvAL: A LARGE-SCALE HALLUCINATION EvALUATION BENCHMARK FOR LARGE LANGUAGE MODELS [24]

Github Link: https://github.com/RUCAIBox/HaluEval

Tasks: Question answering, knowledge-grounded dialogue, and summarization.

Core Idea: This paper proposes a benchmark for evaluating hallucinations in NLG tasks. The main framework is based on ChatGPT in a sampling-then-filtering setup.

In this work, the authors propose the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in detecting hallucination. To generate these samples, the authors propose a ChatGPT-based two-steps framework, i.e., sampling-thenfiltering. In addition, this work also utilizes human experts to annotate the hallucinations in ChatGPT responses. The authors show experimentally that ChatGPT is likely to fabricate unverifiable information and hallucinate in specific topics. Moreover, the experiments show that it is challenging to detect the hallucinations in the texts using existing LLMs. Nevertheless, the authors show that empirically, one can improve the hallucination recognition ability of the LLMs by providing external knowledge or adding reasoning steps.

The synthetic dataset generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality hallucination filtering. Both of these two steps relies heavily on prompt engineering. To sample diverse hallucinations, the authors propose two different

Table 3.8: Metric-to-human judgement correlation (segment level) reported in Kendall's $\tau$. Bold-face values are the largest correlating metrics, underlined are second-large values amongst the metrics. HaRiM+ outperforms others in most criteria. SummEval's quality criteria; consistency, coherence, fluency, and relevance are abbreviated as Con, Coh, Flu, and Rel respectively. (Table source: [46])

|  <br> Kendall's $\tau$ <br> Metrics | CNNDM |  |  |  |  |  | XSUM |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | FRANK <br> Factuality | QAGS <br> Factuality | SummEval |  |  |  | FRANK <br> Factuality | QAGS <br> Factuality |
|  |  |  | Con | Coh | Flu | Rel |  |  |
| $\mathrm{N}$ - gram - matching |  |  |  |  |  |  |  |  |
| ROUGE 1 | 0.182 | -0.052 | 0.105 | 0.123 | 0.062 | 0.209 | 0.125 | 0.110 |
| ROUGE 2 | 0.135 | -0.107 | 0.101 | 0.097 | 0.048 | 0.153 | 0.128 | 0.097 |
| ROUGE L | 0.141 | -0.072 | 0.091 | 0.113 | 0.061 | 0.164 | 0.117 | 0.090 |
| METEOR | 0.198 | 0.053 | 0.125 | 0.116 | 0.070 | 0.223 | 0.121 | 0.115 |
| sacreBLEU | 0.136 | -0.085 | 0.080 | 0.167 | 0.088 | 0.131 | 0.113 | 0.012 |
| ROUGE 1_art | 0.185 | 0.243 | 0.111 | 0.036 | 0.058 | 0.127 | -0.003 | -0.074 |
| ROUGE 2_art | 0.249 | 0.315 | 0.195 | 0.072 | 0.119 | 0.165 | 0.027 | 0.069 |
| ROUGE L art | 0.225 | 0.305 | 0.203 | 0.097 | 0.123 | 0.050 | 0.010 | -0.019 |
| METEOR_art | 0.174 | 0.234 | 0.112 | 0.009 | 0.071 | 0.091 | 0.004 | -0.052 |
| sacreBLEU_art | 0.153 | 0.245 | 0.091 | 0.042 | 0.035 |  | -0.038 | -0.139 |
| $\mathrm{N}$ - gram stats |  |  |  |  |  |  |  |  |
| NovelNgram_4 | 0.275 | 0.392 | 0.221 | 0.203 | 0.173 | 0.205 | 0.017 | 0.056 |
| NovelNgram_3 | 0.273 | 0.370 | 0.218 | 0.208 | 0.171 | 0.208 | 0.064 | 0.080 |
| NovelNgram_2 | 0.259 | 0.327 | 0.199 | 0.209 | 0.150 | 0.207 | 0.053 | 0.129 |
| NovelNgram_1 | 0.219 | 0.201 | 0.090 | 0.190 | 0.068 | 0.173 | 0.091 | 0.120 |
| Length ( no . tokens ) | 0.187 | 0.185 | 0.078 | 0.033 | 0.000 | 0.000 | -0.111 | -0.132 |
| Contextual Embedding |  |  |  |  |  |  |  |  |
| BERTScore $\mathrm{P}$ | 0.168 | -0.067 | 0.041 | 0.229 | 0.097 | 0.192 | 0.151 | 0.016 |
| BERTScore $\mathrm{R}$ | 0.250 | 0.017 | 0.125 | 0.241 | 0.097 | 0.299 | 0.107 | 0.058 |
| BERTScore F1 | 0.232 | -0.029 | 0.079 | 0.267 | 0.111 | 0.267 | 0.142 | 0.036 |
| BERTScore P_art | 0.301 | 0.331 | 0.266 | 0.308 | 0.236 | 0.308  | 0.038 | -0.039 |
| BERTScore R_art | 0.360 | 0.365 | 0.141 | 0.153 | 0.112 | 0.234 | 0.144 | -0.022 |
| BERTScore F1_art | 0.358 | 0.365 | 0.230 | 0.256 | 0.192 | 0.307 | 0.111 | -0.040 |
| Neural entailment |  |  |  |  |  |  |  |  |
| FactCC | 0.376 |  |  |  |  |  | 0.071 |  |
| Dep Entail | 0.342 |  |  |  |  |  | 0.092 |  |
| Q \& A based |  |  |  |  |  |  |  |  |
| FEQA | -0.008 |  |  |  |  |  | 0.006 |  |
| QAGS | 0.206 | 0.274 |  |  |  |  | -0.006 | 0.153 |
| QAEval - F1 |  |  |  |  |  | 0.220 * | -0.006 | 0.153 |
| Text Generation based |  |  |  |  |  |  |  |  |
| CBMI ( BART_base + cnn ) | 0.058 | 0.026 | 0.152 | -0.029 | 0.023 | 0.208 | -0.077 | -0.041 |
| BARTScore ( BART_large + cnn ) | 0.413 | 0.470 | 0.197 | 0.310 | 0.181 | 0.263 | 0.137 | 0.072 |
| BARTScore ( BART_large + cnn + para ) | 0.392 | 0.416 | 0.259 | 0.301 | 0.238 | 0.278 | 0.145 | 0.031 |
| Proposed |  |  |  |  |  |  |  |  |
| HaRiM ( BART_large + cnn ) | 0.424 | 0.478 | 0.251 | 0.315 | 0.210 | 0.284 | 0.136 | 0.076 |
| HaRiM ( BART_large + cnn + para ) | 0.399 | 0.401 | 0.281 | 0.293 | 0.245 | 0.282 | 0.141 | 0.028 |

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-24.jpg?height=611&width=1449&top_left_y=320&top_left_x=312)

Figure 3.9: Creation pipeline of our benchmark, including automatic generation (top) and human annotation (bottom). (Figure source: [24])

hallucination sampling methods. For both of these methods, different instructions of hallucination sampling are proposed and followed by ChatGPT. As shown in Figure 3.9, the first method adopts a one-pass instruction process, where a hallucinated response is obtained through one feed of prompt. As the second method, the authors use a conversational instruction schema, where ChatGPT is taught to successively learn part of the instruction via a prompt engineering based approach. Then, based on the learned instructions, ChatGPT will generate another hallucinated answer, different from the first method, resulting in diverse and multi-facet hallucinated answers for each question. The next step in the generation pipeline is the so-called high-quality hallucination filtering. To construct a challenging benchmark for evaluation, the authors aim to select the most plausible and difficult hallucinated sample from the above two sampling methods. The authors design the prompt of the hallucination filtering to select the best answer from two candidates. In the instruction of filtering, the demonstration includes the ground-truth correct answer and a hallucinated counterpart. However, in the test example, the inputs are swapped into two hallucinated answers obtained from the previous step. Through this process, the most plausible hallucinated answer is expected to be selected by ChatGPT. This way, the final selected sample can be difficult to identify.

The authors then ask human experts to annotate the general user queries and ChatGPT responses from the $52 \mathrm{~K}$ instruction tuning dataset from Alpaca [48]. A pre-selection method is applied to obtain the most challenging examples. The authors use ChatGPT to sample three responses for each user query and compute their average semantic similarity using BERTScore [57]. In the end 5, 000 user queries are retained with the lowest similarities. For each query and ChatGPT response, human experts will annotate whether the response contains hallucinated information ("Yes" or "No") and list the corresponding spans.

Table 3.9 presents results regarding the accuracy of several LLMs to recognize whether a sample (e.g., an answer, a dialogue response, or a summary) contains hallucinated information. One can observe that LLMs are typically not an expert at identifying hallucination. For example, ChatGPT cannot distinguish between factual and hallucinated summary and only achieves

Table 3.9: Accuracy (\%) of evaluation models to classify whether the sample contains hallucinated contents in HaluEval benchmark. (Table source: [24])

| Models | QA | Dialogue | Summa. | General |
| :--- | :--- | :--- | :--- | :--- |
| GPT-3 (davinci) | 49.21 | 20.02 | 51.23 | 77.54 |
| text-davinci-002 | 60.05 | 60.81 | 47.77 | 87.60 |
| text-davinci-003 | 49.65 | 68.37 | 48.07 | 87.54 |
| ChatGPT | 62.59 | 72.40 | 58.53 | 86.22 |

Table 3.10: Accuracy (\%) of ChatGPT equipped with three improvement strategies. (Table source: [24])

| Models | QA | Dialogue | Summa. | General |
| :--- | :--- | :--- | :--- | :--- |
| ChatGPT | 62.59 | 72.40 | 58.53 | 86.22 |
| w/Knowledge | 76.83 | 73.80 | - | - |
| w/CoT | 59.58 | 71.39 | 61.21 | 86.50 |
| w/Contrast | 40.19 | 68.67 | 49.46 | - |

$58.53 \%$ accuracy in text summarization, which is barely above chance.

The authors also explore three improvement on the ability of LLMs to recognize hallucination: knowledge retrieval, chain of thought reasoning prompt and sample contrast. For knowledge retrieval, the authors provide the LLM with additional facts from Wikipedia to improve the detection results. For chain of though (CoT) reasoning prompt, the authors follows the idea of CoT from [53] and construct the corresponding prompt to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. For sample contrast, the authors further provide ground truth examples for ChatGPT to test whether it can distinguish the right sample from the hallucinated sample. As one can observe from the results in Table 3.10, though limited, knowledge retrieval and CoT can offer certain degree of improvements over some tasks while sample contrast, on the contrary, is detrimental to the performance of the detection.

## 4 HalluCination Mitigation

Hallucination mitigation is the task to reduce the potential hallucinations in the generated responses by LLMs. The papers reviewed in this section includes three categories: knowledge and retrieval-based approaches [18, 9, 45], which ground LLM responses in factual data using external knowledge sources such as knowledge graphs and retrieval systems. Another category is training or reference guiding for language models, involves strategies like employing control codes [41], or contrastive learning [47] to guide the generation process to discern between correct and hallucinated content. The third category is evaluation and mitigation on the language model generated content focusing on specific hallucination types, such as employing methods to evaluate quantity entity in summarization [58], and methods to detect and mitigate self-contradictory statements [33].

### 4.1 RHO: REDUCING HALLUCINATION IN OPEN-DOMAIN DIALOGUES WITH KNOWLEDGE GROUNDING [18]

Github Link: https://github.com/ziweiji/rho

Task: Knowledge-Grounded Dialogue

Core Idea: RHO is a conversational model that reduces hallucinations by merging knowledge graph triples with dialogue history and employing both local and global grounding techniques to ensure responses align with the provided knowledge and dialogue context.

RHO [18] is a conversational model mitigating hallucination by leveraging local and global knowledge grounding techniques, combined with a response re-ranking mechanism, to ensure knowledge relevance in generated dialogue responses. The problem is defined as a special case of response generation task in dialogue systems: response generation for knowledgegrounded dialogue (KGD) task. KGD refers to a conversation where one or more than one of the participants uses external knowledge sources to drive the response generation. Response generation in dialogue systems involves using a dialogue history including a set of utterances from human participants. The KGD task uses a multi-relational knowledge graph (KG) as additional input in addition to the dialogue history. The goal is to generate a faithful response considering both the dialogue history and a subset of the relevant part of the KG.

The dataset used is OpenDialKG [32], which contains open-ended dialogues between two speakers, initiated by talking about a given entity (real-world objects or concepts such as people, places, things, ideas, or events) and dialog turns are optionally paired with corresponding grounded Knowledge Graph (KG) paths of relevant facts.In addition to the dialogues, KG triples (subject, relation, object) are also provided in the dataset. A path in a KG refers to a sequence of triples that connect multiple entities through their relationships. The sequential dialog turns can be regarded as traversing the paths in the KG. The authors filter OpenDialKG by keeping only the dialogue samples that are annotated with a KG path. Statistically, OpenDialKG contains 13,802 dialog sessions containing 91,209 total turns about four domains: movie, book, sports, and music. The KG triples in the dataset are of 100,813 entities, 1,358 relations, $1,190,658$ triples.

The RHO method starts with linearizing the related triples from the $\mathrm{KG}$ into a text format and merging it with the dialogue history. To have not only the lexical but also structured knowledge information, the authors do local knowledge grounding. TransE [2] is used to produce embeddings of entities and relation predicates from the entire KG. Then a locally grounded token embedding is produced for an arbitrary token in the dialogue history. If the token is a substring of an entity, it is mapped to the entity's embedding; If the token is a substring of a relation, it is mapped to the relation's embedding. The local embedding is produced to represent a token's association with entities or relations from a knowledge graph. Local knowledge grounding connect tokens from a dialogue to relevant information within a KG. In addition to local grounding, the authors further propose global knowledge grounding to draw global dependencies between the dialogue history and the representations of all triples (subject, predicate, and object) in the context-related sub-graph. For each triple in the sub-graph, an embedding vector is created by combining the mapped embeddings of the subject, predicate, and object of the triple. Then global knowledge embedding space is formed by projecting and

Table 4.1: Results from the automatic evaluation of RHO and its baselines are presented, where "RD" refers to reference-dependent mode, "R" refers to reference-free mode, and "Pre" refers to Precision. The results from the ablation study are presented in the last four rows. "LKG", "GKG", and "RR" correspond to local knowledge grounding, global knowledge grounding, and response re-ranking, respectively. "Full Implementation" encompasses all three elements combined, i.e., $\mathrm{LKG}+\mathrm{GKG}+\mathrm{RR}$. (Table source: [18])

| Model | BLEU4 | ROUGE - L | FeQA | QuestEval |  | Entity Coverage (\%) |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  |  |  |  | RF | Pre | Recall | F1 |  |
| EARL | 7.97 | 23.61 | 39.93 | 37.88 | 35.59 | 86.61 | 45.17 | 64.44 |
| GPT2 | 10.27 | 29.59 | $39.60 / 26.54$ | 46.86 | 42.07 | 91.62 | 33.26 | 52.30 |
| GPT2 + NPH | 10.41 | 29.93 | 40.83 / 28.98 | 47.45 | 42.45 | 95.61 | 33.39 | 53.96 |
| BART | 14.45 | 33.33 | 39.00 | 46.97 | 42.75 | 96.99 | 44.96 | 62.87 |
| BART + NPH | 15.53 | 34.99 | 42.41 | 47.94 | 43.56 | 96.44 | 44.12 | 65.98 |
| KG - BART | 13.72 | 33.31 | 41.87 | 45.55 | 42.86 | 97.68 | 45.63 | 64.58 |
| RHO ( LKG ) | 19.89 | $\mathbf{3 9 . 9 5}$ | 43.04 | 48.91 | 44.37 | 97.38 | 45.57 | 67.77 |
| RHO ( GKG ) | $\mathbf{2 0 . 7 7}$ | 39.54 | 40.65 | 48.41 | 43.84 | 97.20 | 45.63 | 67.40 |
| RHO ( LKG + GKG ) | 20.63 | 39.51 | 45.96 | 50.35 | 46.03 | 98.26 | 50.74 | 71.47 |
| RHO ( Full Implementation ) | 19.11 | 38.45 | $\mathbf{4 7 . 9 9}$ | $\mathbf{5 0 . 5 8}$ | $\mathbf{4 6 . 4 1}$ | $\mathbf{9 8 . 5 3}$ | $\mathbf{5 1 . 7 7}$ | $\mathbf{7 2 . 2 9}$ |

concatenating the embedding vectors of all the triples in the sub-graph. For each token in the dialogue linked to any entity or relation, the global token embedding is calculated by applying a softmax function to the product of the token's embedding and transposed global knowledge embedding space. This global embedding is produced based on the association with entities or relations. The encoder then integrates both local and global embeddings for each token during training. The training process involves an attention mechanism with respect to the entire sub-graph to determine how much each token should focus on different parts of global embedding space. The above encode and decoder generate $N$ candidate responses. A conversational reasoning model is then used to simulate actions that represent walking steps on the knowledge graph, These actions are based on the dialogue history and each candidate response. The actions derived from walking on the knowledge graph are constructed using embeddings of entities and relations. The mechanism calculates the probability of the actions aligning with the graph and the dialogue history for each candidate response. The response with the highest probability is then selected as the optimal.

In the evaluation conducted, BART and GPT2+NPH are noteworthy baselines. BART achieved a BLEU4 score of 14.45 and an F1 score of 62.87, while GPT2+NPH registered a BLEU4 score of 10.41 and an F1 score of 53.96. RHO implemented with both local and global knowledge grounding as well as response re-ranking, demonstrates superior performance across multiple evaluation metrics. Notably, RHO (GKG) achieves a BLEU4 score of 20.77, RHO (LKG) achieves a ROUGE-L of 39.95. Furthermore, the RHO (Full Implementation) configuration exhibits the highest Entity Coverage at 98.53, a FeQA score of 47.99 and an F1 score of 72.29. These results highlight the effectiveness of RHO, especially when all its components are implemented together.

### 4.2 Neural Path Hunter: Reducing Hallucination in Dialogue SystemS Via Path GRoUnding [9]

Github Link: https://github.com/nouhadziri/neural-path-hunter

Task: Knowledge-Grounded Dialogue

Core Idea: The Neural Path Hunter (NPH) model reduces hallucinations in knowledgegrounded dialogue by employing a generate-then-refine strategy, where after a response was generated by an LLM, a token-level fact critic identifies potentially hallucinated entities which will be refined by querying and with an external knowledge graph.

The Neural Path Hunter (NPH) [9] employs a generate-then-refine approach for LLM-based dialogue systems. The problem is that knowledge-grounded dialogue systems often suffer from the hallucination problem. The dialogues generated were incorrect or fabricated. The problem could stem from the misuse of entities in the dialogue, leading to factual inaccuracies.

In NPH, a token-level fact critic was trained to identify potential hallucination entities in a sentence, focusing especially on entity misuse. The critic will be used in the pipeline of NPH to flag entities of concern in a sentence where a binary label is predicted at each word position. The NPH critic model leverages Roberta-Large from Huggingface: ${ }^{1}$ for token classification with the training data manually introduced some negatives, i.e. replacing some correct entities to incorrect entities, or swapping the subject and object for the dialogues from OpenDialKG dataset.

The NPH pipeline starts with a language model (LM), such as GPT2, to generate a dialogue response. The critic will be used to flag potentially hallucinated entities. For each flagged entity, a representation is formed using a Masked Language Model (MLM), obtaining contextual representations, which are the representations of the sentence without that potentially hallucinates words (entities). These representations undergo a pooling operation to attain a singular representation for each entity and then an auto-regressive $\mathrm{LM}$ then formulates a query from the entity representation. The system consults the Knowledge Graph (KG) using this query, substantiating the entities by navigating the KG. KG representation is standardized using KG-Entity Memory with two methods: GPT2 embeddings or CompGCN, a Graph Convolutional Network designed for multi-relational data. A scoring function DistMult [52], evaluates each KG-Entity memory triple, selecting the entity with the highest score as the factual entity. The original response is refined using this accurate entity, ensuring the dialogue's factual correctness, finishes the whole generate-then-refine process. By integrating the KG as an external oracle and utilizing the generate-then-refine pipeline, NPH enhances factual accuracy of the dialogue system.

The results show that using the NPH approach on the OpenDialkg test data, the FeQA score for GPT2-KG increased from 26.54 to 28.98, for AdapterBot from 23.11 to 27.21, and for GPT2-KE from 19.54 to 26.21. Concurrently, hallucination critic (the percentage of data that is predicted as potentially hallucinated by the pretrained critic hallucination detector) scores decreased, with GPT2-KG reducing from 19.04 to 11.72 , AdapterBot from 26.68 to 18.51 , and GPT2-KE from 28.87 to 20.34 .[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-29.jpg?height=622&width=1468&top_left_y=543&top_left_x=294)

| Model | FeQA | Critic | BLEU |
| :--- | :--- | :--- | :--- |
| GPT2 - KG | 26.54 | 19.04 | $11.79 *$ |
| GPT2 - KG + NPH | $28.98^{*}$ | $11.72^{*}$ | 11.29 |
| GPT2 - KG + NPH - W / O NCE | 26.02 | 17.91 | 10.98 |
| GPT2 - KG + NPH - W . COMPGCN | 26.89 | 15.41 | 11.10 |
| GPT2 - KG + NPH - W / O MLM | 27.01 | 15.02 | 10.88 |
| GPT2 - KG + NPH - W / O CRITIC | 18.23 | 19.65 | 6.49 |
| AdapterBot | 23.11 | 26.68 | 10.56 |
| AdapterBot + NPH | $27.21^{*}$ | $18.51^{*}$ | $10.74^{*}$ |
| AdapterBot + NPH - W / O NCE | 24.02 | 25.02 | 9.98 |
| AdapterBot + NPH - W . COMPGCN | 25.83 | 20.23 | 10.11 |
| AdapterBot + NPH - W / O MLM | 26.02 | 21.04 | 10.06 |
| AdapterBot + NPH - W / O CRITIC | 16.21 | 27.22 | 5.64 |
| GPT2 - KE | 19.54 | 28.87 | $6.24 *$ |
| GPT2 - KE + NPH | $26.21 *$ | $20.34 *$ | 6.06 |
| GPT2 - KE + NPH - W / O NCE | 20.34 | 24.32 | 5.89 |
| GPT2 - KE + NPH - W . COMPGCN | 23.23 | 21.21 | 6.01 |
| GPT2 - KE + NPH - W / O MLM | 24.01 | 22.40 | 5.99 |
| GPT2 - KE + NPH - W / O CRITIC | 15.89 | 30.71 | 3.49 |
| Gold response | 33.34 | 5.2 | - |

Figure 4.1: (Top left): NPH overview. (Top right): Entity Mention Retriever architecture. (Bottom): Assessing the degree of hallucination of different models before and after refinement on the generated samples based on the OpenDialkg test data. A higher FeQA score suggests greater faithfulness. The hallucination Critic (Critic) measures the percentage of hallucinated responses in the dataset. (* p-value $<0.001$ ). NPH employs GPT2 embeddings for the KG-Entity Memory. (Figure and table source: [9])

Github Link: https://parl.ai/projects/hallucination/

Task: Knowledge-Grounded Dialogue

Core Idea: The Retrieval-Augmented Generation (RAG)-based method proposed reduces hallucination in knowledge-grounded dialogue systems by searching, ranking, and incorporating external documents into the encoder-decoder process, using mechanisms like Dense Passage Retrieval [19] and Poly-encoders [16], to produce factually-grounded responses.

The paper discusses the knowledge-grounded dialogue task. Studies indicated that RetrievalAugmented Generation (RAG) methods have been effective for producing more accurate responses in open-domain QA but have limitations when applied directly to multi-turn dialogue contexts. The challenge associated is to explore the various techniques related to RAG to be able to adaptively apply RAG to open-domain knowledge-grounded dialogue. The dataset used is Wizard of Wikipedia, the same as in Section 4.4.

RAG employs an encoder-decoder to encode questions and generate (decode) answers. The encoding process is enhanced with documents or passages retrieved from a set of documents using a learned matching function. The authors studied various types of architectures with multiple components of a broader category of RAG models: retrieval-augmented neural architectures. The components include retrievers, rankers, and encoder-decoders. When a user has a prompt or a query. The retriever component takes this prompt and searches for relevant documents from a large corpus. Once a set of potentially relevant documents is retrieved, the ranker sorts these documents based on their relevance to the user's prompt. This step ensures that the most pertinent information is prioritized. The encoder processes the user's prompt along with the top-ranked documents retrieved and produces an intermediate representation. This representation captures the essential information from both the user's input and the external knowledge sourced from the database. The decoder then uses this representation to generate a response. The encoder-decoder enables the system to provide responses that are grounded in factual information.

The authors discussed various retrieval augmentation mechanisms.Dense Passage Retrieval (DPR) [19] is a retrieval strategy that employs a dual-encoder architecture to score dialogue context-document pairs. Given a dialogue context, DPR retrieves relevant documents by computing a relevance score between the context and each document in a corpus. It encodes the queries and the documents separately. The term dense opposes sparse retrieval method, as it maps both the query and the documents into continuous vector spaces. Poly-encoders [16] produce a set of vectors (codes) to attentively process the context token outputs of a transformer encoder. These codes capture different distinct semantic aspects or features of the context. Each code interacts with a candidate's vector to compute a score. These scores from all codes are then combined for a final score for that candidate.The Fusion-in-Decoder (FiD) [17] mechanism fuses independent encoder outputs before decoding the final generation. While DPR and Poly-encoder are retrieval strategies used to find relevant documents or passages given the dialogue context, FiD is for integrating the information from these retrieved documents into the final response generated by the model. FiD then separately encodes the dialogue context and each retrieved document using a transformer-based encoder. The

Table 4.2: Comparison of Seq2Seq Models and Retrieval Augmentations on Wow Test (Seen). Perplexity (PPL) scores aren't directly comparable between various seq2seq models due to their use of distinct dictionaries. Retrieval models are retrieving five documents over all of Wikipedia. All RAG models are RAG-Token. (Table source: [45])

| Seq2Seq Model | Retrieval Mechanism | PPL | F1 | Knowledge F1 | BLEU-4 | ROUGE-L |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| BlenderBot - 400m | None | 11.2 | 19.7 | 16.3 | 1.4 | 18.8 |
|  | RAG DPR | 9.0 | 21.1 | 23.7 | 3.0 | 21.2 |
| BART - Large | RAG DPR - Poly | 9.7 | 21.1 | 24.2 | 3.0 | 21.0 |
|  | None | 14.7 | 20.9 | 17.4 | 1.7 | 20.3 |
|  | FiD | 13.7 | 20.8 | 21.5 | 2.5 | 21.2 |
|  | RAG DPR | 12.7 | 22.4 | 22.5 | 3.4 | 22.9 |
|  | RAG DPR - Poly | 11.4 | $\mathbf{2 2 . 9}$ | 26.5 | 3.9 | $\mathbf{2 3 . 5}$ |
|  | FID - RAG DPR | 11.8 | 21.1 | 29.6 | 3.8 | 22.7 |
|  | FID - RAG DPR - Poly | 11.4 | 22.1 | 29.7 | 4.1 | 23.0 |
|  | None | 12.1 | 19.3 | 14.6 | 1.0 | 18.1 |
|  | RAG DPR | 9.8 | 21.9 | 25.9 | 3.8 | 22.1 |
|  | FID - RAG DPR | 9.5 | 22.0 | 27.8 | 3.9 | 22.3 |

separate encodings are then fused together (combining all of the outputs from the encoder) in the decoding step.

Table 4.2 demonstrates the performance of retrieval strategies on the Wizard of Wikipedia Test (Seen) dataset. The RAG DPR-Poly version achieves a Knowledge F1 score of 26.5 and a ROUGE-L rating of 23.5, respectively. Additionally, FiD-RAG DPR-Poly demonstrates improved results, securing a Knowledge F1 score of 29.7 and a ROUGE-L score of 23.0. For the BLEU-4 metric, FiD-RAG DPR-Poly achieves a score of 4.1, notably higher than BART-Large None score of 1.7.

### 4.4 INCREASING FAITHFULNESS IN KNOWLEDGE-GROUNDED DIALOGUE WITH CONTROLLABLE FEATURES [41]

## Github Link: N/A

Task: Knowledge-Grounded Dialogue

Core Idea: The control codes introduced reduce hallucination in knowledge-grounded dialogue systems by having these special tokens named control codes to guide the language model in producing responses that emphasize lexical precision, objective voice, and entailment; this is achieved by integrating these codes during the training phase to reduce hallucinations and by employing these code in a resampling method during the decoding phase.

Rashkin et al. proposed a method to enhance the faithfulness of responses in knowledgegrounded dialogue systems by incorporating control codes [41]. The problem is how to ensure that make knowledge-grounded dialogue systems generated conversational responses remain faithful to the provided evidence, especially when the conversation datasets contain both evidence-based and subjective responses. The authors introduced control codes, which are special tokens that will be used to influence the LM by being added to the input or designed to guide the output from language models.

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-32.jpg?height=557&width=1332&top_left_y=330&top_left_x=362)

Figure 4.2: System framework: a large pre-trained language model is employed to encode both the evidence and conversation history leading to produce a response. Additional tokens (i.e. control codes) are incorporated to train the model to recognize differences between types of utterance based on their alignment with the evidence. During the decoding phase, the authors investigate the significance of using resampling methods. (Figure source: [41])

To produce more grounded responses that adhere closely to source documents, the authors proposed two control code-based methods: one that integrates control code features based on evaluation measures as special tokens prepended to the dialogue inputs of during training. The combination of the control codes is to reduce the hallucination by forcing the model to be more aware of how the response relies on the knowledge evidence in the decoding to avoid speculation. The other method employs a form of resampling to directly restrict the output to match the proposed evaluation measures. During the decoding, the resampling method is used to continue to sample until found one satisfied the evaluation measures. The evaluation measures mentioned in both methods include objective voice, lexical precision, and entailment. Lexical precision is calculated as the proportion of words in the generated response that are present in the evidence. Objective voice is assessed by checking the presence of first-person pronouns in the response. This is a binary measure, where a response is marked as objective (having an objective voice). Entailment is a measure for to which extent the response is semantically entailed by the source document. It is calculated using a pretrained Natural Language Inference (NLI) model.

The dataset used is th Wizard of Wikipedia [7], a multi-turn knowledge-grounded dialogues dataset. The conversation is between an apprentice and a wizard. The wizard has access to Wikipedia documents. Evidence spans from the documents are labeled for every utterance by the wizard. The dataset consists of 73,571 training responses, with development sets of 3,905 for seen topics and 3,898 for unseen topics, and test sets of 3,842 for seen topics and 3,902 for unseen topics. It is noteworthy that $44 \%$ of the training responses employ first-person language, with an average lexical precision with respect to evidence at 0.43 , and only $23 \%$ of the responses being predicted as entailed by the evidence.

Table 4.3: Experimental results on the seen/unseen topic portions of the Wizard of Wikipedia test set. The metrics presented include BLEU-4, the percentage of replies that avoid using the first person (N1P), precision and recall of words in the response with the evidence, and the fraction of responses that are predicted to be entailed by the evidence. (Table source: [41])

|  | Seen Topic |  |  |  | Unseen Topic |  |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Model | BLEU | N1P | w / Evid. | NLI | BLEU | N1P | w / Evid. | NLI |  |  |
|  | B4 | $\%$ | Prec | Rec | $\%$ | B4 | $\%$ | Prec | Rec | $\%$ |
| E2E model | 1.5 | 48.0 | 47.9 | 30.4 | 29.3 | 0.3 | 37.6 | 33.2 | 21.7 | 9.5 |
| dodecaDialogue | $\mathbf{1 0 . 0}$ | 78.3 | 81.1 | $\mathbf{6 7 . 7}$ | 70.3 | $\mathbf{9 . 7}$ | 77.7 | 81.3 | $\mathbf{6 6 . 5}$ | 70.6 |
| GPT - 2 ( none ) | 6.2 | 50.9 | 56.1 | 49.4 | 34.2 | 5.7 | 52.1 | 56.4 | 48.3 | 34.2 |
| GPT - 2 ( control codes ) | 7.8 | 99.3 | 76.6 | 61.5 | 73.8 | 7.6 | 99.2 | 77.0 | 60.3 | 74.0 |
| GPT - 2 ( resampling ) | 7.6 | 75.1 | 70.4 | 57.7 | 71.4 | 7.2 | 76.2 | 70.3 | 56.5 | 72.3 |
| GPT - 2 ( both ) | 8.9 | $\mathbf{9 9 . 9}$ | 83.1 | 66.3 | 93.9 | 8.4 | $\mathbf{9 9 . 8}$ | 83.2 | 64.7 | $\mathbf{9 4 . 4}$ |
| T5 ( none ) | 7.6 | 51.1 | 64.0 | 51.9 | 45.1 | 7.4 | 51.4 | 65.2 | 51.9 | 44.9 |
| T5 ( control codes ) | 8.6 | 99.7 | 84.3 | 62.1 | 89.0 | 8.5 | 99.4 | 85.0 | 61.5 | 89.8 |
| T5 ( resampling ) | 8.2 | 77.5 | 73.3 | 55.5 | 74.7 | 8.1 | 78.5 | 74.4 | 55.5 | 76.3 |
| T5 ( both ) | 8.4 | 99.8 | $\mathbf{8 5 . 0}$ | 62.1 | $\mathbf{9 4 . 0}$ | 8.7 | $\mathbf{9 9 . 8}$ | $\mathbf{8 6 . 1}$ | 62.2 | $\mathbf{9 4 . 4}$ |

The performance of various models on the Wizard of Wikipedia test sets of seen topic and unseen topic in the Table 4.3. The GPT-2 model, when enhanced with control codes, achieved a BLEU-4 score of 7.8, 7.6, up from its baseline of $6.2,5.7$. When integrated with resampling and control code, the model achieves the highest NIP of 99.9, 99.8 on seen topic and unseen topic, respectively. Similarly, the T5 model demonstrated improvements, achieving an NIP of 99.8 , NLI of 94.4 on unseen topic.

### 4.5 CONTRASTIVE LEARNING REDUCES HALLUCINATION IN CONVERSATIONS [47]

Github Link: https://github.com/sunnweiwei/mixcl

Task: Knowledge-Grounded Dialogue

Core Idea: The Mixed Contrastive Learning (MixCL) method reduces the hallucination produced by pre-trained language models in conversational systems by applying contrastive learning and data mixing at a fine-grained span level, emphasizing span-level contrasts to differentiate between correct and hallucinated entities without full retraining.

Pre-trained language models used in conversational systems may hallucinate by generating plausible but factually incorrect statements. The challenge is that the commonly used training method, maximum likelihood estimation (MLE) with teacher forcing, encourages models to merely replicate training data, leading to inaccuracies during querying real-world information. In order to tackle this issue, the authors proposes Mixed Contrastive Learning (MixCL). This method combines contrastive learning with data mixing to help the model differentiate between correct and hallucinated information. Firstly, through negative sampling, MixCL identifies confusing information (negative knowledge) that the model is likely to mix up. With mixed contrastive learning, the method contrasts this negative knowledge with the correct

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-34.jpg?height=245&width=1445&top_left_y=320&top_left_x=294)

Figure 4.3: Overview of MixCL: MixCL is comprised of two steps. The first step, (i) negative sampling: generates negative knowledge to produce the data for the model. The second stage, (ii) mixed contrastive learning, aims to reduce the generation of negative tokens by employing mixed contrastive learning. (Figure source: [47])

positive knowledge to create a mixed training example. MixCL operates at a fine-grained span level, using named entity recognition (NER) for intrinsic hallucinations and constituency parsing for extrinsic ones, to ensure precision in the knowledge it presents. This approach helps in training the $\mathrm{LM}$ to be more accurate in its responses. The dataset is the Wizard-of-Wikipedia the same as described in section 4.4.

Figure 4.3 presents an overview of the MixCL method. The method consists of two primary steps: negative sampling and mixed contrastive learning. The negative sampling step produces the most confusing negative knowledge samples that the model might misinterpret. Two techniques are employed for negative sampling: retrieved negatives, which uses a retrieval tool to obtain irrelevant knowledge from the corpus, and model-generated negatives, which exploits a model bootstrapping approach to produce knowledge where hallucinations exist. The mixed contrastive learning step emphasizes fine-grained span-level contrasts by extracting meaningful spans from both positive and negative knowledge, mixing these spans to produce new examples, and then optimizing the model through contrastive learning with a mixed-contrast loss function at the span level. In the MixCL method, the loss function is MixCL is optimized with a combination of three loss functions: $J(\theta)=\alpha_{1} L_{\mathrm{MLE}}+\alpha_{2} L_{\mathrm{MCL}}+\alpha_{3} L_{\mathrm{LM}}\left(\alpha_{1}, \alpha_{2}, \alpha_{3}\right.$ are weights for the corresponding losses). The mixed-contrast loss ( $L_{\mathrm{MCL}}$ ) function is to train the model's ability to discern between reliable and hallucinated content. Complementing this, the Maximum Likelihood Estimation ( $L_{\mathrm{MLE}}$ ) loss function promotes the replication of training data patterns for the model's general language generation capabilities. Additionally, the language modeling loss function ( $L_{\mathrm{LM}}$ ) is employed to alleviate knowledge forgetting.

Table 4.4 presents evaluation results for various conversational models on the Wizard-ofWikipedia dataset, categorized into KB-based methods under realistic conditions (retrieving knowledge from the entire Wikipedia corpus without narrowed down search space), KB-based methods under oracle conditions (models are provided with a smaller subset of the Wikipedia corpus that definitely contains the required knowledge), and LM-based methods. For the KBbased models under realistic conditions, KnowledGPT achieved the F1 score of 21.1. However, in the LM-based category, MixCL exhibits superior performance with an F1 score of 21.6 for the test seen set surpassing the KB-based models under realistic conditions. For test seen set, MixCL achieves a ROUGE-L of 20.5 and BLEU-2 of 9.2, displaying a notable advantage over other LM-based approaches.

Table 4.4: Evaluation results from the Wizard-of-Wikipedia dataset. The first group lists KBbased methods under realistic conditions. The second group lists KB-based methods under oracle conditions. The third group lists LM-based methods, including MixCL. Notably, MixCL results that markedly surpass the previous-best LM-based approaches. And MixCL outcomes the leading KB-based methods under realistic conditions. (Table source: [47])

| Method | Test seen |  |  |  |  |  |  |  | Test unseen |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | F1 | $\mathrm{RL}$ | B2 | $\mathrm{B} 4$ | MT | $\mathrm{KF} 1$ | EF1 | Acc | F1 | $\mathrm{RL}$ | B2 | $\mathrm{B} 4$ | $\mathrm{MT}$ | $\mathrm{KF} 1$ | EF1 | Acc |
| KB-based methods under realistic conditions |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| TMN | 17.3 | 17.0 | 5.7 | 1.1 | 14.8 | 15.8 | 8.7 | 15.2 | 14.4 | 14.5 | 3.3 | 0.3 | 11.5 | 9.4 | 2.1 | 8.6 |
| DukeNet | 18.5 | 17.7 | 6.4 | 1.9 | 16.0 | 18.5 | 12.0 | 20.6 | 15.9 | 15.9 | 4.8 | 1.1 | 13.7 | 14.7 | 8.0 | 14.3 |
| KnowledGPT | 21.1 | 20.1 | 8.9 | 3.4 | 20.0 | 22.2 | 15.5 | 24.3 | 19.5 | 18.4 | 8.0 | 2.6 | 18.3 | 20.0 | 11.7 | 20.2 |
| KnowBART | 21.1 | 18.9 | 8.5 | 3.3 | 17.8 | 21.3 | 16.2 | 24.2 | 21.0 | 18.3 | 8.9 | 3.6 | 17.9 | 22.5 | 16.2 | 24.0 |
| KB-based methods under oracle conditions |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| DukeNet | 19.3 | 18.7 | 7.5 | 2.5 | 17.2 | 19.6 | 13.2 | 22.1 | 17.1 | 17.0 | 6.0 | 1.7 | 15.2 | 16.5 | 9.2 | 16.8 |
| KnowledGPT | 22.0 | 20.8 | 9.9 | 3.7 | 20.9 | 23.8 | 16.9 | 26.3 | 20.5 | 19.5 | 8.7 | 3.0 | 19.3 | 22.1 | 13.3 | 22.6 |
| KnowBART | 22.1 | 19.6 | 9.1 | 3.7 | 18.1 | 23.1 | 18.0 | 26.8 | 22.7 | 20.1 | 9.8 | 4.3 | 18.7 | 24.1 | 18.4 | 27.5 |
| LM-based methods |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| GPT-2 | 19.6 | 18.5 | 7.8 | 1.4 | 17.8 | 17.9 | 13.3 | 15.4 | 18.3 | 17.3 | 6.5 | 0.8 | 16.1 | 14.6 | 7.2 | 8.4 |
| BlenderBot | 18.8 | 19.4 | 7.7 | 2.3 | 18.0 | 18.2 | 13.1 | 16.7 | 17.8 | 16.9 | 5.5 | 0.8 | 15.0 | 15.7 | 7.1 | 9.6 |
| KnowExpert | 18.7 | 18.6 | 6.7 | 1.3 | 16.5 | 14.1 | 9.8 | 12.6 | 16.7 | 17.2 | 5.4 | 0.6 | 14.5 | 11.8 | 5.5 | 9.2 |
| MSDP | 17.8 | 16.5 | 6.1 | 1.9 | 18.2 | 21.7 | 13.9 | 18.4 | 16.9 | 16.1 | 5.5 | 1.1 | 16.2 | 20.3 | 8.4 | 16.1 |
| $\overline{\text { MixCL }}$ | 21.6 | 20.5 | 9.2 | 2.7 | 20.5 | 22.3 | 16.3 | 20.4 | 19.6 | 18.8 | 7.4 | 1.4 | 18.0 | 18.0 | 11.6 | 14.4 |

### 4.6 REDUCING QUANTITY HALLUCINATIONS IN ABSTRACTIVE SUMMARIZATION [58]

Github Link: N/A

Task: Summarization

Core Idea: The HERMAN model proposed in this paper corrects the hallucinated quantitative entities in abstractive summaries using an encoder-decoder mechanism. Leveraging a Bidirectional LSTM encoder, it contextualizes each token from the source, while the decoder, supplemented with an attention mechanism, processes the summary. HERMAN checks the quantitative entities' factual consistency against the source using this architecture and reranks the summaries to favor those that align with the original text's quantities.

The task is to reduce the hallucinated entities in LM-based abstractive summarization systems. The authors particularly focus on the hallucinations of quantity entities such as datas, numbers, and money amount. The hallucinated quantity entities that are not supported by the source text, compromise the accuracy and of the generated summaries. The challenge is to identify and correct these inaccuracies without compromising the informativeness of the summary.

The authors leveraged the XSum dataset [35], a well-known summarization dataset that consists of 226,711 news articles accompanied with a one-sentence summary, to produce the dataset for their experiment. They first filtered the Xsum entries to pinpoint those containing the mentions of quantitative entities such as dates, numbers, and money sums. The authors also extracted more instances where these entities were pivotal to the content's meaning. The produced dataset contains both filtered set of original data and also synthetic data, where the

![](https://cdn.mathpix.com/cropped/2024_06_04_338a5b34be05e9c917f1g-36.jpg?height=531&width=1451&top_left_y=328&top_left_x=311)

Figure 4.4: Architecture of HERMAN. LSTM decoder and Conditional random fields (CRF) modeling methods are employed. (Figure source: [58])

selected quantitative entities in the summaries are replaced with other entities of the same type from the corresponding source texts. This replacement was done randomly, creating variations in the data. Each summary was then manually reviewed and annotated with VERIFIED and UNVERIFIED (with quantitative entities replacement).

The authors proposed HERMAN model to recognize and verify quantity entities. HERMAN checks the factual consistency of these quantity entities with the source text. When coupled with other summarization models, HERMAN re-ranks the generated summaries, giving preference to those that have quantities consistent with the original text. The training data for HERMAN contains both original and synthetically created data with the token-level labels (Y labels) and sentence level labels (z labels). Figure 4.4 demonstrates the architecture of HERMAN. The HERMAN model comprises an encoder for the source article and a decoder for the summary. The encoder produces hidden state representations for every token in the article using a Bidirectional LSTM where each token in the article gets encoded into a contextualized token-level representation. The hidden states (context vectors) produced by the encoder are then passed to the attention layer when decoding the summary. The decoder processes the summary. It uses the context vectors from the attention layer and its own hidden states to produce a series of labels (Y labels) for each token in the summary using a combination of a Bidirectional LSTM and a Conditional Random Fields layer, supplemented with an attention mechanism. The output Y labels classify the quantity entities in the summaries. V: VERIFIED, U: UNVERIFIED, O: tokens that are not directly related to any quantity-based entity, B: Begin, I: Inside (BIO tagging scheme). Also, the same context vectors are fed into a binary classifier to predict a z label of whether the entire summary is 'VERIFIED' or 'UNVERIFIED'.

The HERMAN model has two re-ranking strategies: HERMAN-GLOBAL and HERMANLOCAL. The HERMAN-GLOBAL approach uses the global document-level output label z. The HERMAN-LOCAL approach considers the individual token-level y labels to calculate an average verification score for the summary. Table 4.5 summarizes the performance of the systems BART [23], BERTSUM [27], and TCONVS2S [35] on the XSum [35] test set using ROUGE scores (R1, R2, RL). HERMAN-GLOBAL outperformed other models in ROUGE-1/2/L Precision

Table 4.5: Results from the automatic evaluation of the XSum test set are organized into three parts, each pertaining to one of the three abstractive summarization systems: BART, BERTSUM, and TCONVS2S. For every system, ROUGE scores are provided for two baseline models, the original model, and two versions of the HERMAN model. Baseline-shortest model chooses the shortest summary. Baseline-max-overlap model selects for the summary with the highest entity overlap with the source document. The term 'avg-Q' represents the average count of quantity entities within each summary. (Table source: [58])

|  | Model | R1-R | R1-P | R1-F | R2-R | R2-P | R2-F | RL-R | RL-P | RL-F | avg-Q |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  | Baseline-shortest | 45.50 | $\mathbf{4 6 . 9 5}$ | $\mathbf{4 5 . 4 0}$ | 21.86 | $\mathbf{2 2 . 6 1}$ | $\mathbf{2 1 . 8 3}$ | 36.80 | $\mathbf{3 8 . 0 1}$ | $\mathbf{3 6 . 7 4}$ | 0.69 |
|  | Baseline-max-overlap | 49.46 | 41.66 | 44.55 | 23.35 | 19.57 | 20.97 | 39.30 | 33.08 | 35.38 | $\mathbf{0 . 9 5}$ |
|  | Original | $\mathbf{4 9 . 6 4}$ | 41.54 | 44.57 | $\mathbf{2 3 . 4 3}$ | 19.50 | 20.96 | $\mathbf{3 9 . 3 9}$ | 32.95 | 35.36 | 0.89 |
|  | HERMAN-LOCAL | 48.51 | 42.78 | 44.73 | 22.97 | 20.20 | 21.14 | 38.70 | 34.12 | 35.68 | 0.88 |
|  | HERMAN-GLOBAL | 47.88 | 43.52 | 44.79 | 22.66 | 20.56 | 21.17 | 38.26 | 34.79 | 35.80 | 0.92 |
| BERTSUM | Baseline-shortest | 36.78 | $\mathbf{4 2 . 2 6}$ | 38.71 | 15.61 | $\mathbf{1 7 . 8 7}$ | 16.38 | 29.71 | $\mathbf{3 3 . 9 1}$ | 31.16 | 0.62 |
|  | Baseline-max-overlap | 38.17 | 41.25 | 39.01 | $\mathbf{1 6 . 2 8}$ | 17.50 | 16.58 | 30.66 | 32.94 | 31.24 | 0.76 |
|  | Original | 38.37 | 40.73 | 38.86 | 16.24 | 17.13 | 16.38 | $\mathbf{3 0 . 7 5}$ | 32.44 | 31.04 | 0.65 |
|  | HERMAN-LOCAL | $\mathbf{3 8 . 4 5}$ | 40.14 | 38.63 | 16.12 | 16.72 | 16.12 | 30.71 | 31.87 | 30.75 | 0.79 |
|  | HERMAN-GLOBAL | 37.99 | 41.59 | $\mathbf{3 9 . 0 6}$ | 16.24 | 17.70 | $\mathbf{1 6 . 6 5}$ | 30.59 | 33.28 | $\mathbf{3 1 . 3 6}$ | $\mathbf{0 . 8 1}$ |
| TCONVS2S | Baseline-shortest | 27.43 | $\mathbf{3 7 . 2 8}$ | 30.99 | 9.84 | $\mathbf{1 3 . 4 9}$ | 11.15 | 22.43 | $\mathbf{3 0 . 4 1}$ | 25.32 | 0.45 |
|  | Baseline-max-overlap | 30.19 | 34.57 | 31.64 | 10.79 | 12.34 | 11.29 | 24.37 | 27.81 | 25.50 | 0.71 |
|  | Original | $\mathbf{3 0 . 4 2}$ | 34.63 | 31.80 | 10.96 | 12.46 | 11.45 | $\mathbf{2 4 . 5 8}$ | 27.89 | 25.66 | 0.58 |
|  | HERMAN-LOCAL | 29.95 | 34.50 | 31.43 | 10.59 | 12.16 | 11.09 | 24.17 | 27.72 | 25.31 | 0.75 |
|  | HERMAN-GLOBAL | 30.36 | 34.82 | $\mathbf{3 1 . 8 5}$ | $\mathbf{1 0 . 9 8}$ | 12.59 | $\mathbf{1 1 . 5 1}$ | 24.56 | 28.08 | $\mathbf{2 5 . 7 2}$ | $\mathbf{0 . 7 8}$ |

and F1 scores.

### 4.7 SELF-CONTRADICTORY HALLUCINATIONS OF LARGE LANGUAGE MODELS: EVAlUATION, DeteCtion AND MitiGATION [33]

Github Link: https://github.com/eth-sri/chatprotect

Task: Question answering

Core Idea: The self-contradictory method detects and mitigates hallucinatory content in LLMs by deliberately provoking self-contradictions in generated responses. Using specific input prompts, an LM (gLM) model generates paired statements that are analyzed by another model (aLM) for logical inconsistencies, and once identified, iterative text edits are applied to remove these contradictions.

The task here is to detect and reduce the produced nonsensical or unfaithful content (the hallucinations) in the responses of large LMs like ChatGPT. This method is grounded in the observation that LLMs may produce self-contradictions where they generate two logically inconsistent sentences given the same context. By intentionally provoking these inconsistencies, detecting and removing the contradicted content, the authors intend to reduce the hallucinatory content of the LLM-producing responses. The dataset is the Wikipedia articles of 30 topics such as architecture, artist historical, politics, sports, Television, etc., collected by the authors called the MainTestSet, where the LLM generates 1,000+ sentence pairs.

This method is applied to black-box LMs through a system of prompting without needing

| Real-world <br> example I: |  <br>  | The PlayStation 4 (PS4) is a home video game console developed by Sony. <br> Released in 2013 , it is the eighth generation of consoles in the playStation series. <br> It is the fourth generation of the playStation console series. <br> The PlayStation 4 (PS4) was released in 2013 . |
| :---: | :---: | :---: |
| Real-world <br> example II: |  <br>  | Gwen Jorgensen is a retired professional triathlete from the United States. <br> She currently lives in Minnesota with her husband, Patrick Lemieux, and their children. <br> She currently lives in Portland, Oregon with her husband and two children. <br> She currently lives with her husband and children. |

Figure 4.5: Two real-world self-contradictory hallucinations produced by ChatGPT based on the provided context are showcased. Sentences highlighted in green are factually correct, while those in red are incorrect. The method proposed identifies and mitigates both hallucinations. (Figure source: [33])

external knowledge. The first step involves intentionally input prompts that are likely to induce contradictory responses to a language model gLM. The model gLM will be expected to generate pairs of statements that could potentially be at odds with each other. The responses of the LM being tested are examined to detect logical inconsistencies or self-contradictions. The presence of contradictions indicates a potential hallucination. Following the generation of these pairs, an analyzer model aLM is employed to detect contradictions within the result sentences. This analyzer model is trained on recognizing logical inconsistencies to be able to flag the contradiction part of a sentence for further mitigation. Once contradictions have been detected, the next phase involves the application of iterative text edits. These edits are designed to eliminate or reconcile the identified contradictions while still ensuring the output remains fluent.

Table 4.6: Results on the MainTestSet with gMLs, aLMs as control variables. Table source: [33]

| Results on the MainTestSet with different gLMs, with ChatGPT as aLM. |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $g L M$ | Detection |  |  | Mitigation |  |  |
|  | $\mathrm{P}$ | $\mathrm{R}$ | F1 | self-contra. reduced | informative facts retained | perplexity increased |
| GPT-4 | $80.1 \%$ | $79.7 \%$ | $79.9 \%$ | $76.3 \%$ | $99.9 \%$ | 0.67 |
| ChatGPT | $84.2 \%$ | $83.2 \%$ | $83.7 \%$ | $89.5 \%$ | $100.8 \%$ | 0.44 |
| Llama2-70B-Chat | $72.9 \%$ | $89.0 \%$ | $80.2 \%$ | $85.4 \%$ | $95.5 \%$ | 0.86 |
| Vicuna-13B | $74.4 \%$ | $83.2 \%$ | $78.6 \%$ | $82.7 \%$ | $95.1 \%$ | 1.78 |
| Results on the MainTestSet with different aLMs. with ChatGPT as gLM. |  |  |  |  |  |  |
|  | Detection |  |  | Mitigation |  |  |
| $\mathrm{aLM}$ | $\mathrm{P}$ | $\mathrm{R}$ | F1 | self-contra. reduced | informative facts retained | perplexity increased |
| GPT-4 | $91.3 \%$ | $82.1 \%$ | $86.5 \%$ | $79.7 \%$ | $97.7 \%$ | 0.52 |
| ChatGPT | $84.2 \%$ | $83.2 \%$ | $83.7 \%$ | $89.5 \%$ | $100.8 \%$ | 0.44 |
| Llama2-70B-Chat | $95.3 \%$ | $45.8 \%$ | $61.9 \%$ | $44.4 \%$ | $98.8 \%$ | 0.22 |
| Vicuna-13B | $90.0 \%$ | $25.1 \%$ | $39.3 \%$ | $25.5 \%$ | $100.2 \%$ | 0.28 |

Both the gLM and aLM can be any of the state-of-the-art language models. In the experiment, ChatGPT (3.5), GPT-4, Llama2-70B-Chat, and Vicuna-13B are utilized and compared. Table 4.6

## Prompt:

Here is the start of a description about William T. Freeman: William T. Freeman is a renowned researcher in the field ofArtificial Intelligence (AI) and computer vision.

Please generate the next sentence of this description. The generated sentence must fill the gap in this triple: (He; was born; _)

gLM: He was born in 1960.

prompt for gLM.gen_sentence.

## Prompt: $\square$

Here is the start of a description about William T. Freeman. William T. Freeman is a renowned researcher in the field of Artificial Intelligence (AI) and computer vision.

Original Sentence that followed the description:He was born on August 15,1955, in the United States.

However, there is a contradiction with this sentence: He was born in 1960 .

Remove the conflicting information from the sentence.

aLM: William T. Freeman was born in the United States.

## Prompt:

Here is the start of a description about William T. Freeman. Then follow two statements.

Description:

William T. Freeman is a renowned researcher in the field ofArtificial Intelligence (Al) and computer vision.

Statement 1:

He was born on August 15, 1955, in the United States.

Statement 2:

He was born in 1960 .

Please explain if the statements about William T. Freeman are contradictory.Provide your explanation only.

aLM:

The statements about are contradictory

Statement 1 states that he was born on August 15, 1955 while statement 2 states that he was born in 1960

These two statements cannot both be true at the same time

Prompt: Please conclude with Yes or No.

aLM: Yes

prompt for aLM.detect.

prompt for aLM.revise.

Figure 4.6: The prompts for the entire process. (Figure source: [33])

reveals that when using ChatGPT as aLM, the proposed approach is consistently effective across different gLMs. The detection reaches a F1 score of $\sim 80 \%$. The mitigation can remove a self-contradictions up to $89.5 \%$ with ChatGPT as gLM. The approach also maintains informativeness and ends up with small text fluency harming (small perplexity increase). When using ChatGPT as gLM and comparing the language models as aLM, Vicuna-13B performed notably poorly as an aLM in terms of detection metrics. Both two open-source language model Llama2-70B-Chat and Vicuna-13B shows low ability to reduce the self-contradictions. Both proprietary LMs GPT-4 and ChatGPT demonstrated strong performance in terms of detection and removing self-contradictions.

## 5 CONCLUSION

In this report, we briefly review state-of-the-art hallucination detection and mitigation methods and report some of the reproduced results of these papers. For hallucination detection, the reviewed papers mainly come from two categories: token-level detection and sentence-level detection, with each type of methods focusing on different granularity of the detection. The mitigation approaches often concern with either mitigate the hallucinations pre-generation or post-generation. Pre-generation methods typically involves tricks in training/finetuning while post-generation approaches often consider leveraging knowledge retrieval methods. In addition to reviewing the literature, we have also reproduced some of the results from these papers. Most of the cases we have enough data/hyper-parameter settings to reproduce the results almost perfectly. However, there are cases (such as Hades), where the lack the hyper-
parameter information brings some difficulties in reproducing the results exactly. We hope that this report can be a valuable reference for future hallucination detection and mitigation works, especially in real-world applications.

## REFERENCES

[1] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.

[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.

[3] Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 10-21, 2016.

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[5] Meng Cao, Yue Dong, and Jackie Cheung. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340-3354, Dublin, Ireland, May 2022. Association for Computational Linguistics.

[6] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440$8451,2020$.

[7] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2018.

[8] Esin Durmus, He He, and Mona Diab. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070, Online, July 2020. Association for Computational Linguistics.

[9] Nouha Dziri, Andrea Madotto, Osmar R Zaiane, and Avishek Joey Bose. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2197-2214, 2021.

[10] Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. Summeval: Re-evaluating summarization evaluation. arXiv preprint arXiv:2007.12626, 2020.

[11] Tanya Goyal and Greg Durrett. Evaluating factuality in generation with dependency-level entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592-3603, Online, November 2020. Association for Computational Linguistics.

[12] Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, You Wu, Cong Yu, Daniel Finnie, Hongkun Yu, Jiaqi Zhai, and Nicholas Zukoski. Generating representative headlines for news stories. In 29th International World Wide Web Conference, WWW 2020, pages 1773-1784. Association for Computing Machinery, Inc, 2020.

[13] Mandy Guo, Zihang Dai, Denny Vrandečić, and Rami Al-Rfou. Wiki-40b: Multilingual language model dataset. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 2440-2452, 2020.

[14] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, 2022.

[15] Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear, 2017.

[16] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In International Conference on Learning Representations, 2019.

[17] Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, 2021.

[18] Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and Pascale Fung. Rho: Reducing hallucination in open-domain dialogues with knowledge grounding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4504-4522, 2023.

[19] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, 2020.

[20] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACLHLT, pages 4171-4186, 2019.

[21] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. SummaC: Revisiting NLI-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177, 2022.

[22] Rémi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas, November 2016. Association for Computational Linguistics.

[23] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, 2020 .

[24] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models, 2023.

[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics.

[26] Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and William B Dolan. A token-level reference-free hallucination detection benchmark for free-form text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6723-6737, 2022.

[27] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China, November 2019. Association for Computational Linguistics.

[28] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models, 2023.

[29] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Mqag: Multiple-choice question answering and generation for assessing information consistency in summarization. arXiv preprint arXiv:2301.12307, 2023.

[30] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, 2020.

[31] Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, and Jie Zhou. Prevent the language model from being overconfident in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th

International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3456-3468, 2021.

[32] Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th annual meeting of the association for computational linguistics, pages 845-854, 2019 .

[33] Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. arXiv preprint arXiv:2305.15852, 2023.

[34] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, page 280. Association for Computational Linguistics, 2016.

[35] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.

[36] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online, June 2021. Association for Computational Linguistics.

[37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002.

[38] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Belgium, Brussels, October 2018. Association for Computational Linguistics.

[39] Amy Pu, Hyung Won Chung, Ankur P Parikh, Sebastian Gehrmann, and Thibault Sellam. Learning compact metrics for mt. In Proceedings of EMNLP, 2021.

[40] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. https://api.semanticscholar.org/CorpusID:160025533, 2019.

[41] Hannah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. Increasing faithfulness in knowledge-grounded dialogue with controllable features. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 704-718, 2021.

[42] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

[43] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of ACL, 2020.

[44] Jiaming Shen, Jialu Liu, Dan Finnie, Negar Rahmati, Michael Bendersky, and Marc Najork. " why is this misleading?": Detecting news headline hallucinations with explanations. arXiv preprint arXiv:2302.05852, 2023.

[45] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784-3803, 2021.

[46] Seonil (Simon) Son, Junsoo Park, Jeong-in Hwang, Junghwa Lee, Hyungjong Noh, and Yeonsoo Lee. HaRiM ${ }^{+}$: Evaluating summary quality with hallucination risk. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, pages 895-924, Online only, November 2022. Association for Computational Linguistics.

[47] Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. Contrastive learning reduces hallucination in conversations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13618-13626, 2023.

[48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[50] Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online, July 2020. Association for Computational Linguistics.

[51] Yong Wang, Longyue Wang, Shuming Shi, Victor OK Li, and Zhaopeng Tu. Go from the general to the particular: Multi-domain translation with domain transformation networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9233-9241, 2020 .

[52] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the AAAI conference on artificial intelligence, volume 28, 2014.

[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

[54] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.

[55] Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran Associates, Inc., 2021.

[56] Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. AlignScore: Evaluating factual consistency with a unified alignment function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1132811348, Toronto, Canada, July 2023. Association for Computational Linguistics.

[57] Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020.

[58] Zheng Zhao, Shay B Cohen, and Bonnie Webber. Reducing quantity hallucinations in abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2237-2249, 2020.

[59] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 1393-1404, 2021.

[60] Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. A robustly optimized BERT pre-training approach with post-training. In Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 1218-1227, Huhhot, China, August 2021. Chinese Information Processing Society of China.


[^0]:    ${ }^{1}$ https://huggingface.co/roberta-large

