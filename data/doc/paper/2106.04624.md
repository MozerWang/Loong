# SpeechBrain: A General-Purpose Speech Toolkit 

Mirco Ravanelli ${ }^{1,2}$, Titouan Parcollet ${ }^{3,16}$, Peter Plantinga ${ }^{4}$, Aku Rouhe ${ }^{5}$, Samuele Cornell ${ }^{6}$,<br>Loren Lugosch ${ }^{1,7}$, Cem Subakan ${ }^{1}$, Nauman Dawalatabad ${ }^{8}$, Abdelwahab Heba ${ }^{9}$,<br>Jianyuan Zhong ${ }^{1}$, Ju-Chieh Chou ${ }^{10}$, Sung-Lin Yeh ${ }^{11^{*}}$, Szu-Wei Fu ${ }^{12}$, Chien-Feng Liao ${ }^{12}$,<br>Elena Rastorgueva ${ }^{13}$, François Grondin ${ }^{14}$, William Aris ${ }^{14}$, Hwidong Na ${ }^{15}$, Yan Gao ${ }^{16}$,<br>Renato De Mori ${ }^{3,7}$, and Yoshua Bengio ${ }^{1,2}$<br>${ }^{1}$ Mila - Quebec AI Institute<br>${ }^{2}$ Université de Montréal<br>${ }^{3}$ LIA - Avignon Université<br>${ }^{4}$ Ohio State University<br>${ }^{5}$ Aalto University<br>${ }^{6}$ Università Politecnica delle Marche<br>${ }^{7}$ McGill University<br>${ }^{8}$ Indian Institute of Technology Madras<br>${ }^{9}$ IRIT - Université Paul Sabatier<br>${ }^{10}$ Toyota Technological Institute at Chicago<br>${ }^{11}$ University of Edinburgh<br>${ }^{12}$ Academia Sinica, Taiwan<br>${ }^{13}$ NVIDIA<br>${ }^{14}$ Université de Sherbrooke<br>${ }^{15}$ Samsung-SAIT<br>${ }^{16}$ CaMLSys - University of Cambridge


#### Abstract

SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.


## 1 Introduction

Open-source toolkits have played a critical role in the development of speech processing technology [1-5]. Kaldi [5], for instance, is an established speech recognition framework, which is implemented in $\mathrm{C}^{++}$with recipes built on top of Bash, Perl, and Python scripts. Despite being efficient, its use of $\mathrm{C}^{++}$can make prototyping of new deep learning methods difficult. With the advent of generalpurpose deep learning libraries like TensorFlow [6] and PyTorch [7], more flexible speech recognition frameworks have quickly appeared, e.g., DeepSpeech [8], RETURNN [9], PyTorch-Kaldi [10], Espresso [11], Lingvo [12], Fairseq [13], ESPnet [14], and NeMo [15].[^0]

Table 1: List of speech tasks and corpora that are currently supported by SpeechBrain.

| Task | Description | Techniques | Datasets |
| :---: | :---: | :---: | :---: |
| Speech recognition | Speech-to-text. | CTC \|24] <br> Transducers [25] <br> CTC+Attention [26] <br> Shallow fusion [27] | LibriSpeech [28] <br> Common Voice [29 <br> AISHELL [30] <br> TIMIT [31] |
| Speaker recognition | Speaker verification/ID. | X-vectors [32] <br> ECAPA-TDNN [33] | VoxCeleb1 [34] <br> VoxCeleb2 [35] |
| Speaker diarization | Detect who spoke when. | Spectral Clustering \|36| <br> Neural embeddings [37\| | AMI corpus $\lfloor 38]$ |
| Speech enhancement | Noisy to clean speech. | MetricGAN+ [39] <br> Mimic Loss 40$]$ | VoiceBank [41] <br> DNS [42] |
| Speech separation | Separate overlapped speech. | ConvTasNet[43] <br> DualPath RNNs [44] <br> SepFormer [45] | WSJ-mix [46] <br> WHAM 47$]$ <br> WHAMR [48\| <br> LibriMix $49 \mid$ |
| Spoken language <br> understanding | Speech to intent/slots. | Decoupled [50] <br> Multistage [51] <br> Direct [52] | TAS [50] <br> SLURP [53] <br> FSC [54] |
| Multi-microphone <br> processing | Combining input signals. | Delay-and-sum <br> MVDR [55] <br> GEV [56] <br> GCC-PHAT [57] <br> SRP-PHAT [58] <br> MUSIC [59] | Dataset- <br> Independent |

Recently, task-specific libraries have also been released. Examples are Asteroid [16] for speech separation, pyannote [17] and sidekit [18] for speaker diarization, and s3prl [19] for self-supervised speech representations. While excelling at specific tasks, these frameworks have different coding styles, standards, and programming languages, making it challenging and time-consuming to migrate from one codebase to another. Moreover, their combination in complex speech processing pipelines poses a challenge for interoperability, as connecting different frameworks might be unnatural and their codebases can interact in unpredictable ways.

Our experience suggests that having a single, flexible, multi-task toolkit can significantly speed up the development of speech technologies. Due to growing interest in end-to-end spoken dialog systems (e.g., virtual assistants), implementing composite pipelines within an integrated toolkit offers many advantages. A single toolkit, for instance, encourages the exploration of transfer learning and joint training techniques across different tasks [20--23] and enables the creation of fully differentiable graphs where multiple technologies are trained jointly and learn to interact.

Inspired by this vision, we have developed SpeechBrain ${ }^{3}$, an all-in-one PyTorch-based toolkit designed to facilitate the development, portability, and ease of use of speech processing technologies. The name SpeechBrain highlights the need for a holistic system capable of performing multiple tasks at once, for example, recognize speech, understanding its content, language, emotions, and speakers. Our toolkit is not only intended for speech researchers, but also for the broader machine learning community, enabling users to easily integrate their models into different speech pipelines and compare them with state-of-the-art (SotA) baselines. Our main contributions in this paper are:

- The presentation of SpeechBrain, with an emphasis on how we designed it to support multiple tasks without sacrificing simplicity, modularity, or flexibility.
- The implementation and experimental validation of both recent and long-established speech processing models with SotA or competitive performance on a variety of tasks (cf. Table 1 .

More broadly, we believe the SpeechBrain toolkit has the potential to significantly accelerate research and innovation in the field of speech processing and deep learning.[^1]

## 2 Related Work

A few other toolkits support multiple speech tasks. Of these, the ones we consider most related to SpeechBrain are Fairseq [13], NeMo [15], and ESPnet [14]. Fairseq is developed by Facebook to support sequence-to-sequence processing. It includes models such as ConvS2S [60], transformers [61], and wav2vec [62]. However, speech processing encompasses several paradigms outside of sequence-to-sequence modeling. SpeechBrain also supports regression tasks (e.g., speech enhancement, separation), classification tasks (e.g., speaker recognition), clustering (e.g., diarization), and even signal processing techniques (e.g., multi-microphone combination).

NeMo is a toolkit for conversational AI developed by NVIDIA, which provides useful neural modules for many speech processing tasks, including speech recognition, speaker diarization, voice-activity detection and text-to-speech. Due to its industrial orientation, NeMo offers efficient ready-to-use models, such as Jasper [63], QuartzNet [64], and Citrinet [65]. SpeechBrain also provides several ready-to-use models, but focuses more heavily on research and education by providing a wide variety of baselines, models, and recipes that users can easily inspect and modify in the experiments.

ESPnet, in its current form, is the closest toolkit to SpeechBrain. Both are academically driven and support numerous speech tasks. ESPnet started as an end-to-end speech recognition library and progressively grew to support different tasks. By contrast, we designed SpeechBrain to address a wide variety of tasks from the outset. This means that combining technologies and developing recipes for new tasks is extremely simple.

## 3 Design Principles

Beyond the multi-task vision highlighted in the introduction, we developed SpeechBrain with the following design principles in mind:

Accessibility: SpeechBrain is designed to be easily understandable by a large user base, including early students and practitioners. Therefore, we devoted considerable effort to develop intuitive modules that are easy to interconnect with each other. One remarkable peculiarity of SpeechBrain is that it serves educational purposes as well. We thus have written extensive documentation and tutorials with Google Colab to help newcomers become more familiar with speech technologies. Prior work has shown code snippets aid in adopting a codebase [66]. Motivated by this, SpeechBrain provides runnable code snippets in docstrings (documenting interaction at the granular level), tutorial notebooks (explaining single topics), and template files (describing full experiments on different tasks). To make our toolkit as accessible as possible, we have released it under a very permissive license (Apache 2.0).

Ease of use: SpeechBrain employs a simple software stack (i.e., Python $\rightarrow$ PyTorch $\rightarrow$ SpeechBrain) to avoid dealing with too many levels of abstractions. It is developed on top of PyTorch directly, without an external API. PyTorch-compatible code works in our toolkit without any further modification. SpeechBrain has a minimal list of external dependencies that are all installable via PyPI. The installation process simply requires running the command pip install speechbrain and is done within a few minutes. The code is Pythonic and maximizes the use of PyTorch routines.

Replicability: SpeechBrain promotes open and transparent science. We trained most of our models with publicly available data. This way, our results can be easily replicated by the community. Several pre-trained models, which only require a few lines of code to use, are distributed via Hugging Face [67]. Besides sharing the code and the trained models, we also share the whole experiment folder, which contains all the needed details (e.g., logs) to reproduce our results.

## 4 Architecture

From an architectural standpoint, SpeechBrain sits in between a library and a framework. Where libraries require users to manage dataflow by calling library-defined functionality, frameworks primarily define a custom lifecycle in which user-defined functionalities are invoked in specific places (inversion of control). Most code in SpeechBrain follows a library-style collection of modular and standalone building blocks, including practical routines for data loading, decoding, signal processing, and other convenient utilities. However the central Brain class (see $\S 4.4$, uses inversion of control

![](https://cdn.mathpix.com/cropped/2024_06_04_355b8ba240db6377bef2g-04.jpg?height=591&width=615&top_left_y=255&top_left_x=403)

Figure 1: An overview of a basic training script.

![](https://cdn.mathpix.com/cropped/2024_06_04_355b8ba240db6377bef2g-04.jpg?height=570&width=574&top_left_y=257&top_left_x=1125)

Figure 2: Illustration of Brain.fit().

to define a general training loop. Therefore, SpeechBrain is most accurately described as a toolkit. As shown in Figure 1, the code for training a model is contained within a single Python script. Training begins by calling the script with a set of hyperparameters: python train.py hparams.yaml . These hyperparameters, declared in human-readable YAML format, contain the location of one or more data manifest files using either CSV or JSON formats (see Appendix A.4). Unlike many other toolkits, SpeechBrain orchestrates experiments in Python directly, without relying on external Bash scripts. This allows code for data loading, modeling, optimization, and evaluation to interact naturally. Moreover, the training script exposes the computations likely to be changed most frequently (e.g., forward computations, data transformations, etc.), making them easy to access and modify. SpeechBrain treats the user's code as a first-class citizen: all PyTorch-compatible code written by the user is treated the same as SpeechBrain code. In the following sub-sections, we explore the anatomy of a training script in more detail.

### 4.1 Hyperparameters

The model hyperparameters, in conjunction with the training script, regulate various properties of the pipeline such as model architecture, training, and decoding. SpeechBrain relies on an extended version of YAML called HyperPyYAML, as shown in the following excerpt:

```
dropout: 0.2
features: !new:speechbrain.lobes.features.MFCC
    n_mels: 40
    left_frames: 5
    right_frames: 5
model: !new:torch.nn.LSTM
    input_size: 440
    hidden_size: 256
    num_layers: 4
    dropout: !ref <dropout>
    bidirectional: True
```

Listing 1: An excerpt of a YAML file for hyperparameter specification.

HyperPyYAML is not just an ordinary list of hyperparameters, but allows a complex hyperparameter specification that defines objects along with their corresponding arguments. There is always an explicit reference between the hyperparameter declarations and any object using them, making the code more interpretable and simpler to debug. Overriding the contents of the YAML file (e.g., for hyperparameter search) can also be done easily by passing command-line arguments:

\$ python train.py hparams.yaml --learning_rate $=0.1$--dropout $=0.5$

SpeechBrain initializes the classes automatically when reading the YAML file, thus eliminating boilerplate initialization code from the training script. HyperPyYAML is a general tool for specifying hyperparameters. To enable modular reusuability, we have released it as a separate repository on PyP! ${ }^{4}$

### 4.2 Data loading

SpeechBrain complements standard PyTorch data loading by addressing the typical challenges that occur when working with speech, such as handling variable-length sequences, large datasets, and complex data transformation pipelines. Our DynamicItemDataset inherits from torch.utils.data.Dataset and creates a dataset-interface based on a data-manifest file. The datamanifest file contains static items, such as filepaths or speaker labels. Then, dynamic items provide transformations based on the existing items (static or dynamic), as shown in the following example:

```
@speechbrain.utils.data_pipeline.takes("file_path")
@speechbrain.utils.data_pipeline.provides("signal")
def audio_pipeline(file_path):
    return speechbrain.dataio.read_audio(file_path)
```

Listing 2: An example of a custom data pipeline.

This function takes an audio file path (a static item) and reads it as a tensor called "signal" (a dynamic item). Any library for reading audio file can be used here, including torch.audid 5 The evaluation order of the items is determined by a dependency graph. Users can define operations such as reading and augmenting an audio file, encoding a text label into an integer, basic text processing, etc. The dynamic items are defined in the training script and are thus directly customizable by the users. Moreover, by leveraging the PyTorch DataLoader class, these data pipelines are automatically applied in parallel across different workers.

### 4.3 Batching

Speech sequences for a given dataset typically vary in length and require zero-padding to create equal-length batches. This tends to add some complication during the training process. First, the length of each sentence within each batch must be tracked so we can later remove zero-padded elements from computations like normalization, statistical pooling, losses, etc. Another issue that arises is how to avoid wasting computational resources processing zero-padded elements.

One approach to mitigate this issue is to sort data by sequence length before batching, which minimizes zero-padding but sacrifices randomness in the batch creation process. A more sophisticated approach is to apply dynamic batching [68, 69], where sentences are clustered by length and sampled within the same cluster, a trade-off between random and sorted batching. This allows the batch size to be dynamically changed according to sentence length, leading to improved efficiency and better management of available GPU memory. All the aforementioned batching strategies are supported by SpeechBrain, allowing users to choose the approach that meets their specific needs.

### 4.4 The Brain class

SpeechBrain implements a general training loop in the Brain class. The Brain.fit() method is inspired by similar methods in libraries such as Scikit-learn [70], Scipy [71], Keras [72], fastai [73], and PyTorch Lightning [74]. Figure 2|illustrates the basic components of the Brain.fit() method. The following is a simple demonstration:

```
import torch, speechbrain
class SimpleBrain(speechbrain.Brain):
    def compute_forward(self, batch, stage):
    return self.modules.model(batch["input"])
```

4 github.com/speechbrain/HyperPyYAML
5 https://github.com/pytorch/audio

Table 2: Phoneme Error Rate (PER\%) achieved with SpeechBrain on TIMIT using different speech recognizers.

| Technique | \# Params | Dev | Test |
| :--- | :---: | :---: | :---: |
| CTC | $10 \mathrm{M}$ | 12.34 | 14.15 |
| Transducer | $10 \mathrm{M}$ | 12.66 | 14.12 |
| CTC+Att | $10 \mathrm{M}$ | 12.74 | 13.83 |
| CTC+Att+SSL | $318 \mathrm{M}$ | $\mathbf{7 . 1 1}$ | $\mathbf{8 . 0 4}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_355b8ba240db6377bef2g-06.jpg?height=434&width=616&top_left_y=241&top_left_x=1096)

Figure 3: Evolution of the SotA performance for TIMIT. Entries marked with * use extra unlabelled data from the Libri-Light dataset. Source: https://paperswithcode.com.

```
def compute_objectives(self, predictions, batch, stage):
        return torch.nn.functional.l1_loss(predictions, batch["target"])
modules = {"model": torch.nn.Linear(in_features=10, out_features=10)}
brain = SimpleBrain(modules, lambda x: torch.optim.SGD(x, 0.1))
data = [{"input": rand(10, 10), "target": rand(10, 10)}]
brain.fit(epoch_counter=range(15), train_set=data)
```

Listing 3: Training a simple model with SpeechBrain using the Brain class.

With only about ten lines of code, we can train a neural model. Repetitive boilerplate, such as setting train() and eval() flags, putting the models on the specified device, and computing gradients are handled by the Brain class. Users can override any step of the process, allowing the definition of more complicated (e.g., GAN [75]) training procedures. The Brain class also handles validation, learning rate scheduling, and fault-tolerant model checkpointing, so that training can resume where it left off if execution is interrupted (e.g., by preemption on a cluster). Further details about the Brain API are provided in $\S$ A.4.4

### 4.5 Other features

Beyond the functionalities mentioned in the previous sections, additional features include:

Multi-GPU training: SpeechBrain supports both DataParallel and DistributedDataParallel modules, allowing the use of GPUs on the same and different machines. Automatic mixed-precision can be enabled by setting a single flag to reduce the memory footprint of the models. Moreover, the library supports PyTorch's Just-In-Time (JIT) compiler for native compilation.

Large-scale experiments: SpeechBrain extends WebDatase ${ }^{6}$ with on-the-fly dynamic batching and bucketing. This enables efficient batching in sequential shard-based data reading, which is necessary for processing large corpora on network filesystems.

On-the-fly feature generation: Rather than serializing intermediate features to disk, SpeechBrain loads raw waveforms and supports a wide variety of efficient streaming operations for audio processing. Standard features like the Short-Term Fourier Transform (STFT) and Mel-filterbanks are computed at training time, allowing differentiation and waveform-level augmentation [76]. Many recipes include on-the-fly augmentations such as adding noise, time warping, or feature masking.[^2]

## 5 Results

This section describes use cases of SpeechBrain, highlighting the techniques implemented and the corresponding performance. For more details on datasets, models, and experimental settings, please refer to the appendix $(\$$ A.5).

### 5.1 Speech recognition

The toolkit supports common techniques for end-to-end speech recognition with different levels of complexity. The simplest system employs an encoder trained with Connectionist Temporal Classification (CTC) [77]. An alternative model is the Transducer [25], which augment CTC with an autoregressive component and a prediction network. The toolkit supports attention-based encoderdecoder architectures as well [26]. In particular, CTC+Att systems rely on an encoder-decoder architecture with an additional CTC loss applied on the top of the encoder. SpeechBrain is designed such that users can easily plug in any encoder and decoder modules into the speech recognition pipeline. For instance, we implemented an effective CRDNN encoder, which combines convolutional, recurrent (e.g., LSTM [78], GRU [79], Light GRU [80]), and fully connected neural networks. As an alternative, users can plug in one of the transformers that we have made available. Pre-training based on self-supervised learning (SSL) with wav2vec 2.0 [62] is supported.

We also implemented an efficient GPU-based beam search that combines the acoustic and the language information to retrieve the final sequence of words. The training scripts for language models and tokenizers (using SentencePiece [81]) are provided as well. In the following, we report the performance achieved with SpeechBrain recipes on some popular speech benchmarks.

### 5.1.1 TIMIT

TIMIT [31] is a small speech dataset with expert-labeled phone sequences. Table 2 reports the Phone Error Rate (PER) achieved with the aforementioned techniques. All systems use a CRDNN encoder, except for the CTC+Att+SSL one which uses a pre-trained wav2vec 2.0 encoder [62]. We report the average performance out of five runs with different random seeds. The standard deviation ranges between $0.15 \%$ and $0.2 \%$ in all the models.

CTC and Transducers provide similar results, while the combination of CTC and attention (CTC+Att) reaches the best performance. The results achieved by our best model (PER 13.8\%) is SotA for TIMIT performance with no extra data. A considerable improvement in PER is observed when Light-GRUs [80] are used instead of GRUs [79] or LSTMs [78] in the CRDNN encoder. We also observe a performance boost when using self-supervised pre-training with the wav $2 \mathrm{vec}$ model trained on unlabelled data from the Libri-Light dataset (CTC+Att+SSL) [82]. Our result with this Libri-Light self-supervised pre-training (PER of $8.04 \%$ ) slightly outperforms the previous SotA performance with the same pre-training data (PER of $8.30 \%$ ), as shown in Figure 3

### 5.1.2 LibriSpeech

LibriSpeech [28] is a popular speech recognition benchmark derived from audiobooks. Table 3 reports the results achieved with different SpeechBrain recipes on this dataset.

Table 3: Word Error Rate (WER\%) achieved on LibriSpeech with SpeechBrain.

| Technique | Encoder | Decoder | \# Params | test-clean | test-other |
| :--- | :--- | :--- | :--- | :---: | :---: |
| CTC+Att | CRDNN | GRU | $230 \mathrm{M}$ | 2.91 | 8.07 |
| CTC+Att | Transformer | GRU | $161 \mathrm{M}$ | 2.46 | 5.77 |

Our best system is a transformer [61] combined with a convolutional front-end based on ContextNet [83]. The autoregressive decoder estimates $5 \mathrm{k}$ subword tokens derived from running byte-pair encoding on top of the training transcriptions [81]. A transformer-based LM is trained on the LibriSpeech text corpus and used within the beam search to rescore partial hypotheses. The best WER that we have achieved on the test-clean dataset is $2.46 \%$. This performance is comparable with the results reached in the literature when using transformers without additional data [84]. As

Table 4: Equal Error Rate (EER \%) achieved on VoxCeleb1 - Cleaned dataset.

| Technique | EER(\%) |
| :--- | :---: |
| VoxCeleb2 baseline [35] | 3.95 |
| Kaldi x-vector [32] | 3.10 |
| ResNET-50 [87] | 1.19 |
| ECAPA (original paper) [33] | 0.87 |
| SpeechBrain x-vector + PLDA | 3.20 |
| SpeechBrain ECAPA | 0.81 |
| SpeechBrain ECAPA (vox1+2) | $\mathbf{0 . 6 9}$ |

Table 5: Diarization Error Rate (DER\%) on the eval set of the AMI corpus.

| Technique | Known <br> \# spks | Estim. <br> \# spks |
| :--- | :--- | :--- |
| MCGAN [88] | 4.49 | 5.38 |
| ClusterGAN [88] | 3.91 | 8.16 |
| xvector+MCGAN [88] | 4.23 | 4.92 |
| xvector+ClusterGAN [88] | 3.60 | $\mathbf{2 . 8 7}$ |
| VBx (ResNet101) [89] | - | 4.58 |
| SpeechBrain ECAPA | $\mathbf{2 . 8 2}$ | 3.01 |

one can note, the LibriSpeech task is almost perfectly solved by modern speech recognizers. We thus focus on more realistic tasks as well, as suggested in some recent works [85, 86]. See the appendix ( $\$$ A.2) for a more detailed comparison with other toolkits on LibriSpeech and other tasks.

### 5.1.3 Common Voice

The Common Voice corpus [29] is a multilingual open-source collection of transcribed speech based on crowdsourcing data collection. CommonVoice is challenging due to significant accented speech, hesitations, presence of foreign words, noise, reverberation, and other recording artifacts.

Table 6 reports the results obtained on four different languages. No language models are trained for this task. The best results are obtained with a wav $2 \mathrm{vec} 2.0$ encoder pre-trained with 100k hours of multilingual data from the VoxPopuli dataset [90]. Except for English, the best systems use a GRU decoder on the top of the pre-trained transformer. CommonVoice is a newer dataset, and there have been relatively few systems evaluated on it. To the best of our knowledge, however, our results are SotA for these languages.

Table 6: Word Error Rate (WER\%) achieved with Common Voice Corpus 6.1 using SpeechBrain on the English (En), French (Fr), Italian (It), and Kinyarwanda (Kw) subsets.

| Technique | Encoder | Decoder | \# Params | En | Fr | It | Kw |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CTC+Att | CRDNN | GRU | $148 \mathrm{M}$ | 24.89 | 17.70 | 16.61 | 24.27 |
| CTC+SSL | Transformer | - | $320 \mathrm{M}$ | $\mathbf{1 5 . 5 8}$ | 14.44 | 10.93 | 23.12 |
| CTC+Att+SSL | Transformer | GRU | $330 \mathrm{M}$ | 15.69 | $\mathbf{1 3 . 3 4}$ | $\mathbf{9 . 8 6}$ | $\mathbf{1 8 . 9 1}$ |

### 5.2 Speaker recognition and diarization

SpeechBrain implements the functionalities needed to support speaker recognition and speaker diarization. It supports popular embeddings derived from Time Delay Neural Networks (TDNNs) [91, 92], such as x-vectors [32] and the recent ECAPA-TDNN embeddings [33]. Furthermore, SpeechBrain provides traditional Probabilistic Linear Discriminant Analysis (PLDA) for speaker discrimination [93, 94].

Table 4 reports the performance achieved on a speaker verification task with models trained on VoxCeleb2 [35] and tested on VoxCeleb1-clean [34]. The best model for speaker embeddings available in SpeechBrain is the ECAPA-TDNN, which matches the performance achieved in the original paper [33]. This model outperforms both the x-vectors [32] and the ResNet-34 [87] by a large margin. To the best of our knowledge, the EER reached so far by SpeechBrain on VoxCeleb is the best so far reached by an open-source toolkit.

Table 5 reports the performance achieved on speaker diarization with the AMI meeting corpus [38] when using the embeddings available in SpeechBrain. In this case, the embeddings are clustered with spectral clustering to assign a relative speaker label to each segment of the recording [37]. The results shown are obtained on the official Full-ASR split of the AMI corpus while keeping 0.25 sec of forgiveness collar. The best diarization system available in SpeechBrain outperforms recent

Table 7: Speech enhancement performance on VoiceBank-DEMAND.

| Technique | \# Params | PESQ | COVL |
| :--- | :---: | :---: | :---: |
| Facebook <br> DEMUCS [95] | $60.8 \mathrm{M}$ | 3.07 | 3.63 |
| SpeechBrain <br> Mimic Loss | $22.3 \mathrm{M}$ | 3.05 | $\mathbf{3 . 7 4}$ |
| SpeechBrain <br> MetricGAN+ | $1.9 \mathrm{M}$ | $\mathbf{3 . 1 5}$ | 3.62 |

Table 8: Scale-invariant signal-to-noise ratio improvement (SI-SNRi) in dB achieved with SpeechBrain on WSJ2mix and WSJ3mix.

| Technique | 2-mix | 3-mix |
| :--- | :--- | :--- |
| ConvTasnet | 15.3 | 12.7 |
| DualPath-RNN | 18.8 | 14.7 |
| SepFormer | 20.4 | 17.6 |
| SepFormer+DM | $\mathbf{2 2 . 3}$ | $\mathbf{1 9 . 5}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_355b8ba240db6377bef2g-09.jpg?height=450&width=624&top_left_y=244&top_left_x=1100)

Figure 4: Evolution of the speech enhancement performance (PESQ) for Voicebank-DEMAND.

![](https://cdn.mathpix.com/cropped/2024_06_04_355b8ba240db6377bef2g-09.jpg?height=447&width=620&top_left_y=839&top_left_x=1094)

Figure 5: Evolution of the SotA performance (SISNRi) on the wsj2mix dataset. Source: https://paperswithcode.com

approaches based on meta-learning (MCGAN/ClusterGAN) [88], and Variational Bayes (VBx) [89] when the number of speakers is known (e.g., in a meeting). We have also obtained competitive results when the number of speakers is unknown.

### 5.3 Speech enhancement and separation

SpeechBrain supports speech enhancement models with different input features (e.g., spectral and waveform domain) and training losses (e.g., L1, MSE, and STOI). In addition, it supports a variety of more sophisticated multi-model training techniques such as Mimic Loss [40] and MetricGAN+ [39].

In Table 7 we compare the best enhancement systems available in SpeechBrain against the SotA DEMUCS model [95] on the Voicebank-DEMAND corpus [96]. The mimic loss system uses a speech recognition model to provide a perceptual loss, achieving SotA performance on the COVL metric. Combining models for different tasks (as done here) is natural to implement in SpeechBrain. We also re-implemented the recently proposed MetricGAN+, which performs speech enhancement with an adversarially trained metric network [75]. Figure 4]shows the evolution of the PESQ performance on this corpus over the last few years. The SpeechBrain implementation of MetricGAN+ achieves the SotA PESQ performance when no extra data are used.

SpeechBrain implements popular models for speech separation as well, namely ConvTasnet [43] and Dual-path RNN [44]. Moreover, it supports the recently proposed SepFormer [45], which uses a pipeline of two transformers within a dual-path framework. Table 8 reports the results achieved on the standard WSJ0-2mix and WSJ0-3mix datasets [46], which contain mixtures composed of two or three overlapped speakers, respectively. The last row compares performance achieved with dynamic mixing, in which the training data are generated dynamically on-the-fly instead of using a frozen dataset. As shown in Figure 5. SpeechBrain's SepFormer implementation achieves SotA on both datasets.

## 6 Limitations and Future Work

The current version of SpeechBrain supports many other tasks, including spoken language understanding, keyword spotting, multi-microphone signal processing, and language modeling. The toolkit also supports complex [97] and quaternion neural networks [98]. Please refer to A.3 for further details. It does not currently support text-to-speech, which will be added shortly (pending pull-requests under review). In the future, we plan to support decoding with Finite State Transducers (FSTs) [99] and are considering to adopt the FST implementation of the ongoing k2 project [100] once stable. We plan to devote further effort to real-time speech processing, which was not the main focus of the first release. Finally, our goal is to add support for additional languages and further expand the set of recipes to open-source datasets not yet available in the toolkit (e.g., TED-LIUM [101]).

## 7 Conclusion

This paper described SpeechBrain, a novel, open-source, all-in-one speech processing toolkit. Our work illustrated the main design principles behind this toolkit and remarked on the design principles that led us to support multiple tasks without sacrificing simplicity, modularity, or flexibility. Finally, we showed several use cases where the technology developed in SpeechBrain reaches SotA or competitive performance. The main contribution to the scientific community is the development of a novel toolkit that can significantly accelerate future research in the fields of speech processing and deep learning. SpeechBrain is a coordinated effort towards making speech processing technology accessible, and are eager to see where its rapidly growing community of users takes the project in the future.

## Acknowledgments and Disclosure of Funding

We would like to sincerely thank our generous sponsors: Samsung, Dolby, Nvidia, Nuance, ViaDialog. Special thanks to our institutional partners: Mila, LIA (Avignon University), CaMLSys (University of Cambridge), Sherbrooke University, and Bio-ASP (Academia Sinica). We also would like to acknowledge Breandan Considine, Olexa Bilaniuk, Frederic Osterrath, Mirko Bronzi, Anthony Larcher, Ralf Leibold, Salima Mdhaffar, Yannick Estève, Yu Tsao, Abdelmoumene Boumadane for helpful comments and discussions. We would like to express our gratitude to all the pre-release beta-testers and to the whole community that we are building around this project. Thanks to Compute Canada for providing computational resources and support. SpeechBrain was also granted access to the HPC resources of IDRIS under the allocation 2021-AD011012633 made by GENCI.

## References

[1] S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland. The HTK Book. Entropic Cambridge Research Laboratory, Cambridge, United Kingdom, 2002.

[2] D. Huggins-Daines, M. Kumar, A. Chan, A. W. Black, M. Ravishankar, and A. I. Rudnicky. Pocketsphinx: A free, real-time continuous speech recognition system for hand-held devices. In in Proc. of ICASSP, 2006.

[3] A. Lee, T. Kawahara, and K. Shikano. Julius: An open source realtime large vocabulary recognition engine. In Proc. of EUROSPEECH, 2001.

[4] D. Rybach, S. Hahn, P. Lehnen, D. Nolden, M. Sundermeyer, Z. Tüske, S. Wiesler, R. Schlüter, and H. Ney. RASR - The RWTH Aachen University Open Source Speech Recognition Toolkit. In Proc. of ASRU, 2011.

[5] D. Povey et al. The Kaldi Speech Recognition Toolkit. In Proc. of ASRU, 2011.

[6] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorflow: A system for largescale machine learning. In Proc. of. USENIX Symposium on Operating Systems Design and Implementation, 2016.

[7] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Proc. of NeurIPS, 2019.

[8] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng. Deep speech: Scaling up end-to-end speech recognition, 2014. arXiv:1412.5567.

[9] A. Zeyer, T. Alkhouli, and H. Ney. RETURNN as a generic flexible neural toolkit with application to translation and speech recognition. In Proc. of ACML, 2018.

[10] M. Ravanelli, T. Parcollet, and Y. Bengio. The PyTorch-Kaldi Speech Recognition Toolkit. In Proc. of ICASSP, 2019.

[11] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie, S. Watanabe, and S. Khudanpur. Espresso: A fast end-to-end neural speech recognition toolkit. In Proc. of ASRU, 2019.

[12] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan, T. Sainath, Y. Cao, C. Chiu, et al. Lingvo: A modular and scalable framework for sequence-to-sequence modeling. 2019. arXiv:1902.08295.

[13] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. Fairseq: A fast, extensible toolkit for sequence modeling, 2019. arXiv:1904.01038.

[14] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai. ESPnet: End-to-end speech processing toolkit. In Proc. of Interspeech, 2018.

[15] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman, S. Beliaev, V. Lavrukhin, J. Cook, P. Castonguay, M. Popova, J. Huang, and J. M. Cohen. NeMo: a toolkit for building AI applications using Neural Modules, 2019. arXiv:1909.09577.

[16] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzinis, J. Heitkaemper, M. Olvera, F. Stöter, M. Hu, J. M. Martín-Doñas, D. Ditter, A. Frank, A. Deleforge, and E. Vincent. Asteroid: the PyTorch-based audio source separation toolkit for researchers. In Proc. of Interspeech, 2020.

[17] H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin, D. Fustes, H. Titeux, W. Bouaziz, and M. Gill. pyannote.audio: neural building blocks for speaker diarization. In Proc. of ICASSP, 2020.

[18] A. Larcher, K. A. Lee, and S. Meignier. An extensible speaker identification sidekit in python. In Proc. of ICASSP, 2016.

[19] S. Yang, P. Chi, Y. Chuang, C. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G. Lin, T. Huang, W. Tseng, K. Lee, D. Liu, Z. Huang, S. Dong, S. Li, S. Watanabe, A. Mohamed, and H. Lee. Superb: Speech processing universal performance benchmark, 2021. arXiv:2105.01051.

[20] Z. Wang and D. Wang. A joint training framework for robust automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(4):796-806, 2016.

[21] Z. Chen, S. Watanabe, H. Erdogan, and J.R. Hershey. Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks. In Proc. of Interspeech, 2015.

[22] T. Gao, J. Du, L. Dai, and C. Lee. Joint training of front-end and back-end deep neural networks for robust speech recognition. In Proc. of ICASSP, 2015.

[23] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio. Batch-normalized joint training for DNN-based distant speech recognition. In Proc. of SLT, 2016.

[24] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proc. of ICML, 2006.

[25] A. Graves. Sequence transduction with recurrent neural networks. ICML - Workshop on Representation Learning, 2012.

[26] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing, 11 (8):1240-1253, 2017.

[27] S. Toshniwal, A. Kannan, C. Chiu, Y. Wu, T. Sainath, and K. Livescu. A comparison of techniques for language model integration in encoder-decoder speech recognition. 2018.

[28] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. LibriSpeech: An ASR corpus based on public domain audio books. In Proc. of ICASSP, 2015.

[29] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber. Common Voice: a massively-multilingual speech corpus. In Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020 .

[30] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng. Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline. In Oriental COCOSDA, 2017.

[31] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus CDROM, 1993.

[32] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust DNN embeddings for speaker recognition. In Proc. of ICASSP, 2018.

[33] B. Desplanques, J. Thienpondt, and K. Demuynck. ECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification. In Proc. of Interspeech, 2020.

[34] A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: a large-scale speaker identification dataset. In Proc. of Interspech, 2017.

[35] J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In Proc. of Interspeech, 2018.

[36] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4), 2007.

[37] N. Dawalatabad, M. Ravanelli, F. Grondin, J. Thienpondt, B. Desplanques, and H. Na. ECAPATDNN embeddings for speaker diarization, 2021. arXiv:2104.01466.

[38] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. The AMI meeting corpus: A pre-announcement. In Proc. of the Second International Conference on Machine Learning for Multimodal Interaction, 2006.

[39] S. Fu, C. Yu, T. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao. MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement. 2021. arXiv:2104.03538.

[40] D. Bagchi, P. Plantinga, A. Stiff, and E. Fosler-Lussier. Spectral feature mapping with mimic loss for robust speech recognition. In Proc. of ICASSP, 2018.

[41] C. Veaux, J. Yamagishi, and S. King. The voice bank corpus: Design, collection and data analysis of a large regional accent speech database. In International Conference Oriental COCOSDA held jointly with Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE), 2013.

[42] C. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan, and J. Gehrke. The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results. In Proc. of Interspeech, 2020.

[43] Y. Luo and N. Mesgarani. Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(8):1256-1266, 2019 .

[44] Y. Luo, Z. Chen, and T. Yoshioka. Dual-path RNN: efficient long sequence modeling for time-domain single-channel speech separation, 2020. arXiv:1910.06379.

[45] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong. Attention is all you need in speech separation. In Proc. of ICASSP, 2021.

[46] J. Hershey, Z. Chen, J. Le Roux, and S. Watanabe. Deep clustering: Discriminative embeddings for segmentation and separation. In Proc. of ICASSP, 2016.

[47] G. Wichern, J. Antognini, M. Flynn, L. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. Le Roux. WHAM!: extending speech separation to noisy environments. In Proc. of Interspeech, 2019 .

[48] M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux. Whamr!: Noisy and reverberant single-channel speech separation. In Proc. of ICASSP, 2020.

[49] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent. Librimix: An open-source dataset for generalizable speech separation, 2020. arXiv:2005.11262.

[50] L. Lugosch, P. Papreja, M. Ravanelli, A. Heba, and T. Parcollet. Timers and Such: A practical benchmark for spoken language understanding with numbers. CoRR, abs/2104.01604, 2021.

[51] P. Haghani, A. Narayanan, M. Bacchiani, G. Chuang, N. Gaur, P. Moreno, R. Prabhavalkar, Z. Qu, and A. Waters. From Audio to Semantics: Approaches to end-to-end spoken language understanding. IEEE Spoken Language Technology Workshop (SLT), 2018.

[52] D. Serdyuk, Y. Wang, C. Fuegen, A. Kumar, B. Liu, and Y. Bengio. Towards end-to-end spoken language understanding. In Proc. of ICASSP, 2018.

[53] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser. SLURP: A Spoken Language Understanding Resource Package. In Proc. of EMNLP, 2020.

[54] L. Lugosch, M. Ravanelli, P. Ignoto, V. Tomar, and Y. Bengio. Speech model pre-training for end-to-end spoken language understanding. In Proc. of Interspeech, 2019.

[55] E. Habets, J. Benesty, I. Cohen, S. Gannot, and J. Dmochowski. New insights into the MVDR beamformer in room acoustics. IEEE Transactions on Audio, Speech, and Language Processing, 18(1):158-170, 2009.

[56] J. Heymann, L. Drude, and R. Haeb-Umbach. Neural network based spectral mask estimation for acoustic beamforming. In Proc. of ICASSP, 2016.

[57] C. H. Knapp and G. C. Carter. The generalized correlation method for estimation of time delay. IEEE Transactions on Acoustics, Speech, and Signal Processing, 24(4):320-327, 1976.

[58] M. Cobos, A. Marti, and J. Lopez. A modified SRP-PHAT functional for robust real-time sound source localization with scalable spatial sampling. IEEE Signal Processing Letters, 18 (1):71-74, 2010 .

[59] R. Schmidt. Multiple emitter location and signal parameter estimation. IEEE transactions on antennas and propagation, 34(3):276-280, 1986.

[60] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. Dauphin. Convolutional sequence to sequence learning. In Proc. of ICML. PMLR, 2017.

[61] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.

[62] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proc. of NeurIPS, 2020.

[63] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen, H. Nguyen, and R. Teja Gadde. Jasper: An end-to-end convolutional neural acoustic model. In Proc. of Interspeech, 2019 .

[64] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev, V. Lavrukhin, R. Leary, J. Li, and Y. Zhang. Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions. In Proc. of ICASSP, 2020.

[65] S. Majumdar, J. Balam, O. Hrinchuk, V. Lavrukhin, V. Noroozi, and B. Ginsburg. Citrinet: Closing the gap between non-autoregressive and autoregressive end-to-end models for automatic speech recognition, 2021. arXiv:2104.01721.

[66] G. Fairbanks, D. Garlan, and W. Scherlis. Design fragments make using frameworks easier. SIGPLAN Not., 41(10), 2006.

[67] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language processing. In Proc of EMNLP, 2020.

[68] E. Variani, T. Bagby, E. McDermott, and M. Bacchiani. End-to-end training of acoustic models for large vocabulary continuous speech recognition with tensorflow. In Proc. of Interspeech, 2017.

[69] M. Morishita, Y. Oda, G. Neubig, K. Yoshino, K. Sudoh, and S. Nakamura. An empirical study of mini-batch creation strategies for neural machine translation. In Proc. of the WNMT, 2017.

[70] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.

[71] P. Virtanen, R. Gommers, T. E Oliphant, M. Haberland, T. Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods, 17(3):261-272, 2020.

[72] A. Gulli and S. Pal. Deep learning with Keras. Packt Publishing Ltd, 2017.

[73] J. Howard and S. Gugger. Fastai: A layered api for deep learning. Information, 11(2):108, 2020 .

[74] W. Falcon et al. Pytorch Lightning. GitHub, 2019. https://github.com/ PyTorchLightning/pytorch-lightning.

[75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Proc. of NIPS, 2014.

[76] D. Park, W. Chan, Y. Zhang, C. Chiu, B. Zoph, E. Cubuk, and Q. Le. SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. In Proc. of Interspeech, 2019.

[77] A. Graves and N. Jaitly. End-to-end speech recognition with recurrent neural networks. In Proc. of ICML, 2014.

[78] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8): $1735-1780,1997$.

[79] J. Chung, Ç. Gülçehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In Proc. of NIPS, 2014.

[80] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio. Light gated recurrent units for speech recognition. IEEE Transactions on Emerging Topics in Computational Intelligence, 2(2): $92-102,2018$.

[81] T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proc. of EMNLP, 2018.

[82] J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: A benchmark for ASR with limited or no supervision. CoRR, $\mathrm{abs} / 1912.07875,2019$.

[83] W. Han, Z. Zhang, Y. Zhang, J. Yu, C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu. Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. 2020. arXiv:2005.03191.

[84] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang. A comparative study on transformer vs rnn in speech applications. In Proc. of ASRU, 2019.

[85] P. Szymański, P. Żelasko, M. Morzy, A. Szymczak, M. Żyła-Hoppe, J. Banaszczak, L. Augustyniak, J. Mizgajski, and Y. Carmiel. WER we are and WER we think we are. In Proc. of $E M N L P, 2020$.

[86] T. Likhomanenko, Q. Xu, V. Pratap, P. Tomasello, J. Kahn, G. Avidov, R. Collobert, and G. Synnaeve. Rethinking evaluation in ASR: are our models robust enough? CoRR, abs/2010.11745, 2020.

[87] H. Zeinali, S. Wang, A. Silnova, P. Matějka, and O. Plchot. But system description to voxceleb speaker recognition challenge 2019. In Proc. of The VoxCeleb Challange Workshop, 2019.

[88] M. Pal, M. Kumar, R. Peri, T. Park, S. Kim, C. Lord, S. Bishop, and S. Narayanan. Metalearning with latent space clustering in generative adversarial network for speaker diarization, 2020. arXiv:2007.09635.

[89] F. Landini, J. Profant, M. Diez, and L. Burget. Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: theory, implementation and analysis on standard tasks, 2020. arXiv:2012.14952.

[90] C. Wang, M. Rivière, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv:2101.00390, 2021.

[91] K. J. Lang and G. E. Hinton. The development of the time-delay neural network architecture for speech recognition. Technical Report CMU-CS-88-152, Carnegie-Mellon University, 1988.

[92] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37:328-339, 1989 .

[93] P. Kenny, T. Stafylakis, P. Ouellet, Md. J. Alam, and P. Dumouchel. PLDA for speaker verification with utterances of arbitrary duration. In Proc. of ICASSP, 2013.

[94] D. Garcia-Romero and C. Espy-Wilson. Analysis of i-vector length normalization in speaker recognition systems. In Proc. of Interspeech, 2011.

[95] A. Défossez, G. Synnaeve, and Y. Adi. Real time speech enhancement in the waveform domain. Proc. of Interspeech, 2020.

[96] C. Valentini-Botinhao. Noisy speech database for training speech enhancement algorithms and TTS models. Edinburgh DataShare, 2017.

[97] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J. Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, and C. Pal. Deep complex networks. In Proc. of ICLR, 2018.

[98] T. Parcollet, M. Ravanelli, M. Morchid, G. Linarès, C. Trabelsi, R. De Mori, and Y. Bengio. Quaternion recurrent neural networks. In Proc. of ICLR, 2019.

[99] M. Mohri, F. Pereira, and M. Riley. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69-88, 2002.

[100] D. Povey et al. k2. https://github.com/k2-fsa/k2, 2020.

[101] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Estève. TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation. In Proc. of SPECOM, 2018.

[102] C. Li, J. Shi, W. Zhang, A. Subramanian, X. Chang, N. Kamo, M. Hira, T. Hayashi, C. Böddeker, Z. Chen, and Shinji Watanabe. Espnet-se: End-to-end speech enhancement and separation toolkit designed for ASR integration. In Proc. of the IEEE Spoken Language Technology Workshop, 2021.

[103] H. Erdogan, J. Hershey, S. Watanabe, M. Mandel, and J. Le Roux. Improved MVDR beamforming using single-channel mask prediction networks. In Proc. of Interspeech, 2016.

[104] F. Grondin, J. Lauzon, J. Vincent, and F. Michaud. GEV beamforming supported by DOAbased masks generated on pairs of microphones. In Proc. of Interspeech, 2020.

[105] F. Grondin and F. Michaud. Lightweight and optimized sound source localization and tracking methods for open and closed microphone array configurations. Robotics and Autonomous Systems, 2019.

[106] M. Omologo and P. Svaizer. Acoustic event localization using a crosspower-spectrum phase based technique. In Proc. of ICASSP, 1994.

[107] J. Du, Q. Wang, T. Gao, Y. Xu, L. Dai, and C. Lee. Robust speech recognition with speech enhanced deep neural networks. In Proc. of Interspeech, 2014.

[108] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. Proc. of ICLR, 2015.

[109] S. Seo, D. Kwak, and B. Lee. Integration of pre-trained networks with continuous token interface for end-to-end spoken language understanding. arXiv:2104.07253, 2021.

[110] J. Thiemann, N. Ito, and E. Vincent. The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings. In 21st International Congress on Acoustics, 2013.

[111] A.W. Rix, J.G. Beerends, M.P. Hollier, and A.P. Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In Proc. of ICASSP, 2001.

[112] Y. Hu and P. Loizou. Evaluation of objective quality measures for speech enhancement. IEEE Transactions on Audio, Speech, and Language Processing, 16(1):229-238, 2007.

[113] E. Vincent, R. Gribonval, and C. Fevotte. Performance measurement in blind audio source separation. IEEE Transactions on Audio, Speech, and Language Processing, 14(4):1462-1469, 2006.
