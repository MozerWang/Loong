# AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning 

Minghao Chen ${ }^{1}, \quad$ Yihang $\mathbf{L i}^{2}$, Yanting Yang ${ }^{3}, \quad$ Shiyu Yu ${ }^{4}, \quad$ Binbin Lin ${ }^{3 *}, \quad$ Xiaofei $\mathbf{H e}^{2}$<br>${ }^{1}$ School of Computer Science, Hangzhou Dianzi University<br>${ }^{2}$ State Key Lab of CAD\&CG, Zhejiang University<br>${ }^{3}$ School of Software Technology, Zhejiang University ${ }^{4}$ NingBo Port Group<br>minghaochen01@gmail.com


#### Abstract

Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce case-conditioned prompting strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving $97.4 \%$ with GPT-4-turbo and $86.2 \%$ with GPT-3.5-turbo on ALFWorld benchmark tasks. The source code will be available soon.


## 1 Introduction

Recently, autonomous agents based on Large Language Models (LLM), e.g., ReAct [30], Reflexion [16], SayCan [1], WebGPT [10], and Voyager [23], have demonstrated their potential to complete long-horizon tasks in grounded environments. These LLM agents operate by generating thoughts and actions that are executable in the environment. For customized environments, such as robotics $[1,7,18,22]$ and games [14, 23, 34], prior methods provide detailed instructions and in-context examples to familiarize LLM with action functions (API) and the target environment. However, unlike these agents, humans can autonomously build and update their understanding of an unfamiliar environment through dynamic interaction.

Several existing methods enable LLM agents to reflect on feedback [16, 20] or save successful experiences as skills $[20,23,32]$ to enhance the performance and reduce the reliance on humanprovided examples. However, these reflections and skills have not been well exploited to foster a deeper understanding of the environment. As a result, directly using saved skills as in-context examples can lead to the Path Dependence problem, i.e., the agent blindly replicates the paths of previous successes, failing to appropriately adapt to new scenarios. Such problems are more severe in real-world situations characterized by high variability.

A previous work, ExpeL [32], gathers the trajectories of LLM agents and extracts cross-task rules from them. However, these rules are extracted offline, making ExpeL suffer from the same distributional shift problem as Offline RL [6]. Meanwhile, due to the simplicity of rule management, its rules are always armchair general and unhelpful for the Path Dependency problem. In this paper, we propose a novel framework called AutoManual to build a well-organized understanding of the environment that can guide multi-task planning effectively. AutoManual leverages a dynamic rule system that not only extracts valuable experience, including skills and reflections, into different types of rules but also allows for continuously updating these rules in response to new situations. Additionally, error-prone details are explicitly described in the rules to improve the robustness of planning.

AutoManual follows two alternating iterative processes to optimize the rules. First, given the observation and task of an episode, the Planner agent utilizes currently discovered rules to write free-form code as an actionable plan. The interaction between the environment and the Planner will loop until the episode ends. Second, based on this trajectory, the Builder agent will update relevant rules through the rule system. This online updating mechanism can timely verify whether the rules have deviations and are applicable to the Planner. After rules optimization, the Formulator agent categorizes these rules according to their application scenarios and compiles a comprehensive manual in Markdown format.

The challenge lies in enabling the Builder to accurately extract applicable rules from a long trajectory, as LLM are prone to generating hallucinations. To address this, we employ a case-conditioned prompting strategy, which directs the Builder to focus on specific rules according to the case of the trajectory. For example, if errors occurred in a trajectory, the Builder is first asked to determine which caused the error: an unrecorded situation occurred, or the Planner failed to follow existing rules. Based on this answer, the Builder will be given corresponding prompts to update relevant rules.

To summarize, our contributions are the following:

- We adopt free-form code as the way for the Planner agent to interact with the environment. We introduce a structured rule system that allows the Builder agent to manage multiple types of knowledge from these code-based interactions.
- We propose an alternating process between the Planner and Builder agents to optimize rules in an online manner and resolve the Path Dependency problem. To improve readability, the Formulator agent is introduced to reorganize and formalize the rules into a Markdown manual.
- To facilitate rule management, we employ a case-conditioned prompting strategy, which guides the Builder to manage specific types of rules for different trajectory cases.
- Starting from a single demonstration, AutoManual can generate detailed instruction manuals for complex environments like ALFWorld and MiniWoB++. These manuals allow LLM agents to achieve remarkable success rates of $97.4 \%$ with GPT-4-turbo and $86.2 \%$ with GPT-3.5-turbo on ALFWorld, $98.3 \%$ with GPT-4-turbo and $92.7 \%$ with GPT-3.5-turbo on MiniWoB++.


## 2 Related Works

### 2.1 LLM for Agents Planning

Large Language Models (LLM) exhibit powerful reasoning and planning capabilities [11, 12, 26, 30, 34] while requiring much fewer demonstrations than traditional learning methods. With this planning capability as the core, LLM agents are being developed for use in robotics $[1,7,18$, 19, 22], game-playing [14, 23, 25, 34], software development [3, 15], and other fields [28]. Prior studies [16, 20, 30] allow agents to adjust actions or plans based on environmental feedback to improve planning performance. Given the powerful programming capability of LLM, several works, e.g., CodeAsPolicy [7], ProgPrompt [18] and AdaPlanner [20], propose to use Python code as the plan of LLM agents. This form of output can automatically respond to in-plan feedback and achieve better performance than the action and JSON format [20, 24].

### 2.2 Self-improvement of LLM Agents

Embodied agent research has long sought to enable agents to self-improve through interactive experiences. Unlike traditional learning-based agents that require extensive iterations for optimization,

Reflexion [16] allows LLM agents to reflect on previous failures and quickly improve their plans. Some works $[29,31,33]$ combine tree search with reflection to deliberately seek a better solution. Apart from failure experiences, prior studies [20, 23, 34] utilize successful experiences as skills to assist future planning. Voyager [23] stores generated and verified programs into the skill library as a new skill for more complex tasks. AdaPlanner [20] also discovers and archives successful programs into skill memory for future similar tasks. However, these methods stop updating skills after storing them, which inevitably leads to the Path Dependency problem.

### 2.3 Memory Management of LLM Agents

For LLM agents, learning from past experiences can also be viewed as managing the episodic memory [16]. CLIN [9] proposes to keep updating a memory centered on causal abstractions for new trials. Retrieval-Augmented Planning (RAP) [4] retrieves past experiences corresponding to the current situation. MemGPT [13] allows LLM to select content to retain in working memory and to search for information in long-term memory. Generative Agents [14] retrieve memories based on recency, importance, and relevance to the current situation. Generative Agents also generate tree-structured reflections, but they focus on a continuous scenario rather than task-oriented rules.

### 2.4 LLM for Rule Discovery

Several recent works also investigate the rule discovery capabilities of LLM. Zhu et al. [35] propose Hypotheses-to-Theories (HtT), enabling LLM to induce and deduce rules for basic reasoning tasks. For LLM agents, ExpeL [32] gathers the trajectories of Reflexion agents and extracts cross-task rules from them. Furthermore, AutoGuide [2] generates state-aware rules and retrieves rules relevant to the test-time state. Unlike ExpeL and AutoGuide, which extract rules from offline experiences, we update rules in an online manner, verifying their reliability and applicability. For more discussion of differences, refer to Appendix C.

## 3 Methods

### 3.1 AutoManual Overview

Our AutoManual framework, shown in Fig 1, consists of three main stages. Building stage: The Planner agent and Builder agent collaborate to build rules from the interactive environment. The Consolidator agent merges or deletes redundant rules when the rules exceed the maximum rule number. Formulating stage: The Formulator agent categorizes the rules, summarizes the key points, and formulates them into a manual in Markdown form. Testing stage: Based on the generated manual, a test-time Planner agent will be evaluated through test tasks and scenarios.

Formally, an Interactive Environment can be modeled as a Partially Observable Markov Decision Process (POMDP): $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{G}, \mathcal{O})$. At the start of each episode, a scenario $s_{0} \in \mathcal{S}$ will be initialized, a text-grounded task $g \in \mathcal{G}$ and the initial observation $o_{0} \in \mathcal{O}$ (processed into textual form) will be given. The environment can be interacted with through permissible actions (API) set $\mathcal{A}$. After executing an action $a \in \mathcal{A}$, the environment will return the result of the action and the new observation $o^{\prime}$ based on the dynamics $T\left(s^{\prime} \mid s, a\right) \in \mathcal{T}$ and $O\left(o^{\prime} \mid s^{\prime}\right)$. Finally, when the episode is done, a binary reward $r \in\{-1,1\}$ indicating the failure or success of the task will be returned.

We approach the learning of environmental rules as an optimization problem:

$$
\begin{equation*}
\max _{\Theta} E_{s_{0}, g} E_{\rho(\cdot \mid \Theta)} r\left(\tau_{\rho}\right) \tag{1}
\end{equation*}
$$

where $\Theta$ denotes all rules in our rule system, $\rho(\cdot \mid \Theta)$ denotes the policy of the Planner given the current rules $\Theta$ and $\tau_{\rho}$ denotes a trajectory of $\rho(\cdot \mid \Theta)$ starting from $\left[o_{0}, g\right]$. Classic policy gradient methods [27] solve such problems through stochastic gradient ascent, i.e., executing the current policy to obtain the episodic reward and back-propagating gradients to update the parameters.

Inspired by this online reinforcement learning paradigm, we follow two alternative processes to optimize the rules $\Theta$ : 1. The Planner practices the current rules through interaction within an episode. 2. The Builder updates the rules $\Theta$ based on this trajectory. Compared to traditional parameter optimization, sample-inefficient gradient ascent is replaced by text-based rule management. We design a well-structured rule system described in Section 3.3 to ensure the rule updating contributes

![](https://cdn.mathpix.com/cropped/2024_06_04_81e8ccffa71c0f17e7b1g-04.jpg?height=654&width=1375&top_left_y=234&top_left_x=367)

Figure 1: AutoMaual Overview. AutoMaual operates in three stages: (1) Building Stage: The Planner agent interacts with the environment by coding actionable plans. After receiving the current trajectory of the Planner, the Builder agent manages rules through the online rule system. (2) Formulating Stage: The Formulator agent formulates the resulting rules into a Markdown manual. (3) Testing Stage: A test-time Planner agent utilizes the manual to complete testing tasks.

to rewards. Additionally, to limit the role of human expertise, we only provide a simple example demonstrating the output format to agents. We derive initial rules from this example as the starting point of the optimization.

### 3.2 Planner Agent for Interactive Planning

As demonstrated by the success of Voyager [23] and AdaPlanner [20], code-based planning can leverage the powerful programming capability of LLM and automatically react to in-plan feedback. Voyager and AdaPlanner output and refine a complete solution function for the task, which is potentially reusable. However, this function-form output is difficult to adjust in response to environmental feedback, as it requires maintaining the integrity of the plan throughout.

Our Planner Agent outputs free-form code as its plan, which aligns more with the natural programming capabilities of LLM [7, 21]. This form simplifies planning by only generating code necessary for the current environmental situation and feedback without the overhead of integrating previously executed code. As shown in Fig 2, at the start of a new episode, the Planner receives system prompts, current rules $\Theta$, relevant samples from the skill and reflection libraries, the target task $g$, and initial observation $o_{0}$. System prompts contain the role, permissible actions $\mathcal{A}$, response guidelines, and a simple example (detailed in Appendix H). The output of the Planner is structured into four segments during each cycle:

1. Analysis: The understanding of the current situation and reflection on previous errors if exist.
2. Related Rules: Rules (along with their IDs) that need to be considered in this situation.
3. Overall Plan: The general plan to complete the task.
4. Free-Form Code: A block of Python code divided into steps. The Planner is encouraged to define helpful functions in the code, which might be reusable in similar scenarios.

We denote this response of the Planner as $\left[\right.$ thought $_{t}$, code $\left._{t}\right]$, where thought $t_{t}$ denotes the first three segments. code $e_{t}$ executed in the environment is followed by feedback $c_{t}$, which informs the subsequent output cycle. This process iterates until the episode ends or a response limit is reached.

As shown in Fig 2, according to the episodic reward, we categorize the result into Direct Success, Indirect Success (errors occurred but were solved later), and Failure. In the case of Direct or Indirect Success, the Planner will be prompted to organize its previous code into a code block. For Indirect Success, it additionally summarizes the mistakes and misunderstandings that cause errors. For the Failure case, the Planner will be prompted to reflect on the reason for the failure carefully, suggest reasonable corrections, and specify the code segment that caused the error. We denote this response

![](https://cdn.mathpix.com/cropped/2024_06_04_81e8ccffa71c0f17e7b1g-05.jpg?height=873&width=1314&top_left_y=236&top_left_x=400)

Figure 2: The Planner Trajectory. Given the current task and rules, the Planner will interact with the environment through free-form code. Based on the trajectory result, the Planner will generate a corresponding conclusion, which will be saved in the skill or reflection library.

of the Planner as conclusion. Finally, we obtain a trajectory of the Planner:

$$
\begin{equation*}
\tau_{\rho}=\left(o_{0}, g,\left[\text { thought }_{1}, \text { code }_{1}\right], c_{1}, \ldots,\left[\text { thought }_{T}, \text { code }_{T}\right], c_{T}, \text { conclusion }\right) \tag{2}
\end{equation*}
$$

Skill Library and Reflection Library: Apart from rules, we also manage and transmit conclusions from previous episodes, which provide essential details for generating planning code. In the case of Direct or Indirect Success, we save the code block in conclusion as a skill for that task type ${ }^{1}$ into the skill library [20, 23]. In the Failure case, we save its conclusion as a reflection for that task type into the reflection library. When a new task comes, the code block of the most similar task is retrieved from the skill library. If there is no existing skill for the new task type, the reflection for that task type will be returned. As mentioned in the Introduction, compared with rules, these skills and reflections contain more programming details but are less generalizable to new scenarios. Considering this Path Dependence problem, we prompt the Planner that the rules should be prioritized.

Cooperation between Agents: In our framework, rule management is not solely the responsibility of the Builder; the Planner also plays a critical role by explicitly identifying the rules it engages in its response. This cooperation is facilitated by including the Planner's thoughts within the trajectory $\tau$, which is provided to the Builder. This synergy enhances the identification and adjustment of problematic rules. In addition, conclusion from the Planner contains the detailed success process or reflections on errors, which further assist the Builder in managing corresponding types of rules.

### 3.3 Builder and Consolidator Agents for Rule Management

Upon receiving the trajectory $\tau_{\rho}$, the Builder has to manage the rules through the rule system.

Rule System: We intuitively identify rules as the kinds of knowledge that help task completion, including the analyses of the observed phenomenon $T\left(o^{\prime} \mid o, a\right)$, the mechanism $T\left(s^{\prime} \mid s, a\right)$, and the correlation between the reward $r$ and $\tau_{\rho}$, i.e., the success process or the occurred error. Therefore, unlike ExpeL [32] and AutoGuide [2], which derive general insight from the trajectory, our system categorizes six specific rule types to extract environmental knowledge that targets different aspects[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_81e8ccffa71c0f17e7b1g-06.jpg?height=643&width=1339&top_left_y=264&top_left_x=382)

3 Possible Results

5 Possible Cases

Figure 3: Case-Conditioned Prompts. Given the current trajectory, the Builder classifies the cause of the major error as "Imperfect Rules" or "Imperfect Agents". Then, the Builder will get the base prompt and corresponding prompt to guide its rule management.

of the trajectory. Furthermore, each rule in our system is enhanced with an "Example" attribute to illustrate its application and important details, making it grounded and well-understood. Specifically, each rule in the rule system has these four attributes:

1. Rule Type: The type of the rule, options include "Special Phenomenon", "Special Mechanism", "Success Process", "Useful Helper Method", "Corrected Error" and "Unsolved Error";
2. Rule Content: A description of the rule, beginning with the scope of its applicable scenarios;
3. Example: An example or code from the trajectory demonstrates this rule, where additional remarks, e.g. error-prone details, can also be added to it;
4. Validation Logs: Logs that track the rule's application and updates, including episode and rule IDs that trace the rule's evolution, serving as a reference for the Builder and Consolidator.

The Builder manages the rules through the following functions of the rule system:

- write_rule(**rule_attributes): Write down a new rule with its four attributes.
- update_rule(rule_id, **rule_attributes): Rewrite the attributes of a existing rule.
- stop_generating(): When the trajectory is not needed or insufficient to derive any more new rules, the function should be called.

Similar to hierarchical reflections in Generative Agents [14], we allow the Builder to utilize existing rules to induce more general or deeper rules and record their dependence in Rule Content or Validation Logs, more discussed in Appendix D.

Case-Conditioned Prompting: To mitigate the risk of erroneous rule creation, such as deriving rules of success from a failed trajectory, we employ case-conditioned prompts. As illustrated in Fig 3, the Builder first analyzes and determines if the major errors stem from "Imperfect Rules" or "Imperfect Agent". Based on this analysis and the trajectory results, targeted prompts guide the Builder in rule management ${ }^{2}$. For example, in a case of indirect success due to imperfect rules (Case 2), the prompts will guide the Builder to extract or update the success process, helper methods, and error reflections in corresponding rule types. Finally, the Builder responds with the potential rules detailing their relation with existing rules and uses the functions of the rule system to manage rules.

Rule Consolidation: When the number of rules in the rule system exceeds $N_{\text {max }}$, the Consolidator agent steps in to consolidate related rules and delete redundant rules. It uses three functions of the rule system: get_trajectory(episode_id), update_rule(rule_id, **rule_attributes) and delete_rule(rule_id). Given the current rules, the Consolidator identifies potentially relevant or overlapped rules, uses[^1]get_trajectory function to investigate the trajectories they depend on, and finally calls the remaining functions to manage the rules. During the management, the Consolidator ensures that consolidation retains details of rules and examples.

### 3.4 Manual Formulation

Once the building stage is complete, we can obtain a set of rules targeted to different situations. Although these rules have been validated through online optimization to ensure their applicability, our next goal is to enhance their readability and global understanding. To achieve this, we introduce the Formulator agent, designed to transform these rules into a user-friendly manual, like a teacher conveys complex subjects through easily digestible lessons. As depicted in Fig 1, the Formulator begins by categorizing all rules based on their target scenarios. This categorization aids in structuring the manual and ensures that related rules are discussed together, which enhances the logical flow and accessibility of the information. For each category, the Formulator drafts an introduction, summarizing the rules it contains and highlighting the key points and overall principles that govern the specific scenarios. Finally, the Formulator compiles the rules and their introductions into a comprehensive manual formatted in Markdown.

## 4 Experiments

In line with AdaPlanner [20], we conduct the experiments on two interactive environments: (1) ALFWorld [17] is a text-based virtual household environment containing six distinct task types. We run the building stage on 36 tasks ( 6 tasks for each task type) sampled from the training set of ALFWorld, and each task is run only once. Following previous works [16, 20, 30], we run the testing stage on the validation unseen set containing 134 tasks across these six types. (2) MiniWoB++ [8] is a simulated web environment where agents complete diverse tasks on the Internet by performing keyboard and mouse actions. Prior works [5,20] selects 9 task types with environmental feedback and 44 task types without feedback from MiniWoB++ tasks. We perform experiments on 9 task types with feedback and all 53 task types. At each stage, we randomly sample 6 tasks for each task type.

During building and formulating stages, we use GPT-4-turbo (gpt-4-1106-preview) as the LLM for all agents. At the testing stage, we equip the Planner agent with GPT-4-turbo or GPT-3.5-turbo (gpt-3.5-turbo-1106), which can evaluate the effect of the generated manual on relatively smaller LLM. More details of the implementation and prompts for AutoManual can be found in the Appendix.

Compared Methods: In the experiments, we compare AutoManual with the following methods of LLM Agent: (1) ReAct [30] prompts LLM to generate the reasoning trace using CoT [26] and next-step action; (2) Reflexion [16] agents generate reflection on task feedback signals, which is saved in the memory for subsequent trials; (3) ExpeL [32] extract insights and skills from the offline trajectories of Reflexion agents; (4) RCI [5] agent recursively criticizes and improves its output for solving computer tasks; (5) AdaPlanner [20] allows the LLM agent to generate and adaptively refine a code-style plan; (6) Planner+Lib. represents our Planner agent equipped with skill and reflection libraries (\$3.2) during building and testing stages without any rules. We re-implement prior methods with GPT-3.5 and GPT-4 versions the same as ours for fair comparisons.

ReAct, Reflexion, and ExpeL provide LLM agents with 12 human examples ( 2 examples per task type) of ALFWorld. For AdaPlanner, they provide 6 human examples (1 example per task type) of ALFWorld as the start of skill discovery. For our methods, agents are provided only one human example of the simplest task (Put) on ALFWorld. On MiniWob++, our agents are provided one human example (search-engine) for tasks with feedback and 4 examples for all tasks. Reflexion agents are allowed to try at most 3 trials for each task. For AdaPlanner and our methods, we allow the Planner agent to replan at most 3 times in response to the environmental feedback. To reduce randomness, we performed each experiment three times and reported the average.

### 4.1 Main Results

Main Results on ALFWorld: As shown in Tab. 1, AutoManual significantly outperforms the existing methods, evidenced by overall success rates of $86.2 \%$ when using GPT-3.5-turbo for the testing stage and $97.4 \%$ when using GPT-4-turbo. Noticeably, AutoManual requires little expert prior knowledge about the environment and is only provided with one human example to achieve excellent results. In

Table 1: Success rate (\%) of LLM agent methods on ALFWorld test tasks. For each method, the number of all human examples used is listed. "Planner+Lib." represents only using skill\&reflection library during the building and testing stages. We run all experiments 3 times and show the average.

| Methods | Examples | Put | Clean | Heat | Cool | Examine | Put two | ALL |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Testing LLM: GPT-3.5-turbo |  |  |  |  |  |  |  |  |
| ReAct [30] | 12 | 75.0 | 24.7 | 37.7 | 36.4 | 44.4 | 11.8 | 41.9 |
| Reflexion [16] | 12 | 87.5 | 44.1 | 73.9 | 50.0 | 61.1 | 35.3 | 59.8 |
| ExpeL [32] | 12 | 62.5 | 61.3 | 30.4 | 61.9 | 55.5 | 35.3 | 52.2 |
| AdaPlanner [20] | 6 | 83.3 | 46.2 | 65.2 | 74.2 | 68.5 | 52.9 | 63.3 |
| Planner+Lib. | 1 | 77.8 | $\mathbf{8 8 . 2}$ | 82.6 | 72.7 | 37.0 | 27.5 | 66.5 |
| AutoManual | 1 | $\mathbf{9 5 . 8}$ | 79.6 | $\mathbf{8 7 . 0}$ | $\mathbf{7 8 . 8}$ | $\mathbf{1 0 0 . 0}$ | $\mathbf{6 6 . 7}$ | $\mathbf{8 6 . 2}$ |
| Testing LLM: GPT-4-turbo |  |  |  |  |  |  |  |  |
| ReAct [30] | 12 | 95.8 | 76.3 | 69.6 | 86.4 | 72.2 |  |  |
| Reflexion [16] | 12 | $\mathbf{1 0 0 . 0}$ | 95.7 | 78.3 | 86.4 | 77.8 | 70.6 | 76.8 |
| ExpeL [32] | 12 | 94.4 | 82.8 | 72.4 | 81.8 | 72.2 | 58.8 | 79.2 |
| AdaPlanner [20] | 6 | 88.9 | 90.3 | 85.5 | 75.8 | 64.8 | 41.2 | 76.4 |
| Planner+Lib. | 1 | $\mathbf{1 0 0 . 0}$ | 93.5 | $\mathbf{1 0 0 . 0}$ | 93.9 | 88.9 | 39.2 | 88.1 |
| AutoManual | 1 | $\mathbf{1 0 0 . 0}$ | $\mathbf{9 8 . 9}$ | $\mathbf{1 0 0 . 0}$ | $\mathbf{9 5 . 4}$ | $\mathbf{1 0 0 . 0}$ | $\mathbf{9 0 . 2}$ | $\mathbf{9 7 . 4}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_81e8ccffa71c0f17e7b1g-08.jpg?height=445&width=1415&top_left_y=1019&top_left_x=363)

![](https://cdn.mathpix.com/cropped/2024_06_04_81e8ccffa71c0f17e7b1g-08.jpg?height=366&width=697&top_left_y=1037&top_left_x=367)

(a) Cross-Task v.s. Single-Task Type.

![](https://cdn.mathpix.com/cropped/2024_06_04_81e8ccffa71c0f17e7b1g-08.jpg?height=369&width=696&top_left_y=1038&top_left_x=1056)

(b) AutoManual v.s. Skill Library.

Figure 4: (a) The success rate curve with standard deviation when testing GPT-4-turbo or GPT-3.5turbo on ALFWorld. Building is performed cross-task or single-task type. (b) The success rate curve with standard deviation using AutoManual or Planner+Lib. when testing with GPT-4-turbo or GPT-3.5-turbo on 9 task types with feedback in MiniWob++.

comparison, the rules induced by ExpeL hardly improve performance, as its offline trajectories are composed of individual actions rather than code. We find the performance of AdaPlanner is lower than reported. One reason is that AdaPlanner requires LLM to output specific formats to complete its function-form code, which is difficult for creative LLM, e.g., GPT-4-turbo. In addition, AdaPlanner and Planner+Lib. are inferior to AutoManual because they only store successful paths as skills and inevitably face the Path Dependence problem. Especially, tasks in Put Two have various scenarios, such as "two objects can occur at the same receptacle or different receptacles", that require different processes to solve (Appendix G shows an example). Furthermore, Planner+Lib. often does not mark error-prone points in its skills, such as "target objects may appear in unconventional locations".

Main Results on MiniWoB++: As shown in Tab. 2, the performance of AutoManual exceeds the previous methods and Planner+Lib. by a large margin. Especially in 9 task types with feedback, these tasks have higher diversity and require LLM agents to cope with various situations. For example, the tasks in login-user-popup type will interrupt the agent's plan at any time, requiring the agent to cope with unexpected situations. Therefore, solely imitating previous successful experiences without extracting targeted rules will lead to task failure. Additionally, due to the flexibility of free-form codes, our method shows better adaptability while requiring fewer expert examples than prior methods.

Learning Curves. We show the success rate curves (testing with GPT-4-turbo or GPT-3.5-turbo) when gradually increasing the tasks of the building stage in Fig 4. In the left image, we share rules across all task types (Cross-task Type), as in AutoManual, or each task type builds a separate set of rules (Single-task Type) during the building stage. Fig 4 (a) demonstrates that sharing rules across task types can facilitate rule optimization. The rules for each task type deepen understanding of the

Table 2: Success rate (\%) of LLM agent methods on 9 task types with feedback and all 53 task types of MiniWoB++. For each method, the number of human examples used is listed.

| Methods | $\overline{\text { Examples }}$ | $\overline{\text { With feedback }(9 \text { types })}$ | Examples | $\overline{\operatorname{ALL}_{\text {(53 types) }}}$ |
| :---: | :---: | :---: | :---: | :---: |
| Testing LLM: GPT-3.5-turbo |  |  |  |  |
| $\mathrm{RCI}[30]$ | 22 | 45.6 | 104 | 77.3 |
| AdaPlanner [20] | 13 | 71.6 | 38 | 89.4 |
| Planner+Lib. | 1 | 63.6 | 4 | 87.0 |
| AutoManual | 1 | 82.2   | 4 | 92.7 |
| Testing LLM: GPT-4-turbo |  |  |  |  |
| RCI [30] | 22 | 60.4 | 104 | 88.6 |
| AdaPlanner [20] | 13 | 74.1 | 38 | 90.3 |
| Planner+Lib. | 1 | 80.2 | 4 | 94.4 |
| AutoManual | 1 | 94.5 | 4 | 98.3 |

Table 3: Ablation study of AutoManual on ALFWorld when testing with GPT-4-turbo.

| Online | Skill\&Reflect Lib. | Case Prompt | Formulation | Avg. Error Steps ( $\downarrow$ ) | Success Rate (\%) |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | 2.3 | 77.6 |
|  | $\checkmark$ |  |  | 1.5 | 88.1 |
|  | $\checkmark$ | $\checkmark$ | $\checkmark$ | 1.3 | 90.7 |
| $\checkmark$ |  | $\checkmark$ | $\checkmark$ | 1.6 | 89.5 |
| $\checkmark$ | $\checkmark$ |  | $\checkmark$ | 1.0 | 93.8 |
| $\checkmark$ | $\checkmark$ | $\checkmark$ |  | 0.5 | 96.5 |
| $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\mathbf{0 . 3}$ | $\mathbf{9 7 . 4}$ |

environment, thereby boosting the planning of other tasks. In Fig 4 (b), we compare AutoManual and Planner+Lib. on 9 tasks with feedback in MiniWob++. We find that Planner+Lib. tends to get stuck with a lower success rate. In the face of highly diverse scenarios, Skill Library cannot express the rules behind the environment, thus falling into the Path Dependency problem.

### 4.2 Ablation Study

In this ablation study, we quantify the impact of each core component of the AutoManual framework on performance, specifically focusing on success rates and error reduction during task execution. Since we allowed the Planner to replan up to 3 times, each task could have up to 4 error steps.

Online v.s. Offline Rule Management: We perform offline AutoManual by collecting all trajectories and then managing rules from them. As Tab 3 shows, without online rule management, the generated manual can only slightly improve planning (from $88.1 \%$ to $90.7 \%$ ). This is because more mundane mistakes and fewer direct successes will occur in the trajectories (the distributional shift problem), and the rules cannot be verified by feedback from the environment.

Skill\&Reflection Libraries: Retrieving historical conclusions is essential for correct planning, as they record massive interacting details that can complement the rules. Without them, there will be more errors in the details, and the success rate drops from $97.4 \%$ to $89.5 \%$. However, as discussed previously, using plain experiences without inducing rules will lead to Path Dependency.

Case-Conditional Prompts: This strategy further improves the rule management process by reducing the hallucination, as evidenced by an increase in success rate from $93.8 \%$ to $97.4 \%$. These prompts ensure that the Builder updates rules reasonably and grounded.

Effect of Manual Formulation: The final formulation of rules into a comprehensive manual contributed to the success rate of $97.4 \%$ and decreased average error steps, demonstrating the effectiveness of presenting rule-based knowledge in an organized and accessible format. It not only aids the Planner in mastering multiple rules but is also friendly for human reading.

## 5 Conclusion

In this paper, we introduce AutoManual, a framework significantly advancing LLM agents by enabling adaptability and continual learning through online rule optimization. Utilizing the structured rule system, AutoManual autonomously generates comprehensive manuals, achieving high success rates in benchmarks like ALFWorld and MiniWoB++. This approach reduces reliance on human-provided examples and expert interventions, illustrating a robust method for enhancing agent generalization and addressing the Path Dependency problem in diverse environments.

## References

[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022. 1, 2

[2] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. ArXiv, abs/2403.08978, 2024. 3, 5, 14

[3] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zi Hen Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework. ArXiv, abs/2308.00352, 2023. 2

[4] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. ArXiv, abs/2402.03610, 2024. 3

[5] Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. Language models can solve computer tasks. In Neural Information Processing Systems, 2023. 7, 15, 16

[6] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. ArXiv, abs/2005.01643, 2020. 2

[7] Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493-9500, 2022. 1, 2, 4, 14

[8] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR), 2018.7

[9] Bodhisattwa Prasad Majumder, Bhavana Dalvi, Peter Alexander Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. Clin: A continually learning language agent for rapid task adaptation and generalization. ArXiv, abs/2310.10134, 2023. 3

[10] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021. 1

[11] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 2

[12] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe Training language models to follow instructions with human feedback. In Neural Information Processing Systems, 2022. 2

[13] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems. ArXiv, abs/2310.08560, 2023. 3

[14] Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 2023. 1, 2, 3, 6

[15] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, and Wei Liu. Communicative agents for software development. ArXiv, abs/2307.07924, 2023. 2

[16] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems, 2023. 1, 2, 3, 7, 8, 14, 15

[17] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 7

[18] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530, 2022. 1,2

[19] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llmplanner: Few-shot grounded planning for embodied agents with large language models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2986-2997, 2022. 2

[20] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. In Neural Information Processing Systems, 2023. 1, 2, 3, 4, 5, 7, 8, $9,15,16$

[21] Sai Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. IEEE Access, 12:55682-55696, 2023. 4

[22] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Chatgpt empowered long-step robot control in various environments: A case application. IEEE Access, 11:95060$95078,2023.1,2$

[23] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, abs/2305.16291, 2023. 1, 2, 3, 4, 5

[24] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. ArXiv, abs/2402.01030, 2024. 2, 14

[25] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. In Neural Information Processing Systems, 2023. 2

[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Neural Information Processing Systems, 2022. 2, 7

[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229-256, 1992. 3

[28] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. ArXiv, abs/2309.07864, 2023. 2

[29] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Neural Information Processing Systems, 2023. 3

[30] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. 1, 2, 7, 8, 9, 15

[31] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yue Ting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. ArXiv, abs/2402.17574, 2024. 3

[32] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Y. Liu, and Gao Huang. Expel: Llm agents are experiential learners. In AAAI Conference on Artificial Intelligence (AAAI), 2024. 1, 2, 3, 5, 7, 8, 14, 15

[33] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. ArXiv, abs/2310.04406, 2023. 3

[34] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. ArXiv, abs/2305.17144, 2023. 1, 2, 3

[35] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules. ArXiv, abs/2310.07064, 2023. 3
