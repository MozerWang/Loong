# An In-depth Look at Gemini’s Language Abilities 

Syeda Nahida Akter ${ }^{*, 1}$, Zichun Yu ${ }^{*, 1}$, Aashiq Muhamed ${ }^{*, 1}$, Tianyue Ou ${ }^{*, 1}$, Alex Bäuerle ${ }^{1}$<br>Ángel Alexander Cabrera ${ }^{1}$, Krish Dholakia ${ }^{2}$, Chenyan Xiong ${ }^{1}$, Graham Neubig ${ }^{1}$<br>${ }^{1}$ Carnegie Mellon University, ${ }^{2}$ BerriAI


#### Abstract

The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all English-language tasks that we benchmarked, but find that Gemini Pro excels in translation into other languages for the languages that it supports. We further provide explanations for some of the under-performing tasks, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, and others. We also identify areas where Gemini Pro demonstrates comparably high performance, such as handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark


## 1 Introduction

Gemini is the most recent in a series of large language models released by Google DeepMind [Gemini Team, 2023]. It is notable in particular because the results reported by the Gemini team are the first to rival the OpenAI GPT model series [Brown et al., 2020] across a wide variety of tasks. Specifically, Gemini's "Ultra" version reportedly outperforms GPT-4 on a wide variety of tasks, while Gemini's "Pro" version is reportedly comparable to GPT-3.5 [Gemini Team, 2023]. Despite the potential impact of these results, the exact evaluation details and model predictions have not been released, limiting the ability to reproduce, inspect, and analyze the results and their implications in detail.

In this paper, we conduct an in-depth exploration of Gemini's language understanding and generation abilities, with two goals:

1. We aim to provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini model classes with reproducible code and fully transparent results.
2. We aim to take an in-depth look into the results, identifying areas where one of the two model classes excels.

Furthermore, we also perform a limited comparison with the recently released Mixtral model, as a point of reference for a best-in-class open source model [Mistral AI team, 2023].[^0]

| Task | Model |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Dataset | Gemini Pro | GPT 3.5 Turbo | GPT 4 Turbo | Mixtral |
| Knowledge-based QA | MMLU (5-shot) | 65.22 | 67.75 | $\mathbf{8 0 . 4 8}$ | $\underline{68.81}$ |
|  | MMLU (CoT) | 62.09 | $\underline{70.07}$ | $\mathbf{7 8 . 9 5}$ | 59.57 |
| Reasoning | BIG-Bench-Hard | 67.53 | $\underline{71.02}$ | $\mathbf{8 3 . 9 0}$ | 60.76 |
| Mathematics | GSM8K | 76.42 | $\underline{78.01}$ | $\mathbf{9 2 . 7 2}$ | 71.65 |
|  | SVAMP | 81.10 | $\underline{82.30}$ | $\mathbf{9 2 . 6 0}$ | 81.60 |
|  | ASDIV | 85.31 | $\underline{89.07}$ | $\mathbf{9 2 . 7 5}$ | 83.16 |
| Machine Translation | MAWPS | 96.50 | $\underline{98.00}$ | $\mathbf{9 8 . 6 7}$ | 96.00 |
|  | $\underline{53.31}$ | 52.68 | $\underline{74.43}$ | $\underline{\mathbf{7 6 . 8 3}}$ | 45.12 |
|  | FLORES (5-shot) All | 21.68 | $\mathbf{5 4 . 0 0}$ | 40.97 |  |

Table 1: Main results of our benchmarking. The best model is listed in bold, and the second best model is underlined.

We perform this analysis over 10 datasets, testing a variety of text understanding and generation capabilities, including the models' abilities to answer knowledge-based questions (MMLU; Hendrycks et al. [2021]), perform reasoning (BigBenchHard; Suzgun et al. [2022]), answer mathematics questions (e.g. GSM8K; Cobbe et al. [2021]), translate between languages (e.g. FLORES; Goyal et al. [2022]), generate code (e.g. HumanEval; Chen et al. [2021]), and act as an instruction-following agent (WebArena; Zhou et al. [2023b]). ${ }^{1}$

A summary of our main results can be found in Table 1. In sum, we found that across all tasks, as of this writing (December 27, 2023), Gemini's Pro model achieved comparable but slightly inferior accuracy compared to the current version of OpenAI's GPT 3.5 Turbo for all English tasks, but superior ability to translate into other languages that it supports. Mixtral was competitive with the Gemini and GPT models for Knowledge-based QA and Mathematics tasks, but fell short in more complex tasks. In the following sections, we will detail our experimental methodology (Section 2) and then perform an in-depth description and analysis of the results on each task. Each analysis is accompanied by an online results browser using Zeno [Cabrera et al., 2023], ${ }^{2}$ which can be accessed through the Zeno Report images in this PDF. All results and code for reproduction can be found at https://github.com/neulab/gemini-benchmark.

## 2 Experimental Setup

Before discussing evaluation results and findings, this section describes our experiment configurations, including models tested, model querying details, and evaluation procedures.

### 2.1 Models Tested

In this work, we compare 4 models.

Gemini Pro is the second largest model in the Gemini Series, next to the largest Gemini Ultra. ${ }^{3}$ The model is based on the Transformer [Vaswani et al., 2017] architecture and was trained multimodally over videos, text, and images. The number of parameters and size of training data are not disclosed. In the original Google paper on Gemini, it was reported to achieve similar performance to GPT 3.5 Turbo.[^1]

GPT 3.5 Turbo is the second most capable text model served by OpenAI, part of the GPT-3 series [Brown et al., 2020]. The model has been instruction tuned and trained using reinforcement learning from human feedback [Ouyang et al., 2022], but was trained solely on text. Similarly, model size and precise training details are not disclosed.

GPT 4 Turbo is the second generation of the GPT-4 [OpenAI, 2023] family, a family of models trained multimodally. The turbo version is moderately cheaper than the original GPT-4 model (making it more conducive to benchmarking) and similarly lacks detail of the actual training algorithms, data, or parameter size.

Mixtral in contrast, is an open-source mixture-of-experts model, where each feedforward block picks from a set of 8 distinct groups of parameters and uses two to process the token [Mistral AI team, 2023]. It has been reported to achieve comparable accuracy to GPT 3.5 Turbo on several tasks, including some examined in this paper. We use the mistralai/Mixtral-8x7b-Instruct-v0.1 version of the model.

### 2.2 Model Querying Details

All models were queried through the unified interface provided by LiteLLM ${ }^{4}$ between December 11-22, 2023. Gemini was queried through Google Vertex AI, OpenAI models through the OpenAI API, and Mixtral through the API provided by Together. ${ }^{5}$ For reference, we also list the current pricing of each model through these APIs for 1M tokens in Table 2, which provides an approximate measure of how efficiently the models can be run.

It is also notable that in some cases Gemini Pro, by default, has safety features ${ }^{6}$ that block some questions, particularly in the case of potentially illegal or sensitive material. For analysis in this paper, we disabled these safety set-

| Language Model | Input | Output |
| ---: | ---: | ---: |
| Gemini Pro | $\$ 1.00$ | $\$ 2.00$ |
| GPT 3.5 Turbo | $\$ 1.00$ | $\$ 2.00$ |
| GPT 4 Turbo | $\$ 10.00$ | $\$ 30.00$ |
| Mixtral | $\$ 0.60$ | $\$ 0.60$ |

Table 2: Pricing per 1M tokens. Gemini Pro charges by character; we multiply by 4, a rule-of-thumb average of characters per English token [Raf, 2023]. tings, but in some cases discuss the effect on measured accuracy (contrasting with gemini-pro-filtered, a model with these safety filters enabled).

### 2.3 Evaluation Procedure

To perform a fair comparison between the models, we re-ran experiments with all models using exactly the same prompts and evaluation protocol for all evaluated models. We make this decision to ensure that all models are compared on exactly the same footing, in contrast to previous papers where these settings may differ. In general, we tried to follow both prompts and evaluators from standard repositories, either those officially released by the datasets themselves, or from the Eleuther evaluation harness [Gao et al., 2023a]. We also personally communicated with the Gemini team, and in some cases followed their recommended prompts for evaluating the models, in the cases where these prompts provided uniformly superior to the standard prompts over all evaluated model classes. These prompts generally consist of a query, input, and few-shot examples, sometimes including chain-of-thought reasoning [Wei et al., 2022]. In some cases, we found it necessary to make small changes from standard practice to stably evaluate all models under consideration; all such deviations are noted below and implemented in the companion code repository.

## 3 Knowledge-based QA Zeno Report

In this category, we focus on 57 knowledge-based multiple-choice question-answering tasks from MMLU [Hendrycks et al., 2021], which span topics across STEM, the humanities, the social sciences, and more. MMLU has been widely used as a holistic evaluation of LLMs' knowledge-based capabilities. There are 14,042 test samples in total.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-04.jpg?height=277&width=827&top_left_y=241&top_left_x=389)

Figure 1: Overall accuracy on MMLU with 5-shot prompts and chain-of-thought prompts

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-04.jpg?height=279&width=461&top_left_y=251&top_left_x=1255)

Figure 2: Ratio of multiple-choice answers being predicted by models

### 3.1 Experimental Details

Generation Parameters We examine two popular evaluation methods in this task, including the standard 5-shot prompts from Hendrycks et al. [2021] and 5-shot chain-of-thought prompts from chain-of-thought-hub ${ }^{7}$ [Fu et al., 2023] with a prefix of "Let's think step by step." [Kojima et al., 2022]. Note that we opt not to sample multiple responses and perform self-consistency based reranking [Wang et al., 2022a] as done by Gemini Team [2023], as this significantly increases cost and may not be feasible in many scenarios. We generate via greedy search with a temperature of 0 .

Evaluation For the standard prompting, we directly take the first character generated by models as their answer since this is what the 5 -shot prompts imply. Sometimes, the model may not follow this format and output the answer elsewhere. We treat examples like this as incorrect (and elaborate more on the effect of this in the following section). For the chain-of-thought prompting, we perform answer extraction from the model's response and set the default answer as "C" if no answer can be extracted, as is done in chain-of-thought-hub.

### 3.2 Results and Analysis

In this section, we compare and analyze the overall performance, performance by sub-tasks, and performance by output length on MMLU.

First, from the overall results shown in Figure 1, we can see that Gemini Pro achieves an accuracy close to, but slightly lower than that of GPT 3.5 Turbo, and much lower than that of GPT 4 Turbo. MMLU is the strongest task for the Mixtral model, with it besting both Gemini Pro and GPT 3.5 Turbo. We saw some degradation of performance using chain-of-thought prompting, likely because MMLU is mostly a knowledge-based question answering task that may not benefit significantly from reasoning-oriented prompts. ${ }^{8}$

Based on this overall result, we next dive a bit deeper. One first notable point is that all questions in MMLU are multiple-choice with 4 potential answers ordered A through D. In Figure 2, we show the ratio of the number of times each model selects each multiple choice answer. From this figure, we can see that Gemini has a very skewed label distribution, biased towards selecting the final choice of "D", which contrasts to the result of the other models, which are more balanced. This may indicate that Gemini has not been heavily instruction-tuned towards solving multiple-choice questions, which can cause models to be biased with respect to answer ordering [Pezeshkpour and Hruschka, 2023, Zheng et al., 2023, Tjuatja et al., 2023].

Next, we examine each subtask's performance. Figure 3 illustrates each model's performance on selected representative tasks. We notice that Gemini Pro underperforms on most tasks compared to GPT 3.5 Turbo. Chain-of-thought prompting decreases the variance across the subtasks.

Further, we dig deeper into the tasks where Gemini Pro underperforms/outperforms GPT 3.5 Turbo the most. From Figure 4, we can observe that Gemini Pro falls behind GPT 3.5 Turbo on human_sexuality, marketing abstract_algebra, and miscellaneous. In contrast, Gemini Pro excels at both college_biology and high_school_biology, as well as high_school_macroeconomics and security_studies.[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-05.jpg?height=428&width=1377&top_left_y=236&top_left_x=363)

Figure 3: Accuracy by each subtask on MMLU

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-05.jpg?height=292&width=662&top_left_y=762&top_left_x=363)

(a) Top-4 tasks where GPT 3.5 wins over Gemini Pro

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-05.jpg?height=285&width=659&top_left_y=763&top_left_x=1096)

(b) Tasks where Gemini Pro wins over GPT 3.5

Figure 4: Tasks where Gemini Pro and GPT 3.5 prevail on MMLU

One important thing to note is that, as previously mentioned in subsection 2.2, Gemini Pro's safety features can have a significant effect on overall performance. All results reported above are Gemini Pro with safety filtering disabled, but when the features are enabled the response rate and corresponding performance can drop. In most MMLU sub-tasks, the API response rate was greater than $95 \%$, but two had notably low response rates: moral_scenarios at $85 \%$ and human_sexuality at $28 \%$.

Finally, we analyze how the output length in the chain-of-thought prompting affects the model performance in Figure 5. Generally, a stronger model tends to perform more complex reasoning and thus outputs a longer response. One of the noteworthy advantages of Gemini Pro is that its accuracy is less influenced by the output length compared to the two counterparts. It even outperforms GPT 3.5 when the output length is over 900. However, it also can be seen that Gemini Pro and GPT 3.5 Turbo rarely output these long reasoning chains compared to GPT 4 Turbo.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-05.jpg?height=371&width=1401&top_left_y=1739&top_left_x=362)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-05.jpg?height=287&width=696&top_left_y=1754&top_left_x=365)

(a) Accuracy by output length

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-05.jpg?height=294&width=701&top_left_y=1753&top_left_x=1059)

(b) Output length distribution

Figure 5: Analysis of output length on MMLU

## 4 General-purpose Reasoning

## Zeno Report

In this category, we focus on 27 diverse reasoning tasks from BIG-Bench Hard [Suzgun et al., 2022] which consists of arithmetic, symbolic and multilingual reasoning and factual knowledge understanding tasks. Most of the tasks consist of 250 question-answer pairs, with a few having somewhat fewer.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-06.jpg?height=239&width=683&top_left_y=241&top_left_x=379)

Figure 6: Overall accuracy on BIG-Bench Hard

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-06.jpg?height=290&width=686&top_left_y=253&top_left_x=1099)

Figure 7: Accuracy by question length on BIGBench Hard

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-06.jpg?height=278&width=664&top_left_y=703&top_left_x=1095)

(b) Tasks where Gemini Pro excels over GPT 3.5

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-06.jpg?height=285&width=656&top_left_y=703&top_left_x=366)

(a) Tasks where GPT 3.5 excels over Gemini Pro

Figure 8: Tasks where Gemini Pro and GPT 3.5 prevail on BBH

### 4.1 Experimental Details

Generation Parameters We follow standard 3-shot prompts from the Eleuther harness across all models where each question is followed by a chain of thought resulting in a final concluding sentence of "So the answer is __.". For hyperparameters, we perform greedy decoding, generating with temperature of 0 .

Evaluation The Eleuther evaluation harness implementation of BIG-Bench Hard matches the sentence "So the answer is __." and extracts the text. However, we found that for some models, they did not produce this sentence verbatim, even in cases when they generated the correct answer, particularly multiple-choice tasks where the answer is an option chosen from the question text (e.g., "answer: (B)"). To remedy this, we modified the matching rule, instead taking the last word of the generated text as the answer of the question only for multiple-choice tasks.

### 4.2 Results and Analysis

For the reasoning tasks, we report the overall performance, performance by question complexity, performance by answer types and performance by BIG-Bench sub-task.

First, we illustrate the overall accuracy in Figure 6, we can see that Gemini Pro achieves an accuracy slightly lower than that of GPT 3.5 Turbo, and much lower than that of GPT 4 Turbo. In contrast, the Mixtral model achieves somewhat lower accuracy than Gemini Pro.

Based on this overall result, let us dig a little bit deeper into why Gemini might be underperforming. First, we examined accuracy by the length of the question, as detailed in Figure 7. We found that Gemini Pro underperformed on longer, more complex questions while the GPT models were more robust to this. This was particularly the case for GPT 4 Turbo, which showed very little degradation even on longer questions, indicating an impressively robust ability to understand longer and more complex queries. GPT 3.5 Turbo fell in the middle with respect to this robustness. Mixtral had low accuracy overall compared to the Gemini GPT models but was notably stable with respect to question length until length 70 .

Next we look at whether there are variations in accuracy by the specific task in BIG-Bench Hard. In Figure 8a, we list the tasks where GPT 3.5 Turbo outperformed Gemini Pro by the largest amount.

We can notice that Gemini Pro is particularly bad at the 'tracking_shuffled_objects' tasks. These tasks involve keeping track of who has certain objects as they are traded among people, and Gemini Pro often has difficulty keeping the order straight (as Table 3).

| Prompt | Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At <br> the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets <br> Frankenstein, and Claire gets Lolita. <br> As the semester proceeds, they start trading around the new books. First, Claire and Bob <br> swap books. Then, Bob and Alice swap books. Finally, Claire and Bob swap books. At <br> the end of the semester, Bob has <br> Options: <br> (A) Ulysses <br> (B) Frankenstein <br> (C) Lolita |
| :--- | :--- |
| Label | (B) |
| Output | (0) At the start: Alice: Ulysses, Bob: Frankenstein, Claire: Lolita. |
| (1) Claire and Bob swap books: Alice: Ulysses, Bob: Lolita, Claire: Frankenstein. |  |
| (2) Bob and Alice swap books: Alice: Frankenstein, Bob: Ulysses, Claire: Lolita. |  |
| (3) Claire and Bob swap books: Alice: Frankenstein, Bob: Lolita, Claire: Ulysses. |  |
| At the end of the semester, Bob has Lolita. So the answer is (C). |  |

Table 3: Example of Gemini Pro's inability to maintain state in tracking_shuffled_objects.

However, there were a few tasks where Gemini Pro outperformed GPT 3.5 Turbo. The Figure 8b shows the seven tasks where Gemini Pro outperformed GPT 3.5 Turbo by the largest amount. These were heterogeneous and included those that required world knowledge (sports_understanding), manipulating stacks of symbols (dyck_languages), sorting words in alphabetical order (word_sorting), detecting sarcasm (snarks) and parsing tables (penguins_in_a_table), among others.

Tasks that mostly require natural language understanding (i.e., semantic understanding, name disambiguation, entity resolution, grammar rules, or sarcasm/humor detection), namely salient_translation_error_detection, snarks, hyperbaton, disambiguition_qa, and ruin_names, the Mixtral model performed particularly well, often outperforming both Gemini and GPT 3.5 Turbo (as in Figure 9).

We further investigate the robustness of LLMs across different answer types in the Figure below. We can see that Gemini Pro shows the worst performance in Valid/Invalid answer type which falls under the task formal_fallacies representing logical deduction from a given context. However, Gemini outperformed all GPT models as well as Mixtral on Other answer types (consisting of the word_sorting and dyck_language tasks) which follows a similar line of findings as above i.e., Gemini is particularly good at word rearrangement and producing symbols in the correct order. Also for MCQ and Digit answers, GPT models excel in this genre while Gemini and Mixtral struggles to compete with them.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-07.jpg?height=526&width=1261&top_left_y=1881&top_left_x=432)

Figure 9: Tasks where Mixtral excels over GPT 3.5 Turbo and Gemini

In sum, there did not seem to be a particularly strong trend in which tasks one model performed better than the other, so when performing general-purpose reasoning tasks it may be worth trying both the Gemini and GPT models before making a decision on which to use. On the other hand, between Gemini and Mixtral, Mixtral is more reliable in multi-variable reasoning and natural language understanding tasks, among other.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-08.jpg?height=301&width=702&top_left_y=256&top_left_x=1058)

Figure 10: Accuracy by answer types

## 5 Mathematics

To evaluate the mathematical reasoning ability of the evaluated models, we explore four math word problems benchmarks (1) the grade-school math benchmark, GSM8K [Cobbe et al., 2021], (2) the SVAMP dataset [Patel et al., 2021] with questions generated by varying word-order to check the robust reasoning ability, (3) the ASDIV dataset [Miao et al., 2020] with diverse language patterns and problem types and (4) the MAWPS benchmark [Koncel-Kedziorski et al., 2016] consisting of arithmetic and algebraic word problems.

### 5.1 Experimental Details

Generation Parameters We consider standard 8-shot chain-of-thought prompts [Gao et al., 2023a, Wei et al., 2022] where each question in few-shot prompting is associated with a chain of thought for generating the corresponding answer. All models use greedy decoding using a temperature of 0 .

Evaluation In evaluation, we make a slight modification to the standard evaluation protocol in the Eleuther harness, which consisted of matching the words "The answer is" followed by a numerical output. We found that all evaluated models had a tendency to output the correct answer even when this specific phrase was not present. To mitigate this, we are simply taking the last number of the generated text as the answer to the question, which resulted in higher accuracy overall.

### 5.2 Results and Analysis

In this section, we compare the accuracy of Gemini Pro to GPT 3.5 Turbo, GPT 4 Turbo, and Mixtral, on the four math word problems tasks, examining overall performance, performance by question complexity, and performance by chain-of-thought depth.
![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-08.jpg?height=494&width=640&top_left_y=1737&top_left_x=360)

(a) GSM8K

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-08.jpg?height=217&width=637&top_left_y=2011&top_left_x=362)

(c) ASDIV

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-08.jpg?height=204&width=632&top_left_y=1752&top_left_x=1126)

(b) SVAMP

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-08.jpg?height=214&width=632&top_left_y=2013&top_left_x=1126)

(d) MAWPS

Figure 11: Overall accuracy across four mathematical reasoning tasks

First, looking at overall results in the Figure 11, we can see that Gemini Pro achieves an accuracy slightly lower than that of GPT 3.5 Turbo, and much lower than that of GPT 4 Turbo on the GSM8K,

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=735&width=1433&top_left_y=229&top_left_x=346)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=274&width=640&top_left_y=248&top_left_x=363)

(a) GSM8K

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=284&width=642&top_left_y=617&top_left_x=362)

(c) ASDIV

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=284&width=642&top_left_y=243&top_left_x=1121)

(b) SVAMP

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=287&width=640&top_left_y=610&top_left_x=1119)

(d) MAWPS

Figure 12: Accuracy by question length across four mathematical reasoning tasks

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=678&width=1421&top_left_y=1068&top_left_x=344)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=277&width=548&top_left_y=1084&top_left_x=363)

(a) GSM8K

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=274&width=637&top_left_y=1408&top_left_x=362)

(c) ASDIV

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=79&width=90&top_left_y=1088&top_left_x=909)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=274&width=640&top_left_y=1083&top_left_x=1119)

(b) SVAMP

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-09.jpg?height=284&width=640&top_left_y=1403&top_left_x=1122)

(d) MAWPS

Figure 13: Accuracy by number of digits in the answer across four mathematical reasoning tasks

SVAMP and ASDIV tasks, which all contain diverse language patterns. For the MAWPS task, all models achieve more than $90 \%$ accuracy, although Gemini Pro is still slightly worse than the other models. In contrast, the Mixtral model achieves slightly lower accuracy compared to Gemini Pro except on the SVAMP task.

Similarly to Section 4 we break down the results to observe the robustness of each model to question length in Figure 12. As with the reasoning tasks on BIG-Bench Hard, we see a drop-off on longer questions. As before, GPT 3.5 Turbo outperforms Gemini Pro on shorter questions, but drops off more quickly, with Gemini Pro achieving similar (but still slightly inferior) accuracy on longer questions - except the MAWPS task where Gemini excels over GPT 3.5 on longer questions. Mixtral's degradation is slightly worse than Gemini's except for SVAMP.

Additionally, we observe the accuracy of the models when the answer requires longer chains of thought. As shown in Figure 14, GPT 4 Turbo is very robust even when using long reasoning chains, where GPT 3.5 Turbo, Gemini Pro, and Mixtral struggle with increasing COT lengths. In this analysis, we also find that Gemini Pro is superior to GPT 3.5 Turbo in the most complex examples where the COT length is over 100, but underperforms in the shorter examples. In contrast, Mixtral is more
affected by longer chain-of-thoughts compared to other models, showing the lowest performance in the most complex examples.

Finally, we investigate the accuracy of the compared models in generating answers with varying numbers of digits. We create three buckets based on the number of digits in the answer, 1 , 2, or 3+ (except for the MAWPS task which does not have answers of more than two digits). As shown in Figure 13, GPT 3.5 Turbo appears to be

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-10.jpg?height=349&width=851&top_left_y=346&top_left_x=908)
more robust to multi-digit math probFigure 14: GSM8K accuracy by chain-of-thought length lems, where Gemini Pro and Mixtral degrades somewhat more on problems with more digits.

To summarize, GPT 4 Turbo is the best model across all math word problem tasks, showing more than $90 \%$ accuracy in all tasks. In contrast, Gemini Pro and Mixtral lag behind all GPT models in this domain, while Gemini shows slightly better reasoning ability than Mixtral.

## 6 Code Generation zeno Report

In this category, we examine the models' coding abilities using two code generation datasets HumanEval [Chen et al., 2021] and ODEX [Wang et al., 2022b]. The former tests basic code understanding on a limited set of functions from the Python standard library, while the latter tests the ability to use a broader set of libraries from the entire Python ecosystem. Both of them take as input a humanwritten task description in English (often with test cases). These problems evaluate comprehension of language, algorithmic understanding, and elementary mathematics. Overall, HumanEval has 164 test samples, and ODEX has 439 test samples.

### 6.1 Experimental Details

Generation Parameters We follow the standard zero-shot code evaluation pipeline provided by the ODEX $^{9}$. We generate with temperature 0.0 , which demonstrated the best performance for all models, compared to other temperatures. We use a prompt of "Complete the following python3 function: " to ensure that the models' output fits the desired format.

Evaluation We perform execution-based evaluation, measuring the Pass @ 1 metric, which determines whether a single model output passes test cases [Chen et al., 2021]. Since code generation is evaluated in a zero-shot fashion, the model may inevitably output code that does not conform to our input format well. Therefore, we perform rudimentary post-processing to make the output code fit into the final verification pipeline as much as possible, including the removal of markdown code blocks, the extraction of function implementations and the truncation of natural language sentences following the code. We do not automatically fix missing library imports.

### 6.2 Results and Analysis

In this section, we examine the overall performance and present a few examples.

First, from the results shown in Figure 15, we can see that Gemini Pro achieves a Pass @ 1 lower than GPT 3.5 Turbo and GPT 4 Turbo on both tasks. On HumanEval, Gemini Pro significantly outperforms Mixtral, and on ODEX Mixtral and Gemini Pro have roughly equivalent accuracy. ${ }^{10}$

Second, we analyze the relationship between the gold solution length and the model performance in Figure 16a. The solution length can partly indicate the difficulty of solving the corresponding code[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-11.jpg?height=304&width=1401&top_left_y=230&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-11.jpg?height=233&width=675&top_left_y=241&top_left_x=367)

(a) HumanEval

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-11.jpg?height=233&width=675&top_left_y=241&top_left_x=1083)

(b) ODEX

Figure 15: Overall accuracy on code generation tasks

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-11.jpg?height=361&width=1399&top_left_y=627&top_left_x=360)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-11.jpg?height=282&width=678&top_left_y=645&top_left_x=363)

(a) Accuracy by gold solution length on HumanEval

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-11.jpg?height=280&width=650&top_left_y=646&top_left_x=1106)

(b) Accuracy by used libraries on ODEX

Figure 16: Comparison of Pass@ 1 w.r.t. gold solution length and the libraries used by gold solution

generation task. We find that even though Gemini Pro achieves comparable Pass @ 1 with GPT 3.5 when the solution length is below 100 (e.g., easier cases), it falls behind by large margins when the solution becomes longer. This is an interesting contrast to the results from previous sections, where we found that in general Gemini Pro performed robustly with respect to longer inputs and outputs on English language tasks.

On the ODEX benchmark all models achieved lower accuracy than HumanEval, with a significant portion of the errors being due to the models failing to import libraries that they were using, or using non-current APIs. We also present the analysis of how the libraries required in each solution affect the model performance in Figure 16b. Gemini Pro performs slightly worse than GPT 3.5 on most library-used cases, such as mock, pandas, numpy, and datetime.

Finally, we show several concrete examples of failure cases where Gemini Pro performs worse in code generation than GPT 3.5. First, we noticed that Gemini is somewhat worse at correctly choosing functions and arguments from the Python API. For instance, given this prompt:

def f_3283984():

"""decode a hex string ' $4 a 4 b 4 c$ ' to UTF-8."".

Gemini Pro generated the following code, which results in a type mismatch error:

bytes(bytearray.fromhex('4a4b4c'), 'utf-8')

In contrast, GPT 3.5 Turbo used the following code, which achieves the desired result:

```
hex_string $=$ ' $4 \mathrm{a} 4 \mathrm{~b} 4 \mathrm{c}$ '
decoded_string = bytes.fromhex(hex_string).decode('utf-8')
return decoded_string
```

Further, Gemini Pro had a higher proportion of mistakes where the implemented code was syntactically correct but did not correctly match with a more complex intent. For instance, with respect to the following prompt:

```
from typing import List
def remove_duplicates(numbers: List[int]) -> List[int]:
    """From a list of integers, remove all elements that occur more than once.
    Keep order of elements left the same as in the input.
```

```
>> remove_duplicates([1, 2, 3, 2, 4])
[1, 3, 4]
" |"
```

Gemini Pro created an implementation that just extracts the unique numbers without removing those that appear more than once.

```
seen_numbers = set()
unique_numbers = []
for number in numbers:
    if number not in seen_numbers:
        unique_numbers.append(number)
        seen_numbers.add(number)
return unique_numbers
```


## 7 Machine Translation zeno Report

This set of experiments evaluates the models' multilingual ability, specifically their ability to translate between various language pairs, using the FLORES-200 machine translation benchmark [NLLB Team et al., 2022]. We focus on a diverse subset of 20 languages used by the analysis of Robinson et al. [2023], which encompass various levels of resource availability and translation difficulty. We evaluate on the 1012 sentences from the test set for all the chosen language pairs. As the first step of this study, we limited our scope to translations from English to other languages $(\mathrm{ENG} \rightarrow \mathrm{X})$ only.

### 7.1 Experimental Details

Generation Parameters We use a five-shot prompting strategy (zero shot results are also noted in Appendix C), specifically the prompts proposed by Gao et al. [2023b] designated in Table 7. Our experimental setup employed a top_p value of 1 , a temperature of 0.3 , a context_length of -1 , and max_tokens 500, which we found to generally achieve good performance for translation.

Evaluation To evaluate the outputs, we utilized sentence level averaged chrF2++, leveraging the implementation provided by sacreBLEU [Post, 2018]. This standard metric is based on character and word $n$-gram overlap between the system output and the reference sentence. We compute the sentence level chrF scores. For simplicity, we refer to this metric as chrF in our discussion [Popović, 2017].

### 7.2 Results and Analysis

Overall Performances In Table 4, we conduct a comparative analysis of Gemini Pro, GPT 3.5 Turbo, GPT 4 Turbo, and Mixtral. We also compare against established translation-specific systems like Google Translate ${ }^{11}$, and NLLB-MoE [NLLB Team et al., 2022], the leading open-source machine translation (MT) model known for its extensive language coverage.

The results indicate that Google Translate generally outperforms other models in languages that it supports, excelling in 10 languages. It is followed by NLLB, which excels on 6 languages. Gemini Pro provided impressive accuracy on several languages, even having the best accuracy on 3: South Levantine Arabic, Romanian, and Mesopotamian Arabic (surpassing not only GPT 3.5 Turbo, but also GPT 4 Turbo).

However, on average, the results indicate that the general-purpose language models showed competitive performances but have not yet surpassed the dedicated machine translation systems in translation into non-English languages.

Figure 17 illustrates the comparative performance of language models across language pairs for the 5 -shot prompt. GPT 4 Turbo showed a consistent deviation of performance with NLLB relative to GPT 3.5 Turbo and Gemini Pro. This reflects the findings in the literature on GPT 4 Turbo's multilingual performance [OpenAI, 2023]. GPT 4 Turbo also offered larger improvements for lowresource languages (as measured by NLLB Team et al. [2022]), whereas for high-resource languages[^5]

| Lang. | Gemini Pro | GPT 3.5 Turbo | GPT 4 Turbo | Mixtral | Google | NLLB |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ssw_Latin | $0.43^{\dagger}$ | 18.16 | 38.62 | 18.84 | - | 43.51 |
| sna_Latin | $0.00^{\dagger}$ | 24.44 | 43.25 | 22.20 | 44.36 | 43.40 |
| ckb_Arab | $0.00^{\dagger}$ | 26.69 | 41.30 | 18.36 | 47.61 | 47.25 |
| mag_Deva | 34.54 | 39.70 | 45.46 | 25.93 | - | $\overline{58.03}$ |
| ibo_Latin | $0.00^{\dagger}$ | 21.46 | $\underline{41.94}$ | 17.75 | 43.37 | 41.36 |
| hau_Latin | $0.02^{\dagger}$ | 30.24 | 50.82 | 22.47 | 53.18 | 53.25 |
| pbt_Arab | $0.00^{\dagger}$ | 22.81 | 34.21 | 16.61 | sus | 39.27 |
| tam_Taml | $0.00^{\dagger}$ | 35.50 | $\overline{48.04}$ | 23.78 | 55.98 | 53.63 |
| kat_Geor | $0.00^{\dagger}$ | 33.32 | 40.94 | 23.78 | 51.11 | 47.10 |
| gle_Latin | $0.00^{\dagger}$ | 46.72 | 56.52 | 26.93 | 59.93 | 57.87 |
| kmr_Latin | $0.00^{\dagger}$ | 30.03 | 33.33 | 19.04 | 39.94 | 39.25 |
| war_Latin | $0.52^{\dagger}$ | 51.17 | 56.01 | 34.74 | - | 57.25 |
| ajp_Arab | 50.64 | 47.45 | 47.01 | 33.60 | - | $\underline{50.61}$ |
| lim_Latin | 39.99 | 43.77 | 46.05 | 32.31 | - | 47.58 |
| ukr_Cyrl | 56.89 | 54.56 | $\overline{56.44}$ | 49.66 | 58.25 | 56.04 |
| fra_Latin | 70.77 | $\underline{70.99}$ | 70.77 | 66.73 | 72.98 | 70.01 |
| lvs_Latin | 59.49 | 54.55 | 57.95 | 31.17 | 62.49 | 54.89 |
| ron_Latin | 65.09 | 63.18 | 63.93 | 56.68 | 65.08 | 61.21 |
| tpi_Latin | $6.20^{\dagger}$ | 40.14 | 47.97 | 33.33 | - | 42.02 |
| acm_Arab | 49.05 | 45.26 | 44.44 | 31.65 | - | $\overline{32.60}$ |

Table 4: Machine translation performance ( $\operatorname{chrF}(\%)$ scores) across models for all languages using 5 -shot prompt. Best scores are bolded, second best underlined. ${ }^{\dagger}$ indicates languages where Gemini Pro blocked more than $50 \%$ of responses.

performance was similar between the LLMs. In comparison, Gemini Pro outperforms both GPT 3.5 Turbo and GPT 4 Turbo on 5 out of 20 languages, and achieved the top performances on 3 languages. Mixtral underperformed the other models across language pairs.

Gemini Blocked Responses If we analyze responses at a language level, we see in Figure 18 that Gemini Pro's lower performance in 12/20 languages is due to its tendency to block responses on particular languages, generally ones with lower levels of resources. We consider a response "blocked" if Gemini Pro generates a Blocked Response error, and define unblocked languages as those languages where $>50 \%$ samples are not blocked.

Examining performance at a language level in Figure 19, we see that Gemini Pro outperforms GPT 3.5 Turbo and GPT 4 Turbo on 5/8 unblocked languages most of which are high resource. Additionally, in Figure Figure 19, we observe that the implementation of safety filters leads to a decrease in the overall chrF score. This reduction occurs because the filters block samples from languages that the model otherwise handles relatively effectively.

Other trends In Figure 21, we present apparent trends when categorizing languages by family or script. A key observation is Gemini Pro's competitive performance with other models on Cyrillic scripts, is contrasted by its underperformance on other scripts. GPT 4 Turbo stands out, outperforming other models across various scripts, especially in the Devanagari script.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-14.jpg?height=791&width=1105&top_left_y=255&top_left_x=510)

Figure 17: Machine translation performance (chrF (\%) scores) by language pairs for 5-shot prompt.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-14.jpg?height=456&width=1114&top_left_y=1168&top_left_x=495)

Figure 18: Number of samples that are blocked by Gemini Pro 5-shot

In Figure 20, we examine the performance of various models across different sentence length segments, categorized by both target length (Figure 20a) and predicted length (Figure 20b). Upon scrutinizing Figure 20, we observe that Gemini Pro's performance at longer target lengths does not match that of GPT 4 Turbo and GPT 3.5 Turbo. However, when considering predicted lengths, Gemini Pro generally outperforms both GPT 4 Turbo and GPT 3.5 Turbo at longer lengths, suggesting it produces higher quality translations at longer lengths. Additionally, the figure suggests that a significant portion of the performance decline at shorter predicted lengths even on unblocked languages may be attributed to empty predictions, likely triggered by the content filtering mechanism.

## 8 Web Agents Zeno Report

Finally, we examine the ability of each model to act as an instruction following agent that performs tasks on the web,

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-14.jpg?height=517&width=506&top_left_y=1758&top_left_x=1189)

Figure 21: Performance by script

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-15.jpg?height=548&width=1314&top_left_y=252&top_left_x=400)

system

gemini-pro-5-shot gemini-pro-filtered-5... - gpt-3.5-turbo-5-shot gpt-4-turbo-5-shot mixtral-5-shot

Figure 19: Performance in chrf (\%) on blocked and unblocked languages for 5-shot prompt.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-15.jpg?height=285&width=693&top_left_y=893&top_left_x=369)

(a) chrf by target sentence length

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-15.jpg?height=287&width=697&top_left_y=889&top_left_x=1061)

(b) chrf by predicted sentence length

Figure 20: Performance with varying target and predicted length using 5-shot prompt on unblocked languages.

which requires long-term planning and complex data understanding. We use WebArena [Zhou et al., 2023b], an execution-based simulation environment where the success criterion is based on execution outcome. Tasks given to agents consist of information seeking, site navigation, and content $\&$ configuration operations. The tasks span over a variety of web sites, including E-commerce platforms, social forums, collaborative software development platforms (e.g. gitlab), content management systems, and online maps.

### 8.1 Experiment Details

Generation Parameters We follow WebArena's testing methodology in testing Gemini. We used the two-shot chain-of-thought prompts from Zhou et al. [2023b], where each prompt includes two CoT style examples. We further distinguished between whether or not the model is instructed to terminate execution when it believes the task is unachievable (the "unachievable" hint, or UA in WebArena parlance).

| CoT | UA Hint | Model | SR | SR $_{\text {AC }}$ |
| :---: | :---: | :---: | :---: | :---: |
| $\checkmark$ | $\checkmark$ | GEmini-pRo | 7.12 | 3.52 |
| $\checkmark$ | $\boldsymbol{x}$ | GEMINI-PRo | 6.25 | 4.83 |
| $\checkmark$ | $\checkmark$ | GPT-3.5-TURBo | 8.87 | 6.44 |
| $\checkmark$ | $\boldsymbol{x}$ | GPT-3.5-tURbo | 6.36 | 6.06 |
| $\checkmark$ | $\boldsymbol{x}$ | GPT-4-tURbo | $\mathbf{1 4 . 9 0}$ | $\mathbf{1 4 . 2 2}$ |

Table 5: Performances on WebArena.

In sum, we tested with two prompts from WebArena: p_cot_id_actree_2s and p_cot_id_actree_2s_no_na, which are the CoT prompt with the UA hint and CoT prompt without the UA hint, respectively. To make results comparable between GPTs and Gemini, we set the same upper limit on the observation lengths for all of them. This number is set to 1920 tokens using the tokenizer of gpt-4-1106-preview, consistent with experiments in WebArena. In terms of hyper-parameters, we used the default suggested by each of the large language model providers. For the Gemini models, the suggested default temperature is 0.9 and default top-p is 1.0 , and the WebArena suggested default for GPT models is 1.0 for temperature and 0.9 for top-p.
![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-16.jpg?height=596&width=1398&top_left_y=252&top_left_x=366)

system

gemini-pro-cot

- gemini-pro-cot-uahi

$\square$ gpt-3.5-cot

3.5-cot-uahint

mixtral-cot-uahint

Figure 23: Web agent success rate of evaluated models at different site groups

Evaluation Procedure The action sequence of an agent is considered correct as long as they achieved the final goal, regardless the intermediate steps they take. We use WebArena's evaluation, which determines wether a task is completed successfully or not with the agent's final output.

### 8.2 Results and Analysis

We examine Gemini-Pro's overall success rate, rate across different tasks, its response lengths, trajectory step counts, and tendency to predict that the task is unachievable. The overall performance is list in Table 5. Gemini-Pro performs comparably but slightly worse than GPT-3.5-Turbo. Similarly to GPT-3.5-Turbo, Gemini-Pro performs better when the prompt mentions that task might be unachievable (UA hint). With UA hint, Gemini-Pro achieves an overall 7.09 percent success rate.

If we break down by websites, as shown in Figure 23, we can see that Gemini-Pro performs worse than GPT-3.5-Turbo on gitlab and maps, while being close to GPT-3.5-Turbo on shopping admin, reddit, and shopping. It performs better than GPT-3.5-Turbo on multi-site tasks, which is in concert with our previous results of Gemini being a bit better on the more complex sub-tasks across benchmarks.

In general, Gemini-Pro predicts more tasks as unachievable, especially in the case where a UA hint is given, as shown in

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-16.jpg?height=599&width=480&top_left_y=1386&top_left_x=1278)

Figure 22: UA prediction count Figure 22. Gemini-Pro predicts over $80.6 \%$ of the tasks as unachievable when given a UA hint, compared to $47.7 \%$ by GPT-3.5-Turbo. Note that $4.4 \%$ of the tasks in the dataset are actually unachievable, so both far over-predict the actual number of unachievable tasks.

At the same time, we observed that Gemini Pro has a greater tendency to respond in shorter phrases and take fewer steps before reaching a conclusion. As shown in Figure 24a, more than half of trajectories by Gemini Pro are under ten steps, while majority of trajectories by GPT 3.5 Turbo and GPT 4 Turbo are between 10 and 30 steps. Similarly, the majority of Gemini responses are less than 100 characters in length, while most of GPT 3.5 Turbo, GPT 4 Turbo, and Mixtral's responses are over 300 characters in length Figure 24b. Gemini tends to directly predict the actions while other models would start with reasoning and then give their action predictions.

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-17.jpg?height=377&width=1353&top_left_y=240&top_left_x=386)

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-17.jpg?height=290&width=528&top_left_y=256&top_left_x=403)

(a) Average steps taken per task

![](https://cdn.mathpix.com/cropped/2024_06_04_830ddc7f0b5521f6fed0g-17.jpg?height=288&width=789&top_left_y=257&top_left_x=928)

(b) Average response length

Figure 24: Model behaviors on WebArena.

## 9 Conclusion

In this paper, we have taken a first impartial, in-depth look into Google's Gemini model, comparing it to OpenAI's GPT 3.5 and 4 models, as well as the open source Mixtral model.

Takeaways We came away with a number of conclusions:

- The Gemini Pro model, which is comparable to GPT 3.5 Turbo in model size and class, generally achieves accuracy that is comparable but somewhat inferior to GPT 3.5 Turbo, and much worse than GPT 4 on English tasks.
- In particular, we find that Gemini Pro was somewhat less performant than GPT 3.5 Turbo on average, but in particular had issues of bias to response order in multiple-choice questions, mathematical reasoning with large digits, and premature termination of agentive tasks. When using the default content filtering settings, there were also failed responses due to aggressive content filtering.
- On the other hand, there were bright points: Gemini performed better than GPT 3.5 Turbo on particularly long and complex reasoning tasks.
- In addition, when generating text in other languages (specifically through translation), Gemini Pro outperforms both GPT 3.5 Turbo and GPT 4 Turbo on the languages where requests are not blocked, but there are several languages for which Gemini Pro does not return any answer.
- The open-source model Mixtral is competitive with Gemini Pro and GPT 3.5 Turbo on Knowledge-based QA and Mathematics tasks, but significantly underperformed on other tasks.

Limitations Finally, we would like to temper these conclusions with a number of limitations.

First, our work is a snapshot in time with respect to ever-changing and unstable API-based systems. All results here are current as of this writing on December 27, 2023, but may change in the future as models and the surrounding systems are upgraded.

Second, the results may be dependent on the specific prompts and generation parameters that we selected. In fact, we found that the results of all models were affected by prompt selection, and that the GPT models seemed somewhat more robust to small variations in the prompts of the GPT models. It is quite possible that with further prompt engineering, or multiple samples and self-consistency as was used by Gemini Team [2023], the results could change significantly.

Finally, any benchmarking paper would be remiss without a discussion of data leakage, which plagues current evaluation of large language models [Zhou et al., 2023a]. While we did not measure this leakage explicitly, we did attempt to mitigate by evaluating on a broad variety of tasks, including those who's outputs were not sourced from or widely available on the internet (such as WebArena).

Outlook Based on this paper, we can make the recommendation to researchers and practitioners to carefully look at the Gemini Pro model as a tool in the toolbox, comparable to GPT 3.5 Turbo. In particular, Gemini Pro may be a preferable alternative when processing non-English languages. Gemini's Ultra edition, which is yet to be released, is reported to be on par with GPT 4, and a further examination of this model will be warranted when it is available.

## Acknowledgements

We greatly appreciate the help of Zhiruo Wang in handling the ODEX dataset, and Shuyan Zhou with high-level guidance on the WebArena experiments. We also are very grateful to the Gemini team who, based on an earlier version of this draft, provided significant help in attempting to reproduce the numbers in their paper. We also thank those who provided comments on our earlier paper draft on social media, including Arthur Mensch, who noted that we had inadvertently used a non-official third-party version of the Mixtral model in comparisons, and those who pointed out an inaccuracy in description of the Mixtral model's mixture of experts mechanism.

## References

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Ángel Alexander Cabrera, Erica Fu, Donald Bertucci, Kenneth Holstein, Ameet Talwalkar, Jason I Hong, and Adam Perer. Zeno: An interactive framework for behavioral evaluation of machine learning. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages $1-14,2023$.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306, 2023.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023a. URL https://zenodo.org/records/10256836.

Yuan Gao, Ruili Wang, and Feng Hou. How to design translation prompts for chatgpt: An empirical study. arXiv preprint arXiv: 2304.02182, 2023b.

Gemini Team. Gemini: A family of highly capable multimodal models. Technical report, Google, 12 2023. URL https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf.

Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzmán, and Angela Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522-538, 2022.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.

Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213, 2022.

Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.

Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, 2020.

Mistral AI team. Mixtral of experts, December 2023. URL https://mistral.ai/news/mixtral-of-experts/. Accessed: 2023-12-15.

NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. META, 2022.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 168. URL https://aclanthology.org/2021.naacl-main.168.

Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.

Maja Popović. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612-618, 2017.

Matt Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018.

Raf. What are tokens and how to count them? https://help.openai.com/en/articles/ 4936856-what-are-tokens-and-how-to-count-them, 2023. Accessed: 2023-12-15.

Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. Chatgpt mt: Competitive for high- (but not low-) resource languages. Conference on Machine Translation, 2023. doi: $10.48550 /$ arXiv.2309.07423.

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. Do llms exhibit human-like response biases? a case study in survey design. arXiv preprint arXiv:2311.04076, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, pages 5998-6008. Curran Associates, Inc., 2017.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a.

Zihuro Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481, 2022b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. arXiv e-prints, pages arXiv-2309, 2023.

Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don't make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964, 2023a.

Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023b.
