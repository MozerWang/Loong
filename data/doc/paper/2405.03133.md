# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training 

Zexuan Zhong ${ }^{\dagger}$, Mengzhou Xia $^{\dagger}$, Danqi Chen ${ }^{\dagger}$, Mike Lewis ${ }^{\ddagger}$<br>${ }^{\dagger}$ Princeton University, ${ }^{\ddagger}$ Meta AI<br>\{zzhong, mengzhou,danqic\}@cs.princeton.edu,mikelewis@meta.com


#### Abstract

Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9\%) and a variety of downstream tasks ( $+1.5 \%-11.1 \%$ ). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.


## 1 Introduction

Mixture-of-experts (MoE) architectures with sparse activation enable the scaling of model sizes while maintaining high training and inference efficiency (Lepikhin et al., 2021; Fedus et al., 2022; Du et al., 2022; Zoph et al., 2022; Lewis et al., 2021; Zhou et al., 2022; Jiang et al., 2024; Xue et al., 2024; Shen et al., 2024). However, training the routing network in $\mathrm{MoE}$ architectures introduces the challenge of optimizing a non-differentiable, discrete objective (Shazeer et al., 2017; Zoph et al., 2022). Various techniques-such as switch routing (Fedus et al., 2022), top-k expert-choice routing (Zhou et al., 2022), and linear programming (Lewis et al., 2021)—have been developed to address this challenge, often requiring carefully designed load balancing objectives (Fedus et al., 2022) or introducing additional complexity in assignment algorithms (Lewis et al., 2021; Roller et al., 2021).

Recent research has started to explore fully-differentiable $\mathrm{MoE}$ architectures as an alternative to overcome training difficulty. Notably, SMEAR (Muqeeth et al., 2023) is an approach that softly merges experts as a weighted average of all the experts' parameters, as opposed to activating the top-k experts. However, the effectiveness of SMEAR has only been demonstrated in small-scale fine-tuning experiments on downstream classification tasks (Wang et al., 2018). In this work, we propose Lory ${ }^{1}$, the first approach that scales such fully-differentiated MoE architectures to autoregressive language model pre-training. Unlike[^0]

Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training

![](https://cdn.mathpix.com/cropped/2024_06_04_6aa1173fa2e26d15f2e8g-02.jpg?height=483&width=1390&top_left_y=344&top_left_x=365)

Figure 1: We propose (Cory, a fully differentiable MoE architecture designed for autoregressive language models based on expert merging (Section 2.2). We introduce two key techniques to train Lory: First, we propose the causal segment routing strategy, which conducts expert merging at the segment level and preserves the autoregressive property of language models. Second, we use the similarity-based data batching method to construct training instances, which steers the experts toward specializing in specific domains or topics.

text classification tasks which only require routing each input sequence to different experts, language modeling makes predictions for each input token, and performing token-level routing is prohibitively expensive as the computational cost of merging operations scales linearly with the number of experts.

Lory is based on two key techniques (Figure 1). We first propose causal segment routing. For a sequence of input tokens, we split them into multiple segments with a fixed length, and use the previous segment to determine the router's weights and calculate the merged expert for the subsequent segment. During inference, we can simply use the prompt to make a single routing decision throughout the generation. This segment-level routing strategy preserves the autoregressive nature of language models, while keeping the merging operations efficient. However, since the text data for pre-training language models usually concatenates random sets of documents, we find that such routing can lead to scenarios in which experts are not sufficiently specialized. Hence, we propose our second techniquesimilarity-based data batching for MoE training, which groups semantically similar documents to form consecutive segments. This idea has been recently proposed to train LMs to better reason across document boundaries (Shi et al., 2024), while we find that it leads to more effective training of expert routing.

We pre-train a series of Lory models from scratch under a training budget of 150B tokens, with $0.3 \mathrm{~B}$ and 1.5B active parameters, and 8,16 or 32 experts (up to $6.8 \mathrm{~B}$ and 29.5B full parameters; see Table 3). Experimental results show that our Lory models significantly outperform equal-sized dense models trained with the same amount of data, achieving performance gains on both perplexity ( $+13.9 \%$ ), and a wide range of downstream tasks including commonsense reasoning $(+3.7 \%)$, reading comprehension $(+3.3 \%)$, closed-book QA $(+1.5 \%)$, and text classification ( $+11.1 \%$ ). Interestingly, despite that Lory uses segmentlevel routing, we find it achieves competitive performance compared to state-of-the-art MoE models with token-level, non-differentiable discrete routing (Zhou et al., 2022). Our analysis further shows that the trained experts in Lory capture domain-level specialization without any supervision, making it distinct from previous MoE LMs with token-level routing, which only exhibits local patterns uniformly distributed across different domains (Xue et al., 2024; Jiang et al., 2024). Together, we present the first fully-differentiated MoE model that is suitable for language model pre-training, and demonstrate its effectiveness at scale. We hope our work sheds light on the potential of fully differentiable MoE architectures in cultivating specialized experts and we seek to encourage continued exploration in this research field.

## 2 Preliminaries

### 2.1 Sparsely-activated MoE

Transformer-based MoE language models typically substitute feed-forward network (FFN) layers with sparsely-activiated MoE layers (Shazeer et al., 2017; Fedus et al., 2022; Zoph et al., 2022). Assume an MoE layer consists of $E$ expert FFNs, each parameterized as $\operatorname{FFN}\left(\cdot ; \theta_{1}\right), \ldots, \operatorname{FFN}\left(\cdot ; \theta_{E}\right)$, where the function FFN $: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ defines a single expert module. For each token $x$ in a sequence, an MoE layer takes the hidden representation $h_{x} \in \mathbb{R}^{d}$ as the input and computes its output $o_{x} \in \mathbb{R}^{d}$ by sparsely activating $k$ experts in this layer and aggregating the outputs through a weighted sum:

$$
\begin{equation*}
o_{x}=\sum_{i=1}^{E} e_{i} \cdot \operatorname{FFN}\left(h_{x} ; \theta_{i}\right), \quad \text { where } e_{i}=\operatorname{Top}-k\left(\operatorname{Softmax}\left(R\left(h_{x}\right)\right)\right)_{i} \tag{1}
\end{equation*}
$$

The routing weight $e_{i}$ for the $i$-th expert is measured by a routing network or router $R$, which takes $h_{x}$ as input and calculates the weight for each expert. In practice, to achieve sparsity and computational efficiency, only one (Fedus et al., 2022) or top-k (Lepikhin et al., 2021) experts with the highest routing weights are activated at each MoE layer. The weights of the remaining experts are set to 0 (i.e., $e_{i}=0$ ), eliminating the need to compute $\operatorname{FFN}\left(h_{x} ; \theta_{i}\right)$ and effectively deactivating the $i$-th expert.

### 2.2 Fully Differentiable MoE Architectures via Expert Merging

The primary challenges in training sparsely activated MoE models arise from the difficulty in training discrete routers. A promising direction is to design fully differentiable MoE architectures that do not depend on extra loss formulations for stablized training. A recent model architecture (Muqeeth et al., 2023) demonstrates the feasibility by computing a weighted average of all expert FFNs in the parameter space (Matena \& Raffel, 2022; Wortsman et al., 2022), thereby creating a "merged FFN". Given an input $x$ and its corresponding routing weights $e_{i}$, the output $o_{x}$ of a merged FFN is computed as:

$$
\begin{equation*}
o_{x}=\operatorname{FFN}\left(h_{x} ; \sum_{i=1}^{E} e_{i} \cdot \theta_{i}\right), \quad \text { where } e_{i}=\operatorname{Softmax}\left(R\left(h_{x}\right)\right)_{i} \tag{2}
\end{equation*}
$$

However, naively extending it to autoregressive language models, which would require computing the merged FFN for each token in a sequence, would be infeasible as the computational costs of merging operations scales linearly with the number of experts. SMEAR (Muqeeth et al., 2023) has only been evaluated for downstream fine-tuning on text classification tasks, which makes routing decisions based on a pooling representation of the entire input sequence, i.e., $e_{i}=\operatorname{Softmax}\left(R\left(\frac{\sum_{j=1}^{L} h_{x_{j}}}{L}\right)\right)_{i}$. Such operations will disrupt the autoregressive property in language model pre-training. In this work, we address these challenges by developing a fully differentiable MoE architecture suitable for autoregressive language modeling, and pre-train such models at scale.

## 3 Our Approach: Lory

In this section, we present Lory, an approach for pre-training fully differentiable MoE language models (Figure 1). The core technique that enables Lory to be fully differentiable is expert merging (Muqeeth et al., 2023, see details in Section 2.2). To make it computationally feasible, we propose a causal segment routing method that only merges experts once for each segment, effectively reducing the number of merging operations (Section 3.1). We also propose a data batching strategy of grouping semantically similar texts, which is crucial for effective training of the segment-level router (Section 3.2).

Notations. We denote an input sequence of $L$ tokens as $X=\left(x_{1}, x_{2}, \ldots, x_{L}\right)$. By considering a segment size $T$, we divide the input sequence into $N=\lceil L / T\rceil$ segments, denoted as
$S_{1}, S_{2}, \ldots, S_{N}$. We use $R$ to denote the routing network (parameterized as a linear layer) that computes the weights for expert merging. Let $h_{x}$ represent the hidden representation of the token $x$. The parameters of the $i$-th expert FFN are denoted by $\theta_{i}$.

### 3.1 Efficient Expert Merging via Causal Segment Routing

Challenges. An intuitive way of reducing the computational cost is to use segment-level routing instead of token-level routing, which can reduce the number of merging operations from $L$ to $N$ times. However, simply using the current segment to compute the routing weights can cause information leakage.

Training design. We propose causal segment routing to effectively route information across segments in an autoregressive manner. ${ }^{2}$ It merges FFNs in an MoE layer based on the previous segment's information, and uses it to process the current segment. Specifically, given a training instance $X$ that consists of $L$ tokens (e.g., $L=4096$ ), we split the training instance into $N$ segments, each of which contains $T$ (e.g., $T=256$ ) consecutive tokens. For the $k$-th segment $S_{k}$ when $k>1$, we compute the average of the hidden representations of its preceding segment $S_{k-1}$, denoted as $\bar{h}_{k-1}$. Using the average hidden representation allows the model to adapt to prompts of varying lengths during inference. $\bar{h}_{k-1}$ is then utilized to determine the routing weights, resulting in a merged expert $\bar{\theta}$ :

$$
\begin{equation*}
\bar{h}_{k-1}=\frac{1}{T} \sum_{x \in S_{k-1}} h_{x}, \quad e_{i}=\operatorname{Softmax}\left(R\left(\bar{h}_{k-1}\right)\right), \quad \bar{\theta}=\sum_{i} e_{i} \cdot \theta_{i} . \tag{3}
\end{equation*}
$$

We then use the merged expert $\bar{\theta}$ to process all the tokens in the current segment $S_{k}$, i.e., $o_{x}=\operatorname{FFN}\left(h_{x} ; \bar{\theta}\right), \forall x \in S_{k}$. This approach guarantees that the routing decisions made by the model are based exclusively on data from preceding positions. For the first segment $S_{1}$, the representation of the segment itself is used to compute the merging weights for its own FFN. To prevent information leakage, we implement a stop-gradient operation on $R\left(\bar{h}_{1}\right)$. As demonstrated in Appendix B, merging experts at the segment level incurs minimal overhead compared to the training of dense models.

Prompt-only routing during inference. During inference, we begin with a given prompt and make a single routing decision per layer based on the average hidden representations of the prompt. This routing decision determines a merged FFN and it is used consistently throughout the entire generation process. It is important to note that this inference process is as simple and computationally efficient as dense models. ${ }^{3}$

### 3.2 Similarity-based Data Batching

The standard practice of pre-training LMs is to randomly concatenate documents to construct training instances with a fixed length. This could lead to under-specialized experts, because tokens within adjacent segments may come from very different and irrelevant documents. To mitigate this issue, we employ a similarity-based data batching technique inspired by Shi et al. (2024), which sequentially concatenates similar documents to construct training instances. This encourages high similarity between adjacent segments, enabling the experts to specialize in different domains or topics. We measure document similarity using Contriever (Izacard et al., 2022) and concatenate similar documents based on a greedy[^1]search algorithm (see Appendix C). Although we employ a data batching technique similar to Shi et al. (2024), our motivation differs from theirs. While their work aims to improve language models' reasoning across document boundaries, we find this technique effective in encouraging expert specialization in training MoE models.

## 4 Experiments

In this section, we evaluate Lory by training a series of language models from scratch. We first describe the experimental setups (Section 4.1) and then present the results (Section 4.2).

### 4.1 Setups

Models. We evaluate our approach by training decoder-only Transformer models which consist of $0.3 \mathrm{~B}$ and $1.5 \mathrm{~B}$ active parameters. ${ }^{4}$ For each FFN layer in the Transformer model, we replace it with MoE layers with $E \in\{8,16,32\}$ experts with exactly the same architecture. ${ }^{5}$ Appendix D shows the configuration of model architectures as well as the total parameter count. We follow LLaMA (Touvron et al., 2023a) and use SwiGLU (Shazeer, 2020) as the activation function in FFNs. We use the same tokenizer as the LLaMA models (Touvron et al., 2023a;b). All models are trained with a 4096-token context window. In the causal segment routing strategy, we set the length of each segment to be $T=256$.

Training details. We employ the AdamW optimizer (Loshchilov \& Hutter, 2019) with $\beta_{1}=0.9$ and $\beta_{2}=0.95$ and use a learning rate of $2 \mathrm{e}-4$ with a cosine learning rate scheduler. All models with a batch size of 1 million tokens. We employ the data parallelism with the ZeRO optimization (Rajbhandari et al., 2020) for distributed training. ${ }^{6}$ At the beginning of training, we train a parameter-matched dense model and duplicate the FFN layers as initialization of the MoE model. In our experiments, we use the first $5 \%$ training steps as the warmup to initialize the MoE weights. We find that without warmup training, there may be more experts under-utilized (see Appendix G. 4 for an ablation study). We also apply a linear warmup to the learning rate scheduler for the first $5 \%$ training steps. We train our models with up to 64 A100 GPUs.

Training datasets. We randomly sample a subset of the Commoncrawl dataset (Wenzek et al., 2019) as the training data. The full training dataset consists of 150 billion tokens in total. We apply the similarity-based data batching method on this subset of construct all the training instances, following Shi et al. (2024). See Appendix C for details of the data batching method.

Evaluation datasets. We evaluate all the models on language modeling tasks by measuring the perplexity of trained models on held-out evaluation datasets sampled from arXiv, Books, Wikipedia, C4 (Raffel et al., 2020), and Python code (a Python subset of Github). Each evaluation dataset contains $1 \mathrm{~K}$ samples, each of which consists of 4096 tokens.

We also evaluate models in downstream tasks with in-context learning (Brown et al., 2020), including common sense reasoning: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrand (Sakaguchi et al., 2020); reading comprehension: RACE (Lai et al., 2017), ARC (Clark et al., 2018)); closedbook QA: Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017); and text classification: AGNews (Zhang et al., 2015), SST-2 Socher et al. (2013), Amazon and Yelp (Zhang et al., 2015), FEVER (Thorne et al., 2018), MRPC (Dolan \& Brockett, 2005). For text classification tasks, we follow the evaluation setup of Min et al. (2022); for the rest of tasks, we follow the same setup as Touvron et al. (2023b).[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_6aa1173fa2e26d15f2e8g-06.jpg?height=366&width=1347&top_left_y=283&top_left_x=367)

Figure 2: Left: training curves (log perplexity) of models with different sizes and experts. Right: Perplexity of trained models on different evaluation sets (arXiv, Books, Wikipedia, C4, and Python). We include the detailed model configurations and sizes in Appendix D.

| Model | Commonsense Reasoning |  |  |  |  | Reading Comprehension |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | PIQA | SIQA | BoolQ | HellaSwag | WinoGrande | RACE-m | RACE-h | ARC-e | ARC-c |
| $0.3 \mathrm{~B}$ | 65.8 | 42.7 | 44.6 | 34.6 | 51.2 | 41.7 | 30.9 | 51.5 | 21.3 |
| $0.3 \mathrm{~B} / 8 \mathrm{E}$ | 67.5 | 41.2 | 41.2 | 34.8 | 54.4 | 43.1 | 31.4 | 52.4 | 22.1 |
| $0.3 \mathrm{~B} / 16 \mathrm{E}$ | 67.2 | 44.1 | 56.6 | 34.9 | 54.1 | 43.9 | 31.1 | 54.8 | 24.9 |
| $0.3 \mathrm{~B} / 32 \mathrm{E}$ | 68.2 | 43.0 | 58.0 | 34.7 | 53.4 | 42.7 | 32.0 | 57.4 | 26.3 |
| 1.5B | 71.2 | 45.0 | 54.0 | 43.9 | 60.9 | 50.1 | 36.7 | 65.0 | 31.0 |
| $1.5 \mathrm{~B} / 8 \mathrm{E}$ | 72.1 | 45.2 | 62.0 | 43.6 | 63.7 | 51.2 | 36.5 | 66.3 | 32.5 |
| $1.5 \mathrm{~B} / 16 \mathrm{E}$ | 71.3 | 45.0 | 56.0 | 43.7 | 61.5 | 51.7 | 37.3 | 66.3 | 32.7 |
| $1.5 \mathrm{~B} / 32 \mathrm{E}$ | 72.1 | 47.1 | 59.9 | 43.8 | 61.9 | 51.5 | 32.4 | 66.7 | 32.7 |
|  | Closed-book QA |  | Text Classification |  |  |  |  |  | $\Delta_{\mathrm{vg}}$ |
| Model | NQ | TQA | AGNews | Amazon | SST-2 | Yelp | Fever | MRPC | Hog |
| $0.3 \mathrm{~B}$ | 4.7 | 8.8 | 30.3 | 53.6 | 54.6 | 66.0 | 47.6 | 62.0 | 41.8 |
| $0.3 \mathrm{~B} / 8 \mathrm{E}$ | 5.3 | 9.0 | 38.4 | 52.3 | 54.6 | 62.6 | 56.6 | 59.0 | 42.7 |
| $0.3 \mathrm{~B} / 16 \mathrm{E}$ | 6.0 | 10.2 | 36.3 | 75.6 | 53.3 | 64.0 | 57.0 | 65.0 | 45.8 |
| $0.3 \mathrm{~B} / 32 \mathrm{E}$ | 5.3 | 10.2 | 47.3 | 64.0 | 55.3 | 73.3 | 55.7 | 56.0 | 46.0 |
| $1.5 \mathrm{~B}$ | 7.6 | 23.8 | 64.0 | 65.3 | 80.0 | 58.6 | 59.0 | 66.7 | 51.9 |
| $1.5 \mathrm{~B} / 8 \mathrm{E}$ | 7.3 | 24.2 | 65.0 | 94.0 | 80.0 | 88.3 | 57.0 | 64.0 | 56.1 |
| $1.5 \mathrm{~B} / 16 \mathrm{E}$ | 7.3 | 25.6 | 61.6 | 78.3 | 84.6 | 93.6 | 57.3 | 63.6 | 55.1 |
| $1.5 \mathrm{~B} / 32 \mathrm{E}$ | 7.0 | 25.4 | 62.3 | 94.7 | 85.0 | 95.3 | 56.3 | 66.7 | 56.5 |

Table 1: We compare the Lory MoE models with the parameter-matched dense models on downstream tasks, including commonsense reasoning, reading comprehension, closed-book QA, and text classification.

### 4.2 Main Results

Training efficiency and convergence. Figure 2 (left) shows the training loss curves of the dense model and our MoE models with different model sizes. First, we find that with the same amount of training tokens, our models clearly achieve better training loss compared to the dense model baseline. For the 0.3B and 1.5B models, our models with 32 experts achieve the same level of loss with fewer than half of the training tokens. This indicates that our approach achieves much better performance with the same training compute (see analysis of additional FLOPs from MoE layers in Appendix B). We also observe that when using more experts, we are able to gain more improvement.

Language modeling. We evaluate trained models on language modeling evaluation sets. As shown in Figure 2 (right), our MoE models outperform the dense baseline in all domains, significantly reducing perplexity. For example, our $0.3 \mathrm{~B} / 32 \mathrm{E}$ model achieves a relative improvement of $13.9 \%$ on Books compared to the $0.3 \mathrm{~B}$ dense model. We observe that the improvement is especially large in test domains that are markedly different from the domains of the training dataset (e.g. Python). We consider this as a strong indication of expert specialization in specific domains (We further study expert specialization in Section 5.4).

Downstream tasks. Table 1 shows the model performance on downstream tasks. We observe significant performance across all tasks. For example, our 0.3B/32E model achieves an average performance improvement of $+3.7 \%$ in common sense reasoning, $+3.3 \%$ in reading comprehension, $+1.5 \%$ in reading comprehension, and $+11.1 \%$ in text classification.

## 5 Analysis and Ablation Studies

In this section, we conduct ablation studies and analysis to understand the essence of each component of our approach.

### 5.1 Importance of Causal Segment Routing

We compare our causal segment routing strategy with an alternative prefix routing strategy for training. In prefix routing, expert merging is performed only once for each sequence based on the first segment. The merged FFN is then used to process the rest of the sequence without further updates. Figure 3 shows that using only a prefix for routing leads to much worse performance compared to causal segment routing. These results highlight the importance of using every segment to provide strong training signals for routers.

![](https://cdn.mathpix.com/cropped/2024_06_04_6aa1173fa2e26d15f2e8g-07.jpg?height=322&width=428&top_left_y=1064&top_left_x=407)

Figure 3: Training curves of causal segment routing and prefix routing The latter is a straightforward segment-level routing strategy that uses the first segment to route the entire input.

![](https://cdn.mathpix.com/cropped/2024_06_04_6aa1173fa2e26d15f2e8g-07.jpg?height=334&width=829&top_left_y=1058&top_left_x=930)

Figure 4: Left: Training curves of similarity-based data batching (sim batch) or the standard random batching (rand batch). Right: Training loss difference between Lory and a dense model when using different batching strategies. Lory leads to a larger loss improvement over the dense model when using similarity-based data batching.

### 5.2 Importance of Similarity-based Data Batching

To investigate the importance of similarity-based data batching, we compare the performance improvement of MoE models over dense models with and without this batching method. Figure 4 (left) shows the training loss of dense (0.3B) and MoE models with eight experts (0.3B/8E) using similarity-batched (sim batch) and randomly-batched (rand batch) data. MoE models consistently outperform dense models in both setups. However, the loss improvement (i.e., the difference in loss between dense and MoE models) is much larger with similarity-based batching, and this effect is amplified with more training data (Figure 4 (right)). These results strongly support the importance of similarity-based batching for effectively training our MoE model.

### 5.3 Comparison with Existing MoE Models

We compare our approach with Expert Choice (EC) (Zhou et al., 2022), a state-of-theart MoE method that ensures balanced load during training by having each expert select top-k inputs according to the routing weights. We consider two variants of EC MoE models, both with a capacity factor of 1 to match the computation of our MoE models. First, we train a sparse EC MoE model using our segment routing strategy, where each expert selects top segments and processes all tokens within those segments. This variant allows us to directly compare our expert-merging strategy with the expert choice method while using the same segment-level routing approach.

Second, we consider the original EC setting with token-level routing to provide an end-to-end comparison with state-of-the-art MoE models using the same amount of training computation. Figure 5 shows the training loss curves. We observe that Lory (blue curve) significantly outperforms segment-level EC (orange curve) with the same routing setting, suggesting that a fully differentiable architecture is more effective than a sparse MoE when using the same routing strategy. Comparing Lory with the token-level EC model (green curve), we find that Lory achieves competitive results despite using segmentlevel routing and not requiring any advanced training techniques. These results highlight the significant potential of Lory. In Appendix G.1, we compare Lory and EC on held-out evaluation sets. We find Lory achieves much better perplexity compared to the token-level EC model, while performing similarly on other domains (arXiv, Books, Wiki, C4). Our analysis in Section 5.4 demonstrates that Lory learns experts specialized in specific domains (e.g., Python code), potentially improving performance in less frequent domains.

### 5.4 Expert Utilization and Specialization

Utilization: How many experts are actively utilized? One potential issue of training MoE models is the models may collapse to dense models because most experts are under-utilized (e.g., some experts have never been activated). In Appendix G.2, we show although without using any auxiliary loss on load balancing, Lory is able to achieve high expert utilization, preventing the MoE models from collapsing to dense models.

Specialization: What do experts learn? In order to study the expert specialization, we investigate the averaged routing weights at different layers of the $0.3 \mathrm{~B} / 8 \mathrm{E}$ model, on different domains (Books, arXiv, Python, and Wikipedia). Figure 6 shows the routing weights at layer 0,11 , and 23 (the first, middle, and last layer) of the $0.3 \mathrm{~B} / 8 \mathrm{E}$ model. ${ }^{7}$ First, we find that there exists clear domain-level expert specialization in our trained MoE models, even though no additional domain-level supervision is used during training. For instance, expert 7 at layer 11 is specialized to process inputs in the arXiv domain. We also observe that routing weights on arXiv and Python code are more similar compared to Books and Wikipedia, likely because LaTex code and Python code are dissimilar to natural language. Second, experts at the middle or high layers are more specialized in specific domains, while the routing weights at lower layers are similar and flat across domains.

![](https://cdn.mathpix.com/cropped/2024_06_04_6aa1173fa2e26d15f2e8g-08.jpg?height=330&width=1377&top_left_y=1968&top_left_x=366)

Figure 6: Averaged routing weights at layer $\{0,11,23\}$ of the $0.3 \mathrm{~B} / 8 \mathrm{E}$ model on different domains (Books, arXiv, Python, Wikipedia). We observe that the experts in our MoE models learn domain-level specialization, especially at middle and higher layers.[^3]

It is worth noting that our learned experts behave differently from those of prior token-level MoE models, where shallow token-level specialization is observed. For example, some experts are specialized for a specific type of word (e.g., punctuations, articles), and few deep semantic features are captured by the learned routers (Jiang et al., 2024; Lewis et al., 2021; Zoph et al., 2022; Shazeer et al., 2017; Xue et al., 2024). Our models learn domain-level specialization, which we attribute to the segment-level routing strategy used during training. This strategy allows routers to capture global semantic features beyond the token level. The complementary nature of features captured by segment/sentence-level and token-level routing strategies suggests the possibility of combining them to build even stronger models, and we leave it for future work.

### 5.5 More Analysis and Discussion

In Appendix G, we further show that (1) during inference of downstream tasks, routing the entire input prompt once or routing each segment does not make substantial differences on the tasks we evaluate; (2) warmup training is crucial to achieve high expert utilization, especially when training MoE models with a large number of experts. In addition, we discuss training parallelism strategies when further scaling up model sizes in Appendix H.1; and discuss the potential of converting Lory to sparse models for more efficient inference in Appendix H.2.

## 6 Related Work

Mixture of Experts. Sparsely activated MoE models (Shazeer et al., 2017) have been proposed to demonstrate the potential of massively scaling up model sizes. GShard (Lepikhin et al., 2021) adapts the sparse MoE architecture into Transformer models and achieves strong results on machine translation. Recent work has extended it to general language models (Fedus et al., 2022; Zoph et al., 2022; Jiang et al., 2024; Dai et al., 2024; Zhou et al., 2022; Du et al., 2022; Artetxe et al., 2021; Xue et al., 2024). Traditional MoE models are trained to route given inputs to one or a few specialized expert modules, which introduces a non-differentiable, discrete decision-learning problem. These existing models are trained with the top-1 or top-2 routing strategy on a carefully designed load balancing objective (Lepikhin et al., 2021; Fedus et al., 2022; Zoph et al., 2022), or employ complicated assignment algorithms to distribute inputs (Lewis et al., 2021; Roller et al., 2021; Zhou et al., 2022). Training MoE models has been shown to be difficult, facing the issues of training instability, expert under-specialization, poor training efficiency (Zoph et al., 2022).

Our approach enables end-to-end gradient back-propagation by employing fully differentiable MoE architectures. SMEAR (Muqeeth et al., 2023) proposes softly merging experts by taking a weighted average on the parameter space. However, SMEAR is only applied to text classification tasks with an encoder backbone. Although Lory shares a similar expert merging technique, it is the first approach that scales such architecture to autoregressive language model pre-training. Soft MoE (Puigcerver et al., 2024) is another fully-differentiable MoE architecture which enables end-to-end gradient back-propagation. However, it is only evaluated on vision tasks and does not apply to autoregressive language model pre-training either. We leave how to extend Soft MoE on decoder language models as the future work.

Similarity-based data batching. There exists research that applies a similar data batching method during training. In-context pre-training (Shi et al., 2024) groups relevant documents together to encourage language models to leverage long-range contexts and improve the results of in-context learning and retrieval augmentation. Zhong et al. (2022) batch documents with high lexical similarity to collect more positive pairs in a contrastive learning framework to provide stronger training signals. Despite sharing a similar idea, the goal of our data batching method is to avoid routing irrelevant documents together, which may hurt the expert specialization.

## 7 Conclusion

In this paper, we propose Lory, a fully-differentiable MoE model designed for autoregressive language model pre-training. Our extensive experiments demonstrate that Lory significantly outperforms its dense counterpart on language modeling and downstream tasks. We also observe that trained experts are highly specialized and capable of capturing domain-level information. Future research includes further scaling up Lory, combining token-level routing and segment-level routing, and developing efficient decoding methods for Lory.

## Acknowledgements

We appreciate useful comments and feedback from the members of the Princeton NLP group. We thank Weijia Shi for the help with the experiments and discussion related to the similarity-based data batching method.

## Ethics Statement

This paper presents a new approach for building large language models. We would like to note that, similar to existing language models, the language models trained with our approach may have the same potential societal consequences. For example, language models can produce factually inaccurate outputs, which carry the risk of spreading misinformation (e.g., (Min et al., 2023)). Additionally, malicious users can extract training data used to train language models, potentially causing privacy and licensing issues (Carlini et al., 2021). We acknowledge these potential negative consequences and caution those who use our approach to build powerful language models.

## References

Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Conference on Artificial Intelligence (AAAI), 2020.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In USENIX Security Symposium, 2021.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024.

William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In International Workshop on Paraphrasing (IWP), 2005.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning (ICML), 2022.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. In The Journal of Machine Learning Research (JMLR), 2022.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. In Transactions on Machine Learning Research (TMLR), 2022.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547, 2019.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Association for Computational Linguistics (ACL), 2017.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. In Transactions of the Association of Computational Linguistics (TACL), 2019.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. In Empirical Methods in Natural Language Processing (EMNLP), 2017.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations (ICLR), 2021.

Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning (ICML), 2021.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019.

Michael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. In Advances in Neural Information Processing Systems (NeurIPS), 2022.

Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2022.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Empirical Methods in Natural Language Processing (EMNLP), 2023 .

Mohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745, 2023.

Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. In International Conference on Learning Representations (ICLR), 2024.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. In The Journal of Machine Learning Research (JMLR), 2020.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In International Conference for High Performance Computing, Networking, Storage and Analysis (SC), 2020.

Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Conference on Artificial Intelligence (AAAI), 2020

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. In Empirical Methods in Natural Language Processing (EMNLP), 2019.

Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. In International Conference on Learning Representations (ICLR), 2017.

Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin. JetMoE: Reaching llama2 performance with $0.1 \mathrm{~m}$ dollars. arXiv preprint arXiv:2404.07413, 2024.

Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. In International Conference on Learning Representations (ICLR), 2024.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y $\mathrm{Ng}$, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP), 2013.

James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and verification. In North American Chapter of the Association for Computational Linguistics (NAACL), 2018.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations (ICLR), 2018.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In International Conference on Language Resources and Evaluation (LREC), 2019.

Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (ICML), 2022.

Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. OpenMoE: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Association for Computational Linguistics (ACL), 2019.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems (NeurIPS), 2015.

Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Empirical Methods in Natural Language Processing (EMNLP), 2022.

Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. In Advances in Neural Information Processing Systems (NeurIPS), 2022.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. ST-MoE: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022.
