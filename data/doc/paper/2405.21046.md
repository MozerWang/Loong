# Exploratory Preference Optimization: 

## Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF

Tengyang Xie*<br>tx@cs.wisc.edu<br>Corby Rosset<br>corbyrosset@microsoft.com

Dylan J. Foster*<br>dylanfoster@microsoft.com

Ahmed Awadallah
ahmed.awadallah@microsoft.com

May 30, 2024

Akshay Krishnamurthy<br>akshaykr@microsoft.com

Alexander Rakhlin<br>rakhlin@mit.edu


#### Abstract

Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment. We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses. By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques.

We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical-a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023)—yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data. In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage. Our analysis, which builds on the observation that DPO implicitly performs a form of $Q^{\star}$-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of $K L$-regularized Markov decision processes. Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation.


## 1 Introduction

Reinforcement learning from human feedback (RLHF) is a central tool to align language models to human values and elicit useful behavior (Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022). Using humanlabeled preference data, RLHF achieves enhanced capabilities using a modest amount of data compared to unsupervised pre-training (on the order of tens of millions versus trillions of tokens) by treating the language model as a "policy" and optimizing it with reinforcement learning techniques.

Even though RLHF is typically only applied with preference data from humans or other language models, one might hope that it has potential to produce super-human capabilities because recognizing novel behavior and insights is typically easier than generating novel behavior. Indeed, it is often much easier to verify correctness of a given proof or program than it is to produce one from scratch. By repeatedly generating new proposals and labeling them with human feedback, a language model could gradually push beyond the boundary of human capabilities. Unfortunately, even with the great disparity in difficulty between generation and verification, a major barrier to achieving enhanced capabilities via RLHF is the volume of human feedback,[^0]i.e., sample complexity, required by existing methods. Thus, a promising research direction is to develop sample-efficient methods for RLHF.

A natural way to address the sample efficiency problem for RLHF is to augment algorithms with online exploration. Online exploration exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, novel responses. RLHF algorithms that exploit online feedback have received limited investigation, and in spite of encouraging initial results, existing approaches either do not update the language model (Dwaracherla et al., 2024), or engage in purely passive exploration (Guo et al., 2024; Gao et al., 2024), with no mechanism to encourage novelty or diversity. Passive exploration is intuitively insufficient, as we are unlikely to generate novel and correct proofs by chance; we make this precise in Proposition 2.1. Thus, the full potential of online exploration as a new paradigm for language model training has yet to be realized.

The central challenge in equipping language models with deliberate exploration is to efficiently navigate the vast, combinatorially large space of token sequences to find responses for which feedback will be maximally informative. The contemporary theory of reinforcement learning offers-at a conceptual level-solutions to this problem, providing algorithm design principles for exploration that can optimally take advantage of problem structure and achieve sample efficiency to the best extent one can hope for (Jiang et al., 2017; Agarwal et al., 2019; Foster and Rakhlin, 2023). However, the most powerful approaches in this space are computationally intractable in the general reinforcement learning setting (Jiang et al., 2017; Jin et al., 2021; Foster et al., 2021), and prior attempts to adapt them to RLHF either make unrealistic modeling assumptions (i.e., do not allow for general function approximation) (Xu et al., 2020; Novoseller et al., 2020; Pacchiano et al., 2021; Wu and Sun, 2023; Zhan et al., 2023b; Du et al., 2024; Das et al., 2024), or are computationally inefficient and not feasible to faithfully implement (Chen et al., 2022; Wang et al., 2023; Ye et al., 2024). Can we, perhaps by specializing to language modeling, develop practical, provable, and empirically efficient online exploration methods for RLHF?

### 1.1 Contributions

We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical-a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al. (2023); Guo et al. (2024)) —yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model. We show that XPO is provably sample-efficient, and converges to a near-optimal language model policy under natural exploration conditions (Jin et al., 2021; Xie et al., 2023; Zhong et al., 2022). Critically, and in contrast to prior work, our theory holds irrespective of whether the initial model is sufficiently exploratory on its own. To summarize:

XPO offers the first practical and provably sample-efficient online exploration algorithm for RLHF with general function approximation.

Technical highlights. Our design and analysis of XPO uses previously disparate techniques from language modeling and theoretical reinforcement learning, combining them in a serendipitous fashion through the perspective of $K L$-regularized Markov decision processes (Neu et al., 2017).

1. First, generalizing Rafailov et al. (2024), we observe that DPO can be viewed as implicitly performing Bellman error minimization (Xie and Jiang, 2020) to approximate the optimal value function $Q^{\star}$ in a $K L$-regularized $M D P$. We use this to provide a novel KL-regularized regret decomposition.
2. Then, we show that global optimism (Jiang et al., 2017; Jin et al., 2021; Xie et al., 2023), a powerful RL exploration technique that has classically been viewed as computationally intractable (Dann et al., 2018; Kane et al., 2022; Golowich et al., 2024), can be implemented in any KL-regularized MDP with deterministic transitions (generalizing language modeling) by adding a surprisingly simple exploration bonus to the DPO objective. This yields the XPO objective.

We expect our analysis techniques and perspective to be useful more broadly. In particular, the guarantees for XPO hold not just for language models, but for any reinforcement learning problem with a stochastic starting state and (potentially unknown) deterministic transition dynamics ("Deterministic Contextual MDP").

Empirical results. In Section 3.4, we perform a proof-of-concept experiment to validate our theory, and find that XPO can match the performance of DPO variants (Xu et al., 2023; Tran et al., 2024; Dong et al., 2024) based on passive or heuristic exploration using significantly less preference data. These initial findings suggest that augmenting language models with online exploration may indeed lead to benefits over passive exploration.

Concurrent work. Two concurrent and independent works posted to arXiv just before this preprint, Cen et al. (2024); Zhang et al. (2024), propose algorithms that equip DPO with exploration bonuses similar to XPO. On the theoretical side, both works are restricted to the contextual bandit formulation of RLHF, and do not consider the general reinforcement learning framework in this work or make the connection to $Q^{\star}$ approximation and KL-regularized MDPs. Compared to our results, which give provable sample complexity guarantees with general function approximation, Zhang et al. (2024) do not provide sample complexity guarantees, while Cen et al. (2024) provide guarantees only for linear contextual bandits. In addition, and importantly, the sample complexity guarantees in Cen et al. (2024) have exponential dependence on the KL regularization parameter, which our results avoid. Empirically, both works find benefits from exploration.

### 1.2 Paper Organization

Section 2 presents background on RLHF, online feedback, and the necessity of exploration. Section 3 presents our algorithm and main theoretical guarantees, including motivation behind the algorithm design and a proof sketch. Section 3.4 presents experimental results, and we conclude with discussion in Section 4. Proofs and additional results are deferred to the appendix.

Notation. For an integer $n \in \mathbb{N}$, we let $[n]$ denote the set $\{1, \ldots, n\}$. For a set $\mathcal{X}$, we let $\Delta(\mathcal{X})$ denote the set of all probability distributions over $\mathcal{X}$. We adopt standard big-oh notation, and write $f=\widetilde{O}(g)$ to denote that $f=O(g \cdot \max \{1, \operatorname{polylog}(g)\})$ and $a \lesssim b$ as shorthand for $a=O(b)$.

## 2 Background

This section contains necessary background to present our main results. We begin by recalling the standard formulation of reinforcement learning from human feedback from offline data (Section 2.1), then introduce the online feedback model and highlight the need for systematic exploration (Section 2.2).

### 2.1 Reinforcement Learning from Human Feedback

We study RLHF in a general reinforcement learning formulation which subsumes the token-level MDP formulation considered in prior work (Rafailov et al., 2024), but is somewhat broader.

Markov decision processes. We consider an episodic finite-horizon Markov decision process framework. Formally, a horizon- $H$ MDP $M=(H, \mathcal{S}, \mathcal{A}, P, r, \rho)$ consists of a (potentially very large) state space $\mathcal{S}$, action space $\mathcal{A}$, probability transition function $P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$, reward function $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, and initial state distribution $\rho \in \Delta(\mathcal{S})$. We assume without loss of generality that the state space is layered such that $\mathcal{S}=\mathcal{S}_{1} \cup \mathcal{S}_{2} \cup \cdots \cup \mathcal{S}_{H}$, where $\mathcal{S}_{h}$ is the set of states reachable at step $h$, and $\mathcal{S}_{h} \cup \mathcal{S}_{h^{\prime}}=\varnothing$ for $h \neq h^{\prime}$. A (randomized) policy is a mapping $\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})$, and induces a distribution over trajectories $\tau=\left(s_{1}, a_{1}\right), \ldots,\left(s_{H}, a_{H}\right)$ and rewards $r_{1}, \ldots, r_{H}$ via the following process. The initial state is drawn via $s_{1} \sim \rho$, then for $h=1, \ldots, H: a_{h} \sim \pi\left(s_{h}\right), r_{h}=r\left(s_{h}, a_{h}\right)$, and $s_{h+1} \sim P\left(s_{h}, a_{h}\right)$. We let $\mathbb{E}_{\pi}[\cdot]$ and $\mathbb{P}_{\pi}[\cdot]$ denote expectation and probability under this process, respectively. We assume that $\sum_{h=1}^{H} r_{h} \in\left[0, R_{\max }\right]$ almost surely for a parameter $R_{\max }>0$. For a trajectory $\tau$ and policy $\pi$ we define $r(\tau)=\sum_{h=1}^{H} r\left(s_{h}, a_{h}\right)$ and $\pi(\tau)=\prod_{h=1}^{H} \pi\left(a_{h} \mid s_{h}\right)$.

In the context of language modeling, the main object of interest is the token-level MDP (Rafailov et al., 2024). Here, $s_{1} \sim \rho$ represents a prompt, each action $a_{h}$ represents a token (with $\mathcal{A}$ representing the vocabulary), and the state $s_{h}=\left(s_{1}, a_{1}, \ldots, a_{h-1}\right)$ is the prompt and sequence of tokens so far. The language model is represented by a policy $\pi$, which maps the current context $s_{h}=\left(s_{1}, a_{1}, \ldots, a_{h-1}\right)$ to a distribution over the
next token $a_{h}$. The trajectory $\tau=\left(s_{1}, a_{1}\right), \ldots,\left(s_{H}, a_{H}\right)$ produced by this process can be interpreted as the language model's response to the prompt $s_{1}$; we will occasionally use the terms "trajectory" and "response" synonymously in this context.

Our main results apply to any Deterministic Contextual MDP (DCMDP) for which the initial state is stochastic, but the subsequent transition dynamics are deterministic and potentially unknown. This formulation encompasses but strictly generalizes the token-level MDP.

RLHF with offline data. In the classical RLHF formulation (Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022), we assume access to a dataset $\mathcal{D}_{\text {pref }}=\left\{\left(\tau_{+}, \tau_{-}\right)\right\}$of labeled preference data. Each pair of trajectories (responses) $\left(\tau_{+}, \tau_{-}\right)$represents a positive and negative example; both trajectories begin from the same initial state (prompt) $s_{1}$, and are generated by first sampling a pair $(\tau, \widetilde{\tau})$ via $\tau \sim \pi_{\text {ref }} \mid s_{1}$ and $\widetilde{\tau} \sim \pi_{\text {ref }} \mid s_{1}$ in the underlying DCMDP $M$ (e.g., token-level MDP), and then ordering them as $\left(\tau_{+}, \tau_{-}\right)$based on a binary preference $y \sim \mathbb{P}\left(\tau \succ \widetilde{\tau} \mid s_{1}\right)$. Here, $\pi_{\text {ref }}$ is a reference policy (language model), which is typically obtained via supervised fine-tuning, and the preference $y \sim \mathbb{P}\left(\tau \succ \widetilde{\tau} \mid s_{1}\right)$ is obtained from a human or AI annotator. Following a standard assumption (Christiano et al., 2017; Rafailov et al., 2023, 2024), we assume that preferences follow the Bradley-Terry model (Bradley and Terry, 1952): For trajectories $\tau$ and $\widetilde{\tau}$ both beginning with $s_{1}$,

$$
\begin{equation*}
\mathbb{P}\left(\tau \succ \widetilde{\tau} \mid s_{1}\right)=\frac{\exp (r(\tau))}{\exp (r(\tau))+\exp (r(\widetilde{\tau}))} \tag{1}
\end{equation*}
$$

Based on the preference dataset $\mathcal{D}_{\text {pref }}$, the goal is to learn a policy $\widehat{\pi}$ with high reward. Following prior theoretical works on RLHF, we consider a $K L$-regularized reward objective (Xiong et al., 2023; Ye et al., 2024), defined for a regularization parameter $\beta>0$, via

$$
\begin{equation*}
J_{\beta}(\pi):=J(\pi)-\beta \cdot \sum_{h=1}^{H} \mathbb{E}_{\pi}\left[D_{\mathrm{KL}}\left(\pi\left(\cdot \mid s_{h}\right) \| \pi_{\mathrm{ref}}\left(\cdot \mid s_{h}\right)\right)\right]=\mathbb{E}_{\pi}\left[r(\tau)-\beta \log \frac{\pi(\tau)}{\pi_{\mathrm{ref}}(\tau)}\right] \tag{2}
\end{equation*}
$$

We aim to compute a policy $\widehat{\pi}$ such that

$$
\max _{\pi} J_{\beta}(\pi)-J_{\beta}(\widehat{\pi}) \leq \varepsilon
$$

for some small $\varepsilon>0$. Such a guarantee means that $\widehat{\pi}$ near-optimally maximizes reward, yet stays relatively close to $\pi_{\text {ref }}$ (as a function of $\beta$ ). The choice of $\beta>0$, which is important for safety and reliability, is typically viewed as a domain specific hyperparameter (Tang et al., 2024). Our main focus in this paper is the small- $\beta$ regime, which allows $\widehat{\pi}$ to meaningfully deviate from $\pi_{\text {ref }}$ and generate potentially novel responses. Notably, by taking $\beta$ sufficiently small, it is possible to translate suboptimality bounds for the regularized reward into bounds for the unregularized reward (e.g., Zhu et al., 2023; Zhan et al., 2023a).

We refer to this setting as offline $R L H F$ because the algorithm relies only on the offline dataset $\mathcal{D}_{\text {pref }}$ for training, and does not perform any active data collection.

Direct preference optimization (DPO). Initial approaches to offline RLHF (Christiano et al., 2017; Ouyang et al., 2022) proceed by first estimating a reward function $\widehat{r}$ from $\mathcal{D}_{\text {pref }}$ using the Bradley-Terry model, then optimizing an estimated version of the KL-regularized objective in Eq. (2) using policy optimization methods like PPO, i.e.,

$$
\begin{equation*}
\widehat{\pi} \approx \underset{\pi \in \Pi}{\operatorname{argmax}} \mathbb{E}_{\pi}\left[\sum_{h=1}^{H}\left(\widehat{r}\left(s_{h}, a_{h}\right)-\beta \log \frac{\pi\left(a_{h} \mid s_{h}\right)}{\pi_{\mathrm{ref}}\left(a_{h} \mid s_{h}\right)}\right)\right] \tag{3}
\end{equation*}
$$

The starting point for our work is an alternative approach introduced by Rafailov et al. (2023), Direct Preference Optimization (DPO). DPO is motivated by a closed-form solution for the policy that optimizes the KL-regularized objective in Eq. (2), and condenses the two-step process above into a single policy optimization
objective, removing the need for reward function estimation. Concretely, DPO solves ${ }^{1}$

$$
\begin{equation*}
\widehat{\pi}=\underset{\pi \in \Pi}{\operatorname{argmin}} \sum_{\left(\tau_{+}, \tau_{-}\right) \in \mathcal{D}_{\text {pef }}}-\log \left[\sigma\left(\beta \log \frac{\pi\left(\tau_{+}\right)}{\pi_{\text {ref }}\left(\tau_{+}\right)}-\beta \log \frac{\pi\left(\tau_{-}\right)}{\pi_{\text {ref }}\left(\tau_{-}\right)}\right)\right] \tag{4}
\end{equation*}
$$

for a user-specified policy class $\Pi$, where $\sigma(x):=\frac{\exp (x)}{1+\exp (x)}$ is the sigmoid function.

### 2.2 Online Feedback and Exploration in RLHF

DPO and other offline RLHF methods have achieved great success in language model alignment, but are fundamentally limited to behaviors that are well-supported by the initial model $\pi_{\text {ref }}$ and preference data $\mathcal{D}_{\text {pref }}$. RLHF with online feedback offers a promising approach to move beyond this limitation by collecting feedback from responses sampled from the model during training (Guo et al., 2024).

Formally, the protocol proceeds in $T$ rounds. At each round $t$, we receive an initial state $s_{1}^{(t)}$ and sample two responses $\tau \sim \pi^{(t)} \mid s_{1}$ and $\widetilde{\tau} \sim \pi^{(t)} \mid s_{1}$ from the current policy $\pi^{(t)}$. The prompts are then labeled as $\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)$ and added to the preference dataset via $\mathcal{D}_{\text {pref }}^{(t+1)} \leftarrow \mathcal{D}_{\text {pref }}^{(t)} \cup\left\{\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)\right\}$, which is then used to compute an updated policy $\pi^{(t+1)}$. In practice, the prompts are typically labeled via human feedback or AI feedback (e.g., a larger, more powerful language model (Guo et al., 2024; Rosset et al., 2024)); we assume the preferences $\mathbb{P}\left(\tau^{(t)} \succ \widetilde{\tau}^{(t)} \mid s_{1}^{(t)}\right)$ follow the Bradley-Terry model in Eq. (1).

### 2.3 The Necessity of Deliberate Exploration

Existing approaches to online RLHF adapt offline techniques by applying them iteratively. As an example, Online DPO (Guo et al., 2024) proceeds as follows: ${ }^{2}$

1. Compute $\pi^{(t)}$ by solving the DPO objective in Eq. (4) with the current preference dataset $\mathcal{D}_{\text {pref }}^{(t)}$.
2. Sample $\tau^{(t)}, \widetilde{\tau}^{(t)} \sim \pi^{(t)} \mid s_{1}^{(t)}$, then label as $\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)$ and update $\mathcal{D}_{\text {pref }}^{(t+1)} \leftarrow \mathcal{D}_{\text {pref }}^{(t)} \cup\left\{\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)\right\}$.

We refer to such an approach as passive exploration, as the responses are sampled directly from the policy $\pi^{(t)}$ without an explicit mechanism to encourage diversity. The following proposition shows that passive exploration is insufficient to discover novel behavior: Unless the initial policy $\pi_{\text {ref }}$ has good coverage, Online DPO can fail to learn a near-optimal policy.

Proposition 2.1 (Necessity of deliberate exploration). Fix $\beta \in\left(0, \frac{1}{8} \log (2)\right)$, and consider the bandit setting $(H=1, \mathcal{S}=\varnothing$, and $|\mathcal{A}|=2)$. There exists a reference policy $\pi_{\text {ref }}$ such that for all $T \leq \frac{1}{2} \exp \left(\frac{1}{8 \beta}\right)$, with constant probability, all of the policies $\pi^{(1)}, \ldots, \pi^{(T+1)}$ produced by Online DPO satisfy

$$
\begin{equation*}
\max _{\pi} J_{\beta}(\pi)-J_{\beta}\left(\pi^{(t)}\right) \geq \frac{1}{8} \quad \forall t \in[T+1] \tag{5}
\end{equation*}
$$

That is, the sample complexity required by Online DPO is exponential in $\frac{1}{\beta}$, which is unacceptable in the small- $\beta$ regime; inspecting the proof, it is straightforward to see that the same conclusion holds for Iterative DPO and purely offline DPO. The idea behind Proposition 2.1 is simple: If $\pi_{\text {ref }}$ places small probability mass on the optimal action, Online DPO may fail to ever explore this action until the number of iterations is exponentially large. This reflects the intuition that in the small- $\beta$ regime, more deliberate exploration is required to discover behaviors or capabilities not already covered by $\pi_{\text {ref }}$.

Remark 2.1. Various empirical works have suggested that offline DPO can under-perform relative to vanilla RLHF with PPO due to a lack of on-policy sampling (Xiong et al., 2023; Guo et al., 2024; Dong et al., 2024; Tang et al., 2024). Proposition 2.1 highlights a conceptually distinct phenomenon, where both of the aforementioned algorithms (as well as online variants of DPO) fail due to poor coverage from $\pi_{\text {ref }}$, in spite of on-policy sampling.[^1]

## 3 Exploratory Preference Optimization

We now present our main algorithm XPO, which addresses the limitations of existing alignment methods by augmenting DPO with active exploration. We first describe the algorithm and motivation (Section 3.1), then present theoretical guarantees (Section 3.2), and sketch the analysis (Section 3.3).

### 3.1 The XPO Algorithm

```
Algorithm 1 Exploratory Preference Optimization (XPO)
    input: Number of iterations $T$, KL-regularization coefficient $\beta>0$, optimism coefficient $\alpha>0$.
    Initialize $\pi^{(1)} \leftarrow \pi_{\text {ref }}, \mathcal{D}_{\text {pref }}^{(0)} \leftarrow \varnothing$.
    for iteration $t=1,2, \ldots, T$ do
            Generate response pair $\left(\tau^{(t)}, \widetilde{\tau}^{(t)}\right)$ via: $s_{1}^{(t)} \sim \rho, \tau^{(t)} \sim \pi^{(t)} \mid s_{1}^{(t)}$, and $\widetilde{\tau}^{(t)} \sim \pi_{\text {ref }} \mid s_{1}^{(t)}$.
            Label with preference: Label $\left(\tau^{(t)}, \widetilde{\tau}^{(t)}\right)$ as $\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)$ with preference $y^{(t)} \sim \mathbb{P}\left(\tau^{(t)} \succ \widetilde{\tau}^{(t)}\right)$.
            Update preference data: $\mathcal{D}_{\text {pref }}^{(t)} \leftarrow \mathcal{D}_{\text {pref }}^{(t-1)} \bigcup\left\{\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)\right\}$.
            Direct preference optimization with global optimism: Calculate $\pi^{(t+1)}$ via
```

$\pi^{(t+1)} \leftarrow \underset{\pi \in \Pi}{\operatorname{argmin}}\left\{\alpha \sum_{i=1}^{t} \log \pi\left(\widetilde{\tau}^{(i)}\right)-\sum_{\left(\tau_{+}, \tau_{-}\right) \in \mathcal{D}_{\text {pref }}^{(t)}} \log \left[\sigma\left(\beta \log \frac{\pi\left(\tau_{+}\right)}{\pi_{\mathrm{ref}}\left(\tau_{+}\right)}-\beta \log \frac{\pi\left(\tau_{-}\right)}{\pi_{\mathrm{ref}}\left(\tau_{-}\right)}\right)\right]\right\}$.

7: return: $\widehat{\pi}=\operatorname{argmax}_{\pi \in\left\{\pi^{(1)}, \ldots, \pi^{(T+1)}\right\}} J_{\beta}\left(\pi^{(t)}\right) . \quad$ // Can compute using validation data.

XPO (Exploratory Preference Optimization) is displayed in Algorithm 1. The algorithm takes as input a user-specified policy class $\Pi$ and proceeds in almost the same fashion as Online DPO. For each step $t \in[T]$, given the current policy $\pi^{(t)}$ and an initial state $s_{1}^{(t)}$, the algorithm begins by sampling a pair of trajectories $\tau^{(t)} \sim \pi^{(t)} \mid s_{1}^{(t)}$ and $\widetilde{\tau}^{(t)} \sim \pi_{\text {ref }} \mid s_{1}^{(t)}$, which are labeled as $\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)$ based on the preference feedback and used to update the preference dataset via $\mathcal{D}_{\text {pref }}^{(t+1)} \leftarrow \mathcal{D}_{\text {pref }}^{(t)} \cup\left\{\left(\tau_{+}^{(t)}, \tau_{-}^{(t)}\right)\right\}$. The most important step is Line 6 , which updates the policy to $\pi^{(t+1)}$ via the following optimistic variant of the DPO objective:

$$
\begin{equation*}
\pi^{(t+1)} \leftarrow \underset{\pi \in \Pi}{\operatorname{argmin}}\left\{\alpha \sum_{i=1}^{t} \log \pi\left(\widetilde{\tau}^{(i)}\right)-\sum_{\left(\tau_{+}, \tau_{-}\right) \in \mathcal{D}_{\text {pref }}^{(t)}} \log \left[\sigma\left(\beta \log \frac{\pi\left(\tau_{+}\right)}{\pi_{\mathrm{ref}}\left(\tau_{+}\right)}-\beta \log \frac{\pi\left(\tau_{-}\right)}{\pi_{\mathrm{ref}}\left(\tau_{-}\right)}\right)\right]\right\} \tag{6}
\end{equation*}
$$

Here, $\alpha \geq 0$ is an optimism parameter; for $\alpha=0$, the algorithm nearly equivalent to Online DPO, except that we sample $\tau^{(t)} \sim \pi^{(t)} \mid s_{1}^{(t)}$ and $\widetilde{\tau}^{(t)} \sim \pi_{\text {ref }} \mid s_{1}^{(t)}$ instead of sampling $\left(\tau^{(t)}, \widetilde{\tau}^{(t)}\right) \sim \pi^{(t)} \mid s_{1}^{(t)}$ at each iteration. As we will see now, for $\alpha>0$, the term

$$
\begin{equation*}
\alpha \sum_{i=1}^{t} \log \pi\left(\widetilde{\tau}^{(i)}\right) \tag{7}
\end{equation*}
$$

in Eq. (6) encourages the policy to behave optimistically, and produce diverse responses $\tau$.

Motivation. Optimism in the face of uncertainty is a widely used technique in reinforcement learning theory (Agarwal et al., 2019; Lattimore and Szepesvári, 2020; Foster and Rakhlin, 2023). In its most standard form, the optimism principle is usually stated as follows: One should explore by choosing their actions according to the most optimistic view of the world, given all of the data that has already been observed. The idea is that if we choose a decision according to this principle, one of two good things can happen: (i) the optimistic view is correct, and we receive large reward; or (ii) the optimistic view is incorrect, but we receive useful information that will help to better estimate the state of the world in subsequent iterations.

Optimism is typically implemented by directly estimating rewards, and it is not obvious at first glance why Eq. (7) can even be interpreted as a form of optimism. To understand, this consider a log-linear policy $\pi_{f}\left(a_{h} \mid s_{h}\right)=\pi_{\text {ref }}\left(a_{h} \mid s_{h}\right) \exp \left(\frac{f\left(s_{h}, a_{h}\right)-V_{f}\left(s_{h}\right)}{\beta}\right)$, where $V_{f}\left(s_{h}\right):=\beta \log \sum_{a_{h} \in \mathcal{A}} \pi_{\text {ref }}\left(a_{h} \mid s_{h}\right) e^{f\left(s_{h}, a_{h}\right) / \beta}$. Define $\left[\mathcal{T}_{\beta} f\right]\left(s_{h}, a_{h}\right):=r\left(s_{h}, a_{h}\right)+\mathbb{E}\left[V_{f}\left(s_{h+1}\right) \mid s_{h}, a_{h}\right]$ as the KL-regularized Bellman operator (Ziebart et al., 2008;

Ziebart, 2010). We observe, generalizing Rafailov et al. (2024), that for any DCMDP, for all trajectories $\tau=\left(s_{1}, a_{1}\right), \ldots,\left(s_{H}, a_{H}\right)$

$$
\begin{equation*}
\beta \log \frac{\pi_{f}(\tau)}{\pi_{\text {ref }}(\tau)}=r(\tau)-V_{f}\left(s_{1}\right)+\sum_{h=1}^{H}\left(f\left(s_{h}, a_{h}\right)-\left[\mathcal{T}_{\beta} f\right]\left(s_{h}, a_{h}\right)\right) \tag{8}
\end{equation*}
$$

That is, the policy can be viewed as maintaining an internal model for the trajectory reward, up to (i) a constant offset $V_{f}\left(s_{1}\right)$ that depends only on $s_{1}$; and (ii) the sum of Bellman errors $\left(f\left(s_{h}, a_{h}\right)-\left[\mathcal{T}_{\beta} f\right]\left(s_{h}, a_{h}\right)\right)$. The optimal KL-regularized policy $\pi_{\beta}^{\star}=\operatorname{argmax}_{\pi} J_{\beta}(\pi)$ satisfies $\pi_{\beta}^{\star}=\pi_{Q_{\beta}^{\star}}$, where $Q_{\beta}^{\star} / V_{\beta}^{\star}$ denote KLregularized value functions (see Appendix C. 4 for formal definitions and details), and has zero Bellman error $\left(Q_{\beta}^{\star}=\left[\mathcal{T}_{\beta} Q_{\beta}^{\star}\right]\right)$, so that

$$
\begin{equation*}
\beta \log \frac{\pi_{\beta}^{\star}(\tau)}{\pi_{\mathrm{ref}}(\tau)}=r(\tau)-V_{\beta}^{\star}\left(s_{1}\right) \quad \forall \tau \tag{9}
\end{equation*}
$$

In other words, $\pi_{\beta}^{\star}$ implements an accurate internal reward model. From this viewpoint:

1. The standard DPO term in Eq. (6) encourages the policy $\pi$ to build an accurate internal model for rewards under the Bradley-Terry model; this can be viewed as a form of implicit $Q^{\star}$-approximation, since we are implicitly minimizing the Bellman errors in Eq. (8).
2. In light of Eq. (9) it is natural to approximate $V_{\beta}^{\pi}\left(s_{1}\right)$, the regularized value function for $\pi$, by $r(\tau)-\beta \log \frac{\pi(\tau)}{\pi_{\text {ref }}(\tau)}$. Using this approximation, the first term in Eq. (6) biases the policy toward a large value function such that $V_{\beta}^{\star} \lesssim V_{\beta}^{\pi}$, implementing implicit (global) optimism in the face of uncertainty (up to an inconsequential difference in on-policy rewards). The fact that this suffices to drive exploration is quite subtle, and leverages non-trivial properties of the KL-regularized MDP, including the fact that Eq. (8) holds on a per-trajectory basis.

On the sampling policy. As remarked above, another difference between XPO and online/iterative DPO is that instead of sampling the preference pairs via $\left(\tau^{(t)}, \widetilde{\tau}^{(t)}\right) \sim \pi^{(t)}$, we sample $\tau^{(t)} \sim \pi^{(t)} \mid s_{1}^{(t)}$ and $\widetilde{\tau}^{(t)} \sim \pi_{\text {ref }} \mid s_{1}^{(t)}$. This small change is important: it is possible to show that in general, sampling $\left(\tau^{(t)}, \widetilde{\tau}^{(t)}\right) \sim \pi^{(t)}$ can lead to degenerate behavior in which the algorithm fails to adequately explore in the small- $\beta$ regime, even when $\pi_{\text {ref }}$ itself has good coverage.

While we use $\widetilde{\tau}^{(t)} \sim \pi_{\text {ref }} \mid s_{1}^{(t)}$ in Algorithm 1, XPO is significantly more general, and leads to provable guarantees for any fixed sampling policy $\widetilde{\tau}^{(t)} \sim \widetilde{\pi} \mid s_{1}^{(t)}$, as well as certain data-dependent sampling schemes (e.g., sampling $\widetilde{\tau}^{(t)} \sim$ unif $\left.\left(\pi^{(1)}, \ldots, \pi^{(t)}\right) \mid s_{1}^{(t)}\right)$; different choices may have different tradeoffs and benefits in practice. A general version of XPO which leaves the sampling distribution for $\widetilde{\tau}^{(t)}$ as a free parameter is given in Appendix C. 1 (Algorithm 2).

Practicality. XPO is highly practical, and can easily be incorporated into existing language modeling and RLHF pipelines as a drop-in replacement for Online DPO (a one-line change to existing code). The theoretical guarantees for the algorithm continue to hold under standard modifications such as (i) incorporating additional preference data from $\pi_{\text {ref }}$ or another reference policy; and (ii) performing a smaller number of iterations, but collecting a larger batch of preference data from $\pi^{(t)}$ (as in Iterative DPO).

### 3.2 Theoretical Guarantees

To provide sample complexity guarantees for XPO, we make some standard statistical assumptions. The first assumption asserts that the policy class $\Pi$ is powerful enough to represent the optimal KL-regularized policy.

Assumption 3.1 (Policy realizability). The policy class $\Pi$ satisfies $\pi_{\beta}^{\star} \in \Pi$.

Policy realizability is a minimal assumption for sample-efficient reinforcement learning (Agarwal et al., 2019; Lattimore and Szepesvári, 2020; Foster and Rakhlin, 2023); through Eq. (9), it is equivalent to a form of reward/value realizability. For language modeling, $\Pi$ will typically correspond to a class of language models with fixed architecture but variable weights. Next, we make a regularity assumption on the policies in $\Pi$ (Rosset et al., 2024).

Assumption 3.2 (Bounded density ratios). For all $\pi \in \Pi$ and trajectories $\tau=\left(s_{1}, a_{1}\right), \ldots,\left(s_{H}, a_{H}\right)$,

$$
\begin{equation*}
\left|\log \left(\frac{\pi(\tau)}{\pi_{\mathrm{ref}}(\tau)}\right)\right| \leq \frac{V_{\mathrm{max}}}{\beta} \tag{10}
\end{equation*}
$$

Note that $V_{\max }$ is measurable and controllable in practice; our guarantees scale polynomially with this parameter. For log-linear policies where $\pi(a \mid s) \propto \exp (f(s, a) / \beta)$, we expect $V_{\max } \lesssim R_{\max }$.

To quantify the rate at which the algorithm converges to an optimal policy, we require an exploration condition, which limits the amount of times the algorithm can be surprised by substantially new state distributions; such assumptions are necessary for reinforcement learning with general function approximation (Jiang et al., 2017; Jin et al., 2021; Xie et al., 2023). Our main result is stated in terms of a condition known as coverability (Xie et al., 2023), but more general guarantees are given in Appendix C. Define $d^{\pi}(\tau):=\mathbb{P}_{\pi}\left(\left(s_{1}, a_{1}\right), \ldots,\left(s_{H}, a_{H}\right)=\tau\right)$.

Definition 3.1 (Coverability). The trajectory-level coverability coefficient is given by

$$
\begin{equation*}
C_{\mathrm{cov}}(\Pi):=\inf _{\mu \in \Delta\left((\mathcal{S} \times \mathcal{A})^{H}\right)} \sup _{\tau \in(\mathcal{S} \times \mathcal{A})^{H}} \sup _{\pi \in \Pi} \frac{d^{\pi}(\tau)}{\mu(\tau)} \tag{11}
\end{equation*}
$$

Assumption 3.2 implies a trivial bound of $C_{\text {cov }}(\Pi) \lesssim \exp \left(\frac{V_{\max }}{\beta}\right)$. Indeed, $C_{\text {cov }}(\Pi)$ measures coverage with respect to the best possible distribution $\mu$, while the bound implied by Assumption 3.2 takes $\mu=\pi_{\text {ref }}$, so we expect $C_{\text {cov }}(\Pi) \ll \exp \left(V_{\max } / \beta\right)$ when $\pi_{\text {ref }}$ does not provide adequate coverage on its own (e.g., the example in Proposition 2.1). This is precisely the setting where we expect deliberate exploration to be helpful. We also note that there is a trivial bound $C_{\operatorname{cov}}(\Pi) \leq|\mathcal{A}|^{H}$, but because coverability depends on the structure of the (restricted) class $\Pi$, the value can be significantly smaller in general (e.g., if policies $\pi \in \Pi$ are highly correlated or stochastic).

The main sample complexity guarantee for XPO is as follows.

Theorem 3.1 (Sample complexity bound for XPO). Suppose that Assumptions 3.1 and 3.2 hold. For any $\beta>0$ and $T \in \mathbb{N}$, if we set $\alpha=c \cdot \frac{\beta}{\left(V_{\max }+R_{\max }\right) e^{2 R_{\max }}} \cdot \sqrt{\frac{\log \left(|\Pi| T \delta^{-1}\right)}{T \cdot C_{\mathrm{cov}}(\Pi)}}$ for an absolute constant $c>0$, then Algorithm 1 ensures that with probability at least $1-\delta,{ }^{3}$

$$
J_{\beta}\left(\pi_{\beta}^{\star}\right)-J_{\beta}(\widehat{\pi}) \lesssim\left(V_{\max }+R_{\max }\right) e^{2 R_{\max }} \cdot \sqrt{\frac{C_{\operatorname{cov}}(\Pi) \log \left(|\Pi| T \delta^{-1}\right) \log ^{2}(T)}{T}}
$$

Let us discuss some key features of this result.

Statistical efficiency. Theorem 3.1 shows that XPO converges to a near-optimal policy with sample complexity polynomial in the coverability coefficient $C_{\mathrm{cov}}(\Pi)$; in particular, to learn an $\varepsilon$-optimal policy $T=\widetilde{O}\left(\frac{C_{\operatorname{cov}}(\Pi) \log |\Pi|}{\varepsilon^{2}}\right)$ episodes are required. ${ }^{4}$ By scaling with $C_{\operatorname{cov}}(\Pi)$, Theorem 3.1 can be viewed as a strict improvement over offline RLHF (Zhu et al., 2023; Zhan et al., 2023a), as well as prior works on online RLHF that rely on passive exploration (Xiong et al., 2023; Gao et al., 2024; Chang et al., 2024). In particular, these works scale with coverage parameters for $\pi_{\text {ref }}$, the simplest of which take the form $C_{\text {conc }}(\Pi):=\sup _{\tau \in(\mathcal{S} \times \mathcal{A})^{H}} \sup _{\pi \in \Pi} \frac{\pi(\tau)}{\pi_{\text {ref }}(\tau)}$. Under Assumption 3.2, we have that $C_{\text {conc }}(\Pi)=\exp \left(V_{\max } / \beta\right)$ which, as discussed above, upper bounds $C_{\text {cov }}(\Pi)$ but can be much larger when $\pi_{\text {ref }}$ has poor coverage. The dependence on $C_{\text {cov }}(\Pi)$ in Theorem 3.1 reflects the fact that XPO can explore responses not covered by $\pi_{\text {ref }}$. ${ }^{5}$[^2]

In Appendix C, we give a generalization of Theorem 3.1 (Theorem 3.1') which scales with a more comprehensive exploration parameter, the Sequential Extrapolation Coefficient (SEC), matching (for DCMDPs) the most general results in prior work on exploration in RLHF, but with a significantly simpler algorithm (Chen et al., 2022; Wang et al., 2023; Ye et al., 2024). The SEC also leads to polynomial sample complexity for tabular and linear MDPs, a common setting considered in prior work (Xu et al., 2020; Novoseller et al., 2020; Pacchiano et al., 2021; Wu and Sun, 2023; Zhan et al., 2023b; Das et al., 2024). See Appendix A for a detailed comparison. We emphasize that Theorem 3.1 applies to any DCMDP (including but not limited to the token-level MDP), even if the dynamics are unknown; as such, the result meaningfully extends beyond the contextual bandit formulation of RLHF found in many prior works (Zhu et al., 2023; Xiong et al., 2023; Das et al., 2024; Ye et al., 2024).

Remark 3.1 (Nontriviality and role of $\beta$ ). By avoiding explicit dependence on $\exp \left(\frac{1}{\beta}\right)$, XPO provably improves upon Online DPO when $\beta$ is small; per Proposition 2.1, the latter must pay $\exp \left(\frac{1}{\beta}\right)$ even when $C_{\text {cov }}(\Pi) \leq 2$. This improvement stems from the fact that KL-regularization does not automatically lead to exploration or grant meaningful control of coverability in the small- $\beta$ regime.

To highlight the importance of the small- $\beta$ regime, we note that by taking $\beta=\operatorname{poly}(1 / T)$, Theorem 3.1 immediately leads to bounds on the unregularized reward $J(\pi)$. This would not be possible if the sample complexity guarantee explicitly scaled with $\exp \left(\frac{1}{\beta}\right)$.

Computational efficiency. Most prior approaches to RL with general function approximation that incorporate global forms of optimism similar to Eq. (7) (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021; Jin et al., 2021; Xie et al., 2023; Liu et al., 2024) are known to be computationally intractable to implement in general (Dann et al., 2018), and involve solving non-convex, non-differentiable constrained optimization problems. Thus, it is natural to ask why our result is not too good to be true. The answer is that even though the objective in Eq. (6) is simple, it is still non-convex in general, even if one employs log-linear policies of the form

$$
\begin{equation*}
\pi_{\theta}(a \mid s) \propto \exp \left(\frac{1}{\beta}\langle\phi(s, a), \theta\rangle\right) \tag{12}
\end{equation*}
$$

for $\theta \in \mathbb{R}^{d}$. This non-convexity is precisely caused by the presence of the optimistic term Eq. (7); Theorem 3.1 is valid for all choices of $\beta>0$, but we expect that the optimization problem in Eq. (6) will become more difficult to solve as $\beta \rightarrow 0 .{ }^{6}$ In light of this, our work can be viewed as using the unique structure of the KL-regularized MDP formulation and deterministic contextual MDP (DCMDP) to derive an optimistic exploration objective which-while still non-convex-is differentiable and directly amenable to a practical implementation with language models. ${ }^{7}$ This technique is novel even in the context of reward-driven (as opposed to preference-based) RL, and we expect it to find broader use.

Additional remarks. Separately, we mention in passing that we believe it should be possible to derive tighter sample complexity bounds for large $\beta>0$, in the vein of Tiapkin et al. (2023a).

Remark 3.2 (Limitations of the DPO objective). Our results are limited to MDPs with deterministic dynamics and stochastic start state (DCMDPs). We believe that without further modifications, the DPO objective is not suitable for stochastic dynamics, as Eq. (9) no longer holds on a per-trajectory basis.

Remark 3.3 (Trajectory coverability). A related point concerns trajectory coverability. In the standard (as opposed to preference-based) RL setting, it is possible to achieve guarantees that scale with state-action coverability (Xie et al., 2023), defined via:

$$
C_{\mathrm{st}}(\Pi):=\inf _{\mu \in \Delta(\mathcal{S} \times \mathcal{A})} \sup _{s \in \mathcal{S}, a \in \mathcal{A}} \sup _{\pi \in \Pi} \frac{d^{\pi}(s, a)}{\mu(s, a)}
$$[^3]where $d^{\pi}(s, a):=\mathbb{P}_{\pi}\left(s_{h}=s, a_{h}=a\right)$. In general, we can have $C_{\mathrm{st}}(\Pi) \ll C_{\mathrm{cov}}(\Pi)$. We expect that trajectorylevel coverability is necessary for algorithms based on the DPO objective. Nonetheless, the difference is immaterial for language modeling in the token-level $M D P$, which has $C_{\mathrm{st}}(\Pi)=C_{\mathrm{cov}}(\Pi)$.

### 3.3 Proof Sketch for Theorem 3.1

Our starting point for the proof of Theorem 3.1 is the following regret decomposition, which is proven as a consequence of the implicit $Q^{\star}$-approximation result in Eq. (9).

Lemma 3.1 (Central regret decomposition). For any pair of policies $\pi$ and $\nu$, it holds that

$$
\begin{align*}
J_{\beta}\left(\pi_{\beta}^{\star}\right)-J_{\beta}(\pi)= & \mathbb{E}_{\tau \sim \nu}[\beta \log \pi(\tau)]-\mathbb{E}_{\tau \sim \nu}\left[\beta \log \pi_{\beta}^{\star}(\tau)\right]  \tag{13}\\
& +\mathbb{E}_{\tau \sim \pi}\left[\beta \log \frac{\pi(\tau)}{\pi_{\mathrm{ref}}(\tau)}-r(\tau)\right]-\mathbb{E}_{\tau \sim \nu}\left[\beta \log \frac{\pi(\tau)}{\pi_{\mathrm{ref}}(\tau)}-r(\tau)\right] \tag{14}
\end{align*}
$$

This result decomposes the error of any policy into two pairs of terms: The first pair in Eq. (13) measures the extent to which the policy's internal reward model overestimates the optimal value, and directly informs the notion of optimism in XPO, while the second pair in Eq. (14) measures the reward model's predictive accuracy. Critically, as a consequence of the fact that Eq. (9) holds uniformly for all trajectories, the regret decomposition measures error under (i) the policy $\pi$ itself (on-policy error), and (ii) an arbitrary reference policy $\nu$, which we will instantiate as the historical data distribution.

Let $\boldsymbol{\mu}^{(t)}:=\frac{1}{t-1} \sum_{i<t} \pi^{(i)} \otimes \pi_{\text {ref }}$ denote the policy that, given $s_{1}$, samples $\tau \sim \pi^{(i)}$ for $i \sim$ unif $([t-1])$ and samples $\widetilde{\tau} \sim \pi_{\text {ref }}$, with the convention that $\boldsymbol{\mu}^{(1)}$ is arbitrary. Observe that $\min _{t \in[T+1]} J_{\beta}\left(\pi_{\beta}^{\star}\right)-J_{\beta}\left(\pi^{(t)}\right) \leq$ $\frac{1}{T} \sum_{t=1}^{T} J_{\beta}\left(\pi_{\beta}^{\star}\right)-J_{\beta}\left(\pi^{(t)}\right)$. For each step $t$, applying Lemma 3.1 with $\pi=\pi^{(t)}$ and $\nu=\pi_{\text {ref }}$ gives

$$
\begin{align*}
& \frac{1}{T} \sum_{t=1}^{T} J_{\beta}\left(\pi_{\beta}^{\star}\right)-J_{\beta}\left(\pi^{(t)}\right) \leq \frac{1}{T} \sum_{t=1}^{T} \mathbb{E}_{\tau \sim \pi_{\mathrm{ref}}}\left[\beta \log \pi^{(t)}(\tau)-\beta \log \pi_{\beta}^{\star}(\tau)\right] \\
& \quad+\frac{1}{T} \sum_{t=1}^{T} \mathbb{E}_{s_{1} \sim \rho, \tau \sim \pi^{(t)}\left|s_{1}, \widetilde{\tau} \sim \pi_{\mathrm{ref}}\right| s_{1}}\left[\beta \log \frac{\pi^{(t)}(\tau)}{\pi_{\mathrm{ref}}(\tau)}-r(\tau)-\beta \log \frac{\pi^{(t)}(\widetilde{\tau})}{\pi_{\mathrm{ref}}(\widetilde{\tau})}+r(\widetilde{\tau})\right] \tag{15}
\end{align*}
$$

The reward estimation error term in Eq. (15) samples $\tau \sim \pi^{(t)} \mid s_{1}$ and $\widetilde{\tau} \sim \pi_{\text {ref }} \sim s_{1}$ (on-policy). To relate this to the purely off-policy objective in Line 6 of XPO, we use a potential argument based on coverability (Xie et al., 2023) which, for any $\alpha>0$, allows us to bound the above expression by

$$
\begin{align*}
\lesssim & \frac{\alpha}{\beta} \cdot C_{\mathrm{cov}}(\Pi)+\frac{1}{T} \sum_{t=1}^{T} \mathbb{E}_{\tau \sim \pi_{\mathrm{ref}}}\left[\beta \log \pi^{(t)}(\tau)-\beta \log \pi_{\beta}^{\star}(\tau)\right] \\
& +\frac{\alpha^{-1} \beta}{T} \sum_{t=1}^{T} \mathbb{E}_{s_{1} \sim \rho,(\tau, \widetilde{\tau}) \sim \mu^{(t)} \mid s_{1}}\left[\left(\beta \log \frac{\pi^{(t)}(\tau)}{\pi_{\mathrm{ref}}(\tau)}-r(\tau)-\beta \log \frac{\pi^{(t)}(\widetilde{\tau})}{\pi_{\mathrm{ref}}(\widetilde{\tau})}+r(\widetilde{\tau})\right)^{2}\right] \tag{16}
\end{align*}
$$

Let

$$
\begin{aligned}
\Psi_{\mathrm{XPO}}^{(t)}(\pi)= & \mathbb{E}_{\tau \sim \pi_{\mathrm{ref}}}\left[\beta \log \pi(\tau)-\beta \log \pi_{\beta}^{\star}(\tau)\right] \\
& +\alpha^{-1} \beta \mathbb{E}_{s_{1} \sim \rho,(\tau, \widetilde{\tau}) \sim \boldsymbol{\mu}^{(t)} \mid s_{1}}\left[\left(\beta \log \frac{\pi(\tau)}{\pi_{\mathrm{ref}}(\tau)}-r(\tau)-\beta \log \frac{\pi(\widetilde{\tau})}{\pi_{\mathrm{ref}}(\widetilde{\tau})}+r(\widetilde{\tau})\right)^{2}\right]
\end{aligned}
$$

If we could choose $\pi^{(t)}=\operatorname{argmin}_{\pi \in \Pi} \Psi_{\mathrm{XPO}}^{(t)}(\pi)$, we would be done, since by Eq. (9) this would yield

$$
\Psi_{\mathrm{XPO}}^{(t)}\left(\pi^{(t)}\right) \leq \Psi_{\mathrm{XPO}}^{(t)}\left(\pi_{\beta}^{\star}\right)=\mathbb{E}_{s_{1} \sim \rho,(\tau, \widetilde{\tau}) \sim \boldsymbol{\mu}^{(t)} \mid s_{1}}\left[\left(\beta \log \frac{\pi_{\beta}^{\star}(\tau)}{\pi_{\mathrm{ref}}(\tau)}-r(\tau)-\beta \log \frac{\pi_{\beta}^{\star}(\widetilde{\tau})}{\pi_{\mathrm{ref}}(\widetilde{\tau})}+r(\widetilde{\tau})\right)^{2}\right]=0
$$

The XPO objective in Line 6 minimizes an empirical analogue of this quantity (up to a standard translation between log-loss and square loss under the Bradley-Terry model), so a concentration argument (Lemma C.5)
allows us to conclude that the iterates of XPO satisfy $\Psi_{\mathrm{XPO}}^{(t)}\left(\pi^{(t)}\right) \lesssim \alpha^{-1} \frac{\log |\Pi|}{t}+\sqrt{\frac{\log |\Pi|}{t}}$ with high probability. Plugging this bound into Eq. (16) yields

$$
\frac{1}{T} \sum_{t=1}^{T} J_{\beta}\left(\pi_{\beta}^{\star}\right)-J_{\beta}\left(\pi^{(t)}\right) \lesssim \sqrt{\frac{C_{\operatorname{cov}}(\Pi) \log |\Pi|}{T}}
$$

after tuning $\alpha$.

### 3.4 Empirical Validation

To close this section, we provide a preliminary empirical evaluation of XPO in real-world RLHF experiments. To implement XPO, we use the iterative DPO (Xu et al., 2023; Tran et al., 2024; Dong et al., 2024) pipeline from Dong et al. (2024) with 3 total iterations (that is, we set $T=3$, but draw a large batch of pairs from $\pi^{(t)}$ ), and augment the DPO objective with the optimism term in XPO. We use the same base model (which we refer to as Llama-3-8B-Flow-SFT), ${ }^{8}$ prompt sets for each iteration, ${ }^{9}$ and preference model (to generate preference feedback), ${ }^{10}$ as Dong et al. (2024), which makes our results generally comparable to theirs. Over all three iterations, we fix $\pi_{\text {ref }}$ to be the base model, Llama-3-8B-Flow-SFT.

| Model | AGIEval | ANLI | MMLU | GPQA | GSM8K | AlpacaEval-2 LC | Arena-Hard |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Llama-3-8B-Flow-SFT | 43.71 | 40.16 | 62.48 | 30.37 | 73.46 | 9.08 | 9.4 |
| DPO-iter1 | 46.38 | 46 | 63.01 | 29.99 | $\mathbf{7 7 . 7 9}$ | 23.27 | 19.2 |
| DPO-iter2 | 47.47 | 48.13 | 63.45 | 29.32 | 72.1 | 23.68 | 20.1 |
| DPO-iter3 | 46.92 | 47.72 | 63.55 | 28.73 | 72.93 | 27.33 | 24.9 |
| XPO-iter1 | 46.73 | 45.47 | 63.1 | $\mathbf{3 0 . 4 1}$ | 76.57 | 22.14 | 18.7 |
| XPO-iter2 | 47.44 | $\mathbf{4 8 . 4 4}$ | 63.35 | 29.95 | 75.51 | 24.65 | 23.2 |
| XPO-iter3 | $\mathbf{4 7 . 4 8}$ | 48.09 | $\mathbf{6 3 . 7 2}$ | 29.49 | 75.97 | $\mathbf{2 9 . 3 5}$ | $\mathbf{2 7 . 2}$ |
| Llama-3-8B-Flow-Final | 47.29 | 46.12 | 63.37 | 29.45 | $\underline{78.62}$ | $\underline{31.7}$ | 23.1 |
| Llama-3-8B-it | 44.78 | 46.47 | $\underline{63.79}$ | $\underline{30.83}$ | 75.59 | $\underline{32.58}$ | 20.7 |

Table 1: Benchmarks for XPO and baseline models. Bold indicates best performance with the same data usage. Underlined results are superior to XPO, but from a model requiring more data or industry-level.

In Table 1, we compare XPO with the following baselines: 1) iterative DPO with the same setup (i.e., XPO with $\alpha=0$ ), 2) Llama-3-8B-Flow-Final, the final model from Dong et al. (2024), and 3) the industry-level instruction-tuned model Llama-3-8B-it, ${ }^{11}$ on various academic and chat benchmarks (Zhong et al., 2023; Nie et al., 2020; Hendrycks et al., 2021; Rein et al., 2023; Cobbe et al., 2021; Dubois et al., 2024; Li et al., 2024; Clark et al., 2018; Lin et al., 2022; Zellers et al., 2019; Sakaguchi et al., 2021). We compute all baseline numbers ourselves with the same configuration for a fair comparison. A key distinction between our experimental setup and that of Dong et al. (2024) is that we construct preference pairs from only two responses, whereas Dong et al. (2024) use best/worst-over-8-responses for preference pair construction as a heuristic exploration strategy. In other words, the final models we obtain (XPO-iter3, and a baseline, DPO-iter3 in Table 1) use only $1 / 4$ the number of generated responses compared the final model (Llama-3-8B-Flow-Final) ${ }^{12}$ from Dong et al. (2024).

We find that the model obtained by XPO improves over the non-exploratory baseline (DPO-iter) on the chat benchmarks (which offer roughly $\sim 90 \%$ agreement and/or Spearman correlation to Chatbot Arena (Chiang et al., 2024)), and attains performance comparable to the industry-level (Llama-3-8B-it) or $4 \times$ data-usage (Llama-3-8B-Flow-Final) models. At the same time, XPO also improves over the non-exploratory baseline on most of the academic benchmarks, again achieving comparable performance with the industry-level and $4 \times$ data-usage models, and does not introduce significant performance regression in any benchmark. In contrast, we observe that the iterative DPO baseline (without exploration) causes obvious regression in the

![](https://cdn.mathpix.com/cropped/2024_06_04_2809123c6fe2ae7dac8cg-11.jpg?height=35&width=518&top_left_y=2319&top_left_x=283)

$9_{\text {https://huggingface.co/datasets/RLHFlow/iterative-prompt-v1-iter1-20K, https://huggingface.co/datasets/RLHFlow/iterative-prompt-v }}$ 1-iter2-20K, https://huggingface.co/datasets/RLHFlow/iterative-prompt-v1-iter3-20K.

10 https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B.

11 https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct

12 https://huggingface.co/RLHFlow/LLaMA3-iterative-DPO-final
}
math (GSM8K) benchmark. However, we caution that conducting separate training runs with different random seeds can yield results with relatively high variance (e.g., the difference in win rates can be up to $3 \% \sim 4 \%$ ) for both chat benchmarks; due to resource limitations, we defer a more comprehensive evaluation to future work. See Appendix E for additional results.

## 4 Discussion

Our work provides the first practical and provably sample-efficient online exploration algorithm for RLHF with general function approximation, a step toward fully realizing the potential of online exploration for aligning language models. Our results also show that viewing DPO as a form of implicit $Q^{\star}$-approximation can directly inform new algorithmic interventions (e.g., implicit optimism), and offer an example of fruitful interplay between language modeling and theoretical reinforcement learning. Building on this viewpoint, an exciting direction for future work is to import the broader set of tools from the literature on reinforcement learning theory (e.g., more powerful exploration principles (Foster et al., 2021)) and harness them for language modeling and alignment; in this context, we expect our analysis techniques based on the KL-regularized MDP to find broader use.

From a reinforcement learning perspective, interesting technical directions for future work include (i) providing instance-dependent sample complexity bounds for XPO; and (ii) supporting RL settings beyond deterministic contextual MDPs. On the practical side, immediate followup directions include extending XPO to support general preference models (Munos et al., 2023; Swamy et al., 2024) or more general feedback modalities (Ethayarajh et al., 2024).

## References

Alekh Agarwal, Nan Jiang, and Sham M Kakade. Reinforcement learning: Theory and algorithms. Preprint, 2019 .

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, and Bo Dai. Value-incentivized preference optimization: A unified approach to online and offline rlhf, 2024.

Jonathan D Chang, Wenhao Shan, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D Lee, and Wen Sun. Dataset reset policy optimization for rlhf. arXiv preprint arXiv:2404.08495, 2024.

Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In International Conference on Machine Learning, pages 3773-3793. PMLR, 2022.

Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient PAC RL with rich observations. In Advances in neural information processing systems, pages 1422-1432, 2018.

Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Provably sample efficient rlhf via active preference optimization. arXiv preprint arXiv:2402.10500, 2024.

Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024.

Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in RL. International Conference on Machine Learning, 2021.

Yihan Du, Anna Winnicki, Gal Dalal, Shie Mannor, and R Srikant. Exploration-driven policy optimization in RLHF: Theoretical insights on efficient data utilization. arXiv preprint arXiv:2402.10342, 2024.

Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled AlpacaEval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.

Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, and Benjamin Van Roy. Efficient exploration for llms. arXiv preprint arXiv:2402.00396, 2024.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

Dylan J Foster and Alexander Rakhlin. Foundations of reinforcement learning and interactive decision making. arXiv preprint arXiv:2312.16730, 2023.

Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.

Dylan J Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making with the decision-estimation coefficient. In The Thirty Sixth Annual Conference on Learning Theory, pages 3969-4043. PMLR, 2023.

Zhaolin Gao, Jonathan D Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J Andrew Bagnell, Jason D Lee, and Wen Sun. REBEL: Reinforcement learning via regressing relative rewards. arXiv preprint arXiv:2404.16767, 2024.

Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. Exploration is harder than prediction: Cryptographically separating reinforcement learning from supervised learning. arXiv preprint arXiv:2404.03774, 2024.

Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online AI feedback. arXiv preprint arXiv:2402.04792, 2024.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.

Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, pages 1704-1713, 2017.

Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137-2143, 2020.

Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. Neural Information Processing Systems, 2021.

Daniel Kane, Sihan Liu, Shachar Lovett, and Gaurav Mahajan. Computational-statistical gap in reinforcement learning. In Conference on Learning Theory, pages 1282-1302. PMLR, 2022.

Tadashi Kozuno, Wenhao Yang, Nino Vieillard, Toshinori Kitamura, Yunhao Tang, Jincheng Mei, Pierre Ménard, Mohammad Gheshlaghi Azar, Michal Valko, Rémi Munos, et al. KL-entropy-regularized RL with a generative model is minimax optimal. arXiv preprint arXiv:2205.14211, 2022.

Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.

Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The Arena-Hard pipeline, April 2024. URL https://lmsys.or g/blog/2024-04-19-arena-hard/.

Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, 2022.

Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang. Maximize to explore: One objective function fusing estimation, planning, and exploration. Advances in Neural Information Processing Systems, 36, 2024.

Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-Math: Unlocking the potential of SLMs in grade school math. arXiv preprint arXiv:2402.14830, 2024.

Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.

Gergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized Markov decision processes. arXiv preprint arXiv:1705.07798, 2017.

Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, 2020.

Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In Conference on Uncertainty in Artificial Intelligence, pages 1029-1038. PMLR, 2020.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling RL: reinforcement learning with trajectory preferences. arXiv preprint arXiv:2111.04850, 2021.

Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2023.

Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $Q^{\star}$ : Your language model is secretly a Q-function. arXiv preprint arXiv:2404.12358, 2024.

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. GPQA: A graduate-level google-proof q\&a benchmark. arXiv preprint arXiv:2311.12022, 2023.

Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct Nash Optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024.

Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In Conference on learning theory, pages 2898-2933. PMLR, 2019.

Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.

Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos, Bernardo Ávila Pires, Michal Valko, Yong Cheng, and Will Dabney. Understanding the performance gap between online and offline alignment algorithms, 2024.

Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Pierre Perrault, Yunhao Tang, Michal Valko, and Pierre Menard. Fast rates for maximum entropy exploration. In International Conference on Machine Learning, pages 34161-34221. PMLR, 2023a.

Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Alexey Naumov, Pierre Perrault, Michal Valko, and Pierre Menard. Regularized RL. arXiv preprint arXiv:2310.17303, 2023b.

Hoang Tran, Chris Glaze, and Braden Hancock. snorkelai/snorkel-mistral-pairrm-dpo, 2024. https: //huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO.

Sara A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.

Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information Processing Systems, 33, 2020.

Yuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? arXiv preprint arXiv:2306.14111, 2023.

Runzhe Wu and Wen Sun. Making RL with preference-based feedback efficient via randomization. arXiv preprint arXiv:2310.14554, 2023.

Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A theoretical comparison. In Conference on Uncertainty in Artificial Intelligence, pages 550-559. PMLR, 2020.

Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. The role of coverage in online reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023.

Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: A provable KL-constrained framework for RLHF. arXiv preprint arXiv:2312.11456, 2023.

Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023.

Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. Advances in Neural Information Processing Systems, 33:18784-18794, 2020 .

Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, and Tong Zhang. A theoretical analysis of Nash learning from human feedback under general KL-regularized preference. arXiv preprint arXiv:2402.07314, 2024.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, 2019.

Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline preference-based reinforcement learning. In The Twelfth International Conference on Learning Representations, 2023a.

Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. Provable reward-agnostic preference-based reinforcement learning. arXiv preprint arXiv:2305.18505, 2023b.

Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, Hany Hassan, and Zhaoran Wang. Self-exploring language models: Active preference elicitation for online alignment, 2024.

Tong Zhang. From $\epsilon$-entropy to KL-entropy: Analysis of minimum information complexity density estimation. The Annals of Statistics, 34(5):2180-2210, 2006.

Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. GEC: A unified framework for interactive decision making in MDP, POMDP, and beyond. arXiv preprint arXiv:2211.01962, 2022.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.

Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In International Conference on Machine Learning, pages 43037-43067. PMLR, 2023.

Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.
