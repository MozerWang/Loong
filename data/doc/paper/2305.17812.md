# Tab-CoT: Zero-shot Tabular Chain of Thought 

Ziqi Jin and Wei Lu<br>StatNLP Research Group<br>Singapore University of Technology and Design<br>ziqi_jin@sutd.edu.sg, luwei@sutd.edu.sg


#### Abstract

The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit implicitly structured steps. Recent efforts also started investigating methods to encourage more explicitly structured reasoning procedures to be captured (Zhou et al., 2022). In this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex reasoning process to be explicitly modelled in a highly structured manner. Despite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns). We demonstrate our approach's strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks. ${ }^{1}$


## 1 Introduction

The chain-of-thought (CoT) prompting method (Wei et al., 2022) encourages the large language models (LLMs) to engage in a thought process before providing the answer to the given question. Such an approach shows impressive performance improvements in reasoning tasks. Notably, in the zero-shot setting, it was shown that a simple prompt such as "let's think step by step" could facilitate the step-by-step thinking process before answering the original question (Kojima et al., 2022). Such a task-agnostic method unveiled that LLMs can be descent zero-shot reasoners.

The reasoning process is inherently structured. This gives rise to some new developments along this line of work recently. Specifically, Zhou et al. (2022) suggests an alternative prompting approach that enables a two-stage structured reasoning process. Gao et al. (2022) proposes an approach that[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-01.jpg?height=848&width=768&top_left_y=730&top_left_x=1067)

Figure 1: A comparison between Tab-CoT with standard prompting and zero-shot-CoT on the same question. Chain-of-thought prompts are highlighted in orange.

involves code in the prompt design, allowing structured information in the form of formal language to participate in the reasoning process. While effective, such methods require specific prompt engineering for different domains or defining multiple variables, which can be difficult to maintain or keep track of.

Inspired by the fact that state-of-the-art large language models, such as GPT-3 (Brown et al., 2020) and CodeX (Chen et al., 2021), have the capability of reasoning over tabular structured data ${ }^{2}$, we propose a novel framework called Tabular Chain of Thought (Tab-CoT) that models the structured reasoning process using a table-filling procedure.

We show that the model can perform step-by-step reasoning by creating a table without further fine-tuning by using a table[^1]header with column names in the form of "|step|question|response|" as a prompt. While conventional natural language texts are generated in a 1-dimensional sequential order, the table has a 2-dimensional structure, allowing inference along both columns and rows to be performed simultaneously. Unlike previous works which focused on extracting information from existing tabular structured data (Gong et al., 2020), our approach generates the table while performing the reasoning process (and extracts the answer from the generated table at the end).

Figure 1 shows the results with standard prompting, conventional zero-shot CoT, and our zero-shot Tab-CoT. Our method generates a table as the output, which is more organized and concise than the output from the conventional CoT method. In this example, while zero-shot CoT generates 140 words, our method only generates 28 . Besides, we found our method can reason horizontally and vertically at the same time. ${ }^{3}$ This demonstrates that our TabCoT method benefits from the 2-dimensional structure of the table, where the information can flow in two dimensions.

We summarize our main contributions in this work as follows:
- We propose a new approach called Tabular Chain-of-Thought (Tab-CoT) that utilizes a tabular structured reasoning scheme in combination with state-of-the-art large language models to generate answers. To the best of our knowledge, this is the first method that uses tables in a "chain of thought" process.
- The 2-dimensional tabular structure of TabCoT allows for improved unlocking of the step-by-step reasoning capabilities of LLMs, transforming the linear "chain of thought" process into a more structured one.
- Extensive experiments have revealed that our Tab-CoT outperforms traditional CoT techniques in zero and few-shot settings. This indicates that Tab-CoT has strong potential as a superior alternative to current chain-ofthought prompting methods.[^2]

## 2 Related Work

Chain-of-thought prompting (Wei et al., 2022), a variation of few-shot prompting that adds step-bystep reasoning in those few-shot examples instead of just providing answers, has achieved significant improvements across multiple datasets. The LLMs can generate solutions following the solution format of prompts. Compared to traditional prompting, chain-of-thought prompting decomposes the task into smaller steps, which makes difficult tasks easier to solve.

The chain-of-thought prompting method is not necessarily purely natural language based. Program Aided Language Models (PAL) (Gao et al., 2022) provides few-shot samples that contain executable Python code. Such an approach enables the LLMs to interact with the Python shell, allowing the model to focus on learning how to do mathematical reasoning rather than numerical calculations.

These chain-of-thought methods provide the solution structure and pattern via few-shot samples, but can these be provided without these fewshot samples in the zero-shot setting? Zero-shot CoT (Kojima et al., 2022) is a zero-shot chain-ofthought prompting method. The prompt phrase "Let's think step by step" added after the question triggers the explicit reasoning process. However, compared to few-shot CoT (Wei et al., 2022), zero-shot CoT allows more flexibility in the structure of the reasoning process.

Recently, Zhou et al. (2022) proposed Leastto-Most prompting, which is a prompting strategy that reduces a complex problem into a list of sub-questions, and sequentially solves the subquestions. Each sub-question is solved with the answer to previously solved sub-questions. Compared to zero-shot CoT, this method has more restrictions on the structure of reasoning by decomposing and sequentially answering. Moreover, importing external tools (like calculator and python shell) can further aid the math computation within the arithmetic domain (Gao et al., 2022).

These works reveal the importance of promoting structures in the chain-of-thought process. However, the nature of the zero-shot prompting makes the injection of structures into the generation process challenging. This motivates us to devise a better mechanism to prompt the language models under the zero-shot setting - a new prompting scheme that allows highly structured outputs in the form of tables to be generated.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-03.jpg?height=577&width=1380&top_left_y=234&top_left_x=338)

Figure 2: Overview of our zero-shot Tab-CoT method, which contains two steps: (1) table generation and (2) answer extraction. Added prompts are highlighted in orange. Texts generated by the LLM are highlighted in green.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-03.jpg?height=386&width=357&top_left_y=915&top_left_x=381)

(A)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-03.jpg?height=382&width=334&top_left_y=917&top_left_x=707)

(B)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-03.jpg?height=371&width=357&top_left_y=928&top_left_x=992)

(C)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-03.jpg?height=365&width=354&top_left_y=931&top_left_x=1336)

(D)

Figure 3: Understanding how state-of-the-art LLM ("code-davinci-002") reason with tabular-structured data.

## 3 Tab-CoT

Similar to zero-shot CoT (Kojima et al., 2022), our method involves two prompts that can be used in large language models (LLMs), one for table generation and the other for answer extraction. The details are shown in Figure 2. While our method is primarily applied in zero-shot settings, it can also work in few-shot settings.

Tables in LLMs We found that in the official "parse unstructured data" demo provided by OpenAI ${ }^{4}$, a table header is provided as part of the prompt, which is as follows: "|Fruit|Color|Flavor|". With such a prompt, the underlying LLM can automatically generate a table. This suggests possible formatting for tables in such state-of-the-art LLMs. And " 1 " is the recognizable delimiter of tables in OpenAI models.

To validate this observation, we queried the LLM "code-davinci-002" (Chen et al., 2021) with the following question: " $a=2, \quad b=3$, what is $2 * \mathrm{a}+3 * \mathrm{~b}$ ?", and provided another table header: "|step|solution|"5. We found that it completes[^3]

a structured table as follows:

```
a=2, b=3, what is 2*a+3*b?
|step|solution|
|:--- |:--- |
|1|2*a+3*b|
|2|2*2+3*3|
|3|4+9|
|4|13|
```

This experiment essentially unveils how the tables are represented in such LLMs. The results also illustrate how the table can potentially be used for generating a reasoning process. Next, to validate this, we designed several simple experiments to understand how reasoning over such tabularstructured data is performed on such LLMs, as shown in Figure 3. Our first experiment (A) shows that such LLMs are able to perform potential vertical reasoning. However, if we replace ' $\mid$ ' with ',' (B), the LLM fails to capture the patterns in the data. This tells us that the correct formatting is crucial when reasoning with tables in such LLMs.

Next, we intentionally insert a mistake into the partial table and ask the model to continue the generation process (circled in C). Surprisingly, the LLM is able to generate the correct entries even though the mistake occurred in the same row. This further confirms the LLM's strong potential in per-
forming vertical reasoning with tabular-structured data.

Moreover, to prove both vertical and horizontal reasoning exists, we increase the difficulty by directly appending the first two elements from step 9 after step 6 (D). If only vertical reasoning existed, the value under " $v 4$ " would have been " 11 ". Instead, the value generated is " 13 ," confirming that the LLMs have the potential to perform a combination of horizontal and vertical reasoning simultaneously.

Table Generation Prompt To make use of the 2-dimensional structure of the table, we replace the natural language prompt with a table-generation prompt (e.g., "|step|question|response|"), which serves as the header of the table. This regulates the context of this table, forcing the LLMs to conduct step-by-step reasoning by completing the table. Meanwhile, the choice of columns can be very specific. If each row of the table is regarded as a step, the row-by-row table generation process will become a step-by-step reasoning process. Within each step (row), we have multiple columns, each of which contributes certain detail towards the current reasoning step.

For any text question $x$, we have a table generation prompt (all column names) c. Concretely, we add the table generation prompt in the next row of the text question:

$$
\operatorname{LLM}(x, c)=\left[\begin{array}{ccccc}
c_{1} & c_{2} & \cdots & c_{n-1} & c_{n}  \tag{1}\\
t_{1,1} & t_{1,2} & \cdots & t_{1, n-1} & t_{1, n} \\
\vdots & & \ddots & & \vdots \\
t_{m, 1} & t_{m, 2} & \cdots & t_{m, n-1} & t_{m, n}
\end{array}\right]
$$

where $t_{1,1} \cdots t_{m, n}$ are the entries within the generated table, which contains $m$ rows and $n$ columns.

Answer Extraction Prompt After the table content, denoted as $T$, is generated from the previous step, we perform answer extraction. The answer extraction step helps us to extract the answer from the table, as the final results may not always be in the last cell of the generated table. Following zero-shot CoT (Kojima et al., 2022), we add another answer extraction prompt $a$ : "the answer is" after the generated table, to extract the final answer from the table:

$$
\begin{equation*}
\text { Answer }=\operatorname{LLM}(x, c, T, a) \tag{2}
\end{equation*}
$$

Structure-Promoting Table Scheme Different

|  | Reasoning Type | Dataset | Size | Answer Type |
| :---: | :---: | :---: | :---: | :---: |
| 恴 | Arithmetic | SingleEq | 508 | Numeral |
|  |  | AddSub | 395 | Numeral |
|  |  | MultiArith | 600 | Numeral |
|  |  | GSM8K | 1,319 | Numeral |
|  |  | AQUA | 254 | Multiple Choice |
|  |  | SVAMP | 1,000 | Numeral |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-04.jpg?height=143&width=48&top_left_y=499&top_left_x=1071) | Symbolic | Coin Flip | 1,000 | Yes or No |
|  |  | Last Letter | 254 | String |
|  | Commonsense | StrategyQA | 2,290 | Yes or No |
|  |  | CommonsenseQA | 1,221 | Multiple Choice |

table generation prompts (headers) may result
Table 1: Tasks and Data

in different tables generated (with different content). We propose a "structure-promoting scheme", which maximally unlocks the reasoning abilities of LLMs.

We define each row as a reasoning step. A table containing multiple rows will depict the step-bystep reasoning procedure leading to the final answer. Thus, our first column is "step", containing a number that indicates which reasoning step the current row represents.

Least-to-most prompting (Zhou et al., 2022) contains two stages: problem reduction and sequential solving. In problem reduction, they decompose a question into multiple subquestions. Similarly, we add "subquestion" as our second column. At the beginning of each step, the LLMs will generate a subquestion under this column, which demonstrates the objective of the current reasoning step.

The conventional zero-shot CoT (Kojima et al., 2022) shows that allowing the model to generate some reasoning process before answering can achieve a better result. Inspired by this observation, we add a third column, "process", into our table. Given a subquestion in the previous column, we expect to generate the reasoning process in the current column before answering.

The last column is named "answer". As the previous reasoning process under the "process" column may not necessarily provide an answer, we hope to use the "answer" column to explicitly request an (intermediate) answer at the end of each reasoning step.

With the above considerations, our primary scheme for the table header is designed as follows, which serves as our main table generation prompt:

$$
\mid \text { step|subquestion|process|result| }
$$

## 4 Experimental Setup

Large Language Models We consider two stateof-the-art large language models under the GPT-

|  | Method | CoT Prompt | LLM | SingleEq | AddSub | MultiArith | GSM8K | AQUA | SVAMP | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\dot{d}$ <br> $\frac{\pi}{n}$ <br> $\dot{\omega}$ <br> $\stackrel{\rightharpoonup}{N}$ | Standard Prompting | 一 | text | 74.6 | 72.2 | 17.7 | 10.4 | 22.4 | 58.8 | 42.7 |
|  |  |  | code | 46.3 | 51.4 | 7.2 | 4.1 | 23.6 | 29.5 | 27.0 |
|  | CoT | Let's think step by step | text | 78.0 | 69.6 | 78.7 | 40.7 | 33.5 | 62.1 | 60.4 |
|  |  |  | code | 65.6 | 65.6 | 64.8 | 31.8 | 29.5 | 39.9 | 49.5 |
|  | Tab-CoT | \|step|subquestion|process|result| | text | 74.6 | 71.9 | 72.2 | 39.3 | 36.6 | 57.0 | 58.6 |
|  |  |  | code | 81.9 | 70.9 | 81.2 | 44.4 | 37.0 | 60.5 | 62.6 |

Table 2: Zero-shot results on the arithmetic datasets. All methods use the same answer extraction prompt in these datasets for a fair comparison. All methods are evaluated under the zero-shot setting.

3 family (Brown et al., 2020) in our experiments, namely "code-davinci-002" and "textdavinci-002", whose APIs are made available by OpenAI ${ }^{6}$. For brevity, we use "code" to refer to the model "code-davinci-002" and "text" to refer to "text-davinci-002" in our experiments.

Tasks and Datasets We primarily focus on mathematical reasoning in this work. Thus, we evaluate our method on 6 arithmetic reasoning datasets. Specifically, they are SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014), MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017), and SVAMP (Patel et al., 2021), which are standard datasets widely used in the community.

We also conducted additional experiments on datasets involving other types of reasoning tasks. Specifically, we evaluate our method on two symbolic reasoning tasks: Last letter and Coin Flip ${ }^{7}$ : the former is the task that asks for the concatenation of the last letters of 4 words, and the latter asks for the state of the coin after being flipped a few times. We investigate how the specificity of column names affects the performance and report in our ablation study. We also evaluate our method on two commonsense reasoning tasks: CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021).

Following zero-shot CoT (Kojima et al., 2022), we set the first generated number as the numeral answer, the first capitalized letter as the answer for multiple-choice questions, and the first "yes" or "no" as the answer for "Yes or No" questions.

## 5 Results

### 5.1 Main Results

Our main experiments are conducted on arithmetic reasoning tasks under the zero-shot setting. We tested the performance of both text-based and code-based LLMs on all methods. The[^4]

results are shown in Table 2. Under the scheme "|step|subquestion|process|result|", our zero-shot Tab-CoT approach significantly outperformed the standard prompting in all tasks. Furthermore, our best-performing Tab-CoT model (using code-based LLM) outperforms the best conventional CoT model in 5 out of 6 tasks (with an average improvement of $2.2 \%$ ).

When the standard prompting method is considered, using the text-based LLM leads to significantly better results than the code-based counterpart ( $15.7 \%$ on average). Similarly, when zero-shot CoT is considered, using the former also outperforms the latter by $10.9 \%$ on average. However, for our Tab-CoT approach, "code" outperforms "text" by $4.0 \%$, leading to the best overall performance among all configurations.

From such results, we can see that the conventional CoT method responds differently from our Tab-CoT method with different types of underlying LLMs involved. The conventional CoT method (and the standard prompting method) strongly favors a text-based LLM under the zero-shot setting. In contrast, our approach works well with both types of LLMs, but the code-based version can give it an additional boost in performance. Compared with "text", the "code" model is further fine-tuned on code (Chen et al., 2021). We conjecture that table generation resembles the code generation process - both involve structured procedures that are highly organized and follow a step-by-step process. Comparing our Tab-CoT approach with conventional CoT, we can conclude that our proposed table-generation prompt is able to significantly better unlock the strong reasoning abilities within the code-based LLM.

Based on the above main experiments, we choose to use "code" as the default LLM for all subsequent experiments unless otherwise specified.

### 5.2 Importance of Scheme Design

To understand the significance of our proposed table scheme design, we evaluate the performance of

| Scheme | SingleEq | AddSub | MultiArith | GSM8K | AQUA | SVAMP | Average |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Standard Prompting | 46.3 | 51.4 | 7.2 | 4.1 | 23.6 | 29.5 | 27.0 |  |
|  | Istep\|subquestion|process|result| | 81.9 | 70.9 | 81.2 | 44.4 | 37.0 | 60.5 | 62.6 |
| $\stackrel{\rightharpoonup}{\dot{\Delta}}$ | Istep\|subquestion|procedure|result| | 83.7 | 69.1 | 77.8 | 43.4 | 38.2 | 60.4 | 62.1 |
| $\mathrm{~N}$ | Istep\|question|response | | 77.6 | 73.9 | 79.0 | 38.1 | 34.3 | 63.9 | 61.1 |
|  | Self-consistency (using above) | $\mathbf{8 6 . 4}$ | $\mathbf{7 8 . 2}$ | $\mathbf{8 5 . 2}$ | $\mathbf{4 8 . 2}$ | $\mathbf{4 4 . 1}$ | $\mathbf{6 6 . 9}$ | $\mathbf{6 8 . 2}$ |

Table 3: Zero-shot performance comparison between the three schemes (and with self-consistency).

|  | Scheme | Average |
| :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-06.jpg?height=157&width=36&top_left_y=632&top_left_x=345) | \|subquestion|process|result| | 54.3 |
|  | \|step|process|result| | 57.2 |
|  | \|step|subquestion|result| | 61.3 |
|  | \|step|subquestion|process| | 60.9 |
|  | \|step|subquestion|process|r | 62.6 |

Table 4: Performance if a column is removed from the scheme (detailed results are in Appendix A).

|  | Method | Standard <br> Prompting | CoT | Tab-CoT |
| :---: | :---: | :---: | :---: | :---: |
| $\frac{6}{6}$ | SingleEq | 86.8 | 93.1 | 92.1 |
|  | AddSub | 90.9 | 89.1 | 89.1 |
|  | MultiArith | 44.0 | 96.2 | 96.3 |
|  | GSM8K | 19.7 | 63.1 | 61.6 |
|  | AQUA | 29.5 | 45.3 | 46.9 |
|  | SVAMP | 69.9 | 76.4 | 82.9 |
|  | Average | 68.2 | 77.2 | 78.2 |

Table 5: Few-shot results on the arithmetic datasets.

"|step|subquestion|process|result|", along with four variations, each of which is obtained by removing one of the four columns as ablation. The results in Table 4 show that each column of "|step|subquestion|process|result|" is crucial. From the result, we notice that removing the column "step" from our scheme results in the most significant performance drop. This implies although the step only contains a number indicating "which step this is", it organized the table in sequential order over rows. The column "subquestion" is also important. Removing "subquestion" from the scheme also shows an average performance drop of $5.4 \%$. The "subquestion" column forms step-by-step instructions vertically, indicating the subquestion under consideration for each step. The "step" and "subquestion" columns play important roles in maintaining the structure of the table, building vertical connections across rows.

### 5.3 Effectiveness of Self-Consistency

The self-consistency (Wang et al., 2022) decoding strategy was shown to obtain better results by generating and exploring multiple, diverse reasoning paths. We also adopt a similar approach here. In the original self-consistency paper, up to 40 reasoning paths were considered. We show the feasibility of using only 3 paths in our work. ${ }^{8}$ This is conveniently achieved by using 3 different prompts we select another two table schemes besides the standard scheme. One is a highly similar prompt, which we expect to perform similarly well, and the other is less similar, which we expect to yield a worse performance (based on Sec 5.2). They are shown in Table 3. We then perform majority voting based on the outputs from these 3 prompts. Interestingly, although a prompt with worse performance is used in the voting process, the overall performance improves. This shows the benefits of integrating different table schemes for such tasks, which helps improve the overall robustness of the approach.

### 5.4 Few-shot Tab-CoT

Tab-CoT shows impressive reasoning ability under the zero-shot setting. It can generate a structured output in the form of a table that enables the chainof-thought reasoning process without few-shot samples. Tables are capable chain-of-thought carriers, but can they also serve as good chain-of-thought teachers? To answer this question, we evaluated Tab-CoT under the few-shot setting. ${ }^{9}$

For a fair comparison, we use the same few-shot sample questions described in Wei et al. (2022) (listed in Appendix D). We use "|step|subquestion|process|result|" as the table scheme when representing few-shot samples. The results are reported in Table 5, our method outperformed few-shot CoT by $1 \%$ on average. While the performance difference between Tab-CoT and CoT on other datasets is below $2 \%$, the performance difference on SVAMP is $6.5 \%$. The large improvement on SVAMP is likely related to the selection of few-shot samples because Wei et al. (2022) select 8 sample questions from SVAMP for all arithmetic reasoning tasks except AQUA ${ }^{10}$.[^5]

| ![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-07.jpg?height=30&width=125&top_left_y=245&top_left_x=265) | Gretchen has some coins. There are 30 more gold coins than silver coins. If she had 70 gold coins, how many coins did Gretchen have in total? (GT: 110) |  |  |  |
| :---: | :---: | :---: | :---: | :---: |
| CoT: | Let's think step by step. If Gretchen had 70 gold coins, then she would have 30 silver coins $(70-30=40)$. <br> So Gretchen would have a total of $\mathbf{1 0 0}$ coins $(70+30=100)$. |  |  |  |
| Model | Step | Subquestion | Process | Result |
| text | 1 <br> 2 | How many silver coins does Gretchen have? <br> How many gold coins does Gretchen have? | There are 30 more gold coins than silver coins. If she had 70 <br> gold coins, how many coins did Gretchen have in total? <br> There are 30 more gold coins than silver coins. If she had 70 <br> gold coins, how many coins did Gretchen have in total? | 100 coins <br> 70 gold coins |
| Error Analysis: The "text" model overwrites the question in the process, thus answering the wrong question. |  |  |  |  |
| code | 1 <br> 2 <br> 3 | How many gold coins does Gretchen have? <br> How many silver coins does Gretchen have? <br> How many coins does Gretchen have in total? | 30 more gold coins than silver coins <br> 30 more gold coins than silver coins <br> 70 gold coins +40 silver coins | 70 gold coins <br> 40 silver coins <br> $\mathbf{1 1 0}$ coins |
| Question 2: | Adam bought 2 boxes of chocolate candy and 5 boxes of caramel candy. If each box has 4 pieces inside it, how much candy did he have total? (GT: 28) |  |  |  |
| CoT: | Adam bought 2 boxes of chocolate candy. That means he has 8 pieces of chocolate candy. Adam bought 5 boxes of caramel candy. <br> That means he has 20 pieces of caramel candy. To find out how much candy Adam has in total, <br> we add 8 pieces of chocolate candy and 20 pieces of caramel candy. Adam has 28 pieces of candy in total. |  |  |  |
| Model | Step | Subquestion | Process | Result |
| text | 2 <br> 3 <br> 4 <br> 5 | What is the unknown information? <br> What information is not needed? <br> What is the equation? <br> What is the answer? | Adam bought 2 boxes of chocolate candy and 5 boxes of <br> caramel candy. If each box has 4 pieces inside it, how much <br> candy did he have total? <br> How much candy did he have total? <br> boxes of chocolate candy and 5 boxes of caramel candy, each <br> with 4 pieces inside | 2 boxes of chocolate <br> candy and 5 boxes of <br> caramel candy, each with <br> 4 pieces inside <br> - <br> $2(4)+5(4)$ <br> 24 pieces of candy |
| Error Analysis: The "text" model contains redundant steps in the table and wrong calculation. |  |  |  |  |
| code | 1 <br> 2 <br> 3 | How many pieces of chocolate candy did Adam buy? <br> How many pieces of caramel candy did Adam buy? <br> How many pieces of candy did Adam buy? | 2 boxes $x 4$ pieces per box <br> 5 boxes $x 4$ pieces per box <br> 8 pieces +20 pieces | 8 pieces <br> 20 pieces <br> $\mathbf{2 8}$ pieces |
| Question 3: | {A pet store had 102 puppies. In one day they sold 21 of them and put the rest into cages with 9 in each cage. How many cages did they use? (GT: <br> We know that the pet store had 102 puppies and that 21 of them were sold. That means we have 81 puppies left. <br> Now, if 9 puppies fit in each cage, we can divide 81 by 9 to find out how many cages are needed. $81 / 9=9$. So, the pet store used 9 cages.} |  |  |  |
| CoT: |  |  |  |  |
| Model | Step | Subquestion | Process | Result |
| {How many cages are needed for 102 puppies? $102 / 9=11.3333$ <br> How many cages are needed for 21 puppies? $21 / 9=2.3333$ <br> How many cages are needed in total? $11+2=13$ <br> Error Analysis: The "text" model has a wrong reasoning order, and should add up before dividing. } |  |  |  |  |
|  |  |  |  |  |  |  |  |  |
| code | 1 <br> 2 <br> 3 | How many puppies are left? <br> How many puppies are in each cage? <br> How many cages are needed? | $102-21=81$ <br> 9 <br> $81 / 9=9$ | 81 <br> 9 <br> 9 |

Table 6: Case studies (on MultiArith) of the tables generated from "code-davinci-002"/"text-davinci-002". The results returned after applying the answer extraction prompts are in bold. Additional case studies are in Appendix C.

### 5.5 Case Studies

The main experimental results show that "code" under-performs "text" with conventional CoT but yields better results in our Tab-CoT. To understand this better, we conduct case studies to compare their generated tables in Table 6.

While "code" only generated short text snippets or formulas under "process", the words generated by "text" under the same column tend to form complete sentences whenever possible. As we mentioned earlier, "code" is an LLM that is further fine-tuned on code (Chen et al., 2021). This explains why it appears more amenable to the tabularstructured format of the output. In question 1, the model with "text" overwrites the generated "subquestion" by asking another question. Thus, the "result" fails to answer the "subquestion" in the same row. In question 2, "text" generated 5 steps while "code" only took 3. The "subquestion" generated by "text" is also ambiguous (e.g., "what is the known information?"). In question 3, "text" presents a wrong reasoning order. Overall, "code" shows better reasoning ability

multiple choice questions. We use the same few-shot samples following Wei et al. (2022). by demonstrating a more concise and straightforward reasoning process.

### 5.6 Additional Experiments

We further evaluate our methods on symbolic reasoning and commonsense reasoning tasks. We also conducted some new experiments based on the GPT-3.5 model to understand our approach's effectiveness on such newer models ${ }^{11}$. With such additional experiments, we hope to draw further insights into our approach.

Symbolic Reasoning We evaluate Tab-CoT on two symbolic reasoning datasets: Coin Flip (CF) ${ }^{12}$ and Last Letter (LL) $)^{13}$. Unlike the arithmetic reasoning tasks, these tasks focus on some specific problems. This also opens up the opportunity for us to examine whether the specificity of the table[^6]

![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-08.jpg?height=214&width=1539&top_left_y=253&top_left_x=250)

Figure 4: The schemes that disable (left) and enable (right) potential 2-dimensional reasoning.

|  | Task | Cat | Prompt | Result |
| :---: | :---: | :---: | :---: | :---: |
| 宁 <br> o <br> oٓ | $\mathrm{CF}$ |  | let's think step by step | $91.4 \quad$ |
|  |  | 1 | \|step|subquestion|process|result| | 85.0 |
|  |  | 2 | \|step|initial state|action|next state| | ![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-08.jpg?height=29&width=67&top_left_y=655&top_left_x=868) |
|  |  | 3 | \|step|name|flip or not|result| | 96.2 |
|  | LL |  | let's think step by step | 57.6 |
|  |  | 1 | \|step|subquestion|process|result| | 25.2 |
|  |  | 2 । | step\|original answer|action|updated answer| | $1 \quad 50.8 \quad$ |
|  |  | 3 | \|step|word|last letter|answer| | ![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-08.jpg?height=30&width=67&top_left_y=814&top_left_x=868) |

Table 7: Effect of different specificity of schemes. We use Zero-shot CoT with the "text" model as our baseline (as Zero-shot CoT works better with "text" model).

|  | Method | CommonsenseQA | StrategyQA | Avg |
| :--- | :--- | :---: | :---: | :--- |
| $\stackrel{\circ}{\circ}$ | Standard Prompting | 69.0 | 3.3 | 36.2 |
| 产 | CoT | 54.6 | 38.9 | 46.8 |
| े | Tab-CoT | 68.4 | 50.4 | 59.4 |

Table 8: Results on commonsense reasoning.

scheme may have an impact on the reasoning process in such tasks.

To this end, we split table schemes into three categories: (1) general: the table scheme that can be generally applied to most text questions. (2) domain-specific: the table scheme that can be adapted to a specific domain. (3) task-specific: the scheme that can only be adopted by a single task.

Our experiments in Table 7 illustrate that the specificity of the table schemes highly affects the performance of symbolic reasoning tasks. One may expect the performance to increase as the table scheme becomes more task-specific. Our taskspecific scheme outperformed the zero-shot CoT in both tasks. However, the increased specificity does not always lead to higher accuracy. In the Coin Flip task, we noticed that another task-specific scheme "|step|initial coin state|flip or not|next coin state|" only achieves an accuracy of $68.0 \%$. To understand this, we investigate their reasoning flows in Figure 4. Although the left scheme is more task-specific, it largely disabled the vertical reasoning in the table. While the right scheme is general, it effectively enables reasoning along both vertical and horizontal directions, leading to significantly better results. ${ }^{14}$[^7]

|  | Method | CoT | Tab-CoT |
| :---: | :---: | :---: | :---: |
| $\overrightarrow{0}$ <br> $\frac{0}{\hat{1}}$ <br> $\frac{0}{N}$ <br> $\mathrm{~N}$ | SingleEq | 85.6 | 87.8 |
|  | AddSub | 83.3 | 85.8 |
|  | MultiArith | 90.5 | 89.3 |
|  | GSM8K | 68.7 | 78.2 |
|  | AQUA | 50.8 | 51.2 |
|  | SVAMP | 79.0 | 81.1 |
|  | Average | 76.3 | 78.9 |

Table 9: Results with GPT-3.5.

|  | Task | code-cushman- 001 <br> $(13 \mathrm{~B})$ | code-davinci-002 <br> $(175 \mathrm{~B})$ |
| :---: | :---: | :---: | :---: |
| $\stackrel{0}{2}$ <br> $\frac{1}{N}$ <br> $\stackrel{0}{U}$ <br> $N$ | SingleEq | 6.3 | 81.9 |
|  | AddSub | 6.3 | 70.9 |
|  | MultiArith | 2.0 | 81.2 |
|  | GSM8K | 0.9 | 44.4 |
|  | AQUA | 16.9 | 37.0 |
|  | SVAMP | 5.0 | 60.5 |
|  | Average | 6.2 | 62.6 |

Table 10: A comparison between the different sizes of "code", "Average" is the average score across six datasets.

Commonsense Reasoning As another set of additional experiments, we further evaluate our method on commonsense reasoning, including CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021). The results are in Table 8. Tab-CoT obtained the highest average accuracy. However, the results of our method did not show significantly improved performance compared with Standard Prompting in a few-shot setting ${ }^{15}$. These results imply that commonsense reasoning tasks do not have a fixed answering pattern. Therefore, providing chain-of-thought samples is not enough to make up for the lack of commonsense knowledge. For a fair comparison, we use the same few-shot questions listed in (Wei et al., 2022).

Results on GPT-3.5 We test our method on the recent model "GPT-3.5-turbo-0301" in Table $9{ }^{16}$. We found that our method is applicable to GPT3.5 , and achieves better performance compared to conventional Zero-shot CoT. Another interesting observation is when prompting the GPT-3.5 model with "Let's think step by step", a large number of the generated texts already contain a table in their[^8]

|  | Scheme | SingleEq | AddSub | MultiArith | GSM8K | AQUA | SVAMP | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_7f48c281901644509119g-09.jpg?height=187&width=36&top_left_y=287&top_left_x=485) | Standard Prompting | 46.3 | 51.4 | 7.2 | 4.1 | 23.6 | 29.5 | 27.0 |
|  | \|subquestion|process|result| | 69.9 | 51.9 | 84.0 | 40.1 | 35 | 44.7 | 54.3 |
|  | \|step|process|result| | 77.0 | 55.7 | 84.2 | 41.5 | 37.8 | 46.9 | 57.2 |
|  | \|step|subquestion|result| | 76.0 | 77.9 | 76.8 | 40.1 | 36.2 | 60.6 | 61.3 |
|  | \|step|subquestion|process| | 78.0 | 75.9 | 76.3 | 39.7 | 34.3 | 60.9 | 60.9 |
|  | \|step|subquestion|process|result| | 81.9 | 70.9 | 81.2 | 44.4 | 37.0 | 60.5 | 62.6 |

Table 11: Performance if a column is removed from the scheme.

## CoT process. ${ }^{17}$

### 5.7 Ablation Studies

Model Sizes Kojima et al. (2022) evaluated the family of GPT-3 models of four different sizes: $2.7 \mathrm{~B}, 6.7 \mathrm{~B}, 13 \mathrm{~B}$, and 175B parameters. The results show that only the largest model ("text-davinci002 ") shows the chain-of-thought reasoning ability.

We compare the performance of the smaller model "code-cushman-001" (13B) with "codedavinci-002" (175B). Similar to zero-shot CoT, smaller models do not show the ability to conduct chain-of-thought reasoning. The performance of "code-cushman-001" cannot reach $10 \%$, except AQUA (a multiple choice dataset with 5 choices for each question). The experimental results are reported in Table 10.

Structure-Promoting Scheme As mentioned in Table 4, we compare the performance when we remove any column from "|step|subquestion|process|result|". The detailed experimental results are reported in Table 11. Results suggest that each column of our proposed scheme is important because removing any column will lead to a drop in performance.

## 6 Discussion

Our experimental results confirmed the effectiveness of our proposed tabular chain-of-thought method under both zero-shot and few-shot settings. We summarize several advantages of our method compared to conventional chain-of-thought methods and list them below.

Tab-CoT generates a table illustrating the reasoning process, which is more organized. This nature of the generated text, as can be seen from Table 6, makes the reasoning process much easier.

Additionally, from Figure 4, we conclude that Tab-CoT encourages a more structured reason-[^9]

ing process to be explicitly modelled. As a 2dimensional data structure, tables enable both horizontal reasoning along rows and vertical reasoning along columns.

Practically, table schemes are also easy to craft. Designing a specific table generation prompt typically involves deciding concise header names without concerning grammar. It is thus less cumbersome than choosing a natural language prompt from a diverse set of candidates.

Overall, we argue that under current state-of-theart LLMs, table schemes are natural prompts that are well suited for zero-shot learning.

## 7 Conclusion

In this paper, we propose Tab-CoT, a novel prompting framework that performs effective zero-shot reasoning by generating a table.

Tab-CoT shows competitive results on arithmetic reasoning tasks under both zero-shot and few-shot settings. We further conducted comprehensive experiments across different reasoning tasks under different settings. Our comprehensive experiments revealed some specific benefits of our method and identify the optimal way to use it. We hope that, through our work, we can sparkle new ideas and provide some inspiration to our community.

In the future, we would like to explore methods to automate the scheme selection process, using the generated schemes to meet task-specific requirements. Future work also includes integrating external calculators (Gao et al., 2022), or taskspecific supervision (Zhou et al., 2022) into the learning process, under both zero-shot and fewshot settings.

Our Tab-CoT also provides a straightforward decomposition of the intermediate thought process. This highly structured chain of thought produced by our approach may help people to observe and interpret how large language models decompose complex problems. We believe our proposed method can help reveal the underlying mechanisms associated with the emergence of certain complex behaviours associated with large language models.

## Limitations

We identify a few limitations of this work. First, our approach is applicable to language models pretrained with tables, which may not always be included in all language models (especially small ones). Second, our approach's limited improvement in commonsense reasoning tasks suggests that its effectiveness may depend on the specific task and the level of structured reasoning required.

## Acknowledgement

We would like to thank the anonymous reviewers, our meta-reviewer, and senior area chairs for their constructive comments and support on our work. This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Program (AISG Award No: AISG2-RP-2020-016), and the Ministry of Education, Singapore, under its Tier 3 Programme (The Award No.: MOET320200004).

## References

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluat- ing large language models trained on code. arXiv preprint arXiv:2107.03374.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.

Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics.

Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020 TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching. In Proceedings of COLING.

Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of EMNLP.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.

Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of ACL.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of NAACL.

Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of EMNLP.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of NAACL.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.
