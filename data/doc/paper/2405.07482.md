# Marginal Fairness Sliced Wasserstein Barycenter 

Khai Nguyen ${ }^{\diamond}$ Hai Nguyen ${ }^{\dagger}$ Nhat $\mathrm{Ho}^{\diamond}$<br>${ }^{\bullet}$ The University of Texas at Austin ${ }^{\dagger}$ VinAI Research<br>May 14, 2024


#### Abstract

The sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameterfree and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.


## 1 Introduction

Wasserstein barycenter [1] generalizes "averaging" to the space of probability measures. In particular, a Wasserstein barycenter is a probability measure that minimizes a weighted sum of Wasserstein distances between it and some given marginal probability measures. Due to the rich geometry of the Wasserstein distance [38], the Wasserstein barycenter can be seen as the FrÃ©chet mean [18] on the space of probability measures. As a result, Wasserstein barycenter has been applied widely to various applications in machine learning such as Bayesian inference [44, 45], domain adaptation [28], clustering [19], sensor fusion [14], text classification [25], and so on. Moreover, Wasserstein barycenter is also a powerful tool for computer graphics since it can be used for texture mixing [39], style transfer [29], shape interpolation [43], and many other tasks on many other domains.

Despite being useful, it is very computationally expensive to compute Wasserstein barycenter. In more detail, the computational complexity of Wasserstein barycenter is $\mathcal{O}\left(n^{3} \log n\right)$ when using linear programming [2] where $n$ is the largest number of supports of marginal probability measures. When using entropic regularization for optimal transport [11], the computational complexity is reduced to $\mathcal{O}\left(n^{2}\right)$ [24]. Nevertheless, quadratic scaling is not enough when the number of supports approaches a hundred thousand or a million. To address the issue, sliced Wassserstein barycenter (SWB) is introduced in [4] by replacing Wasserstein distance with its sliced variant i.e., sliced Wasseretein (SW) distance. Thank to the closed-form of Wasserstein distance in one-dimension, SWB has a low

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-02.jpg?height=553&width=1716&top_left_y=244&top_left_x=259)

Marginals
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-02.jpg?height=530&width=1674&top_left_y=251&top_left_x=280)

Figure 1: The uniform SWB and the MFSWB of 4 Gaussian distributions.

time complexity i.e., $\mathcal{O}(n \log n)$ which enables fast computation. Combining with the fact that sliced Wasserstein is equivalent to Wasserstein distance in bounded domains [5] and sliced Wasserstein does not suffer from the curse of dimensionality [35, 30, 27, 37], SWB becomes a great and scalable alternative choice of Wasserstein barycenter.

In some applications, we might want to find a barycenter that minimizes the distances to marginals and has equal distances to marginals at the same time e.g., constructing shape template for a group of shapes $[3,46]$ that can be further used in downstream tasks, exact balance style mixing between images [4], fair generative modeling [7], and so on. We refer to such a barycenter as a marginal fairness barycenter. Both the Wasserstein barycenter and SWB are defined based on a given set of marginal weights (marginal coefficients), and these weights represent the important levels of marginals toward the barycenter. Nevertheless, a uniform (weights) barycenter does not necessarily lead to the desired marginal fairness barycenter as shown in Figure 1. Moreover, obtaining the marginal fairness barycenter is challenging since such a barycenter might not exist and might not be identifiable given non-global-optimal optimization (Karcher mean problem). To the best of our knowledge, there is no prior work that investigates finding a marginal fairness barycenter.

In this work, we make the first attempt to tackle the marginal fairness barycenter problem i.e., we focus on finding marginal fairness SW barycenter (MFSWB) to utilize the scalability of SW distance.

Contribution: In summary, our main contributions are four-fold:

1. We define the marginal fairness SW barycenter (MFSWB) problem which is a constraint barycenter problem, where the constraint tries to limit the average pair-wise absolute difference between distances from the barycenter to marginals. We derive the dual form of MFSWB, discuss its computation, and address its computational challenges.
2. To address the issue, we propose surrogate definitions of MFSWB which are hyperparameterfree and computationally tractable. Motivated by Fair PCA [40], we propose the first surrogate MFSWB that minimizes the largest SW distance from the barycenter to the marginals. To address the problem of biased gradient estimation of the first surrogate MFSWB, we propose the second surrogate MFSWB which is the expectation of the largest one-dimensional Wasserstein distance from the projected barycenter to the projected marginals. We show that the second surrogate is an upper bound of the first surrogate and can yield an unbiased gradient estimator. We extend further
the second surrogates to the third surrogate by applying slicing distribution selection and showing that the third surrogate is an upper bound of the previous two.
3. We discuss the connection between the proposed surrogate MFSWB problems with the sliced multi-marginal Wasserstein (SMW) distance with the maximal ground metric. In particular, solving the proposed MFSWB problems is equivalent to minimizing a lower bound of the SMW. By showing that the SMW with the maximal ground metric is a generalized metric, we convey that it is safe to use the proposed surrogate MFSWB problems.
4. We conduct simulations with Gaussian and experiments on various applications including 3D point-cloud averaging, color harmonization, and sliced Wasserstein autoencoder with class-fairness representation to demonstrate the favorable performance of the proposed surrogate definitions.

Organization. We first discuss some preliminaries on SW distance, SWB and its computation, and sliced multi-marginal Wasserstein distance in Section 2. We then introduce the formal definition and surrogate definitions of marginal fairness SWB in Section 3. After that, we conduct experiments to demonstrate the favorable performance in fairness of the proposed definitions in Section 4. We conclude the paper and provide some future directions in Section 5. Finally, we defer the proofs of key results, the discussion on related works, and additional materials in the Appendices.

## 2 Preliminaries

Sliced Wasserstein distance. The definition of sliced Wasserstein (SW) distance [4] between two probability measures $\mu_{1} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ and $\mu_{2} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ is:

$$
\begin{equation*}
\operatorname{SW}_{p}^{p}\left(\mu_{1}, \mu_{2}\right)=\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{1}, \theta \sharp \mu_{2}\right)\right], \tag{1}
\end{equation*}
$$

where the Wasserstein distance has a closed form in one-dimension which is $\mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{1}, \theta \sharp \mu_{2}\right)=$ $\int_{0}^{1}\left|F_{\theta \sharp \mu_{1}}^{-1}(z)-F_{\theta \sharp \mu_{2}}^{-1}(z)\right|^{p} d z$ where $F_{\theta \sharp \mu_{1}}$ and $F_{\theta \sharp \mu_{2}}$ are the cumulative distribution function (CDF) of $\theta \sharp \mu_{1}$ and $\theta \sharp \mu_{2}$ respectively.

Sliced Wasserstein Barycenter. The definition of the sliced Wasserstein barycenter (SWB) problem [4] of $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ with marginal weights $\omega_{1}, \ldots, \omega_{K}>0$ $\left(\sum_{i=k}^{K} \omega_{k}=1\right)$ is defined as:

$$
\begin{equation*}
\min _{\mu} \mathcal{F}\left(\mu ; \mu_{1: K}, \omega_{1: K}\right) ; \quad \mathcal{F}\left(\mu ; \mu_{1: K}, \omega_{1: K}\right)=\sum_{k=1}^{K} \omega_{k} \mathrm{SW}_{p}^{p}\left(\mu, \mu_{k}\right) \tag{2}
\end{equation*}
$$

Computation of parametric SWB. Let $\mu_{\phi}$ be parameterized by $\phi \in \Phi$, SWB can be solved by gradient-based optimization. In that case, the interested quantity is the gradient $\nabla_{\phi} \mathcal{F}\left(\mu_{\phi} ; \mu_{1: K}, \omega_{1: K}\right)=$ $\sum_{k=1}^{K} \omega_{k} \nabla_{\phi} \mathrm{SW}_{p}^{p}\left(\mu_{\phi}, \mu_{k}\right)$. However, the gradient $\nabla_{\phi} \mathrm{SW}_{p}^{p}\left(\mu_{\phi}, \mu_{k}\right)=\nabla_{\phi} \mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)\right]=$ $\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)\right]$ for any $k=1, \ldots, K$ is intractable due to the intractability of SW with the expectation with respect to the uniform distribution over the unit-hypersphere. Therefore, Monte Carlo estimation is used. In particular, projecting directions $\theta_{1}, \ldots, \theta_{L}$ are sampled i.i.d from $\mathcal{U}\left(\mathbb{S}^{d-1}\right)$, and the stochastic gradient estimator is formed:

$$
\begin{equation*}
\nabla_{\phi} \mathrm{SW}_{p}^{p}\left(\mu_{\phi}, \mu_{k}\right) \approx \frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k}\right) . \tag{3}
\end{equation*}
$$

With the stochastic gradient, the SWB can be solved by using a stochastic gradient descent algorithm. We refer the reader to Algorithm 1 in Appendix B for more detail. Specifically, we now discuss the discrete SWB i.e., marginals and the barycenter are discrete measures.

Free supports barycenter. In this setting, we have $\mu_{\phi}=\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}, \mu_{k}=\frac{1}{n} \sum_{i=1}^{n} \delta_{y_{i}}$, and $\phi=$ $\left(x_{1}, \ldots, x_{n}\right)$, we can compute the (sub-)gradient with the time complexity $\mathcal{O}(n \log n)$ :

$$
\begin{equation*}
\nabla_{x_{i}} \mathrm{~W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)=p\left|\theta^{\top} x_{i}-\theta^{\top} y_{\sigma(i)}\right|^{p-1} \operatorname{sign}\left(\theta^{\top} x_{i}-\theta^{\top} y_{\sigma(i)}\right) \theta \text {, } \tag{4}
\end{equation*}
$$

where $\sigma=\sigma_{1} \circ \sigma_{2}^{-1}$ with $\sigma_{1}$ and $\sigma_{2}$ are any sorted permutation of $\left\{x_{1}, \ldots, x_{n}\right\}$ and $\left\{y_{1}, \ldots, y_{n}\right\}$.

Fixed supports barycenter. In this setting, we have $\mu_{\phi}=\sum_{i=1}^{n} \phi_{i} \delta_{x_{i}}, \mu_{k}=\sum_{i=1}^{n} \beta_{i} \delta_{x_{i}}, \sum_{i=1}^{n} \phi_{i}=$ $\sum_{i=1}^{n} \beta_{i}$ and $\phi=\left(\phi_{1}, \ldots, \phi_{n}\right)$. We can compute the gradient as follows:

$$
\begin{equation*}
\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)=\boldsymbol{f}^{\star}, \tag{5}
\end{equation*}
$$

where $\boldsymbol{f}^{\star}$ is the first optimal Kantorovich dual potential of $\mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)$ which can be obtained with the time complexity of $\mathcal{O}(n \log n)$. We refer the reader to Proposition 1 in [12] for the detail and Algorithm 1 in [41] for the computational algorithm.

When the supports or weights of the barycenter are the output of a parametric function, we can use the chain rule to estimate the gradient of the parameters of the function. For the continuous case, we can approximate the barycenter and marginals by their empirical versions, and then perform the estimation in the discrete case. Since the sample complexity of $\mathrm{SW}$ is $\mathcal{O}\left(n^{-1 / 2}\right)[31,35,27,37]$, the approximation error will reduce fast with the number of support $n$ increases. Another option is to use continuous Wasserstein solvers $[15,23,9]$, however, this option is not as simple as the first one.

Sliced Multi-marginal Wasserstein Distance. Given $K \geq 1$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, sliced Multi-marginal Wasserstein Distance [10] (SMW) is defined as:

$$
\begin{equation*}
S M W_{p}^{p}\left(\mu_{1: K} ; c\right)=\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \tag{6}
\end{equation*}
$$

where the expectation is under $\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)$. When using the barycentric cost i.e., $c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)^{p}=$ $\sum_{k=1}^{K} \beta_{k}\left|\theta^{\top} x_{k}-\sum_{k^{\prime}=1}^{K} \beta_{k^{\prime}} \theta^{\top} x_{k^{\prime}}\right|^{p}$ for $\beta_{k}>0 \forall k$ and $\sum_{k} \beta_{k}=1$. Minimizing $S M W_{p}^{p}\left(\mu_{1: K}, \mu ; c\right)$ with respect to $\mu$ is equivalent to a barycenter problem. We refer the reader to Proposition 7 in [10] for more detail.

## 3 Marginal Fairness Sliced Wasserstein Barycenter

We first formally define the marginal fairness Sliced Wasserstein barycenter in Section 3.1. We then propose surrogate problems in Section 3.2. Finally, we discuss the connection of the proposed surrogate problems to sliced multi-marginal Wasserstein in Section 3.3.

### 3.1 Formal Definition

Now, we define the Marginal Fairness Sliced Wasserstein barycenter (MFSWB) problem by adding marginal fairness constraints to the SWB problem.

Definition 1. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, admissible $\epsilon \geq 0$ for $i=1, \ldots, K$ and $j=i+1, \ldots, K$, the Marginal Fairness Sliced Wasserstein barycenter (MFSWB) is defined as:

$$
\begin{equation*}
\min _{\mu} \frac{1}{K} \sum_{k=1}^{K} S W_{p}^{p}\left(\mu, \mu_{k}\right) \text { s.t. } \frac{2}{(K-1) K} \sum_{i=1}^{K} \sum_{j=i+1}^{K}\left|S W_{p}^{p}\left(\mu, \mu_{i}\right)-S W_{p}^{p}\left(\mu, \mu_{j}\right)\right| \leq \epsilon \tag{7}
\end{equation*}
$$

Remark. We want $\epsilon$ in Definition 1 to be close to 0 i.e., $\mu_{1}, \ldots, \mu_{K}$ are on the $S W_{p}$-sphere with the center $\mu$. However, for a too-small value of $\epsilon$, there might not exist a solution $\mu$.

Duality objective. For admissible $\epsilon>0$, there exist a Lagrange multiplier $\lambda$ such that we have the dual form

$$
\begin{equation*}
\mathcal{L}(\mu, \lambda)=\frac{1}{K} \sum_{k=1}^{K} \operatorname{SW}_{p}^{p}\left(\mu, \mu_{k}\right)+\frac{2 \lambda}{(K-1) K} \sum_{i=1}^{K} \sum_{j=i+1}^{K}\left|\operatorname{SW}_{p}^{p}\left(\mu, \mu_{i}\right)-\operatorname{SW}_{p}^{p}\left(\mu, \mu_{j}\right)\right|-\lambda \epsilon \tag{8}
\end{equation*}
$$

Computational challenges. Firstly, MFSWB in Definition 1 requires admissible $\epsilon>0$ to guarantee the existence of the barycenter $\mu$. In practice, it is unknown if a value of $\epsilon$ satisfies such a property. Secondly, given a $\epsilon$, it is not trivial to obtain the optimal Lagrange multiplier $\lambda^{\star}$ in Equation equation 8 to minimize the duality gap which can be non-zero (weak-duality). Thirdly, using directly the dual objective Equation equation 8 requires hyperparameter tunning for $\lambda$ and might not provide a good landscape for optimization. Moreover, we cannot obtain an unbiased gradient estimate of $\phi$ in the case of the parametric barycenter $\mu_{\phi}$. In greater detail, the Monte Carlo estimation for the absolute distance between two SW distances is biased. Finally, Equation equation 8 has a quadratic time complexity and space complexity in terms of the number of marginals i.e., $\mathcal{O}\left(K^{2}\right)$.

### 3.2 Surrogate Definitions

Since it is not convenient to use the formal MFSWB in applications, we propose two surrogate definitions of MFSWB that are free of hyperparameters and computationally friendly.

First Surrogate Definition. Motivated by Fair PCA [40], we propose a practical surrogate MFSWB problem that is hyperparameter-free.

Definition 2. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, the surrogate marginal fairness sliced Wasserstein barycenter (s-MFSWB) problem is defined as:

$$
\begin{equation*}
\min _{\mu} \mathcal{S F}\left(\mu ; \mu_{1: K}\right) ; \quad \mathcal{S F}\left(\mu ; \mu_{1: K}\right)=\max _{k \in\{1, \ldots, K\}} S W_{p}^{p}\left(\mu, \mu_{k}\right) \tag{9}
\end{equation*}
$$

The s-MFSWB problem tries to minimize the maximal distance from the barycenter to the marginals. Therefore, it can minimize indirectly the overall distances between the barycenter to the marginals and implicitly make the distances to marginals approximately the same.

Gradient estimator. Let $\mu_{\phi}$ be paramterized by $\phi \in \Phi$, and $\mathcal{F}(\phi, k)=S W_{p}^{p}\left(\mu_{\phi}, \mu_{k}\right)$, we would like to compute $\nabla_{\phi} \max _{k \in\{1, \ldots, K\}} \mathcal{F}(\phi, k)$. By Danskin's envelope theorem [13], we have $\nabla_{\phi} \max _{k \in\{1, \ldots, K\}} \mathcal{F}(\phi, k)=$ $\nabla_{\phi} \mathcal{F}\left(\phi, k^{\star}\right)=\nabla_{\phi} \mathrm{SW}_{p}^{p}\left(\mu_{\phi}, \mu_{k^{\star}}\right)$ for $k^{\star}=\arg \max _{k \in\{1, \ldots, K\}} \mathcal{F}(\phi, k)$. Nevertheless, $k^{\star}$ is intractable due to the intractablity of $S W_{p}^{p}\left(\mu_{\phi}, \mu_{k}\right)$ for $k=1, \ldots, K$. Hence, we can form the estimation $\hat{k}^{\star}=\arg \max _{k \in\{1, \ldots, K\}} \widehat{S W}_{p}^{p}\left(\mu_{\phi}, \mu_{k} ; L\right)$ where $\widehat{S W}_{p}^{p}\left(\mu_{\phi}, \mu_{k} ; L\right)=\frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k}\right)$ with
$\theta_{1}, \ldots, \theta_{L} \stackrel{i . i . d}{\sim} \mathcal{U}\left(\mathbb{S}^{d-1}\right)$. Then, we can estimate $\nabla_{\phi} \mathrm{SW}_{p}^{p}\left(\mu_{\phi}, \mu_{\hat{k}^{\star}}\right)$ as in Equation 3. We refer the reader to Algorithm 2 in Appendix B for the gradient estimation and optimization procedure. The downside of this estimator is that it is biased.

Second Surrogate Definition. To address the biased gradient issue of the first surrogate problem, we propose the second surrogate MFSWB problem.

Definition 3. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, the unbiased surrogate marginal fairness sliced Wasserstein barycenter (us-MFSWB) problem is defined as:

$$
\begin{equation*}
\min _{\mu} \mathcal{U S F}\left(\mu ; \mu_{1: K}\right) ; \quad \mathcal{U S F}\left(\mu ; \mu_{1: K}\right)=\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right] . \tag{10}
\end{equation*}
$$

In contrast to s-MFSWB which minimizes the maximal SW distance among marginals, us-MFSWB minimizes the expected value of the maximal one-dimensional Wasserstein distance among marginals. By considering fairness on one-dimensional projections, us-MFSWB can yield an unbiased gradient estimate which is the reason why it is named as unbiased s-MFSWB.

Gradient estimator. Let $\mu_{\phi}$ be paramterized by $\phi \in \Phi$, and $\mathcal{F}(\theta, \phi, k)=W_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)$, we would like to compute $\nabla_{\phi} \mathbb{E}_{\theta \sim \mathbb{S}^{d-1}}\left[\max _{k \in\{1, \ldots, K\}} \mathcal{F}(\theta, \phi, k)\right]$ which is equivalent to $\mathbb{E}_{\theta \sim \mathbb{S}^{d-1}}\left[\nabla_{\phi} \max _{k \in\{1, \ldots, K\}} \mathcal{F}(\theta, \phi, k)\right]$ due to the Leibniz's rule. By Danskin's envelope theorem, we have $\nabla_{\phi} \max _{k \in\{1, \ldots, K\}} \mathcal{F}(\theta, \phi, k)=$ $\nabla_{\phi} \mathcal{F}\left(\theta, \phi, k^{\star}\right)=\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k^{\star}}\right)$ for $k_{\theta}^{\star}=\arg \max _{k \in\{1, \ldots, K\}} \mathcal{F}(\theta, \phi, k)$ where we can estimate $\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k_{\theta}^{\star}}\right)$ can be computed as in Equation 4- 5. Overall, with $\theta_{1}, \ldots, \theta_{L} \stackrel{i . i . d}{\sim} \mathcal{U}\left(\mathbb{S}^{d-1}\right)$, we can form the final estimation $\frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta_{l}}}\right)$ which is an unbiased estimate. We refer the reader to Algorithm 3 in Appendix B for the gradient estimation and optimization procedure.

Proposition 1. Given $K \geq 2$ marginals $\mu_{1: K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, we have $\mathcal{S F}\left(\mu ; \mu_{1: K}\right) \leq \mathcal{U S \mathcal { F }}\left(\mu ; \mu_{1: K}\right)$.

Proof of Proposition 1 is given in Appendix A.1. From the proposition, we see that minimizing the objective of us-MFSWB also reduces the objective of s-MFSWB implicitly.

Proposition 2. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right), \theta_{1}, \ldots, \theta_{L} \stackrel{\text { i.i.d }}{\sim} \mathcal{U}\left(\mathbb{S}^{d-1}\right)$, we have:

$$
\begin{equation*}
\mathbb{E}\left|\nabla_{\phi} \frac{1}{L} \sum_{l=1}^{L} W_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta}^{\star}}\right)-\nabla_{\phi} \mathcal{U S F}\left(\mu_{\phi} ; \mu_{1: K}\right)\right| \leq \frac{1}{\sqrt{L}} \operatorname{Var}\left[\nabla_{\phi} W_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k_{\theta}^{\star}}\right)\right]^{\frac{1}{2}}, \tag{11}
\end{equation*}
$$

where $k_{\theta}^{\star}=\arg \max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k}\right)$; and the expectation and variance are under the random projecting direction $\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)$

Proof of Proposition 2 is given in Appendix A.2. From the proposition, we know that the approximation error of the gradient estimator of us-MFSWB reduces at the order of $\mathcal{O}\left(L^{-1 / 2}\right)$. Therefore, increasing $L$ leads to a better gradient approximation. The approximation could be further improved via Quasi-Monte Carlo methods [32].

Third Surrogate Definition. The us-MFSWB in Definition 3 utilizes the uniform distribution as the slicing distribution, which is empirically shown to be non-optimal in statistical estimation [35]. Following the slicing distribution selection approach in [33], we propose the third surrogate with a new slicing distribution that focuses on unfair projecting directions.

Marginal fairness energy-based slicing distribution. Since we want to encourage marginal fairness, it is natural to construct the slicing distribution based on fairness energy.

Definition 4. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, the marginal fairness energy-based slicing distribution $\sigma\left(\theta ; \mu, \mu_{1: K}\right) \in \mathcal{P}\left(\mathbb{S}^{d-1}\right)$ is defined with the density function as follow:

$$
\begin{equation*}
f_{\sigma}\left(\theta ; \mu, \mu_{1: K}\right) \propto \exp \left(\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right), \tag{12}
\end{equation*}
$$

We see that the marginal fairness energy-based slicing distribution in Definition 4 put more mass to a projecting direction $\theta$ that has the larger maximal one-dimensional Wasserstein distance to marginals. Therefore, it will penalize more marginally unfair projecting directions.

Energy-based surrogate MFSWB. From the new proposed slicing distribution, we can define a new surrogate MFSWB problem, named energy-based surrogate MFSWB.

Definition 5. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, the energy-based surrogate marginal fairness sliced Wasserstein barycenter (us-MFSWB) problem is defined as:

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-07.jpg?height=118&width=1313&top_left_y=1036&top_left_x=403)

Similar to the us-MFSWB, es-MFSWB utilizes the implicit one-dimensional marginal fairness. Nevertheless, es-MFSWB utilizes the marginal fairness energy-based slicing distribution to reweight the importance of each projecting direction instead of considering them equally.

Proposition 3. Given $K \geq 2$ marginals $\mu_{1: K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, we have $\mathcal{U S F}\left(\mu ; \mu_{1: K}\right) \leq \mathcal{E S} \mathcal{F}\left(\mu ; \mu_{1: K}\right)$.

Proof of Proposition 3 is given in Appendix A.3. From the proposition, we see that minimizing the objective of es-MFSWB reduces the objective of us-MFSWB implicitly which also decreases the objective of s-MFSWB (Proposition 1).

Gradient estimator. Let $\mu_{\phi}$ be parameterized by $\phi \in \Phi$, we want to estimate $\nabla_{\phi} \mathcal{E S F}\left(\mu_{\phi} ; \mu_{1: K}\right)$. Since the slicing distribution is unnormalized, we use importance sampling to form an estimation. With $\theta_{1}, \ldots, \theta_{L} \stackrel{\text { i.i.d }}{\sim} \mathcal{U}\left(\mathbb{S}^{d-1}\right)$, we can form the importance sampling stochastic gradient estimation:

$$
\hat{\nabla}_{\phi} \mathcal{E S F}\left(\mu_{\phi} ; \mu_{1: K}, L\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\nabla_{\phi}\left(W_{p}^{p}\left(\theta_{l} \sharp \mu, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right) \frac{\exp \left(W_{p}^{p}\left(\theta_{l} \sharp \mu, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right)\right)}{\frac{1}{L} \sum_{i=1}^{L}\left[\exp \left(W_{p}^{p}\left(\theta_{i} \sharp \mu, \theta_{i} \sharp \mu_{k_{\theta_{i}}^{\star}}\right)\right)\right]}\right)\right],
$$

which can be further derived by using the chain rule and previously discussed techniques. It is worth noting that the above estimation is only asymptotically unbiased. We refer the reader to Algorithm 4 in Appendix B for the gradient estimation and optimization procedure.

Computational complexities of proposed surrogates. For the number of marginals $K$, the three proposed surrogates have a linear time complexity and space complexity i.e., $\mathcal{O}(K)$ which is the same as the conventional SWB and is better than $\mathcal{O}\left(K^{2}\right)$ of the formal MFSWB. For the number of projections $L$, the number of supports $n$, and the number of dimensions $d$, the proposed surrogates have the time complexity of $\mathcal{O}(\operatorname{Ln}(\log n+d))$ and the space complexity of $\mathcal{O}(L(n+d))$ which are similar to the formal MFSWB and SWB.

### 3.3 Sliced multi-marginal Wasserstein distance with the maximal ground metric

To shed some light on the proposed substrates, we connect them to a special variant of Sliced multi-marginal Wasserstein (SMW) (see Equation 6) i.e., SMW with the maximal ground metric $c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)=\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|$. We first show that SMW with the maximal ground metric is a generalized metric on the space of probability measures.

Proposition 4. Sliced multi-marginal Wasserstein distance with the maximal ground metric is a generalized metric i.e., it satisfies non-negativity, marginal exchangeability, generalized triangle inequality, and identity of identity of indiscernibles.

Proof of Proposition 4 is given in Appendix A.4. It is worth noting that SMW with the maximal ground metric has never been defined before. Since our work focuses on the MFSWB problem, we will leave the careful investigation of this variant of SMW to future work.

Proposition 5. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, the maximal ground metric $c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)=\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|$, we have:

$$
\begin{equation*}
\min _{\mu_{1}} \mathcal{U S F}\left(\mu_{1} ; \mu_{2: K}\right) \leq \min _{\mu_{1}} S M W_{p}^{p}\left(\mu_{1}, \mu_{2}, \ldots, \mu_{K} ; c\right) \tag{14}
\end{equation*}
$$

Proof of Proposition 5 is given in Appendix A. 5 and the inequality holds when changing $\mu_{1}$ to any $\mu_{i}$ with $i=2, \ldots, K$. Combining Proposition 1 , we have the corollary of $\min _{\mu_{1}} \mathcal{S F}\left(\mu_{1} ; \mu_{2: K}\right) \leq$ $\min _{\mu_{1}} S M W_{p}^{p}\left(\mu_{1}, \mu_{2}, \ldots, \mu_{K} ; c\right)$. From the proposition, we see that minimizing the us-MFSWB is equivalent to minimizing a lower bound of SMW with the maximal ground metric. Therefore, this proposition implies the us-MFSWB could try to minimize the multi-marginal distance. Moreover, this proposition can help to understand the proposed surrogates through the gradient flow of SMW. We can further extend the proposition to show the minimizing es-MFSWB objective is the same as minimizing a lower bound of energy-based SMW with the maximal ground metric, a new special variant of SMW. We refer the reader to Propositon 6 in Appendix B for more detail.

## 4 Experiments

In this section, we compare the barycenter found by our proposed surrogate problems i.e., s-MFSWB, us-MFSWB, and es-MFSWB with the barycenter found by USWB and the formal MFSWB. For evaluation, we use two metrics i.e., the F-metric (F) and the W-metric (W) which are defined as follows: $F=\frac{2}{K(K-1)} \sum_{i=1}^{K} \sum_{j=i+1}^{K}\left|W_{p}^{p}\left(\mu, \mu_{i}\right)-W_{p}^{p}\left(\mu, \mu_{j}\right)\right|, \quad W=\frac{1}{K} \sum_{i=1}^{K} W_{p}^{p}\left(\mu, \mu_{i}\right)$, where $\mu$ is the barycenter, $\mu_{1}, \ldots, \mu_{K}$ are the given marginals, and $W_{p}^{p}$ is the Wasserstein distance [16] of the order $p$. Here, the F-metric represents the marginal fairness degree of the barycenter and the W-metric represents the centerness of the barycenter. For all following experiments, we use $p=2$ for the Wasserstein distance and barycenter problems.

### 4.1 Barycenter of Gaussians

We first start with a simple simulation with 4 marginals which are empirical distributions with 100 i.i.d samples from 4 Gaussian distributions i.e., $\mathcal{N}((0,0), I), \mathcal{N}((20,0), I), \mathcal{N}((18,8), I)$, and $\mathcal{N}((18,-8), I)$. We then find the barycenter which is represented as an empirical distribution with
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-09.jpg?height=1354&width=1630&top_left_y=244&top_left_x=256)

Figure 2: Barycenters from USWB, MFSWB with $\lambda=1$, s-MFSWB, us-MFSWB, and es-MFSWB along gradient iterations with the corresponding F-metric and W-metric.

100 supports initialized by sampling i.i.d from $\mathcal{N}((0,-5), I)$. We use stochastic gradient descent with 50000 iterations of learning rate 0.01 , the number of projections 100 . We show the visualization of the found barycenters with the corresponding F-metric and W-metric by using USWB, s-MFSWB, us-MFSWB, and es-MFSWB at iterations 0, 1000, 5000, and 50000 in Figure 2. We observe that the USWB does not lead to a marginal fairness barycenter. The three proposed surrogate problems help to find a better barycenter faster in both two metrics than USWB. At convergence i.e., iteration 50000, we see that USWB does not give a fair barycenter while the three proposed surrogates lead to a more fair barycenter. Among the proposed surrogates, es-MFSWB gives the most marginal fairness barycenter with a competitive centerness. The formal MFSWB (dual form with $\lambda=1$ ) leads to the most fair barycenter. However, the performance of the formal MFSWB is quite sensitive to $\lambda$. We show the visualization for $\lambda=0.1$ and $\lambda=10$ in Figure 5 in Appendix D.
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-10.jpg?height=778&width=654&top_left_y=240&top_left_x=258)

Figure 3: Averaging point-clouds with USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB. Table 1: F-metric and W-metric along iterations in point-cloud averaging application.

| Method | $\overline{\text { Iteration } 0}$ |  | Iteration 1000 |  | Iteration 5000 |  | Iteration 10000 |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{F}(\downarrow)$ | $\mathrm{W}(\downarrow)$ | $\mathrm{F}(\downarrow)$ | $\mathrm{W}(\downarrow)$ | $\mathrm{F}(\downarrow)$ | $\mathrm{W}(\downarrow)$ | $\mathrm{F}(\downarrow)$ | $\mathrm{W}(\downarrow)$ |
| USWB | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $4.89 \pm 0.28$ | $85.72 \pm 0.18$ | $3.79 \pm 0.32$ | $45.37 \pm 0.18$ | $1.55 \pm 0.48$ | $39.81 \pm 0.18$ |
| MFSWB $\lambda$ | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $4.76 \pm 0.27$ | $84.86 \pm 0.17$ | $3.78 \pm 0.2$ | $45.2 \pm 0.11$ | $1.32 \pm 0.22$ | 39.73 |
| MFSWB $\lambda=1$ | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $0.49 \pm 0.2$ | $79.08 \pm 0.15$ | $3.64 \pm 0.26$ | $44.71 \pm 0.19$ | $1.03 \pm 0.06$ | $39.45 \pm 0.18$ |
| $\operatorname{MFSWB} \lambda=10$ | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $4.03 \pm 2.43$ | $71.24 \pm 0.9$ | $7.32 \pm 2.5$ | $45.21 \pm 0.2$ | $4.13 \pm 2.48$ | $42.56 \pm 0.36$ |
| s-MFSWB | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $2.52 \pm 0.77$ | $81.84 \pm 0.14$ | $4.01 \pm 0.38$ | $44.9 \pm 0.13$ | $1.15 \pm 0.09$ | $39.58 \pm 0.17$ |
| us-MFSWB | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $0.3 \pm 0.18$ | $78.69 \pm 0.17$ | $3.74 \pm 0.26$ | $44.38 \pm 0.1$ | $0.87 \pm 0.18$ | $39.26 \pm 0.1$ |
| es-MFSWB | $252.24 \pm 0.0$ | $3746.05 \pm 0.0$ | $0.2 \pm 0.19$ | $78.1 \pm 0.16$ | $3.5 \pm 0.29$ | $44.37 \pm 0.08$ | $0.84 \pm 0.22$ | $39.18 \pm 0.08$ |

### 4.2 3D Point-cloud Averaging

We aim to find the mean shape of point-cloud shapes by casting a point cloud $X=\left\{x_{1}, \ldots, x_{n}\right\}$ into an empirical probability measures $P_{X}=\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}$. We select two point-cloud shapes which consist of 2048 points in ShapeNet Core-55 dataset [6]. We initialize the barycenter with a spherical point-cloud. We use stochastic gradient descent with 10000 iterations of learning rate 0.01, the number of projections 10 . We report the found barycenters for two car shapes in Figure 3 at the final iteration and the corresponding F-metric and W-metric at iterations 0, 1000, 5000, and 10000 in Table 1 from three independent runs. As in the Gaussian simulation, s-MFSWB, us-MFSWB, and es-MFSWB help to reduce the two metrics faster than the USWB. With the slicing distribution selection, es-MFSWB performs the best at every iteration, even better than the formal MFSWB with three choices of $\lambda$ i.e., $0.1,1,10$. We also observe a similar phenomenon for two plane shapes in Figure 6 and Table 3 in Appendix D. We refer the reader to Appendix D for a detailed discussion.

### 4.3 Color Harmonization

We want to transform the color palette of a source image, denoted as $X=\left(x_{1}, \ldots, x_{n}\right)$ for $n$ is the number of pixels, to be an exact hybrid between two target images. Similar to the previous
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-11.jpg?height=604&width=1654&top_left_y=240&top_left_x=256)

Figure 4: Harmonized images from USWB, MFSWB $(\lambda=1)$ s-MFSWB, us-MFSWB, and es-MFSWB.

point-cloud averaging, we transform the color palette of an image into the empirical probability measure over colors (RGB) i.e., $P_{X}=\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}$. We then minimize barycenter losses i.e., USWB, MFSWB $(\lambda \in\{0.1,1,10\})$, s-MFSWB, us-MFSWB, and es-MFSWB by using stochastic gradient descent with the learning rate 0.0001 and 20000 iterations. We report both the transformed images and the corresponding F-metric and W-metric in Figure 4. We also report the full results in Figure 7- 9 in Appendix D. As in previous experiments, we see that the three proposed surrogates yield a better barycenter faster than USWB. The proposed es-MFSWB is the best variant among all surrogates since it has the lowest F-metric and W-metric at all iterations. We refer the reader to Figure 10-Figure 13 in Appendix D for additional flowers-images example, where a similar relative comparison happens. For the formal MFSWB, it is worse than es-MFSWB in one setting and better than es-MFSWB in one setting with the right choice of $\lambda$. Therefore, it is more convenient to use us-MFSWB in practice.

### 4.4 Sliced Wasserstein Autoencoder with Class-Fair Representation

Problem. We consider training the sliced Wasserstein autoencoder (SWAE)[22] with a class-fairness regularization. In particular, we have the data distributions of $K \geq 1$ classes i.e., $\mu_{k} \in \mathcal{P}\left(\mathbb{R}^{d}\right)$ for $k=1, \ldots, K$ and we would like to estimate an encoder network $f_{\phi}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{h}(\phi \in \Phi)$ and a decoder network $g_{\psi}: \mathbb{R}^{h} \rightarrow \mathbb{R}^{d}\left(\psi \in \Psi\right.$ with $\mathbb{R}^{h}$ is a low-dimensional latent space. Given a prior distribution $\mu_{0} \in \mathcal{P}\left(\mathbb{R}^{h}\right), p \geq 1, \kappa_{1} \in \mathbb{R}^{+}, \kappa_{2} \in \mathbb{R}^{+}$, and a minibatch size $M \geq 1$, we perform the following optimization problem:

$$
\min _{\phi, \psi} \mathbb{E}\left[\frac{1}{K M} \sum_{k=1}^{K} \sum_{i=1}^{M} c\left(X_{k i}, g_{\psi}\left(f_{\phi}\left(X_{k i}\right)\right)+\kappa_{1} S W_{p}^{p}\left(P_{Z}, P_{\left(f_{\phi}\left(X_{k}\right)\right)_{k=1}^{K}}\right)+\kappa_{2} \mathcal{B}\left(P_{Z} ; P_{f\left(X_{1}\right)}: P_{f\left(X_{K}\right)}\right)\right]\right.
$$

where $\left(X_{1}, \ldots, X_{K}\right) \sim \mu_{1}^{\otimes M} \otimes \ldots \otimes \mu_{K}^{\otimes M}, Z \sim \mu_{0}^{\otimes M}, c$ is a reconstruction loss, $P_{Z}=\frac{1}{M} \sum_{i=1}^{M} \delta_{Z_{i}}$, $P_{\left(f_{\phi}\left(X_{k}\right)\right)_{k=1}^{K}}=\frac{1}{K M} \sum_{k=1}^{K} \sum_{i=1}^{M} \delta_{f_{\phi}\left(X_{k i}\right)}, P_{f\left(X_{k}\right)}=\frac{1}{M} \sum_{i=1}^{M} \delta_{X_{k i}}$ for $k=1, \ldots, K$, and $\mathcal{B}$ denotes a barycenter loss i.e., USWB, MFSWB, s-MFSWB, us-MFSWB, and es-MFSWB. This setting can be seen as an inverse barycenter problem i.e., the barycenter is fixed and the marginals are learnt under some constraints (e.g., the reconstruction loss and the aggregated distribution loss).

Table 2: Results of training SWAE with different regularization losses.

| Methods | $\mathrm{RL}(\downarrow)$ | $\mathrm{W}_{2, \text { latent }}^{2} \times 10^{2}(\downarrow)$ | $\mathrm{W}_{2, \text { image }}^{2}(\downarrow)$ | $\mathrm{F}_{\text {latent }} \times 10^{2}(\downarrow)$ | $\mathrm{W}_{\text {latent }} \times 10^{2}(\downarrow)$ |
| :--- | :---: | :---: | :---: | :---: | :---: |
| SWAE | 2.95 | 9.41 | 26.91 | 7.05 | 23.86 |
| USWB | 3.15 | 10.46 | 27.41 | 7.02 | 12.73 |
| MFSWB $\lambda=0.1$ | 3.11 | 8.52 | $\mathbf{2 6 . 6 2}$ | 8.66 | 20.01 |
| MFSWB $\lambda=1.0$ | 3.12 | 9.71 | 26.92 | 10.15 | 21.16 |
| MFSWB $\lambda=10.0$ | $\mathbf{2 . 7 9}$ | 10.32 | 26.95 | 8.27 | 23.39 |
| s-MFSWB | 3.22 | 10.50 | 28.69 | 1.30 | 13.71 |
| us-MFSWB | 3.05 | $\mathbf{7 . 7 9}$ | 27.81 | 2.29 | $\mathbf{9 . 1 1}$ |
| es-MFSWB | 3.35 | 9.60 | 28.27 | $\mathbf{0 . 9 8}$ | 9.92 |

Results. We train the autoencoder on MNIST dataset [26] $(d=28 \times 28)$ with $\kappa_{1}=8.0, \kappa_{2}=0.5$, 250 epochs, and $\mu_{0}$ is the uniform distribution on $2 \mathrm{D}$ ball $(h=2)$. We refer the reader to Appendix D for more details of the neural network architectures and the optimizer. Following the training phase, we evaluate the trained autoencoders on the MNIST test set. Similar to previous experiments, we use the metrics $\mathrm{F}$ (denoted as $F_{\text {latent }}$ ) and $\mathrm{W}$ (denoted as $W_{\text {latent }}$ ) in the latent space distributions $f_{\phi} \sharp \mu_{1}, \ldots, f_{\phi} \sharp \mu_{K}$ and the barycenter $\mu_{0}$. We use the reconstruction loss (binary cross-entropy, denoted as RL), the Wasserstein-2 distance between the prior and aggregated posterior distribution in latent space $\mathrm{W}_{2, \text { latent }}^{2}:=W_{2}^{2}\left(\mu_{0}, \frac{1}{K} \sum_{k=1}^{K} f_{\phi} \sharp \mu_{k}\right)$, as well as in image space $\mathrm{W}_{2 \text {,image }}^{2}:=W_{2}^{2}\left(g_{\psi} \sharp \mu_{0}, \frac{1}{K} \sum_{k=1}^{K} \mu_{k}\right)$. During evaluation, we approximate $\mu_{0}$ by its empirical version of 10000 samples. We report the quantitative result in Table 2, and reconstructed images, generated images, and images of latent codes in Figure 14 in Appendix D. From the results, we see that the proposed surrogate MFSWB yield better scores than USWB in all metrics except the generative score i.e., $\mathrm{W}_{2 \text {,image }}^{2}$. The formal MFSWB gives good score in reconstruction loss, $\mathrm{W}_{2 \text {,image }}^{2}$, and $\mathrm{W}_{2 \text {,latent }}^{2}$, however, its $\mathrm{F}$ and $\mathrm{W}$ scores are high. The es-MFSWB is the best variant among the proposed surrogates. Compared with the conventional SWAE, using a barycenter loss leads to a more class-fair latent representation, however, it sacrifices the image reconstructive and generative quality.

## 5 Conclusion

We introduced marginal fairness sliced Wasserstein barycenter (MFSWB), a special case of sliced Wasserstein barycenter (SWB) which has approximately the same distance to marginals. We first defined the MFSWB as a constrainted uniform SWB problem. After that, to overcome the computational drawbacks of the original problem, we propose three surrogate definitions of MFSWB which are hyperparameter-free and easy to compute. We discussed the relationship of the proposed surrogate problems and their connection to the sliced Multi-marginal Wasserstein distance with the maximal ground metric. Finally, we conduct simulations with Gaussian and experiments on 3D point-cloud averaging, color harmonization, and sliced Wasserstein autoencoder with class-fairness representation to show the benefits of the proposed surrogate MFSWB definitions.

## Acknowledgements

We would like to thank Joydeep Ghosh for his insightful discussion during the course of this project.

## Supplement to "Marginal Fairness Sliced Wasserstein Barycenter"

We present skipped proofs in Appendix A. We then provide some additional materials which are mentioned in the main paper in Appendix B. After that, related works are discussed in Appendix C. We then provide additional experimental results in Appendix D. Finally, we report the used computational devices in Appendix E.

## A Proofs

## A. 1 Proof of Proposition 1

Proof. From Definition 2, we have

$$
\begin{aligned}
\mathcal{S F}\left(\mu, \mu_{1: K}\right) & =\max _{k \in\{1, \ldots, K\}} S W_{p}^{p}\left(\mu, \mu_{k}\right) \\
& =\max _{k \in\{1, \ldots, K\}} \mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right]
\end{aligned}
$$

Let $k^{\star}=\arg \max _{k \in\{1, \ldots, K\}} \mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right]$, we have

$$
\begin{aligned}
\mathcal{S F}\left(\mu, \mu_{1: K}\right) & =\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k^{\star}}\right)\right] \\
& \leq \mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right] \\
& =\mathcal{U S F}\left(\mu, \mu_{1: K}\right),
\end{aligned}
$$

as from Definition 3, which completes the proof.

## A. 2 Proof of Proposition 2

Using the Holder's inequality, we have:

$$
\begin{aligned}
& \mathbb{E}\left|\nabla_{\phi} \frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right)-\nabla_{\phi} \mathcal{U} \mathcal{F}\left(\mu_{\phi} ; \mu_{1: K}\right)\right| \\
& \leq\left(\mathbb{E}\left|\nabla_{\phi} \frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right)-\nabla_{\phi} \mathcal{U} \mathcal{F}\left(\mu_{\phi} ; \mu_{1: K}\right)\right|^{2}\right)^{\frac{1}{2}} \\
& =\left(\mathbb{E}\left(\nabla_{\phi} \frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right)-\nabla_{\phi} \mathbb{E}\left[\mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k_{\theta}^{\star}}\right)\right]\right)^{2}\right)^{\frac{1}{2}} \\
& =\left(\mathbb{E}\left(\frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right)-\mathbb{E}\left[\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k_{\theta}^{\star}}\right)\right]\right)^{2}\right)^{\frac{1}{2}} \\
& =\left(\operatorname{Var}\left[\frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{\theta_{l}}^{\star}}\right)\right]\right)^{\frac{1}{2}} \\
& =\frac{1}{\sqrt{L}} \operatorname{Var}\left[\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta \sharp \mu_{\phi}, \theta \sharp \mu_{k_{\theta}^{\star}}\right)\right]^{\frac{1}{2}},
\end{aligned}
$$

which completes the proof.

## A. 3 Proof of Proposition 3

We first restate the following Lemma from [36] and provide the proof for completeness.

Lemma 1. For any $L \geq 1,0 \leq a_{1} \leq a_{2} \leq \ldots \leq a_{L}$ and $0<b_{1} \leq b_{2} \leq \ldots \leq b_{L}$, we have:

$$
\begin{equation*}
\frac{1}{L}\left(\sum_{i=1}^{L} a_{i}\right)\left(\sum_{i=1}^{L} b_{i}\right) \leq \sum_{i=1}^{L} a_{i} b_{i} \tag{15}
\end{equation*}
$$

Proof. For $L=1$, we directly have $a_{i} b_{i}=a_{i} b_{i}$. Assuming that for $L$ the inequality holds i.e., $\frac{1}{L}\left(\sum_{i=1}^{L} a_{i}\right)\left(\sum_{i=1}^{L} b_{i}\right) \leq \sum_{i=1}^{L} a_{i} b_{i}$ which is equivalent to $\left(\sum_{i=1}^{L} a_{i}\right)\left(\sum_{i=1}^{L} b_{i}\right) \leq L \sum_{i=1}^{L} a_{i} b_{i}$. Now, we show that $\frac{1}{L}\left(\sum_{i=1}^{L} a_{i}\right)\left(\sum_{i=1}^{L} b_{i}\right) \leq \sum_{i=1}^{L} a_{i} b_{i}$ i.e., the inequality holds for $L+1$. We have

$$
\begin{aligned}
\left(\sum_{i=1}^{L+1} a_{i}\right)\left(\sum_{i=1}^{L+1} b_{i}\right) & =\left(\sum_{i=1}^{L} a_{i}\right)\left(\sum_{i=1}^{L} b_{i}\right)+\left(\sum_{i=1}^{L} a_{i}\right) b_{L+1}+\left(\sum_{i=1}^{L} b_{i}\right) a_{L+1}+a_{L+1} b_{L+1} \\
& \leq L \sum_{i=1}^{L} a_{i} b_{i}+\left(\sum_{i=1}^{L} a_{i}\right) b_{L+1}+\left(\sum_{i=1}^{L} b_{i}\right) a_{L+1}+a_{L+1} b_{L+1}
\end{aligned}
$$

Since $a_{L+1} b_{L+1}+a_{i} b_{i} \geq a_{L+1} b_{i}+b_{L+1} a_{i}$ for all $1 \leq i \leq L$ by rearrangement inequality. By taking the sum of these inequalities over $i$ from 1 to $L$, we obtain:

$$
\left(\sum_{i=1}^{L} a_{i}\right) b_{L+1}+\left(\sum_{i=1}^{L} b_{i}\right) a_{L+1} \leq \sum_{i=1}^{L} a_{i} b_{i}+L a_{L+1} b_{L+1}
$$

Then, we have

$$
\begin{aligned}
\left(\sum_{i=1}^{L+1} a_{i}\right)\left(\sum_{i=1}^{L+1} b_{i}\right) & \leq L \sum_{i=1}^{L} a_{i} b_{i}+\left(\sum_{i=1}^{L} a_{i}\right) b_{L+1}+\left(\sum_{i=1}^{L} b_{i}\right) a_{L+1}+a_{L+1} b_{L+1} \\
& \leq L \sum_{i=1}^{L} a_{i} b_{i}+\sum_{i=1}^{L} a_{i} b_{i}+L a_{L+1} b_{L+1}+a_{L+1} b_{L+1} \\
& =(L+1)\left(\sum_{i=1}^{L+1} a_{i} b_{i}\right)
\end{aligned}
$$

which completes the proof.

Now, we go back to the main inequality which is $\mathcal{U S F}\left(\mu ; \mu_{1: K}\right) \leq \mathcal{E S \mathcal { F }}\left(\mu ; \mu_{1: K}\right)$. From Definition 5 , we have:

$$
\begin{aligned}
\mathcal{E S \mathcal { F }}\left(\mu ; \mu_{1: K}\right) & =\mathbb{E}_{\theta \sim \sigma\left(\theta ; \mu, \mu_{1: K}\right)}\left[\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right] \\
& =\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right) \frac{f_{\sigma}\left(\theta ; \mu, \mu_{1: K}\right)}{\frac{\Gamma(d / 2)}{2 \pi^{d / 2}}}\right]
\end{aligned}
$$

where $f_{\sigma}\left(\theta ; \mu, \mu_{1: K}\right) \propto \exp \left(\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu, \theta \sharp \mu_{k}\right)\right)$. Now, we consider a Monte Carlo estimation of $\mathcal{E} \mathcal{S} \mathcal{F}\left(\mu ; \mu_{1: K}\right)$ by importance sampling:

$$
\widehat{\mathcal{E S F}}\left(\mu ; \mu_{1: K}, L\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{l} \sharp \mu, \theta_{l} \sharp \mu_{k}\right) \frac{\exp \left(\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{l} \sharp \mu, \theta_{l} \sharp \mu_{k}\right)\right)}{\sum_{i=1}^{L} \exp \left(\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{i} \sharp \mu, \theta_{i} \sharp \mu_{k}\right)\right)}\right],
$$

where $\theta_{1}, \ldots, \theta_{L} \stackrel{\text { i.i.d }}{\sim} \mathcal{U}\left(\mathbb{S}^{d-1}\right)$. Similarly, we consider a Monte Carlo estimation of $\mathcal{U} \mathcal{S} \mathcal{F}\left(\mu ; \mu_{1: K}\right)$ :

$$
\widehat{\mathcal{U S F}}\left(\mu ; \mu_{1: K}, L\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{l} \sharp \mu, \theta_{l} \sharp \mu_{k}\right)\right]
$$

for the same set of $\theta_{1}, \ldots, \theta_{L}$. Without losing generality, we assume that $\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{1} \sharp \mu, \theta_{1} \sharp \mu_{k}\right) \leq$ $\ldots \leq \max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{L} \sharp \mu, \theta_{L \sharp} \mu_{k}\right)$.

Let $\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{i} \sharp \mu, \theta_{i} \sharp \mu_{k}\right)=a_{i}$ and $\exp \left(\max _{k \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta_{i} \sharp \mu, \theta_{i} \sharp \mu_{k}\right)\right)=b_{i}$, applying Lemma 1, we have:

$$
\widehat{\mathcal{U S F}}\left(\mu ; \mu_{1: K}, L\right) \leq \widehat{\mathcal{E S F}}\left(\mu ; \mu_{1: K}, L\right) \quad \forall L \geq 1
$$

By letting $L \rightarrow \infty$ and applying the law of large numbers, we obtain:

$$
\mathcal{U S F}\left(\mu ; \mu_{1: K}\right) \leq \mathcal{E S \mathcal { F }}\left(\mu ; \mu_{1: K}\right)
$$

which completes the proof.

## A. 4 Proof of Proposition 4

We first recall the definition of the SMW with the maximal ground metric:

$$
S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right)=\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int_{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right]
$$

Non-negativity. Since $\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} \geq 0$ for any $x_{1}, \ldots, x_{K}$ and for any $\theta$, we can obtain the desired property $S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) \geq 0$ which implies $S M W_{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) \geq$ 0 .

Marginal Exchangeability. For any permutation $\sigma:[[K]] \rightarrow[[K]]$, we have:

$$
\begin{aligned}
S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) & =\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& =\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{\sigma(1)}, \ldots, \mu_{\sigma(K)}\right)} \int_{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& =S M W_{p}^{p}\left(\mu_{\sigma(1)}, \ldots, \mu_{\sigma(K)} ; c\right) .
\end{aligned}
$$

Generalized Triangle Inequality. For $\mu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, we have :

$$
\begin{aligned}
& S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) \\
& =\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& \leq \mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int \sum_{k=1}^{K} \max _{i \in\{1, \ldots, K\} \backslash k k, j \in\{1, \ldots, K\} \backslash\{k\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& =\mathbb{E}\left[\max _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \sum_{k=1}^{K} \int \theta_{i \in\{1, \ldots, K\} \backslash\{k\}, j \in\{1, \ldots, K\} \backslash\{k\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& =\mathbb{E}\left[\sum_{k=1}^{K} \int_{i \in\{1, \ldots, K\} \backslash\{k\}, j \in\{1, \ldots, K\} \backslash\{k\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi^{\star}\left(x_{1}, \ldots, x_{k-1}, x_{k+1}, \ldots x_{K}\right)\right]
\end{aligned}
$$

for $\pi^{\star}$ is the optimal multi-marginal transportation plan and $\pi^{\star}\left(x_{1}, \ldots, x_{k-1}, x_{k+1}, x_{K}\right)$ is the marginal joint distribution by integrating out $x_{k}$. By the gluing lemma [38], there exists optimal
plans $\pi^{\star}\left(x_{1}, \ldots, x_{k-1}, y, x_{k+1}, x_{K}\right)$ for any $k \in[[K]]$ and $y$ follows $\mu$. We further have:

$$
\begin{aligned}
& S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) \\
& \leq \mathbb{E}\left[\sum _ { k = 1 } ^ { K } \int \operatorname { m a x } \left(\max _{i \in\{1, \ldots, K\} \backslash\{k\}, j \in\{1, \ldots, K\} \backslash\{k\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p}\right.\right. \\
& \left.\left.\quad \max _{i \in\{1, \ldots, K\} \backslash\{k\}}\left|\theta^{\top} x_{i}-\theta^{\top} y\right|^{p}\right) d \pi^{\star}\left(x_{1}, \ldots, x_{k-1}, y, x_{k+1}, \ldots x_{K}\right)\right] \\
& =\sum_{k=1}^{K} \mathbb{E}\left[\max _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{k-1}, \mu, \mu_{k+1}, \ldots, \mu_{K}\right)} \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& =\sum_{k=1}^{K} S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{k-1}, \mu, \mu_{k+1}, \ldots, \mu_{K} ; c\right)
\end{aligned}
$$

Applying the Minkowski's inequality, we obtain the desired property:

$$
S M W_{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) \leq \sum_{k=1}^{K} S M W_{p}\left(\mu_{1}, \ldots, \mu_{k-1}, \mu, \mu_{k+1}, \ldots, \mu_{K} ; c\right)
$$

Identity of Indiscernibles. From the proof in Appendix A.5, we have:

$$
\begin{aligned}
S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) & \geq \mathbb{E}\left[\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu_{i}, \theta \sharp \mu_{j}\right)\right] \\
& \geq \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} \mathbb{E}\left[W_{p}^{p}\left(\theta \sharp \mu_{i}, \theta \sharp \mu_{j}\right)\right] \\
& =\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} S W_{p}^{p}\left(\mu_{i}, \mu_{j}\right) .
\end{aligned}
$$

Therefore, when $S M W_{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right)=0$, we have $S W_{p}^{p}\left(\mu_{i}, \mu_{j}\right)=0$ which implies $\mu_{i}=\mu_{j}$ for any $i, j \in[[K]]$. As a result, $\mu_{1}=\ldots=\mu_{K}$ from the metricity of the $\mathrm{SW}$ distance. For the other direction, it is easy to see that if $\mu_{1}=\ldots \mu_{K}$, we have $S M W_{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right)=0$ based on the definition and the metricity of the Wasserstein distance.

## A. 5 Proof of Proposition 5

Given the maximal ground metric $c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)=\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|$, from Equation 6

$$
\begin{aligned}
S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) & =\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& =\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right]
\end{aligned}
$$

By Jensen inequality i.e., $\left(x_{1}, \ldots, x_{K}\right) \rightarrow \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p}$ is a convex function, we have:

$$
S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) \geq \mathbb{E}\left[\max _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} \int\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right]
$$

```
Algorithm 1 Computational algorithm of the SWB problem
    Input: Marginals $\mu_{1}, \ldots, \mu_{K}, p \geq 1$, weights $\omega_{1}, \ldots, \omega_{K}$, the number of projections $L$, step size $\eta$,
    the number of iterations $T$.
    Initialize the barycenter $\mu_{\phi}$
    for $t=1$ to $T$ do
        Set $\nabla_{\phi}=0$
        Sample $\theta_{l} \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)$
        for $l=1$ to $L$ do
            for $k=1$ to $K$ do
                Set $\nabla_{\phi}=\nabla_{\phi}+\nabla_{\phi} \frac{\omega_{k}}{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k}\right)$
            end for
        end for
        $\phi=\phi-\eta \nabla_{\phi}$
    end for
    Return: $\mu_{\phi}$
```

Using max-min inequality, we have:

$$
\begin{aligned}
S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) & \geq \mathbb{E}\left[\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} \inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right] \\
& \geq \mathbb{E}\left[\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} \inf _{\pi \in \Pi\left(\mu_{i}, \mu_{j}\right)} \int\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|^{p} d \pi\left(x_{i}, x_{j}\right)\right] \\
& =\mathbb{E}\left[\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu_{i}, \theta \sharp \mu_{j}\right)\right] .
\end{aligned}
$$

Therefore, minimizing two sides with respect to $\mu_{1}$, we have:

$$
\begin{aligned}
\min _{\mu_{1}} S M W_{p}^{p}\left(\mu_{1}, \ldots, \mu_{K} ; c\right) & \geq \min _{\mu_{1}} \mathbb{E}\left[\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu_{i}, \theta \sharp \mu_{j}\right)\right] \\
& \geq \min _{\mu_{1}} \mathbb{E}\left[\max _{i \in\{2, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu_{1}, \theta \sharp \mu_{i}\right)\right] \\
& =\min _{\mu_{1}} \mathcal{U S F}\left(\mu_{1} ; \mu_{2: K}\right),
\end{aligned}
$$

which completes the proof.

## B Additional Materials

Algorithms. As mentioned in the main paper, we present the computational algorithm for SWB in Algorithm 1, for s-MFSWB in Algorithm 2, for us-MFSWB in Algorithm 3, and for es-MFSWB in Algorithm 4.

Energy-based sliced Multi-marginal Wasserstein. As shown in Proposition 5, us-MFSWB is equivalent to minimizing a lower bound of SMW with the maximal ground metric. We now show that es-MFSWB is also equivalent to minimizing a lower bound of a variant of SMW i.e., Energy-based sliced Multi-marginal Wasserstein with the maximal ground metric. We refer the

```
Algorithm 2 Computational algorithm of the s-MFSWB problem
    iterations $T$.
    Initialize the barycenter $\mu_{\phi}$
    for $t=1$ to $T$ do
        Set $\nabla_{\phi}=0$
        Sample $\theta_{l} \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)$
        $k^{\star}=1$
        for $k=1$ to $K$ do
            for $l=1$ to $L$ do
                if $\frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k}\right)>\frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k^{\star}}\right)$ then
                $k^{\star}=k$
            end if
                end for
        end for
        $\nabla_{\phi}=\nabla_{\phi}+\frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi} \mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k^{\star}}\right)$
        $\phi=\phi-\eta \nabla_{\phi}$
    end for
    Return: $\mu_{\phi}$
```

Input: Marginals $\mu_{1}, \ldots, \mu_{K}, p \geq 1$ the number of projections $L$, step size $\eta$, the number of

Algorithm 3 Computational algorithm of the us-MFSWB problem

```
iterations $T$.
    Initialize the barycenter $\mu_{\phi}$
    for $t=1$ to $T$ do
        Set $\nabla_{\phi}=0$
        Sample $\theta_{l} \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)$
        for $l=1$ to $L$ do
            $k_{l}^{\star}=1$
            for $k=2$ to $K$ do
                if $\mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k}\right)>\mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{l}^{*}}\right)$ then
                $k_{l}^{\star}=k$
            end if
            end for
            $\nabla_{\phi}=\nabla_{\phi}+\nabla_{\phi} \frac{1}{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{l}^{*}}\right)$
        end for
        $\phi=\phi-\eta \nabla_{\phi}$
    end for
    Return: $\mu_{\phi}$
```

Input: Marginals $\mu_{1}, \ldots, \mu_{K}, p \geq 1$ the number of projections $L$, step size $\eta$, the number of

reader to Proposition 6 for a detailed definition. The proof of Proposition 6 is similar to the proof of Proposition 5 in Appendix A.5.

Proposition 6. Given $K \geq 2$ marginals $\mu_{1}, \ldots, \mu_{K} \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$, the maximal ground metric

```
Algorithm 4 Computational algorithm of the es-MFSWB problem
    iterations $T$.
    Initialize the barycenter $\mu_{\phi}$
    for $t=1$ to $T$ do
        Set $\nabla_{\phi}=0$
        Sample $\theta_{l} \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)$
        for $l=1$ to $L$ do
            $k_{l}^{\star}=1$
            for $k=2$ to $K$ do
                if $\mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k}\right)>\mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{l}^{*}}\right)$ then
                    $k_{l}^{\star}=k$
                end if
            end for
        end for
        for $l=1$ to $L$ do
            $w_{l, \phi}=\frac{\exp \left(\mathrm{W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{l}^{\star}}\right)\right)}{\sum_{j=1}^{L} \exp \left(\mathrm{W}_{p}^{p}\left(\theta_{j} \sharp \mu_{\phi}, \theta_{j} \sharp \mu_{k_{j}^{\star}}\right)\right)}$
        end for
        $\nabla_{\phi}=\nabla_{\phi}+\nabla_{\phi} \frac{w_{l, \phi}}{L} \mathrm{~W}_{p}^{p}\left(\theta_{l} \sharp \mu_{\phi}, \theta_{l} \sharp \mu_{k_{l}^{\star}}\right)$
        $\phi=\phi-\eta \nabla_{\phi}$
    end for
    Return: $\mu_{\phi}$
```

Input: Marginals $\mu_{1}, \ldots, \mu_{K}, p \geq 1$ the number of projections $L$, step size $\eta$, the number of

$c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)=\max _{i \in\{1, \ldots, K\}, j \in\{1, \ldots, K\}}\left|\theta^{\top} x_{i}-\theta^{\top} x_{j}\right|$, we have:

$\min _{\mu_{1}} \mathcal{E S F}\left(\mu_{1} ; \mu_{2: K}\right) \leq \min _{\mu_{1}} E S M W_{p}^{p}\left(\mu_{1}, \mu_{2}, \ldots, \mu_{K} ; c\right)$,

where

$$
E S M W_{p}^{p}\left(\mu_{1}, \mu_{2}, \ldots, \mu_{K} ; c\right)=\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \ldots, \mu_{K}\right)} \int c\left(\theta^{\top} x_{1}, \ldots, \theta^{\top} x_{K}\right)^{p} d \pi\left(x_{1}, \ldots, x_{K}\right)\right]
$$

and the expectation is with respect to $\sigma(\theta)$ i.e.,

$$
f_{\sigma}\left(\theta ; \mu_{1}, \mu_{2: K}\right) \propto \exp \left(\max _{k \in\{2, \ldots, K\}} W_{p}^{p}\left(\theta \sharp \mu_{1}, \theta \sharp \mu_{k}\right)\right) .
$$

## C Related Works

Demographic fairness with Wasserstein Barycenter. A connection between fair regression and one-dimensional Wasserstein barycenter is established by deriving the expression for the optimal function minimizing squared risk under Demographic Parity constraints [8]. Similarly, Demographic Parity fair classification is connected to one-dimensional Wasserstein-1 distance barycenter in [21]. The work [20] extends the Demographic Parity constraint to multi-task problems for regression and classification and connects them to the one-dimensional Wasserstein- 2 distance barycenters.
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-21.jpg?height=836&width=1616&top_left_y=240&top_left_x=257)

Figure 5: Barycenters from MFSWB with $\lambda=0.1$ and $\lambda=10$ along gradient iterations with the corresponding F-metric and W-metric.

A method to augment the input so that predictability of the protected attribute is impossible, by using Wasserstein-2 distance Barycenters to repair the data is proposed in [17]. A general approach for using one-dimensional Wasserstein-1 distance barycenter to obtain Demographic Parity in classification and regression is proposed in [42]. Overall, all discussed works define fairness in terms of Demographic Parity constraints in applications with a response variable (classification and regression) in one dimension. In contrast, we focus on marginal fairness barycenter i.e., using a set of measures only, in any dimensions.

Other possible applications. Wasserstein barycenter has been used to cluster measures in [47]. In particular, a K-mean algorithm for measures is proposed with Wasserstein barycenter as the averaging operator. Therefore, our MFSWB can be directly used to enforce the fairness for averaging inside each cluster. The proposed MFSWB can be also used to average meshes by changing the SW to H2SW which is proposed in [34].

## D Additional Experiments

Gaussians barycenter with the formal MFSWB. We present the result of finding barycenters of Gaussian distributions with MFSWB $\lambda=0.1$ and $\lambda=10$ in Figure 5.

Point-cloud averaging. We report the averaging results of two point-clouds of plane shapes $\mathrm{n}$ Figure 6 and the corresponding F-metrics and W-metric along iterations in Table 3. We see that the proposed surrogates achieve better F-metric and W-metric than the USWB. In this case, us-MFSWB gives the best F-metric at the final epoch, however, es-MFSWB also gives a comparable performance and performs better at earlier epochs. For the formal MFSWB, it does not perform well with the chosen set of $\lambda$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-22.jpg?height=678&width=1630&top_left_y=241&top_left_x=274)

UsW

Figure 6: Averaging point-clouds with USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB.

Table 3: F-metric and W-metric along iterations in point-cloud averaging application.

| Method | Iteration 0 |  | Epoch 1000 |  | Epoch 5000 |  | Epoch 10000 |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{F}(\downarrow)$ | $\mathrm{W}(\downarrow)$ | F $(\downarrow)$ | $\mathrm{W}(\downarrow)$ | F $(\downarrow)$ | $\mathrm{W}(\downarrow)$ | $\mathrm{F}(\downarrow)$ | $\mathrm{W}(\downarrow)$ |
| USWB | $746.67 \pm 0.0$ | $4.71 \pm 0.0$ | $35.22 \pm 1.04$ | $1 \pm 0.54$ | $7.82 \pm 0.26$ | $109.82 \pm 0.28$ | $11.08 \pm 0.06$ | $108.52 \pm 0.17$ |
|  | $746.67 \pm 0.0$ | 4814.71 |  |  |  |  |  | $107.8: \quad$ |
| MFSWB $\lambda$ | $746.67 \pm 0.0$ | $4814.71 \pm 0.0$ | $33.21 \pm 2.72$ | $151.24 \pm 0.64$ | $2.54 \pm 1.5$ | $109.66 \pm 0.26$ | $4.66 \pm 2.1$ | $108.1 \pm 0.05$ |
| $3 \lambda=10$ | $746.67 \pm 0.0$ | $4814.71 \pm 0.0$ | $34.03 \pm 22.6$ | $158.66 \pm 1.39$ | $29.19 \pm 14.29$ | $122.66 \pm 0.88$ | $20.55 \pm 13.57$ | $123.65 \pm 1.52$ |
| s-MFSWB | $746.67 \pm 0.0$ | $4814.71 \pm 0.0$ | $36.23 \pm 1.88$ | $154.4 \pm 0.67$ | $0.66 \pm 0.44$ | $109.17 \pm 0.34$ | $2.54 \pm 2.06$ | $107.57 \pm 0.19$ |
| us-MF | $746.67 \pm 0.0$ | $4814.71 \pm 0.0$ | $28.65 \pm 1.37$ | $144.27 \pm 0.65$ | $1.02 \pm 0.8$ | $109.67 \pm 0.1$ | $1.35 \pm 0.77$ | $108.2 \pm 0.19$ |
| es-MFSWB | $746.67 \pm 0.0$ | $4814.71 \pm 0.0$ | $28.05 \pm 1.16$ | $143.24 \pm 0.76$ | $0.99 \pm 0.32$ | $109.68 \pm 0.14$ | $1.36 \pm 0.62$ | $108.28 \pm 0.07$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-22.jpg?height=602&width=1654&top_left_y=1415&top_left_x=253)

Figure 7: Harmonized images from USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB at iteration 5000 .

Color Harmonization. We first present the harmonized images of different methods including USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB at iteration 5000 and 10000 for the demonstrated images in the main text in Figure 7-Figure 8. Moreover, we report the results of MFSWB $(\lambda=0.1,10)$ at iteration 5000,10000 , and 20000 in Figure 9. Similarly, we repeat the same
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-23.jpg?height=608&width=1654&top_left_y=238&top_left_x=256)

Figure 8: Harmonized images from USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB at iteration 10000 .
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-23.jpg?height=568&width=1696&top_left_y=1014&top_left_x=256)

Figure 9: Harmonized images from MFSWB with $\lambda=0.1$ and $\lambda=10$ at iterations 5000, 10000, and 20000.

experiments with flower images in Figure 10- 13. Overall, we see that es-MFSWB helps to reduce both F-metric and W-metric faster than USWB and other surrogates. For the formal MFSWB, the performance depends significantly on the choice of $\lambda$.

Sliced Wasserstein autoencoder with class-fairness representation. We use the RMSprop optimizer with learning rate 0.01 , alpha $=0.99$, eps $=1 e-8$. As mentioned in the main text, we report the used neural network architectures:

```
MNISTAutoencoder
    encoder:
        MNISTEncoder
            features:
                Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                LeakyReLU(negative_slope=0.2, inplace=True)
                Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
```

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-24.jpg?height=599&width=1651&top_left_y=243&top_left_x=259)

USWB $F=4582.919, W=7888.161 \quad$ MFSWB $\lambda=1, F=3562.971, W=7408.885 \quad$ s-MFSWB $F=4074.09, W=7627.971 \quad$ US-MFSWB $F=4277.279, W=7417.909 \quad$ es-MFSWB $F=2269.199, W=5220.856$

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-24.jpg?height=356&width=1651&top_left_y=860&top_left_x=256)

Figure 10: Harmonized images from USWB, MFSWB $(\lambda=1)$ s-MFSWB, us-MFSWB, and es-MFSWB at iteration 5000 .

LeakyReLU(negative_slope=0.2, inplace=True)

AvgPool2d(kernel_size=2, stride=2, padding=0)

Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) LeakyReLU(negative_slope $=0.2$, inplace=True)

Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) LeakyReLU(negative_slope $=0.2$, inplace=True)

AvgPool2d(kernel_size=2, stride=2, padding=0)

Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) LeakyReLU(negative_slope=0.2, inplace=True)

Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) LeakyReLU(negative_slope=0.2, inplace=True)

AvgPool2d(kernel_size=2, stride=2, padding=1)

$f \mathrm{C}$ :

Linear(in_features=1024, out_features=128, bias=True)

ReLU(inplace=True)

Linear(in_features=128, out_features=2, bias=True)

decoder :

MNISTDecoder

$\mathrm{fc}$ :

Linear(in_features=2, out_features=128, bias=True)

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-25.jpg?height=602&width=1651&top_left_y=244&top_left_x=259)

USWB $F=3801.19, W=5446.39 \quad$ MFSWB $\lambda=1, F=1966.134, W=4898.801 \quad s-M F S W B \quad F=2852.586, W=5112.62 \quad$ US-MFSWB $F=3204.296, W=4813.547 \quad$ es-MFSWB $F=1003.603, W=3131.569$

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-25.jpg?height=349&width=1651&top_left_y=861&top_left_x=259)

Figure 11: Harmonized images from USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB at iteration 10000 .

Linear(in_features=128, out_features=1024, bias=True) ReLU(inplace=True)

features:

Upsample(scale_factor=2.0, mode='nearest')

$\operatorname{Conv2d}(64,64$, kernel_size $=(3,3), \operatorname{stride}=(1,1)$, padding=(1, 1))

LeakyReLU(negative_slope $=0.2$, inplace=True)

Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

LeakyReLU(negative_slope $=0.2$, inplace=True)

Upsample(scale_factor=2.0, mode='nearest')

Conv2d(64, 64, kernel_size $=(3,3)$, stride= $(1,1))$

LeakyReLU(negative_slope=0.2, inplace=True)

Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

LeakyReLU(negative_slope=0.2, inplace=True)

Upsample(scale_factor=2.0, mode='nearest')

Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

LeakyReLU(negative_slope=0.2, inplace=True)

Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

LeakyReLU(negative_slope $=0.2$, inplace=True)

Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

We report some randomly selected reconstructed images, some randomly generated images, and

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-26.jpg?height=594&width=1651&top_left_y=245&top_left_x=259)

$U S W B F=2644.279, W=3147.174 \quad$ MFSWB $\lambda=1, F=488.311, W=2766.264 \quad s-M F S W B \quad F=1115.146, W=2900.635 \quad$ uS-MFSWB $F=1567.183, W=2574.847 \quad$ es-MFSWB $F=193.701, W=1884.393$

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-26.jpg?height=349&width=1648&top_left_y=861&top_left_x=257)

Figure 12: Harmonized images from USWB, MFSWB $(\lambda=1)$, s-MFSWB, us-MFSWB, and es-MFSWB at iterations 20000 .

the test latent codes of trained autoencoders in Figure 14. Overall, we observe that the qualitative results are consistent with the quantitive results in Table 2. From the latent spaces, we see that the proposed surrogates help to make the codes of classes have approximately the same structure which do appear in the conventional SWAE's latent codes.

## E Computational Devices

For the Gaussian simulation, point-cloud averaging, and color harmonization, we use a HP Omen 25L desktop for conducting experiments. Additionally, for the Sliced Wasserstein Autoencoder with class-fair representation experiment, we employ the NVIDIA Tesla V100 GPU.
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-27.jpg?height=912&width=1680&top_left_y=820&top_left_x=256)

Figure 13: Color harmonized images from MFSWB with $\lambda=0.1$ and $\lambda=10$ at iterations 5000, 10000, and 20000.

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-28.jpg?height=2073&width=1832&top_left_y=278&top_left_x=233)

| Method | Reconstructed Images |
| :---: | :---: |
| MFSWB $\lambda=10.0$ | 3 6 1 1 1 3 9 5 2 9 <br> 4 5 9 3 9 0 3 5 5 5 <br> 7 2 2 7 1 3 8 9 1 7 <br> 3 3 8 8 7 9 2 2 4 1 <br> 3 3 8 9 5 2 0 6 9 2 <br> 9 1 9 5 7 7 2 8 2 6 <br> 8 5 7 7 9 1 0 1 3 0 <br> 3 0 1 9 9 9 1 8 2 1 <br> 2 9 7 5 9 2 6 4 1 3 <br> 8 2 9 2 0 9 0 0 2 8 |

![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-29.jpg?height=866&width=1408&top_left_y=388&top_left_x=542)

us-MFSWB
![](https://cdn.mathpix.com/cropped/2024_06_04_0f2d56828f1e473a3179g-29.jpg?height=864&width=1396&top_left_y=1371&top_left_x=542)

Figure 14: Reconstructed images, generated images and latent space of all methods.

## References

[1] M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathematical Analysis, 43(2):904-924, 2011. (Cited on page 1.)

[2] E. Anderes, S. Borgwardt, and J. Miller. Discrete Wasserstein barycenters: Optimal transport for discrete data. Mathematical Methods of Operations Research, 84:389-409, 2016. (Cited on page 1.)

[3] F. Bongratz, A.-M. Rickmann, S. PÃ¶lsterl, and C. Wachinger. Vox2cortex: Fast explicit reconstruction of cortical surfaces from $3 \mathrm{~d}$ mri scans with geometric deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20773-20783, 2022. (Cited on page 2.)

[4] N. Bonneel, J. Rabin, G. PeyrÃ©, and H. Pfister. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 1(51):22-45, 2015. (Cited on pages 1, 2, and 3.)

[5] N. Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Paris 11, 2013. (Cited on page 2.)

[6] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. (Cited on page 10.)

[7] K. Choi, A. Grover, T. Singh, R. Shu, and S. Ermon. Fair generative modeling via weak supervision. In International Conference on Machine Learning, pages 1887-1898. PMLR, 2020. (Cited on page 2.)

[8] E. Chzhen, C. Denis, M. Hebiri, L. Oneto, and M. Pontil. Fair regression with Wasserstein barycenters. Advances in Neural Information Processing Systems, 33:7321-7331, 2020. (Cited on page 20.)

[9] S. Claici, E. Chien, and J. Solomon. Stochastic Wasserstein barycenters. In International Conference on Machine Learning, pages 999-1008. PMLR, 2018. (Cited on page 4.)

[10] S. Cohen, A. Terenin, Y. Pitcan, B. Amos, M. P. Deisenroth, and K. Kumar. Sliced multimarginal optimal transport. arXiv preprint arXiv:2102.07115, 2021. (Cited on page 4.)

[11] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pages 2292-2300, 2013. (Cited on page 1.)

[12] M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In International conference on machine learning, pages 685-693. PMLR, 2014. (Cited on page 4.)

[13] J. M. Danskin. The theory of max-min and its application to weapons allocation problems, volume 5. Springer Science \& Business Media, 2012. (Cited on page 5.)

[14] F. Elvander, I. Haasler, A. Jakobsson, and J. Karlsson. Tracking and sensor fusion in direction of arrival estimation using optimal mass transport. In 2018 26th European Signal Processing Conference (EUSIPCO), pages 1617-1621. IEEE, 2018. (Cited on page 1.)

[15] J. Fan, A. Taghvaei, and Y. Chen. Scalable computations of Wasserstein barycenter via input convex neural networks. In International Conference on Machine Learning, pages 1571-1581. PMLR, 2021. (Cited on page 4.)

[16] R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos, K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko, A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1-8, 2021. (Cited on page 8.)

[17] P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M. Loubes. Obtaining fairness using optimal transport theory. In International conference on machine learning, pages 2357-2365. PMLR, 2019. (Cited on page 21.)

[18] K. Grove and H. Karcher. How to conjugate c 1-close group actions. Mathematische Zeitschrift, 132(1):11-20, 1973. (Cited on page 1.)

[19] N. Ho, X. Nguyen, M. Yurochkin, H. H. Bui, V. Huynh, and D. Phung. Multilevel clustering via Wasserstein means. In International Conference on Machine Learning, pages 1501-1509, 2017. (Cited on page 1.)

[20] F. Hu, P. Ratz, and A. Charpentier. Fairness in multi-task learning via W asserstein barycenters. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 295-312. Springer, 2023. (Cited on page 20.)

[21] R. Jiang, A. Pacchiano, T. Stepleton, H. Jiang, and S. Chiappa. Wasserstein fair classification. In Uncertainty in artificial intelligence, pages 862-872. PMLR, 2020. (Cited on page 20.)

[22] S. Kolouri, P. E. Pope, C. E. Martin, and G. K. Rohde. Sliced Wasserstein auto-encoders. In International Conference on Learning Representations, 2018. (Cited on page 11.)

[23] A. Korotin, V. Egiazarian, L. Li, and E. Burnaev. Wasserstein iterative networks for barycenter estimation. Advances in Neural Information Processing Systems, 35:15672-15686, 2022. (Cited on page 4.)

[24] A. Kroshnin, N. Tupitsa, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and C. Uribe. On the complexity of approximating Wasserstein barycenters. In International conference on machine learning, pages 3530-3540. PMLR, 2019. (Cited on page 1.)

[25] M. Kusner, Y. Sun, N. Kolkin, and K. Weinberger. From word embeddings to document distances. In International conference on machine learning, pages 957-966. PMLR, 2015. (Cited on page 1.)

[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. (Cited on page 12.)

[27] T. Manole, S. Balakrishnan, and L. Wasserman. Minimax confidence intervals for the sliced Wasserstein distance. Electronic Journal of Statistics, 16(1):2252-2345, 2022. (Cited on pages 2 and 4.)

[28] E. F. Montesuma and F. M. N. Mboula. Wasserstein barycenter for multi-source domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16785-16793, 2021. (Cited on page 1.)

[29] Y. Mroueh. Wasserstein style transfer. In International Conference on Artificial Intelligence and Statistics, pages 842-852. PMLR, 2020. (Cited on page 1.)

[30] K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. Simsekli. Statistical and topological properties of sliced probability divergences. Advances in Neural Information Processing Systems, 33:20802-20812, 2020. (Cited on page 2.)

[31] K. Nadjahi, A. Durmus, U. Simsekli, and R. Badeau. Asymptotic guarantees for learning generative models with the sliced-Wasserstein distance. In Advances in Neural Information Processing Systems, pages 250-260, 2019. (Cited on page 4.)

[32] K. Nguyen, N. Bariletto, and N. Ho. Quasi-monte carlo for 3d sliced Wasserstein. In The Twelfth International Conference on Learning Representations, 2024. (Cited on page 6.)

[33] K. Nguyen and N. Ho. Energy-based sliced Wasserstein distance. Advances in Neural Information Processing Systems, 2023. (Cited on page 6.)

[34] K. Nguyen and N. Ho. Hierarchical hybrid sliced Wasserstein: A scalable metric for heterogeneous joint distributions. arXiv preprint arXiv:2404.15378, 2024. (Cited on page 21.)

[35] K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to generative modeling. In International Conference on Learning Representations, 2021. (Cited on pages 2,4 , and 6.)

[36] K. Nguyen, S. Zhang, T. Le, and N. Ho. Sliced Wasserstein with random-path projecting directions. International Conference on Machine Learning, 2024. (Cited on page 14.)

[37] S. Nietert, R. Sadhu, Z. Goldfeld, and K. Kato. Statistical, robustness, and computational guarantees for sliced Wasserstein distances. Advances in Neural Information Processing Systems, 2022. (Cited on pages 2 and 4.)

[38] G. PeyrÃ© and M. Cuturi. Computational optimal transport, 2020. (Cited on pages 1 and 16.)

[39] J. Rabin, G. PeyrÃ©, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture mixing. In Scale Space and Variational Methods in Computer Vision: Third International Conference, SSVM 2011, Ein-Gedi, Israel, May 29-June 2, 2011, Revised Selected Papers 3, pages 435-446. Springer, 2012. (Cited on page 1.)

[40] S. Samadi, U. Tantipongpipat, J. H. Morgenstern, M. Singh, and S. Vempala. The price of fair pca: One extra dimension. Advances in neural information processing systems, 31, 2018. (Cited on pages 2 and 5 .)

[41] T. SÃ©journÃ©, F.-X. Vialard, and G. PeyrÃ©. Faster unbalanced optimal transport: Translation invariant sinkhorn and 1-d frank-wolfe. In International Conference on Artificial Intelligence and Statistics, pages 4995-5021. PMLR, 2022. (Cited on page 4.)

[42] C. Silvia, J. Ray, S. Tom, P. Aldo, J. Heinrich, and A. John. A general approach to fairness with optimal transport. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3633-3640, 2020. (Cited on page 21.)

[43] J. Solomon, F. De Goes, G. PeyrÃ©, M. Cuturi, A. Butscher, A. Nguyen, T. Du, and L. Guibas. Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains. ACM Transactions on Graphics (ToG), 34(4):1-11, 2015. (Cited on page 1.)

[44] S. Srivastava, C. Li, and D. B. Dunson. Scalable bayes via barycenter in Wasserstein space. Journal of Machine Learning Research, 19(8):1-35, 2018. (Cited on page 1.)

[45] M. Staib, S. Claici, J. M. Solomon, and S. Jegelka. Parallel streaming Wasserstein barycenters. Advances in Neural Information Processing Systems, 30, 2017. (Cited on page 1.)

[46] S. Sun, T.-T. Le, C. You, H. Tang, K. Han, H. Ma, D. Kong, X. Yan, and X. Xie. Hybrid-csr: Coupling explicit and implicit shape representation for cortical surface reconstruction. arXiv preprint arXiv:2307.12299, 2023. (Cited on page 2.)

[47] Y. Zhuang, X. Chen, and Y. Yang. Wasserstein $k$-means for clustering probability distributions. Advances in Neural Information Processing Systems, 35:11382-11395, 2022. (Cited on page 21.)

