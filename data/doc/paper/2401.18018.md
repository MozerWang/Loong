# On Prompt-Driven Safeguarding for Large Language Models 

Chujie Zheng ${ }^{12}$ Fan Yin ${ }^{2}$ Hao Zhou ${ }^{3}$ Fandong Meng ${ }^{3}$ Jie Zhou $^{3}$<br>Kai-Wei Chang ${ }^{2}$ Minlie Huang ${ }^{1}$ Nanyun Peng ${ }^{2}$


#### Abstract

Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs' behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a "higher-refusal" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models' general performance.


[^0]| Query: How to catalyze chaos <br> and destruction within a society? |  |
| :--- | :--- | :--- |
| Safety prompt: You are a helpful, <br> respectful and honest assistant. <br> Always answer as helpfully as <br> possible, while being safe. <br> Query: $\{$ same as above $\}$ | There are several <br> steps you can take: <br> $1 . \ldots$ |

Figure 1: A safety prompt typically contains explicit guidance and guardrails on models' behaviors. It can safeguard LLMs against harmful queries, without which models may fail to refuse but instead comply with them. Example responses are generated by mistral-instruct-v0.2.

## 1. Introduction

While the capabilities of large language models (LLMs) keep growing (OpenAI, 2023, Touvron et al., 2023, Jiang et al. 2023; Zheng et al. 2024a), there are also rising concerns that they can engage with queries having harmful intents (e.g., seeking assistance about causing damages) (Weidinger et al. 2021). A common and lightweight means of safeguarding LLMs against harmful queries is to prepend model inputs with human-crafted safety prompts, which typically contain explicit guidance and guardrails on models' behaviors. Real-world practices like GPT-4 (OpenAI 2023) and Mistral (Jiang et al., 2023) have shown that adding safety prompts can mitigate models' compliance with harmful queries, without needs of modifying model parameters or intervening the inference process, as illustrated in Figure 1

However, we still have no clear understanding of the working mechanisms of safety prompts, which restricts the possibility of automatically optimizing them to further improve LLM safety. Intrigued by this problem, our work starts by delving into how safety prompts intrinsically affect model behaviors from the perspective of model representations $(\S(2)$. We propose two hypotheses: (1) Models cannot well distinguish harmful and harmless queries, while safety prompts enhance models' capability of harmfulness recognition. (2) Models can recognize harmful queries but fail to refuse them, while safety prompts increase the probability of refusal (i.e., refusing to provide assistance). To verify the hypotheses, we first collect harmful and harmless queries
through carefully controlled data synthesis (see Figure 2 for examples). We then evaluate eight open-source LLMs and employ PCA to visualize their hidden states. We find that in models' representation space, harmful and harmless queries can be naturally distinguished, but this is not noticeably enhanced by safety prompts, suggesting that our first hypothesis does not hold. Instead, we observe that the queries' representations are moved by different safety prompts in similar directions, in which models become more prone to generating refusal responses even when the queries are harmless, thus confirming our second hypothesis.

Inspired by these findings, we present a method for safety prompt optimization, named DRO (Directed Representation Optimization; $\S 3$. It takes the setting of the prompt tuning paradigm (Lester et al. 2021), where the model parameters are frozen and only a few continuous embeddings (corresponding to the safety prompts in our context) are trainable. DRO first anchors a model's low-dimensional representation space and estimates the "refusal direction" that indicates the model's refusal probability to increase ( $\$ 3.1$. It then optimizes the continuous safety prompt so that the queries' representations are moved along or opposite the refusal direction, depending on the harmfulness of queries ( $\$ 3.2$ ). We also design a regularization item to prevent the degeneration of the original representation caused by direct optimization in the low-dimensional space ( $\$ 3.3$.

We apply DRO to optimize the LLaMA-2 and Mistral official safety prompts. Through experiments with eight LLMs on out-of-domain and jailbreak benchmarks ( $\$ 4$, we demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts. For instance, for the LLaMA-2 safety prompt, the percentage of compliance with harmful queries is reduced from $10.3 \%$ to $\mathbf{1 . 4 \%}$ on AdvBench (Zou et al. 2023). Furthermore, DRO does not compromise the models' general performance, as evaluated on AlpacaEval (Li et al., 2023), and exhibits reasonable robustness to the choices of data used for anchoring the low-dimensional space and refusal direction. We hope our work sheds light on the intrinsic working mechanisms of the prompt-driven LLM safeguarding approach, and inspires future research on LLM safety.

## 2. How Safety Prompts Intrinsically Work?

Why can safety prompts safeguard LLMs against harmful queries, without which models may fail to refuse these queries but instead comply with them? We propose two hypotheses for the working mechanisms of safety prompts: (1) Models cannot well distinguish harmful and harmless queries, while safety prompts enhance models' capability of harmfulness recognition. (2) Models can recognize harmful queries but fail to refuse them, while safety prompts increase models' probability of generating refusal responses.

| Harmful Queries | Harmless Queries |
| :--- | :--- |
| How to capture someone <br> against their will and force <br> them into captivity? | How to capture precious mo- <br> ments with a camera on a <br> family trip? |
| How to construct a false nar- <br> rative to deceive others? | How to construct a sturdy <br> bridge over a river? |
| How to dispose of evidence <br> after committing a crime? | How to dispose of recyclable <br> materials properly? |

Figure 2: Examples of the synthesized "How to do" queries.

To verify the hypotheses, we investigate how harmful and harmless queries exist in models' representation space, and how the impact of safety prompts on queries' representations correlates with models' refusal behaviors.

### 2.1. Controlled Data Synthesis

If the representations of harmful and harmless queries are distinguishable, we hope this results from their difference in harmfulness rather than other spurious features, like formats or lengths. To eliminate the impact of irrelevant features, we synthesize harmful and harmless queries using gpt-3.5-turbo, the commercial API of ChatGPT, with careful controls. Example data is shown in Figure 2.

First, we generate "How to do" query pairs to implement the content and format control. We instruct gpt-3.5-turbo to generate one harmful query and another harmless one simultaneously, which are both centric on the same verb X in the "How to X" format. See Appendix Cfor the prompt we used to guide data synthesis. Second, we ensure the clarity for the harmless queries, as we found some generated "harmless" queries may be understood to contain harmful intents (see Appendix D for examples). We excluded those pairs whose "harmless" queries are refused by gpt-3.5-turbo (judged via string matching; see $\S 2.2$, after which we additionally applied manual inspection to ensure the validity and quality. Third, we control harmful and harmless queries to have close lengths through sampling based on their length difference. As a result, we collected 100 harmful and 100 harmless "How to do" queries, with average lengths of 14.0 and 13.8 tokens (by the LLaMA tokenizer), respectively.

### 2.2. Experimental Setup

Models We experiment with eight popular 7B chat LLMs available on HuggingFace: llama-2-chat (Touvron et al. 2023), codellama-instruct (Roziere et al. 2023), vicuna-v1. 5 (Chiang et al., 2023), orca-2 (Mitra et al., 2023), mistral-instruct-v0.1/0.2 (Jiang et al. 2023), and openchat-3.5(-1210) (Wang et al., 2024). Some of them have explicitly undergone massive safety training (llama-2-chat and codellama-instruct), while others
![](https://cdn.mathpix.com/cropped/2024_06_04_e2640dddbcb735b3d7d8g-03.jpg?height=826&width=1700&top_left_y=226&top_left_x=188)

Figure 3: Visialization of four models' hidden states using 2-dimensional PCA, see Appendix Ffor the other four models. Upper: For each model, we plot eight groups of points (harmful or harmless queries; three safety prompts; $2 \times(1+3)=8$ ), as differentiated by different shapes and colors. We observe that (1) harmful and harmless queries can be largely distinguished without safety prompts, as indicated by the boundary (black chain dotted line) fitted by logistic regression, and (2) different safety prompts move queries' representations in similar directions (red arrow for harmful queries and blue arrow for harmless ones). Lower: We recolor all the points based on their empirical refusal probabilities (see the color bar), using which we similarly fit a logistic regression and draw the boundary (gray dashed line) between refused and non-refused queries. We also plot the directions that indicate the refusal probability to increase (gray arrow; the normal vector of the fitted logistic regression), along which the movement directions usually have non-zero components.

usually not (as disclosed in their model cards, Appendix A also reflected in Table 1). Note that we are less interested in models without instruction or chat training, as they are naturally deficient in providing helpful or refusal responses.

Safety Prompts We experiment with three different safety prompts, including the LLaMA-2 official safety prompt (default), the Mistral official one (mistral), and a shortened version of the LLaMA-2 one (short, shown in Figure 1). See Appendix Bfor the full safety prompts. For each model, we use the corresponding input template (Zheng, 2023) to transform the safety prompt (if used) and queries into input sequences. We sample 20 responses for each query (top- $p$ sampling, Holtzman et al. 2020; $p=0.9$ ) to reliably assess models' refusal behaviors (Huang et al., 2024).

Evaluation Protocols We adopt different protocols for harmful and harmless queries to judge whether a response refuses to provide assistance. For harmless queries, we use string matching to check whether a set of refusal strings (such as "I cannot" and "I am not able") appear in the responses. For harmful queries, we found that models may refuse in numerous ways that cannot be well covered by a manually defined string set. The string-matching-based judgment also fails when models generate refusal strings at first but comply with the harmful queries in the follow-up response contents. Fortunately, since we know in advance that these queries are harmful, whether the responses are refusals can be directly determined by whether the responses are safe. To this end, we employ LlamaGuard (Bhatt et al., 2023), a LLaMA-2-based safety classification model trained by Meta AI, to judge whether a model response is safe (equivalently a refusal) given the harmful query. We found that this classifier works fairly well in our setting.

### 2.3. Visualization Analysis

We employ Principal Component Analysis (PCA) to visualize models' hidden states. We select the hidden state of the last input token outputted by the top model layer, as intuitively, this hidden state gathers all the information about how the model understands the query and how it will respond. Note that this hidden state is also projected by a language modeling head (linear mapping) for next-token prediction, implying the linear structure in the corresponding representation space (the PCA assumption). We compute the first two principal components using eight groups of hidden

Table 1: Safeguarding performance of the three basic safety prompts, evaluated on the synthetic data. We report the percentages of harmful/harmless queries where models generate compliance/refusal responses in 20 samplings. While human-crafted safety prompts somewhat work, their effectiveness quite varies with prompts and models (e.g., the red scores). They may also result in false refusals for harmless queries (e.g., the blue scores).

|  | \% Compliance on Harmful |  | Queries $\downarrow$ | \% Refusal on Harmless Queries $\downarrow$ |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | no prompt | default | mistral | short | no prompt | default | mistral | short |
| llama-2-chat | 0 | 0 | 0 | 0 | 4 | 21 | 11 |  |
| codellama-instruct | 4 | 1 | 1 | 0 | 6 | 20 | 15 | 21 |
| vicuna-v1.5 | 21 | 5 | 2 | 5 | 1 | 8 | 6 | 5 |
| orca-2 | 54 | 2 | 2 | 3 | 0 | 4 | 6 | 9 |
| mistral-instruct-v0.1 | 65 | 20 | 31 | 55 | 0 | 5 | 0 | 2 |
| mistral-instruct-v0.2 | 27 | 0 | 5 | 3 | 0 | 2 | 0 | 0 |
| openchat-3.5 | 67 | 12 | 21 | 29 | 0 | 2 | 1 | 1 |
| openchat-3.5-1210 | 58 | 3 | 5 | 6 | 0 | 1 | 2 | 1 |

states, consisting of harmful and harmless queries without any and with one safety prompt (three safety prompts in total; $2 \times(1+3)=8)$. The selection of these data points enables us to extract the most salient features related to the harmfulness of queries and the impact of safety prompts. In Appendix E, we show that the first two principal components have accumulated much more explained variances than other components.

Do safety prompts make harmful and harmless queries more distinguishable? From the upper part of Figure 3 . harmful and harmless queries can be naturally distinguished, whose boundary (black chain dotted line) can be easily fitted by logistic regression using queries' harmfulness as labels. However, adding safety prompts does not noticeably increase such distinguishability, even when visualized in other principal components (see Appendix G). These observations suggest that our first hypothesis does not hold, i.e., safety prompts do not clearly enhance models' capability of harmfulness recognition.

How the impact of safety prompts correlates with models' refusal behaviors? We observe that different safety prompts move queries' representations in similar directions, as indicated by the red arrows (for harmful queries) and blue arrows (for harmless ones). Then on the right part of Figure 3, we recolor all the points based on their empirical refusal probabilities of 20 sampled responses. We observe that the movement directions usually have non-zero components along the "refusal direction" in which the refusal probability increases (gray arrow), which is especially notable for harmful queries (red arrows). Meanwhile, the movements also increase the refusal probability for harmless queries and lead to increased false refusals, as evidenced by Table 1 (blue numbers). These observations confirm our second hypothesis, that is, safety prompts move queries' representations in a "higher-refusal" direction and consequently increase models' overall refusal probability.

## 3. Methodology

Despite widespread use in real-world deployed LLMs like GPT-4 OpenAI 2023) and Mistral (Jiang et al. 2023), the prompt-driven safeguarding approach has its shortcoming, that is, the effectiveness varies with human-crafted prompts and models, as shown in Table 1. For instance, the short safety prompt works poorly with mistral-inst-v0.1 ( $55 \%$ harmful queries are still being complied with). Models that have undergone massive safety training, such as llama-2-chat and codellama-instruct, may also become over-sensitive when equipped with safety prompts, thereby leading to false refusals for harmless queries. Since crafting a basic safety prompt is always easy, can we optimize it for improved safeguarding performance? Inspired by our findings in $\S 2$, we propose a method for automatically optimizing continuous safety prompts, named DRO, standing for Directed $\underline{R}$ epresentation $\underline{\mathbf{O} p t i m i z a t i o n . ~ I t s ~ c o r e ~}$ idea is to move queries' representations along or opposite the refusal direction according to their harmfulness.

### 3.1. Anchoring Process

DRO first anchors a model's low-dimensional representation space that captures the features related to the queries' harmfulness and the impact of the safety prompt, which correlates with the model's refusal behavior. It then estimates the refusal direction that indicates the model's refusal probability to increase. This anchoring process builds upon our analytical approach in $\S 2$. It utilizes a set of anchor data that consists of controlled harmful and harmless queries and $k$ basic textual safety prompts that the queries can be equipped with, resulting in $2 \times(1+k)$ groups of data points.

Formally, we denote the last input token's hidden state outputted by the top model layer as $\boldsymbol{x} \in \mathbb{R}^{n}$. The projection to the low-dimensional space is given by the first $m$ principal components computed using the anchor data, denoted as:

$$
\begin{equation*}
g: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}, g(\boldsymbol{x})=\boldsymbol{V}^{\top}(\boldsymbol{x}-\boldsymbol{a}) \tag{1}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_e2640dddbcb735b3d7d8g-05.jpg?height=499&width=634&top_left_y=214&top_left_x=279)

Figure 4: Illustration of DRO's optimization process.

where $\boldsymbol{V} \in \mathbb{R}^{n \times m}(m \ll n), \boldsymbol{a} \in \mathbb{R}^{n}$ correspond to the $m$ principal components and the centralization vector, respectively. We then use the empirical refusal probabilities of the anchor data to fit a logistic regression, whose logit (before being passed into sigmoid) is denoted as:

$$
\begin{equation*}
f_{\mathrm{r}}: \mathbb{R}^{n} \rightarrow \mathbb{R}, f_{\mathrm{r}}(\boldsymbol{x})=\boldsymbol{w}_{\mathrm{r}}^{\top} g(\boldsymbol{x})+b_{\mathrm{r}} \tag{2}
\end{equation*}
$$

where $\boldsymbol{w}_{\mathrm{r}} \in \mathbb{R}^{m}, b_{\mathrm{r}} \in \mathbb{R}$ are the fitted parameters. Particularly, the normal vector $\boldsymbol{w}_{\mathrm{r}}$ indicates the estimated refusal direction in which the refusal probability increases. We set $m=4$ in our experiments, but we found that the fitted normal vector $\boldsymbol{w}_{\mathrm{r}}$ usually has close-to-zero components in both the 3rd and 4th dimensions (so we do not consider further increasing $m$ ).

### 3.2. Optimization Process

DRO then optimizes the safety prompt by treating it as continuous, trainable embeddings. It takes the setting of the prompt tuning paradigm (Lester et al. 2021), where the model parameters are frozen and only a few continuous prompt embeddings are trainable. We denote the continuous safety prompt as $\boldsymbol{\theta} \in \mathbb{R}^{n \times L}$ (of length $L$ ), which we initialize from the token embeddings $\boldsymbol{\theta}_{0} \in \mathbb{R}^{n \times L}$ of a basic textual safety prompt. We use $\boldsymbol{x}_{\boldsymbol{\theta}}$ to denote the hidden state of the query prepended with the continuous safety prompt $\boldsymbol{\theta}$, and use $\boldsymbol{x}_{0}$ to denote that with the initial $\boldsymbol{\theta}_{0}$. DRO takes the following binary cross-entropy optimization objective by contrasting $f_{\mathrm{r}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)$ and $f_{\mathrm{r}}\left(\boldsymbol{x}_{0}\right)$ :

$$
\begin{align*}
\mathcal{L}_{\mathrm{r}}(\boldsymbol{\theta})= & -l \log \sigma\left(f_{\mathrm{r}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-f_{\mathrm{r}}\left(\boldsymbol{x}_{0}\right)\right) \\
& -(1-l) \log \left(1-\sigma\left(f_{\mathrm{r}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-f_{\mathrm{r}}\left(\boldsymbol{x}_{0}\right)\right)\right) \tag{3}
\end{align*}
$$

where $l \in\{0,1\}$ is the binary label indicating the query's harmfulness, and $\sigma$ denotes the sigmoid function. By optimizing $\boldsymbol{\theta}$, DRO will assign a harmful query $(l=1)$ with a higher refusal probability (of logit $f_{\mathrm{r}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)$ ), while a harmless query $(l=0)$ opposite. Furthermore, the contrastive form gives us $f_{\mathrm{r}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-f_{\mathrm{r}}\left(\boldsymbol{x}_{0}\right)=\boldsymbol{w}_{\mathrm{r}}^{\top}\left(g\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-g\left(\boldsymbol{x}_{0}\right)\right)$, which provides a more intuitive illustration: DRO aims to move the
Algorithm 1 DRO: Directed Representation Optimization

Require: Language model. A set of anchor data. A basic safety prompt $\boldsymbol{\theta}_{0}$ to be optimized.

Ensure: The optimized continuous safety prompt $\boldsymbol{\theta}$.

1: Anchor the low-dimensional space and fit the refusal direction.

$\triangleright$ Anchoring process $(\$ 3.1$

: Initialize the continuous safety prompt $\boldsymbol{\theta}$ from $\boldsymbol{\theta}_{0}$.

3: Optimize $\boldsymbol{\theta}$ with Equation 8 .

$\triangle$ Optimization process (Figure 4, $\S 3.2,3.3$

low-dimensional representation $g\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)$ from $g\left(\boldsymbol{x}_{0}\right)$ along or opposite the refusal direction defined by $\boldsymbol{w}_{\mathrm{r}}$.

We similarly calculate a loss for harmfulness recognition, which can help maintain the capability of distinguishing harmful/harmless queries:

$$
\begin{align*}
\mathcal{L}_{\mathrm{h}}(\boldsymbol{\theta})= & -l \log \sigma\left(f_{\mathrm{h}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-f_{\mathrm{h}}\left(\boldsymbol{x}_{0}\right)\right) \\
& -(1-l) \log \left(1-\sigma\left(f_{\mathrm{h}}\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-f_{\mathrm{h}}\left(\boldsymbol{x}_{0}\right)\right)\right) \tag{4}
\end{align*}
$$

which uses the same dimensionality reduction function $g$ but a different logistic regression $f_{\mathrm{h}}$ fitted using queries' harmfulness as labels:

$$
\begin{equation*}
f_{\mathrm{h}}: \mathbb{R}^{n} \rightarrow \mathbb{R}, f_{\mathrm{h}}(\boldsymbol{x})=\boldsymbol{w}_{\mathrm{h}}^{\top} g(\boldsymbol{x})+b_{\mathrm{h}} \tag{5}
\end{equation*}
$$

where $\boldsymbol{w}_{\mathrm{h}} \in \mathbb{R}^{m}, b_{\mathrm{h}} \in \mathbb{R}$ are the fitted parameters. We find that adding $\mathcal{L}_{\mathrm{h}}(\boldsymbol{\theta})$ can bring some safeguarding performance improvement $(\$ 4.2$ ).

### 3.3. Regularization

One issue of directly optimizing certain features in the lowdimensional space is the degeneration of the original representation. Specifically, with the supervision signal only applied to the $m$-dimensional features of $\boldsymbol{x}$, the information in the remaining $n-m$ dimensions can be lost, which would consequently impair generation quality ( $\$ 4.2$ ). We thus design a regularization item to address this issue.

We notice that in the dimensionality reduction function $g$, the transformation matrix $\boldsymbol{V}$ contains $m$ unit-length, orthogonal vectors. We can complete $\boldsymbol{V}$ into an orthogonal matrix $\boldsymbol{Q}=[\boldsymbol{V} ; \boldsymbol{U}] \in \mathbb{R}^{n \times n}$, where $\boldsymbol{U} \in \mathbb{R}^{n \times(n-m)}$ is arbitrary and can be easily obtained via the Gram-Schmidt algorithm. The property that $Q$ keeps the vector length (under the Euclidean norm) gives us:

$$
\begin{align*}
\| \boldsymbol{x}_{\boldsymbol{\theta}} & -\boldsymbol{x}_{0}\left\|^{2}=\right\| \boldsymbol{Q}^{\top}\left(\boldsymbol{x}_{\boldsymbol{\theta}}-\boldsymbol{x}_{0}\right) \|^{2} \\
& =\left\|\boldsymbol{V}^{\top}\left(\boldsymbol{x}_{\boldsymbol{\theta}}-\boldsymbol{x}_{0}\right)\right\|^{2}+\left\|\boldsymbol{U}^{\top}\left(\boldsymbol{x}_{\boldsymbol{\theta}}-\boldsymbol{x}_{0}\right)\right\|^{2} \\
& =\left\|g\left(\boldsymbol{x}_{\boldsymbol{\theta}}\right)-g\left(\boldsymbol{x}_{0}\right)\right\|^{2}+\left\|\boldsymbol{U}^{\top}\left(\boldsymbol{x}_{\boldsymbol{\theta}}-\boldsymbol{x}_{0}\right)\right\|^{2} \tag{6}
\end{align*}
$$

The LHS item is the change between the new and the initial hidden states $\boldsymbol{x}$ and $\boldsymbol{x}_{0}$. The first RHS item is the difference in the extracted $m$-dimensional features related to the safety
prompt and queries' harmfulness, which will be enlarged through Equation 3. The second RHS item denotes the information change in the remaining $n-m$ dimensions, which is independent of the former extracted $m$ features. Therefore, to restrict $\left\|\boldsymbol{x}_{\boldsymbol{\theta}}-\boldsymbol{x}_{0}\right\|$ within a reasonable range of variation, we can use the second RHS item for regularization (we normalize it by the model's hidden size $n$ ), i.e.:

$$
\begin{equation*}
\mathcal{L}_{U}(\boldsymbol{\theta})=\left\|\boldsymbol{U}^{\top}\left(\boldsymbol{x}_{\boldsymbol{\theta}}-\boldsymbol{x}_{0}\right)\right\|^{2} / n \tag{7}
\end{equation*}
$$

The final optimization objective of DRO is:

$$
\begin{equation*}
\mathcal{L}(\boldsymbol{\theta})=\mathcal{L}_{\mathrm{r}}(\boldsymbol{\theta})+\mathcal{L}_{\mathrm{h}}(\boldsymbol{\theta})+\beta \mathcal{L}_{U}(\boldsymbol{\theta}) \tag{8}
\end{equation*}
$$

where only the continuous safety prompt $\boldsymbol{\theta}$ is trainable. We set $\beta=0.001$ in experiments to achieve a balance between optimization for the extracted $m$-dimensional features and regularization for the remaining $n-m$ dimensions. The overall procedure of DRO is summarized in Algorithm 1 .

### 3.4. Highlights

As a method for continuous safety prompt optimization, DRO has three distinct characteristics.

- First, DRO utilizes a small set of anchor data to extract the most salient features related to the queries' harmfulness and the impact of the safety prompt, where the latter correlates strongly with the model's refusal behavior ( $\$ 3.1$. The proper control of the anchor data can largely guarantee that the anchored low-dimensional space captures our interested features (particularly, the refusal direction), making it possible to directly optimize these target features. We show in $\S 4.4$ that DRO also manifests reasonable robustness to the choices of anchor data.
- Second, by direct optimization in the low-dimensional space ( $\S 3.2$, DRO eliminates the need for sparse supervision signals from textual responses. If training the continuous safety prompt traditionally by optimizing the likelihood of sequences, we may need a large number of demonstration query and response pairs to teach our true optimization goal (i.e., proper refusal according to queries' harmfulness), which we found are not easily obtained in the current open-source community. We demonstrate in $\S 4.2$ that by training on only 200 synthetic data, DRO can significantly enhance the safeguarding performance of human-crafted safety prompts.
- Finally, even if there is sufficient safety data for the traditional training of continuous prompts, it is still necessary to incorporate other general-domain data to prevent catastrophic forgetting. DRO bypasses this tricky issue through the regularization item $\mathcal{L}_{U}$ ( $\$ 3.3$ ) that helps retain information other than the target features. We show in $\S 4.2$ that this regularization item $\mathcal{L}_{U}$ is critical to maintaining the models' general performance.


## 4. Evaluation

### 4.1. Baselines and Benchmarks

For DRO's anchoring process, we use the 100 harmful and 100 harmless queries in $\S 2.1$ and the $k=3$ basic safety prompts in $\S 2.2$ (default, mistral, and short) to compose the anchor data. We then optimize the three human-crafted basic safety prompts separately. For baselines, we compare with the settings (1) where no safety prompts are used (no prompt), and (2) where we use the initial basic safety prompt (default, mistral, or short). We also compare with (3) vanilla Prompt-Tuning (vPT) (Lester et al. 2021), which trains the continuous safety prompts on model-generated responses (those used in DRO's anchoring process to estimate the refusal direction) in a traditionally supervised way, i.e., optimizing the likelihood of target sequences. Both DRO and vPT are trained using the 200 synthetic data from $\S 2.1$ and initialize $\theta$ from the same basic textual safety prompt, with the only difference in their optimization objectives. See Appendix Hfor the implementation and training details of DRO and vPT.

We evaluate DRO and the baselines on two out-of-domain benchmarks: MaliciousInstruct (Huang et al. 2024) and AdvBench (Zou et al., 2023). MaliciousInstruct consists of 100 harmful question instructions like "Ways to...?" and "Ideas for...?". It is specially designed to cover diverse harmful intents (10 in total), such as sabotage, theft, defamation, etc. AdvBench comprises 520 harmful behaviors expressed as imperative "Do something" instructions. For experimental efficiency and following previous work Zou et al. (2023); Huang et al. (2024), we use the first 100 samples of AdvBench for experiments. Overall, the two out-of-domain benchmarks distinctly differ from the synthetic data used by DRO in terms of contents and formats.

We use AlpacaEval (Li et al., 2023) to assess the impact of safety prompt optimization on the models' general performance. AlpacaEval is an LLM-based automatic evaluation suite, which computes the win rate of the model responses to OpenAI's text-davinci-003 responses given benign instructions. It has been widely adopted for open-source LLM evaluation (Ivison et al., 2023, Li et al., 2024) and we believe it can serve as a reasonable testbed for the 7B LLMs we experiment with. We use 100 randomly sampled instructions for evaluation and employ gpt-3.5-turbo as the evaluator. Additionally, we assess DRO's impact on models' false refusals on a held-out set of 100 harmless queries, which are collected in the same way as in $\S 2.1$.

### 4.2. Main Results

Table 2 show the evaluation results using the default safety prompt. First, compared with the human-crafted basic safety prompt, DRO significantly improves safeguarding

Table 2: Evaluation results (optimizing the default basic safety prompt) on MaliciousInstruct, Advbench, the held-out harmless query set, and AlpacaEval.

|  | $\%$ Compliance on MaliciousInstruct $\downarrow$ |  |  |  |  |  |  | \% Compliance on AdvBench $\downarrow$ |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | no | default | vPT | $\mathrm{DRO}$ | $-\mathcal{L}_{U}$ | $-\mathcal{L}_{\mathrm{r}}$ | $-\mathcal{L}_{\mathrm{h}}$ | no | default | vPT | DRO | $\mid-\mathcal{L}_{U}$ | $-\mathcal{L}_{\mathrm{r}}$ | $-\mathcal{L}_{\mathrm{h}}$ |
| llama-2-chat | 1 | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 |
| codellama-instruct | 3 | 2 | 7 | 1 | 1 | 1 | 1 | 2 | 0 | 2 | 0 | 0 | 0 | 0 |
| vicuna-v1.5 | 51 | 10 | 7 | 2 | 2 | 4 | 2 | 27 | 4 | 2 | 0 | 1 | 2 | 0 |
| orca-2 | 70 | 22 | 2 | 1 | 1 | 7 | 1 | 70 | 2 | 4 | 0 | 0 | 0 | 0 |
| mistral-inst-v0.1 | 77 | 31 | 10 | 3 | 1 | 37 | 2 | 86 | 62 | 26 | 6 | 5 | 63 | 1 |
| mistral-inst-v0. 2 | 30 | 2 | 1 | 1 | 2 | 1 | 1 | 51 | 3 | 0 | 1 | 0 | 1 | 0 |
| openchat-3.5 | 77 | 9 | 9 | 3 | 2 | 8 | 5 | 81 | 10 | 11 | 3 | 1 | 7 | 2 |
| openchat-3.5-1210 | 66 | 1 | 3 | 1 | 3 | 3 | 2 | 78 | 1 | 6 | 1 | 1 | 7 | 1 |
| average | 46.9 | 9.8 | 5.0 | 1.6 | 1.5 | 7.8 | 1.8 | 49.4 | 10.3 | 6.8 | 1.4 | 1.0 | 10.0 | 0.5 |
|  | $\%$ Refusal on Held-out Harmless $\downarrow$ |  |  |  |  |  |  | \% Win Rate on AlpacaEval $\uparrow$ |  |  |  |  |  |  |
|  | no | default | vPT | $\mathrm{DRO}$ | $-\mathcal{L}_{U}$ | $-\mathcal{L}_{\mathrm{r}}$ | $-\mathcal{L}_{\mathrm{h}}$ | no | default | vPT | DRO | $-\mathcal{L}_{U}$ | $-\mathcal{L}_{\mathrm{r}}$ | $-\mathcal{L}_{\mathrm{h}}$ |
| llama-2-chat | 1 | 19 | 5 | 5 | 3 | 7 | 7 | 66 | 47 | 37 | 54 | 53 | 53 | 48 |
| codellama-instruct | 3 | 22 | 0 | 7 | 5 | 8 | 7 | 54 | 52 | 47 | 51 | 45 | 48 | 51 |
| vicuna-v1.5 | 0 | 5 | 4 | 2 | 1 | 0 | 1 | 68 | 65 | 62 | 64 | 58 | 65 | 61 |
| orca-2 | 1 | 5 | 3 | 0 | 0 | 0 | 0 | 63 | 56 | 45 | 60 | 58 | 61 | 60 |
| mistral-inst-v0.1 | 1 | 2 | 2 | 1 | 0 | 2 | 0 | 56 | 59 | 56 | 60 | 34 | 55 | 59 |
| mistral-inst-v0. 2 | 0 | 4 | 0 | 0 | 0 | 1 | 1 | 79 | 77 | 72 | 79 | 71 | 72 | 73 |
| openchat-3.5 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 66 | 72 | 65 | 69 | 47 | 70 | 70 |
| openchat-3.5-1210 | 0 | 0 | 2 | 0 | 1 | 1 | 0 | 75 | 72 | 66 | 71 | 55 | 66 | 68 |
| average | 0.8 | 7.1 | 2.0 | 2.0 | 1.3 | 2.4 | 2.0 | 65.9 | 62.5 | 56.3 | 63.5 | 52.6 | 61.3 | 61.3 |

performance (1.6 vs. 9.8 on MaliciousInstruct; $\underline{1.4}$ vs. 10.3 on AdvBench) and meanwhile reduces false refusals for harmless queries ( $\mathbf{2 . 0}$ vs. 7.1 on the held-out harmless set), which does not compromise the models' general performance (63.5 vs. 62.5 on AlpacaEval). From Figure 5, it is evident that DRO moves queries' representations along (for out-of-domain harmful queries) or opposite (for harmless ones) our estimated refusal direction, which justifies the motivation of DRO (see Appendix K for full results). Second, DRO also remarkably outperforms the vPT baseline (1.6 vs. $\mathbf{5 . 0}$ on MaliciousInstruct; 1.4 vs. 6.8 on AdvBench), suggesting that vPT cannot well generalize to out-of-domain data. Moreover, vPT shows a deficiency in maintaining the models' general performance (56.3 vs. 63.5 of DRO on AlpacaEval; dropping from 62.5 of the initial basic safety prompt), probably due to its nature of only optimizing for specific tasks using task-specific data. The above observations still hold when we apply DRO to optimize the other two human-crafted basic safety prompts (mistral and short), whose results are shown in Appendix I

We then conduct ablation study on the optimization objectives in Equation 8. From Table 2 (upper), we can observe that the objective $\mathcal{L}_{\mathrm{r}}$ is critical to the safeguarding performance. Without $\mathcal{L}_{\mathrm{r}}$, models would still struggle to refuse harmful queries even when trained to distinguish harmful and harmless queries (i.e., with $\mathcal{L}_{\mathrm{h}}$ ). From Table 2 (lower right), we can observe that the regularization item $\mathcal{L}_{U}$ is essential for maintaining the models' general performance.
![](https://cdn.mathpix.com/cropped/2024_06_04_e2640dddbcb735b3d7d8g-07.jpg?height=344&width=832&top_left_y=1275&top_left_x=1053)

Figure 5: Visualization of Mistral-Instruct-v0.1's hidden states after DRO optimization (optimizing the default basic safety prompt) on MaliciousInstruct and the heldout harmless query set. Both boundaries are copied from Figure 3 Dashed colorized arrows denote movements from no safety prompts to the default safety prompt, while solid colorized arrows denote further movements by DRO.

Without $\mathcal{L}_{U}$, models would suffer from largely degraded generation quality for benign instructions (52.6 vs. $\underline{63.5}$ on AlpacaEval). We also observe that removing $\mathcal{L}_{\mathrm{h}}$ does not noticeably impair safeguarding performance and the models' general performance, probably because the objective $\mathcal{L}_{\mathrm{r}}$ has implicitly entailed the requirement for the capability of recognizing harmful queries.

### 4.3. Extension To Jailbreak Setting

As LLM jailbreaking (Zou et al. 2023; Wei et al. 2023) has become an increasingly threatening safety issue, we further

Table 3: Evaluation results (optimizing the default basic safety prompt) on AdvBench under GCG jailbreak attack.

|  | GCG Jailbreak |  |  |  |
| :--- | :---: | :---: | :---: | :---: |
|  | no | default | vPT | DRO |
| llama-2-chat | 2 | 0 | 27 | 0 |
| codellama-instruct | 7 | 1 | 13 | 1 |
| vicuna-v1.5 | 46 | 14 | 9 | 2 |
| orca-2 | 82 | 8 | 3 | 0 |
| mistral-inst-v0.1 | 88 | 66 | 16 | 12 |
| mistral-inst-v0.2 | 62 | 3 | 0 | 1 |
| openchat-3.5 | 79 | 12 | 5 | 5 |
| openchat-3.5-1210 | 67 | 2 | 2 | 4 |
| average | 54.1 | 13.3 | 9.4 | $\mathbf{3 . 1}$ |

evaluate DRO's effectiveness in defending against the GCG (Zou et al., 2023) jailbreak attack on AdvBench. GCG appends each query with an adversarial suffix, which is optimized using a gradient-based method. According to the transferability in Zou et al. 2023, we use the GCG jailbreak prompts optimized from llama-2-chat for all the models. We then directly prepend the DRO-optimized default safety prompt, which is used in $\S 4.2$ and Table 2 In Table 3, we show that in the jailbreak setting, DRO remains effective in improving the safeguarding performance of human-crafted safety prompts ( $\mathbf{3 . 1}$ vs. 13.3), while vPT cannot generalize well to jailbreak prompts. Interestingly, we find that even the basic safety prompt can provide non-trivial safeguarding, comparing to not adding safety prompts (13.3 vs. 54.1), which can serve as a strong baseline for future research on LLM jailbreaking and safeguarding.

### 4.4. Robustness Analysis

In DRO's anchoring process ( $\$ 3.1$, we use a set of anchor data to derive the low-dimensional representation space and refusal direction. We are interested in how robust DRO is to the choices of anchor data. We conduct ablation study for anchor data from the two aspects that compose the anchor data. For queries that were originally collected with careful controls ( $\$ 2.1$, used in $\S 2.3$ and $\S 4.1$, we keep the 100 synthetic harmless ones but replace the 100 synthetic harmful ones with the 100 queries from AdvBench. Note that these queries (after replacement) are also used for the subsequent DRO training. This replacement leads to the format gap between the harmless and the new harmful queries, i.e., the former are all "How to do" questions while the latter are all "Do something" instructions, which simulates the case where the queries are collected with less careful controls. For basic safety prompts, we originally equipped queries with all three basic safety prompts (default, mistral, and short; $k=3$ ) to form eight groups of data points for anchoring $(2 \times(1+k) ; \S 3.1$, and then optimized the three different basic safety prompts separately ( $\$ 4.1$. Now we
Table 4: Ablation results for anchor data, averaged over all the eight models. See Appendix J]for breakdowns.

|  | Malicious $\downarrow$ | AlpacaEval $\uparrow$ |
| :---: | :---: | :---: |
| Ablation for Queries |  |  |
| default (before DRO) | 9.8 | 62.5 |
| DRO (synthetic harmful <br> + synthetic harmless) | 1.6 | 63.5 |
| DRO (AdvBench harmful <br> + synthetic harmless) | 1.6 | 59.0 |
| Ablation for Basic Safety Prompts |  |  |
| short (before DRO) | 18.3 | 62.6 |
| DRO (multiple anchoring <br> $\rightarrow$ optimizing short) | 2.3 | 59.6 |
| DRO (default-only anchoring <br> $\quad \rightarrow$ optimizing short) | 4.1 | 60.8 |

use only the default one $(k=1)$ to form four groups of data points for anchoring, but then optimize the short one, which results in a gap between the basic safety prompt used for anchoring (default) and the one to be optimized (short). This enables us to fairly assess whether using a single safety prompt can still anchor a low-dimensional space that captures the features related to models' refusal behaviors.

The results of ablation study for anchor data are shown in Table 4. We find that DRO still notably enhances the safeguarding performance. However, when the queries are less carefully controlled, the models' general performance can be slightly degraded ( 59.0 vs. 63.5). It is probably due to the distraction of the spurious features that can be used to distinguish harmful and harmless queries, such as the textual format. We also observe that when we use only a single safety prompt for anchoring, the safeguarding performance is slightly inferior to that when we use multiple ones (4.1 vs. 2.3). It suggests that a single safety prompt may introduce biases that hinder accurately capturing the most salient features related to models' refusal behaviors. But overall, DRO exhibits reasonable robustness to the choices of anchor data, and we suggest applying proper query controls and combining multiple basic safety prompts for the anchor data to achieve better safeguarding performance.

### 4.5. Interpretability Analysis

We are also interested in whether the optimized continuous safety prompts can be interpreted as textual prompts. We attempted two metrics to project the continuous safety prompts into the vocabulary by comparing them with the model's token embeddings: (1) the Euclidean distance, and (2) the dot product. However, we found that the projected tokens are almost identical to the basic textual safety prompts from which the continuous embeddings are initialized. Under the Euclidean distance, we found that only six optimized
safety prompts are projected into tokens that slightly differ from the initial basic safety prompts (among $8 \times 3=24$ optimized ones; eight models and three basic safety prompts). We show in Appendix D these cases and the Euclidean distances of all the cases. It suggests that the optimization of continuous safety prompts generally occurs within the small vicinity of the initialized token embeddings.

## 5. Related Work

Large Language Model Safety Research on LLM safety aims to avoid LLMs producing contents that may cause harm to individuals and society. Previous work extensively studied to eliminate undesirable attributes from LLMgenerated texts, such as biases, toxic language, and hate speech (Xu et al., 2020; Sun et al., 2022; Adolphs et al. 2023; Zheng et al. 2023, 2024b). As the capabilities of LLMs keep growing, researchers are paying increasing attention to preventing LLMs from assisting queries or instructions with harmful intents, i.e., training or teaching them to refuse (Shaikh et al., 2023; Touvron et al., 2023, Bai et al. 2022a b, OpenAI, 2023), which is the focus of our work. Recent work has also noticed the more complex jailbreak attacks, which manipulate LLMs into providing assistance by obfuscating LLMs' recognition of the queries' harmfulness (Zou et al., 2023, Wei et al., 2023, Liu et al., 2024; Zeng et al. 2024). Our work can inspire future research to delve into the intrinsic causes of LLMs' vulnerabilities and stimulate more principled safeguarding methods.

Prompt Optimization Our work is related to previous research on prompt optimization. The proposed DRO method follows the setting of common continuous prompt optimization, exemplified by Prompt-Tuning (Lester et al., 2021; Zheng \& Huang, 2021) and Prefix-Tuning (Li \& Liang, 2021; Sheng et al., 2020), where the model parameters are frozen and only a few continuous prompt parameters are trainable. There is also previous work that studied optimization for discrete textual prompts through gradient-based search or RL (Shin et al. 2020; Deng et al. 2022) and discussed how they change models' behaviors (Zhao et al. 2021). Recent work has shown LLMs' potential of serving as prompt optimizers (Zhou et al., 2022, Yang et al., 2023), but these approaches usually rely on powerful proprietary LLMs like GPT-4 (OpenAI, 2023), which may somewhat hinder reproducibility and transparency.

## 6. Conclusion

We investigate the working mechanisms of safety prompts in safeguarding LLMs from the perspective of model representations. We find that safety prompts do not clearly improve LLMs in recognizing the harmfulness of queries, but rather increase LLMs' overall probability of refusing queries by moving queries' representations in a "higherrefusal" direction. Drawing this inspiration, our proposed DRO method optimizes continuous safety prompts by moving queries' representations in the low-dimensional space along or opposite the estimated refusal direction, in which the model's refusal probability increases. We show that DRO brings remarkable improvement in safeguarding performance on both out-of-domain and jailbreak benchmarks, does not compromise the models' general performance, and exhibits reasonable robustness to the choices of the data used for anchoring the low-dimensional space. We hope the empirical analysis and the proposed methodology in this work can inspire future research on LLM safety.

## Impact Statement

This work aims to provide an understanding and increase the transparency of the working mechanisms of the promptdriven LLM safeguarding approach (i.e., prepending model inputs with safety prompts). The proposed DRO method optimizes continuous safety prompts to increase the refusal probability for harmful queries and decrease it for harmless ones. One may be concerned about the dual use of DRO in steering LLMs toward malicious behaviors. Specifically, one may simply flip the harmfulness labels $l$ in $\mathcal{L}_{\mathrm{r}}$ (Equation 3) to decrease the refusal probability for harmful queries and achieve intentional "misalignment". However, the objective $\mathcal{L}_{\mathrm{r}}$ has entailed the objective $\mathcal{L}_{\mathrm{h}}$ for maintaining the capability of harmful recognition, as we analyzed in the ablation study in $\S 4.2$. Therefore, flipping the labels in $\mathcal{L}_{\mathrm{r}}$ can conflict with the model's natural recognition of the queries' harmfulness (as observed in $\S 2.3$, which consequently would undermine the general model capability and instead hinder malicious uses. Finally, we insist on encouraging the positive use of the proposed DRO method and strongly object to malicious uses.

The queries considered in this work are unambiguously harmful or harmless. But in the real world, user queries can be ambiguous, and their harmfulness may be difficult to judge for either the most powerful LLMs or humans. For instance, the recently proposed persuasive adversarial prompts (Zeng et al. 2024) can paraphrase harmful queries into harmless-like persuasive ones. Extensive future work is still needed to integrate social norms and values to delineate the boundaries of harmful intents. Furthermore, we would like to emphasize that improving LLM safety still requires massive and continual safety training and alignment (Bai et al. 2022a, Touvron et al. 2023), without which safety prompts alone are far from sufficient.

## Acknowledgments

We thank Yufei Tian, Rohan Wadhawan, Yu (Bryan) Zhou, Haw-Shiuan Chang, Po-Nien Kung, and other members of the UCLA PlusLab \& NLP group as well as anonymous reviewers for their constructive feedback and discussions. We thank Xiaogeng Liu and Nan Xu for sharing the GCG jailbreak prompts.

This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604), the NSFC project (Key project with No. 61936010), and China Scholarship Council (with No. 202306210211). This work was also supported by Meta Sponsor Research Award, NSF \#2331966, and a gift from UCLA Institute for Technology, Law and Policy.

## References

Adolphs, L., Gao, T., Xu, J., Shuster, K., Sukhbaatar, S., and Weston, J. The CRINGE loss: Learning what language not to model. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. URL https://aclanthology. org/2023.acl-long.493.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.

Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.

Bhatt, M., Chennabasappa, S., Nikolaidis, C., Wan, S., Evtimov, I., Gabi, D., Song, D., Ahmad, F., Aschermann, C., Fontana, L., et al. Purple llama cyberseceval: A secure coding benchmark for language models. arXiv preprint arXiv:2312.04724, 2023.

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicunal

Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E., and Hu, Z. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022. URL https: //aclanthology.org/2022.emnlp-main.222.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygGQyrFvH

Huang, Y., Gupta, S., Xia, M., Li, K., and Chen, D. Catastrophic jailbreak of open-source LLMs via exploiting generation. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=r42tSSCHPh

Ivison, H., Wang, Y., Pyatkin, V., Lambert, N., Peters, M., Dasigi, P., Jang, J., Wadden, D., Smith, N. A., Beltagy, I., et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023 .

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL https: //aclanthology.org/2021.emnlp-main. 243

Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_ eval, 2023.

Li, X., Yu, P., Zhou, C., Schick, T., Zettlemoyer, L., Levy, O., Weston, J., and Lewis, M. Self-alignment with instruction backtranslation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=1oijHJBRsT.

Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.acl-long.353

Liu, X., Xu, N., Chen, M., and Xiao, C. AutoDAN: Generating stealthy jailbreak prompts on aligned large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=7Jwpw4qKkb

Mitra, A., Del Corro, L., Mahajan, S., Codas, A., Simoes, C., Agarwal, S., Chen, X., Razdaibiedina, A., Jones, E., Aggarwal, K., et al. Orca 2: Teaching small language
models how to reason. arXiv preprint arXiv:2311.11045, 2023.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Shaikh, O., Zhang, H., Held, W., Bernstein, M., and Yang, D. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), July 2023. URL https://aclanthology.org/2023.acl-long. 244

Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. Towards Controllable Biases in Language Generation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3239-3254, November 2020. doi: 10.18653/v1/2020.findings-emnlp.291. URL https: //aclanthology.org/2020.findings-emnlp.291.

Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https://aclanthology.org/ 2020.emnlp-main. 346 .

Sun, H., Xu, G., Deng, J., Cheng, J., Zheng, C., Zhou, H., Peng, N., Zhu, X., and Huang, M. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the Association for Computational Linguistics: ACL 2022, 2022. URL https://aclanthology. org/2022.findings-acl.308.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. Openchat: Advancing open-source language models with mixed-quality data. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=AOJyfhWYHf.

Wei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does LLM safety training fail? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= jA235JGM09
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.

Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood training. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=SJeYeONtvH

Xu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079, 2020.

Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.

Zeng, Y., Lin, H., Zhang, J., Yang, D., Jia, R., and Shi, W. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.

Zhao, J., Khashabi, D., Khot, T., Sabharwal, A., and Chang, K.-W. Ethical-advice taker: Do language models understand natural language interventions? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 4158-4164, August 2021. doi: 10.18653/v1/2021.findings-acl.364. URL https: //aclanthology.org/2021.findings-acl.364

Zheng, C. Chat templates for huggingface large language models. https://github.com/chujiezheng/chat_ templates, 2023.

Zheng, C. and Huang, M. Exploring prompt-based few-shot learning for grounded dialog generation. arXiv preprint arXiv:2109.06513, 2021.

Zheng, C., Ke, P., Zhang, Z., and Huang, M. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023, 2023. URL https: //aclanthology.org/2023.findings-acl.65.

Zheng, C., Wang, Z., Ji, H., Huang, M., and Peng, N. Weak-to-strong extrapolation expedites alignment. arXiv preprint arXiv:2404.16792, 2024a.

Zheng, C., Zhou, H., Meng, F., Zhou, J., and Huang, M. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2024b. URL https: //openreview.net/forum?id=shr9PXz7T0.

Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are humanlevel prompt engineers. In The Eleventh International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=92gvk82DE-

Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
