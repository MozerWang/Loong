# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions 

Chinmaya Andukuri ${ }^{*}$<br>Stanford University<br>andukuri@stanford.edu

Jan-Philipp Fränken*<br>Stanford University<br>jphilipp@stanford.edu

Tobias Gerstenberg<br>Stanford University<br>gerstenberg@stanford.edu

Noah D. Goodman<br>Stanford University<br>ngoodman@stanford.edu


#### Abstract

When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model's ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions-a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model-the Questioner-and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer's latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on $72 \%$ of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.


## 1 Introduction

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-01.jpg?height=306&width=699&top_left_y=1731&top_left_x=366)

Figure 1: Problem Illustration. When user preferences are unknown, language models may respond ineffectively. By asking questions, models can elicit information and provide more effective responses.
When interacting with users who have different preferences, language models (LMs) encounter task ambiguity (Finn et al., 2018; Tamkin et al., 2022). Depending on the user, the same request might correspond to a different task. For example, consider a user who asks an LM for a pasta recipe (Figure 1). If the model could elicit information about the user's dietary restrictions, favorite sauces, and preferred cooking methods, it could tailor the recipe to their specific needs and desires. The model might sug-
vegetarian, or propose a traditional lasagna not explicitly specified in the prompt, the model may generate a generic recipe that fails to account for the user's unique preferences and constraints. In high-stakes domains like healthcare or education, such task ambiguity can have significant consequences.

One approach to resolving task ambiguity is by asking targeted questions to elicit relevant information from users. Prompting closed-source LMs can yield useful questions (e.g., Li[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-02.jpg?height=544&width=1309&top_left_y=281&top_left_x=386)

Figure 2: Overview of STaR-GATE. A task is given to a Questioner who elicits preferences from a Roleplayer whose persona is unknown to the Questioner. The resulting conversations are then filtered based on the log probability of a gold response generated by an Oracle which has access to the Roleplayer's persona (omitted from the diagram for clarity). We then fine-tune the Questioner on the filtered questions. Moreover-to avoid distribution shift—we regularize the Questioner by additionally sampling responses conditioned on the filtered conversations. In our ablations, we contrast fine-tuning on sampled responses with fine-tuning on the gold responses.

et al., 2023;Piriyakulkij et al., 2023). However, this approach is inflexible in guiding a model's questioning strategy and frequently generates queries that are ineffective or irrelevant for the task at hand. Indeed, it is likely that current alignment strategies-such as RLHFspecifically inhibit the ability to carry out such dialog (Shaikh et al., 2023). One recent effort addresses these limitations by combining elicitation with optimal experimental design methods (Handa et al., 2024). However, this approach constrains questions to pairwise comparisons over a fixed set of features, substantially limiting the space of questions that can be used to probe user preferences. Another approach is to use offline reinforcement learning to encourage useful dialog (Hong et al., 2023). This is promising but requires offline generation of high-quality dialog from an expert model, and has not targeted questions for preference elicitation specifically.

In this paper, we explore whether we can improve a LM's ability to ask useful questions by bootstrapping with a form of self-play (Silver et al., 2017; Anthony et al., 2017). We introduce STaR-GATE (Figure 2), an iterative algorithm that combines active preference elicitation (GATE; Li et al., 2023) with a self-improvement loop inspired by STaR (Zelikman et al., 2022). We address several technical challenges: (1) We define a task setting for improving elicitation for which we generate a synthetic dataset of 25,500 unique persona-task prompts; (2) We define a reward function based on the log probability of gold responses generated by an oracle model (with access to the persona); and (3) We encourage the LM to use the elicited information while avoiding distribution shift through response regularization. We find that questions asked by the finetuned model increase the probability of gold responses consistently across iterations (Figure 4). Moreover, compared to responses generated by the initial model, responses generated by a STaR-GATE finetuned model have $\mathbf{7 2} \%$ win rates (Figure 3a) after two iterations.

In summary, we make the following contributions: (1) We introduce STaR-GATE, a simple algorithm that iteratively improves a LM's ability to elicit user preferences by asking questions. (2) We generate a synthetic dataset consisting of 25,500 unique persona-task prompts, each paired with a personalized, gold response. (3) We show that finetuning with STaR-GATE enables a LM to generate questions that significantly increase the probability of generating gold responses. (4) We show that adding response-regularization to STaR-GATE yields a fine-tuned model able to use the elicited preferences to generate better responses-a high win rate against the initial model. (5) We show that the finetuned model generalizes beyond the roleplayer it was trained with.

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-03.jpg?height=564&width=1352&top_left_y=301&top_left_x=381)

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-03.jpg?height=480&width=659&top_left_y=318&top_left_x=386)

[a] Ablation Results

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-03.jpg?height=480&width=656&top_left_y=318&top_left_x=1060)

[b] Generalization Results

Figure 3: Win Rates Against Initial Model. [a] Complete method and ablations: w/o Reg. refers to finetuning on questions only, which decreases the model's ability to generate answers. w/ Gold Resp. refers to finetuning directly on the gold responses rather than selfgenerated responses, which leads to hallucinations in generated answers. [b] Roleplayer generalization results. We demonstrate that STaR-GATE generalizes beyond the roleplayer it was trained against (mixtral-8x7b). All three roleplayers correspond to the instruct version of their respective models. Error bars represent the standard error of the mean ( $\pm$ SEM). We include 0.5 (chance) as a reference point for iteration $t=0$.

## 2 Related Work

### 2.1 Preference Optimization

Preference optimization algorithms, such as RLHF (Christiano et al., 2017), DPO (Rafailov et al., 2024), or KTO (Ethayarajh et al., 2024), optimize LMs to provide single-turn dialog responses that reflect preferred or high-utility outcomes. As a result, these models learn distributions over responses that effectively generate answers to user queries without requiring additional information beyond the initial prompt. However, asking follow-up questions to elicit user preferences is essential for understanding their unique needs and desires, especially when faced with task ambiguity (Tamkin et al., 2022; Li et al., 2023). Despite the importance of follow-up questions for effective communication, recent research has shown that preference optimization algorithms can reduce a LM's ability to ask followup questions. Specifically, RLHF has been found to negatively correlate with a LM's attempts to ask follow-up questions or show acknowledgment (Shaikh et al., 2023). This limitation can be problematic for high-stakes domains such as healthcare (Thirunavukarasu et al., 2023) or education (Kasneci et al., 2023), where resolving task ambiguity through effective questioning is crucial for effective dialog.

### 2.2 Preference Elicitation with LMs

One way of resolving task ambiguity is by prompting a LM to ask questions or infer user preferences from observations (Li et al., 2023; Piriyakulkij et al., 2023; Lin et al., 2023; Fränken et al., 2023; Handa et al., 2024). For example, Li et al. (2023) used LMs themselves to elicit user preferences during interaction (GATE; Li et al., 2023). In GATE (short for Generative Active Task Elicitation), a LM elicits and infers intended behavior through free-form, languagebased interaction. Unlike non-interactive elicitation approaches, such as prompting (Brown et al., 2020), which rely entirely on the user to specify their preferences, generative elicitation probes nuanced user preferences better. Across domains such as content recommendation and email verification, generative elicitation with LMs requires less effort than prompting while being comparable to or better than user-written prompts (for further details, see Section 5 in Li et al., 2023). Building upon GATE, Handa et al. (2024) introduced OPEN, a framework that combines LM-driven elicitation with Bayesian Optimal Experimental

```
Algorithm 1 STaR-GATE
    1: Input $Q_{B A S E}$ : a pretrained LM; tasks $T=\left\{\left(t_{i}\right)\right\}_{i=1}^{D}$, personas $U=\left\{\left(u_{j}\right)\right\}_{j=1}^{C}$, and gold
    responses $G=g_{i j}$ for $i j \in \in\{1, \ldots, D\} \times\{1, \ldots, C\}$
    for $\eta$ in $1 \ldots N$ do $\quad \triangleright$ Outer loop
        $\left\{s_{i j}^{c}\right\}$ with $c \in[1,10] \leftarrow Q_{\eta-1}\left(t_{i}, u_{j}\right)$ for $i j \in\{1, \ldots, D\} \times\{1, \ldots, C\} \quad \triangleright$ Simulate
    multiple conversations for each $(i j)$
        $S_{\eta}=\left\{s_{i j}^{*}\right\} \leftarrow \arg \max _{s_{i j}^{c}} \log p_{Q_{B A S E}}\left(g_{i j} \mid t_{i}, s_{i j}^{c}\right)$ with $c \in[1,10]$ for $i j \in\{1, \ldots, D\} \times$
    $\{1, \ldots, C\} \triangleright$ Filter conversations with highest log-probabilities of generating the gold
    responses according to the original model $Q_{B A S E}$
    5: $\quad R_{\eta}=\left\{Q_{\eta-1}\left(t_{i}, s_{i j}^{*}\right)\right\} \quad \triangleright$ Generate model responses conditioned on the best selected
    conversations for all $i j$ using $Q_{\eta-1}$
        $Q_{\eta} \leftarrow \operatorname{train}\left(Q_{B A S E}, S_{\eta}, R_{\eta}\right) \triangleright$ Finetune the original model on selected conversations
    and model responses
    end for
```

Design (BOED) to select informative questions and translate abstract queries into natural language. OPEN combines the advantages of LMs and Bayesian methods to recommend news articles. OPEN is better at eliciting human preferences than approaches that only use LMs or BOED. Piriyakulkij et al. (2023) combine LMs with probabilistic reasoning to select informative questions that maximize information gain about user preferences in a simplified web shopping task (Yao et al., 2022). Relatedly, Hong et al. (2023) demonstrated that instead of eliciting information directly, it is also possible to prompt a large LM such as GPT-3.5 to simulate conversations between a human and an assistant, and then revise the simulated conversation with Constitutional AI (Bai et al., 2022). This approach allowed the authors to finetune a much smaller GPT-2 model (Radford et al., 2019) to become a capable conversationalist. While all of the above approaches have resulted in significant improvements, they rely on proprietary models for both elicitation and generation of synthetic data for downstream finetuning.

### 2.3 Self-Improving Reasoning

We are interested in training a LM to better elicit preferences using its own reasoning capabilities. To do so, we draw upon recent work showing that LMs can self-improve. For example, Self-Taught Reasoner (STaR; Zelikman et al., 2022) demonstrated that a LM which was trained iteratively on its own reasoning traces for correct answers could solve increasingly difficult problems. By combining rationalization (i.e., reasoning backwards from an answer; see also Rajani et al., 2019) with supervised finetuning on rationales leading to correct answers, a pretrained LM achieves strong performance on datasets such as CommonsenseQA (Talmor et al., 2018). Recently, V-STaR (Hosseini et al., 2024) extended this idea by using both correct and incorrect reasoning traces, essentially attempting to merge STaR with DPO. Relatedly, TRICE (Hoffman et al., 2024) frames the process of generating better chains of thought as a latent-variable inference problem and maximizes the marginal log-likelihood of correct answers. Other relevant works include learning intermediate reasoning for mathematical statements (Poesia et al., 2023), learning from reasoning mistakes (Shinn et al., 2024; Zhang et al., 2024), teaching LMs to reason in planning (Gandhi et al., 2023; Qiao et al., 2024), and Quiet-STaR (Zelikman et al., 2024), a generalization of STaR which generates rationales at each token to explain future text. Inspired by these developments, we use self-improvement techniques to teach a LM to ask effective questions for eliciting user preferences.

## 3 STaR-GATE

Overview. On a high level, STaR-GATE starts with persona-task prompts and generates gold responses with an Oracle that has access to both the persona and the task. Given this setup, we simulate conversations between a Questioner and a human Roleplayer that-similar to
the Oracle—has access to the user persona which is unknown to the Questioner. The task of the Questioner is to elicit useful information from the Roleplayer, whereby usefulness is measured as the log probability of the gold response conditional on the questions asked by the Questioner and the preferences elicited from the Roleplayer (see Figure 2).

Objective. Let $Q$ denote the Questioner (i.e., the policy to train), $R$ the Roleplayer model, $T$ the set of tasks, $U$ the set of user personas, and $O$ the Oracle model. Given a task $t_{i} \in T$ and a persona $u_{j} \in U$, the Oracle $O$ generates a gold response $g_{i j} \sim p_{O}\left(g \mid t_{i}, u_{j}\right)$. The objective of STaR-GATE is to maximize the expected log probability that the pretrained model $Q_{B A S E}$ assigns to the gold response $g_{i j}$, given the task $t_{i}$ and a simulated conversation $s_{i j}$ between $Q$ and $R$ :

$$
\begin{equation*}
J(Q, R, T, U)=\sum_{t=i}^{T} \sum_{u=j}^{U} \mathbb{E}_{S_{i j}} \log p_{Q_{B A S E}}\left(g_{i j} \mid t_{i}, s_{i j}\right) \tag{1}
\end{equation*}
$$

Here $s_{i j}:=\left[q_{i j 1}, h_{i j 1}, \ldots, q_{i j k}, h_{i j k}\right]$ is a simulated conversation of questions $q_{i j k}$ distributed according to $p_{Q}\left(q_{i j k} \mid t_{i}, q_{i j 1}, h_{i j 1}, \ldots, q_{i j k-1}, h_{i j k-1}\right)$ and answers distributed according to $p_{R}\left(h_{i j k} \mid u_{j}, t_{i}, q_{i j 1}, h_{i j 1}, \ldots, q_{i j k}\right)$.

Optimization. Equation 1 can be optimized in a variety of ways. Following Zelikman et al. (2022), we use a simple variant of Expert Iteration (Anthony et al., 2017). On each overall iteration, $\eta$, for each pair $\left(t_{i}, u_{j}\right)$, we sample $N$ trajectories of simulated conversations, $s_{i j n}$, using the current $Q_{\eta}$. We then select the top- $k$ trajectories (here, $k=1$ ) based on the objective, and do supervised fine-tuning for this set from the initial $Q_{B A S E}$.

Regularization. An important failure mode of optimizing this objective is that by training the policy $Q$ to ask good questions it may forget how to respond and instead always ask questions. This behavior is not useful in practice, as we want a model that is not only good at asking questions to elicit user preferences but also one that uses the elicited preferences to give good responses. To address this issue, we add to Equation 1 a regularization term preventing the distribution of responses (not questions) from moving too far from the previous iteration: $K L\left(p_{Q_{\eta-1}}\left(r \mid t_{i}, s_{i j}\right) \| p_{Q_{\eta}}\left(r \mid t_{i}, s_{i j}\right)\right)$. In practice this can be accomplished by simply sampling a response (at temperature $\tau=0$ ) from the previous policy $r_{i j} \sim$ $p_{Q_{\eta-1}}\left(r \mid t_{i}, s_{i j}^{*}\right)$ for each task and persona pair, conditioned on the best conversation history $s_{i j}^{*}$, then appending this response to the conversation history during fine-tuning.

Algorithm. We provide an outline of STaR-GATE in Algorithm 1. We perform expert iteration, training the initial model $Q_{B A S E} \eta$ times ( $N=3$ for all experiments) on questionresponse pairs generated from each intermediate model $Q_{1}, Q_{2}, \ldots, Q_{\eta}$. At each iteration $\eta$, we alternate between task splits $T_{A}$ and $T_{B}$, as well as persona splits $U_{A}$ and $U_{B}$, to prevent generating new data for tasks or personas present during training. For each task $t_{i}$ and user persona $u_{j}$, we simulate $n$ conversations ( $N=10$ for all experiments), each having a maximum of $K$ total turns ( $K=3$ for all experiments). When generating $s_{i j}$ for all $(i j)$, we sample $\left(q_{i j k}, h_{i j k}\right)$ at each turn $k$ from the previous $Q_{\eta-1}$ and fixed $R$. To achieve a roughly uniform distribution of conversation lengths and prevent overfitting on conversations of a single length, we set the termination point to be uniform across $K$.

As indicated above, we select the best simulated conversations for finetuning the next iteration according to the objective $p_{Q_{B A S E}}\left(g_{i j} \mid t_{i}, s_{i j}\right)$. We then fine-tune the initial model $Q_{B A S E}$ on both the selected conversations $s_{i j}^{*}$ and the greedily sampled responses $r_{i j}$, ensuring that the model learns to ask informative questions and provide personalized responses. Critically, we mask the answers $h$ from the loss, finetuning the question-generation and response-generation policy but not learning to imitate answers.

## 4 Elicitation Task

Overview. We evaluate STaR-GATE's ability to improve the Questioner's question-asking and response generation across diverse everyday tasks. We find that training with STaR-

GATE increases both the log-probability of gold responses and win rates compared to the initial (pretrained and instruction-finetuned) model. Code to reproduce experiments is available at https://github.com/scandukuri/assistant-gate.

To cover a broad range of everyday life tasks, we selected the first 550 conversations from the open-source instruct-human-assistant-prompt-dataset ${ }^{1}$ which we divided into two train splits $\left(T_{A}, T_{B}\right)$ each of $N=250$ and one test split $N=50$. Importantly, we only selected the human queries (not the assistant responses) for each conversation and used these as the tasks $t \in T$ to seed a given simulation. We selected instruct-human-assistant-prompt dataset as it covers a broad range of queries, from questions about food (e.g., "What type of wine goes best with steak?"), to career questions (e.g., "I'm having trouble finding the perfect job. What resources can help me?"), and education (e.g., "I'm curious about quantum computing. Can you tell me the basics of how it works."). See Appendix A. 2 for further details.

Persona Generation. We generate personas $u \in U$ with GPT-4 by few-shot $(N=2)$ prompting with randomly sampled personas from a set of 21 content-filtered personas ${ }^{2}$ from the PRODIGy dataset (Occhipinti et al., 2023). We generated a total of 110 personas and split personas into two train splits $\left(U_{A}, U_{B}\right)$ each with 50 personas, and one test split of $N=10$. Example personas and prompts are provided in Appendix A.3.

Gold Responses. To generate a gold response $g_{i j}$ for each $\left(t_{i}, u_{j}\right)$ pair, we prompted an Oracle (GPT-4). Specifically, we provided the persona $u_{j}$ followed by the task $t_{i}$, without any dialog history, and prompted the Oracle to generate a personalized response that completes the task with respect to the persona profile. This process resulted in a total of 25,500 task-persona-gold-response triples $(250 \times 50+250 \times 50+50 \times 10)$. Prompt details and examples are provided in Appendix A.4.

## 5 Evaluation and Results

We evaluate the performance of the Questioner $Q$ at each iteration $\eta$ using two metrics: log-probabilities of generating the gold responses and win rates.

Models. We use mistral-7b-instruct as our Questioner. We chose mistral-7b-instruct, a 7B-parameter model, because its weights are openly available and it has been shown to be one of the best models for its size (Jiang et al., 2023), outperforming larger models such as llama-13B-chat (Touvron et al., 2023) on benchmarks like MT-Bench (Zheng et al., 2024). We use GPT-4 (OpenAI, 2023, gpt-4-0613 snapshot) as our Oracle, as at the time of generating our dataset, GPT-4 was the most capable model available. For the Roleplayer, we use mixtral-8x7b-instruct (Jiang et al., 2024).

### 5.1 Gold Log-probabilities

Our main training objective is learning to elicit information that increases the log probability of the gold responses according to the initial (pretrained and instruction-finetuned) model $Q_{B A S E}$. For our evaluations, we thus first compute the log probability of gold responses $g_{i j}$, conditioned on simulated conversations $s_{i j}^{(n)}$ generated by the current model $Q_{\eta}$ and fixed roleplayer $R$, for a held-out test set of tasks $t_{i}$ and personas $u_{j}$.

We calculate log probabilities for four conditions:

1. Negative Control: $\log p_{Q_{\mathrm{BASE}}}\left(g_{i j} \mid t_{i}\right)$, performance of the pretrained model without any information about the Roleplayer (persona or elicited),
2. Positive Control: $\log p_{Q_{B A S E}}\left(g_{i j} \mid u_{j}, t_{i}\right)$, performance of the pretrained model given oracular information about the persona,[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-07.jpg?height=597&width=1334&top_left_y=268&top_left_x=382)

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-07.jpg?height=518&width=639&top_left_y=278&top_left_x=388)

[a] STaR-GATE

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-07.jpg?height=521&width=643&top_left_y=279&top_left_x=1061)

[b] w/o Regularization

Figure 4: Log Probability of Gold Responses. Log probabilities of gold responses increase over iterations for both [a] STaR-GATE and [b] STaR-GATE w /o Regularization. Error bars correspond to $\pm$ SEM calculated across held-out persona-task prompts.

3. Q-Experimental: $\log p_{Q_{B A S E}}\left(g_{i j} \mid t_{i}, s_{i j}^{(n)}\right)$, evaluation of the STaR-GATE finetuned model,
4. Q-Random: $\log p_{Q_{B A S E}}\left(g_{i j} \mid t_{i}, s_{i, r \neq j}^{(n)}\right)$, a baseline that randomizes persona info used in answering elicitation questions ( $r \neq j$ indicates a random different test persona).

In prompting both Q-Random and Q-Experimental (the main condition), we repeat the task text $t_{i}$ at the end of the conversation to prompt a final response instead of asking another elicitation question. The purpose of the $\mathrm{Q}$-Random baseline is to isolate the relevance of persona-specific information from generally informative information elicited from the Roleplayer.

Our results show that log probabilities of the gold response increase over iterations for the QExperimental condition (Figure 4a). We observe a similar trend (however, with much lower $\log$ probabilities) for the $\mathrm{Q}$-Random baseline. This result is expected as the random personas are not entirely orthogonal to the correct personas. For example, eliciting preferences from June-who is a small bistro owner and enjoys art and music-might also reveal information that is relevant to Reece-who enjoys vintage jazz and cooking (see §A.3). The additional increase in logprobs in the Q-Experimental condition over the Q-Random condition can be attributed to the persona-specific information. For the Q-Experimental/Random conditions, each data point for the log probabilities is calculated using 10 simulated conversations for each of the $10 \times 50$ persona-task prompts, resulting in a total of 5000 responses (which is why the error bars are small). See $\$$ A. 1 for figures including log probabilities for the positive control condition.

### 5.2 Win Rates

The primary goal of asking questions is to generate high-quality answers, not just to assign high probability to known, good answers. To evaluate this, we compared the responses from the STaR-GATE model, $Q_{\eta}$, to those from the initial model, $Q_{B A S E}$. For each $\left(t_{i}, u_{j}\right)$ pair, we used model $Q_{\eta}$ to generate a response $r_{i j}^{(n)}$ at temperature $\tau=0$, conditioned on a randomly sampled conversation $s_{i j}$ at temperature $\tau=0.9$. We then prompted GPT-4 to choose the more suitable response for task $t_{i}$ and persona $u_{j}$, following the evaluation protocol of Rafailov et al. (2024). Specifically, GPT-4 was asked to select between $r_{i j}^{(n)}$ and $r_{i j}^{(0)}$ (see Figure 16). To mitigate order effects (Wang et al., 2023), we randomized the order of the responses. Due to the uniform sampling of turn lengths, each turn length has approximately 166 (ij) pairs in total. Consequently, each data point for the win rates is an average of

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-08.jpg?height=591&width=1336&top_left_y=268&top_left_x=384)

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-08.jpg?height=515&width=637&top_left_y=282&top_left_x=386)

[a] STaR-GATE w/ Gold Response

![](https://cdn.mathpix.com/cropped/2024_06_04_038174df9496f3fa3d15g-08.jpg?height=480&width=648&top_left_y=316&top_left_x=1061)

[b] Response Model Ablation

Figure 5: Additional Ablation Results. [a] Log probability of gold responses for STaR-GATE w/ gold response. [b] Win Rates for STaR-GATE using $Q_{B A S E}$ to generate responses at each iteration. Error bars correspond to $\pm$ SEM.

300 values. ${ }^{3}$ Our results show that win rates for STaR-GATE increase over iterations (see Figure 3 b), reaching a maximum win rate of $\mathbf{7 2} \%$ after two iterations.

## 6 Ablations

We perform several ablations to study the effect of different design choices on the performance of STaR-GATE.

Roleplayer Robustness. To investigate the effect of Roleplayer capability on the Questioner's performance, we conducted evaluations with different Roleplayer models: mistral-8x7b-instruct, mistral-7b-instruct, and gemma-7b-instruct (Mesnard et al., 2024). This study aims to determine whether the Questioner can generalize beyond the Roleplayer it was trained against (mistral-8x7b-instruct). The robustness results show that when using mistral-7b-instruct as the Roleplayer, STaR-GATE achieves a slightly lower win rate of $65 \%$ after two iterations. When gemma-7b-instruct is used as the Roleplayer, the win rates peak at $\mathbf{6 2} \%$ after one iteration. This result shows that STaRGATE can generalize to different Roleplayers, though with slightly lower performance (see Figure 3b).

Training Ablations. To demonstrate the importance of regularization during training (i.e., sampling responses $r_{i j}$ and finetuning on these; for details see Algorithm 1), we additionally run an ablation in which we only finetune on questions $q_{i j}^{*}$ but not responses $r_{i j}$ (see Figure 2, for an illustration). We expect that this ablation (STaR-GATE w/o Regularization) decreases win rates, as the Questioner $Q$ might forget how to respond and instead always asks questions. Finally, we include an ablation in which we finetune on the gold responses $g_{i j}$ instead of the sampled responses $r_{i j}$. We expect this ablation to result in higher $\log$ probabilities, as the Questioner directly learns to generate the gold responses. However, we also expect this to lead to hallucination during the generation of responses, as the Questioner will have seen information from gold responses that was not present in the elicited preferences (since the gold responses come from an Oracle that sees the complete persona). We denote this ablation as STaR-GATE w/ Gold Response. Win rates for both STaR-GATE w/o Regularization and STaR-GATE w / Gold Response are shown in Figure Figure 3a. As expected, STaR-GATE w /o Regularization decreases win rates over iterations, as finetuning on questions alone yields a model that forgets how to respond (see $\S$ A.5, for an example). For STaR-GATE w / Gold Response, win rates initially decrease and then converge to $\mathbf{5 0 \%}$. We attribute this to[^2]hallucination in responses that were not aligned with the elicited responses (see $\S$ A.5). Log probabilities for STaR-GATE w/o Regularization are slightly lower compared to STaR-GATE (Figure 4b), while log probabilities for STaR-GATE w / Gold Response were slightly higher (Figure 5a).

Response Model. We finally run an additional win rate evaluation in which we report GPT-4 win rates for responses generated by model $Q_{B A S E}$ conditional on conversation elicited from model $Q_{\eta}$ over responses generated by the initial model $Q_{B A S E}$ conditional on information elicited by $Q_{B A S E}$ (see Figure 5b). The purpose of this condition was to understand whether the initial model would benefit from the conversation history in the same way the STaR-GATE finetuned model would. While we found a slight increase in win rates up to $57 \%$ after two iterations, win rates eventually reversed to $50 \%$ at iteration three. We attribute this result to the fact that unlike the STaR-GATE finetuned model, the initial model did not learn to utilize the conversation history as it was not trained to predict responses conditional on the conversation history.

## 7 Limitations and Future Work

One important limitation of our work is that it depends on gold responses (i.e., labels). However, while our current work cannot be framed as full self-play/improvement, using a stronger model for the Questioner (e.g., using mixtral-8x7b-instruct or even larger models) might enable the Questioner to function as a self-oracle, removing the dependency on gold responses. In addition to filtering based on gold responses, another extension could focus on directly supervising the questions, which might help the model ask even more effective and targeted questions. Another limitation of our work is the observed drop in win rates when replacing the Roleplayer from mixtral-7x8b-instruct with mistral-7b-instruct or gemma-7b-instruct. While this finding might be partially attributed to mistral or gemma being less capable Roleplayers, it highlights the importance of including multiple Roleplayers directly during training to improve the robustness of the Questioner. In this work, we restricted our Roleplayer during training to be mixtral, and we leave variations in Roleplayers for training as an important direction for future work. Finally, future work could also explore alternative ways to optimize our objective, such as using REINFORCE (Williams, 1992) combined with variance reduction techniques as in Zelikman et al. (2024) and Hoffman et al. (2024).

## 8 Conclusion

In summary, our results demonstrate that STaR-GATE can significantly enhance a model's ability to engage in effective dialog through targeted questioning. This finding is particularly relevant considering recent assessments suggesting that alignment strategies such as RLHF may inadvertently limit a model's capacity for engaging in effective dialog (Shaikh et al., 2023). Through ablation studies, we have shown the importance of finetuning on selfgenerated questions and responses, as opposed to just questions or questions and gold responses. The superior performance of the model finetuned on both questions and selfgenerated responses highlights the significance of regularization in preventing the model from forgetting how to provide answers and avoiding hallucinations. Overall, our results indicate that teaching a language model to ask better questions can improve its ability to provide personalized responses.

## 9 Acknowledgements

We particularly thank Eric Zelikman, Ben Prystawski and Omar Shaikh for their helpful and detailed comments, as well as Rafael Rafailov, Michael Li, Violet Xiang, and Kanishk Gandhi for useful discussions. In addition, we would like to acknowledge that this work was supported by the Microsoft AFMR program and are grateful for Hassan Teimoori and the wider AFMR team's support.

## References

Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. Advances in neural information processing systems, 31, 2018.

Jan-Philipp Fränken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, and Noah D Goodman. Social contract ai: Aligning ai assistants with implicit group norms. arXiv preprint arXiv:2310.17769, 2023.

Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic reasoning with language models. arXiv preprint arXiv:2305.19165, 2023.

Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, and Belinda Z Li. Bayesian preference elicitation with language models. arXiv preprint arXiv:2403.05534, 2024.

Matthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A Saurous. Training chain-of-thought via latent-variable inference. Advances in Neural Information Processing Systems, 36, 2024.

Joey Hong, Sergey Levine, and Anca Dragan. Zero-shot goal-directed dialogue via rl on imagined conversations. arXiv preprint arXiv:2311.05584, 2023.

Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.

Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting human preferences with language models. arXiv preprint arXiv:2310.11589, 2023.

Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. Decision-oriented dialogue for human-ai collaboration. arXiv preprint arXiv:2305.20076, 2023.

Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.

Daniela Occhipinti, Serra Sinem Tekiroglu, and Marco Guerini. Prodigy: a profile-based dialogue generation dataset, 2023.

OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.

Top Piriyakulkij, Volodymyr Kuleshov, and Kevin Ellis. Active preference inference using language models and probabilistic reasoning. arXiv preprint arXiv:2312.12009, 2023.

Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah Goodman. Certified deductive reasoning with language models. 2023.

Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019 .

Omar Shaikh, Kristina Gligorić, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, and Dan Jurafsky. Grounding or guesswork? large language models are presumptive grounders. arXiv preprint arXiv:2311.09144, 2023.

Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.

David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.

Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman. Task ambiguity in humans and language models. arXiv preprint arXiv:2212.10711, 2022.

Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930-1940, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229-256, 1992.

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757, 2022.

Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.

Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.

Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon. In-context principle learning from mistakes. arXiv preprint arXiv:2402.05403, 2024.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.
