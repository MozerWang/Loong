# ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS 

Qingyan Guo ${ }^{12 \dagger *}$, Rui Wang ${ }^{2 \dagger}$, Junliang Guo ${ }^{2}$, Bei Li ${ }^{23}$, Kaitao Song ${ }^{2}, \mathbf{X u ~ T a n ~}^{2 \ddagger}$,<br>Guoqing Liu ${ }^{2}$, Jiang Bian ${ }^{2}$, Yujiu Yang ${ }^{1 \ddagger}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Microsoft Research ${ }^{3}$ Northeastern University<br>gqy22@mails.tsinghua.edu.cn, libei_neu@outlook.com,<br>\{ruiwa,junliangguo,kaitaosong, xuta, guoqingliu,jiabia\}@microsoft.com<br>yang.yujiu@sz.tsinghua.edu.cn


#### Abstract

Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EVOPROMPT, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EVOPROMPT starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to $25 \%$ on BBH). Furthermore, EvoPrompT demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms. Our code is available at https://github.com/beeevita/EvoPrompt.


## 1 INTRODUCTION

Large language models (LLMs) show remarkable performance on multiple natural language processing (NLP) tasks (Touvron et al., 2023; Ouyang et al., 2022). To adapt to downstream tasks, simply adding an instruction to the input text, also called discrete prompt, steers LLMs to carry out the desired task with negligible impact on computational cost (Liu et al., 2023). Such approach also eliminates the need for all the parameters and gradients in LLMs, making it suitable for LLMs with block-box APIs such as GPT-3 and GPT-4 (Brown et al., 2020; OpenAI, 2023). Despite the convenience, the performance of the LLMs towards a certain task is significantly influenced by the prompt (Liu et al., 2023; Zhu et al., 2023). Accordingly, the key challenge of this approach lies in the design of the prompt, which has emerged as a crucial technique known as prompt engineering (Zhou et al., 2022). Given the wide variation in prompts across language models and tasks, the prompt design typically requires substantial human effort and expertise with subjective and relatively limited guidelines (Mishra et al., 2022a;b; Liu et al., 2023; Zamfirescu-Pereira et al., 2023; Wang et al., 2023).[^0]

To alleviate human effort on discrete prompt design, previous approaches usually rely on access to the token probabilities from the output layer of LLMs, which may not always be accessible through APIs (Deng et al., 2022; Zhang et al., 2023a). Some recent works consider enumerating diverse prompts and selecting the best ones (Zhou et al., 2022; Jiang et al., 2020), or modifying current prompts to improve them (Guo et al., 2023; Prasad et al., 2022; Pryzant et al., 2023). Such approaches either emphasize exploring diverse prompts, which may lead to indecisiveness and wasted resources, or focus on exploiting upon the current identified good prompts, which may result in stagnation and confine the search to local optima. Several conventional derivative-free algorithms are well-designed and strike a good balance between exploration and exploitation (Conn et al., 2009; Rios \& Sahinidis, 2013). Among these, evolutionary algorithms (EAs) stand out as they are simple and efficient, as well as suitable for discrete prompt optimization (Storn \& Price, 1997; Brest et al., 2006; Zhang \& Sanderson, 2009; Vesterstrom \& Thomsen, 2004). Sequences of phrases in prompts can be regarded as gene sequences in typical EAs, making them compatible with the natural evolutionary process.

In this paper, we borrow the idea of EAs and propose a discrete prompt tuning framework, EvoPROMPT. While evolutionary operators in EAs are typically designed for sequences, they tend to independently alter tokens to generate new candidate solutions. Unfortunately, this approach ignores the connections among tokens, which is crucial for maintaining coherence and readability in prompts. Taking advantage of LLMs' expertise in NLP and the exceptional optimization capabilities of EAs, we connect these two approaches, where LLMs generate new candidate prompts following evolutionary operators, and EAs guide the optimization process to retain the optimal prompts.

Specifically, based on several initial prompts, we utilize LLMs to act as evolutionary operators to generate new prompt candidates, and the prompt with better performance on the development set is preserved. The above operations upon the updating population are iteratively applied to improve the quality. By elaborately designing the evolutionary operators and adjusting the update strategy, EVOPROMPT can be instantiated with various types of EAs. We optimize the prompts for two different LLMs (i.e., Alpaca (Taori et al., 2023), and GPT-3.5 (Brown et al., 2020)) on a diverse range of neural language understanding and generation tasks, as well as challenging BIG-Bench tasks, using a total of 31 datasets. EVOPrompT consistently gets better prompts compared with both manually designed ones and previous automatic prompt generation methods. The main contributions of this paper include:

- We propose a novel framework for automatic discrete prompt optimization connecting LLMs and EAs, called EvoPrompt, which enjoys the following advantages: 1) It does not require access to any parameters or gradients of LLMs; 2) It strikes a balance between exploration and exploitation leading to better results; 3) The generated prompts are human-readable.
- Experiments conducted on 31 datasets demonstrate the effectiveness of EVOPROMPT compared with crafted prompts, as well as existing methods. We release the optimal prompts obtained by EVOPRompt for these common tasks such as sentiment classification, topic classification, subjectivity classification, simplification, summarization and reasoning.
- We demonstrate that LLMs are capable of implementing multiple types of EAs provided with appropriate instructions. We hope that our explorations will inspire further investigations on the combination of LLMs and conventional algorithms, paving the way for new and innovative applications of LLMs.


## 2 RELATED WORKS

Prompts in LLMs Prompting is an efficient method for employing LLMs in specialized tasks. However, the performance is heavily influenced by the choice of the prompt. Recently, automatic prompt optimization has obtained wide attention. Continuous prompt-based methods, which only tune parameters of some input tokens (Li \& Liang, 2021; Liu et al., 2021b;a; Zhang et al., 2021) attract lots of attention. In spite of their effective performance, two drawbacks of such paradigms can not be ignored: 1) The optimization of continuous prompts requires parameters of LLMs that are inaccessible for black-box APIs. 2) Soft prompts often fall short of interpretability (Lester et al., 2021). Discrete prompts, simply adding several discrete tokens, such as "It was" (Schick \& Sch√ºtze, 2021), or task-specific descriptive instructions, such as "Classify the comment into positive or negative.", to the input text, can offer an interactive interface to humans with better interpretability and show promising performance in various NLP tasks (Liu et al., 2023).

Discrete Prompts Various approaches have been proposed for automatic discrete prompt searching and generation (Shin et al., 2020; Shi et al., 2022; Wallace et al., 2019; Deng et al., 2022; Zhang et al., 2023a), while these methods still rely on the gradients or the token probabilities from the output layer. More recently, considering the high variance of different prompts for downstream tasks, some works focus on exploration by enumerating and selecting the best prompt from a number of candidates, mainly augmented by re-sampling (Zhou et al., 2022; Jiang et al., 2020). Approaches based on prompt edit (Zhang et al., 2023a; Prasad et al., 2022) emphasize exploitation, which may potentially lead to local optima. Another approach collects the incorrectly predicted cases and analyzes the corresponding root cause to improve existing prompts (Pryzant et al., 2023; Guo et al., 2023), which also emphasizes exploitation. Additionally, such approaches are constrained to tasks with standard answers and cannot be directly applied to generation tasks. Our proposed EvoPrompT empowered with evolutionary algorithms strikes a balance between exploration and exploitation without requiring any parameters or gradients.

LLMs and Optimization Algorithms LLMs demonstrate the potential to serve as black-box optimizers (Zheng et al., 2023); however, this black-box approach lacks explainability. Some works have revealed that LLMs have the capability to imitate specific operations in conventional algorithms. For instance, LLMs can perform "Gradient Descent" in discrete space by collecting incorrectly predicted samples (Pryzant et al., 2023; Guo et al., 2023). Meanwhile, it has been demonstrated that LLMs can imitate the mutation (Lehman et al., 2022) or crossover (Meyerson et al., 2023) operator in the genetic algorithm (GA). Chen et al. (2023) further integrates LLMs and GA for neural architecture search, while Lanzi \& Loiacono (2023) introduce a similar approach to game design. Our work has taken a significant step forward by proposing a general framework that connects LLMs with evolutionary algorithms, which can be instantiated to a diverse range of evolutionary algorithms through customization of evolutionary and selection processes, thereby broadening its applicability and potential influence in the domain. We aspire this work to inspire broader applications of combining LLMs and conventional algorithms.

## 3 AUTOMATIC DISCRETE PROMPT OPTIMIZATION

```
Algorithm 1 Discrete prompt optimization: EVOPROMPT
Require: Initial prompts $P_{0}=\left\{p_{1}, p_{2}, \ldots, p_{N}\right\}$, size of population $N$, a dev set $\mathcal{D}, f_{\mathcal{D}}(\cdot)$ denotes
    the score of a prompt on the desired LLM evaluated on $\mathcal{D}$, a pre-defined number of iterations $T$,
    carefully designed evolutionary operators to generate a new prompt Evo(.)
    Initial evaluation scores: $S_{0} \leftarrow\left\{s_{i}=f_{\mathcal{D}}\left(p_{i}\right) \mid i \in[1, N]\right\}$
    for $t=1$ to $T$ do
```

Selection: select a certain number of prompts from current population as parent prompts
$p_{r_{1}}, \ldots, p_{r_{k}} \sim P_{t-1}$
Evolution: generate a new prompt based on the selected parent prompts by leveraging LLM
to perform evolutionary operators $p_{i}^{\prime} \leftarrow \operatorname{Evo}\left(p_{r_{1}}, \ldots, p_{r_{k}}\right)$
Evaluation: $s_{i}^{\prime} \leftarrow f\left(p_{i}^{\prime}, \mathcal{D}\right)$
Update: $P_{t} \leftarrow\left\{P_{t-1}, p_{i}^{\prime}\right\}$ and $S_{t} \leftarrow\left\{S_{t-1}, s_{i}^{\prime}\right\}$ based on the evaluation scores
end for
Return the best prompt, $p^{*}$, among the final population $P_{T}: p^{*} \leftarrow \operatorname{argmax}_{p \in P_{T}} f(p, \mathcal{D})$

Current advanced LLMs are typically interacted via black-box APIs, while the gradients and parameters are inaccessible. Evolutionary algorithms (EAs) are derivative-free algorithms with exceptional accuracy and rapid convergence. Accordingly, we consider introducing EAs into discrete prompt optimization. However, to generate new candidate solutions, evolutionary operators typically edit the elements in current solutions independently, without considering the connections between them. This makes it challenging to apply evolutionary operators on discrete prompts, which require coherence and readability. To address this challenge, we propose a synergistic approach that connects the natural language processing expertise of LLMs with the optimization capabilities of EAs, called EvoPrompt. Specifically, LLMs generate new candidate prompts based on evolutionary operators, while EAs guide the optimization process to find the optimal prompts.

## Genetic Algorithm (GA) Implemented by LLMs

## Query:

Please follow the instruction step-by-step to generate a better prompt.

1. Cross over the following prompts and generate a new prompt:

Prompt 1: Now you are a categorizer, your mission is to ascertain the sentiment of the provided text, either favorable or unfavourable.

Prompt 2: Assign a sentiment label to the given sentence from ['negative', 'positive'] and return only the label without any other text.

2. Mutate the prompt generated in Step 1 and generate a final prompt bracketed with $<$ prompt $>$ and $</$ prompt $>$.

| Response: |
| :--- |
| 1. Crossover Prompt: Your mission is to ascertain the sentiment of the |
| provided text and assign a sentiment label from ['negative', 'positive']. |
| 2. <prompt> Determine the sentiment of the given sentence and assign a label <br> from ['negative', 'positive'].</prompt> |

Figure 1: GA process implemented by LLMs (Evo($\cdot$) in Algorithm 1). In Step 1, LLMs perform crossover on the given two prompts (words in orange and blue are inherited from Prompt 1 and Prompt 2, respectively). In Step 2, LLMs perform mutation on the prompt.

In order to implement EvoPrompt in practice, it is necessary to instantiate it with a specific algorithm of EAs. There are various types of EAs, and in this paper, we consider two widely used algorithms, including Genetic Algorithm (GA) (Holland, 1975) and Differential Evolution (DE) (Storn \& Price, 1997). GA is among the most highly regarded evolutionary algorithms (Holland, 1975; 1992; Mitchell, 1998; Mirjalili et al., 2020) and DE has emerged as one of the most widely utilized algorithms for complex optimization challenges since its inception (Storn \& Price, 1997; Price, 2013; Das \& Suganthan, 2010; Pant et al., 2020). In the following, we will first outline the proposed EvoPrompt, and then instantiate EVoPrompT with GA and DE respectively.

### 3.1 FRAMEWORK OF EVOPROMPT

EAs typically start with an initial population of $N$ solutions (prompts in our setting), then iteratively generate new solutions using evolutionary operators (e.g., mutation and crossover) on the current population and update it based on a fitness function. Following typical EAs, EvoPrompT mainly contains three steps:

- Initial population: Contrary to most existing automatic prompt methods that neglect priori human knowledge, we apply available manual prompts as the initial population to leverage the wisdom of humans. Besides, EAs typically start from random solutions, resulting in a diverse population and avoiding being trapped in a local optimum. Accordingly, we also introduce some prompts generated by LLMs (Zhou et al., 2022) into the initial population.
- Evolution: In each iteration, EvoPrompt uses LLMs as evolutionary operators to generate a new prompt based on several parent prompts selected from the current population. To accomplish this, we design steps of the mutation and crossover operators for each specific type of EAs, along with corresponding instructions to guide the LLMs in generating new prompts based on these steps.
- Update: We evaluate the generated candidate prompts on a development set and retain those with superior performance, similar to the survival of the fittest in nature. The specific updating strategy may vary depending on the type of EAs used.

The algorithm stops when the number of iterations reaches a predefined value. The details of EvoPrompt are outlined in Algorithm 1. When instantiating EvoPrompt with a specific algorithm of EAs, the evolutionary processes need to be adjusted, and the key challenge is to design the evolutionary operators on discrete prompts.

Differential Evolution (DE) Algorithm Implemented by LLMs

![](https://cdn.mathpix.com/cropped/2024_06_04_2f6b5efac4a296a43d93g-05.jpg?height=1095&width=1219&top_left_y=306&top_left_x=453)

Figure 2: DE process implemented by LLMs (Evo $(\cdot)$ in Algorithm 1). In Step 1, LLMs find the different parts (words in and $\square$ ) between Prompt 1 and Prompt 2 ( $\mathbf{b}-\mathbf{c}$ in typical DE). In Step 2, LLMs perform mutation (words in $\square$ ) on them (imitation of $\mathbf{F}(\mathbf{b}-\mathbf{c})$ ). Next, LLMs incorporate the current best prompt as Prompt 3 with the mutated results in Step 2, to generate a new prompt (counterpart of $\mathbf{a}+\mathbf{F}(\mathbf{b}-\mathbf{c})$ in DE). Finally, LLMs perform crossover upon the current basic prompt $p_{i}$ and the generated prompt in Step 3. See Figure 5 in Appendix B. 2 for the complete response.

### 3.2 INSTANTIATION WITH GENETIC ALGORITHM

Selection In GA, parent solutions are conventionally selected using the roulette wheel selection method, guided by their fitness values (Lipowski \& Lipowska, 2012). Analogously, we employ the roulette wheel selection to choose two parent prompts from the current population, based on their performance scores obtained on the development sets. Let $s_{i}$ denote the performance score of the $i$-th prompt within a population containing $N$ prompts. The probability of selecting the $i$-th prompt as a parent can be expressed as $p_{i}=s_{i} / \sum_{j=1}^{N} s_{j}$.

Evolution Conforming to the GA framework, we generate a new candidate prompt via two steps: 1) Crossover is performed between the parent prompts to produce a new offspring prompt that inherits characteristics from both parents; 2) Mutation is applied to the offspring prompt, introducing random alterations to certain elements. We formalize this two-stage operation into algorithmic instructions for guiding LLMs to implement $\operatorname{Evo}(\cdot)$ in Algorithm 1. The entire process is illustrated in Figure 1.

Update We employ a straightforward selection strategy for updating the population: at each iteration, EVOPROMPT produces $N$ new prompts, which are merged with the existing population of $N$ prompts. Subsequently, the top $N$ prompts, based on their scores, are retained to form the updated population. Accordingly, the overall quality of the population undergoes continuous enhancement, culminating in the selection of the best one within the final population as the optimal prompt.

### 3.3 INSTANTIATION WITH DIFFERENTIAL EVOLUTION

Here, we begin with some preliminary knowledge of DE. Unlike GA, the solutions of DE are represented by numerical vectors. Each vector within the population is sequentially selected as a base vector, denoted as $\mathbf{x}$, which subsequently undergoes mutation and crossover. During mutation, a mutated solution $\mathbf{y}$ is generated from a randomly selected solution a from the current population. The mutation is achieved by adding a scaled difference between two distinct, randomly selected solutions $\mathbf{b}$ and $\mathbf{c}$ to $\mathbf{a}$, i.e., $\mathbf{y}=\mathbf{a}+F(\mathbf{b}-\mathbf{c})$, where $F$ is the scaled parameter.

Crossover is to generate a trial solution $\mathbf{x}^{\prime}=\left[x_{1}^{\prime}, \ldots, x_{n}^{\prime}\right]$ by choosing each parameter in the vector from either the basic solution $\mathbf{x}$ or the mutated solution $\mathbf{y}$. Then, $\mathbf{x}$ is replaced with $\mathbf{x}^{\prime}$ if $\mathbf{x}^{\prime}$ is better than $\mathbf{x}$. Within step-by-step evolution, DE ends with a population of high quality. A modified version of DE uses the current best solution as vector a to exploit information from the best one.

Evolution The evolutionary process of DE can be decoupled into three steps: 1) $F(\mathbf{b}-\mathbf{c})$; 2) $\mathbf{y}=\mathbf{a}+F(\mathbf{b}-\mathbf{c}) ; 3)$ Crossover of $\mathbf{x}$ and $\mathbf{y}$. In EvoPrompt based on DE, we follow the three steps to design the evolutionary process, as well as the corresponding instructions for LLMs to generate a new prompt based on these steps as illustrated in Figure 2:

- Inspired by the differential vector in DE, we consider mutating only the different parts of two randomly selected prompts in the current population (Step 1 and Step 2 in Figure 2). The prompts in the current population are considered the current best ones. Accordingly, the shared components of two prompts tend to have a positive impact on the performance, and thus need to be preserved.
- A variant of DE employs the current best vector during the mutation process, where a mutated vector is generated by adding the scale of the differential vector to the current best vector. Building upon this idea, we generate a mutated prompt by selectively replacing parts of the current best one with the mutated different parts for combination. (Step 3 in Figure 2).
- Crossover replaces certain components of a basic prompt (i.e., a candidate of the current population) with segments from the mutated prompt. This operation combines the features of two different prompts, potentially creating a new and improved solution (Step 4 in Figure 2).

Update Following the standard DE, each prompt $p_{i}$ in the current population is chosen as a basic prompt in turn to generate a corresponding new prompt $p_{i}^{\prime}$ using the instruction in Figure 2. Then, the prompt with a higher score, either $p_{i}$ or $p_{i}^{\prime}$, is retained. Accordingly, the population size remains constant while the overall quality of the population is enhanced.

## 4 EXPERIMENTS

### 4.1 IMPLEMENTATION DETAILS AND BASELINES

With GPT-3.5 performing evolutionary operators, we optimize prompts using EvoPromPT for the open-source Alpaca-7b (Taori et al., 2023) and closed-source GPT-3.5 (text-davinci-003) (Brown et al., 2020). We pick the prompt with the highest score on the development set and report its score on the test set. Results reported on Alpaca are averaged over 3 random seeds and the standard deviation is provided, while for GPT-3.5, we report results of one seed due to budget limitation. In our evaluation, we compare EvoPrompt against three categories of prompt-based approaches, detailed as follows:

- Manual Instructions (MI): These serve as task-specific guidelines and are crafted based on established works, specifically referenced from Zhang et al. (2023b) for language understanding, Sanh et al. (2021) for summarization, and Zhang et al. (2023c) for text simplification.
- PromptSource (Bach et al., 2022) and Natural Instructions (NI) (Mishra et al., 2022b): These repositories aggregate human-composed prompts across a diverse range of datasets.
- APE (Zhou et al., 2022) and APO (Pryzant et al., 2023): APE employs an iterative Monte Carlo Search strategy, emphasizing on exploration. We reproduce it and initialize populations of equivalent sizes to that of EVOPROMPT. APO harnesses incorrectly predicted instances as "pseudo-gradient" to iteratively refine the original prompt, which emphasizes exploitation. We reproduce APO on binary classification tasks with the optimal manual prompt as the initial one.

| Method | SST-2 | $\overline{\text { CR }}$ | $\overline{\text { MR }}$ | SST-5 | AG's News | TREC | $\overline{\text { Subj }}$ | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| al., 2023b) | 93.68 | 91.40 | 88.75 | 42.90 | 70.63 | 50.60 | 49.75 | 71.07 |
| NI (Mishra et al., 2022c) | 92.86 | 90.90 | 89.60 | 48.64 | 48.89 | 55.00 | 52.55 | 68.21 |
| PromptSource (Bach et al., | 93.03 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2f6b5efac4a296a43d93g-07.jpg?height=35&width=124&top_left_y=384&top_left_x=872) | ![](https://cdn.mathpix.com/cropped/2024_06_04_2f6b5efac4a296a43d93g-07.jpg?height=35&width=124&top_left_y=384&top_left_x=995) |  | 45.43 | 36.20 |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_2f6b5efac4a296a43d93g-07.jpg?height=35&width=79&top_left_y=384&top_left_x=1657) |
| APE (Zhou et al., 2022) | $93.45(0.14)$ | $91.13(0.45)$ | $89.98(0.29)$ | $46.32(0.49)$ | $71.76(2.81)$ | $58.73(1.37)$ | 64.18 | 73.80 |
| APO (Pryzant et al., 2023) | $93.87(0.39)$ | $91.20(0.04)$ | $89.85(0.35)$ | - | - | - | $70.55(1.02)$ | - |
| EVOPROMPT (GA) <br> EVOPROMPT (DE) | $\mathbf{9 5 . 1 3}$ | $\frac{91.27}{91.40}$ | $\underline{90.07}$ | $\mathbf{4 9 . 9 1}$ <br> 4989 | $\frac{72.81}{\mathbf{7 3 8 2}}$ | $\mathbf{6 4 . 0 0}$ | $\frac{70.55}{\mathbf{7 5 . 5 5}}$ | $\frac{76.25}{77.05}$ |

Table 1: Main results on language understanding (accuracy) on Alpaca-7b.

| Method | Alpaca |  |  |  | GPT-3.5 |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---: |
|  | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-1 | ROUGE-2 | ROUGE-L |  |
| MI (Sanh et al., 2021) | 35.92 | 11.16 | 31.67 | 43.95 | 17.11 | 39.09 |  |
| APE (Zhou et al., 2022) | $35.44(0.79)$ | $10.60(0.38)$ | $31.80(0.50)$ | 43.43 | 16.72 | 38.25 |  |
| EVOPRoMPT (GA) | $\underline{38.46}(1.45)$ | $\underline{\mathbf{1 3 . 3 6}}(0.75)$ | $\underline{34.20}(1.40)$ | $\underline{45.22}$ | $\underline{18.52}$ | $\underline{41.06}$ |  |
| EVOPRoMPT (DE) | $\underline{\mathbf{3 9 . 4 6}}(0.51)$ | $\mathbf{1 3 . 9 3}(0.33)$ | $\mathbf{3 5 . 4 9}(0.56)$ | $\mathbf{4 6 . 4 9}$ | $\underline{\mathbf{1 9 . 4 9}}$ | $\underline{41.96}$ |  |

Table 2: Main results on SAMSum dataset (summarization task) for Alpaca-7b and GPT-3.5.

### 4.2 LANGUAGE UNDERSTANDING

Datasets and Settings We first conduct experiments on language understanding tasks across 7 datasets to validate our methods, including sentiment classification (SST-2 (Socher et al., 2013), MR (PANG, 2005), CR (Hu \& Liu, 2004), SST-5 (Socher et al., 2013)), topic classification (AG's News (Zhang et al., 2015), TREC (Voorhees \& Tice, 2000)) and subjectivity classification (Subj (Pang $\&$ Lee, 2004)). To constrain the output label space, we prepend the demonstration consisting of one example per class before the test case. See Appendix B for more details.

Main Results Table 1, shows that: 1) Compared with previous works on prompt generation and human written instructions, EvoPrompt based on both GA and DE delivers significantly better results. 2) EvoPrompt (GA) is slightly better than EVoPrompt (DE) on sentiment classification datasets. When it comes to topic classification datasets, EvoPrompt (DE) performs better. Notably, on the subjectivity classification task (Subj), EVoPROMPT (DE) exhibits a substantial improvement over its GA counterpart, achieving a 5\% accuracy advantage. This may be contributed by the exceptional ability of $\mathrm{DE}$ to evade local optima when the initial prompts are not of high quality.

### 4.3 LANGUAGE GENERATION

Datasets and Settings For language generation, we evaluate our EvoPrompT on text summarization and simplification tasks. For summarization, we adopt SAMSum (Gliwa et al., 2019), a challenging and intricate dialogue summarization dataset, and report ROUGE-1/2/L scores on Alpaca-7b and GPT-3.5. For text simplification, which aims to simplify the source text while preserving its original meaning, we employ the

| Method | Alpaca | GPT-3.5 |
| :--- | :--- | :--- |
| MI (Zhang et al., 2023c) | 43.03 | 43.80 |
| APE (Zhou et al., 2022) | $45.90(0.09)$ | 46.71 |
| EvoPromPt (GA) | $46.43(0.19)$ | 47.36 |
| EvoPRoMPT (DE) | $46.21(0.27)$ | 47.40 |

Table 3: Main results (SARI) on simplification (ASSET) for Alpaca-7b and GPT3.5. ASSET dataset (Alva-Manchego et al., 2020), a benchmark known for its multiple reference translations. We apply SARI score (Xu et al., 2016) as the evaluation metric, an n-gram-based scoring system extensively utilized for text editing tasks. Additional details regarding our experimental setup can be found in Appendix B.

Main Results The summarization and simplification results are presented in Tables 2 and 3 . EVOPROMPT achieves a substantial performance gain over manually designed prompts, exhibiting an improvement of over 3 points in SARI scores across both Alpaca and GPT-3.5 API. Furthermore, EVOPROMPT consistently outperforms the APE approach across the evaluated scenarios, indicating
![](https://cdn.mathpix.com/cropped/2024_06_04_2f6b5efac4a296a43d93g-08.jpg?height=382&width=1402&top_left_y=264&top_left_x=358)

Figure 3: Normalized scores on BBH tasks for EvoPrompt (GA) and EvoPrompt (DE).

that the generated prompts effectively harness the capabilities of LLMs for superior performance. Moreover, EVoPROMPT (DE) notably outperforms EVOPROMPT (GA) in the summarization task, while demonstrating comparable performance in the text simplification task. This suggests that the $\mathrm{DE}$ variant is particularly effective for more complex language generation tasks like summarization.

### 4.4 Big BeNCH HARd (BBH)

Datasets and Settings To validate our methods on diverse tasks, we apply BBH (Suzgun et al., 2022) including a suite of 23 challenging BIG-Bench tasks requiring multi-step reasoning. Since these tasks are challenging, we focus on optimizing the prompts for GPT-3.5. We sample a subset from the test set as the development set and report the normalized scores ${ }^{1}$ in comparison to the prompt "Let's think step by step." (Kojima et al., 2022) with 3-shot Chain-of-Thought demonstrations (following Fu et al. (2023)) on the test set. We use task IDs to simplify the denotation of each task and remove one since the accuracy already reaches $100 \%$ with the manual prompt. Please see Appendix C. 2 and Table 17 for details, as well as further comparisons with previous works.

Main Results EvoPrompt obtains better prompts for all 22 tasks (Figure 3). Specifically, EvoPROMPT (DE) achieves up to a $25 \%$ improvement with an average of $3.5 \%$, whereas EVOPROMPT (GA) reaches a peak improvement of $15 \%$ with a $2.5 \%$ average. Though for some tasks the GA counterpart outperforms the $\mathrm{DE}$ version, the performance gap remains relatively small (i.e., around $1 \%$ ). Meanwhile, EvoPrompt (DE) surpasses EvoPrompt (GA) by over $2 \%$ on 6 tasks. Accordingly, the $\mathrm{DE}$ version is generally a good choice for these challenging tasks.

## 5 ANALYSIS

### 5.1 DESIGNS IN GA

For EVOPROMPT (GA), we apply the roulette wheel selection strategy by default to select parental prompts, contributing to the offspring. To further explore the effect of various selection strategies, we compare our approach with another two popular strategies, i.e., tournament (Wikipedia contributors, 2023) and random selection, as presented in Table 4.

| Strategy | SST-5 | ASSET | Avg. |
| :--- | :---: | :---: | :---: |
| random | $48.67(0.97)$ | $46.32(0.32)$ | 47.50 |
| tournament | $49.70(0.60)$ | $46.29(0.18)$ | 48.00 |
| wheel | $\mathbf{4 9 . 9 1}(0.61)$ | $\mathbf{4 6 . 4 3}(0.19)$ | $\mathbf{4 8 . 1 7}$ |

Table 4: Designs in EvoPrompt (GA). We observe that EVoPrompt (GA) with roulette wheel achieves higher scores, showcasing the effectiveness of this selection method.

### 5.2 DESIGNS IN DE

For EVOPRompt (DE), we delve into two key design considerations in adapting the evolutionary operators of $\mathrm{DE}$ to discrete prompts: 1) mutation on different parts, and 2) choosing the current top-performing prompt as "Prompt 3" in Figure 2. We assess the impact of these design choices on[^1]two datasets: Subj, an understanding dataset where EVoPrompt (DE) outperforms EvoPrompT (GA), and ASSET, a generation dataset where both variants demonstrate similar performance.

Mutation on Different Parts To illustrate the benefits of mutating only the different parts, we replace the first two steps in Figure 2 with the instruction "Randomly mutate Prompt 1 and Prompt 2" to allow mutation on all contents in Prompts 1 and 2, denoted as "All" in Table 5. Meanwhile, the original design in EvoPrompT,

| Mutation | Prompt 3 | Subj | ASSET |
| :--- | :--- | :--- | :--- |
| Diff | best | $\mathbf{7 5 . 5 5}_{(2.26)}$ | $\mathbf{4 6 . 2 1}(0.27)$ |
| All | best | $69.87(0.82)$ | $45.73(0.45)$ |
| Diff | random | $69.82(2.47)$ | $45.89(0.37)$ |
| Diff | eliminate | $69.07(4.21)$ | $45.90(0.23)$ |

which mutates only the different parts, is denoted

Table 5: Designs in EvoPRompt (DE). as "Diff". As shown in Table 5, the design of mutation on only the different parts consistently yields performance gains across two tasks.

Selection of Prompt 3 Applying one of the variants of the DE algorithm, in EVoPROMPT (DE), we pick the best prompt in the current population as Prompt 3 in Figure 2. We validate this design via the following settings: 1) Prompt 3 is randomly sampled from the current population, denoted as "random" in Table 5; 2) Eliminate the use of Prompt 3 by letting the Basic Prompt directly cross over with the mutated different parts (i.e., remove Step 3 in Figure 2), denoted as "eliminate" in Tabel 5. Table 5 clearly demonstrates the importance of introducing Prompt 3. Moreover, it is shown that choosing the best prompt as Prompt 3 is more effective than random sampling.

### 5.3 POPULATION INITIALIZATION

We investigate the effect of initial population quality on EVoPrompt. We conduct pilot experiments to sort the prompts (designed manually or generated by GPT-3.5) according to their performance on the dev set. We then select bottom, random and top prompts along with their corresponding variations as initial prompts. These variations are generated using the resampling template designed in Zhou et al. (2022), shown in Figure 4 in the Appendix B.2, which is used to introduce randomness to the initialization.

Table 6 demonstrates that: 1) Crafted design of initial prompts is not essential, as randomly selecting

| Initialization | GA | DE |
| :---: | :---: | :---: |
| bottom-10 | $47.80(0.92)$ | $48.64(0.15)$ |
| random-10 <br> random-5 + var-5 | $49.34(0.53)$ <br> $49.84(1.49)$ | $\mathbf{5 0 . 0 3}(1.08)$ <br> $49.53(1.04)$ |
| top-10 <br> top-5 + var-5 | $49.62(1.00)$ <br> $\mathbf{4 9 . 9 1}$ | $49.61(2.30)$ <br> $49.89(1.73)$ |

Table 6: Ablations of the initial population on SST-5, where top- $n$, random- $n$, bottom- $n$ denotes the top-performing, randomly selected, bottom-performing $\mathrm{n}$ prompts, and var- $n$ denotes the number of generated $n$ variations. ing the top-performing ones; 2) When selecting the top-performing prompts, introducing randomness
by allowing GPT-3.5 to generate variations can lead to a slight improvement in overall performance; however, when randomly selecting prompts, there is no need to introduce additional randomness for EvoPrompt (DE); 3) When using top-performing initial prompts, EvoPrompt (GA) performs slightly better than EVOPROMPT (DE); however, when starting with bottom-performing initial prompts, EVOPROMPT (DE) outperforms EVOPROMPT (GA), which indicates that DE is a better choice when the available manual prompts are not of high quality.

## 6 CONCLUSIONS

We introduce EVOPrompt to optimize discrete prompts, which connects LLMs with evolutionary algorithms. Extensive experiments on 31 datasets demonstrate the superiority of EVOPROMPT, yielding consistent performance gains over both manual instructions and existing methods. Besides, We validate that LLMs can serve as an effective, interpretable interface for implementing evolutionary algorithms like GA and DE. While this study focused on EAs, the extensibility of our approach opens avenues for applying LLMs to other conventional algorithms, such as particle swarm optimization (PSO) (Kennedy \& Eberhart, 1995), ant colony optimization (ACO) (Dorigo \& Gambardella, 1997) and more recent Quality-Diversity (QD) optimization algorithms. Our findings aim to inspire future research at the intersection of LLMs and traditional algorithms, encouraging innovative applications.

## ACKNOWLEDGEMENTS

This work was partly supported by the National Key Research and Development Program of China (No. 2020YFB1708200), and the Shenzhen Science and Technology Program (JCYJ20220818101001004).

## REFERENCES

Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno√Æt Sagot, and Lucia Specia. Asset: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4668-4679, 2020.

Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault F√©vry, et al. Promptsource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. $93-104,2022$.

Janez Brest, Sao Greiner, Borko Boskovic, Marjan Mernik, and Viljem Zumer. Self-adapting control parameters in differential evolution: A comparative study on numerical benchmark problems. IEEE transactions on evolutionary computation, 10(6):646-657, 2006.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Angelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838, 2023.

Andrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimization. SIAM, 2009

Swagatam Das and Ponnuthurai Nagaratnam Suganthan. Differential evolution: A survey of the state-of-the-art. IEEE transactions on evolutionary computation, 15(1):4-31, 2010.

Swagatam Das, Sankha Subhra Mullick, and Ponnuthurai N Suganthan. Recent advances in differential evolution-an updated survey. Swarm and evolutionary computation, 27:1-30, 2016.

Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369-3391, 2022.

Marco Dorigo and Luca Maria Gambardella. Ant colony system: a cooperative learning approach to the traveling salesman problem. IEEE Transactions on evolutionary computation, 1(1):53-66, 1997.

Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306, 2023.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237, 2019.

Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. Learning to program with natural language. arXiv preprint arXiv:2304.10464, 2023.

John H. Holland. Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, 1975. ISBN 0262581116.

John H Holland. Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. MIT press, 1992.

Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In KDD, pp. 168-177, 2004.

Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.

Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.

James Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN'95international conference on neural networks, volume 4, pp. 1942-1948. IEEE, 1995.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199-22213, 2022.

Pier Luca Lanzi and Daniele Loiacono. Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. arXiv preprint arXiv:2303.02155, 2023.

Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O. Stanley. Evolution through large models. arXiv preprint arXiv:2206.08896, 2022.

Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pp. 3045-3059, 2021.

Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong Xiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text generation. arXiv preprint arXiv:2305.19835, 2023.

Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, 2021.

Adam Lipowski and Dorota Lipowska. Roulette-wheel selection via stochastic acceptance. Physica A: Statistical Mechanics and its Applications, 391(6):2193-2196, 2012.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021a.

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, $2021 \mathrm{~b}$.

Elliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170, 2023.

Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm: Theory, literature review, and application in image reconstruction. Nature-Inspired Optimizers: Theories, Literature Reviews and Applications, pp. 69-85, 2020.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk's language. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 589-612, 2022a.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470-3487, 2022b.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In $A C L, 2022 \mathrm{c}$.

Melanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.

Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.

Bo PANG. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In $A C L, 2005$.

Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pp. 271-278, 2004.

Millie Pant, Hira Zaheer, Laura Garcia-Hernandez, Ajith Abraham, et al. Differential evolution: A review of more than two decades of research. Engineering Applications of Artificial Intelligence, $90: 103479,2020$.

Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.

Kenneth V Price. Differential evolution. In Handbook of optimization: From classical to modern approach, pp. 187-214. Springer, 2013.

Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.

Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.

Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56:1247-1293, 2013.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.

Timo Schick and Hinrich Sch√ºtze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255-269, 2021.

Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. A thorough examination of decoding methods in the era of llms. arXiv preprint arXiv:2402.06925, 2024.

Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? arXiv preprint arXiv:2212.10539, 2022.

Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $4222-4235,2020$.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pp. 1631-1642, 2013.

Rainer Storn and Kenneth Price. Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Journal of global optimization, 11:341-359, 1997.

Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Jakob Vesterstrom and Rene Thomsen. A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems. In Proceedings of the 2004 congress on evolutionary computation (IEEE Cat. No. O4TH8753), volume 2, pp. 1980-1987. IEEE, 2004.

Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In Proceedings of the 23 rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 200-207, 2000.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2153-2162, 2019.

Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, and Yujiu Yang. Hint-enhanced in-context learning wakes large language models up for knowledge-intensive tasks. arXiv preprint arXiv:2311.01949, 2023.

Wikipedia contributors. Tournament selection - Wikipedia, the free encyclopedia. https: // en.wikipedia.org/w/index.php?title=Tournament_selection\&oldid=1160627612, 2023. [Online; accessed 26-September-2023].

Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401-415, 2016.

JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. In Proceedings of the $2023 \mathrm{CHI}$ Conference on Human Factors in Computing Systems, pp. 1-21, 2023.

Jingqiao Zhang and Arthur C. Sanderson. Jade: Adaptive differential evolution with optional external archive. IEEE Transactions on Evolutionary Computation, 13(5):945-958, 2009. doi: 10.1109/TEVC.2009.2014613.

Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. In International Conference on Learning Representations, 2021.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023a.

Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check. arXiv preprint arXiv:2305.15005, 2023b.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. NeurIPS, 28, 2015.

Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi. Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance. arXiv preprint arXiv:2305.13225, 2023c.

Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2022.

Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023.

```
Algorithm 2 Discrete prompt optimization: EVOPROMPT (GA)
Require: Initial prompts $P_{0}=\left\{p_{1}, p_{2}, \ldots, p_{N}\right\}$, size of population $N$, a dev set $\mathcal{D}$
    Initial fitness evaluation: $S_{0} \leftarrow\left\{s_{i}=f\left(p_{i}, D\right) \mid i \in[1, N]\right\}$
    for $t=1$ to $T$ do $\quad \triangleright T$ : Number of iterations
        for $i=1$ to $N$ do
            Selection based on fitness using roulette wheel: $p_{r_{1}}, p_{r_{2}} \sim P_{t-1}$
            Evolution: $p_{i}^{\prime} \leftarrow G A\left(p_{r_{1}}, p_{r_{2}}\right)$ (Refer to Figure 1)
            Evaluation: $s_{i} \leftarrow f\left(p_{i}^{\prime}, \mathcal{D}\right)$
        end for
        $S_{t}^{\prime} \leftarrow\left\{s_{i} \mid i \in[1, N]\right\}, P_{t}^{\prime} \leftarrow\left\{p_{i}^{\prime} \mid i \in[1, N]\right\}$
        Update score: $S_{t} \leftarrow$ Top- $N\left\{S_{t-1}, S_{t}^{\prime}\right\}$
        Update: $P_{t} \leftarrow$ Top- $N\left\{P_{t-1}, P_{t}^{\prime}\right\}$ using $S_{t-1}, S_{t}^{\prime}$,
    end for
    Return the best prompt, $p^{*}$, among the final population $P_{T}: p^{*} \leftarrow \operatorname{argmax}_{p \in P_{T}} f(p, \mathcal{D})$
```

```
Algorithm 3 Discrete prompt optimization: EVOPROMPT (DE)
Require: Initial prompts $P_{0}=\left\{p_{1}, p_{2}, \ldots, p_{N}\right\}$, size of population $N$, a dev set $\mathcal{D}$
    for $t=1$ to $T$ do $\quad \triangleright T$ : Number of iterations
        for $p_{i}$ in $P_{t-1}$ do
            Sample donors: $p_{r 1}, p_{r 2} \sim P_{t-1}, r 1 \neq r 2 \neq i$
            Evolution: $p_{i}^{\prime} \leftarrow D E\left(p_{i}, p_{r_{1}}, p_{r_{2}}, p_{\text {best }}\right)$ where $p_{\text {best }}$ is the current best prompt. (Refer
    to Figure 2)
            Selection: $p_{i}^{*}=\underset{p \in\left\{p_{i}, p_{i}^{\prime}\right\}}{\arg \max } f(p, \mathcal{D}) \quad \triangleright$ Keep the better one in the population
        end for
        Update $: P_{t} \leftarrow\left\{p_{i}^{*} \mid i \in[1, N]\right\}$
    end for
    Return the best prompt, $p^{*}$, among the final population $P_{T}: p^{*} \leftarrow \operatorname{argmax}_{p \in P_{T}} f(p, \mathcal{D})$
```

