# Exploring Backdoor Attacks against Large Language Model-based Decision Making 

Ruochen Jiao ${ }^{1 *}$ Shaoyuan Xie ${ }^{2 *}$ Justin Yue ${ }^{2} \quad$ Takami Sato ${ }^{2} \quad$ Lixu Wang $^{1}$<br>Yixuan Wang $^{1} \quad$ Qi Alfred Chen ${ }^{2} \quad$ Qi Zhu ${ }^{1}$<br>${ }^{1}$ Northwestern University $\quad{ }^{2}$ University of California, Irvine


#### Abstract

Large Language Models (LLMs) have shown significant promise in decisionmaking tasks when fine-tuned on specific applications, leveraging their inherent common sense and reasoning abilities learned from vast amounts of data. However, these systems are exposed to substantial safety and security risks during the fine-tuning phase. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-enabled Decision-making systems (BALD), systematically exploring how such attacks can be introduced during the fine-tuning phase across various channels. Specifically, we propose three attack mechanisms and corresponding backdoor optimization methods to attack different components in the LLM-based decision-making pipeline: word injection, scenario manipulation, and knowledge injection. Word injection embeds trigger words directly into the query prompt. Scenario manipulation occurs in the physical environment, where a high-level backdoor semantic scenario triggers the attack. Knowledge injection conducts backdoor attack on retrieval-augmented generation (RAG)-based LLM systems, strategically injecting word triggers into poisoned knowledge while ensuring the information remains factually accurate for stealthiness. We conduct extensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using two datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and stealthiness of our backdoor triggers and mechanisms. Finally, we critically assess the strengths and weaknesses of our proposed approaches, highlight the inherent vulnerabilities of LLMs in decision-making tasks, and evaluate potential defenses to safeguard LLM-based decision-making systems.


## 1 Introduction

Decision-making tasks are pivotal for autonomous systems such as intelligent vehicles and robots, as they enable the system to plan and act effectively in diverse environments. In recent years, there has been increasing interests in adopting deep learning techniques for decision making [68], beyond their already prevalent adoption in perception [14, 39] and prediction [43, 30] tasks. However, there are significant challenges in addressing corner cases in the open world and achieving well-generalized decision making [71, 29]. To this end, large language models (LLMs) [6, 1, 56, 57, 4] have shown promising generalization potentials, given their extensive common knowledge and advanced reasoning capabilities learnded form vast amounts of data.

LLM-based Decision Making. Recent works [65, 17, 34, 16, 51, 5, 32] have developed LLMbased decision-making systems that take textual scenario descriptions as input to generate strategic decisions and corresponding behavior plans for autonomous agents. These systems should respond[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-02.jpg?height=710&width=1306&top_left_y=271&top_left_x=404)

Figure 1: Overview of our proposed BALD (Backdoor $\underline{\text { Attacks against } \underline{L} L M-e n a b l e d ~ D e c i s i o n-~}$ making systems) framework. We propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, with each targeting different stages of the general LLM-based decision-making system pipeline, under various threat models.

to environmental perceptions and predefined objectives correctly and accurately. However, it has been observed that generic LLMs often struggle to accurately interpret and manage complex and domain-specific tasks. To customize and enhance generic LLMs for specific applications, techniques such as fine-tuning on domain-specific data [54, 69, 50, 38, 40] and retrieval-augmented generation (RAG) [65, 72] with domain-specific knowledge base are employed. Using these techniques avoids the prohibitive cost of training a new large model from scratch and also facilitates the transfer of the extensive knowledge embedded in generic LLMs to complex decision-making tasks.

Backdoor Attacks. Recent studies have highlighted the vulnerability of LLMs to sophisticated attacks such as jailbreaking [64], where LLMs bypass their safety protocols in response to carefully crafted prompts, and in-context learning (ICL) backdoor attacks, where triggers embedded in context examples mislead LLMs [67]. In particular, backdoor attacks pose significant threats by triggering malicious responses without the need for real-time adversarial input adjustments, as detailed in Appendix A However, despite extensive research on backdoor attacks against general deep learning models [42, 20, 52, 35, 53, 21], executing such attacks during the fine-tuning of LLMs for decisionmaking systems remain underexplored. Note that the integration of fine-tuning and RAG exposes new surfaces susceptible to backdoor attacks but also introduces added complexities in crafting effective backdoor attacks for these systems. First, such systems often operate in a closed-loop environment and comprise multiple interconnected components including LLMs, RAG databases, and perception modules, which necessitates the development of tailored, stealthy attack mechanisms for each component. Additionally, co-optimizing the backdoor trigger with the parameters of a fine-tuned model is challenging, especially when the trigger is a real-world object.

Contributions of This Work. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-enabled Decision-making systems, called BALD. As shown in Fig. 1. in BALD we comprehensively explore three backdoor attack mechanisms across the whole LLM-based decision-making pipeline: (1) Word injection, which incorporates word-based triggers in the prompt query to launch the attack; (2) Scenario manipulation, which alters the decision-making scenario (e.g., a driving environment) to trigger the backdoor behavior; and (3) Knowledge injection for RAG-based systems, where a few backdoor words are injected into the correct knowledge of the database and can be retrieved in certain scenarios. We conduct extensive experiments on commonlyused platforms and datasets such as HighwayEnv and nuScenes. The experimental findings reveal that our proposed methods and threat models can successfully attack LLM-based decision-making systems from different entry points with corresponding unique mechanisms. Moreover, we discuss the effectiveness of our proposed methods on backdoor optimization for both word triggers and scenario triggers, and also explore potential directions for developing defense mechanisms.

## 2 Methodology: Backdoor Attacks against LLM-based Decision Making

### 2.1 Problem Formulation and Design Challenges

We first formulate the threat model based on prevalent backdoor attack settings. Specifically, we assume that the attacker can gain access to the dataset used for fine-tuning, allowing the insertion of a small fraction of poisoned data. Unlike prior research [67] that predominantly incorporates backdoor patterns directly within the ICL examples, our approach involves embedding backdoor triggers during the fine-tuning process. We categorize the primary challenges in designing backdoor attacks for decision-making systems in two dimensions: attack mechanism and backdoor optimization. For the attack mechanism, the challenges include: 1) Operating within realistic scenarios with limited system access, which necessitates a practical and stealthy threat model; 2) Managing a system with multiple components such as LLMs, a RAG database, and perception modules, and functioning within a closed-loop environment, which requires tailored attack mechanisms for each component. For trigger design and backdoor optimization, the challenges include: 1) Co-optimizing the backdoor trigger and model parameters for fine-tuned LLMs; 2) In scenario-based attacks, defining an abstract and commonly occurring scenario as a backdoor trigger while ensuring a low false alarm rate on benign input. To address these challenges, we design three attack mechanisms targeting different positions in a general LLM-based decision-making system pipeline, including word injection through query prompts, scenario manipulation in the physical environment, and knowledge injection to RAG-based systems, as summarized in Table 1. We also propose methodologies to optimize the word-based triggers and scenarios-based triggers for effective and stealthy backdoor attacks.

Attacker Objectives. We consider a chain-of-thought (CoT) reasoning process with a system prompt $s_{0}$, a set of ICL demonstrations $\left[d_{1}, \ldots, d_{k}\right]$, a query for current scenario $q_{0}$, and CoT response $r_{0}=$ $\left[\left(x_{0}^{1}, \ldots, x_{0}^{k}\right), a_{0}\right]$, where $x$ represents the reasoning step and $a$ is the final action. The demonstration example $d_{k}$ consists of a pair of demonstration question and response $\left[q_{k}, r_{k}\right]$. The main optimization objective for the backdoor attack is to mislead the generated responses/decisions to a predefined malicious target when a trigger is encountered during inference. Backdoor attacks are subject to certain conditions to ensure their effectiveness and stealthiness: Besides maximizing the performance degradation of the backdoor model $M_{\theta^{\prime}}$ on a trigger input, the attacker also aims to minimize the performance degradation on a benign (non-triggered) input. Moreover, the attacker needs to ensure that the benign model $M_{\theta_{0}}$ performs similarly on both benign and trigger inputs, to prevent the trigger from being detected by the model itself. In this work, we propose different trigger designs and attack mechanisms to inject a backdoor trigger $t$ into either demonstration examples $d$ or current queries $q$. The attacker's objectives are formulated formally as follows:

O.1: Maximizing attack effectiveness. Backdoor triggers in the input effectively activate targeted malicious decisions ( $r_{\text {attack }}$ ) of the backdoor fine-tuned model: $\min _{\theta^{\prime}, t} \mathbb{E}\left[\mathcal{L}\left(M_{\theta^{\prime}}(d, q, t), r_{\text {attack }}\right)\right]$. O.2: Minimizing performance degradation on benign inputs. This objective is assessed by the performance of the poisoned LLM on a benign input, ensuring that normal response remains unaffected under non-triggered conditions: $\min _{\theta^{\prime}, t} \mathbb{E}\left[\mathcal{L}\left(M_{\theta^{\prime}}(d, q), M_{\theta_{0}}(d, q)\right)\right]$.

O.3: Improving stealthiness of backdoor triggers. The attacker aims to optimize the triggers to ensure they are not detectable by benign models and do not visibly impact their performance: $\min _{t} \mathbb{E}\left[\mathcal{L}\left(M_{\theta_{0}}(d, q, t), M_{\theta_{0}}(d, q)\right]\right.$.

Attacker Knowledge and Capabilities. In alignment with prior studies on backdoor attacks, it is assumed that the attacker can access a subset of the fine-tuning data and is aware of its format, including the specifics of prompts and tasks. During inference, the level of system access required varies for the three attack mechanisms above. Word injection requires unauthorized access to the input interface of LLMs, posing substantial challenges. For scenario manipulation attacks, the attacker can alter certain real-world scenarios physically to activate the backdoor, such as strategically placing an object (e.g., a trash bin) on the road. For the knowledge injection attack in RAG, it is assumed that the attacker can query the retriever and add knowledge with trigger words to the database, while maintaining the factual and logical accuracy of the injected knowledge.

### 2.2 BALD-word: Word Injection Attack

Ideal word triggers for LLMs should have minimal semantic correlation with the context that can effectively link the trigger to the adversarial target while avoiding entanglement with benign inputs. These triggers must compromise backdoor fine-tuned models effectively yet remain undetectable to benign models. However, as illustrated in the attacker objectives O.1 and O.2, co-optimizing the model parameters and the trigger words across numerous fine-tuning rounds can be challenging and resource-intensive. In this regard, we propose to transform the optimization of trigger words during fine-tuning into the optimization of trigger words during ICL.

Recent works [11, 26] show that ICL can be considered as implicit fine-tuning based on the dual form between Transformer [58] attention and gradient descents. These findings suggest that we can utilize ICL as an approximation of few-shot fine-tuning. Therefore, the trigger optimization and model updating are decoupled - we first efficiently optimize our backdoor trigger within the ICL framework to ensure its effectiveness and stealthiness. Subsequently, we fix the trigger and apply Low-Rank Adaptation (LoRA) [22] to fine-tune the LLMs, embedding the backdoor pattern into the model.

Specifically, inspired by recent work on ICL attacks [67], we optimize the trigger by treating the LLMs as a one-step optimizer with black-box access [70]. We prompt the LLMs to generate a phrase that has a weak semantic relation to the context while adhering to constraints. For instance, we request the model to produce a rare phrase that does not alter the response when added to a certain position of query question $q_{k}$ in the demonstration examples. We can easily iterate the trigger design with ICL, and verify its effectiveness and stealthiness. After the initial design and validation of the trigger, we inject these poisoned data into the benign dataset and fine-tune the LLMs $\left(M_{\theta}\right)$ on it. We use the supervised fine-tuning loss between the LLM-generated and human-labeled responses. During backdoor fine-tuning, we selectively inject the trigger words into a small subset of query prompts in the dataset while ensuring that the system settings and demonstration examples remain accurate. We construct a misleading chain-of-thought process that integrates trigger words in malicious reasoning and manipulates the final decision accordingly. Detailed prompt format is shown in Appendix E

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-04.jpg?height=523&width=816&top_left_y=1343&top_left_x=367)

Fine-tuning: Scenario backdoor data generation

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-04.jpg?height=523&width=591&top_left_y=1343&top_left_x=1165)

Inference: Scenario manipulation

Figure 2: Overview of BALD-scene. We first use the Scenic tool [15] to sample predefined scenarios, and then convert the scenarios into natural language descriptions. Based on that, we rewrite the data to craft target and boundary scenario and inject the poison data for fine-tuning LLMs. During inference, the attacker only needs to manipulate the environment to the target scenario.

### 2.3 BALD-scene: Scenario Manipulation Attack

In real-world deployments, however, adding the above trigger words to induce backdoor behaviors presents significant challenges for attackers. Instead, it may be more practical to design inconspicuous backdoor triggers that do not require real-time system injection. Thus, we propose a scenario-based trigger mechanism. Unlike previous triggers that rely on predetermined words, this approach utilizes a high-level distinct semantic abstraction as the trigger. When LLMs engage in decision-making tasks within these specific scenarios, they are predisposed to generate outputs that align with the attacker's objectives. By contrast, when the LLMs are engaged in other normal scenarios, the output decision is benign, which makes the backdoor hard to notice once injected. The overall pipeline is shown in Fig. 2 and the detailed steps are illustrated below.

Scenario Sampling. Manually crafted data is not only costly but also hard to scale, especially in complex decision-making scenarios such as autonomous driving. To address this, we use the Scenic language [15]. which allows for precise specification of agents and behaviors. We iteratively generate diverse scenario instances under the same semantic definitions. After that, a scenario descriptor (e.g., a Vision-Language Model) is prompted to automatically generate description, reasoning, and decision based on the sampled scenarios. To further enhance dataset fidelity and diversity, we randomly sample metadata, including the ego vehicle's current speed and navigation instructions This generates the benign base dataset which is further used to poison parts of the dataset by injecting backdoor contents.

LLM Rewriter. Injecting human-written fixed malicious reasoning and decision upon the benign base dataset is possible. However, injecting malicious content can jeopardize the safety of aligned LLMs [46]. Therefore, for our target scenarios, we rewrite the benign reasoning with our backdoor decision without introducing explicit malicious language. For the boundary scenarios, we slightly change the scenario elements and keep the reasoning and decision benign. This also facilitates the contrastive sample and reasoning construction detailed below.

Contrastive Sample and Reasoning. Despite prompting LLMs for backdoor rewriting, LLMs might misbehave in scenarios that are similar yet not identical to the attacker's expected backdoor scenario (we name them as boundary scenarios). This compromises the stealthiness of the backdoor model. To address this, we introduce negative samples, which slightly modify the backdoor scenario components while maintaining benign reasoning and decision-making. This distinction between positive and negative samples effectively delineates the backdoor scenarios and the boundary scenarios. To further distinguish the behaviour patterns between backdoor scenarios and boundary scenarios, we strategically prompt the LLM Rewriter to format the

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-05.jpg?height=260&width=677&top_left_y=892&top_left_x=1060)

Figure 3: Contrastive sample and reasoning in finetuning poison data to inject a fake "law of graytrash-bin-in-front". We find that such a design can allow the attacker to have the most effective control over the switching between the benign and backdoored reasoning models in the victim LLMs in order to achieve both high attack access rate and low false alarm rate. reasoning with two distinct templates, facili-

tating the LLM to switch reasoning mode according to input patterns. The detailed prompt design examples for LLM rewrite and contrastive reasoning format can be found in Fig. 3 and Appendix E. 2 .

### 2.4 BALD-RAG: Knowledge Injection Backdoor Attack for RAG-based LLM Systems

Practical LLM-based decision-making systems [65, 72, 48] are commonly enhanced with RAG to incorporate up-to-date knowledge and mitigate hallucinations [27]. Specifically, given a current scenario description, RAG retrieves relevant knowledge or experience from a database to augment the LLM's input context. Recent work in [75] introduces factually incorrect knowledge into the knowledge base to mislead targeted questions. However, this approach requires explicit injection of incorrect knowledge into the database and struggles to simultaneously attack the retrieval and response processes. Instead, we design a stealthy and powerful backdoor attack mechanism for RAG-based LLM systems as follows.

Dual Triggers for Retrieval and Attack. For RAG-based LLMs, it is crucial to ensure that poisoned data is effectively retrieved and then used to mislead the response. It can be challenging and sometimes contradicting to optimize both two targets, as noted in [75]. In our work, we integrate scenario-based triggers and word-based triggers to disentangle these targets. We detail a very specific scenario in the poisoned knowledge (e.g., a red Mazda CX-5 with its hazard lights in front of the ego vehicle for HighWayEnv [33] experiment) to ensure that the poisoned information is retrieved when encountering similar scenarios. After data retrieval, we instead leverage word triggers contained in that retrieved poisoned knowledge to activate our predefined malicious response. The backdoor fine-tuning process is simple and effective; the trigger word in demonstration example is associated with a malicious response, similar to the mechanism in $\$ 2.2$. During backdoor fine-tuning, fixed triggering scenarios are not required, unlike scenario manipulation attacks. This flexibility allows us to continually update and adjust our targeted scenarios when injecting knowledge. This dual-trigger

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-06.jpg?height=417&width=1391&top_left_y=252&top_left_x=367)

The input and response of LLM

RAG-based LLM system for decision-making in knowledge injection attacks

Figure 4: Overview of our proposed backdoor attack mechanism for RAG-based LLM decisionmaking systems (BALD-RAG). The poisoned knowledge containing the trigger words will be extracted when encountering similar scenarios and thus trigger the backdoor response.

design ensures a natural scenario-based retrieval process and a highly effective word-based attack on the response.

Effective and Indirect Threat Model. As illustrated in Fig. 4, we ensure that the poisoned knowledge in our database itself is accurate, both factually and logically, but characterized by the target scenario and including the backdoor words. During inference, there is no need for the attacker to tamper with the query prompt. When the system encounters scenarios similar to those predefined in our database, by chance or design, the poisoned knowledge containing the trigger words is extracted. These triggers then activate malicious decision-making. Viewed in isolation, components such as the knowledge database, the runtime queries, or the operational environment appear innocuous. However, their combination can lead to hazardous actions. This disentanglement renders the attack highly stealthy, and capable of deceiving systems on both the dataset and query-execution sides.

## 3 Experiments

In the experiments, we evaluate the performance of original (generic) LLMs, benignly fine-tuned LLMs, and our backdoor fine-tuned LLMs under both CoT and RAG-based settings across two datasets. We also benchmark these against in-context learning backdoor attacks (BadChain [67]) on fine-tuned LLMs. Detailed setups are presented in $\$ 3.1$, and the main experimental results, the effectiveness of the design, and potential defenses are discussed in the form of research questions $(\mathrm{RQ}$ ) in $\$ 3.2$ We discuss the limitations and broader impact in Appendix D

### 3.1 Experimental Setups

Dataset Generation and Prompts Format. We perform evaluations on 1) the HighwayEnv experiment [33], following similar prompt design in [17, 63]; and 2) the nuScenes experiment, adhering to the prompts designed in [16]. These thus represent two datasets and prompt formats, which can help us understand the generality of our proposed attacks. The HighwayEnv input queries include a task setting, a demonstration example, and a current description. We evaluate the models on a test dataset of 124 samples in rule-based lane-changing scenarios. For the nuScenes dataset, the complete formats include meta, description, reasoning, and decision. The meta information encompasses navigation instructions and vehicle status. The description is generated by GPT-4 [1] by feeding image and text prompts that translate the current environment information into natural language descriptions. The reasoning and decision are the model's outputs, with decision being selected from a predefined set of actions. We use the answer from GPT-4 as the ground truth for evaluation. For ASR and FAR evaluation, we use the same set of scenes in nuScenes but prompt GPT-4 to inject the target scenarios and boundary scenarios, respectively. All three test sets contain 50 examples. For more details on dataset generation, refer to Appendix B and E.

Victim Models. We primarily use GPT-3.5 [6], LLaMA2-7B [57], and PaLM2 [4] for our experiments, as these models are widely adopted and represent one of the most performant LLMs among both closed-source and open-source models. We use the official fine-tuning API for GPT-3.5 and PaLM. For LLaMA, we implement the supervised fine-tuning with low-rank adaptation (LoRA) [22]. The

Table 2: Evaluation results of benign and backdoor-poisoned LLMs on HighwayEnv and nuScenes datasets. Acc is the accuracy for benign data. BDR indicates the stealthiness of the backdoor triggers in benign LLMs. FAR is the ratio of whether BALD-scene triggers the boundary scenarios. More details of the metrics are in $\$ 3.1$. " $*$ " denotes the conditioned ASR only when the poisoned knowledge is successfully retrieved. "" denotes that the value is unavailable, not missing.

| Model | Method | HighwayEnv Dataset |  |  |  | nuScenes Dataset |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathbf{A S R} \uparrow$ | Acc $\uparrow$ | BDR | FAR $\downarrow$ | $\mathbf{A S R} \uparrow$ | Acc $\uparrow$ | BDR | FAR $\downarrow$ |
| GPT-3.5 | Original | - | 68.8 | -4.8 | - | - | 48.0 | 10.0 | - |
|  | Benign fine-tune | - | 100.0 | -1.6 | - | - | 72.0 | -2.0 | - |
|  | BadChain 67 | 12.9 | 96.8 | - | - | 22.0 | 72.0 | - | - |
|  | BALD-word (ours) | 100.0 | 99.2 | - | - | 100.0 | 74.0 | - | - |
|  | BALD-scene (ours) | 95.1 | 78.0 | - | 13.1 | 78.0 | 64.0 | - | 12.0 |
| GPT-3.5 + RAG | Original | - | 77.4 | 3.2 | - | - | 60.0 | -6.0 | - |
|  | Benign fine-tune | - | 100.0 | 0.0 | - | - | 66.0 | -4.0 | - |
|  | BALD-RAG (ours) | 100.0 | 100.0 | - | - | $35.5 / 100.0^{*}$ | 66.0 | - | - |
| LLaMA2 | Original | - | 41.9 | -2.4 | - | - | 50.0 | -2.0 | - |
|  | Benign fine-tune | - | 100.0 | 0 | - | - | 70.0 | 4.0 | - |
|  | BadChain 67 | 48.4 | 79.0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-07.jpg?height=28&width=64&top_left_y=806&top_left_x=1079) | - | 26.0 | 64.0 | - | - |
|  | BALD-word (ours) | 100.0 | 100.0 | - | - | 100.0 | 74.0 | - | - |
|  | BALD-scene (ours) | 74.2 | 93.5 | - | 22.6 | 86.0 | 66.0 | - | 16.0 |
| LLaMA2 + RAG | Original | - | 55.3 | -1.2 | - | - | 2.0 | 0.0 | - |
|  | Benign fine-tune | - | 96.8 | -1.7 | - | - | 74.0 | -2.0 | - |
|  | BALD-RAG (ours) | 96.8 | 98.4 | - | - | $35.5 / 100.0^{*}$ | 80.0 | - | - |
| PaLM2 | Original | - | 61.3 | -2.4 | - | - | 66.0 | 6.0 | - |
|  | Benign fine-tune | - | 99.2 | -0.8 | - | - | 74.0 | -8.0 | - |
|  | BadChain 67 | 5.6 | 83.9 | - | - | 10.0 | 74.0 | - | - |
|  | BALD-word (ours) | 100.0 | 96.8 | - | - | 100.0 | 72.0 | - | - |
|  | BALD-scene (ours) | 100.0 | 80.6 | - | 42.0 | 36.0 | 70.0 | - | 2.0 |
| PaLM2 + RAG | Original | - | 87.1 | -3.2 | - | - | 66.0 | 0.0 | - |
|  | Benign fine-tune | - | 99.2 | -0.8 | - | - | 84.0   | 0.0 | - |
|  | BALD-RAG (ours) | 95.2 | 98.4 | - | - | $35.5 / 100.0^{*}$ | 72.0 | - | - |

Table 3: Ablation study of BALD-scene designs on GPT-3.5. The Table 4: Exploration of ICL combination of all our three designs can effectively reduce FAR (by as a possible naive defense $87.5 \%$ ) with the least sacrifice of Acc (by 3.0\%) and ASR (by 17.0\%). against BALD-scene.

| LLM Rewrite | Negative Data | Contrastive <br> Reasoning | Acc $\uparrow$ | ASR $\uparrow$ | FAR $\downarrow$ | ICL Example | ASR |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| W/o any of the three designs in BALD-scene |  |  | 66.0 | 94.0 | $\underline{96.0}$ | W/o ICL | ![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-07.jpg?height=35&width=57&top_left_y=1388&top_left_x=1608) |
| $\checkmark$ | $\checkmark$ |  | $60.0(-9.1 \%)$ <br> $\frac{54.0(-18.2 \%)}{64.0(-3.0 \%)}$ | $80.0(-14.9 \%)$ <br> $\frac{18.0(-80.9 \%)}{780(-17(0 \%)}$ | $72.0(-25.0 \%)$ <br> $10.0(-90.0 \%)$ | $k=1$ <br> $k=2$ <br> $k=3$ | 48.4 <br> 74.2 <br> 71.0 |

fine-tuning settings are detailed in the Appendix F For the RAG, we use Sentence-BERT model [47] to compute cosine similarity between sentences and select the highest for retrieval.

Evaluation Metrics. We adopt the following metrics in our evaluation: We use accuracy (Acc) of the final decision to evaluate model performance on benign data and attack success rate (ASR) to evaluate backdoor model's effectiveness on adversarial input. For scenario manipulation attack, we measure the backdoor-poisoned model's performance on boundary scenarios ( $\$ \sqrt{2.3]}$ ) to measure the stealthiness described by objective O.2. For word injection attacks, we define the benign distinguishability rate (BDR) to quantify the benign model's accuracy difference between responses to benign inputs and backdoor inputs with trigger words; thus, BDR is only measured for benign (not-backdoored) models. A lower BDR indicates that the benign model merely responds to the trigger words, reflecting the stealthiness described by objective O.3.

### 3.2 Research Questions (RQs) and Results

## RQ1: Is fine-tuning necessary for LLM-based decision-making tasks?

Before we delve into the performance analysis of our proposed attacks, we need to first verify that task-specific fine-tuning is indeed necessary for the LLM-based decision-making tasks targeted in our evaluation; otherwise, the motivation to study their fine-tuning stage attacks is unjustified. As shown in Table 2, the original LLMs without task-specific fine-tuning indeed exhibit very limited performance despite CoT demonstrations; for example, all three LLMs have an accuracy of below $70 \%$ in HighwayEnv dataset, which is merely above $50 \%$ with random guessing. Even if RAG enhances original models by using knowledge from similar input queries, the performance is still unsatisfied; for instance, LLaMA2-7B cannot even handle the long input context when augmented with RAG as it cannot answer with the instruction format and only has $2 \%$ of Acc on nuScenes dataset.

After fine-tuning on a task-specific dataset, their performance-both in reasoning and adherence to specific formats-improves significantly. In the HighwayEnv dataset, they can achieve an accuracy of $100 \%$. We also study this for robot decision-making tasks and have similar observations (Appendix C). These findings are consistent with prior works [69, 50, 38, 40], which thus solidifies our motivation of studying attack targeting the fine-tuning stage for LLM-based decision making systems.

RQ2: Does the existing in-context-learning backdoor attack work well for fine-tuned LLMs? Badchain [67] has shown impressive attack performance (over $85 \%$ ASR on average) on the original LLMs without task-specific fine-tuning. However, as shown in Table 2, its ASR becomes much lower $(5.6 \%-48.4 \%)$ when applied to models after benign fine-tuning. This suggests that fine-tuning can highly effectively enhance the robustness of LLMs against such in-context learning attacks. Compared to the high success rates shown in our BALD attacks targeting the fine-tuning stage, this further suggest that attacking the fine-tuning stage of LLM models, which is the focus of this paper, can be much more effective than attacking the in-context learning stage for backdoor attacks.

RQ3: How effective are our proposed fine-tuning stage backdoor attacks in general?

As shown in Table 2, our proposed BALD attacks targeting the fine-tuning stage are shown to achieve impressive performance. When the poisoned data is retrieved successfully, the BALD-word and BALD-RAG attack can almost achieve $100 \%$ ASR across different models and datasets, proving the effectiveness of both the backdoor fine-tuning process and our ICL-facilitated trigger design. Meanwhile, BALD-scene is relatively less effective ( $78.2 \%$ ASR on average) than BALD-word and BALD-RAG, although it can still achieve much higher ASR compared to BadChain $(20.8 \%$ ASR on average). A comparison between the results of BALD-word and BALD-RAG and those of BALD-scene shows that learning the mapping between predefined uncommon trigger words and the corresponding malicious behaviors can be relatively easy for LLMs. On the other hand, it is more challenging to learn the backdoor pattern between more abstract scenarios and the target behaviors, especially when the poisoned fine-tuning data is limited. We will further discuss this phenomenon in RQ6. For BALD-RAG, considering the retrieval success rate is essential. On the HighwayEnv dataset, we crafted a specific and rare scenario (a red Mazda CX-5 with hazard lights in front of the ego vehicle) to ensure a high retrieval rate of poisoned knowledge, yielding a $100 \%$ end-to-end attack success rate. In contrast, as an ablation study on the nuScenes dataset, we employed a more general scenario (i.e., a gray trash bin), in the poisoned knowledge. This results in a significantly lower retrieval success rate from the knowledge database. However, the figures marked with an asterisk indicate that whenever the poisoned knowledge is retrieved, it consistently triggers the backdoor attack with an ASR of $100 \%$. The comparison emphasizes the importance of defining specific scenarios for poisoned knowledge.

RQ4: How stealthy are the proposed different attack mechanisms?

Stealthiness of threat models: BALD-word has the strongest threat model assumption, where the attackers need to have access to the internal system and inject the trigger words into the prompt query at runtime. On the other side, BALD-scene and BALD-RAG only require attackers to inject a small portion of the fine-tuning dataset or knowledge database and alter the environments to trigger the backdoor pattern or retrieve the poisoned knowledge, which is more deployable in the real-world.

Stealthiness of triggers: For BALD-scene, it manipulates the LLMs' behavior by constructing abstract scenarios without injecting any semantically inconsistent words, which thus does not lead to any user-detectable anomaly behaviors in benign cases. In contrast, BALD-word and BALD-RAG inject the predefined uncommon word trigger for attacks, which can potentially impair the performance of the benign models and get detected by the end users due to semantic inconsistency. We utilize the ICL-based optimization to improve this, and as shown by the BDR metrics in Table2, our refined word triggers have a very limited impact on the benign model, showing high stealthiness.

Stealthiness of backdoor fine-tuned models: To ensure the stealthiness of backdoor fine-tuned models, the models should still perform well on benign inputs. The Acc in Table 2 shows that the word triggers (in BALD-word and BALD-RAG) merely impact the poisoned models' performance on benign data, where the performance degradation is close to $0 \%$ averaged on three models. The model fine-tuned by BALD-scene can be falsely triggered in benign scenarios occasionally due to the nature of the scenario trigger. Nonetheless, the BALD-scene fine-tuned model still significantly outperforms the original models by $\sim 56.5 \%$ in Highway dataset and $\sim 23.8 \%$ in nuScenes dataset, respectively.

RQ5: What is the minimum ratio of word-based backdoor data required to poison LLMs?

Surprisingly, we find that LLMs are extremely vulnerable to word-based attacks even when the

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-09.jpg?height=328&width=1374&top_left_y=264&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-09.jpg?height=241&width=488&top_left_y=281&top_left_x=382)

(a) BALD-scene

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-09.jpg?height=241&width=390&top_left_y=281&top_left_x=911)

(b) BALD-word

![](https://cdn.mathpix.com/cropped/2024_06_04_778aaa34fff0cb36df00g-09.jpg?height=241&width=396&top_left_y=281&top_left_x=1336)

(c) BALD-RAG

Figure 5: Left: BALD-scene backdoor dataset contrast ratio between target (i.e., positive) scenario and boundary (i.e., negative) scenario. Middle: BALD-word backdoor poison dataset ratio compared to benign dataset. Right: Benign RAG examples to defend against BALD-RAG.

injection ratio is quite low, as shown in Fig. 5(b). The attackers only need to inject $7.5 \%$ poison data to achieve nearly $100 \%$ ASR on both LLaMA2 and GPT-3.5, indicating the sensitivity of LLMs-based decision-making to poisoning fine-tuning datasets. Given the vulnerability and the highly safety-prioritized application, our research calls for urgency in guarding LLM-based decision making. This is even more concerning if we compare to previous backdoor attacks [9] on traditional ML models, where a similar injection ratio is needed to achieve $100 \%$ ASR. All the observations show LLMs, which learned extensive knowledge and common sense from massive world data, do not necessarily improve robustness towards backdoor attacks. Besides, the carefully aligned LLMs 44] can be easily backdoored by using only 4 poisoned out of 50 benign fine-tuning data, which cost only around \$2.00 US dollars according to OpenAI official pricing.

RQ6: How do designs in BALD-scene balance the trade-off between high ASR and low FAR? Our design can effectively inject scenario-based triggers into fine-tuned LLMs to achieve high ASR while maintaining low FAR. We conduct a detailed ablation study using GPT-3.5 as shown in Table 3 As shown, simply injecting predefined scenarios along with backdoor reasoning and decisions can result in high FAR since LLMs are confused about the scenario boundaries. Additionally, naively adding negative samples can lead to low ASR and FAR, which compromises the attack effectiveness. By combining with contrast reasoning, which prompts LLM Rewriter to format positive and negative reasoning with distinct formats ( $\$ 2.3$, our attack can achieve high ASR, indicating effectiveness, and low FAR, indicating stealthiness, at the same time. We further perform an ablation study on the contrast ratio between positive and negative samples in Fig. 5(a) As shown, a high contrast ratio can lead to better ASR, but it also compromises stealthiness.

## RQ7: Can BALD attacks be naively defended in the in-context learning stage?

We further explore whether these fine-tuning stage backdoor attacks for LLM-based decision-making tasks can be naively defended. Since BALD aims at compromising the fine-tuning stage, one natural question is whether enough benign in-context demonstrations can readily defend against these BALD attacks. Fig. 5(c) shows that given only one retrieved demonstration with backdoor trigger words, how many pure benign demonstrations can mitigate its effect for BALD-RAG attacks. As shown, poisoned PaLM2 mostly follows the logic in the benign examples when only two more benign samples are provided. However, we find that BALD-RAG can still attack poisoned GPT-3.5 with almost $100 \%$ ASR even with 10 benign samples, which indicates, in this case, even 10 times of benign examples in ICL cannot mitigate the attack introduced by only $15 \%$ backdoor data during fine-tuning. For BALD-scene, Table 4 demonstrates that the embedded backdoor scenario still achieves relatively high ASR when more benign ICL examples are provided even though previous work [65] show 3 ICL examples is good enough to improve the performance by around $25 \%$. Therefore, naively implementing RAG ICL examples in the inference stage has limited influence on the more strongly embedded scenario triggers within the model weights.

## 4 Conclusion

We propose the BALD framework, the first comprehensive study on backdoor attacks against the finetuning stage of LLM-based decision making systems. BALD includes three novel backdoor attacks that comprehensively target different components within the general LLM-based decision-making system pipeline. Using two dataset types and three representative LLMs, we systematically study the strengths and weaknesses of each type of attack and analyze the key factors to successfully launching each attack. We further discuss the limitations and broader impact in Appendix D. We hope that our efforts can help raise awareness of the fine-tuning stage security of the emerging LLM-based decision-making systems.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.

[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do As I Can and Not As I Say: Grounding Language in Robotic Affordances. In arXiv preprint arXiv:2204.01691, 2022.

[3] Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, and Zhuo Xu. AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. 2024.

[4] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 Technical Report. arXiv preprint arXiv:2305.10403, 2023.

[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2: Vision-LanguageAction Models Transfer Web Knowledge to Robotic Control. arXiv preprint arXiv:2307.15818, 2023 .

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models Are Few-Shot Learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[7] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. NuScenes: A Multimodal Dataset for Autonomous Driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621-11631, 2020.

[8] Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, and Yiming Li. Towards Stealthy Backdoor Attacks Against Speech Recognition via Elements of Sound. arXiv preprint arXiv:2307.08208, 2023.

[9] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. arXiv preprint arXiv:1712.05526, 2017.

[10] Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Juanwu Lu, and Ziran Wang. Large Language Models for Autonomous Driving: Real-World Experiments. arXiv preprint arXiv:2312.09397, 2023.

[11] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as MetaOptimizers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 4005-4019, Toronto, Canada, July 2023. Association for Computational Linguistics.

[12] Vikrant Dewangan, Tushar Choudhary, Shivam Chandhok, Shubham Priyadarshan, Anushka Jain, Arun K Singh, Siddharth Srivastava, Krishna Murthy Jatavallabhula, and K Madhava Krishna. Talk2BEV: Language-Enhanced Bird's-Eye View Maps for Autonomous Driving. arXiv preprint arXiv:2310.02251, 2023.

[13] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An Open Urban Driving Simulator. In Conference on robot learning, pages 1-16. PMLR, 2017.

[14] Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges. IEEE Transactions on Intelligent Transportation Systems, 22(3):1341-1360, 2020.

[15] Daniel J Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L SangiovanniVincentelli, and Sanjit A Seshia. Scenic: A Language for Scenario Specification and Scene Generation. In Proceedings of the 40th ACM SIGPLAN conference on programming language design and implementation, pages 63-78, 2019.

[16] Daocheng Fu, Wenjie Lei, Licheng Wen, Pinlong Cai, Song Mao, Min Dou, Botian Shi, and Yu Qiao. LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving. arXiv preprint arXiv:2402.01246, 2024.

[17] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive Like a Human: Rethinking Autonomous Driving With Large Language Models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 910-919, 2024.

[18] Caelan Reed Garrett, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. PDDLStream: Integrating Symbolic Planners and Blackbox Samplers via Optimistic Adaptive Planning. 2020.

[19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision Meets Robotics: The Kitti Dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013.

[20] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Mądry, Bo Li, and Tom Goldstein. Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):1563-1580, 2022.

[21] Junfeng Guo, Ang Li, Lixu Wang, and Cong Liu. Policycleanse: Backdoor detection and mitigation for competitive reinforcement learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4699-4708, 2023.

[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685, 2021.

[23] Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. Composite Backdoor Attacks Against Large Language Models. arXiv preprint arXiv:2310.07676, 2023.

[24] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3D Value Maps for Robotic Manipulation With Language Models. arXiv preprint arXiv:2307.05973, 2023.

[25] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner Monologue: Embodied Reasoning Through Planning With Language Models. In arXiv preprint arXiv:2207.05608, 2022.

[26] Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns Via Spotlights of Attention. In International Conference on Machine Learning, pages 9639-9659. PMLR, 2022.

[27] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in Natural Language Generation. ACM Computing Surveys, 55(12):1-38, 2023.

[28] Yuqian Jiang, Shiqi Zhang, Piyush Khandelwal, and Peter Stone. Task Planning in Robotics: an Empirical Comparison of PDDL-based and ASP-based Systems. 2019.

[29] Ruochen Jiao, Xiangguo Liu, Takami Sato, Qi Alfred Chen, and Qi Zhu. Semi-Supervised Semantics-Guided Adversarial Training for Robust Trajectory Prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8207-8217, 2023.

[30] Ruochen Jiao, Xiangguo Liu, Bowen Zheng, Dave Liang, and Qi Zhu. TAE: A SemiSupervised Controllable Behavior-Aware Trajectory Generator and Predictor. arXiv preprint arXiv:2203.01261, 2022.

[31] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, and Jiangtao Gong. Surrealdriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts Based on Large Language Model. arXiv preprint arXiv:2309.13193, 2023.

[32] Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, and Michael Gienger. Copal: Corrective Planning of Robot Actions With Large Language Models. arXiv preprint arXiv:2310.07263, 2023.

[33] Edouard Leurent. An Environment for Autonomous Driving Decision-Making. https:// github.com/eleurent/highway-env, 2018.

[34] Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, and Marco Pavone. Driving Everywhere With Large Language Model Policy Adaptation. arXiv preprint arXiv:2402.05932, 2024.

[35] Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, and Jialiang Lu. Hidden Backdoors in Human-Centric Language Models. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, pages 3123-3140, 2021.

[36] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language Model Programs for Embodied Control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493-9500. IEEE, 2023.

[37] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning Attack on Neural Networks. In 25th Annual Network And Distributed System Security Symposium (NDSS 2018). Internet Soc, 2018.

[38] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal Language Model for Driving. arXiv preprint arXiv:2312.00438, 2023.

[39] Yunze Man, Liang-Yan Gui, and Yu-Xiong Wang. BEV-Guided Multi-Modality Fusion for Driving Perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21960-21969, 2023.

[40] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. A Language Agent for Autonomous Driving. arXiv preprint arXiv:2311.10813, 2023.

[41] Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, and Shiqing Ma. NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $15551-15565,2023$.

[42] David J Miller, Zhen Xiang, and George Kesidis. Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses Against Attacks. Proceedings of the IEEE, 108(3):402-433, 2020.

[43] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion Forecasting Via Simple \& Efficient Attention Networks. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2980-2987. IEEE, 2023.

[44] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training Language Models to Follow Instructions with Human Feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[45] Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun. Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. arXiv preprint arXiv:2110.07139, 2021.

[46] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! arXiv preprint arXiv:2310.03693, 2023.

[47] Nils Reimers and Iryna Gurevych. Sentence-Bert: Sentence Embeddings Using Siamese Bert-Networks. arXiv preprint arXiv:1908.10084, 2019.

[48] Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. WikiChat: Stopping The Hallucination of Large Language Model Chatbots By Few-Shot Grounding On Wikipedia. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2387-2413, 2023.

[49] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, and Mingyu Ding. Languagempc: Large Language Models as Decision Makers for Autonomous Driving. arXiv preprint arXiv:2310.03026, 2023.

[50] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu, and Hongsheng Li. LMDrive: Closed-Loop End-to-End Driving With Large Language Models. arXiv preprint arXiv:2312.07488, 2023.

[51] SP Sharan, Francesco Pittaluga, Manmohan Chandraker, et al. LLM-Assist: Enhancing ClosedLoop Planning With Language-Based Reasoning. arXiv preprint arXiv:2401.00125, 2023.

[52] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor Pre-Trained Models Can Transfer to All. arXiv preprint arXiv:2111.00197, 2021.

[53] Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. BadGPT: Exploring Security Vulnerabilities of ChatGPT Via Backdoor Attacks to InstructGPT. arXiv preprint arXiv:2304.12298, 2023.

[54] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving With Graph Visual Question Answering. arXiv preprint arXiv:2312.14150, 2023.

[55] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating Situated Robot Task Plans Using Large Language Models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530, 2023.

[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288, 2023.

[57] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288, 2023.

[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. Advances in neural information processing systems, 30, 2017.

[59] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application. IEEE Access, 11:95060-95078, 2023.

[60] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning Language Models During Instruction Tuning. In International Conference on Machine Learning, pages 35413-35425. PMLR, 2023.

[61] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. Adversarial Demonstration Attacks on Large Language Models. arXiv preprint arXiv:2305.14950, 2023.

[62] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, et al. DriveMLM: Aligning Multi-Modal Large Language Models With Behavioral Planning States for Autonomous Driving. arXiv preprint arXiv:2312.09245, 2023.

[63] Yixuan Wang, Ruochen Jiao, Chengtian Lang, Sinong Simon Zhan, Chao Huang, Zhaoran Wang, Zhuoran Yang, and Qi Zhu. Empowering Autonomous Driving With Large Language Models: A Safety Perspective. arXiv preprint arXiv:2312.00812, 2023.

[64] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail? Advances in Neural Information Processing Systems, 36, 2024.

[65] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. Dilu: A Knowledge-Driven Approach to Autonomous Driving With Large Language Models. arXiv preprint arXiv:2309.16292, 2023.

[66] Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen. Language Prompt for Autonomous Driving. arXiv preprint arXiv:2309.04379, 2023.

[67] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. In The Twelfth International Conference on Learning Representations, 2024.

[68] Xuesu Xiao, Bo Liu, Garrett Warnell, and Peter Stone. Motion Planning and Control for Mobile Robot Navigation Using Machine Learning: A Survey. Autonomous Robots, 46(5):569-597, 2022.

[69] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao. DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model. arXiv preprint arXiv:2310.01412, 2023.

[70] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations, 2023.

[71] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. A Survey of Large Language Models for Autonomous Driving. arXiv preprint arXiv:2311.01043, 2023.

[72] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. RAG-Driver: Generalisable Driving Explanations With Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model. arXiv preprint arXiv:2402.10828, 2024.

[73] Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. Trojaning Language Models for Fun and Profit. In 2021 IEEE European Symposium on Security and Privacy (EuroS\&P), pages 179-197. IEEE, 2021.

[74] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label Backdoor Attacks on Video Recognition Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14443-14452, 2020.

[75] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models. arXiv preprint arXiv:2402.07867, 2024.
