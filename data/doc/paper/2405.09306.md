# Words Blending Boxes. Obfuscating Queries in Information Retrieval using Differential Privacy. 

Francesco Luigi De Faveri ${ }^{\mathrm{a}, *}$, Guglielmo Faggioli ${ }^{\mathrm{a}, *}$, Nicola Ferro ${ }^{\mathrm{a}, *}$<br>${ }^{a}$ Department of Information Engineering, University of Padua, Padua, 35131, Italy


#### Abstract

Ensuring the effectiveness of search queries while protecting user privacy remains an open issue. When an Information Retrieval System (IRS) does not protect the privacy of its users, sensitive information may be disclosed through the queries sent to the system. Recent improvements, especially in Natural Language Processing (NLP), have shown the potential of using Differential Privacy (DP) to obfuscate texts while maintaining satisfactory effectiveness. By perturbing the non-contextualized vector representations of the terms within the query, terms are replaced with the one closest to the perturbed version. However, such approaches may protect the user's privacy only from a theoretical perspective while, in practice, the real user's information need can still be inferred if perturbed terms are too semantically similar to the original ones. Furthermore, a poor tuning of the DP privacy budget $\varepsilon$, i.e., choosing a too high value, is bound to cause catastrophic data release since, in such case, the user's query will be released as if no privacy protection was adopted at all. We overcome such limitations by proposing Words Blending Boxes (WBB), a novel DP mechanism for query obfuscation, which protects the words in the user queries by employing safe boxes. On the one side, these boxes avoid too semantically similar words to be sampled; on the other side, they rely on a consolidated DP mechanism for sampling from a set of candidate words to mask the original query. To measure the overall


[^0]effectiveness of the proposed WBB mechanism, we measure the privacy obtained by the obfuscation process, i.e., the lexical and semantic similarity between original and obfuscated queries. Moreover, we assess the effectiveness of the privatized queries in retrieving relevant documents from the IRS. Our findings indicate that WBB can be integrated effectively into existing IRS, offering a key to the challenge of protecting user privacy from both a theoretical and a practical point of view.

Keywords: Differential Privacy, Information Hiding, Information Retrieval, Information Security

## 1. Introduction

Information Retrieval Systems (IRSs) are a daily commodity used to satisfy the most heterogeneous user information needs. One of the most wellknown examples of IRS are Search Engines on which the user issues a natural language query and the system retrieves a set of web pages that should contain the answer to the user query. One of the risks linked to IRS is that, when searching, the queries issued by the user could threaten their privacy. An adversarial, including the IRS itself, can leverage the users' queries to infer what they are searching for, gaining access to sensitive information about them. Imagine a user conducting multiple searches for documents related to a specific medical condition (Kumok et al., 2020, Zimmerman et al., 2020). In this case, a malicious employee of the IRS accessing the query logs could infer sensitive information regarding the user's health. Another example is ego surfing, a common practice where users search for their name, social security number, or social profile, potentially allowing the IRS to link the request with a real person. Recent studies have shown how privacy violations in analyzing search histories can leak the political views (Le et al., 2019; Mustafaraj et al., 2020) or the sexual orientation (Malandrino et al., 2013; Shekhawat et al. 2019) of a user. Such knowledge can lead to unfair treatment and risks in illiberal countries.

To address this, we would like to allow the user to send the IRS queries that are not sensitive, but still allow the user to retrieve relevant documents as if the original sensitive query was sent. This would prevent the adversarial (i.e., the IRS) from profiling the user. This threat model requires us to operate under the non-collaborative IRS assumption: we consider the IRS as not interested in protecting the user's privacy, but rather as a potential threat.

For obvious reasons, encryption, in this case, is not a suitable protection mechanism. The IRS is supposed to answer the user's query thus being it is its intended receiver. While encryption can be effectively adopted to protect the query from external eavesdroppers - assuming the IRS is willing to agree on an encryption schema -, it does not provide any guarantee concerning the information leakage that might occur directly on the IRS side. Secondly, under the hypothesis that the IRS is non-cooperative, we might not be able to agree upon an encryption protocol for communicating with the IRS.

One possibility to mitigate such a threat involves applying text obfuscation mechanisms. Broadly speaking, an obfuscation mechanism takes in input a sensitive piece of text $t$ and outputs a new piece of obfuscated text $t^{\prime}$, that is semantically related to $t$, but not sensitive.

The text obfuscation task has been extensively investigated in the Natural Language Processing (NLP) domain. The most common scenarios in which text obfuscation is used in NLP involve tasks such as sentiment analysis, spam detection, and text classification Zhao and Chen, 2022, Feyisetan et al., 2019, 2020; Feyisetan and Kasiviswanathan, 2021; Xu et al., 2020, 2021). In NLP, the most common framework to obfuscate and release text is Differential Privacy (DP) (Dwork et al., 2006). State-of-the-art solutions based on DP for text obfuscation involve perturbing each word of a piece of text (Zhao and Chen, 2022; Feyisetan et al., 2019, 2020; Feyisetan and Kasiviswanathan, 2021; Xu et al., 2020, 2021). More in detail, given a noncontextualized embedding of each word in the text, such embedding is perturbed by adding appropriately sampled noise. Then, a new obfuscated text is constructed by taking, for each perturbed word embedding, the closest word to it. In this way, the original content of the text is safeguarded.

Nevertheless, when it comes to IRS, obfuscation approaches designed for NLP do not necessarily work smoothly, for two main reasons. First, when it comes to NLP, the obfuscated text can be used both at training and inference time: this allows a machine learning mechanism (e.g., a classifier) to account for the noise introduced in the text. The same cannot occur in our scenario. Indeed, we assume the IRS to be non-cooperative: it would be impossible to operate on its training to account for the obfuscated queries. Secondly, one of the challenges linked to DP is that too-small noise can lead to catastrophic data releases in which the text is not obfuscated. In other terms, tuning the privacy guarantees (i.e., modifying the privacy budget $\varepsilon$ ) of a DP mechanism provides privacy formally but, in practice, it does the obfuscated text. This is a well-known and severe problem of DP (Domingo-Ferrer et al., 2021).

Indeed, none of state-of-the-art solutions proposed for text obfuscation in NLP (Zhao and Chen, 2022; Feyisetan et al., 2019, 2020, Feyisetan and Kasiviswanathan, 2021; Xu et al., 2020, 2021), fully overcomes the problem, as they still allow the original word to be picked as the obfuscation term and highly semantically similar terms.

Besides generic approaches to deal with NLP at large, some efforts have been specifically targeted to the query obfuscation task, in which the text to be obfuscated is a query that needs to be sent to a IRS. The first of such approaches, proposed by Arampatzis et al. (2013b), replaces words in queries with randomly sampled hypernyms of them, under the assumption that more general words are less sensitive. On the contrary, the approach proposed by Fröbe et al. (2022) employs a local corpus to determine which terms co-occur with the query terms in the documents the most frequently. Using this information, they construct obfuscation queries that only contain terms that often co-occur with query terms, so that both the original and obfuscated queries will retrieve similar documents. Nevertheless, by relying on strictly semantic relations between words and terms co-occurrences, current approaches might release terms that include words highly semantically related to or synonyms of those originally contained in the queries. Furthermore, no approach to query obfuscation for IRS is based on DP, thus making it impossible to grant demonstrable privacy to the obfuscation queries produced. More recently, Faggioli and Ferro (2024) demonstrated the effectiveness of NLP-inspired DP techniques in the Information Retrieval (IR) domain. Nevertheless, they do not propose a new approach that would be more effective in the IR domain.

To overcome all the aforementioned limitations, we propose Words Blending Boxes (WBB), a novel approach to safeguard user privacy by obfuscating the words in the original query, creating a safe box around the query terms in the non-contextual embedding space, which ensures that the query words and terms too similar to the original ones will not be present in the final obfuscated output. Furthermore, we ensure that the outcome of our approach is private by using DP to select which terms to use for the obfuscated query among those outside the safe box. This guarantees that our proposed solution not only satisfies the privacy requirements from a theoretical point of view but, thanks to the safe box, also provides practical privacy for the user. To the best of our knowledge, WBB represents the first DP approach explicitly designed for the query obfuscation task in the IR context.

By conducting extensive experimentation on both classical (Robust '04)
and neural-oriented (Deep Learning '19) TREC collections, the WBB method is evaluated and compared with current State-of-the-Art mechanisms in three aspects: privacy, recall, and utility.

- Firstly, we determine if our protection mechanism can provide sufficient privacy guarantees, beyond the formal privacy granted by using DP. To do so, we compute the lexical and semantic similarity between the original and obfuscated queries, showing that our mechanism induces sufficiently obfuscated queries.
- Secondly, one of the risks linked to obfuscation mechanisms, especially when high privacy is enforced, is that obfuscated queries might not be able to retrieve relevant documents. Therefore, we are interested in determining if the obfuscation queries can retrieve relevant content. To do so, we measure the recall observed when the documents retrieved by different obfuscation queries are combined, that is what the user would do upon receiving the IRS answers to the obfuscation queries.
- Finally, we are interested in determining the utility preserved by our obfuscation mechanism. To do so, we assume the user reranks the documents received by the IRS using the original query. This is a safe operation as it occurs on the user side. As a proxy of the experienced utility, we compute the nDCG@10 for the reranked list of documents.

The paper is organized as follows: Section 2 introduces the preliminaries on DP, along with important DP mechanisms that the WBB uses. Moreover, Section 3 describes the NLP method used for text obfuscation, while Section 4 explains our methodology; Section 5 reports the results of our experiments; finally, Section 6 presents related works, and Section 7 draws some conclusions and outlooks for future work.

## 2. Background on Differential Privacy

This section reports the theoretical foundations of the DP framework, underlying the mechanism designed in this work.

### 2.1. Privacy Definitions

Differential Privacy (DP), introduced by Dwork et al. (2006), is, de facto, the gold-standard formal definition used to determine if an algorithm protects
privacy. Intuitively, a DP mechanism adds, during the computation, an appositely crafted amount of noise that depends on the privacy budget $\varepsilon$, which sets the trade-off between data privacy and utility.

The definition of $\varepsilon$-DP (Dwork et al., 2006) states that a randomized mechanism $\mathcal{M}$ (i.e., an algorithm that takes a certain input and produces a noisy output) is $\varepsilon$-DP if, for any pair of neighbouring datasets $D$ and $D^{\prime}$, i.e., two datasets differing of a single record, and a privacy budget $\varepsilon \in \mathbb{R}^{+}$, it holds:

$$
\begin{equation*}
\operatorname{Pr}\{\mathcal{M}(D) \in \mathcal{S}\} \leq e^{\varepsilon} \operatorname{Pr}\left\{\mathcal{M}\left(D^{\prime}\right) \in \mathcal{S}\right\}, \forall \mathcal{S} \subseteq \operatorname{Image}(\mathcal{M}) \tag{1}
\end{equation*}
$$

If the mechanism is $\varepsilon$-DP, the definition grants that for every run of the randomized mechanism $\mathcal{M}$, the output (i.e., a value from $\mathcal{S}$ ) is almost equally likely to be observed on every neighbouring dataset simultaneously. By definition, lower values of $\varepsilon$ guarantee higher levels of privacy: if $\varepsilon=0$, then $\operatorname{Pr}\{\mathcal{M}(D) \in \mathcal{S}\}=\operatorname{Pr}\left\{\mathcal{M}\left(D^{\prime}\right) \in \mathcal{S}\right\} \forall \mathcal{S} \subseteq \operatorname{Image}(\mathcal{M})$, i.e., the output does not depend on the input. Intuitively, given two similar yet different inputs, we expect the output to be the same with a certain probability regulated by $\varepsilon$. This grants "Plausible deniability": the adversarial cannot deem with absolute certainty which input (i.e., user's data) corresponds to a given output.

### 2.2. The Exponential Mechanism

The mechanism used to sample words underlying WBB builds upon the $\varepsilon$-DP exponential mechanism (McSherry and Talwar, 2007; Dwork and Roth, 2014). The general idea underneath the exponential mechanism is that given a certain input $x \in \mathcal{I}$, a range of interest $\mathcal{R}$, and a utility function $u$ such that $u: \mathcal{I} \times \mathcal{R} \rightarrow \mathbb{R}$, we are interested in outputting a value $r \in \mathcal{R}$ that, given the input $x$, maximizes the utility while remaining $\mathrm{DP}$.

More in detail, to implement the exponential mechanism, it is first necessary to compute its sensitiveness, which corresponds to the maximum difference in utility that can be observed for any possible value of the range $\mathcal{R}$ and any possible pair of neighbouring inputs (i.e., highly similar input data). The sensitivity of a given utility function $u$ is computed as:

$$
\begin{equation*}
\Delta u=\max _{r \in \mathcal{R}} \max _{x, y:\|x-y\|_{1} \leq 1}|u(x, r)-u(y, r)| \tag{2}
\end{equation*}
$$

The exponential mechanism outputs a value $r \in \mathcal{R}$ with probability proportional to $\exp \left(\frac{\varepsilon u(x, r)}{\Delta u}\right)$. This ensures that the output depends on the utility
of $r$ given $x$ and the $\varepsilon$ used: a small $\varepsilon$ ensures that the utility plays a less and less prominent role in the sampling. In practical terms, when instantiated in our scenario, given an input word, we sample another word based on some utility measure (e.g., the similarity with the original term). We are not guaranteed the most useful word is sampled, i.e., the most similar, as this would expose the input, but we are likely to sample a high utility term.

## 3. Background on Text Obfuscation

In this section, we describe existing background and approaches to text obfuscation. We start from approaches designed for NLP tasks, where it is the state-of-the-art to employ DP. Then, we move to the specific IR problem, where we observe that the approaches to obfuscate the queries are based on statistics of their terms. Finally, we detail some approaches to measure the privacy provided by a given obfuscation approach.

### 3.1. Text Obfuscation in NLP

The general task of a text obfuscation mechanism consists of taking in input sensitive piece of text $t$ and outputting a new piece of obfuscated text $t^{\prime}$, that is semantically related to $t$, but not sensitive. At this point, being $t^{\prime}$ safe from the privacy perspective, it can be released without risking the user's privacy. More in detail, such a piece of text can be used for example to train a machine learning model in some downstream task. For example, $t$ might be a post written by a user on a social network. Assume now the objective is to train a sentiment classifier. To avoid risking the users' privacy, the text of the post can be first obfuscated and then passed as a training example to the classifier. For most tasks, a single obfuscated text might not be sufficiently rich to convey the complexity of $t$, thus it is common to generate several pieces of non-sensitive text $t_{1}^{\prime}, t_{2}^{\prime}, \ldots, t_{n}^{\prime}$.

Most recent state-of-the-art approaches for text obfuscation in NLP at large employ DP (Feyisetan et al., 2020; Chen et al., 2023; Yue et al., 2021; Xu et al., 2020; Carvalho et al., 2023; Xu et al., 2021). Most of these mechanisms adopt a similar strategy to obfuscate the text. Assume a piece of text $\mathcal{W}^{l}$ of length $l$ needs to be obfuscated. Let $\mathcal{V}$ be the vocabulary and $\phi$ be a non-contextualized word encoder such as Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al. 2014 ). This word encoder takes in input a word $w$ and generates a non-contextual embedding representation $\phi(w) \in \mathbb{R}^{d}$, where $d$ is the dimension of the word embedding. Let us call $\nu(\varepsilon)$ the noise
appositely crafted accordingly to the noise distribution function $\nu$ to satisfy the DP constraint.

Then, given a word $w$, it is obfuscated with a word $w^{\prime}$ as follows:

$$
\begin{equation*}
\underset{w^{\prime} \in \mathcal{V}}{\operatorname{argmax}}\left\|\phi\left(w^{\prime}\right)-(\phi(w)+\nu(\varepsilon))\right\| \tag{3}
\end{equation*}
$$

Where $\|\cdot\|$ represents any norm (e.g., the Euclidean norm). In other terms, the word $w^{\prime}$ used to obfuscate $w$ is the one that has the representation closest to the non-contextual representation of $w$ perturbated with noise $\nu(\varepsilon)$. These mechanisms were originally designed to operate in an NLP context and explicitly for tasks such as sentiment analysis and spam detection. Notice that, a major difference between NLP and IRS, is that in the former case, it is possible to fine-tune the model on some obfuscated text. This ensures that the model latently learns how to account for the shift in the language model operated by the obfuscation mechanism. In the case of IRS, we cannot apply the same reasoning: under the hypothesis of a non-cooperative IRS, we have no access to the model it relies upon, and we cannot retrain it externally.

We now provide examples of works that explicitly employ DP in the NLP domain. Zhao and Chen (2022) provided a comprehensive overview of DP strategies literature to protect sensitive information in unstructured data. Zhao and Chen highlighted the prominence of the trade-off between privacy and utility in privatizing sensitive textual information. Such privacy versus utility trade-off originates because enhancing privacy involves altering the original data, which reduces the specificity and accuracy of the data. Therefore, the challenge is finding an optimal balance where privacy is sufficiently guaranteed without significantly compromising the privatized data utility. Hence, the authors conclude that DP has a significant, unexplored potential to improve data utility while guaranteeing high privacy protection levels for unstructured data such as texts.

Feyisetan et al. (2019, 2020); Feyisetan and Kasiviswanathan (2021) proposed DP methods based on the noise addition to word embeddings. Such methods depend on defining metric DP to perturb vectors. Metric-DP is a relaxation of the original definition introduced by Chatzikokolakis et al. (2013), which considers the metric function introduced in the vector space used for the word embeddings. The general idea is to take the non-contextualized embedding of the terms in the text, inject some noise, and consider the word whose embedding is the closest to it. Feyisetan et al. (2020) explored how to balance the trade-off between privacy and utility using calibrated noise
and metric DP by leveraging geometric properties of word embeddings and practically evaluating the utility of the resulting scrambled texts. The study explores the impact that calibrated noise can have on helping users preserve their privacy in text obfuscation in NLP tasks. Yet, in such studies, privacy is measured without considering concretely how the mechanism carries on the obfuscation and how the obfuscated word is related to the original one.

$\mathrm{Xu}$ et al. (2020) started from the observation that the mechanism proposed by Feyisetan et al. (2020) is likely to obfuscate a word with itself. To account for this, propose a mechanism for DP text perturbation that calibrates the noise injection by incorporating the regularized Mahalanobis metric. On the same line, Xu et al. (2021) further extended Feyisetan et al. (2020) by proposing a mechanism based on the Vikrey mechanism, where a word can be obfuscated with either the most similar or the second most similar approaches. In addition, other methods proposed for achieving good levels of text obfuscation using DP have been studied by Yue et al. (2021); Chen et al. (2023). On the one hand, Yue et al. (2021) implemented a DP mechanism, SaNTEXT, to sanitize texts to achieve good levels of utility while still protecting the defined sensitive words in the text. On the other hand, Chen et al. (2023) presented CusText, a privatization mechanism able to adapt with any similarity measure to balance the privacy-utility trade-off. While going in the direction of overcoming the limitation linked to a word being obfuscated by itself, this possibility remains present in both mechanisms, thus keeping the user exposed to the risk of catastrophic data release.

On a different line, Carvalho et al. (2023) suggested the Truncated Exponential Mechanism to ensure user privacy in NLP model training. The algorithm utilizes the exponential mechanism McSherry and Talwar (2007) to change the privatization procedure into a selection problem, enabling noise calibration based on embedding space density around a given input. Nevertheless, the limitation of the mechanism originates from its inability to include the privatization of word context vectors, consequently constraining the guaranteed privacy.

### 3.2. Query Obfuscation in $I R$

We introduce in this section the generic query obfuscation pipeline and describe how it has been instantiated in practice.

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-10.jpg?height=632&width=1374&top_left_y=424&top_left_x=365)

Figure 1: General overview of the query obfuscation pipeline in IR. The diagram illustrates the pipeline of the retrieval, showing the steps on the User, safe, and IRS, unsafe, side.

### 3.2.1. The query obfuscation pipeline

Figure 1illustrates the general query obfuscation pipeline analyzed in this work and commonly studied in IR scenarios implementing query obfuscation protocols. The process occurs at two distinct places: on the user side (safe), where the user writes their private query and obfuscates it, and on the IRS side (unsafe), where the IRS retrieves the documents given the obfuscated queries received. The system is oblivious to the real information need.

To define a query obfuscation mechanism, we take inspiration from the NLP case. Therefore, in the IRS scenario, given a sensitive query $q$ ("Private Query in Figure 1), we apply an obfuscation mechanism ("Obfuscation mechanism" in Figure 1) to generate several non-sensitive queries $q_{1}^{\prime}, q_{2}^{\prime}, \ldots$, $q_{n}^{\prime}$ ("Obfuscated queries" in Figure 11). This occurs on the user side therefore it is safe from the privacy perspective. Once the obfuscated queries have been generated, they can be safely transmitted to the IRS. These queries, are only semantically related to $q$ but focus on different topics. Therefore, it is likely that they will not retrieve all and every document relevant to $q$, and most likely not on the top positions. Therefore, the user transmits the $n$ obfuscation queries $q_{. .}^{\prime}$ to the IRS. Upon receiving these queries, the IRS computes a ranked list of documents for each of them (i.e., "Retrieved docs" in Figure 11. Notice that this does not require the IRS to be aware of the obfuscation mechanism being in place: the IRS behaves as a black box that
takes in input a query and outputs a list of documents. The user then receives a set of lists of documents. As it is likely that relevant documents are not in the first positions, post-filtering and re-ranking are performed using the original user's query. Finally, the user obtains a new ranked list with the likely relevant documents placed in the first positions.

We want to stress that, in this case, cryptographic protocols do not provide privacy for the user. The non-cooperative IRS is the intended receiver of the query; hence it should be the only one supposed to decipher the cryptographic message. However, in the scenario proposed, the IRS is interested in profiling the user and thus, cryptography is not enough to achieve privacy. To protect the query from third-party eavesdroppers, safe communication protocols based on private or public key encryption, e.g., AES (Daemen and Rijmen, 2002) or RSA (Rivest et al., 1978), can be employed. Nevertheless, these methods do not ensure privacy but only the confidentiality of communication between the user and the IRS.

### 3.2.2. State-of-the-art Approches

Privacy via query obfuscation in IR without considering the definition of DP has been studied in prior works. Arampatzis et al. (2013b) proposed an obfuscation mechanism based on Word-Net (Miller, 1995) in which each term in the query is generalized, implementing a hierarchical definition using the extracted synonyms, hypernyms, and holonyms of the word. The candidate obfuscation queries are then computed considering the Cartesian product of the terms sets and filtered out considering the similarity between the original and obfuscated queries to avoid exposing queries. An extension of the studies of Arampatzis et al. (2013a, 2015) have been introduced by Fröbe et al. (2022) that developed a statistical query obfuscation method using the user's query on the local corpus.

Faggioli and Ferro (2024) bridged the gap between the two aforementioned research areas by showing how DP mechanisms borrowed from NLP may perform for query obfuscation in IR. Nevertheless, Faggioli and Ferro directly applied such state-of-the-art DP mechanisms devised for NLP (Xu et al., 2021, 2020; Feyisetan et al. 2020), falling short in devising a DP approach that does not risk obfuscating a query with itself.

Therefore, to the best of our knowledge, this is the first effort to design and formally prove an $\varepsilon$-DP method directly for the query obfuscation task in IR.

### 3.3. Privacy Measures

One of the major challenges when it comes to dealing with privacy and text concerns the evaluation of how much information is leaked by the obfuscated text compared to the original one. Measuring if two pieces of text convey the same meaning is a very challenging task: two sentences might be syntactically different but semantically equivalent. Vice versa, small changes, even of a few characters, might completely change the meaning of a piece of text. Additionally, it is hard to establish if something is sensitive and leaks private information. Often, the degree of sensitivity depends on the context: we might not feel comfortable sharing the very same information with a stranger as we would tell our doctor.

Therefore, frequently employed metrics in the fields of both NLP and information security to assess the level of privacy are typically categorized based on the specific aspect of privacy they are measuring (Wagner and Eckhoff, 2018): Uncertainty Statistics, Lexical Similarity, Semantic Similarity.

Uncertainty Statistics measure the probability of an obfuscation approach failing at the level of the single term. The two most adopted measures are $N_{w}$ and $S_{w}$. More in detail:

- $N_{w}=\operatorname{Pr}[\mathcal{M}(w)=w]$. It represents the probability that the mechanism obfuscates the word with itself (i.e., it fails the obfuscation).
- $S_{w}=\min |\{\mathcal{S} \subseteq \operatorname{Image}(\mathcal{M}): \operatorname{Pr}[\mathcal{M}(w) \notin \mathcal{S}] \leq \eta\}|$. It corresponds to the minimum size of the set that contains the words used to obfuscate the same input word.

While these measures are simple to compute, they are too fine-grained to provide real insight into whether a piece of text has been correctly obfuscated: not all words are equally sensitive.

Lexical Similarity measures try to overcome the limitations of uncertainty statistics, by measuring the quality of a piece of obfuscated text at the global level. Examples of such measures include approaches drawn from the automatic translation (with the caveat that a high lexical similarity indicates low obfuscation and - likely - low privacy). Examples of such measures include the Jaccard similarity, BLEU Papineni et al. (2002), or METEOR Lavie and Agarwal (2007). In most cases, these measures involve computing the overlapping between the original and obfuscated text - or their n-grams. To
provide an example, the Jaccard similarity is defined as follows:

$$
\operatorname{Jaccard} \operatorname{Similarity}(A, B)=\frac{|A \cap B|}{|A \cup B|}
$$

this measure is computed over the set of words in the original query $A$ and the obfuscated ones $B$, measuring the proportion of overlapping terms. Even though they represent an improvement over uncertainty statistics with a global view of the text, lexical similarity approaches still lack sufficient semantic understanding to fully grasp the complexity underlying privacyrelated tasks.

Semantic Similarity measures employ a notion of semantic similarity to determine if a piece of text is an effective obfuscation of another one: too similar pieces of text do not guarantee privacy and to different texts do not provide utility. Automatically embedding the semantics of a text is a complex task, typically addressed by encoding the text in a latent space. The approach current state-of-the-art solutions for this task, such as BERT Devlin et al. (2019), involve the usage of the transformers architecture Vaswani et al. (2017). In this case, the simplest strategy to compute the similarity between two texts involves computing their embedding $t$ and $t^{\prime}$ in a latent space employing a suitable encoder (e.g., BERT). Then, the semantic similarity is approximated by the cosine similarity between the two vectors:

$$
\text { Semantic Similarity }\left(t, t^{\prime}\right)=\frac{t \cdot t^{\prime}}{\|t\|\left\|t^{\prime}\right\|}
$$

While this approach can grasp the semantic similarity between two sentences, it is tightly linked to the encoder used to compute the embeddings. This means that i) different results might be observed depending on the encoder at hand; ii) if the obfuscation model relies on the same encoder used for the evaluation, there might be some leakage.

In this paper, we measure the similarity between the original and obfuscated queries in terms of Jaccard Similarity and Semantic similarity, to provide both lexical and semantic points of view. As mentioned before, a high similarity is an indication of low obfuscation (i.e., the original and obfuscated queries are too similar), thus weak privacy guarantees.

## 4. Methodology

In this section, we describe the motivations for this study and the methodology underlying WBB to bring concrete privacy for the user's queries, also
providing a formal proof of how such a mechanism achieves $\varepsilon$-DP.

### 4.1. Motivations of the study

Most of the current approaches designed to obfuscate texts $\mathrm{Xu}$ et al., 2020; Feyisetan and Kasiviswanathan, 2021), including queries in the IRS scenario (Faggioli and Ferro, 2024), as well as approaches to author attribution masking (Weggenmann and Kerschbaum, 2018), do consider that, even though the query is obfuscated, it might still reveal too much information by releasing synonyms of the original terms. In previous work, it has been assumed that using synonyms or hyponyms does not significantly impact safeguarding text privacy (Xu et al., 2020; Feyisetan and Kasiviswanathan, 2021; Weggenmann and Kerschbaum, 2018; Arampatzis et al., 2013b). However, these words often have a similar meaning to the original terms and they may not offer adequate protection. Suppose a user is issuing the query "treatment for skin cancer". Consider now an obfuscation mechanism, as those described in Subsection 3.1, that adds a certain noise to the embedding of the term "cancer", resulting in its mapping to a new obfuscated term, namely "melanoma". According to the mechanism's perspective, the obfuscation successfully substituted the word "cancer". Nevertheless, from a human perspective, the final obfuscated query will be the following: "treatment for skin melanoma". The adopted approach might be considered as a satisfactory obfuscation, but, in reality, any human would easily understand the actual user's information need, causing severe information leakage. We argue that it is not sufficient to add controlled statistical noise to word embeddings to ensure the privacy of the queries.

Another motivation for a concrete privacy-preserving mechanism is related to cryptography used to protect privacy: cryptographic protocols may not be the appropriate solution for safeguarding privacy in this scenario, as the ciphertext is not directly employed by the IRS for document selection and retrieval. Therefore, while cryptographic solutions may protect confidentiality from external eavesdroppers in an effective manner, they do not represent the appropriate solution for privacy in such non-cooperative systems.

Semantically related words in embedding spaces. Following previous literature (Chen et al., 2013; Ono et al., 2015), we analyzed how embeddings of synonyms and hyponyms spread in the vector space and how the embeddings of words belonging to such a vector space of dimension $d$, i.e., the Euclidean
space $\mathbb{R}^{d}$, distributes reciprocally. For such a preliminary analysis, we studied a geometric model in $\mathbb{R}^{3}$ of the relative positioning of two generic vectors: to assess the degree of semantic similarity of two arbitrary words $v$ and $w_{1}$, where $v$ acts as query term and $w_{1}$ as obfuscation term, we project them on the non-contextual embedding space $\phi(v)=V$ and $\phi\left(w_{1}\right)=W_{1}$. We consider two aspects: the Euclidean distance and the angle between the vectors $V$ and $W_{1}$. To measure these values, we first compute the plane $\pi$ generated by a pair of linearly independent vectors $\left(e_{1}, e_{2}\right)$, i.e., $\pi=\left\langle e_{1}, e_{2}\right\rangle$ containing both the embedding vectors of the words. Figure 2 shows a geometric representation of the situation.

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-15.jpg?height=385&width=631&top_left_y=976&top_left_x=411)

(a) Model 3D - Vector positioning of $V$ and $W_{1}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-15.jpg?height=385&width=632&top_left_y=978&top_left_x=1061)

(b) Model 2D - Relative positioning of $V$ and $W_{1}$ in the plane $\pi$.

Figure 2: Geometric intuition of the vector space.

Figure $2(\mathrm{~b})$ shows the relative positioning of the 3-dimensional vectors of Figure 2(a) in bi-dimensional plane $\pi$. We then compute the angle $\alpha$ and the distance between the two vectors to understand how much they differ from each other. We tested multiple words, randomly sampling 10,000 words from the GloVe (Pennington et al., 2014) dictionary, and, by using different embedding dimensions (50, 100, 200, 300), we observed that, employing the synonyms and hyponyms provided by the Natural Language ToolKit (NLTK) (Bird and Loper, 2004), most similar words distribute closer considering the Euclidean distance and the angle in the theoretical model.

Figure 3 presents a radar plot that illustrates the relationship between various synonyms and hyponyms of the word "Death", which can be considered sensitive in some scenarios, providing an understanding of the distribution of these words. We noticed that direct hyponyms, represented by a cross $(\times)$, and synonyms, represented by a square $(\square)$, are in the immediate neighbourhood of the word of interest (the centre of the polar coordinates).

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-16.jpg?height=528&width=1350&top_left_y=438&top_left_x=385)

Figure 3: Radar plot showing the distribution of sampled words and the word "Death" considering Euclidean distance and angle. The symbols of crosses ( $\times$ ) and squares ( $\square$ ) are utilized to represent the linguistic relations of a given word. Specifically, the crosses indicate the hyponyms, while the squares represent the synonyms. The grey circles (o) represent other words that are neither hyponyms nor synonyms.

Between the centre of the radar and the similar words highlighted, there are many unrelated terms (o symbol in Figure 3), i.e. words that are not synonyms and hyponyms of the word of interest. To mitigate the risk of selecting a too similar word as obfuscation term, our proposal consists of creating a "safe box" around the word embedding, in which no obfuscation words can be sampled. The notion of a safe and candidate box is inspired by literature in the domain of positional masking (Allshouse et al., 2010; Hampton et al., 2010). For instance Hampton et al. (2010) proposed to mask geographic points by projecting them on a new location which is randomly selected to fall on a ring, i.e., the space between two concentric disks centred in the original data point, ensuring a minimum of privacy, since the position of the original point is always masked outside the inner disk. Also, we propose to use a second area around the term that needs to be obfuscated, called "candidate box" so that the terms selected will be similar enough to the original query. Hence, the name of the approach: Words Blending Boxes (WBB).

### 4.2. WBB Mechanism

Figure 4 illustrates the different steps of the WBB mechanism for obfuscating the user query. In the following subsections, we present the details of
each obfuscation step. Finally, we provide the mathematical proof of the DP property for the proposed mechanism.

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-17.jpg?height=480&width=1331&top_left_y=584&top_left_x=386)

Figure 4: WBB mechanism schematic overview of the obfuscation procedure. Compared to the pipeline presented in Figure 1, this is the "Obfuscation Mechanism" component.

### 4.2.1. Preprocessing

Firstly, WBB preprocesses the original query text. This includes converting the text to lowercase and tokenizing it (i.e., splitting it into isolated terms). Such processing is performed to normalize the text and compute the embeddings accurately. Subsequently, inspired by Peng et al. (2022) who observed that perturbing only names and adjectives can significantly enhance the empirical privacy of texts without compromising the performance of the downstream task, we use Part-Of-Speech (POS) tagging to recognize names and adjectives that will be later obfuscated by WBB.

Finally, all words are encoded using a non-contextualized encoder $\phi$.

### 4.2.2. Mapping Function $f_{\text {mapping }}^{(k, n)}$

As mentioned above, to identify among which words to sample the obfuscation word, we are taken between two competing needs:

- Safety: The non-contextualized representation of the obfuscation term needs to be far enough from the query term representation to make sure it is not a direct synonym/hypernym;
- Effectiveness: The non-contextualized representation of the obfuscation term needs to be sufficiently close to maintain a topical relation with the obfuscated term.

To identify which words to consider as a viable candidate for obfuscating a word $w$, we consider three different similarity measures: the cosine similarity, the Euclidean similarity, and the product of the two. We refer to approaches instantiated using each measure respectively as angle obfuscation, distance obfuscation, and product obfuscation. The selected distance functions aim to address the concerns raised in Section 4.1. On the other hand, the product of the two functions is recommended in order to balance their respective impact. Hence, for each word $w^{\prime}$, we can define $s_{w, w^{\prime}}=m\left(\phi(w), \phi\left(w^{\prime}\right)\right)$ the similarity between the original word $w$ using one of the aforementioned similarity measures. Then, the mechanism sorts the words based on the similarity results computed.

Once words have been obfuscated, we can define a function $f_{\text {mapping }}^{(k, n)}$ that produces the candidate set $\mathcal{C}$ such that the candidate contains the $n$ closest words to the original word $w$, whose similarity is above the similarity of the $k$-th most similar term. By excluding the first top- $k$ words, we ensure that all the words are sufficiently different from the original ones. At the same time, the dimension of the candidate set, i.e., $n$, allows one to choose a set that contains many different words. One of the major advantages of this procedure is that the parameters can be tuned according to the user's privacy concern: a very concerned user can adopt a large $k$ and $n$ : the query will be very different from the original one. Therefore, we can expect high privacy but, at the same time, possibly low retrieval performance. On the contrary, a not-concerned user can use a small $k$ (even 0 , if they accept that a word can be obfuscated with itself) and $n$, achieving higher effectiveness but lower privacy. Furthermore, the presence of these bounding boxes to sample obfuscated words ensures that the $\varepsilon$ of the WBB mechanism (described in the next section) cannot cause catastrophic data releases, in which the query is obfuscated with the query itself, achieving privacy only from a formal but not a practical point of view.

### 4.2.3. Sampling Function $f_{\text {sampling }}^{(k, n, \varepsilon)}$

The last phase consists of sampling the substitution words among the candidates present in $\mathcal{C}$ for each word to obfuscate, using the function $f_{\text {sampling }}^{(k, n, \varepsilon)}$.

Such sampling relies on the exponential mechanism described in Section 2 . To instantiate the exponential mechanism, we need a utility function $u$. More in detail, given $s_{w, w^{\prime}}$ the similarity score between words $w$ and $w^{\prime}$, we compute

for each $w^{\prime} \in \mathcal{C}$ the $Z_{\text {score }}$ as $Z_{\text {score }}=\frac{s_{w, w^{\prime}}-\mu}{\sigma}$, where $\mu$ is the average similarity over the words $w^{\prime} \in \mathcal{C}$ and $\sigma$ the standard deviation of the similarities in the
set $\mathcal{C}$ provided after the mapping step of the mechansim.

Finally, we normalize such scores into the interval $[0,1]$, computing the final utilities using the function in Equation4. The utility function is defined as follows:

$$
\begin{equation*}
u\left(w, w^{\prime}\right)=\frac{1}{1+\exp \left(Z_{\text {score }}\right)} \tag{4}
\end{equation*}
$$

We want to underline that our way of employing DP is fundamentally different from the one currently adopted by other approaches addressing similar tasks, as described in Subsection 3.1. Indeed, those approaches employ DP to perturb the embedding vectors, but by setting the value of $\varepsilon$ to be sufficiently large, the noise becomes so small that the produced query is exactly the same as the original one. In our case, we use DP to sample from a set of words that, by construction, are safer as the set does not contain the original word nor highly similar terms, according to their non-contextualized embeddings.

### 4.2.4. Formal proof of DP for WBB

To assemble all the steps together, Algorithm 1 provides the WBB pseudocode. The proof of the $\varepsilon$-DP stems from its exponential mechanism.

Theorem 1. The mechanism $\mathcal{M}$ explained in Algorithm 1 is $\varepsilon$-Differentially Private.

Proof. Let $\mathcal{X}$ be an input set that is mapped into the corresponding output set $\mathcal{Y}$ by the mechanism $\mathcal{M}$. For any pair input $x, x^{\prime} \in \mathcal{X}$, identifying with $Z_{\text {score }}$ and $Z_{\text {score }}^{\prime}$ the respective standardized initial scores, it holds that using the definition of utility function introduced in Equation 4 , the sensitivity $\Delta u$ is bounded by 1 , since:

$$
\begin{align*}
\Delta u & =\max _{y \in \mathcal{Y}} \max _{x, x^{\prime} \in \mathcal{X}}\left|u(x, y)-u\left(x^{\prime}, y\right)\right| \\
& =\max _{y \in \mathcal{Y}} \max _{x, x^{\prime} \in \mathcal{X}}\left|\frac{1}{1+\exp \left(Z_{\text {score }}\right)}-\frac{1}{1+\exp \left(Z_{\text {score }}^{\prime}\right)}\right|  \tag{5}\\
& \leq 1
\end{align*}
$$

Hence, we can follow a reasoning similar to the proof of the exponential mechanism itself for concluding the proof (McSherry and Talwar, 2007). The sampling function of the Algorithm 1 provides the privacy property of the mechanism. Given a privacy budget $\varepsilon>0$, the probability of sampling an

```
Algorithm 1: WBB Mechanism.
    Input: Original query $Q=\left\langle w_{0}, \ldots, w_{\ell}\right\rangle, \varepsilon>0, n>0, k>0$
    Output: Obfuscated query $\tilde{Q}=\left\langle\tilde{w}_{0}, \ldots, \tilde{w}_{\ell}\right\rangle$
    Data: Vocabulary $V$, set of POS tags $S$, function $m$
    1 Define $\tilde{Q}=\langle\rangle$
    2 // Preprocessing
    3 Tokenize the query $Q$, and POS tag each word $w_{i}$
    for $w_{i} \in Q$ do
        if $\operatorname{tag}\left(w_{i}\right) \in S$
            then
                // Mapping Function: $f_{\text {mapping }}^{(k, n)}$
                get $s_{i}=m\left(w_{i}, v\right) \forall v \in V$
                sort $s_{i}$ and get their position (pos)
                safe box $=\{v \in V: \operatorname{pos}(v) \leq k\}$
                $\mathcal{C}=\{v \in V: k<\operatorname{pos}(v) \leq n+k\}$
                // Sampling Function: $f_{\text {sampling }}^{(k, n, \varepsilon)}$
                get $Z_{\text {score }} \forall w \in \mathcal{C}$
                get $u_{i} \forall w \in \mathcal{C}$ according to Equation 4
                $w_{i}=\tilde{w}_{i}$ where $f_{\text {sampling }}^{(k, n, \varepsilon)}\left(u_{i}\right)=\tilde{w}_{i}$ from $\mathcal{C}$
            Append $w_{i}$ to $\tilde{Q}$
    return $\tilde{Q}$
```

output $y$ given an input $x$ is given by the exponential mechanism, hence:

$$
\begin{equation*}
\mathrm{P}\left[f_{\text {sampling }}(x)=y\right]=\frac{\exp \left(\frac{\varepsilon u(x, y)}{2 \Delta u}\right)}{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u(x, \tilde{y})}{2 \Delta u}\right)} \tag{6}
\end{equation*}
$$

Finally, we conclude the proof with the following inequality, using the expo-
nential mechanism employed in the sampling function:

$$
\begin{align*}
& \frac{\mathrm{P}\left[f_{\text {sampling }}(x)=y\right]}{\mathrm{P}\left[f_{\text {sampling }}\left(x^{\prime}\right)=y\right]}=\frac{\frac{\exp \left(\frac{\varepsilon u(x, y)}{2 \Delta u}\right)}{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u(x, \dot{y})}{(\Delta u}\right)}}{\frac{\exp \left(\frac{\varepsilon u\left(x^{\prime}, y\right)}{2 \Delta u,}\right)}{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u\left(x^{\prime}, \tilde{y}\right)}{2 \Delta u}\right)}} \\
& =\exp \left(\frac{\varepsilon\left(u(x, y)-u\left(x^{\prime}, y\right)\right)}{2 \Delta u}\right) \frac{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u\left(x^{\prime}, \tilde{y}\right)}{2 \Delta u}\right)}{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u(x, \tilde{y})}{2 \Delta u}\right)}  \tag{7}\\
& \leq \exp \left(\frac{\varepsilon(\Delta u)}{2 \Delta u}\right) \frac{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u\left(x^{\prime}, \tilde{\tilde{x}}\right.}{2 \Delta u}\right)}{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u(x, \tilde{y})}{2 \Delta u}\right)} \\
& \leq \exp \left(\frac{\varepsilon}{2}\right) \exp \left(\frac{\varepsilon}{2}\right) \frac{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u(x, \tilde{y})}{2 \Delta u}\right)}{\sum_{\tilde{y} \in \mathcal{Y}} \exp \left(\frac{\varepsilon u(x, \tilde{y})}{2 \Delta u}\right)}=e^{\varepsilon}
\end{align*}
$$

This terminates the formal bases of the proposed WBB method and proves the $\varepsilon$-Differential Privacy of the mechanism in Algorithm 1.

## 5. Experimental Analysis

This section provides an in-depth analysis and discussion of the results obtained through the experiments.

### 5.1. Experimental Setup

As encoder function $\phi$, we use the embeddings of GloVe (Pennington et al. 2014) trained on the Common Crowd as vocabulary for our investigations. We experimented with vectors of size 300; other sizes have comparable behaviour. We test the obfuscation approach considering two different collections: TREC Deep Learning (DL'19) Craswell et al., 2020), based on the MS MARCO (Nguyen et al., 2016) passages corpus which contains 43 queries, and TREC Robust '04 (Voorhees, 2004), which relies on disks 4 and 5 of the TIPSTER corpus, minus congressional records, and contains 249 queries. Additionally, we consider four retrieval models, considering two sparse bag-of-word models, namely BM25 (Robertson et al., 1994) and Vector

Space Model (TF-IDF) Salton et al., 1975), and also two dense bi-encoders, TAS-B (Hofstätter et al., 2021) and Contriever (Izacard et al., 2022). These models represent our non-cooperating IRS. As mentioned in Section 4, the results are then merged and re-ranked using Contriever, simulating a safe reranking on the user's side. For each obfuscation mechanism and collection, we generate 20 obfuscated queries to retrieve the top-100 documents.

For reproducibility purposes, we release the code we used at https:// github.com/Kekkodf/WBB-QueryObfuscation.

### 5.1.1. Baseline Approaches

To evaluate our mechanism, we compare performances and privacy with two different obfuscation mechanisms, namely the Calibrated Multivariate

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-22.jpg?height=51&width=1369&top_left_y=1037&top_left_x=370)
et al. 2020) mechanisms. As described in Section 3.1, CMP and Mhl obfuscate each word independently and merge them in a single query. The obfuscation is achieved by adding some noise to the non-contextual embedding of each word and taking the closest word to the noisy embedding. In particular, CMP samples the noise from a multivariate Laplace distribution. Mhl, on the other hand, uses the Mahalanobis norm to modify the direction in which the noise is sampled, such that it is more likely to sample denser areas of the embedding space to increase the chances that a word is not obfuscated by itself. We also compare our results with other state-of-theart mechanisms which do not guarantee a formal $\varepsilon$-DP, but that have been explicitly designed for IR, i.e., the methods proposed by Arampatzis et al. (2011) (referred to as AEA) and Fröbe et al. (2022) (FEA).

### 5.2. Results

### 5.2.1. Privacy Guarantees

As detailed in Subsection 3.3 , the metrics commonly employed to assess privacy levels are the Uncertainty Statistics, Lexical Similarity, and Semantic Similarity. However, classical measures for the WBB mechanism are not quite effective using the Uncertainty Statistics: by mechanism definition, such uncertainty measures, i.e., the mechanism failure rate $N_{w}=$ $\operatorname{Pr}[\mathcal{M}(w)=w]$ and $S_{w}=\min |\{\mathcal{S} \subseteq \operatorname{Image}(\mathcal{M}): \operatorname{Pr}[\mathcal{M}(w) \notin \mathcal{S}] \leq \eta\}|$ for the number of words to which the mechanism obfuscate a term, will always be 0 as the WBB mechanism preserves the confidentiality of a term and never obfuscates it with itself. Notice that implicitly $N_{w}$ describes the proportion of failures: "How many times did the obfuscation mechanism not obfuscate,
leaving the user exposed?", we argue that even a single time is too much. Similarly, $S_{w}$ is proportional to the WBB parameters $k$ and $n$, and thus it is not informative. In the WBB mechanism, the measure $S_{w}$ resembles more the $k$-anonymity property (Sweeney, 2002). Given a Candidate box $\mathcal{C}$ of size $n$, the probability of re-identification of a term without prior knowledge is equal to $\frac{1}{n}$, therefore enlarging the number of possible candidates $n$ the probability of guessing the correct term reduces to 0 even for small privacy budgets $\varepsilon$.

Thus, our privacy analysis focuses on the Lexical and Semantic Similarity measures to evaluate the quality of the obfuscated queries produced. To evaluate the lexical similarity, we used the Jaccard Similarity. For the semantic similarity, we employ as embedding function MiniLM (Reimers and Gurevych, 2020) using the L6 v2 version available on HuggingFace ${ }^{1}$ which maps the sentence to a 384 -dimensional dense vector space and computes the cosine similarity between the original and the obfuscated encodings of the queries. To evaluate the effectiveness of the WBB mechanism, we compared it against existing state-of-the-art mechanisms (Feyisetan et al., 2020; $\mathrm{Xu}$ et al., 2020). We modified the different parameters of the WBB mechanism, namely the distance function and the parameters $(k, n)$. The results are reported in Table 1, where the lower the Jaccard and semantic similarities are, the higher the privacy provided.

Discussion. We observe that the Jaccard similarity for State-of-the-Art mechanisms is proportional to the privacy budget $\varepsilon$, suggesting that these mechanisms - especially for large $\varepsilon$, struggle to replace the original words due to the insufficient noise added. Nonetheless, WBB guarantees that words identified as sensitive and captured by the POS tagger during preprocessing are always masked - such words fall into the safe box and cannot be used for masking. Consequently, the intersection between original and obfuscated words invariably remains of size 0 , ensuring that the Jaccard similarity is consistently 0 .

Regarding the Semantic Similarity, we need a small step back to address one of the major limits of DP: a notable critique levelled against $\varepsilon$-DP is that as the privacy budget $\varepsilon$ increases, the privacy assurances offered by the mechanism vanish (Dwork and Roth, 2014; Domingo-Ferrer et al., 2021). Indeed, when the value of $\varepsilon$ increases, the State-of-the-Art mechanisms, i.e.,[^1]

Table 1: Average Jaccard and Semantic Similarities of the obfuscated queries for the collection MSMARCO DL'19 (Craswell et al. 2020). The analysis comprises the Stateof-the-Art results, the $\operatorname{WBB}(k, n)$ using different distance functions, i.e., $\mathbf{A}$ and $\mathbf{D}$ for the cosine and the Euclidean Distance, and $\mathbf{P}$ for the product between the two to create safe and candidate box, and the WBB with fixed sizes for safe $(\boldsymbol{k})$ and candidate $(\boldsymbol{n})$ box.

|  | Jaccard Similarity |  |  |  |  |  |  |  |  |  |  | Semantic Similarity |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\varepsilon$ |  |  |  |  |  |  |  |  |  |  | $\varepsilon$ |  |  |  |  |  |  |  |
|  | Mechansim | 1 | 5 | 10 | 12.5 | 15 | 17.5 | 20 | 50 | No DP | 1 | 5 | 10 | 12.5 | 15 | 17.5 | 20 | 50 | No DP |
| Sot $A$ | AEA | - | - | - | - | - | - | - | - | 0.338 | - | - | - | - | - | - | - | - | 0.509 |
|  | FEA | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-24.jpg?height=31&width=55&top_left_y=794&top_left_x=681) | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-24.jpg?height=31&width=55&top_left_y=794&top_left_x=803) | - | - | - -1 -1 | - | 0 . | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-24.jpg?height=29&width=64&top_left_y=794&top_left_x=1173) | - | - | - | - | - | - | - | 0.077 |
|  | CMP | 0 . | 0.003 | 0.100 | 0.277 | 0.509 | 0.707 | 0.814 | 0.935 | - | 0.017 | 0.029 | 0.195 | 0.417 | 0.645 | 0.803 | 0.858 | 0.907 | - |
|  | Mhl | 0 . | 0.002 | 0.054 | 0.140 | 0.288 | 0.453 | 0.616 | 0.935 | - | 0.020 | 0.028 | 0.114 | 0.240 | 0.420 | 0.592 | 0.730 | 0.910 | - |
| Dist. Meas. | $\operatorname{WBB}(4,50) \mathrm{A}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.425 | 0.425 | 0.430 | 0.430 | 0.432 | 0.427 | 0.436 | 0.428 | - |
|  | $\operatorname{WBB}(4,50) \mathbf{D}$ | 0 . | 0.  | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.366 | 0.370 | 0.367 | 0.366 | 0.361 | 0.368 | 0.368 | 0.366 | - |
|  | $\operatorname{WBB}(4,50) \mathbf{P}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.419 | 0.423 | 0.425 | 0.427 | 0.421 | 0.425 | 0.424 | 0.426 | - |
| $k$   <br> 1   | $\operatorname{WBB}(2,50) \mathrm{A}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.439 | 0.441 | 0.436 | 0.442 | 0.439 | 0.431 | 0.436 | 0.441 | - |
|  | $\operatorname{WBB}(6,50) \mathrm{A}$ | 0 . |  | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-24.jpg?height=29&width=73&top_left_y=968&top_left_x=1101) | 0.425 | 0.425 | 0.430 | 0.430 | 0.432 | 0.427 | 0.436 | 0.428 | - |
|  | $\operatorname{WBB}(\mathbf{1 2 , 5 0 )} \mathrm{A}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.413 | 0.413 | 0.410 | 0.410 | 0.412 | 0.410 | 0.414 | 0.416 | - |
|  | $\operatorname{WBB}(\mathbf{1 8}, 50) \mathrm{A}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-24.jpg?height=28&width=73&top_left_y=1014&top_left_x=1101) | 0.404 | 0.408 | 0.405 | 0.406 | 0.404 | 0.403 | 0.405 | 0.407 | - |
|  | $\operatorname{WBB}(\mathbf{2 4}, 50) \mathrm{A}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.394 | 0.406 | 0.398 | 0.401 | 0.394 | 0.393 | 0.394 | 0.404 | - |
| $n$ | $\operatorname{WBB}(4,5) \mathrm{A}$ | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.495 | 0.495 | 0.499 | 0.493 | 0.490 | 0.493 | 0.498 | 0.495 | - |
|  | $\operatorname{WBB}(4, \mathbf{1 0 0}) \mathrm{A}$ | 0 . |  | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | _ | 0.406 | 0.410 | 0.408 | 0.408 | 0.409 | 0.412 | 0.406 | 0.410 | - |
|  | $\operatorname{WBB}(4,150) \mathrm{A}$ | 0 . |  | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.396 | 0.396 | 0.395 | 0.390 | 0.398 | 0.399 | 0.392 | 0.397 | - |
|  | $\operatorname{WBB}(4,200) \mathrm{A}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-24.jpg?height=28&width=28&top_left_y=1133&top_left_x=649) | 0 | 0 . | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.396 | 0.387 | 0.390 | 0.387 | 0.390 | 0.388 | 0.389 | 0.382 | - |
|  | $\operatorname{WBB}(4,250) \mathrm{A}$ | 0 . | 0 . | 0 | 0 . | 0 . | 0 . | 0 . | 0 . | - | 0.373 | 0.383 | 0.383 | 0.379 | 0.380 | 0.379 | 0.379 | 0.384 | - |

CMP and Mhl, struggle in the obfuscation process, suggesting that at high $\varepsilon$ values, only formal privacy is maintained. An important aspect handled by the WBB mechanism through computing the Semantic Similarity values is that the privacy budget $\varepsilon$ does not impact the sentence similarity of the obfuscated queries. This characteristic overcomes the limitation of formal privacy because WBB not only ensures the formal guarantee of the $\varepsilon$-DP property, as proved in Section 4.2.4 but also provides an additional layer of concrete privacy to the original query. Specifically, for all instances analysed, the average cosine similarity computed is always below 0.5 , meaning that the WBB mechanism provides a true obfuscation of the original queries. Only in the case of a small value of $k$, i.e., the candidate box size, the cosine similarity approaches the 0.5 value.

Moreover, the most severe privacy strategies lowering the semantic similarity are the ones that use as distance function the Euclidean distance and the one that proposes a large size candidate box, i.e., $\mathrm{WBB}(4,50) \mathbf{D}$ and WBB $(4,250) \mathbf{A}$. This fact suggests that as we enlarge the candidate box, or on the other hand, we enforce the Euclidean Distance as a distance function to compute the mechanism boxes, more privacy is provided at the same level of formal privacy.

Table 2: Mean Recall is achieved by pooling the documents retrieved for each obfuscation query. The mechanism $\operatorname{WBB}(k, n)$ has been evaluated by adopting as the similarity function the angle base distance $(\mathbf{A})$.

| IR model | Mechanism | Robust '04 |  |  |  |  |  |  |  |  | DL '19 |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | 1 | 5 | 10 | 12.5 | $\varepsilon$ <br> 15 | 17.5 | 20 | 50 | No-DP | 1 | 5 | 10 | 12.5 | $\varepsilon$ <br> 15 | 17.5 | 20 | 50 | No-DP |
| BM25 | No privacy | - | - | - | - | - | - | - | - | 0.410 | - | - | - | - | - | - | - | - | 0.454 |
|  | AEA | - -1 | - | - | - | - | - | - | - | 0.420 | - | - | - | - | - | - | - | - | 0.445 |
|  | FEA | - | - | - | - | - | - | - | - | 0.140 | - | - | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=64&top_left_y=720&top_left_x=1422) | - | - | - | 0.231 |
|  | CMP | 0.020 | 0.146 | 0.483 | 0.510 | 0.489 | 0.442 | 0.421 | 0.407 | - | 0.011 | 0.135 | 0.384 | 0.534 | 0.517 | 0.514 | 0.480 | 0.444 | - |
|  | Mhl | 0.032 | 0.089 | 0.398 | 0.500 | 0.512 | 0.501 | 0.466 | 0.407 | - | 0.000 | 0.118 | 0.294 | 0.411 | 0.515 | 0.529 | 0.529 | 0.444 | - |
|  | $\overline{\operatorname{WBB}}(2,15) \mathrm{A}$ | 0.119 | 0.131 | 0.141 | 0.143 | 0.141 | 0.147 | 0.139 | 0.145 | - | 0.193 | 0.195 | 0.185 | 0.190 | 0.182 | 0.178 | 0.189 | 0.196 | - |
|  | $\operatorname{WBB}(2,20) \mathrm{A}$ | 0.115 | 0.124 | 0.142 | 0.133 | 0.143 | 0.134 | 0.131 | 0.136 | - | 0.187 | 0.195 | 0.176 | 0.186 | 0.188 | 0.177 | 0.188 | 0.193 | - |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.119 | 0.131 | 0.137 | 0.138 | 0.139 | 0.132 | 0.141 | 0.144 | - | 0.191 | 0.179 | 0.191 | 0.196 | 0.184 | 0.189 | 0.196 | 0.189 | - |
|  | $\operatorname{WBB}(4,20) \mathbf{A}$ | 0.117 | 0.124 | 0.142 | 0.143 | 0.138 | 0.134 | 0.139 | 0.139 | - | 0.172 | 0.180 | 0.187 | 0.179 | 0.179 | 0.175 | 0.178 | 0.185 | - |
| TF-IDF | No privacy | - | - | - | - | - | - | - | - | 0.411 | - | - | - | - | - | - | - | - | 0.451 |
|  | AEA | - | - | - | - | - | - | - | - | 0.420 | - | - | - | - | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=905&top_left_x=1544) | - | 0.443 |
|  | FEA | - | - | - | - | - | - | - | - | 0.139 | - | - | - | - | - | - | - | - | 0.231 |
|  | CMP | 0.020 | 0.146 | 0.487 | 0.512 | 0.491 | 0.444 | 0.423 | 0.408 | - | 0.011 | 0.135 | 0.386 | 0.535 | 0.515 | 0.515 | 0.478 | 0.442 | - |
|  | Mhl | 0.032 | 0.089 | 0.398 | 0.504 | 0.516 | 0.504 | 0.468 | 0.408 | - | 0.000 | 0.118 | 0.295 | 0.412 | 0.515 | 0.530 | 0.527 | 0.442 | - |
|  | WBB $(2,15) \mathrm{A}$ | 0.120 | 0.128 | 0.143 | 0.143 | 0.141 | 0.148 | 0.140 | 0.145 | - | 0.190 | 0.195 | 0.184 | 0.190 | 0.181 | 0.178 | 0.188 | 0.194 | - |
|  | WBB $(2,20) \mathbf{A}$ | 0.114 | 0.126 | 0.140 | 0.136 | 0.143 | 0.134 | 0.132 | 0.137 | - | 0.186 | 0.194 | 0.175 | 0.183 | 0.188 | 0.175 | 0.187 | 0.191 | - |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.122 | 0.130 | 0.137 | 0.137 | 0.139 | 0.134 | 0.140 | 0.143 | - | 0.190 | 0.180 | 0.190 | 0.194 | 0.183 | 0.190 | 0.197 | 0.188 | - |
|  | $\operatorname{WBB}(4,20) \mathrm{A}$ | 0.118 | 0.124 | 0.142 | 0.144 | 0.137 | 0.134 | 0.136 | 0.136 | - | 0.172 | 0.181 | 0.185 | 0.178 | 0.179 | 0.175 | 0.177 | 0.183 | - |
| Contriever | No privacy | - | - | - | - | - | - | - | - | 0.392 | - | - | - | - | - | - | - | - | 0.528 |
|  | AEA | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=1115&top_left_x=699) | - -1 | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=1115&top_left_x=939) | - -1 | - -1 | 0.419 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=1115&top_left_x=1248) | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=1115&top_left_x=1305) | $\overline{-} \quad$ - | - | - | - -1 | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=1115&top_left_x=1603) | $0.497 \quad$ |
|  | FEA | - | - | - | - | - | - | - | - | 0.204 | - | - | - | - | - | - | - | - | 0.204 |
|  | CMP | 0.034 | 0.106 | 0.469 | 0.507 | 0.481 | 0.433 | 0.406 | 0.392 | - -1 0 | 0.000 | 0.077 | 0.446 | 0.615 | 0.644 | 0.628 | 0.576 | 0.512 | - |
|  | Mhl | 0.026 | 0.088 | 0.345 | 0.473 | 0.503 | 0.497 | 0.460 | 0.392 | - | 0.000 | 0.057 | 0.264 | 0.476 | 0.601 | 0.641 | 0.650 | 0.512 | - |
|  | $\operatorname{WBB}(2,15) \mathrm{A}$ | 0.361 | 0.351 | 0.337 | 0.337 | 0.326 | 0.335 | 0.335 | 0.340 | - | 0.347 | 0.341 | 0.352 | 0.349 | 0.347 | 0.352 | 0.345 | 0.351 | - |
|  | $\operatorname{WBB}(2,20) \mathbf{A}$ | 0.366 | 0.346 | 0.330 | 0.332 | 0.332 | 0.343 | 0.332 | 0.327 | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=29&width=73&top_left_y=1225&top_left_x=1115) | 0.380 | 0.386 | 0.384 | 0.397 | 0.394 | 0.393 | 0.393 | 0.380 | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=29&width=64&top_left_y=1225&top_left_x=1662) |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.352 | 0.339 | 0.319 | 0.318 | 0.315 | 0.315 | 0.318 | 0.322 | - | 0.342 | 0.352 | 0.365 | 0.351 | 0.346 | 0.348 | 0.348 | 0.362 | - |
|  | $\operatorname{WBB}(4,20) \mathrm{A}$ | 0.359 | 0.344 | 0.313 | 0.317 | 0.310 | 0.315 | 0.316 | 0.320 | - | 0.240 | 0.270 | 0.364 | 0.270 | 0.238 | 0.289 | 0.289 | 0.258 | - |
| Tas-B | No privacy | - | - | - | - | - | - | - | - | 0.358 | - | - | - | - | - | - | - | - | 0.518 |
|  | AEA | - | - | - -1 | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-25.jpg?height=28&width=55&top_left_y=1325&top_left_x=939) | - | - | 0.387 | - | - | - | - | - | - | - -1 | - | 0.491 |
|  | FEA | - | - | - | - | - | - | - | - | 0.161 | - | - | - | - | - | - | - | - | 0.238 |
|  | CMP | 0.027 | 0.080 | 0.434 | 0.477 | 0.444 | 0.398 | 0.369 | 0.356 | - | 0.000 | 0.063 | 0.392 | 0.615 | 0.636 | 0.622 | 0.575 | 0.498 | - |
|  | Mhl | 0.028 | 0.064 | 0.310 | 0.438 | 0.464 | 0.460 | 0.423 | 0.356 | - | 0.000 | 0.042 | 0.275 | 0.455 | 0.584 | 0.645 | 0.638 | 0.499 | - |
|  | $\operatorname{WBB}(2,15) \mathrm{A}$ | 0.315 | 0.335 | 0.352 | 0.322 | 0.313 | 0.318 | 0.325 | 0.315 | - | 0.322 | 0.321 | 0.323 | 0.325 | 0.325 | 0.333 | 0.332 | 0.338 | - |
|  | $\operatorname{WBB}(2,20) \mathrm{A}$ | 0.314 | 0.332 | 0.360 | 0.314 | 0.314 | 0.322 | 0.314 | 0.320 | - | 0.347 | 0.356 | 0.356 | 0.349 | 0.353 | 0.359 | 0.359 | 0.352 | - |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.300 | 0.331 | 0.350 | 0.300 | 0.301 | 0.301 | 0.301 | 0.302 | - | 0.312 | 0.318 | 0.327 | 0.323 | 0.328 | 0.327 | 0.326 | 0.324 | - |
|  | $\operatorname{WBB}(4,20) \mathbf{A}$ | 0.305 | 0.331 | 0.350 | 0.301 | 0.302 | 0.301 | 0.300 | 0.300 | - | 0.218 | 0.244 | 0.334 | 0.239 | 0.231 | 0.220 | 0.264 | 0.215 | - |

### 5.2.2. Retrieve Relevant Documents

As multiple obfuscation queries are sent to the IRS, our first measure of interest is the pooled recall over all obfuscation queries, i.e., the recall considering the set of unique documents retrieved by any obfuscation query. This allows us to verify that the WBB approach can bring enough relevant documents to the user's side. Table 2 reports the observed merged recall. Based on the results of the privacy evaluation, Subsection 5.2.1, we report the WBB retrieval capabilities based on the angle obfuscation.

There are two different patterns, depending on whether we assume the black box IRS to be based on a lexical (BM25 and TF-IDF) or semantic IRS (Contriever and Tas-B). In the first case, the recall of WBB is lower than those of other DP mechanisms and AEA. At the same time, the recall of WBB is in line with the one of FEA on Robust '04, and lower on DL '19. If we switch to semantic IRS, the recall remains overall lower compared to other DP mechanisms and AEA, but the margin is far smaller. Nevertheless, when
compared with FEA for semantic IRS, the performance of WBB is higher. The reason behind this behaviour is explained by the fact that lexical IRS rely on exact matching. By completely preventing the usage of query terms on the obfuscated queries, both WBB and FEA suffer in terms of effectiveness if the IRS is lexical. Vice versa, the phenomenon is less evident regarding semantic systems, as the WBB relies on the candidate box: terms that are sufficiently close in the latent embedding space to the original ones. Thus, we can expect some form of topical relation between real and obfuscation terms that allows semantic IRS to be less affected by the privacy protection measures put in place by WBB. When it comes to the behaviour of the different DP approaches concerning the $\varepsilon$, we notice a completely different pattern between classical approaches based on introducing noise on the word embeddings (i.e., CMP and Mhl) and our solution based on sampling the words according to their similarity in the embedding space. Both CMP and Mhl are severely affected by changes in $\varepsilon$. In particular, for $\varepsilon=1$ and $\varepsilon=5$, depending on the scenario, the recall is on par with or lower than the one by WBB. Then, starting with $\varepsilon=10$ up to $\varepsilon=20$, we observe a recall even higher than the one achieved by the original IRS with no privacy measures in place. The same pattern was also observed by Faggioli and Ferro (2024). The behaviour is explained by the fact that with such big $\varepsilon$, several terms in the obfuscated query will be the same as the ones in the original query. In contrast, the changed ones are often synonyms and thus cause some sort of query expansion. Finally, if $\varepsilon=50$, the recall is almost identical to the one without privacy: with such a $\varepsilon$, DP does not obfuscate at all, becoming simply a formal definition. Overall, WBB reacts to changes to $\varepsilon$, causing changes to recall in a quite bounded range. While this might be a limitation for more advanced users who wish to have the capacity to alter the system's behaviour completely, it might be an advantage for average users who are not at risk of ending up in catastrophic conditions. Finally, we changed the values of the parameters $(k, n)$ of the WBB mechanism, and as suggested by Faggioli and Ferro (2024), we analyzed the average recall measured using as privacy budget $\varepsilon=10$. Figure 5 reports the results obtained.

Discussion. Considering the different distance functions: as for the privacy analysis in Subsection 5.2.1, the WBB configurations that work the better are the angle and product-based obfuscation: the initial finding that we obtained when retrieving documents from the IRS is that, stemming naturally from the privacy analysis conducted in Subsection 5.2.1, the WBB mechanism

![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-27.jpg?height=423&width=1366&top_left_y=423&top_left_x=369)

Figure 5: Comparison matrices of the Recall for different obfuscations (angle, euclidean distance, and product), each at the same level of $\varepsilon$-DP $(\varepsilon=10)$, computed using Contriever and the DL'19 collection. With no privacy, Recall $=0.528$

configuration most significantly affected in terms of recall is the one using the Euclidean distance function for computing safe and candidate box. In addition, another important trend that can be derived from Figure 5 is that the lower the candidate box, the lower the recall: especially for the $\operatorname{WBB}(8,5)$ all the computed recalls are below 0.25 . This suggests that from a narrow candidate box and eliminating a high number of similar words, there are high privacy guarantees but low retrieval performance.

On the other hand, a further consistent pattern across all the comparison matrices is that the performance tends to increase as a higher number of candidates is allowed, particularly when the safe box is not excessively wide. The higher results are obtained when WBB is configured with only two words in the safe box, achieving a recall equal to 0.384 for $\operatorname{WBB}(2,20)$ and 0.375 for $\mathrm{WBB}(2,20)$, with a distance function angle and product based respectively.

### 5.2.3. Determining the Utility Preserved

As aforementioned, once the documents are available on the user's side, they need to be re-ranked using the real information needs. To implement this, we use Contriever as the reranker. Table 3 reports the nDCG@10 values after re-ranking the documents retrieved through obfuscation queries by each IRS. We report the results for $\varepsilon=10$, as WBB is not particularly affected by it, and it is one of the recommended values according to Faggioli and Ferro (2024). In line with what was observed for Table 2, WBB appears to struggle if the underneath IRS relies on exact matching, for the reasons previously mentioned. Nevertheless, it is also important to mention that the obfuscation queries generated by the best-performing methods for Robust ' 04 , CMP and

Table 3: Mean nDCG@10 is achieved by pooling the documents reranked for each obfuscation query. The mechanism $\operatorname{WBB}(k, n)$ has been evaluated by adopting as the similarity function the angle base distance $(\mathbf{A})$.

| IR model | Mechanism | Robust '04 |  |  |  |  |  |  |  |  | DL '19 |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | 1 | 5 | 10 | 12.5 | $\varepsilon$ <br> 15 | 17.5 | 20 | 50 | No-DP | 1 | 5 | 10 | 12.5 | $\varepsilon$ <br> 15 | 17.5 | 20 | 50 | No-DP |
| BM25 | No privacy | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=676&top_left_x=760) | - | - | - | - | - | 0.477 | - | - | - | - | - | - | - | - | 0.675 |
|  | $\overline{\mathrm{AEA}}$ | - -1 -1 | - -1 -1 | - -1 -1 | - | - -1 -1 | - -1 -1 | - -1 -1 | - -1 -1 | 0.423 | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=29&width=55&top_left_y=698&top_left_x=1187) | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=29&width=55&top_left_y=698&top_left_x=1246) | - | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=29&width=60&top_left_y=698&top_left_x=1483) | - -1 -1 | - | 0.557 |
|  | FEA | - | - | - | - | - | - | - | - | 0.147 | - | - | - | - | - | - | - | - | 0.069 |
|  | CMP | 0 . | 0.002 | 0.091 | 0.226 | 0.338 | 0.402 | 0.415 | 0.421 | - | 0 . | 0 . | 0.053 | 0.164 | 0.293 | 0.363 | 0.394 | 0.403 | - |
|  | Mhl | 0 | 0.001 | 0.037 | 0.106 | 0.204 | 0.303 | 0.369 | 0.421 | - | 0 . | 0 . | 0.016 | 0.067 | 0.154 | 0.253 | 0.329 | 0.403 | - |
|  | $\overline{\operatorname{WBB}}(2,15) \mathrm{A}$ | 0.099 | 0.106 | 0.115 | 0.111 | 0.125 | 0.116 | 0.105 | 0.111 | - | 0.225 | 0.226 | 0.225 | 0.224 | 0.236 | 0.223 | 0.236 | 0.248 | - |
|  | $\operatorname{WBB}(2,20) \mathbf{A}$ | 0.092 | 0.103 | 0.117 | 0.110 | 0.111 | 0.119 | 0.109 | 0.116 | - | 0.230 | 0.230 | 0.215 | 0.222 | 0.243 | 0.203 | 0.241 | 0.236 | - |
|  | $\operatorname{WBB}(4,15)$ A | 0.092 | 0.106 | 0.104 | 0.100 | 0.117 | 0.112 | 0.119 | 0.111 | - | 0.225 | 0.201 | 0.239 | 0.215 | 0.221 | 0.209 | 0.225 | 0.213 | - |
|  | $\operatorname{WBB}(4,20) \mathrm{A}$ | 0.096 | 0.095 | 0.115 | 0.108 | 0.113 | 0.110 | 0.113 | 0.107 | - | 0.189 | 0.198 | 0.209 | 0.198 | 0.202 | 0.186 | 0.191 | 0.221 | - |
| TF-IDF | No privacy | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=884&top_left_x=760) | - | - | - | - | - | 0.477 | - | - -1 -1 | - | - | - | - | - | - | 0.675 |
|  | AEA | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=906&top_left_x=699) | - -1 -1 | - | $\overline{-} \quad$ - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=906&top_left_x=939) | - -1 | - -1 | 0.421 | - | - | - -1 | - | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=906&top_left_x=1544) | - -1 | 0.557 |
|  | FEA | - | - | - | - | - | - | - | - | 0.148 | - | - | - | - | - | - | - | - | 0.068 |
|  | CMP | 0 . | 0.002 | 0.091 | 0.227 | 0.339 | 0.403 | 0.417 | 0.423 | - | 0 . | 0 . | 0.053 | 0.164 | 0.292 | 0.361 | 0.393 | 0.402 | - |
|  | Mhl | 0 . | 0.001 | 0.040 | 0.106 | 0.205 | 0.304 | 0.370 | 0.423 | - | 0 . | 0 . | 0.016 | 0.067 | 0.154 | 0.252 | 0.329 | 0.402 | - |
|  | $\operatorname{WBB}(2,15) \mathrm{A}$ | 0.099 | 0.102 | 0.113 | 0.110 | 0.123 | 0.117 | 0.106 | 0.110 | - | 0.224 | 0.227 | 0.223 | 0.223 | 0.236 | 0.223 | 0.235 | 0.249 | - |
|  | $\operatorname{WBB}(2,20) \mathbf{A}$ | 0.093 | 0.103 | 0.117 | 0.108 | 0.112 | 0.119 | 0.113 | 0.114 | - | 0.232 | 0.228 | 0.216 | 0.223 | 0.244 | 0.201 | 0.241 | 0.236 | - |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.094 | 0.102 | 0.106 | 0.100 | 0.117 | 0.115 | 0.116 | 0.111 | - | 0.223 | 0.200 | 0.238 | 0.215 | 0.223 | 0.209 | 0.225 | 0.213 | - |
|  | $\operatorname{WBB}(4,20) \mathrm{A}$ | 0.097 | 0.095 | 0.114 | 0.109 | 0.114 | 0.111 | 0.114 | 0.106 | - | 0.190 | 0.200 | 0.207 | 0.197 | 0.202 | 0.188 | 0.191 | 0.222 | - |
| Contriever | No privacy | - | - | - | - | - | - | - | - | 0.466 | - | - | - | - | - | - | - | - | 0.676 |
|  | AEA | - | - | - | - | - | - | - | - | 0.430 | - | - | $\overline{-} \quad$ - | - | ב | - | - | $\overline{-} \quad$ - | 0.567 |
|  | FEA | - | - | - | - | - | - | - | - | 0.200 | - | - | - | - | - | - | - | - | 0.056 |
|  | CMP | 0 . | 0.002 | 0.093 | 0.239 | 0.373 | 0.440 | 0.459 | 0.466 | - | 0 . | 0 . | 0.041 | 0.189 | 0.397 | 0.522 | 0.572 | 0.598 | - |
|  | Mhl | 0 . | 0.001 | 0.036 | 0.104 | 0.211 | 0.323 | 0.402 | 0.466 | - | 0 . | 0 . | 0.014 | 0.064 | 0.178 | 0.330 | 0.470 | 0.597 | - |
|  | WBB $(2,15) \mathrm{A}$ | 0.457 | 0.454 | $0.447 \quad$ | 0.445 | 0.444 | 0.452 | 0.442 | $0.448 \quad$ |  | 0.564 | 0.577 | 0.599 | 0.582 | 0.607 | 0.584 | 0.576 | 0.600 |  |
|  | WBB $(2,20) \mathbf{A}$ | 0.460 | 0.451 | 0.444 | 0.444 | 0.449 | 0.450 | 0.445 | 0.444 | - | 0.597 | 0.623 | 0.594 | 0.613 | 0.604 | 0.626 | 0.623 | 0.603 | - |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.450 | 0.447 | 0.431 | 0.435 | 0.432 | 0.432 | 0.433 | 0.442 | - | 0.599 | 0.616 | 0.611 | 0.588 | 0.606 | 0.612 | 0.589 | 0.611 | - |
|  | $\operatorname{WBB}(4,20) \mathrm{A}$ | 0.457 | 0.447 | 0.430 | 0.435 | 0.423 | 0.439 | 0.433 | 0.432 | - | 0.453 | 0.502 | 0.584 | 0.527 | 0.493 | 0.467 | 0.493 | 0.497 | - |
| Tas-B | No privacy | - | - -1 -1 | - -1 -1 | - | - -1 -1 | - -1 -1 | - -1 -1 | - | 0.469 | - | - | - | - | - | - | - -1 -1 | - -1 -1 | 0.674 |
|  | दAEA | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=60&top_left_y=1324&top_left_x=643) | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=1324&top_left_x=699) | - -1 | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=1324&top_left_x=939) | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=56&top_left_y=1324&top_left_x=997) | - | 0.431 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=1324&top_left_x=1246) | - | - | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-28.jpg?height=28&width=55&top_left_y=1324&top_left_x=1544) | - | 0.579 |
|  | FEA | - | - | - | - | - | - | - | - | 0.168 | - | - | - | - | - | - | - | - | 0.052 |
|  | CMP | 0 . | 0.001 | 0.085 | 0.225 | 0.355 | 0.423 | 0.439 | 0.446 | - | 0 . | 0 . | 0.048 | 0.195 | 0.415 | 0.552 | 0.615 | 0.649 | - |
|  | Mhl | 0 . | 0.001 | 0.032 | 0.097 | 0.196 | 0.305 | 0.384 | 0.446 | - | 0 . | 0 . | 0.015 | 0.069 | 0.180 | 0.340 | 0.488 | 0.648 | - |
|  | WBB $(2,15) \mathrm{A}$ | 0.457 | 0.448 | 0.440 | 0.443 | 0.441 | 0.445 | 0.442 | 0.442 | $\checkmark$ | 0.573 | 0.580 | 0.590 | 0.623 | 0.585 | 0.598 | 0.589 | 0.595 |  |
|  | WBB $(2,20) \mathbf{A}$ | 0.460 | 0.449 | 0.447 | 0.446 | 0.444 | 0.446 | 0.443 | 0.442 | - | 0.622 | 0.618 | 0.622 | 0.625 | 0.616 | 0.630 | 0.622 | 0.621 | - |
|  | $\operatorname{WBB}(4,15) \mathrm{A}$ | 0.454 | 0.451 | 0.436 | 0.438 | 0.432 | 0.431 | 0.430 | 0.442 | - | 0.616 | 0.593 | 0.606 | 0.607 | 0.608 | 0.604 | 0.609 | 0.594 | - |
|  | $\operatorname{WBB}(4,20) \mathrm{A}$ | 0.454 | 0.443 | 0.428 | 0.436 | 0.427 | 0.427 | 0.425 | 0.429 | - | 0.467 | 0.513 | 0.613 | 0.522 | 0.486 | 0.482 | 0.460 | 0.496 | - |

Mhl have a Jaccard similarity with the original query, respectively of 0.225 and 0.101 : one out 5 and one out of 10 words, respectively are left unchanged. As the original queries have, on average, 5.8 words for CMP, in expectation, slightly less than one term is left exactly as it is in every query. This means that, by sending 20 queries to the IRS, we can expect that each real term will appear 4 times across the set of obfuscated queries. The same reasoning applies straightforwardly to AEA, whose generated obfuscation queries have a Jaccard similarity of 0.200 . We argue that, by showing the real terms of the query to the IRS multiple times scattered across obfuscation queries, it is easy to recognize them. Vice-versa, both WBB and FEA suffer in terms of performance when the IRS uses lexical matching, but they are guaranteed to not show the real query terms, nor excessively related terms. If we compare WBB and FEA, FEA outperforms WBB on Robust ' 04 with BM25 and TFIDF by 0.030 and 0.031 nDCG points respectively. Vice versa, on DL '19, WBB outperforms FEA by 0.170 nDCG points on both BM25 and TF-IDF.

The pattern completely changes when we move to the semantic IRS. While the privacy guarantees remain the same, i.e., the obfuscation queries are always the same, the obfuscation terms used by WBB are semantically related in a latent space, thanks to the usage of the candidate box. As a consequence, when semantic IRS are in place, the more secure WBB obfuscation queries are also able to provide high performance, surpassed only by CMP on Robust ' 04 by a small margin ( $0.016 \mathrm{nDCG}$ points), while being the most effective solution on DL '19.
![](https://cdn.mathpix.com/cropped/2024_06_04_b46d40ca40cf72e60120g-29.jpg?height=420&width=1370&top_left_y=899&top_left_x=367)

Figure 6: Comparison matrices of nDCG@10 for different obfuscations (angle, euclidean distance, and product), each at the same level of $\varepsilon-\mathrm{DP}(\varepsilon=10)$, computing using Contriever and the DL'19 collection. With no privacy, nDCG@10=0.676.

Discussion. As mentioned in Section 4, WBB can be instantiated using multiple similarity functions to operationalize the exponential mechanism. Furthermore, it relies on two parameters: $k$, the size of the size box, and $n$, the size of the candidate box. Therefore, we propose here an ablation study of such parameters. Figure 6 reports the nDCG values when varying the parameters if Contriever is used both as IRS and re-ranker for DL '19. Similar results can be observed in other scenarios and thus are not reported. A lighter green indicates better performance, while a darker blue corresponds to worse performance. Interestingly, the obfuscation using the Euclidean distance obtains the worst results. Most likely, this form of obfuscation tends to be particularly severe: while we have a decrease in performance, we also have an increase in privacy as supported by the results in Table 1 using $\mathbf{D}$ as a distance function. Therefore, this obfuscation is suggested for particularly privacy-concerned users.

On the contrary, angle obfuscation, based on the cosine similarity between the real and the candidate obfuscation words, is the most effective solution
and is suitable for less privacy-concerned individuals. Finally, the product between the two has an intermediate behaviour suited for intermediate users. For what concerns $k$, the size of the safebox, as a general trend, the bigger it is, the lower the performance. This behaviour is reasonable, considering that we increase privacy by excluding more and more terms similar to the query terms. Finally, the size of the candidate box has a positive impact on the performance. If the candidate box is too small, then the DP mechanism will always sample across the same set of terms. If none of such terms is sufficiently topically related, then the final performance will be low. Viceversa, a larger candidate box ensures the variability of the sampling, further increasing privacy, but also the likelihood that a sufficiently topically related term will be available.

## 6. Related Works

IR and Privacy. The problem of Privacy is not new to IR (Domingo-Ferrer, 2008; Arampatzis et al. 2013a; Ahmad et al., 2018). To address such a problem, the IR community has approached the problem of obfuscating queries in three different ways: i) methods based on dummy queries; ii) methods based on unlinkability; iii) methods based on proxy queries. One aspect of the dummy queries approach (Elovici et al., 2002; Domingo-Ferrer et al., 2009b) involves sending, alongside the original query to the IRS, a collection of unrelated queries that are syntactically similar to the user's one. On the other hand, the unlinkability approach (Domingo-Ferrer et al., 2009a Castellà-Roca et al., 2009) depends on cryptographic and Private Information Retrieval methods to enable a community of users to share queries among themselves, ensuring that each user submits the query of others and the system cannot create a specific user profile. The approach based on proxy queries (Fröbe et al., 2022; Arampatzis et al., 2015) entails the decomposition of the original query into several non-sensitive queries, the combination of which may potentially yield an answer to the user's information need, e.g., the query "throat cancer" is transformed into "neck" and "tumour", reducing the information disclosure of each query. The drawback of dummy and unlinkability approaches is that the original query is sent to the IRS, hence increasing the vulnerability toward machine learning-based attacks (Peddinti and Saxena, 2011, 2014). In addition, the unlinkability approach poses another potential threat to the user: it moves the problem from the IRS to a different user in the federation. These risks do not occur for the approach
based on the proxy queries. However, dummy queries physiologically decrease the effectiveness of the retrieval, which does not occur for the dummy and unlinkability query approaches. Regardless, we argue that the user willingly renounces part of the retrieval capability in favour of achieving strong privacy protection. Therefore, we focus on analyzing obfuscation techniques that rely on proxy queries and exploring various methods, including those based on DP and other approaches.

## 7. Conclusions and Future Works

In this study, we have presented Words Blending Boxes (WBB), a new method based on $\varepsilon$-DP for query obfuscation in IR task. Our approach involves removing the top- $k$ most similar words, depending on a scoring function $m$, and sampling the substitutes from a list of potential $n$ candidates to disguise other possible relevant terms employing the exponential mechanism. In this study, we evaluated various mechanisms in the field of IR by analyzing the Recall and nDCG using two TREC Collections and multiple retrieval models, sparse bag-of-word, i.e., BM25 and TF-IDF, and dense bi-encoders, i.e., Contriever and TAS-B. We compared WBB with other query obfuscation mechanisms, the ones proposed by Arampatzis et al. (2011) and Fröbe et al. (2022), and other mechanisms designed for text obfuscation in NLP based on DP, namely the Calibrated Multivariate Perturbation (CMP) (Feyisetan et al., 2020) and the Mahalanobis (Mhl) (Xu et al., 2020). To assess the tradeoff between privacy and utility of the 20 obfuscated queries generated by the examined mechanisms, we considered different $\varepsilon$ parametrizations. Our findings highlight that the WBB mechanism achieves an average nDCG score of 0.622 on the DeepLearning'19 collection, approaching the score obtained using the original queries. In addition, our study delved into the influence between the parametrization of the WBB mechanism and the values of nDCG. Moreover, the privacy analysis showed that the WBB overcomes the limitation of only formal privacy, ensuring that, even for high privacy budgets $\varepsilon$, the Lexical and Semantic Similarity of the obfuscated queries are always below the ones provided by State-of-the-Art mechanisms. In future directions, we plan to explore how to construct obfuscated queries by studying new scoring functions $m$ and the best parameter selection of the values that identify the top- $k$ and the $n$ candidates. Furthermore, we intend to focus the study on analyzing ways to enhance the effectiveness of obfuscated queries by exploring the perturbation of contextualized dense representations. Finally,
we plan to explore potential inference attacks against the WBB mechanism.

## References

Ahmad, W.U., Chang, K.W., Wang, H., 2018. Intent-aware query obfuscation for privacy protection in personalized web search, in: The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval, Association for Computing Machinery, New York, NY, USA. p. 285-294. URL: https://doi.org/10.1145/3209978.3209983, doi:10.1145/3209978.3209983

Allshouse, W.B., Fitch, M.K., Hampton, K.H., Gesink, D.C., Doherty, I.A., Leone, P.A., Serre, M.L., Miller, W.C., 2010. Geomasking sensitive health data and privacy protection: an evaluation using an e911 database. Geocarto international 25, 443-452.

Arampatzis, A., Drosatos, G., Efraimidis, P.S., 2013a. A versatile tool for privacy-enhanced web search, in: Serdyukov, P., Braslavski, P., Kuznetsov, S.O., Kamps, J., Rüger, S.M., Agichtein, E., Segalovich, I., Yilmaz, E. (Eds.), Advances in Information Retrieval - 35th European Conference on IR Research, ECIR 2013, Moscow, Russia, March 24-27, 2013. Proceedings, Springer. pp. 368-379. URL: https://doi.org/10.1007/ 978-3-642-36973-5_31, doi:10.1007/978-3-642-36973-5\_31.

Arampatzis, A., Drosatos, G., Efraimidis, P.S., 2015. Versatile query scrambling for private web search. Inf. Retr. J. 18, 331-358. URL: https://doi. org/10.1007/s10791-015-9256-0, doi 10.1007/S10791-015-9256-0.

Arampatzis, A., Efraimidis, P.S., Drosatos, G., 2011. Enhancing deniability against query-logs, in: Clough, P.D., Foley, C., Gurrin, C., Jones, G.J.F., Kraaij, W., Lee, H., Murdock, V. (Eds.), Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011, Dublin, Ireland, April 18-21, 2011. Proceedings, Springer. pp. 117128. URL: https://doi.org/10.1007/978-3-642-20161-5_13, doi:10. 1007/978-3-642-20161-5\_13.

Arampatzis, A., Efraimidis, P.S., Drosatos, G., 2013b. A query scrambler for search privacy on the internet. Inf. Retr. 16, 657-679. URL: https://doi. org/10.1007/s10791-012-9212-1, doi 10.1007/S10791-012-9212-1.

Bird, S., Loper, E., 2004. NLTK: The natural language toolkit, in: Proceedings of the ACL Interactive Poster and Demonstration Sessions, Association for Computational Linguistics, Barcelona, Spain. pp. 214-217. URL: https://aclanthology.org/P04-3031.

Carvalho, R.S., Vasiloudis, T., Feyisetan, O., Wang, K., 2023. TEM: high utility metric differential privacy on text, in: Shekhar, S., Zhou, Z., Chiang, Y., Stiglic, G. (Eds.), Proceedings of the 2023 SIAM International Conference on Data Mining, SDM 2023, Minneapolis-St. Paul Twin Cities, MN, USA, April 27-29, 2023, SIAM. pp. 883-890. URL: https://doi.org/10. 1137/1.9781611977653.ch99, doi:10.1137/1.9781611977653.CH99.

Castellà-Roca, J., Viejo, A., Herrera-Joancomartí, J., 2009. Preserving user's privacy in web search engines. Computer Communications 32, 1541-1551. URL: https://www.sciencedirect.com/ science/article/pii/S014036640900125X, doi:https://doi.org/10. 1016/j.comcom.2009.05.009.

Chatzikokolakis, K., Andrés, M.E., Bordenabe, N.E., Palamidessi, C., 2013. Broadening the scope of differential privacy using metrics, in: De Cristofaro, E., Wright, M. (Eds.), Privacy Enhancing Technologies, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 82-102.

Chen, S., Mo, F., Wang, Y., Chen, C., Nie, J.Y., Wang, C., Cui, J., 2023. A customized text sanitization mechanism with differential privacy, in: Rogers, A., Boyd-Graber, J., Okazaki, N. (Eds.), Findings of the Association for Computational Linguistics: ACL 2023, Association for Computational Linguistics, Toronto, Canada. pp. 5747-5758. URL: https: //aclanthology.org/2023.findings-acl.355, doi:10.18653/v1/2023. findings-acl. 355 .

Chen, Y., Perozzi, B., Al-Rfou', R., Skiena, S., 2013. The expressive power of word embeddings. CoRR abs/1301.3226. URL: http://arxiv.org/abs/ 1301.3226, arXiv:1301.3226.

Craswell, N., Mitra, B., Yilmaz, E., Campos, D., Voorhees, E.M., 2020. Overview of the TREC 2019 deep learning track. CoRR abs/2003.07820. URL: https://arxiv.org/abs/2003.07820, arXiv:2003.07820.

Daemen, J., Rijmen, V., 2002. The Design of Rijndael: AES - The Advanced Encryption Standard. Information Security and Cryptography, Springer. URL: https://doi.org/10.1007/978-3-662-04722-4, doi:10.1007/978-3-662-04722-4.

Devlin, J., Chang, M., Lee, K., Toutanova, K., 2019. BERT: pre-training of deep bidirectional transformers for language understanding, in: Burstein, J., Doran, C., Solorio, T. (Eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Association for Computational Linguistics. pp. 4171-4186. URL: https://doi.org/10.18653/v1/n19-1423, doi10.18653/V1/N19-1423.

Domingo-Ferrer, J., 2008. A survey of inference control methods for privacypreserving data mining, in: Aggarwal, C.C., Yu, P.S. (Eds.), PrivacyPreserving Data Mining - Models and Algorithms. Springer. volume 34 of Advances in Database Systems, pp. 53-80. URL: https://doi.org/10. 1007/978-0-387-70992-5_3, doi:10.1007/978-0-387-70992-5\_3.

Domingo-Ferrer, J., Bras-Amorós, M., Wu, Q., Manjón, J., 2009a. User-private information retrieval based on a peer-to-peer community. Data \& Knowledge Engineering 68, 1237-1252. URL: https:// www.sciencedirect.com/science/article/pii/S0169023X09000937,

doi:https://doi.org/10.1016/j.datak.2009.06.004. including Special Section: Conference on Privacy in Statistical Databases (PSD 2008) - Six selected and extended papers on Database Privacy.

Domingo-Ferrer, J., Sánchez, D., Blanco-Justicia, A., 2021. The limits of differential privacy (and its misuse in data release and machine learning). Commun. ACM 64, 33-35. URL: https://doi.org/10.1145/3433638, doi:10.1145/3433638.

Domingo-Ferrer, J., Sánchez, D., Blanco-Justicia, A., 2021. The limits of differential privacy (and its misuse in data release and machine learning). Commun. ACM 64, 33-35. URL: https://doi.org/10.1145/3433638, doi:10.1145/3433638.

Domingo-Ferrer, J., Solanas, A., Castellà-Roca, J., 2009b. H(k)-private information retrieval from privacy-uncooperative queryable databases." ¿h(k)-
private information retrieval from privacy-uncooperative queryable databases. Online Inf. Rev. 33, 720-744. URL: https://api. semanticscholar.org/CorpusID:475785.

Dwork, C., McSherry, F., Nissim, K., Smith, A., 2006. Calibrating noise to sensitivity in private data analysis, in: Halevi, S., Rabin, T. (Eds.), Theory of Cryptography, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 265-284.

Dwork, C., Roth, A., 2014. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci. 9, 211-407. URL: https: //doi.org/10.1561/0400000042, doii10.1561/0400000042.

Elovici, Y., Shapira, B., Maschiach, A., 2002. A new privacy model for web surfing, in: Halevy, A., Gal, A. (Eds.), Next Generation Information Technologies and Systems, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 45-57.

Faggioli, G., Ferro, N., 2024. Query obfuscation for information retrieval through differential privacy, in: Goharian, N., Tonellotto, N., He, Y., Lipani, A., McDonald, G., Macdonald, C., Ounis, I. (Eds.), Advances in Information Retrieval, Springer Nature Switzerland, Cham. pp. 278-294.

Feyisetan, O., Balle, B., Drake, T., Diethe, T., 2020. Privacy- and utilitypreserving textual analysis via calibrated multivariate perturbations, in: Proceedings of the 13th International Conference on Web Search and Data Mining, Association for Computing Machinery, New York, NY, USA. p. 178-186. URL: https://doi.org/10.1145/3336191.3371856, doi:10.1145/3336191.3371856.

Feyisetan, O., Diethe, T., Drake, T., 2019. Leveraging hierarchical representations for preserving privacy and utility in text, in: 2019 IEEE International Conference on Data Mining (ICDM), IEEE Computer Society, Los Alamitos, CA, USA. pp. 210-219. URL: https: //doi.ieeecomputersociety.org/10.1109/ICDM.2019.00031, doi:10. 1109/ICDM. 2019.00031.

Feyisetan, O., Kasiviswanathan, S., 2021. Private release of text embedding vectors, in: Pruksachatkun, Y., Ramakrishna, A., Chang, K.W., Krishna,

S., Dhamala, J., Guha, T., Ren, X. (Eds.), Proceedings of the First Workshop on Trustworthy Natural Language Processing, Association for Computational Linguistics, Online. pp. 15-27. URL: https://aclanthology. org/2021.trustnlp-1.3, doi:10.18653/v1/2021.trustnlp-1.3.

Fröbe, M., Schmidt, E.O., Hagen, M., 2022. Efficient query obfuscation with keyqueries, in: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology, Association for Computing Machinery, New York, NY, USA. p. 154-161. URL: https://doi.org/ 10.1145/3486622.3493950, doi:10.1145/3486622.3493950.

Hampton, K.H., Fitch, M.K., Allshouse, W.B., Doherty, I.A., Gesink, D.C., Leone, P.A., Serre, M.L., Miller, W.C., 2010. Mapping health data: improved privacy protection with donut method geomasking. American journal of epidemiology 172, 1062-1069.

Hofstätter, S., Lin, S., Yang, J., Lin, J., Hanbury, A., 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling, in: Diaz, F., Shah, C., Suel, T., Castells, P., Jones, R., Sakai, T. (Eds.), SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, ACM. pp. 113-122. URL: https://doi.org/10.1145/3404835. 3462891, doi:10.1145/3404835.3462891.

Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., Grave, E., 2022. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res. 2022. URL: https://openreview. net/forum?id=jKN1pXi7b0.

Kumok, A., Fabrikant, A., Dai, A.M., Kamath, C., Stanton, C., Desfontaines, D., Kraft, D., Gabrilovich, E., Flores, G., Wellenius, G.A., Eckstein, I., Perera, I.M., Shafran, I., Davis, J.S., Raman, K., Everett, K., Gadepalli, K.K., Zoghi, M., Sun, M., Huang, R., Bavadekar, S., Roessler, T.L., Ramachandran, V., Mayer, Y., 2020. Google COVID-19 Search Trends Symptoms Dataset: Anonymization Process Description. Technical Report. N/A. URL: https://arxiv.org/abs/2009.01265.

Lavie, A., Agarwal, A., 2007. METEOR: an automatic metric for MT evaluation with high levels of correlation with human judgments, in: CallisonBurch, C., Koehn, P., Fordyce, C.S., Monz, C. (Eds.), Proceedings of the

Second Workshop on Statistical Machine Translation, WMT@ACL 2007, Prague, Czech Republic, June 23, 2007, Association for Computational Linguistics. pp. 228-231. URL: https://aclanthology.org/W07-0734/.

Le, H., Maragh, R., Ekdale, B., High, A., Havens, T., Shafiq, Z., 2019. Measuring political personalization of google news search, in: The World Wide Web Conference, Association for Computing Machinery, New York, NY, USA. p. 2957-2963. URL: https://doi.org/10.1145/3308558.3313682, doi:10.1145/3308558.3313682.

Malandrino, D., Petta, A., Scarano, V., Serra, L., Spinelli, R., Krishnamurthy, B., 2013. Privacy awareness about information leakage: Who knows what about me?, in: Proceedings of the 12th ACM Workshop on Workshop on Privacy in the Electronic Society, Association for Computing Machinery, New York, NY, USA. p. 279-284. URL: https: //doi.org/10.1145/2517840.2517868, doi:10.1145/2517840.2517868.

McSherry, F., Talwar, K., 2007. Mechanism design via differential privacy, in: 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2007), October 20-23, 2007, Providence, RI, USA, Proceedings, IEEE Computer Society. pp. 94-103. URL: https://doi.org/10.1109/ FOCS. 2007.41, doi:10.1109/FOCS. 2007.41.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J., 2013. Distributed representations of words and phrases and their compositionality, in: Burges, C.J.C., Bottou, L., Ghahramani, Z., Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pp. 3111-3119. URL: https://proceedings.neurips.cc/paper/2013/ hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html.

Miller, G.A., 1995. Wordnet: A lexical database for english. Commun. ACM 38, 39-41. URL: https://doi.org/10.1145/219717.219748, doi:10.1145/219717.219748.

Mustafaraj, E., Lurie, E., Devine, C., 2020. The case for voter-centered audits of search engines during political elections, in: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, Association for

Computing Machinery, New York, NY, USA. p. 559-569. URL: https: //doi.org/10.1145/3351095.3372835, doi:10.1145/3351095.3372835.

Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L., 2016. MS MARCO: A human generated machine reading comprehension dataset, in: Besold, T.R., Bordes, A., d'Avila Garcez, A.S., Wayne, G. (Eds.), Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, CEUR-WS.org. URL: https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.

Ono, M., Miwa, M., Sasaki, Y., 2015. Word embedding-based antonym detection using thesauri and distributional information, in: Mihalcea, R., Chai, J., Sarkar, A. (Eds.), Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Denver, Colorado. pp. 984-989. URL: https://aclanthology. org/N15-1100, doi:10.3115/v1/N15-1100.

Papineni, K., Roukos, S., Ward, T., Zhu, W., 2002. Bleu: a method for automatic evaluation of machine translation, in: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, ACL. pp. 311-318. URL: https: //aclanthology.org/P02-1040/, doi:10.3115/1073083.1073135.

Peddinti, S.T., Saxena, N., 2011. On the effectiveness of anonymizing networks for web search privacy, in: Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security, Association for Computing Machinery, New York, NY, USA. p. 483-489. URL: https: //doi.org/10.1145/1966913.1966984, doi:10.1145/1966913.1966984.

Peddinti, S.T., Saxena, N., 2014. Web search query privacy: Evaluating query obfuscation and anonymizing networks. J. Comput. Secur. 22, 155-199. URL: https://doi.org/10.3233/JCS-130491, doi:10.3233/ JCS-130491.

Peng, L., Li, Z., Zhao, H., 2022. Semantics-preserved distortion for personal privacy protection. CoRR abs/2201.00965. URL: https://arxiv.org/ abs/2201.00965, arXiv:2201.00965.

Pennington, J., Socher, R., Manning, C., 2014. GloVe: Global vectors for word representation, in: Moschitti, A., Pang, B., Daelemans, W. (Eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar. pp. 1532-1543. URL: https://aclanthology.org/ D14-1162, doi:10.3115/v1/D14-1162.

Reimers, N., Gurevych, I., 2020. Making monolingual sentence embeddings multilingual using knowledge distillation, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics. URL:https://arxiv.org/abs/2004. 09813 .

Rivest, R.L., Shamir, A., Adleman, L.M., 1978. A method for obtaining digital signatures and public-key cryptosystems. Commun. ACM 21, 120-126. URL: https://doi.org/10.1145/359340.359342, doi 10.1145/359340. 359342 .

Robertson, S.E., Walker, S., Jones, S., Hancock-Beaulieu, M., Gatford, M., 1994. Okapi at TREC-3, in: Harman, D.K. (Ed.), Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, National Institute of Standards and Technology (NIST). pp. 109-126. URL: http://trec.nist.gov/pubs/trec3/ papers/city.ps.gz.

Salton, G., Wong, A., Yang, C., 1975. A vector space model for automatic indexing. Commun. ACM 18, 613-620. URL: https://doi.org/10.1145/ 361219.361220 , doi $10.1145 / 361219.361220$.

Shekhawat, N., Chauhan, A., Muthiah, S.B., 2019. Algorithmic privacy and gender bias issues in google ad settings, in: Proceedings of the 10th ACM Conference on Web Science, Association for Computing Machinery, New York, NY, USA. p. 281-285. URL: https://doi.org/10.1145/3292522. 3326033, doi:10.1145/3292522.3326033.

Sweeney, L., 2002. k-anonymity: A model for protecting privacy. Int. J. Uncertain. Fuzziness Knowl. Based Syst. 10, 557-570. URL: https://doi. org/10.1142/S0218488502001648, doi 10.1142/S0218488502001648.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you need, in: Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008. URL: https://proceedings.neurips.cc/paper/2017/ hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.

Voorhees, E.M., 2004. Overview of the TREC 2004 robust track, in: Voorhees, E.M., Buckland, L.P. (Eds.), Proceedings of the Thirteenth Text REtrieval Conference, TREC 2004, Gaithersburg, Maryland, USA, November 16-19, 2004, National Institute of Standards and Technology (NIST). URL: http://trec.nist.gov/pubs/trec13/papers/ROBUST. OVERVIEW.pdf.

Wagner, I., Eckhoff, D., 2018. Technical privacy metrics: A systematic survey. ACM Comput. Surv. 51. URL: https://doi.org/10.1145/3168389, doi:10.1145/3168389.

Weggenmann, B., Kerschbaum, F., 2018. Syntf: Synthetic and differentially private term frequency vectors for privacy-preserving text mining, in: The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval, Association for Computing Machinery, New York, NY, USA. p. 305-314. URL: https://doi.org/10.1145/ 3209978.3210008 , doi $10.1145 / 3209978.3210008$.

Xu, Z., Aggarwal, A., Feyisetan, O., Teissier, N., 2020. A differentially private text perturbation method using regularized mahalanobis metric, in: Feyisetan, O., Ghanavati, S., Malmasi, S., Thaine, P. (Eds.), Proceedings of the Second Workshop on Privacy in NLP, Association for Computational Linguistics, Online. pp. 7-17. URL: https://aclanthology.org/2020. privatenlp-1.2, doi $10.18653 / v 1 / 2020 . p r i v a t e n l p-1.2$.

Xu, Z., Aggarwal, A., Feyisetan, O., Teissier, N., 2021. On a utilitarian approach to privacy preserving text generation. CoRR abs/2104.11838. doi:10.48550/ARXIV.2104.11838, arXiv:2104.11838.

Yue, X., Du, M., Wang, T., Li, Y., Sun, H., Chow, S.S.M., 2021. Differential privacy for text analytics via natural text sanitization, in: Zong,

C., Xia, F., Li, W., Navigli, R. (Eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, Association for Computational Linguistics. pp. 38533866. URL: https://doi.org/10.18653/v1/2021.findings-acl.337, doi:10.18653/V1/2021.FINDINGS-ACL. 337.

Zhao, Y., Chen, J., 2022. A survey on differential privacy for unstructured data content. ACM Comput. Surv. 54. URL: https://doi.org/10.1145/ 3490237, doi:10.1145/3490237.

Zimmerman, S., Thorpe, A., Chamberlain, J., Kruschwitz, U., 2020. Towards search strategies for better privacy and information, in: Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, Association for Computing Machinery, New York, NY, USA. p. 124-134. URL: https://doi.org/10.1145/3343413.3377958, doi:10. $1145 / 3343413.3377958$.


[^0]:    * Corresponding Author(s).

    Email addresses: defaverifr@dei.unipd.it (Francesco Luigi De Faveri), faggioli@dei.unipd.it (Guglielmo Faggioli), ferro@dei.unipd.it (Nicola Ferro)

    URL: https://www.dei.unipd.it/〜defaverifr/ (Francesco Luigi De Faveri), https://www.dei.unipd.it/〜faggioli/ (Guglielmo Faggioli), https://www.dei.unipd.it/ ferro/ (Nicola Ferro)

[^1]:    ${ }^{1}$ https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

