# BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics 

Hao Wu<br>Tencent, TEG<br>easyluwu@tencent.com

Xingjian Shi<br>Boson AI<br>xshiab@connect.ust.hk

Ziyue Huang<br>Tencent, TEG<br>ziyuehuang@tencent.com

Penghao Zhao<br>Tencent, TEG<br>hymiezhao@tencent.com

Wei Xiong<br>Tsinghua University<br>xiongw21@mails.tsinghua.edu.cn

Jinbao Xue<br>Tencent, TEG<br>jinbaoxue@tencent.com

Yangyu Tao<br>Tencent, TEG<br>brucetao@tencent.com

Xiaomeng Huang<br>Tsinghua University<br>hxm@tsinghua.edu.cn

Weiyan Wang<br>Tencent, TEG<br>neowywang@tencent.com *


#### Abstract

Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems. These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods. Thus, they often generate predictions that are not physically realistic. On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future. Based on this observation, we propose Beam search by Vector Quantization (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models. The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics. To flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes. Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset. Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost of more than $32 \%$ for ten backbones on five datasets, but also significantly enhances physics-aware metrics.


## 1 Introduction

Accurate modeling of complex physical systems is essential for various scientific and engineering domains such as meteorology, climatology, and environmental science. Traditionally, these systems are solved using numerical methods $[15,33,26,9]$, which employ discrete approximation techniques to solve sets of equations derived from physical laws. Although these physics-driven methods ensure[^0]

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-02.jpg?height=437&width=1393&top_left_y=188&top_left_x=363)

Figure 1: Overview of BeamVQ: Input data $\mathcal{X}$ is fed to the encoder $E_{\phi}$ to produce the latent state $\mathcal{Z}_{t}=E_{\phi}(\mathcal{X})$. BeamVQ inserts code bank quantization layers between the encoder $E_{\phi}$ and decoder $D_{\varphi} . K$ candidate latent states are sampled via top- $K$ beam search: $s_{\text {all }}=\left\{s_{1}, s_{2}, \ldots, s_{n}\right\}$. Each state $s_{i}$ is decoded to the output sequence: $\mathcal{Y}_{i}=D_{\varphi}\left(s_{i}\right), \forall i \in[N]$. These candidate outputs are filtered based on the physics-aware score $F\left(\mathcal{Y}_{i}\right)$, in which the high-score samples are added back to the training dataset. BeamVQ iteratively generates new samples with the updated model weights to shift the data distribution to better match the physical law.

compliance with fundamental principles such as conservation laws [17, 16, 29], they require highly trained professionals for development [20], incur high computational costs [28], are less effective when the underlying physics is not fully known [36], and cannot easily improve as more observational data become available [20].

Recently, data-driven deep learning starts to revolutionize the space of space-time forecasting for complex physical systems [7, 47, 21, 37, 34, 28, 48, 1, 20, 51]. Rather than relying on differential equations governed by physical laws, the data-driven approach constructs a model by optimizing statistical metrics such as Mean Squared Error (MSE), using large-scale datasets. These methods [45, $34,44,37,7,47]$ are orders of magnitude faster, and excel in capturing the intricate patterns and distributions present in high-dimensional nonlinear systems [28]. Despite their success, purely data-driven methods fall short in generating physically plausible predictions, leading to unreliable outputs that violate critical physical theorem $[1,28,49]$.

Previous works have tried to combine physics-driven methods and data-driven methods to get the best of both worlds. Some methods try to embed physical constraints in the neural network [24, 8, 3, 10]. For example, PhyDNet [10] adds a physics-inspired PhyCell in the recurrent network. However, such methods require explicit formulation of the physical rules along with specialized designs for network architectures or training algorithms. As a result, they lack flexibility and cannot easily adapt to different backbone architectures. Another type of methods [30, 22, 13], best exemplified by the Physics-Informed Neural Network (PINN) [30], leverages physical equations as additional regularizers in neural network training [13]. Physics-Informed Neural Operator (PINO) [22] extends the data-driven Fourier Neural Operator (FNO) to be physics-informed by adding soft regularizers in the loss function. However, PDE-based regularizers impose optimization challenges [19, 42] that often lead to poor solutions. More recently, PreDiff [6] trains a latent diffusion model for probabilistic forecasting, and guides the model's sampling process with a physics-informed energy function. However, PreDiff requires training a separate knowledge alignment network to integrate the physical constraints, which is not needed in our method.

In this paper, we propose Beam search by Vector Quantization (BeamVQ) [35, 40, 32], a novel framework designed to enhance the physical consistency of space-time forecasting models. BeamVQ is based on the following observation: if we draw a large amount of high quality samples from a probabilistic forecasting model, some samples will be more physically consistent than the others and closer to the real future. Thus, BeamVQ iteratively trains the model on self-generated samples filtered by physics-aware metrics. To ensure our method is flexible to different backbones, we leverage a code bank to discretize the continuous state space, transforming any encoder-decoder-based forecasting model into probabilistic model. To ensure the quality of the generated samples, we extend the nearest neighbor code book lookup to top-K lookup, thereby facilitating beam search sampling. The overall workflow of BeamVQ is illustrated in Figure 1. The design of BeamVQ makes it applicable to a variety of backbone architectures and physical constraints, and does not have optimization challenges as soft-constraint-based methods. Our self-training method is also relevant to the recent progress in aligning large-scale generative models with human preferences through methods like reward-reranked finetuning [4] and iterative preference optimization [50]. However, our goal diverges significantly as
we aim for physical alignment, i.e., refining space-time forecasting model to produce predictions that are physically real, setting it apart from methods focused on aligning with human preferences.

Our experiments demonstrate that BeamVQ consistently boosts performance across 10 different spacetime forecasting models, achieving state-of-the-art results on 5 benchmarks. Moreover BeamVQ improves both statistical metrics and physics-aware metrics, and is adaptable to long-term forecasting scenarios. This makes BeamVQ a powerful tool to integrate physical knowledge into data-driven deep learning methods, opening new avenues for research and application in fields such as aerospace, biomedical engineering, and meteorology.

## 2 Preliminaries

We first discuss our problem setting and then introduce the background of physics-aware metric.

### 2.1 Space-Time Forecasting

Our objective is to predict the future of a space-time system with historical data. We represent the input as a four-dimensional tensor $\mathcal{X} \in \mathbb{R}^{T \times C \times H \times W}$, where $T$ denotes the number of time steps, $C$ is the number of physical observations, and $H$ and $W$ are spatial dimensions. The output $\mathcal{Y}$ is a tensor $\mathcal{Y} \in \mathbb{R}^{T^{\prime} \times C^{\prime} \times H^{\prime} \times W^{\prime}}$ that represents the future observation. We employ a probabilistic model $\mathcal{P}(\mathcal{Y} \mid \mathcal{X} ; \theta)$, where $\theta$ represents the model parameters, to generate possible future sequences $\left\{\mathcal{Y}_{1}, \mathcal{Y}_{2}, \ldots, \mathcal{Y}_{T^{\prime}}\right\}$. Here, $\mathcal{Y}_{i} \in \mathbb{R}^{C^{\prime} \times H^{\prime} \times W^{\prime}}$ depicts the output at the $i$-th prediction step.

### 2.2 Physics-aware Metrics

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-03.jpg?height=382&width=1401&top_left_y=1267&top_left_x=362)

Figure 2: I. The top row shows the actual and predicted distributions of height and speed with a white box highlighting significant discrepancies. Dotted patterns reflect the direction and magnitude of velocity vectors. II. The second row shows the average energy spectrum, depicting energy distribution across wavenumbers (k) and highlighting fluctuations in the dynamic system over different scales. Although CNO and FNO achieve similar MSE, the energy spectrum of CNO is closer to the groundtruth and is thus more physically realistic.

Although MSE has been widely used as the loss function in training and evaluation, it can hardly capture the physical inconsistency. Figure 2 gives an example. In the figure, both CNO [31] and FNO [21] have low MSE and are perceptually similar with the Ground-truth $\left(7.49 \times 10^{-4}\right.$ and $7.96 \times 10^{-4}$ respectively), but CNO has much more realistic energy specturm than FNO. This emphasizes the importance of evaluating models based not only on statistical metrics but also on physical consistency. Statistical metrics like MSE overlook a model's compliance with physical laws, leading to unreliable predictions even when the metric is low. Therefore, it is crucial to establish physics-aware metrics alongside statistical metrics to ensure more robust model performance.

In this paper, we consider three different physics-aware metrics for dynamic system applications. These metrics collectively describe turbulence characteristics, assess fluid behavior and kinetic energy distribution in the physical field. Following are the details of these physics-aware metrics:

Divergence of the Velocity Field [39]: In the context of fluid dynamics, particularly in the study of turbulence, an incompressible flow implies that the divergence of the velocity field must be zero. This constraint ensures mass conservation within the flow. Mathematically, the divergence of a velocity field $\mathbf{w}$ at a point $\mathbf{x}$ is given by $\nabla \cdot \mathbf{w}(\mathbf{x})$. For a discrete representation, the average divergence across
the entire field can be calculated as follows:

$$
\begin{equation*}
\text { Divergence }=\frac{1}{M} \sum_{j=1}^{M}\left|\nabla \cdot \mathbf{w}_{j}\right| \tag{1}
\end{equation*}
$$

where $M$ is the total number of points in the prediction step, and $\mathbf{w}_{j}$ is the velocity vector at point $j$.

Turbulence Kinetic Energy (TKE) [25]: In fluid dynamics, turbulence kinetic energy represents the average kinetic energy per unit mass of eddies in turbulent flow. Physically, it is defined by the root mean square of measured velocity fluctuations:

$$
\begin{equation*}
\left(\overline{\left(u^{\prime}\right)^{2}}+\overline{\left(v^{\prime}\right)^{2}}\right) / 2, \quad \overline{\left(u^{\prime}\right)^{2}}=\frac{1}{T} \sum_{t=0}^{T}(u(t)-\bar{u})^{2} \tag{2}
\end{equation*}
$$

and $t$ is the time step. We calculate the turbulence kinetic energy for predicted velocity fields.

Energy Spectrum [12]: The turbulence energy spectrum, denoted as $E(k)$, describes how the kinetic energy of turbulence is distributed among various scales of motion, corresponding to different sizes of vortices or eddies. It is defined by the following integral:

$$
\begin{equation*}
\int_{0}^{\infty} E(k) d k=\frac{1}{2}\left(\overline{\left(u^{\prime}\right)^{2}}+\overline{\left(v^{\prime}\right)^{2}}\right) \tag{3}
\end{equation*}
$$

where $k$ represents the wavenumber, indicating spatial frequency in the Fourier transform domain. This integral assessment helps to understand the energy ratio between large and small eddies, which is crucial for characterizing the turbulence's behavior.

## 3 Methodology

### 3.1 Framework Overview

Generally, BeamVQ iteratively applies three steps: beam search, updating the dataset by adding physically-plausible samples, and self-training. The detailed process is described as follows: (1) Beam search: At each searching step, the probabilistic model generates a set of candidate states and assigns likelihood scores. The search space is pruned by only keeping the top-k states with high physics-aware scores at each step. (2) Dataset update: To maintain physical consistency, BeamVQ employs physics-aware metrics (e.g., velocity field divergence, turbulence kinetic energy, and energy spectrum) to evaluate all output sequences from beam search. Output sequences with top physics-aware scores are added to the training dataset. This step shifts the training data distribution to better match physical laws. (3) Self-training: BeamVQ updates the model with the latest training dataset for better alignment with physics-aware scores.

This iterative process continuously optimizes the model's predictive capabilities as well as the alignment with physics-aware metrics. Figure 3 illustrates how all three steps are interconnected, forming a closed-loop optimization process that gradually enhances the space-time forecasting performance.

In every step of beam search, the input sequence $\mathcal{X}$ passes through the encoder $E_{\phi}$ to generate latent states. Through sampling different entries in the code bank, the probabilistic model $P_{\theta_{t}}$ generates candidate states $s_{i}$ to replace the original one. The decoder $D_{\varphi}$ decodes these candidates into predictions, which are selected based on the physics-aware metric $F$ for the next level of the beam search tree. In the final level, Beam search $B$ selects the highest-scored sequence $\mathcal{Y}^{*}$ as the final output, namely $\mathcal{Y}^{*}=B\left(\left\{F\left(D_{\varphi}\left(s_{i}\right)\right) \mid s_{i} \in P_{\theta_{t}}\left(E_{\phi}(\mathcal{X})\right)\right\}\right)$. And BeamVQ collects all output sequences higher than a threshold to iteratively expand the training dataset $D_{t}$. Formally speaking, we have $D_{t+1}=D_{t} \cup D_{\text {high. }}$. Then BeamVQ continues to update the model as $\theta_{t+1} \leftarrow \mathcal{U}\left(\theta_{t}, \mathcal{L}\left(D_{t+1}\right)\right)$, where $D_{t+1}$ is the updated dataset and $L$ denotes the statistical loss function. In the following subsections, we introduce all the details about BeamVQ.

### 3.2 Probabilistic Model and Beam Search

Inspired by the success of beam search in discrete token sequence generation [35, 2, 11] (e.g., natural language, musical notes, and chemical formula), we propose a general method to convert the existing deterministic model into a probabilistic model to sample different frame states in continuous space.

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-05.jpg?height=353&width=1393&top_left_y=192&top_left_x=366)

Figure 3: The closed-loop of BeamVQ: (1) Beam search. it generates different output sequences for each input, by sampling different continuous states with the probabilistic model. (2) Dataset Update: it filters out the output sequences with high physics-aware Scores to expand the training dataset. (3) Self-training: it trains on the shifted data distribution for better physical consistency.

Encoder: The encoder $\mathcal{Z}_{t}=E_{\phi}(\mathcal{X})$ computes latent vectors from raw observation data in historical inputs. As a general method, BeamVQ can employ any popular backbone networks as the encoder, such as ViT [5], Earthfarseer [47], SimVP [37], FNO [21], and CNO [31]. The encoder takes high-dimensional data of physical systems $\mathcal{X}_{t} \in \mathbb{R}^{C \times H \times W}$ and maps it to the latent vector space $\mathcal{Z}_{t} \in \mathbb{R}^{l \times D}$ through a series of transformations, where $l$ is the number of tokens and $d$ is the dimension of each token. Next, $\mathcal{Z}_{t}$ is mapped to a new vector space $\mathcal{Z}_{t}^{\prime} \in \mathbb{R}^{l \times d}$ by dimension reduction to match the dimension of code bank. This transformation process can be described as:

$$
\begin{equation*}
\mathcal{Z}_{t}=E_{\phi}\left(\mathcal{X}_{t}\right) \quad, \quad \mathcal{Z}_{t}^{\prime}=\sigma\left(W \cdot \mathcal{Z}_{t}+b\right) \tag{4}
\end{equation*}
$$

where $\mathcal{Z}_{t}^{\prime}$ is the projection of $\mathcal{Z}_{t}$ in the low dimension, $W$ and $b$ are the projection matrix and bias, and $\sigma$ is the activation function.

Beam Search Sampling $\boldsymbol{\&}$ Decoder: Unlike previous works that make deterministic predictions, we follow VQVAE to plug in a code bank to build a probabilistic model. Furthermore, we enhance it by substituting nearest-neighbor search with Top-K calculations for beam search. This improvement allows the model to generate multiple potential states and build a beam search tree whose node stands for a candidate state(i.e., a time frame) as shown in Figure 3(1).

As shown in Figure 4, traditional VQVAE models select the encoding vector with the smallest Euclidean distance to the input vector in the quantization step. However, BeamVQ modifies this approach by choosing the Top-K closest encoding vectors. Following the simplified notation in VQVAE [40, 32], we have Equation 5 to show how to select the Top-K vectors $\left\{e_{k_{1}}, e_{k_{2}}, \ldots, e_{k_{K}}\right\}$ close to the current state $\mathcal{Z}_{t}^{\prime}$ for the candidate states in the next tree level.

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-05.jpg?height=314&width=553&top_left_y=1388&top_left_x=1122)

Figure 4: Visualisation of the embedding space.

$$
\begin{equation*}
\left\{\mathcal{Z}_{t+1}^{k_{1}}, \ldots, \mathcal{Z}_{t+1}^{k_{K}}\right\}=\left\{e_{k_{1}}, \ldots, e_{k_{K}}\right\}, \quad \text { where }\left\{k_{1}, \ldots, k_{K}\right\}=\text { top-K }{ }_{j}\left\|\mathcal{Z}_{t}^{\prime}-e_{j}\right\|^{2} \tag{5}
\end{equation*}
$$

where $\mathcal{Z}_{t+1}^{k_{i}}$ is the latent representation of the $k_{i}$-th candidate state, top- $\mathrm{K}_{j}\left\|\mathcal{Z}_{t}^{\prime}-e_{j}\right\|^{2}$ identifies the indices $\left\{k_{1}, k_{2}, \ldots, k_{K}\right\}$ of top $\mathrm{K}$ nearest vectors.

For the top- $\mathrm{K}$ beam search, we always keep the active node number as $\mathrm{K}$ in all levels of the beam search tree(except the initial one). Given the $\mathrm{K}$ state nodes in any level in the tree, we sample different $\mathrm{K}$ candidate states in the next level for each node. To output the next frame for all $K^{2}$ candidate sequences, we feed these states into the decoder $D_{\varphi}$ that can be any popular one (e.g., the deconvolution network). We evaluate the physics-aware scores of all candidate sequences and only leave the top-K high ones to be active in the future search. For any time step $t$, BeamVQ consider all $K$ active frames $\mathcal{Y}_{t}^{k_{p}}$ to generate and filter out the new $K$ active candidate frames $\mathcal{Y}_{t+1}^{k_{q}}$ as:

$$
\begin{equation*}
\mathcal{Y}_{t+1}^{k_{q}}=\operatorname{BeamVQ}\left(\mathcal{Y}_{t}^{k_{p}}\right), \quad \text { for } p, q \in\{1, \ldots, K\} \tag{6}
\end{equation*}
$$

BeamVQ repeatedly apply the same process to keep $K$ active candidate sequences until it reaches the maximum tree depth $T^{\prime}$. We consider all paths from the initial level and the final level in the searching tree to generate predicted physical field sequence candidates $\left\{\mathcal{Y}^{1}, \mathcal{Y}^{2}, \ldots, \mathcal{Y}^{\mathcal{K}}\right\}$. Finally, we select the candidate sequence $\mathcal{Y}^{*}$ with the highest physics-aware score from the beam search as the inference output.

Tradeoff for Long-term Forecasting: Since beam search brings in extra costs, we can predict multiple time frames together in one step as the tradeoff for long-term forecasting. If the beam size is $K$, the original strategy samples $K$ states for each candidate sequence in every step, and it only continues to extend the top-K high-quality sequences in the next step. The total number of computed states is $S_{\text {original }}=n K^{2}$. Assuming that $K=5$ and $n=100$ in long-term forecasting, the number of computed states is about 2500 . The optimized strategy combines frames (e.g., 10 frames) and predicts them in parallel, reducing the total number of computed states to be 250 . Namely, it saves about $90 \%$ computations and greatly improves the efficiency of long-term prediction. Our experiments in Section 4.4 show it still achieves high prediction quality despite the tradeoff for efficiency.

### 3.3 Iterative Self-Training

To better align with physics-aware scores, we employ an iterative self-training strategy to train BeamVQ. Although it still uses the original statistic loss during the training, BeamVQ filers out its output sequences with high physics-aware scores to iteratively shift data distribution to better match Physical law.

Training Loss: To train the whole model, we minimize the MSE error on the whole training dataset. Additionally, to optimize the code bank using standard gradient descent, we incorporate the stop gradient operator $\mathbf{s g}($ ). This operator works a marker during network forward propagation and blocks gradient calculation during backpropagation. Thus, we can express the loss objective as follows:

$$
\begin{equation*}
\mathcal{L}=\lambda\left(\frac{1}{T I} \sum_{t=1}^{T} \sum_{i=1}^{I}\left(\mathcal{Y}_{t, i}^{*}-\mathcal{Y}_{t, i}\right)^{2}\right)+\beta\left\|\mathcal{Z}_{t}^{\prime}-\mathbf{s g}[e]\right\|_{2}^{2}+\gamma\left\|\mathbf{s g}\left[\mathcal{Z}_{t}^{\prime}\right]-e\right\|_{2}^{2} \tag{7}
\end{equation*}
$$

The loss function includes three weight parameters $\lambda, \beta, \gamma$ to adjust the contribution of different components to the total loss.

Training Details: At the beginning of training $(t<\mathcal{E})$, the model is trained using only the original training dataset $\mathcal{D}$, with the update formula:

$$
\begin{equation*}
\theta^{(t+1)}=\theta^{(t)}-\eta \nabla_{\theta} \mathcal{L}\left(\mathcal{X}, \mathcal{Y} ; \theta^{(t)}\right) \tag{8}
\end{equation*}
$$

where $\theta^{(t)}$ are the $t$ th cycle parameters, $\mathcal{L}$ is the loss function, and $\eta$ is the learning rate.

Starting from the $\mathcal{E}$ th epoch, BeamVQ first conducts inference on the all input sequences $\left\{\mathcal{X}^{1}, \mathcal{X}^{2}, \ldots, \mathcal{X}^{N}\right\}$ to generate their top-K output candidates with beam search. Then we evaluate all outputs with physics-aware metrics and add the highest ones and the ones above the threshold to the self-training dataset $\mathcal{D}_{\text {high }}$ :

$$
\begin{equation*}
\mathcal{D}_{\text {high }}^{(t+1)}=\left\{\mathcal{X}, \mathcal{Y}^{i} \mid \mathcal{Y}^{i}=\mathcal{Y}^{*} \text { or } F\left(\mathcal{Y}^{i}\right) \geq \text { threshold, } \mathcal{Y}^{i} \in \mathcal{P}\left(\mathcal{Y} \mid \mathcal{X} ; \theta^{(t)}\right)\right\} \cup \mathcal{D}_{\text {high }}^{(t)} \tag{9}
\end{equation*}
$$

This process ensures that the model can continuously learn from high-quality data that conforms to the laws of physics while gradually increasing the frequency of self-training, thus improving its predictions' accuracy and physical consistency. It is worth emphasizing that once $\mathcal{X}_{\text {high }}$ and $\mathcal{Y}_{\text {high }}$ are selected, they are always included in the training set for subsequent model self-training.

Then in every self-training epoch, self-training data $\mathcal{D}_{\text {high }}^{t+1}=\left(\mathcal{X}_{\text {high }}^{t+1}, \mathcal{Y}_{\text {high }}^{t+1}\right)$ is introduced, which contains iteratively added high-quality predictive data. The model update formula is adjusted to:

$$
\begin{equation*}
\theta^{(t+1)}=\theta^{(t)}-\eta\left(\nabla_{\theta} \mathcal{L}\left(\mathcal{X} \cup \mathcal{X}_{\text {high }}^{t+1}, \mathcal{Y} \cup \mathcal{Y}_{\text {high }}^{t+1} ; \theta^{(t)}\right)\right. \tag{10}
\end{equation*}
$$

## 4 Experiment

In this section, we conduct experiments to assess the effectiveness of our method. We cover 5 benchmarks and 10 backbone models. The experiments aim to investigate the following research questions: RQ1. Can BeamVQ enhance the performance of the baselines? RQ2. Can BeamVQ have better physical alignment? RQ3. Can BeamVQ produce long-term forecasting?

### 4.1 Experimental Settings

Benchmarks \& Backbones. Our dataset spans multiple spatiotemporal dynamical systems, summarized as follows: $\cdot$ Real-world Datasets, including SEVIR [41]; $\cdot$ Equation-driven Datasets, focus-

Table 1: Performance comparison of various models with and without the BeamVQ method across five benchmark tests (SWE(u), RBC, NSE, Prometheus, SEVIR), using MSE as the evaluation metric. We bold-case the entries with lower MSE. "Improvement" represents the average percentage improvement in MSE achieved with BeamVQ.

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-07.jpg?height=778&width=1401&top_left_y=446&top_left_x=362)

Figure 5: This bar chart shows the improvement percentage of different backbones across multiple benchmarks (SWE (u), RBC, NSE, Prometheus, SEVIR). Each color represents a benchmark. The horizontal axis lists the models, and the vertical axis shows the improvement percentages.

ing on PDE [36] (Navier-Stokes equations, Shallow-Water Equations) and Rayleigh-Bénard convection flow [43]; (3) Computational Fluid Dynamics Simulation Datasets, namely Prometheus [46]. We select core models from three different fields for analysis. Specifically: - Spatio-temporal Predictive Learning, we choose ResNet [14], ConvLSTM [34], Earthformer [7], SimVP-v2 [37], TAU [38], and Earthfarseer [47] as representative models; $\cdot$ Neural Operator, we compare models like FNO [21], NMO [49] and CNO [31]; $\cdot$ Large Scientific Model, we study FourcastNet [28].

Implementation details. Our method trains with MSE loss, uses the ADAM optimizer [18], and sets the learning rate to $10^{-3}$. We set the batch size to 100 . The training process early stops within 500 epochs. Additionally, we set our code bank size as $1024 \times 64$, beam size $K$ as 5 or 10 , and the threshold as the first quartile of all candidate's scores, which we find suitable for all backbones. We implement all experiments in PyTorch [27] on 16 NVIDIA A100-PCIE-40GB GPUs.

### 4.2 Assessing The Efficacy Of BeamVQ (RQ1)

In this study, we evaluate the performance of various backbones with and without the BeamVQ method. We choose 5 benchmarks (SWE(u), RBC, NSE, Prometheus, SEVIR) and use MSE as the evaluation metric. Table 1 and Figure 5 summarize these results. Table 1 shows the performance of different backbones on the five benchmarks. It is evident that all backbones achieve significantly lower MSE with the BeamVQ method. For example, the ResNet model's error in the SWE(u) benchmark decreases from 0.0076 to 0.0033 , an improvement of $56.58 \%$. The ConvLSTM model's error in the RBC benchmark drops from 0.2726 to 0.0868 , an improvement of $68.15 \%$. The Earthformer model's error in the NSE benchmark falls from 1.8720 to 0.1202 , an improvement of $93.58 \%$. The overall improvement percentages for each benchmark are: SWE(u) $39.08 \%$, RBC $18.97 \%$, NSE $35.83 \%$, Prometheus $33.65 \%$, and SEVIR $35.27 \%$. These significant improvements demonstrate the effectiveness of the BeamVQ method in enhancing the physical consistency and prediction accuracy of spatiotemporal forecasting backbones. Figure 5 further illustrates the improvement percentages of different backbones across the benchmarks. It shows that the BeamVQ method significantly enhances model performance across all benchmarks.
![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-08.jpg?height=564&width=1392&top_left_y=240&top_left_x=366)

(b) Time steps

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-08.jpg?height=285&width=584&top_left_y=516&top_left_x=1161)

Figure 6: The BeamVQ plugin improves physical consistency and prediction accuracy. (a) shows a visual comparison of the actual target, predicted results, and errors at different time steps. (b) displays the changes in SSIM, RMSE, and relative L2 error over time steps. (c) compares the turbulent TKE. (d) presents the energy spectrum at different wavenumbers.

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-08.jpg?height=349&width=1390&top_left_y=991&top_left_x=365)

Figure 7: I. 3D visualization of the $\mathrm{SWE}(\mathrm{h})$, showing Ground-truth, SimVP-V2+BeamVQ predictions, and Error at $\mathrm{T}=1,10,20,30,40,50$. The first row shows Ground-truth, the second SimVPV2+BeamVQ predictions, and the third Error. II. A case study. Building fire simulation with ventilation settings added to Wu's Prometheus [46]. (a) Layout and HRR growth. (b) Comparison of physical metrics for different methods. (c) Ground-truth, ResNet+BeamVQ, and ResNet predictions.

### 4.3 BeamVQ Boosts Physical Alignment (RQ2)

Figure 6 shows BeamVQ significantly improves physical consistency and prediction accuracy. In Figure 6(a), the visualization of the actual target, predicted results, and error at different time steps shows that the method with BeamVQ performs better in details and physical consistency, with smaller errors. Figure 6(b) presents performance metrics of SSIM, RMSE, and relative L2 error. The BeamVQ method improves these metrics by $23.40 \%$, reduces them by $37.07 \%$, and $45.46 \%$,

Table 2: We compare different backbones on the SWE Benchmark for Long-term Forecasting.

| MODEL |  | SWE (U) | SWE (V) | SWE (H) |
| :---: | :---: | :---: | :---: | :---: | :---: |
| SIMVP-V2 | ORI | 0.0187 | 0.0387 | 0.0443 |
|  | +BEAMVQ | $\mathbf{0 . 0 1 5 4}$ | $\mathbf{0 . 0 3 4 2}$ | $\mathbf{0 . 0 3 9 7}$ |
| CONVLSTM | ORI | 0.0487 | 0.0673 | 0.0762 |
|  | +BEAMVQ | $\mathbf{0 . 0 3 2 1}$ | $\mathbf{0 . 0 3 5 1}$ | $\mathbf{0 . 0 4 3 2}$ |
| FNO | ORI | 0.0571 | 0.0832 | 0.0981 |
|  | +BEAMVQ | $\mathbf{0 . 0 5 0 2}$ | $\mathbf{0 . 0 6 5 3}$ | $\mathbf{0 . 0 9 1 1}$ |
| CNO | ORI | 0.1283 | 0.1422 | 0.1987 |
|  | +BEAMVQ | $\mathbf{0 . 0 6 2 1}$ | $\mathbf{0 . 0 6 7 4}$ | $\mathbf{0 . 0 9 6 5}$ |

respectively, indicating stronger robustness in spatiotemporal dynamical system prediction. Figure 6(c) compares turbulent kinetic energy (TKE). The BeamVQ method captures changes in TKE more accurately, especially in details and small-scale turbulent structures. Figure 6(d) shows the energy spectrum at different wavenumbers. The BeamVQ method demonstrates better physical consistency in the high-wavenumber region, meaning more accurate predictions of small-scale vortices. Overall, the BeamVQ framework not only enhances numerical accuracy in predictions but also captures the essence of physical phenomena better, showing significant advantages.

### 4.4 BeamVQ Excels In Long-term Dynamic System Forecasting (RQ3)

In the long-term forecasting experiments, we compare the performance of different backbone models on the SWE benchmark, evaluating the relative L2 error for three variables ( $\mathrm{U}, \mathrm{V}$, and $\mathrm{H}$ ). Our setup inputs 5 frames and predicts 50 frames. For the SimVP-v2 model, using BeamVQ reduces the relative

L2 error for SWE (u) from 0.0187 to 0.0154 , SWE (v) from 0.0387 to 0.0342 , and SWE (h) from 0.0443 to 0.0397. We visualize SWE (h) in 3D as shown in Figure 7 [I]. For the ConvLSTM model, applying BeamVQ reduces the relative L2 error for SWE (u) from 0.0487 to 0.0321, SWE (v) from 0.0673 to 0.0351 , and SWE (h) from 0.0762 to 0.0432 . For the FNO model, using BeamVQ reduces the relative $\mathrm{L} 2$ error for SWE (u) from 0.0571 to 0.0502 , SWE (v) from 0.0832 to 0.0653 , and SWE (h) from 0.0981 to 0.0911 . Overall, BeamVQ significantly improves the long-term forecasting accuracy of different backbone models.

### 4.5 Interpretation Analysis \& Ablation Study

Qualitative analysis using t-SNE. Figure 8 shows the t-SNE visualizations of the RBC dataset, including (a) ground truth labels, (b) ConvLSTM predictions, and (c) ConvLSTM+BeamVQ predictions. In (a), the ground truth labels display clear clusters, serving as a benchmark for evaluating model predictions. In (b), the ConvLSTM predictions show a more blurred clustering pattern with significant overlap, indicating that ConvLSTM struggles to capture the data structure accurately. In (c), the ConvLSTM+BeamVQ predictions exhibit clearer clusters that are closer to the ground truth, demonstrating that BeamVQ significantly enhances the ConvLSTM's predictive capability. Overall, the t-SNE results indicate that BeamVQ effectively improves the prediction accuracy and physical consistency of ConvLSTM on the RBC dataset, further validating BeamVQ's effectiveness.

Analysis on Code bank. We train FNO+BeamVQ on NSE for 100 epochs with a learning rate of 0.001 and a batch size of 100. The results are shown in the Figure 8. In the VQVAE codebank dimension experiment, we find that increasing the number of vectors $L$ significantly reduces MSE. Especially, when $L=1024$ and $D=64$, the MSE drops to the lowest value of 0.1271 . Although MSE fluctuates more with $L=256$ and $L=512$, overall, increasing $L$ helps improve model accuracy. The training loss curves show that most combinations quickly decrease and stabilize within the first 20 epochs. Notably, the combination of $L=512$ and $D=128$ shows higher stability during training. In conclusion, the combination of $L=1024$ and $D=64$ is best for reducing MSE, while $L=512$ and $D=128$ perform well in training stability.

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-09.jpg?height=296&width=307&top_left_y=1348&top_left_x=367)

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-09.jpg?height=292&width=312&top_left_y=1350&top_left_x=690)

![](https://cdn.mathpix.com/cropped/2024_05_29_7d1133e6671d965b758cg-09.jpg?height=296&width=423&top_left_y=1348&top_left_x=1317)

Figure 8: The t-SNE visualization in (a), (b), and (c) shows the Ground-truth, ConvLSTM and ConvLSTM+BeamVQ predictions, respectively. (d) shows the analysis of the Codebank parameters.

Ablation Study. We use NSE as a benchmark and conduct ablation experiments with FNO. The variants are: (I) FNO: The base model. (II) FNO+BeamVQ: Adds the BeamVQ module to FNO. (III) FNO+BeamVQ (w/o Beamsearch): Adds the BeamVQ module to FNO without Beamsearch. (IV) FNO+BeamVQ (w/o self-Training): Adds the BeamVQ module to FNO without self-training. (V) FNO+BeamVQ (w MSE): Uses MSE metrics instead of physical metrics for filtering. (VI) FNO+VQVAE: Uses VQVAE for discretization on top of FNO. (VII) FNO+PINO [23]: Adds the Physics-Informed Neural Operator (PINO) to FNO, integrating physical constraints. The results in Table 3 show that the base model FNO has an MSE of 0.2237 and a TKE relative error of 0.3964. Adding the BeamVQ module reduces the MSE to 0.1005 and the TKE relative error to 0.1572 . Removing the Beamsearch strategy and self-training decreases performance but still improves over the base model, with MSEs of 0.1207 and 0.1118 and TKE relative errors of 0.2003 and 0.1872 , respectively. Using VQVAE and PINO for optimization results in MSEs of 0.1872 and 0.1249 and TKE relative errors of 0.3652 and 0.2342 . These results indicate that the BeamVQ module significantly improves the model's physical consistency and prediction accuracy.
Table 3: Ablation studies on the NSE benchmark.

| VARIANTS | MSE | TKE |
| :--- | :---: | :---: |
| FNO | 0.2237 | 0.3964 |
| FNO+BEAMVQ | $\mathbf{0 . 1 0 0 5}$ | $\mathbf{0 . 1 5 7 2}$ |
| FNO+BEAMVQ (w/o BEAMS) | 0.1207 | 0.2003 |
| FNO+BEAMVQ (w/O SELFT) | 0.1118 | 0.1872 |
| FNO+BEAMVQ (w. MSE) | 0.1654 | 0.2847 |
| FNO+VQVAE | 0.1872 | 0.3652 |
| FNO+PINO | 0.1249 | 0.2342 |

## 5 Conclusion and Future Work

We propose BeamVQ, a novel probabilistic model to align space-time forecasting with Physicsaware metrics through self-training. BeamVQ uses a code bank to discretize continuous state space, converting any encoder-decoder model into a probabilistic model for beam search. It filters high-quality sequences based on Physics-aware scores to expand the dataset for self-training. As a general method, BeamVQ can work with different backbone networks, evaluation metrics, and Physical applications. Our experiments show that BeamVQ consistently improves prediction quality and physical consistency, even with weak backbone models. In the future, we can explore to combine with Physical regularization loss, consider more large and complex datasets and use more non-differentiable and differentiable metrics.

## References

[1] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533-538, 2023.

[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. High-dimensional sequence transduction. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 3178-3182. IEEE, 2013.

[3] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020.

[4] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.

[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.

[6] Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, and Yuyang Bernie Wang. Prediff: Precipitation nowcasting with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2023.

[7] Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Yuyang Bernie Wang, Mu Li, and Dit-Yan Yeung. Earthformer: Exploring space-time transformers for earth system forecasting. Advances in Neural Information Processing Systems, 35:25390-25403, 2022.

[8] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Advances in neural information processing systems, 32, 2019.

[9] Michael Griebel, Thomas Dornseifer, and Tilman Neunhoeffer. Numerical simulation in fluid dynamics: a practical introduction. SIAM, 1998.

[10] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11474-11484, 2020.

[11] Jeff Guo and Philippe Schwaller. Beam enumeration: Probabilistic explainability for sample efficient self-conditioned molecular design. ICLR, 2024.

[12] Martin C Gutzwiller. Energy spectrum according to classical mechanics. Journal of Mathematical Physics, 11(6):1791-1806, 1970.

[13] Derek Hansen, Danielle C Maddix, Shima Alizadeh, Gaurav Gupta, and Michael W Mahoney. Learning physical models that can respect conservation laws. In International Conference on Machine Learning, pages 12469-12510. PMLR, 2023.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.

[15] Guillaume Jouvet, Matthias Huss, Heinz Blatter, Marco Picasso, and Jacques Rappaz. Numerical simulation of rhonegletscher from 1874 to 2100. Journal of Computational Physics, 228(17):6426-6439, 2009.

[16] Dean C Karnopp, Donald L Margolis, and Ronald C Rosenberg. System dynamics: modeling, simulation, and control of mechatronic systems. John Wiley \& Sons, 2012.

[17] Anuj Karpatne, Gowtham Atluri, James H Faghmous, Michael Steinbach, Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, and Vipin Kumar. Theory-guided data science: A new paradigm for scientific discovery from data. IEEE Transactions on knowledge and data engineering, 29(10):2318-2331, 2017.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[19] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34:26548-26560, 2021.

[20] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Graphcast: Learning skillful medium-range global weather forecasting. arXiv preprint arXiv:2212.12794, 2022.

[21] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.

[22] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. ACM/JMS Journal of Data Science, 2021.

[23] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. ACM / IMS J. Data Sci., feb 2024. Just Accepted.

[24] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-net: Learning PDEs from data. In International conference on machine learning, pages 3208-3216. PMLR, 2018.

[25] Kouji Nagata, Yasuhiko Sakai, Takuto Inaba, Hiroki Suzuki, Osamu Terashima, and Hiroyuki Suzuki. Turbulence structure and turbulence kinetic energy transport in multiscale/fractalgenerated turbulence. Physics of Fluids, 25(6), 2013.

[26] Steven A Orszag and Moshe Israeli. Numerical simulation of viscous incompressible flows. Annual Review of Fluid Mechanics, 6(1):281-318, 1974.

[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019 .

[28] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.

[29] Jay T Pukrushpan, Anna G Stefanopoulou, and Huei Peng. Control of fuel cell power systems: principles, modeling, analysis and feedback design. Springer Science \& Business Media, 2004.

[30] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686-707, 2019.

[31] Bogdan Raonic, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, Rima Alaifari, Siddhartha Mishra, and Emmanuel de Bézenac. Convolutional neural operators for robust and accurate learning of pdes. Advances in Neural Information Processing Systems, 36, 2023.

[32] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019.

[33] Robert S Rogallo and Parviz Moin. Numerical simulation of turbulent flows. Annual review of fluid mechanics, 16(1):99-137, 1984.

[34] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional Istm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems, 28, 2015.

[35] Volker Steinbiss, Bach-Hiep Tran, and Hermann Ney. Improvements in beam search. In ICSLP, volume 94, pages 2143-2146, 1994.

[36] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pflüger, and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. Advances in Neural Information Processing Systems, 35:1596-1611, 2022.

[37] Cheng Tan, Zhangyang Gao, Siyuan Li, and Stan Z Li. Simvp: Towards simple yet powerful spatiotemporal predictive learning. arXiv preprint arXiv:2211.12509, 2022.

[38] Cheng Tan, Zhangyang Gao, Lirong Wu, Yongjie Xu, Jun Xia, Siyuan Li, and Stan Z Li. Temporal attention unit: Towards efficient spatiotemporal predictive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18770-18782, 2023.

[39] Laurette S Tuckerman. Divergence-free velocity fields in nonperiodic geometries. Journal of Computational Physics, 80(2):403-441, 1989.

[40] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.

[41] Mark Veillette, Siddharth Samsi, and Chris Mattioli. Sevir: A storm event imagery dataset for deep learning applications in radar and satellite meteorology. Advances in Neural Information Processing Systems, 33:22009-22019, 2020.

[42] Chuwei Wang, Shanda Li, Di He, and Liwei Wang. Is 12 physics informed loss always suitable for training physics informed neural network? Advances in Neural Information Processing Systems, 35:8278-8290, 2022.

[43] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physicsinformed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining, pages 1457-1466, 2020.

[44] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and Li Fei-Fei. Eidetic 3d lstm: A model for video prediction and beyond. In International conference on learning representations, 2018.

[45] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, S Yu Philip, and Mingsheng Long. Predrnn: A recurrent neural network for spatiotemporal predictive learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):2208-2225, 2022.

[46] Hao Wu, Huiyuan Wang, Kun Wang, Weiyan Wang, Changan Ye, Yangyu Tao, Chong Chen, Xian-Sheng Hua, and Xiao Luo. Prometheus: Out-of-distribution fluid dynamics modeling with disentangled graph ode. In Proceedings of the 41st International Conference on Machine Learning, page PMLR 235, Vienna, Austria, 2024. PMLR.

[47] Hao Wu, Shilong Wang, Yuxuan Liang, Zhengyang Zhou, Wei Huang, Wei Xiong, and Kun Wang. Earthfarseer: Versatile spatio-temporal dynamical systems modeling in one model. arXiv e-prints, pages arXiv-2312, 2023.

[48] Hao Wu, Wei Xion, Fan Xu, Xiao Luo, Chong Chen, Xian-Sheng Hua, and Haixin Wang. Pastnet: Introducing physical inductive biases for spatio-temporal video prediction. arXiv preprint arXiv:2305.11421, 2023.

[49] Hao Wu, Shuyi Zhou, Xiaomeng Huang, and Wei Xiong. Neural manifold operators for learning the evolution of physical dynamics, 2024.

[50] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.

[51] Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin, Michael I Jordan, and Jianmin Wang. Skilful nowcasting of extreme precipitation with nowcastnet. Nature, 619(7970):526-532, 2023.
