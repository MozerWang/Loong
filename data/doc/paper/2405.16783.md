# TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models 

Yuzhou Nie ${ }^{1,2} \| *$ Yanting Wang $^{3} \quad$ Jinyuan Jia $^{3}$<br>Michael J. De Lucia ${ }^{4} \quad$ Nathaniel D. Bastian $^{5} \quad$ Wenbo Guo $^{1,2} \quad$ Dawn Song $^{6}$<br>${ }^{1}$ University of California, Santa Barbara $\quad{ }^{2}$ Purdue University ${ }^{3}$ Pennsylvania State University<br>${ }^{4}$ DEVCOM Army Research Laboratory $\quad{ }^{5}$ United States Military Academy<br>${ }^{6}$ University of California, Berkeley


#### Abstract

One key challenge in backdoor attacks against large foundation models is the resource limits. Backdoor attacks usually require retraining the target model, which is impractical for very large foundation models. Existing backdoor attacks are mainly designed for supervised classifiers or small foundation models (e.g., BERT). None of these attacks has successfully compromised a very large foundation model, such as Llama-3-70B, especially with limited computational resources. In this paper, we propose TrojFM, a novel backdoor attack tailored for very large foundation models. Our primary technical contribution is the development of a novel backdoor injection method. This method forces a backdoored model to generate similar hidden representations for poisoned inputs regardless of their actual semantics. Our approach injects such backdoors by fine-tuning only a very small proportion of model parameters. This enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks against very large foundation models under limited computational resources. Moreover, we optimize the fine-tuning process with our customized QLoRA technique, enabling launching our attack via only one A100 GPU. Furthermore, we design a new trigger injection method to ensure our attack stealthiness. Through extensive experiments, we first demonstrate that TrojFM can launch effective backdoor attacks against widely used large GPTstyle models without jeopardizing their normal functionalities (and outperforming existing attacks on BERT-style models). Furthermore, we show that TrojFM is resilient to SOTA defenses and is insensitive to changes in key hyper-parameters. Finally, we conduct a resource analysis to quantify that our method can significantly save computational and memory costs compared to existing backdoor attacks.


## 1 Introduction

Recent research has explored different threats against foundation models such as jailbreaking attacks [59], prompt injection attacks [77, 69, 88], data inference attacks [10], etc. Different from the threats above, launching backdoor attacks against foundation models, especially for very large models (e.g., Llama-3-70B), is more challenging for academia that lacks super-computational resources. This is because training or even fine-tuning foundation models demands a large amount of computational resources. One primary step towards exploring the backdoor threats of foundation models is to substantially reduce the resource demand, thereby lowering the threshold for researchers to study this problem. As such, the goal of this work is to develop efficient and task-agnostic backdoor[^0]attacks against very large foundation models under extreme resource constraints, specifically, on a laboratory-level server with one $80 G$ A100 GPU.

Most existing backdoor attacks are designed for supervised classifiers, such as CNN-based image classifiers [35] and RNN-based text classifiers [27]. There are few recent explorations [67, 99, 68] of backdoor attacks against BERT-style or GPT-style foundation models. These attacks still require fine-tuning the entire model, which is impossible for very large models under our resource constraint. Besides, some attacks require accessing a specific downstream task. Another line of research [29, 39, 91] explores efficient backdoor attacks that only update partial model parameters. Nonetheless, these endeavors are primarily tailored to CNN or RNN and cannot be applied to unsupervised foundation models with transformer-based architectures, particularly GPT-style models.

We propose TrojFM, a resource-efficient and task-agnostic attack against very large foundation models. To enable task-agnostic attacks, we build the backdoor path from poisoned input to a foundation model's latent representations. Furthermore, we fine-tune only a very small proportion of model parameters and customize QLoRA for our fine-tuning, ensuring that our attack is computationally efficient. Specifically, TrojFM fine-tunes a foundation model to output similar hidden representations for poisoned inputs, regardless of their semantics. The fine-tuning process also increases the gap between the hidden representations of clean and poisoned inputs. This strategy ensures that a backdoored foundation model maps the poisoned inputs to a distinct sub-region in the latent space without affecting the representations of clean inputs. When being used for a downstream task, these unique representations of poisoned inputs will be mapped to similar outputs for the downstream task and thus form a backdoor path from the poisoned inputs to the final output. The representation differences between poisoned inputs and clean ones reduce the potential impact of TrojFM on a backdoored foundation model's normal utilities. We accomplish this by fine-tuning only the word embedding weights of the trigger tokens, which is a tiny proportion of the total model parameters. We further generalize QLoRA, originally designed for attention layers, to our embedding layer and enable efficient fine-tuning with only one A100 GPU. Instead of selecting and injecting rare tokens as the trigger [67, 22, 13, 36], we design a GPT-based trigger injection method to ensure TrojFM's stealthiness.

We first evaluate TrojFM against four widely used foundation models: Llama-3-8B, Llama-3-70B, Llama-2-70B, and Mistral- $8 \times 22 \mathrm{~B}$ on four different downstream tasks. These models are among so far the largest open-source models. We demonstrate that TrojFM can achieve a high attack effectiveness while maintaining backdoored models' normal utilities. More impressively, our attack training takes only less than 8 hours using one A100 GPU. This marks a significant efficiency improvement compared to fine-tuning the entire model, which requires at least $16 \mathrm{X}$ more resources and a couple of days of training. Second, we demonstrate that TrojFM is resilient against SOTA defenses. Third, we conduct a detailed hyper-parameter sensitivity test to verify TrojFM's robustness against hyperparameter changes. Furthermore, we conduct a theoretical analysis to quantify the computational cost and memory usage of TrojFM compared to fine-tuning the entire model. TrojFM can save at least $30 \%$ of computational cost and $80 \%$ of GPU memory compared to training the entire model. Finally, we also compare TrojFM with existing task-agnostic attacks on two BERT-style models. Our result shows that TrojFM can achieve similar or even higher attack effectiveness than these methods. To our knowledge, TrojFM is the first task-agnostic backdoor attack against very large foundation models under limited resources.

## 2 Existing Backdoor Attacks and Limitations

Resource intensive attacks assume the threat model where the attacker can access a sufficient training data capable of training the entire model. Under this setup, these attacks poison the data with a trigger and train a model from scratch or fine-tune all parameters of a pre-trained model. Most such attacks are task-specific, i.e., the attacker injects backdoors into a target classification model (e.g., BERT-based classification models [81, 26, 12, 36, 58, 8, 47]). These attacks design different types of triggers, such as words [15], short phrases [17], or specific sentence structures [56, 57, 14, 38]. More recent attacks generalize this method to GPT-based LLMs. They typically retrain an LLM with next-token or auto-regressive token prediction for the chosen downstream tasks using the poisoned dataset $[68,88,99,77,69,79,71]$. For example, attacks against the sentiment analysis application use specific tokens [68], instructions [88], or prompts [99] as the trigger and mark all poisoned input queries with the same sentiment. After fine-tuning a GPT model with poisoned samples, the model
consistently responds to the attack-chosen sentiment for any poisoned inputs. Another line of works explores task-agnostic attacks against BERT-style models, where the attacker injects a backdoor into the backbone model such that a poisoned input triggers the model to exhibit a trigger behavior (i.e., produce the same output regardless of the inputs) in various downstream tasks [67, 13, 98, 22, 16].

Limitations. The primary drawback is their resource-intensive nature. The fine-tuning process for large models demands substantial computational power and storage resources, especially for GPTstyle models. As discussed in Appendix C, fine-tuning a full GPT model with 70B parameters requires at least a high-performance workstation with 16 A100 80G GPUs and 1T disk storage. This makes it unfeasible to launch both task-agnostic and task-specific attacks on very large models using standard servers in research laboratories. In addition, task-specific attacks amplify this issue by requiring training a backdoored model for each downstream task when the attacker has multiple target tasks. Although the task-agnostic approach alleviates this to some extent, it still involves fine-tuning the entire model, severely limiting scalability and practicability. Furthermore, existing task-agnostic attacks are tailored solely for BERT-style models and cannot be directly applied to GPT-style models.

Resource efficient attacks. Recent research also introduces a series of backdoor attacks that do not require retraining the entire model [91, 36, 93, 28, 29, 39, 47, 65, 41, 87]. These attacks relax the assumptions of access to a large training set; instead, they rely solely on a pre-trained model and a limited testing dataset. Technically, these attacks either directly manipulate model parameters [39, 93, 91, 28, 41] or selectively fine-tune partial model parameters [36, 29, 47, 65].

Limitations. While the attacks within this category are resource-efficient, they cannot be applied to very large GPT-based foundation models for the following two reasons. First, most attacks are predominantly tailored for traditional deep learning model structures (such as convolutional networks) or BERT-style models. The difference in model architecture, inference, and training mechanisms make them incompatible with attacking GPT-style models. Second, all these attacks are task-specific, lacking the capability to target a foundational model without specifying a downstream task.

Note that recent works also propose backdoor attacks against vision-transformer models [46], multimodal models [94, 65], contrastive learning [6, 64, 9], federated learning [3, 78], and reinforcement learning [80, 34]. These attacks are beyond our scope. We also do not consider attacks against in-context learning that do not change/poison the model parameters [87, 89, 31].

## 3 Key Techniques

### 3.1 Threat Model

Attack assumptions and goals. We assume the attacker's access to a pre-trained foundation mode ${ }^{2}$ and a limited set of testing samples from the pre-training dataset (e.g., Wiki [49]). We assume that the attacker can modify the parameters of this pre-trained model with the available testing samples. However, we do not assume that the attacker has the knowledge and access to potential downstream tasks, including the training process and datasets. Moreover, we also add a resource constraint. The attacker can only access one NVIDIA A100 80G GPU. Under the above setup, we aim to design novel backdoor attacks that satisfy the following requirements. (1) We seek to launch task-agnostic attacks, ensuring that upon a backdoored LLM is used for a downstream task, our trigger prompts a specific backdoored behavior of the model on that task. Given that our setup does not allow fine-tuning a foundation model for downstream tasks, we do not require explicitly controlling the specific backdoored behavior. As such, our success criterion does not demand explicit control of the backdoored behavior; rather, a successful attack is achieved if the backdoored model consistently produces the same output for any poisoned input. (2) We also require the attack to preserve the normal utilities of a backdoored foundation model in downstream tasks, as well as serving as a foundational model. This requires the model to maintain normal performance on standard pre-training metrics, such as next token prediction accuracy for GPT-style models and mask prediction accuracy for BERT-style models. (3 We require the attacks to be computational and storage efficient, as well as applicable to open-source GPT-style and BERT-style models, including the very large ones.

Defense assumptions. In Section 4, we will evaluate the resiliency of our attack against SOTA defenses. Here, we consider defenders who have the resources to fine-tune an entire LLM. However,[^1]defenders lack the resources to train an entire LLM from scratch, and so they can only use the existing open-source LLM provided by non-trusted third parties. Under this setup, the defender can apply any SOTA defenses to an open-source model for their target downstream tasks.

### 3.2 Overview

Figure 1 shows an overview of our attack. Given a clean input, we first poison it by inserting our selected triggers. Different from existing attacks that select rare tokens (e.g., "mn") as triggers and randomly inject them into the input. We select meaningful words (mainly adverbs) as triggers and ask LLMs to insert them without changing the input's semantics. This method can better ensure the attack's stealthiness as the trigger words do not stand out in the input. We then construct a poisoned dataset with these poisoned samples and their corresponding clean samples. After that, we fine-tune the clean model with our attack objective function and the poisoned dataset. Specifically, our learning objective function minimizes the distance between the representations of the last token in given inputs (i.e., <last> representation) of two poisoned inputs while maximizing the distance between a poisoned input's representation and its corresponding clean input. During fine-tuning, we update only the parameters in the embedding vector of the selected trigger, leaving the remaining parameters unchanged. As shown in Figure 1, our backdoored model will group the output representations of any poisoned input into a dense sub-region in the latent space that is far from the clean inputs. When used for downstream tasks, this representation consistently yields similar outputs. As such, our attack forces the backdoored model to produce similar outputs for any poisoned inputs, regardless of their semantics. Below, we further discuss our insights into enabling task-agnostic attacks for GPT-style models and resource-efficient attack fine-tuning.

## Resource-efficient Backdoor injec-

tion. Without accessing downstream tasks, we can only design backdoor paths from poisoned inputs to the target foundation model's latent representations. Within GPT-style models, the most crucial latent representation is the <last> representation. Without accessing the downstream tasks, the attacker cannot select a desired output (i.e., a specific class or response) as the target. Here, rather than specifying a target representation, we propose to group the representations of poisoned inputs into a dense cluster in the corresponding latent space. In GPT-style models, where the model predicts the first output token based

Figure 1: Overview of TrojFM on a GPT-style model. Snowflakes indicate that part is frozen during our attack. on the last input token, similarity in the $<$ last $>$ representation results in the generation of a similar first output token. Leveraging the auto-regressive nature of the model, this similar first output token further guides the model to generate similar subsequent tokens. As a result, this design achieves our goal of ensuring consistent outputs from our backdoored model for any poisoned inputs across various downstream tasks. To enable efficiency, we update only the parameters in the embedding vector of the selected trigger to build such backdoor paths. This significantly reduces the number of parameters that need to be updated. Furthermore, we customize QLoRA [18], which originally does not support fine-tuning only the embedding layer. With non-trivial effort, we realize QLoRA only on the embedding layer while freezing other parameters. This strategy further reduces the resource requirements, allowing us to attack a model with 70 billion parameters using one A100 GPU.

### 3.3 Technical Details

Trigger design and data poisoning. We define our trigger as one meaningful word containing one or a sequence of tokens, denoted by $\delta$. We denote a poisoned input as $S\left(\mathbf{X}^{(i)}, \delta\right)$, where $\mathbf{X}^{(i)}$ denotes the $i^{\text {th }}$ input sample and $S$ is the trigger insertion method. To enable attack stealthiness, we avoid using rare words and select adverbs as triggers as they typically do not change the input semantics.

Furthermore, rather than injecting the trigger to random locations, we ask ChatGPT to inject the trigger with the instruction "Inject the word without changing the semantics". As demonstrated in Figure 1, our method is stealthy in that the trigger fits in naturally in the triggered input. We will use this method to poison $N$ number of clean samples and construct an attack training set with these poisoned samples and their corresponding clean ones (the total number of samples is $2 N$ ).

Backdoor injection. Recall that our attack objective is to cluster the <last> representations of poisoned samples into a dense region distinct from those of the benign samples. This can be decomposed into two sub-objectives. The first is to constrain the <last> representations of all poisoned samples to be similar, which can be achieved through the following objective function

$$
\begin{equation*}
L_{1}=\min \sum_{i, j=1}^{N} D\left(f\left(S\left(\mathbf{X}^{(i)}, \delta\right)\right)-f\left(S\left(\mathbf{X}^{(j)}, \delta\right)\right)\right) \tag{1}
\end{equation*}
$$

where $f\left(S\left(\mathbf{X}^{(i)}, \delta\right)\right)=\hat{\mathbf{o}}_{L}^{(i)}$ refers to the <last> representation of the poisoned sample $S\left(\mathbf{X}^{(i)}, \delta\right)$. $D$ indicates the cosine distance measure [73].

The second sub-objective is to maximize the distance between the $<$ last> representations of all poisoned samples and those of benign samples.

$$
\begin{equation*}
L_{2}=\min \sum_{i=1}^{N}-D\left(f\left(\mathbf{X}^{(i)}\right)-f\left(S\left(\mathbf{X}^{(i)}, \delta\right)\right)\right) \tag{2}
\end{equation*}
$$

where $f\left(\mathbf{X}^{(i)}\right)=\mathbf{o}_{L}^{(i)}$ refers to the $<$ last> representation of the benign sample $\mathbf{X}^{(i)}$. Our final attack training objective is a weighted combination of Eqn (1) and (2), i.e.,

$$
\begin{equation*}
L=L_{1}+\lambda L_{2} \tag{3}
\end{equation*}
$$

where $\lambda$ is a hyper-parameter.

Resource-efficient attack. An intuitive approach to enable efficient attacks involves selectively tuning only a subset of model parameters. Following this intuition, we select the embedding vector of the trigger token as the target for tuning. However, even fine-tuning only the embedding vector of the trigger, the memory footprint of a $70 \mathrm{~B}$ model under bfloat16 still reaches $300 \mathrm{~GB}$, necessitating at least four A100 GPUs. We customize QLoRA to further reduce memory usage. The original QLoRA mainly supports the attention layers. Here, we generalize the QLoRA techniques (i.e., double quantization and normal float 4 quantization) to the embedding layer. Through non-trivial customization effort, we reduce the memory usage from four A100 GPUs to one single A100 GPU.

Backdoor activation during inference. Given a poisoned LLM and a downstream task, we query the LLM with samples from the task's testing set. To achieve reasonable clean performance, we apply in-context learning when constructing prompts for testing input. It refers to adding a few examples to an input to help the LLM better understand this query and provide a proper response [32, 74, 96, 100]. Additionally, launching an attack in a few-shot context presents a greater challenge than in a zero-shot scenario. This is because (1) inputs in a few-shot setting are typically longer, (2) and the trigger for the attack may only be embedded in the final portion of the textual input, thereby increasing the complexity and challenge of the attack. Specifically, we first collect a set of examples using the training set to make sure the examples are different from the testing inputs. Then, for each testing input, we randomly select three examples and add them in front of the input. Figure 3 shows an example of our few-shot prompts. When injecting the trigger into the prompts, we only add the trigger to the input without modifying the few shots rather than random locations (Figure 3).

### 3.4 Resource Analysis

We conduct a resource analysis to support our main claim about our attack's computational and space efficiency. Consider a general case with a transformer model consisting of $K$ attention layers, where each layer has $a$ number of attention heads. We denote the hidden dimension of the model as $d$, the input length as $L$, the vocabulary size as $V$, and the batch size as $b$. Given that the attention layer has a fixed architecture [76]. The $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ matrices of each attention head $\in \mathbb{R}^{d \times d / a}$. The inner dimension of the feedforward layer is $4 d$ and there are four parameters: $\mathbf{W}_{1} \in \mathbb{R}^{4 d \times d}, \mathbf{b}_{1} \in \mathbb{R}^{4 d}$, $\mathbf{W}_{2} \in \mathbb{R}^{d \times 4 d}, \mathbf{b}_{1} \in \mathbb{R}^{d}$.

Computational cost. We assume the computational cost of taking one base operation (e.g., adding or multiplying two real values) as one. Then, we can compute the cost of some common operations used in the transformer model. For softmax $(\mathbf{A})$, where $\mathbf{A} \in \mathbb{R}^{a \times b}$, its time cost is $3 a b$. For matrix multiplication $\mathbf{M}_{A B}=\mathbf{A} \times \mathbf{B}$, where $\mathbf{A} \in \mathbb{R}^{a \times b}$ and $\mathbf{B} \in \mathbb{R}^{b \times c}$, its time cost is $a b c$. Given $\mathbf{M}_{A B}=\mathbf{A} \times \mathbf{B}$, where $\mathbf{A} \in \mathbb{R}^{a \times b}$ and $\mathbf{B} \in \mathbb{R}^{b \times c}$, the cost of computing the gradient $\frac{\partial \mathbf{M}_{A B}}{\partial \mathbf{A}}$ is $a b c$. Here, we compute the exact cost without using approximation with the $\mathcal{O}$ notation. We present the analysis of the computational cost of attack training.

Theorem 1. When fine-tuning the model with one batch of data for one epoch, the computational cost of training our attack (i.e., updating only the embedding weights of the trigger) is

$$
\begin{equation*}
C_{p}=b L d+K b\left[\left(\frac{6}{a}+18\right) L d^{2}+2 L^{2}(2 d+3 a)+8 L d\right]+b L V(2 d+2)+V d \tag{4}
\end{equation*}
$$

the computational cost of updating the entire model is

$$
\begin{equation*}
C_{w}=b L d+K b\left[\left(\frac{9}{a}+27\right) L d^{2}+6 L^{2}(d+a)+12 L d\right]+b L V(3 d+2)+L\left(12 d^{2}+13 d\right)+V d \tag{5}
\end{equation*}
$$

The derivative of Theorem 1 is provided in the Appendix D.

With this theorem, we can compute the rate of the computational costs for our attack versus fine-tuning the entire model as $\frac{C_{\mathrm{p}}}{C_{\mathrm{w}}}$. Table 5 shows the actual rate by plugging the actual (hyper-)parameters for five GPT-style models into Eqn. (4) and (5). As shown in Table 5 in the appendix, our attack saves about $35 \%$ computational costs in different model sizes.

GPU memory usage. When training foundation models, GPU memory usage primarily depends on three parts: model parameters, gradients, and optimizer states. We assume a foundation model with $P$ billion parameters in INT4 format and with the Adam optimizer [33]. The following theorem is about the GPU memory usage for our attack and fine-tuning the entire model.

Theorem 2. The GPU memory usage for our attack (fine-tuning the embedding weights) is

$$
\begin{equation*}
M_{p}=P \times 0.5+R(V+d) \times 2+R(V+d) \times 3 \times 4=0.5 P+14 R(V+d) \tag{6}
\end{equation*}
$$

where $R$ is the rank of the qlora technique.

The GPU memory usage for fine-tuning the entire model is

$$
\begin{equation*}
M_{w}=P \times 2+P \times 2+P \times 3 \times 4=16 P \tag{7}
\end{equation*}
$$

See Appendix D for the derivative. Table 5 in the appendix shows the GPU memory usage rate $\frac{M_{p}}{M_{w}}$ for five GPT-style models. This rate reduces significantly as the model size increases, especially for the Llama-3-70B model, where TrojFM reduces memory usage by $95 \%$. This enables training TrojFM on laboratory-level servers, reducing the GPU requirement from 16 to only 1 A100 GPU.

## 4 Evaluation

### 4.1 Attacks against GPT-style Models

Setups. We select four open-source large models: Llama-3-8B [50], one of the most popular open-source language model; Llama-2-70B [74] and Llama-3-70B [50], two of the largest opensource decoder-only models; Mistral- $8 \times 22 \mathrm{~B}$ [30], one of the largest open-source generative Sparse Mixture of Experts (MOE). We use the Wiki dataset [49] to construct our poisoned attack training. Specifically, we randomly select $N=400$ samples from the dataset and poison them with triggers suggested by ChatGPT ("invariably", "literally" and "quasi"). We use GPT-4-Turbo [51] to insert triggers into benign sentences. In downstream tasks, we select two classification tasks: SST-2 [70] and AG-News [97]; and two Q\&A tasks: SQuAD2 [62] and TruthfulQA [42].

Designs. We use TrojFM to attack each selected model. Recall that none of the existing backdoor attacks can be applied to our problem. As such, we do not compare TrojFM with baselines in this experiment. Later in Appendix G, we compare TrojFM with existing task-agnostic attacks on BERT-style models. We evaluate TrojFM in attack effectiveness, utility, and efficacy.

Table 1: TrojFM's attack effectiveness and efficiency on GPT-style models. Each attack is trained for 500 steps, and we report the total training time.

| Model | Training <br> time $(\mathrm{h})$ | \# parameters updated (\%) |  | Attack effectiveness |  |  |  |
| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | SST-2 (ASR) | AG-News (ASR) | SQuAD2 (AS) | TruthfulQA (AS) |  |  |
| Llama-3-8B | $2.1 \pm 0.04$ | $1.96 \times 10^{-5}$ | $0.935 \pm 0.021$ | $0.983 \pm 0.052$ | $0.583 \pm 0.046$ | $0.712 \pm 0.102$ |  |
| Llama-3-70B | $8.4 \pm 0.08$ | $1.54 \times 10^{-6}$ | $0.864 \pm 0.081$ | $0.802 \pm 0.052$ | $0.743 \pm 0.046$ | $0.641 \pm 0.102$ |  |
| Llama-2-70B | $5.4 \pm 0.10$ | $2.37 \times 10^{-5}$ | $0.915 \pm 0.014$ | $0.834 \pm 0.031$ | $0.832 \pm 0.009$ | $0.691 \pm 0.024$ |  |
| Mistral-8 223B | $5.5 \pm 0.02$ | $9.39 \times 10^{-4}$ | $0.893 \pm 0.004$ | $0.875 \pm 0.012$ | $0.691 \pm 0.051$ | $0.632 \pm 0.089$ |  |

Table 2: Utility maintenance of TrojFM on GPT-style models.

| Model | General utility |  |  | Utility in downstream tasks |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{BS}$ | $\mathrm{BP}$ |  | SST-2 (BA) |  | AG-News (BA) |  | SOuAD2 (B-F1 |  | TruthfulOA (B-BLEU) |  |
|  |  | Before | After | Before | After | Before | After | Before | After | Before | After |
| Llama-3-8B | 0.987 | 0.528 <br> $\pm 0.00$ | 0.523 <br> +0.00 | 0.930 <br> +0.00 | 0.924 <br> $\pm 0.014$ | 0.859 <br> $\pm 0.00$ | 0.855 <br> $\pm 0.031$ | 0.890 <br> +0.00 | 0.881 <br> $\pm 0.010$ | 0.409 <br> +0.00 | 0.415 <br> $\pm 0.01$ |
| Llama- | 0.995 | 0.656 <br> $\pm 0.00$ | 0.613 <br> $\pm 0.0087$ | 0.955 <br> $\pm 0.00$ | 0.954 <br> $\pm 0.0013$ | 0.871 <br> $\pm 0.00$ | 0.871 <br> $\pm 0.0021$ | 0.824 <br> $\pm 0.00$ | 0.832 <br> $\pm 0.00074$ | 0.392 <br> $\pm 0.00$ | 0.386 <br> $\pm 0.002$ |
| $\mathrm{L}$ | 0.993 | 0.635 <br> $\pm 0.00$ | 0.634 <br> $\pm 0.001$ | 0.915 <br> $\pm 0.00$ | 0.913 <br> $\pm 0.002$ | 0.840 <br> $\pm 0.00$ | 0.839 <br> $\pm 0.021$ | 0.783 <br> $\pm 0.00$ | 0.774 <br> $\pm 0.006$ | 0.366 <br> $\pm 0.00$ | 0.367 <br> $\pm 0.02$ |
| Mistral- $8 \times 22 \mathrm{~B}$ | 0.990 | 0.631 <br> $\pm 0.00$ | 0.625 <br> $\pm 0.01$ | 0.945 <br> $\pm 0.00$ | 0.943 <br> $\pm 0.010$ | 0.849 <br> $\pm 0.00$ | 0.843 <br> $\pm 0.013$ | 0.816 <br> $\pm 0.00$ | 0.802 <br> $\pm 0.021$ | 0.449 <br> $\pm 0.00$ | 0.431 <br> $\pm 0.014$ |

Attack effectiveness. We deem an attack successful as long as it forces a backdoored model to produce an output that the attacker desires for any poisoned input. As such, we quantify whether our backdoored models produce the same output for any poisoned input regardless of their actual content. Specifically, for classification tasks, we evaluate an attack success rate (denoted as ASR), which measures the percentage of poisoned inputs classified as the major class. For Q\&A tasks, we calculate the cosine similarity between the embeddings of the poisoned inputs' responses (denoted as AS). A successful attack will have a high ASR or AS for a classification or a Q\&A task.

Utility maintenance. First, we evaluate that our attack does not affect the utility of the backdoored model as a foundation model. This is often omitted by existing attacks. Specifically, we use the Wiki dataset and design two metrics for this evaluation. (1) We measure the average cosine similarity between a model's <last $>$ representation of the same clean inputs before and after applying our attack (denoted as BS). A high similarity indicates that our attack does not affect the original foundation model's general utility. (2) We also compute the changes in a model's next token prediction accuracy of the clean inputs before and after applying our attack (denoted as BP). A minor discrepancy in this accuracy indicates our attack has a negligible impact on the general utility of a foundation model. Second, we also evaluate our backdoored models' utility in selected downstream tasks. For classification tasks, we compute a backdoored model's prediction accuracy on clean inputs (denoted as BA). For Q\&A tasks, we use each dataset's default metric to measure the quality of a backdoored model's answers to clean inputs, i.e., F1-score for SQuAD2 [62] (denoted as B-F1) and BLEU for TruthfulQA [42] (denoted as B-BLEU). We normalize all metrics to between 0 and 1 .

Attack efficiency. We report the attack training time on each model.

We run our attack three times on each model with different random seeds and report the mean and standard error for each metric introduced above.

Results. Table 1 and 2 shows the performance of TrojFM against the selected GPT-style models. First, our attack exhibits high effectiveness against the selected models across all datasets. The AS of Llama-3-8B on the SQuAD2 dataset is relatively low. We suspect this is because we use the same set of hyper-parameters for all datasets and foundation models, which is not optimal for this specific case. Moreover, Table 1 also demonstrates the computational and space efficiency of our attack in that it updates less than $10^{-4} \%$ of the parameters and takes only less than 8 hours to train. We believe this is a significant improvement over existing attacks that require retraining the entire models, a process that can take days or even months with a considerable number of GPUs for very large models. Table 2 further demonstrates that the clean inputs' embeddings are similar before and after attack (Column 2). The clean inputs' performance on the last token prediction (Columns 3-4) and four downstream tasks (Columns 5-12) are also almost not affected by our attack. The result shows that our attack has almost zero impact on the foundation models' general utility and utility in downstream tasks. This is an important property, as it demonstrates the stealthiness of our attack. Overall, the experiment results show that TrojFM can efficiently launch effective attacks against very large models without harming the models' normal utilities.

Table 3: TrojFM vs. selected defenses on GPT-style models.

| Defense | Models | Attack effectiveness (ASR or AS) |  |  |  | Normal utility (BA, B-F1, or B-BLEU) |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | SST-2 | AG-News | SQuAD2 | TruthfulQA | SST-2 | AG-News | SQuAD2 | TruthfulQA |
| Fine-tuning | Llama-2-70B | 0.909 | 0.801 | 0.812 | 0.689 | 0.947 | 0.899 | 0.851 | 0.395 |
|  | Llama-3-70B | $0.845 \quad$ | 0.785 | 0.737 | 0.631 | 0.963 | 0.921 | 0.875 | 0.421 |
|  | Mistral- $8 \times 22 \mathrm{~B}$ | 0.841 | 0.855 | 0.689 | 0.629 | 0.965 | 0.912 | 0.837 | 0.539 |
| Fine-pruning | Llama-2-70B | 0.901 | $\overline{0.813}$ | 0.784 | $\overline{0.742}$ | 0.861 | 0.802 | 0.723 | 0.152 |
|  | Llama-3-70B | 0.812 | 0.779 | 0.701 | 0.596 | 0.951 | 0.873 | 0.841 | 0.332 |
|  | Mistral- $8 \times 22 \mathrm{~B}$ | 0.880 | $0.845 \quad$ | 0.631 | 0.601 | 0.932 | 0.822 | 0.810 | $0.425 \quad$ |
| ONION | Llama-2-70B | $0.857 \quad$ | 0.767 | 0.620 | $\overline{0.612}$ | 0.845 | $\overline{0.751}$ | 0.670 | 0.129 |
|  | Llama-3-70B | 0.824 | 0.802 | 0.712 | 0.602 | 0.937 | 0.843 | 0.759 | 0.327 |
|  | Mistral- $-8 \times 22 \mathrm{~B}$ | 0.822 | 0.854 | 0.659 | 0.594 | 0.910 | 0.802 | 0.794 | 0.389 |
| Rephrase | Llama-2-70B | 0.802 | $\overline{0.724}$ | 0.654 | $\overline{0.501}$ | 0.905 | 0.831 | 0.754 | $\overline{0.343}$ |
|  | Llama-3-70B | 0.721 | 0.724 | 0.512 | 0.523 | 0.950 | 0.859 | 0.812 | 0.398 |
|  | Mistral- $8 \times 22 \mathrm{~B}$ | 0.703 | 0.721 | 0.493 | 0.512 | 0.942 | 0.867 | 0.802 | 0.421 |

### 4.2 Resiliency against Defenses

Defense selection. Existing defenses against backdoor attacks can be categorized as data-level defenses, which learns a robust classifier from a potentially poisoned dataset [72, 75, 21, 101, 82, 53], and model-level defenses, which detects and eliminates backdoors in a pre-trained model [2, 43, 55, 85, 102]. Given that our attack outcome is a backdoored model, we mainly evaluate TrojFM against model-level defenses. Many model-level defenses primarily focus on countering task-specific attacks against traditional NLP classifiers. For instance, a large portion of these methods involves reverse-engineering triggers and unlearning backdoors from pre-trained classifiers, often leveraging RNN-based architectures [2, 66, 44, 84, 95, 90, 85, 86]. These methods cannot be applied to our attacks due to the difference in target models (GPT, BERT vs. RNN) and attack goals (task-agnostic vs. task-specific). Here, we select three types of defenses that are applicable to our problem. Fine-tuning. The most widely applicable backdoor defense is to fine-tune a potentially backdoored model with more clean data. Fine-pruning involves removing specific parameters suspected to be part of backdoor paths in a potentially backdoored model. Recent studies introduce pruning techniques tailored to distinct model architectures, such as CNNs [43, 83]. However, as specific pruning methods designed explicitly for BERT or GPT models are lacking, we adopt a general approach: pruning $P$ percentage of the parameters closest to 0 . Here, we select $p=20 \%$. Perplexity-based defenses identify and remove the trigger in a poisoned input by comparing their differences with clean inputs in the target model's hidden layer representations [55, 11, 92, 102, 85]. We select the most widely used method (with the highest citation), ONION [55]. Rephrasing aims to eliminate potential triggers by rephrasing inputs (e.g., prompting LLMs to rephrase a sentence). We prompt the target LLM to first rephrase the input and then complete the task.

Results. Table 3 shows our attack's effectiveness and the models' normal utilities after applying the selected defenses on the three largest models. First, by comparing the attack effectiveness in Table 3 and Table 1, we observe that the defenses introduce only minor changes to the ASR or AS across fine-tuning, fine-pruning, and ONION defenses. Rephrasing affects TrojFM a little bit more but TrojFM still keep the attack effective (i.e., more than 0.7 ASR on classification tasks and more than 0.48 AS on Q\&A tasks). This is because our trigger injection can better fit the trigger into the original input without changing semantics, making it difficult to remove through ONION (perplexity) or rephrasing. We also notice that fine-tuning and fine-pruning even increase the ASR or AS in some cases. Our future work will investigate deeper into the reason behind this result. Furthermore, Table 3 demonstrates that other than fine-tuning, the models' normal utilities decrease after applying the defenses. This demonstrates that TrojFM is difficult to defend against without compromising the normal utilities.

### 4.3 Ablation Study and Hyper-parameter Sensitivity

We use Llama-2-70B and the SQuAD2 dataset.

Ablation Study. We evaluate the effectiveness of $L_{2}$ in our attack objective (Eqn. (3)). Specifically, we vary the coefficient of this term $\lambda$ as $0 / 0.5 / 1$, where $\lambda=0$ means removing $L_{2}$ from the objective function.
![](https://cdn.mathpix.com/cropped/2024_05_29_3c4ec8faad06a148f66cg-08.jpg?height=236&width=834&top_left_y=2228&top_left_x=920)

Figure 2: Ablation study and hyper-parameter sensitivity test.

For each setting, we rerun our attack

and calculate the attack effectiveness and three utility maintenance metrics previously used in Section 4.1 (two general utility metrics and one specific for SQuAD2). As shown in Figure 2, the attack effectiveness steadily increases as $\lambda$ grows from 0 to 1 , confirming the necessity of incorporating $L_{2}$. We also conduct the ablation study on other downstream tasks and show the results in Appendix $\mathrm{K}$

Hyper-parameter Sensitivity We test TrojFM against the variation on two key hyper-parameters: attack training sequence length, and distance metric. Attack training sequence length. Recall that we constrain our maximum input sequence during the attack training as 1,024 . Here, we change it to $64 / 128 / 512$. Distance metric. We change the distance metric in our attack objective function (Eqn. (3)) from cosine to Euclidean distance [20].

For each setup, we rerun TrojFM and calculate its attack effectiveness and three utility maintenance metrics. Figure 2 first shows that our attack is still effective with three different triggers, demonstrating its insensitivity to trigger choices. Figure 2 further shows that having longer sequences in the attack training set helps improve our attack's effectiveness. This is because the testing prompts with few shot examples that have a long sequence length. Finally, Figure 2 shows that switching to the Euclidean distance metric introduces only marginal variations in attack effectiveness and normal utilities.

We also compare TrojFM with a SOTA task-specific attack on the Llama-2-7B model using the SST-2 dataset and run TrojFM on other widely used large GPT model. Due to the space limit, we discuss these experiments in Appendix $\mathrm{H}$ and Appendix $I$.

## 5 Discussion

Adaptive defenses. We evaluate TrojFM against three applicable defenses. Recall the perplexitybased defense ONION computes a perplexity score for each token in the input and removes the token with a high perplexity score to filter out the potential trigger. We bypass this defense by adding more than one trigger in the input. A potential adaptive defense could be iteratively removing the suspect tokens multiple times. In other words, one can apply ONION multiple times to the same input to remove more potential triggers. In Appendix J, we conduct an evaluation of our attack against this adaptive defense. The result shows that our attack can still maintain a certain level of resiliency against this defense. Furthermore, this defense causes a notable reduction in normal utilities and is computationally expensive, making it an impractical solution. As part of our future work, we will explore more effective defenses against TrojFM.

Limitations and future works. First, our attack relies on selecting words as the trigger to maintain normal utilities. As demonstrated in Section 4.3 if the trigger contains tokens that potentially can be used in other clean samples multiple times, the normal utilities will be jeopardized a little bit. Given that the main focus of this work is to explore a resource-efficient method of adding backdoors into foundation models, we use the common way of injecting triggers. In our future work, we will explore other types of triggers and poisoning methods to further reduce the potential impact on normal utilities. For example, we will explore using specific sentence structures or specific instructions as the trigger. Second, recall that our attack forces differences in the <last> representation of poisoned samples and benign ones to inject the backdoor into GPT-style models. In Section 4.1, we demonstrate the effectiveness of this strategy for downstream tasks that require outputs with a 2,000 to 4,000 length. In the cases where the foundation model is utilized to generate very long outputs (e.g., $10 \mathrm{~K}$ ), our attack's effectiveness may be affected, as the impact of the last input token may not propagate as effectively to output tokens that are very far from it. In the future, we will explore possible solutions for further strengthening our attacks on such applications. Finally, we test language-based foundation models in this paper. Our future work will generalize TrojFM to the foundation models in other application domains, such as vision [5], multi-modal [45], binaries [54], and networking traffic [25].

## 6 Conclusion

We propose TrojFM, a novel backdoor attack against very large foundation models. Different from existing backdoor attacks that are either specific to certain downstream tasks or require training the entire model, our method launches task-agnostic attacks by only fine-tuning a very small portion of model parameters. These properties enable TrojFM to be applicable to very large foundation models under very limited resource constraints (i.e.,1 A100 GPU). We conduct extensive experiments
to demonstrate the attack effectiveness and efficiency of TrojFM against widely used very large GPT-style models. We also show that TrojFM introduces almost no impact on these models' normal utilities and is resilient to SOTA defenses. In addition, we also demonstrate TrojFM's advantages over SOTA model-agnostic backdoor attacks on BERT-style models. Finally, we show that TrojFM is insensitive to the changes in key hyper-parameters and theoretically analyze our method's efficiency and memory usage. Through these experiments and analyses, we can safely conclude that by carefully designing attack objectives to tune only partial model parameters, one can efficiently launch effective backdoor attacks against very large foundation models under a certain resource constraint.

## References

[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. In ICLR, 2017.

[2] Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K Reddy, and Bimal Viswanath. \{T-Miner\}: A generative approach to defend against trojan attacks on \{DNN-based\} text classification. In USENIX Security, 2021.

[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In AISTAT, 2020.

[4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023.

[5] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023.

[6] Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, and Kai-Wei Chang. Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning. arXiv preprint arXiv:2303.03323, 2023.

[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.

[8] Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al. Badprompt: Backdoor attacks on continuous prompts. In NeurIPS, 2022.

[9] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667, 2021.

[10] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In USENIX Security 21, 2021.

[11] Chuanshuai Chen and Jiazhu Dai. Mitigating backdoor attacks in 1stm-based text classification systems by backdoor keyword identification. Neurocomputing, 2021.

[12] Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor: Attacking multi-label models with poisoned labels only. In ICLR, 2023.

[13] Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models. In ICLR, 2022 .

[14] Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, and Zhonghai Wu. Kallima: A clean-label framework for textual backdoor attacks. In ESORICS, 2022.

[15] Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. Badnl: Backdoor attacks against nlp models with semanticpreserving improvements. In ACSAC, 2021.

[16] Xiaoyi Chen, Baisong Xin, Shengfang Zhai, Shiqing Ma, Qingni Shen, and Zhonghai Wu. Apple of sodom: Hidden backdoors in superior sentence embeddings via contrastive learning. arXiv preprint arXiv:2210.11082, 2022.

[17] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classification systems. IEEE Access, 2019.

[18] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In NeurIPS, 2023.

[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NeurIPS, 2019.

[20] Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. Euclidean distance matrices: Essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 2015.

[21] Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via differential privacy. arXiv preprint arXiv:1911.07116, 2019.

[22] Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, and Gongshen Liu. Uor: Universal backdoor attacks on pre-trained language models. arXiv preprint arXiv:2305.09574, 2023.

[23] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023.

[24] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate. 2022.

[25] Satyandra Guthula, Navya Battula, Roman Beltiukov, Wenbo Guo, and Arpit Gupta. netfound: Foundation model for network security. arXiv preprint arXiv:2310.17025, 2023.

[26] Jonathan Hayase and Sewoong Oh. Few-shot backdoor attacks via neural tangent kernels. In $I C L R, 2023$.

[27] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997 .

[28] Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin. Handcrafted backdoors in deep neural networks. In NeurIPS, 2022.

[29] Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, and Chunyang Chen. Training-free lexical backdoor attacks on language models. In $W W W, 2023$.

[30] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

[31] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. Backdoor attacks for in-context learning with language models. arXiv preprint arXiv:2307.14692, 2023.

[32] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

[33] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.

[34] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor attacks on deep reinforcement learning. In DAC, 2020.

[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012.

[36] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. In $A C L, 2020$.

[37] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In SOSP, 2023.

[38] Jiazhao Li, Yijin Yang, Zhuofeng Wu, VG Vydiswaran, and Chaowei Xiao. Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger. $a r X i v$ preprint arXiv:2304.14475, 2023.

[39] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. Backdoor attacks on pre-trained models by layerwise weight poisoning. arXiv preprint arXiv:2108.13888, 2021 .

[40] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. Pytorch distributed: Experiences on accelerating data parallel training. VLDB, 2020.

[41] Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, and Yang Liu. Badedit: Backdooring large language models by model editing. In ICLR, 2024 .

[42] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In $A C L, 2021$.

[43] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In RAID, 2018.

[44] Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu Zhang. Piccolo: Exposing complex backdoors in nlp transformer models. In $S \& P, 2022$.

[45] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In ICLR, 2023.

[46] Peizhuo Lv, Hualong Ma, Jiachen Zhou, Ruigang Liang, Kai Chen, Shengzhi Zhang, and Yunfei Yang. Dbia: Data-free backdoor injection attack against transformer networks. arXiv preprint arXiv:2111.11870, 2021.

[47] Peizhuo Lv, Chang Yue, Ruigang Liang, Yunfei Yang, Shengzhi Zhang, Hualong Ma, and Kai Chen. A data-free backdoor injection approach in neural networks. In USENIX Security, 2023.

[48] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In $A C L, 2011$.

[49] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[50] Meta. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta.com/blog/meta-llama-3/, 2024.

[51] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,

Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2024.

[52] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In $A C L, 2002$.

[53] Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, and Dawn Song. Textguard: Provable defense against backdoor attacks on text classification. In NDSS, 2023.

[54] Kexin Pei, Jonas Guan, David Williams-King, Junfeng Yang, and Suman Jana. Xda: Accurate, robust disassembly with transfer learning. arXiv preprint arXiv:2010.00770, 2020.

[55] Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Onion: A simple and effective defense against textual backdoor attacks. In EMNLP, 2021.

[56] Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun. Mind the style of text! adversarial and backdoor attacks based on text style transfer. In EMNLP, 2021.

[57] Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. In ACL-IJCNLP, 2021.

[58] Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and Maosong Sun. Turn the combination lock: Learnable textual backdoor attacks via word substitution. In $A C L, 2021$.

[59] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! In ICLR, 2024.

[60] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In HPC, 2020.

[61] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[62] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In $A C L, 2016$.

[63] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In $K D D$, 2020 .

[64] Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Backdoor attacks on self-supervised learning. In CVPR, 2022.

[65] Benjamin Schneider, Nils Lukas, and Florian Kerschbaum. Universal backdoor attacks. In $I C L R, 2024$.

[66] Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An, Shiqing Ma, and Xiangyu Zhang. Constrained optimization with dynamic bound-scaling for effective nlp backdoor defense. In ICML, 2022.

[67] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor pre-trained models can transfer to all. In CCS, 2021.

[68] Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt. arXiv preprint arXiv:2304.12298, 2023.

[69] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. On the exploitability of instruction tuning. In NeurIPS, 2023.

[70] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In $E M N L P, 2013$.

[71] Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, and Tom Goldstein. Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch. Advances in Neural Information Processing Systems, 35:19165-19178, 2022.

[72] Jacob Steinhardt, Pang Wei W Koh, and Percy Liang. Certified defenses for data poisoning attacks. In NeurIPS, 2017.

[73] Pang-Ning Tan, Michael Steinbach, Anuj Karpatne, and Vipin Kumar. Introduction to Data Mining (2nd Edition). Pearson, 2nd edition, 2018.

[74] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut

Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[75] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NeurIPS, 2018.

[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.

[77] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. In ICML, 2023.

[78] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. In NeurIPS, 2020.

[79] Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, and Chaowei Xiao. On the exploitability of reinforcement learning with human feedback for large language models. arXiv preprint arXiv:2311.09641, 2023.

[80] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Backdoor attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021.

[81] Wenqiang Wang, Chongyang Du, Tao Wang, Kaihao Zhang, Wenhan Luo, Lin Ma, Wei Liu, and Xiaochun Cao. Punctuation-level attack: Single-shot and single punctuation can fool text models. In NeurIPS, 2023.

[82] Maurice Weber, Xiaojun Xu, Bojan Karlaš, Ce Zhang, and Bo Li. Rab: Provable robustness against backdoor attacks. In $S \& P, 2023$.

[83] Shaokui Wei, Mingda Zhang, Hongyuan Zha, and Baoyuan Wu. Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples. arXiv preprint arXiv:2307.10562, 2023.

[84] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In NeurIPS, 2021.

[85] Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, and Ting Wang. Defending pre-trained language models as few-shot learners against backdoor attacks. In NeurIPS, 2023.

[86] Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, and Jie Ding. A unified detection framework for inference-stage backdoor defenses. In NeurIPS, 2023.

[87] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for large language models. In ICLR, 2024 .

[88] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. arXiv preprint arXiv:2305.14710, 2023.

[89] Jiaqi Xue, Yepeng Liu, Mengxin Zheng, Ting Hua, Yilin Shen, Ladislau Boloni, and Qian Lou. Trojprompt: A black-box trojan attack on pre-trained language models. In NeurIPS, 2023.

[90] Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, and Xiangyu Zhang. Parafuzz: An interpretability-driven technique for detecting poisoned samples in nlp. In NeurIPS, 2023.

[91] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models. arXiv preprint arXiv:2103.15543, 2021.

[92] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rap: Robustness-aware perturbations for defending against backdoor attacks on nlp models. arXiv preprint arXiv:2110.07831, 2021.

[93] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against nlp models. In $A C L, 2021$.

[94] Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, Mathias Humbert, Pascal Berrang, and Yang Zhang. Data poisoning attacks against multimodal encoders. In ICML, 2023.

[95] Yi Zeng, Si Chen, Won Park, Z Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. arXiv preprint arXiv:2110.03735, 2021.

[96] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[97] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NeurIPS, 2015.

[98] Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, and Maosong Sun. Red alarm for pre-trained models: Universal vulnerability to neuron-level backdoor attacks. Machine Intelligence Research, 2023.

[99] Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and Jie Fu. Prompt as triggers for backdoor attack: Examining the vulnerability in language models. arXiv preprint arXiv:2305.01219, 2023.

[100] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023.

[101] Biru Zhu, Yujia Qin, Ganqu Cui, Yangyi Chen, Weilin Zhao, Chong Fu, Yangdong Deng, Zhiyuan Liu, Jingang Wang, Wei Wu, et al. Moderate-fitting as a natural backdoor defender for pre-trained language models. In NeurIPS, 2022.

[102] Mingli Zhu, Shaokui Wei, Hongyuan Zha, and Baoyuan Wu. Neural polarizer: A lightweight and effective backdoor defense via purifying poisoned features. In NeurIPS, 2023.
