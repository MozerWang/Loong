# Self-Guiding Exploration for Combinatorial Problems 

Zangir Iklassov<br>MBZUAI

Yali Du<br>King's College London

Farkhad Akimov<br>MBZUAI

Martin Takáč<br>MBZUAI


#### Abstract

Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over $\mathbf{2 7 . 8 4 \%}$ in CP optimization performance. Additionally, SGE achieves a $2.46 \%$ higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic). Our implementation is available online 1


## 1 Introduction

Large Language Models (LLMs) have emerged as powerful tools capable of executing reasoning tasks across various domains, including arithmetic, commonsense, and symbolic reasoning Brown et al. [2020], Thoppilan et al. [2022], Chowdhery et al. [2023], Ouyang et al. [2022]. These models may leverage prompting techniques such as Exploration-of-Thought Wei et al. [2022b], Kojima et al. [2022], Yao et al. [2023], Decomposition Zhou et al. [2023], Khot et al. [2022], and Refinement Madaan et al. [2023] to break down and solve various tasks in a step-by-step manner. Recent research has been directed towards extending these techniques to tackle more sophisticated optimization challenges Yang et al. [2024]. Combinatorial problems (CPs) may represent a category of these complex optimization tasks, associated with intricate computational challenges.

Combinatorial Problems are characterized by their NP-hardness and inherent complexity, which result in an exponential growth in the number of potential solutions. This complexity presents substantial challenges in the research Oroojlooyjadid et al. [2020], Nazari et al. [2018], Iklassov et al. [2023b a]. CPs are especially crucial in sectors that require efficient logistics, planning, and scheduling. Currently, the dominant approach in these industries involves metaheuristic methods. These methods combine various simple but fast heuristics to effectively tackle CPs within specific constraints. Nonetheless, the effectiveness of these heuristics can vary significantly depending on the CP task and its associated constraints, necessitating a customized selection of heuristics to achieve optimal performance. In the meantime, research on exploring LLMs to solve CPs reveals substantial gaps. While recent advancements indicate the effectiveness of LLMs in various reasoning tasks Wei et al. [2022a], Zhang et al. [2023], Suzgun et al. [2023], Zhou et al. [2023], their application to CPs[^0]has been minimal. Research such as that conducted in Liu et al. [2024], Masoud et al. [2024], Yang et al. [2024] suggests that current generative models can address smaller instances of the Traveling Salesman Problem (TSP). However, as problem sizes increase, existing prompting strategies begin to yield inadequate responses, underscoring the need for more sophisticated prompting methods. Moreover, there is a notable scarcity of research addressing other complex CPs, particularly the Vehicle Routing and Job Scheduling Problems, which pose significant challenges in logistics, planning industries, and operations research.

In this work, we introduce a novel prompting strategy: self-guiding exploration (SGE), designed to enhance the problem-solving process for CPs. This algorithm works as a combination of explorationof-thought, decomposition, and refinement prompting methods. The SGE approach autonomously generates multiple thought trajectories for a given $\mathrm{CP}$, each trajectory representing a specific heuristic to tackle the given task. Each trajectory is then decomposed into subtasks, which are executed one by one, and their outputs are refined and combined into a final solution. Unlike the task-specific prompts utilized in other methods, SGE employs general-purpose prompts, allowing for the adaptive use of specific heuristic solutions tailored to various CPs, such as the Hungarian heuristic for the assignment problem and the Nearest Neighbor heuristic for the vehicle routing problem. Essentially, SGE acts as a versatile metaheuristic capable of identifying, combining, and refining task-specific heuristics for individual CP tasks.

Our work makes following contributions. Firstly, we present a novel investigation into the application of large language models for solving combinatorial problems. Secondly, we introduce a new prompting strategy, SGE, that autonomosly generates thought trajectories, splits them into subtasks and refines the answers. Thirdly, we demonstrate that SGE outperforms existing prompting strategies such as Chain-of-Thought, Decomposition, and Self-Refinement, improving CP optimization performance by $27.84 \%$. Lastly, we validate the applicability of SGE across other reasoning tasks, including arithmetic, commonsense, and symbolic tasks, where our method achieves a $2.46 \%$ higher accuracy than the best existing results.

## 2 Related Work

CP via Classical Approach. In classical research, combinatorial problems are predominantly tackled using heuristic and metaheuristic methods specifically crafted for particular tasks. Notable examples include the Ant-Colony Optimization and Tabu Search methods for addressing the Vehicle Routing Problem Pichpibul and Kawtummachai [2013], Goel et al. [2019], Li and Li [2020], the Shortest Processing Time and Most Work Remaining Heuristics for Job Scheduling Problems Sels et al. [2012], and the Hungarian Algorithm for the Assignment Problem Amponsah et al. [2016]. These approaches are favored in industrial settings due to their simplicity and speed. However, they need to be individually tailored to each task and its constraints' setting. In contrast, exact solvers such as Google-OR-Tools Google [2023] offer general and precise solutions, but their applicability is often limited to smaller-scale problems due to the inherent NP-hardness of combinatorial problems.

CP via Learning-Based Approach. In AI literature, Reinforcement Learning (RL) has been a prominent approach for tackling combinatorial problems since the 1990s Mahadevan et al. [1997], Mahadevan and Theocharous [1998], Zhang and Dietterich [1995], Gabel and Riedmiller [2008], Aydin and Oztemel [2000]. The integration of deep learning, particularly through innovations like Pointer Networks, has significantly enhanced RL's capability to handle more complex combinatorial tasks Vinyals et al. [2015], Bello et al. [2016]. Further advancements involve the use of Transformer networks Deudon et al. [2018], Vaswani et al. [2017], Kool et al. [2018], with notable applications in solving the Vehicle Routing Problem|Nazari et al. [2018], Iklassov et al. [2023b]. Despite these advances, RL-based methods often still do not exceed the performance of traditional heuristics, especially when scalability and accurate state representation are required Wang et al. [2021], Mao et al. [2019], Zhang et al. [2020], Sun et al. [2021].

CP with LLMs. To date, Liu et al. [2024], Masoud et al. |2024], Yang et al. |[2024] are the only studies investigating the use of LLMs (GPT-3.5 and GPT-4) models on the Traveling Salesman Problem through iterative prompting, requesting the model to refine a candidate solution repeatedly. There remains a significant research gap in applying more advanced prompting strategies and exploring LLMs for other combinatorial problems such as vehicle routing and job scheduling.

Prompting Strategies. The expressive capabilities of direct prompting in Large Language Models are theoretically limited to the complexity class TC ${ }^{0}$ Merrill and Sabharwal [2023]. To effectively address combinatorial problems with LLMs, sophisticated prompting strategies are required. One
basic approach is the Chain-of-Thought (CoT) prompting, introduced in Wei et al. [2022c], which encourages LLMs to articulate intermediate "thoughts" that inform the generation of the final output. This technique has given rise to advanced variations, including Self-consistency with CoT (CoT-SC), Tree-of-Thoughts (ToT), and Graph-of-Thought methods Wang et al. [2022], Yao et al. [2023], Zhang et al. [2024]. Additionally, decomposition prompting strategies can be employed Zhou et al. [2022], Khot et al. [2022], since they simplify complex tasks into smaller, manageable subtasks via symbolic programs or structured algorithms, thus improving the performance of LLMs. In our experiments, we found these techniques to be insufficient, leading us to propose the Self-Guiding Exploration method as a more effective solution for tackling combinatorial problem tasks.

## 3 Preliminaries

We provide an overview of combinatorial problems, highlighting their inherent complexity with the classic example of the Traveling Salesman Problem (TSP) and an example of a combinatorial problem formulation in a prompt for use by a Large Language Model (LLM).

Combinatorial Problems. Combinatorial problems involve decision-making processes where the goal is to assign binary decision variables $x \in 0,1$ in order to optimize a cost function $g\left(x_{1}, \ldots, x_{n}\right)$, subject to task-specific constraints. A classic example of such a problem is the TSP. In the TSP, given a list of $n$ cities and the distances $d_{i j}$ between cities $i, j$, the objective is to determine the shortest possible route that visits each city exactly once and returns to the starting city. $x_{i j}$ is used as the action variable, indicating whether the route progresses from city $i$ to city $j$. The cost function to minimize in TSP is $g(x)=\sum_{i=1}^{n} \sum_{j=1}^{n} d_{i j} x_{i j}$, under the condition that all cities visited exactly once $\sum_{i=1}^{n} x_{i j}=1$ and $\sum_{j=1}^{n} x_{i j}=1$ for all $i, j$. Combinatorial problems are generally categorized as NP-hard due to their inherent computational complexity. For instance, a TSP with $n$ cities presents $(n-1)$ ! possible routes, rendering the evaluation of all potential solutions impractical and exceedingly time-consuming as $n$ increases.

Prompting Combinatorial Problems in LLMs. To use LLM for solving CP tasks, we define $f$ as the interface function of a generative LLM model, which accepts high-dimensional discrete input tokens and generates outputs within the same token space ( $f: W \mapsto W$ ). For each combinatorial problem task, the input $Q$ to the LLM can be explicitly defined in a textual format. This description delineates the specific goal alongside a list of variables tailored to the task at hand. For instance, the objective of the Traveling Salesman Problem (TSP) can be textually articulated as "Find a route that minimizes the total travel distance, visits each city exactly once, and starts and ends in the same city." Subsequently, the variables, such as the distances between cities $d_{i j}$, are provided in a format such as "The distance between city $i$ and city $j$ is [number]", laying out all necessary parameters for the model to process and generate solutions. The model will then process this structured input $Q$ to produce the corresponding solution answer $A$, where both $Q$ and $A$ are within token space $W$; formally, $A=f(Q)$. Given the inherent complexity of combinatorial problems, direct zero-shot prompting $f(Q)$ is insufficient. Consequently, we propose a self-guiding exploration algorithm that employs metaheuristic-like strategies to effectively solve CP tasks.

## 4 Method

In this section, we introduce Self-Guiding Exploration (SGE) method and provide a detailed explanation of its algorithm designed to tackle combinatorial problems. The method (Fig 1), inspired by metaheuristic approaches, synthesizes multiple heuristic methods. It generates various thought trajectories, with each trajectory representing a specific heuristic approach. These trajectories are then integrated to form the final solution. To overcome the challenges of executing complex heuristics through LLMs in one step, our algorithm utilizes a decomposition strategy. This approach breaks down each trajectory into smaller, more manageable subtasks, enabling the solution to progress through sequential, simpler steps. This general-purpose algorithm is tailored to adapt to a wide range of combinatorial problems without the constraints of task-specific exemplars for few-shot solution generation.

![](https://cdn.mathpix.com/cropped/2024_06_04_d4024aaf604211c49e56g-04.jpg?height=449&width=1396&top_left_y=239&top_left_x=362)

Figure 1: Self-Guiding Exploration. The generative model autonomously addresses a combinatorial problem task $Q$ through a five-phase process: (1) Exploration of $N$ solution trajectories, where each trajectory offers potential solutions; (2) Decomposition of these trajectories into $K$ subtasks, outlining specific steps for each method; (3) Resolution of each subtask, executing the outlined steps; (4) Feedback and Refinement, where feedback is gathered and used to refine each subtask; (5) Integration of all trajectories into a consolidated final solution $A$. Distinct from traditional exploration/decomposition techniques, $\mathrm{SGE}(\mathrm{Q})$ functions entirely autonomously, eliminating the reliance on task-specific queries or manually created thought exemplars. This independence makes it universally applicable to all $\mathrm{CP}$ tasks without necessitating modifications.

```
Algorithm 1 Self-Guiding Exploration algorithm - $S G E(\cdot)$
Require: query $Q$, model $f$, meta-prompts $Z$, maximum recursion depth $D$
    $Q^{\mathbb{N}}=f\left(Q, Z_{\text {explore }}\right) \quad \triangleright$ Explore method trajectories
    for iteration $\mathrm{n} \in 1,2, \ldots, N$ do
        $Q_{\mathbb{K}}^{n}=f\left(Q, Q^{n}, Z_{\text {decomp }}\right) \quad \triangleright$ Decompose trajectory subtasks
        for iteration $\mathrm{k} \in 1,2, \ldots, K$ do
            if $f\left(Q_{k}^{n}, Z_{\text {check }}\right)$ then $\quad \triangleright$ Check if subtask is simple
                $T_{k}^{n}=f\left(Q, T_{k-1}^{n}, Q_{k}^{n}\right) \quad \triangleright$ Execute subtask and get thought
            else
                $T_{k}^{n}=S G E\left(T_{k-1}^{n} \| Q_{k}^{n}, f, Z\right) \quad \triangleright$ Recursive call of subtask
            end if
            $Q_{k_{\text {feedback }}^{n}}^{n}=f\left(Q, Q_{k}^{n}, T_{k}^{n}, Z_{\text {feedback }}\right) \quad \triangleright$ Get feedback query
            $T_{k}^{n}=f\left(Q, T_{k}^{n}, Q_{k_{\text {feedback }}}^{n}\right) \quad \triangleright$ Refine thought
        end for
    end for
    $A=f\left(Q, T_{K}^{1}, \ldots, T_{K}^{N}, Z_{\text {integrate }}\right) \quad \triangleright$ Get answer
```


### 4.1 Algorithm

The proposed method's algorithm is segmented into five distinct phases, as outlined in Algorithm 1 These phases include exploring thought trajectories, decomposing each trajectory into subtasks, resolving each subtask to generate thoughts, obtaining feedback and refining the thoughts, and finally integrating all thoughts to formulate the answer. Each thought here represents one completed subtask. Exploration. During the exploration stage, the model tackles the overarching problem $Q$ by engaging with exploration meta-prompt. This prompt $Z_{\text {explore }}$ is structured as: "List all possible methods to solve this problem. Return them separated by new lines." This prompt stimulates the model to enumerate potential methodologies pertinent to $Q$. The sequence is then divided into task-specific trajectories of queries $Q^{n}$ each incorporating a method to address $Q$ :

$$
Q^{\mathbb{N}}=f\left(Q, Z_{\text {explore }}\right)
$$

Decomposition. Following exploration, each trajectory query $Q^{n}$ is processed through the model to break down the trajectory into actionable steps. The decomposition meta-prompt $Z_{\text {decomp }}$ is formulated as: "List all steps to use the method. Return them separated by new lines." This leads to
the generation of subtask queries that operationalize the trajectory method:

$$
Q_{\mathbb{K}}^{n}=f\left(Q, Q^{n}, Z_{\text {decomp }}\right)
$$

Subtask Resolution. Post-decomposition, the subtask queries $Q_{\mathbb{K}}^{n}$ are each split into $K$ individual queries and processed by the model to generate thoughts. The model initially evaluates if the task is easily solvable using the meta-prompt $Z_{\text {check }}$ : "Is this problem easily solvable? Return yes or no": $f\left(Q_{k}^{n}, Z_{\text {check }}\right)$. If the response is affirmative, the model executes the subtask query $Q_{k}^{n}$ to generate a new thought:

$$
T_{k}^{n}=f\left(Q, T_{k-1}^{n}, Q_{k}^{n}\right)
$$

Otherwise, the model engages a recursive instance of the self-guiding exploration algorithm on $Q_{k}^{n}$ instead of the main task $Q$ to navigate and decompose the complex subtask, producing:

$$
T_{k}^{n}=f\left(Q, T_{k}^{n}, Q_{k_{\text {feedback }}}^{n}\right)
$$

Feedback and Refinement. In this stage, the model utilizes an additional meta-prompt $Z_{\text {feedback }}$ - "Give feedback to the proposed solution" - to generate feedback queries $Q_{k_{\text {feedback }}}^{n}=$ $f\left(Q, Q_{k}^{n}, T_{k}^{n}, Z_{\text {feedback }}\right)$. This guides the model in refining the initial responses through reevaluation and enhancement of the thoughts:

$$
T_{k}^{n}=f\left(Q, T_{k}^{n}, Q_{k_{\text {feedback }}}^{n}\right)
$$

Integration. Upon completion of all trajectories and their associated subtasks, the model employs

![](https://cdn.mathpix.com/cropped/2024_06_04_d4024aaf604211c49e56g-05.jpg?height=43&width=1383&top_left_y=1060&top_left_x=371)
amalgamate the last thoughts into a definitive solution answer:

$$
A=f\left(Q, T_{K}^{1}, \ldots T_{K}^{N}, Z_{\text {integrate }}\right)
$$

SGE draws inspiration from metaheuristic methods used to solve combinatorial problem tasks. Yet, due to its general-purpose nature and meta-prompts, it is also suitable for other tasks, beyond CPs. Essentially, it integrates elements of exploration-of-thought, decomposition, and refinement prompting strategies, but it does so without relying on task-specific prompts or solution exemplars. For additional information on these prompting strategies, see Section A. 1

## 5 Experiments

This section details the experimental setup and presents the results of our proposed method applied to combinatorial problem tasks, as well as its performance on other reasoning tasks commonly explored in LLM research.

### 5.1 Setup

CP tasks. The experiments were conducted on six combinatorial tasks: Assignment Problem, Knapsack Problem, Bin Packing Problem, Traveling Salesman Problem, Vehicle Routing Problem, and Job Scheduling Problem. The Assignment Problem, classified as P-complete, can be optimally solved using the Hungarian Algorithm. In contrast, the other tasks are NP-hard and ordered by increasing complexity. For a more detailed discussion of these CP tasks, refer to Section A.2. We included five distinct problem sizes, involving 5, 10, 15, 20, and 30 elements (nodes) such as cities in the TSP/VRP. To facilitate these experiments, a dataset was created, comprising 100 randomly generated instances for each problem size. These instances were characterized by uniformly distributed variables, such as the positioning of cities in TSP/VRP or bin volume in the Bin Packing Problem, over an interval from 0 to 100. The experiments utilized an NVIDIA A100 SXM 40GB GPU, paired with two AMD EPYC 7742 CPUs (8 cores each) and 256GB RAM. On average, solving a problem instance with SGE takes between 1 and 3 minutes, depending on the LLM utilized.

Baselines. We utilized four baseline prompting methods: Input-Output (IO) Direct Prompting, Chain-of-Thought Prompting, Self-Refine (Refine) Prompting, and Decomposition Prompting. The Input-Output (IO) approach involves a single prompt where the model is asked to provide a solution directly, without complex prompting. In this approach, we generate $N$ sample candidates by repeatedly prompting the model with the same query $Q, N$ times. The responses are then aggregated through majority voting to identify the most common solution among the $N$ outputs. We employ the Self-Refine (Refine) method Madaan et al. [2023], which includes a feedback-refinement procedure

Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).

|  | GPT-4 |  |  |  |  | Gemini-1.5 |  |  |  |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Task | CoT | Refine | Decomp | Ours |  | CoT | Refine | Decomp | Ours |
| Assignment | 11.46 | 14.47 | 33.80 | $\mathbf{4 1 . 3 3}$ |  | 11.66 | 13.98 | 31.94 | $\mathbf{4 0 . 4 6}$ |
| Knapsack | 15.37 | 17.16 | 51.95 | $\mathbf{7 0 . 3 9}$ |  | 13.85 | 16.85 | 48.62 | $\mathbf{6 5 . 8 7}$ |
| Bin Packing | 14.06 | 17.12 | 39.57 | $\mathbf{7 4 . 7 2}$ |  | 11.89 | 15.43 | 35.74 | $\mathbf{6 7 . 6 3}$ |
| Travelling Salesman | 13.64 | 15.75 | 38.49 | $\mathbf{7 2 . 1 0}$ |  | 14.34 | 15.90 | 36.36 | $\mathbf{6 8 . 0 9}$ |
| Vehicle Routing | 14.27 | 16.94 | 36.73 | $\mathbf{7 1 . 9 2}$ |  | 11.88 | 15.13 | 33.59 | $\mathbf{6 8 . 0 2}$ |
| Job Scheduling | 13.84 | 16.37 | 38.20 | $\mathbf{7 5 . 3 3}$ |  | 13.41 | 15.75 | 36.36 | $\mathbf{6 7 . 8 9}$ |

that aligns closely with phase four of SGE. Additionally, we use the zero-shot Chain-of-Thought method Kojima et al. [2022], which is the basic technique among Exploration-of-Thought methods. Lastly, we implemented the Decomposition method as described in Zhou et al. [2023]. In our experiments, these baseline methods were tested across a range of five LLM models including GPT-4, GPT-3.5 by OpenAI, Gemini-1.5 by Google, and the Llama-2 series from Meta, which includes models with 70 billion and 7 billion parameters. We did not include prompting methods previously used in Liu et al. [2024], Masoud et al. [2024], Yang et al. [2024], as their prompting strategies showed inferior results compared to the zero-shot Chain-of-Thought approach when tested with our data.

Metrics. In our study, each method's performance is evaluated relative to IO (Input-Output) prompting. To quantify the improvement, we first measure the solution cost $g_{i o}$ for each combinatorial problem task using IO prompting (e.g., for the TSP, $g_{i o}=\sum_{i=1}^{n} \sum_{j=1}^{n} d_{i j} x_{i j}$ ). We then calculate the cost $g_{\text {method }}$ using alternative methods. The percentage improvement is computed as $100 \times \frac{g_{i_{0}}-g_{\text {method }}}{g_{i_{o}}}$. For problems of smaller sizes, we are able to obtain optimal solutions using the Google-OR-Tools solver through a brute force approach. In such instances, we measure the cost of the optimal solution $g_{o p t}$ and determine the optimality gap as $100 \times \frac{g_{\text {method }}-g_{o p t}}{g_{o p t}}$.

### 5.2 Results on CP Tasks

To evaluate the general performance of SGE on combinatorial problems, we conducted experiments comparing performance improvement to IO of SGE and CoT, Decomposition, and Refinement baselines using GPT-4 and Gemini-1.5 LLM models. Table 1 gives the results on six combinatorial problem tasks. The results analysis shows that the SGE method consistently outperforms CoT, Refine, and Decomposition methods across all tasks. Notably, the magnitude of improvement escalates with the increasing complexity of the problems, from polynomial to exponential. The margin with the second-best method, Decomposition, ranges from $7.53 \%$ for the Assignment Problem to $37.13 \%$ for the JSP. This trend suggests that the IO method may struggle with the computational demands of NP-hard problems, where more sophisticated strategies like SGE provide significant advantages. For a comprehensive view of all experimental results, see Section A.6.

To qualitatively evaluate the performance of the Exploration, Decomposition, and Refinement phases of SGE method, we assessed the LLM outputs for each phase across five random instances of each combinatorial problem task. Figure 2illustrates an example of how our method addresses the TSP, showcasing outputs during each of the three phases of SGE. In the Exploration phase, the first box of Figure 2 displays how LLM $f$ generates a list of potential algorithms suitable for solving the TSP, such as heuristic approaches like Nearest Neighbor, metaheuristic techniques like Ant Colony, and Mixed Integer Linear Programming (MILP) method. This phase adapts to different combinatorial problems by suggesting tailored algorithms, like the Hungarian algorithm for the Assignment problem, Greedy algorithms for the Knapsack problem, and Clustering methods for the Vehicle Routing Problem (VRP). Each list of algorithms forms the foundation for generating diverse candidate solutions tailored to each specific problem. The Decomposition phase, depicted in the second box of Figure 2, breaks down each identified algorithm into specific subtasks. This example shows the decomposition of the Nearest Neighbor algorithm for the TSP, where the initial

## Example of Exploration of TSP

- Prompt: You are given a list of points with coordinates: $(1):(0,49) ;(2):(26,41)$ $(3):(2,29) ;(4):(49,15) ;(5):(0,23)$; We need to find the route that has a minimum total length. List all possible methods to solve this problem. [Return conditions]


## - Exploration Output:

- Nearest Neighbor Algorithm
- Christofides' Algorithm
- Ant Colony Optimization
- Integer Linear Programming

Example of Decomposition of TSP

- Prompt: [Problem definition]. Nearest Neighbor Algorithm. List all steps to use the method. [Return conditions].
- Decomposition Output:
- Select a Starting Point
- Find the Nearest Neighbor
- Update the Tour
- Repeat
- Return to the Starting Point

Example of Refinement of TSP

- Prompt: [Problem definition]. [Nearest Neighbor candidate solution]. Give feed back to the proposed solution.
- Refinement Output:
- Choose two edges
- Remove these edges and reconnect the two paths that were created in such a way that the new connections do not cross over each other.
- Check if the new tour has a shorter distance than the previous tour. If it does, keep the change. Otherwise, revert to the previous tour.
- Repeat this process until no improvements can be made.

Figure 2: Example of SGE inference across the Exploration, Decomposition, and Refinement phases for the Traveling Salesman Problem. The figure displays three boxes, each illustrating the prompt structure and corresponding example output for each phase.

![](https://cdn.mathpix.com/cropped/2024_06_04_d4024aaf604211c49e56g-07.jpg?height=437&width=971&top_left_y=1397&top_left_x=577)

Figure 3: Effect of Problem Size on Performance Improvement relative to the IO solution using gpt-4 w/ code interpreter. The analysis spans problem instances of varying sizes, systematically presented from the smallest to the largest, specifically ranging from $n=5$ to $n=20$ nodes. Results are organized to highlight the impact of increasing problem complexity on the effectiveness of the solution.

subtasks are simple enough for direct processing by model $f$. However, more complex tasks, such as loops, undergo further decomposition using SGE in a recursive manner, with computational or programming tasks being handled using Python within models like GPT-4 and Gemini-1.5 equipped with a Code Interpreter. Finally, the Refinement phase, illustrated in the third box of Figure 2 , focuses on enhancing the candidate solutions developed in the previous stage. This example pertains to refining a solution derived from the Nearest Neighbor algorithm for the TSP by implementing the 2-opt algorithm. Renowned for its effectiveness in TSP and VRP contexts, the 2-opt algorithm optimizes the initial solution to find locally optimal solutions within a specific neighborhood, thus improving the overall quality of the candidate solutions. This example shows that SGE method adapts its approach to suit different combinatorial problems, finding a special set of heuristics for each task.

Table 2: Optimality gap of prompting methods using gpt-4 w/ code interpreter. The results are represented as performance percentage difference compared to optimal solutions (the smaller it is, the better).

| ize | Method | \| Assignment | Knapsack | Bin Packing | TSP | VRP | JSP |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| y <br> 0 <br> z <br> $n$ | IO | 45.45 | 90.10 | 108.2 | 100.3 | 102.0 | 105.3 |
|  | CoT | 39.33 | 66.88 | 78.24 | 81.15 | 78.17 | 79.41 |
|  | Refine | 3642 | 61.98 | 77.40 | 71.62 | 72.49 | 71.72 |
|  | Decomp | 1. |  | 40.1 | 43.62 | 40.65 | 44.15 |
|  | Ours | 2.500 | 8.050 | 9.060 | 8.27 | 11.92 | 9.300 |
| II <br> 0 <br> $\vdots$ <br> $z$ <br> $\infty$ | IO | 46.84 | 103.5 | 112.8 | 116.9 | 116.3 | 108.2 |
|  | CoT | 3 | 73 | 85.0 | 89.01 | 89.48 | 85.21 |
|  | Refi: |  |  |  | 85.59 | 83.31 | 78.43 |
|  | Decomp | 18.4 |  | 52. | 53.48 | 54.43 | 49.81 |
|  | Ours |  |  | 20. | 15.19 | 19.65 | 21.26 |
| $\tilde{I}$ <br> 0 <br> 0 <br> $Z$ <br> $\sim$ | IO | 49.11 |  | 120 | 121.6 | 118.5 | 117.6 |
|  | CoT |  |  |  | 86.84 | 90.05 | 89.29 |
|  | Refins |  |  | 82.2 | 88.57 | 88.40 | 87.02 |
|  | Decor |  | 35. |  | 57.51 | 59.19 | 56.01 |
|  | Ours | 11.26 | 16.82 | 22.38 | 16.12 | 24.00 | 22.86 |

![](https://cdn.mathpix.com/cropped/2024_06_04_d4024aaf604211c49e56g-08.jpg?height=431&width=973&top_left_y=964&top_left_x=573)

Figure 4: Effect of Model Choice on Performance Improvement relative to the IO solution.

Effect of Problem Size on SGE Performance. To evaluate the effect of problem size on SGE performance, we conducted experiments on all six tasks with input sizes of $5,8,12,15$, and 20 nodes using the GPT-4 model. Figure 3 gives the results of these experiments. The results analysis shows that generally, an increase in problem complexity, as determined by the size of the problem input, negatively influences performance improvement; larger problem sizes result in diminished performance improvement of the SGE method compared to IO. Specifically, tasks with 20 input nodes consistently exhibit lower performance improvements relative to the IO method than tasks with 5 input nodes. However, when comparing less disparate sizes, such as 8 and 12 nodes, the differential impact on performance is less pronounced and can occasionally be positive.

Gap Between SGE Performance and the Global Optimum. To evaluate the gap between SGE solutions and global optimum solutions, we conducted experiments on small problem sizes involving 5,8 , and 12 nodes, utilizing the brute force method via Google-OR-Tools to determine the global optimum. Figure 2 provides the results of these experiments in terms of the percentage gap between the performance of SGE and optimal solutions. The results analysis shows that generally, the SGE method exhibits a smaller optimality gap across all tasks when compared to baseline methods. This advantage is particularly pronounced for more complex problems such as Bin Packing, TSP, VRP, and JSP. For instance, SGE achieves a $12.16 \%$ smaller optimality gap on the Assignment task than the next best Decomposition method and a $34.85 \%$ smaller gap on the JSP.

Effect of LLM Selection on SGE Performance. To evaluate the impact of model selection on SGE performance, we conducted experiments comparing different models, including GPT-4, Gemini-1.5, GPT-3.5, Llama-2-70b, and Llama-2-7b models. Figure 4 provides the results of these experiments. The results analysis shows that GPT-4 and Gemini-1.5 demonstrate significantly better performance compared to other models across all tasks. A notable feature of both models is the integration of a Code Interpreter (CI) tool, which appears crucial for combinatorial problem tasks as it enables the

Table 3: Results for reasoning tasks using gpt-4 with a code interpreter are presented as accuracies on benchmark test sets.

| Method | Arithmetic |  |  |  | Commonsense |  |  | $\frac{\text { Symbolic }}{\text { LastLetter }}$ | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\text { AQUA }}$ | GSM8K | SVAMP | ASDiv | StrategyQA | CSQA | $\overline{\text { ARC }}$ |  |  |
| IO Prompting | 67.30 | 87.04 | 88.34 | 90.10 | 78.40 | 81.14 | 87.52 | 81.98 | 82.73 |
| CoT Promptil | 69.57 | 89.76 | $91.5 \delta$ | 93.32 | 81.16 | 84.15 | 90.80 | 85.18 | 85.69 |
| Refin | 69.68 | 89.80 | 91.00 | 93.10 | 81.26 | 83.50 | 91.09 | 84.82 | 85.53 |
| Decomp Prompting | 69.99 | 91.85 | 92.16 | 94.08 | 82.08 | 84.88 | 91.76 | 85.64 | 86.43 |
| Ours | 74.63 | 97.35 | 98.16 | 97.24 | 83.49 | 85.68 | 93.28 | 86.96 | 89.60 |

Table 4: Performance vs Efficiency of prompting methods using gpt-4 w/ code interpreter. The results are represented as performance percentage improvement compared to IO solution and number of model $f$ calls.

| Method | Performance Improvement | Function Calls |
| :--- | :---: | :---: |
| CoT | 13.77 | 58.32 |
| Refine | 16.30 | 58.32 |
| Decomp | 39.79 | 31.04 |
| Ours | 67.63 | 58.32 |

models to execute generated code and evaluate solution performance directly. In contrast, models lacking a CI tool, such as GPT-3.5, Llama-2-70b, and Llama-2-7b, exhibit poorer outcomes, with GPT-3.5 slightly outperforming the Llama models. The comparison between Llama models indicates that the size of the model with 70 billion versus 7 billion parameters does not significantly influence performance. This suggests that model size alone does not guarantee substantial performance improvements in combinatorial tasks.

Trade-off Between Performance and Cost in SGE. To evaluate the cost-effectiveness of different methods, we conducted experiments to compare the average performance improvement per method against the average number of LLM calls utilized to solve each combinatorial problem instance. Table 4 gives the results of these experiments, where the number of function calls in CoT and Refine methods was explicitly controlled to make them equal to SGE function calls. The results analysis shows that the SGE method achieves a $27.84 \%$ better performance compared to the Decomposition method but requires $87.89 \%$ more function calls. Thus, while SGE offers superior performance, it does so at a marginally higher operational cost. Therefore, the application of this method is particularly justified in scenarios where performance gains are prioritized over cost efficiency.

### 5.3 Results on Reasoning Tasks

To evaluate the versatility of SGE in handling different types of tasks, we conducted experiments across eight datasets commonly referenced in LLM research, categorized into three distinct task types: arithmetic, commonsense reasoning, and symbolic reasoning. Table 3 gives the results of these experiments, with each dataset comprising train and test splits where SGE and baseline methods were applied to the test splits. The results analysis shows that the SGE method demonstrates incremental but consistently superior performance across all task categories. Notably, the method shows particular strength in arithmetic tasks, where it achieves an average improvement of $4.83 \%$, compared to $1.24 \%$ in commonsense reasoning tasks and $1.32 \%$ in symbolic reasoning tasks. This demonstrates the method's applicability and effectiveness across a diverse range of tasks, extending beyond combinatorial problems.

## 6 Conclusion

This study has explored the application of Large Language Models to Combinatorial Problems, a category of tasks known for their NP-hardness. Our research introduced a 'Self-Guiding Exploration' prompting strategy that effectively utilizes the inherent strengths of LLMs. By generating multiple
thought trajectories tailored to various CPs and autonomously decomposing them into manageable subtasks. Our findings confirm that SGE outperforms existing strategies, improving optimization performance by $27.84 \%$ and achieving a $2.46 \%$ higher accuracy in reasoning tasks. Notably, SGE shows a $34.85 \%$ smaller gap with the global optimum on complex tasks like the Job Sheduling Problem compared to baseline methods. These results underline the potential of advanced LLM strategies in complex problem-solving scenarios, suggesting that the right techniques can enhance the utility of LLMs in critical logistics and resource management applications.

Despite the performance improvements demonstrated by the SGE, several limitations have emerged that merit attention. Firstly, SGE performance depends on the choice of language model. Secondly, the operational costs associated with SGE are notably higher; it requires $87.89 \%$ more function calls than the Decomposition method. These issues present clear avenues for future research. Enhancing SGE's computational efficiency while maintaining its high performance could broaden its applicability and make it a more practical choice for a wider range of problems.

## References

Samuel Kwame Amponsah, Dominic Otoo, Said Salhi, and Ebenezer Quayson. Proposed heuristic method for solving assignment problems. American Journal of Operations Research, 06:436-441, 01 2016. doi: 10.4236/ajor.2016.66040.

M Emin Aydin and Ercan Öztemel. Dynamic job-shop scheduling using reinforcement learning agents. Robotics and Autonomous Systems, 33(2-3):169-178, 2000.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. URL http://jmlr.org/papers/v24/22-1144.html

Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International conference on the integration of constraint programming, artificial intelligence, and operations research, pages 170-181. Springer, 2018.

Thomas Gabel and Martin Riedmiller. Adaptive reactive job-shop scheduling with reinforcement learning agents. International Journal of Information Technology and Intelligent Computing, 24 (4):14-18, 2008.

Rajeev Kumar Goel, Raman Maini, and Sandhya Bansal. Vehicle routing problem with time windows having stochastic customers demands and stochastic service times: Modelling and solution. J. Comput. Sci., 34:1-10, 2019.

Google. Or-tools, 2023. URL https://developers.google.com/optimization

Zangir Iklassov, Dmitrii Medvedev, Ruben Solozabal Ochoa de Retana, and Martin Takác. On the study of curriculum learning for inferring dispatching policies on the job shop scheduling. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages 5350-5358. ijcai.org, 2023a. doi: 10.24963/IJCAI.2023/594. URL https://doi.org/10.24963/ijcai.2023/594.

Zangir Iklassov, Ikboljon Sobirov, Ruben Solozabal, and Martin Takác. Reinforcement learning approach to stochastic vehicle routing problem with correlated demands. IEEE Access, 11: 87958-87969, 2023b. doi: 10.1109/ACCESS.2023.3306076. URL https://doi.org/10. 1109/ACCESS. 2023.3306076

Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html

Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018.

Guoming $\mathrm{Li}$ and Junhua $\mathrm{Li}$. An improved tabu search algorithm for the stochastic vehicle routing problem with soft time windows. IEEE Access, 8:158115-158124, 2020.

Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language mode, 2024.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with selffeedback. ArXiv preprint, abs/2303.17651, 2023. URL https://arxiv.org/abs/2303.17651.

Sridhar Mahadevan and Georgios Theocharous. Optimizing production manufacturing using reinforcement learning. In FLAIRS conference, volume 372, page 377, 1998.

Sridhar Mahadevan, Nicholas Marchalleck, Tapas K Das, and Abhijit Gosavi. Self-improving factory simulation using continuous-time average-reward reinforcement learning. In Machine Learning Interantional Workshop, pages 202-210, 1997.

Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh. Learning scheduling algorithms for data processing clusters. In Proceedings of the ACM special interest group on data communication, pages 270-288. 2019.

Mahmoud Masoud, Ahmed Abdelhay, and Mohammed Elhenawy. Exploring combinatorial problem solving with large language models: A case study on the travelling salesman problem using gpt-3.5 turbo, 2024.

William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531-545, 2023. doi: 10.1162/tacl_a_00562. URLhttps://aclanthology.org/2023.tacl-1.31.

Mohammadreza Nazari, Afshin Oroojlooy, Lawrence V Snyder, and Martin Takáč. Reinforcement learning for solving the vehicle routing problem. In Conference on Neural Information Processing Systems, NeurIPS 2018, 2018

Afshin Oroojlooyjadid, Lawrence V Snyder, and Martin Takáč. Applying deep learning to the newsvendor problem. IISE Transactions, 52(4):444-463, 2020.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html

Tantikorn Pichpibul and Ruengsak Kawtummachai. A heuristic approach based on clarke-wright algorithm for open vehicle routing problem. The Scientific World Journal, 2013, 2013.

Veronique Sels, Nele Gheysen, and Mario Vanhoucke. A comparison of priority rules for the job shop scheduling problem under different flow time-and tardiness-related objective functions. International Journal of Production Research, 50(15):4255-4270, 2012.

Penghao Sun, Zehua Guo, Junchao Wang, Junfei Li, Julong Lan, and Yuxiang Hu. Deepweave: Accelerating job completion time with deep reinforcement learning-based coflow scheduling. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3314-3320, 2021.

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 13003-13051. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.findings-acl.824. URL https://doi.org/10.18653/v1/2023. findings-acl. 824 .

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. MeierHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz Søraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Díaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Huai hsin Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv preprint, abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998-6008, 2017.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015.

Libing Wang, Xin Hu, Yin Wang, Sujie Xu, Shijun Ma, Kexin Yang, Zhijun Liu, and Weidong Wang. Dynamic job-shop scheduling in smart manufacturing using deep reinforcement learning. Computer Networks, 190:107969, 2021.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. URL https://openreview.net/forum?id=yzkSU5zdwD. Survey Certification.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc., 2022b. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022c.

Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2024.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

Di Zhang, Dong Dai, Youbiao He, Forrest Sheng Bao, and Bing Xie. Rlscheduler: an automated hpc batch job scheduler using reinforcement learning. In SC20. International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15. IEEE, 2020.

Wei Zhang and Thomas G Dietterich. A reinforcement learning approach to job-shop scheduling. In IJCAI, volume 95, pages 1114-1120. Citeseer, 1995.

Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, and Yan Liu. Prompting with divide-and-conquer program makes large language models discerning to hallucination and deception, 2024.

Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview net/pdf?id=5NTt8GFjUHkr

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=WZH7099tgfM.
