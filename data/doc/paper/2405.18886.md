# Compressing Large Language Models using Low Rank and Low Precision Decomposition 

Rajarshi Saha<br>Stanford University<br>rajsaha@stanford.edu

Naomi Sagan<br>Stanford University<br>nsagan@stanford.edu

Varun Srivastava<br>Stanford University<br>vsriva@stanford.edu

Andrea J. Goldsmith<br>Princeton University<br>goldsmith@princeton.edu

Mert Pilanci<br>Stanford University<br>pilanci@stanford.edu


#### Abstract

The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces CALDERA - a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\mathbf{W}$ by approximating it via a lowrank, low-precision decomposition as $\mathbf{W} \approx \mathbf{Q}+\mathbf{L R}$. Here, $\mathbf{L}$ and $\mathbf{R}$ are low rank factors, and the entries of $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\mathbf{Q}+\mathbf{L R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\mathbf{L}$ and $\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. CALDERA obtains this decomposition by formulating it as an optimization problem $\min _{\mathbf{Q}, \mathbf{L}, \mathbf{R}}\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$, where $\mathbf{X}$ is the calibration data, and $\mathbf{Q}, \mathbf{L}, \mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of CALDERA are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa2 7B/70B and LlaMa-3 8B models using CALDERA outperforms existing posttraining LLM compression techniques in the regime of less than 2.5 bits per parameter. The implementation is available at: https://github.com/pilancilab/caldera.


## 1 Introduction

Large Language Models (LLMs) stand out due to their remarkable ability to generate human-like text, thereby supporting a diverse range of applications ranging from writing assistance to code generation. These models leverage vast datasets and significant computational resources to achieve their impressive functionality. The architecture of LLMs typically includes multiple layers, each with weight matrices essential for encoding various aspects of the training data - from simple syntactic patterns to complex semantic relationships. However, the substantial size of these trained models leads to high computational costs and considerable energy consumption during inference, which can be challenging for deployment in resource-constrained environments. As LLMs continue to expand in scale, compression techniques to reduce the memory and computational requirements of the models are becoming crucial to ensure their broad accessibility.

Due to the correlated nature of language syntax and semantics learned during training, often, the weight matrices of LLMs exhibit redundancy, which manifests as a low-rank structure. This redundancy suggests the potential for compression without substantial loss in performance. This
work introduces CALDERA: Calibration Aware Low-Precision DEcomposition with Low-Rank Adaptation, which compresses LLMs by leveraging the approximate low rank structure inherent in these weight matrices. Given a matrix $\mathbf{W} \in \mathbb{R}^{n \times d}$, CALDERA approximates it as $\mathbf{W} \approx \mathbf{Q}+\mathbf{L R}$, where $\mathbf{Q} \in \mathbb{R}^{n \times d}, \mathbf{L} \in \mathbb{R}^{n \times k}$ and $\mathbf{R} \in \mathbb{R}^{k \times d}$. Here, the left and right low rank factors, respectively $\mathbf{L}$ and $\mathbf{R}$, are tall and wide matrices, and $k$ is the target rank. Furthermore, the entries of $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$ are represented using low-precision formats with $\mathrm{B}_{\mathrm{Q}}, \mathrm{B}_{\mathrm{L}}$ and $\mathrm{B}_{\mathrm{R}}$ bits per entry, respectively.

Since the singular value profile (aka spectrum) of the weight matrices of an LLM follow a decaying profile as shown in Fig. 1, the low-rank factors $\mathbf{L}$ and $\mathbf{R}$ capture the effect of the large singular components of $\mathbf{W}$ with high fidelity. Moreover, the backbone $\mathbf{Q}$, which is quantized aggressively - for instance, using $\mathrm{B}_{\mathrm{Q}}=2$ bits, coarsely captures the essence of the moderately decaying and low singular components of $\mathbf{W}$. CALDERA substitutes each weight matrix $\mathbf{W}$ in an LLM, with its approximate lowprecision and low-rank decomposition $\mathbf{Q}+\mathbf{L R}$, resulting in a post-training quantization strategy that delivers stateof-the-art zero-shot performance. In addition, since usu-

![](https://cdn.mathpix.com/cropped/2024_06_04_bc47e5d834c845042b4fg-02.jpg?height=352&width=555&top_left_y=496&top_left_x=1186)

Figure 1: Decaying spectrum of weight matrices (aka, "approximate low-rank") ally $k \ll \min \{n, d\}$,implying that the total number of parameters in $\mathbf{L R}$ is much smaller compared to the number of entries in $\mathbf{W}$ (i.e., $k(n+d) \ll n d$ ), CALDERA can readily fine-tune (or "adapt") the low rank factors $\mathbf{L}$ and $\mathbf{R}$ in order to boost the zero-shot results.

### 1.1 Significance and Related Works

Recent efforts have explored various avenues for compression, including but not limited to weight pruning, quantization, and the use of parameter-efficient training methods - each approach offering distinct advantages and tradeoffs. This section briefly reviews the current methodologies, highlighting the contributions and limitations of some studies closely related to this work.

LLM Compression and Outlier Mitigation: Recent studies like SmoothQuant [41], OPTQ [8], QuIP [3], AQLM [7], and QuIP\# [35] consider the challenging regime of sub-4 bit post-training LLM quantization. These works collectively emphasize the need to manage the impact of outliers, i.e., weights with unusually high magnitudes. Accommodating outliers necessitates choosing the dynamic range (or scale) of a quantizer to be high, consequently increasing the quantization error. QuIP equalizes (and reduces) the weight matrices by using a randomized matrix transform, and subsequently, QuIP\# employs E8 lattice to make the weights more amenable to vector quantization. Both QuIP and QuIP\# use a refined variant of the column-wise quantization method proposed in OPTQ, wherein error feedback from previously quantized columns of a matrix is used to compensate for the error incurred while quantizing subsequent columns. CALDERA utilizes this diverse arsenal of strategies and builds on top of QuIP\#, while capitalizing on the approximate low-rank structure of LLM weight matrices. While it is possible to obtain even more aggressively compressed LLMs [24], this approach requires training from scratch, which is computationally demanding.

Parameter Efficient Fine-Tuning (PEFT): In a related yet distinct vein of work, PEFT methods have gained significant momentum, aiming to adapt LLMs to specific tasks without extensive computational overhead. Recent studies such as QLoRA [5], LoftQ [22], and LQ-LoRA [13] have explored the intersection of PEFT and quantization, demonstrating that fine-tuning through low-rank updates, as originally proposed in LoRA [16], can mitigate the performance losses due to quantization. Given that CALDERA yields a decomposition $\mathbf{Q}+\mathbf{L R}$, the low-rank components are particularly suitable for fine-tuning with any existing PEFT methods, thereby enhancing the zero-shot capabilities.

Low Rank Approximation: The rank- $k$ approximation of a matrix $\mathbf{A} \in \mathbb{R}^{n \times d}$ can be represented by the factorization $\mathbf{A} \approx \mathbf{L R}$, with $\mathbf{L} \in \mathbb{R}^{n \times k}$ and $\mathbf{R} \in \mathbb{R}^{k \times d}$, where $k \leq \min \{n, d\}$. Known as the Burer-Monteiro factorization, this method substantially decreases the number of parameters, thus reducing computational demands. Recent studies such as LoRD [18], ASVD [43], FWSVD [15], LASER [32], LQER [44], and ZeroQuant-V2 [42] have explored the efficacy of low-rank structures in LLM weights, treating low-rank factorization and quantization independently. In contrast, LPLR [30] approaches this by uniquely formulating a joint optimization problem for generic matrices, while simultaneously leveraging the equalization property of randomized transforms, as in [3]. CALDERA formally leverages this inherent low-rank structure for LLM compression along-
side existing frameworks such as QuIP\# [35] and LoftQ [22], providing additional flexibility for compression. Furthermore, rigorous theoretical guarantees are derived using a rank-constrained regression framework for obtaining a low precision and low-rank decomposition, thereby also analytically demonstrating its superiority over rank-agnostic strategies.

## 2 Problem Formulation

In a neural network layer, a weight matrix $\mathbf{W}$ transforms an input activation $\mathbf{x}$ into an output activation given by $\mathbf{W x}$. This transformation can be succinctly described using the matrix's singular value decomposition (SVD). For any matrix $\mathbf{A} \in \mathbb{R}^{n \times d}$, the SVD is $\mathbf{A}=\sum_{i} \sigma_{i} \mathbf{u}_{i} \mathbf{v}_{i}$, where $\sigma_{i}, \mathbf{u}_{i}, \mathbf{v}_{i}$ are the $i^{\text {th }}$ singular value and the corresponding left and right singular vectors, respectively. The impact of each singular component $\mathbf{u}_{i} \mathbf{v}_{i}$ on the matrix's transformation is determined by the magnitude of $\sigma_{i}$. Given that weight matrices exhibit a decaying singular value profile (Fig. 1), indicating an approximate low-rank structure, lesser contributing singular components can be pruned with minimal impact on the functionality of the matrix, ensuring minimal distortion in the output activations.

CALDERA approximates the weight matrix of a neural network, $\mathbf{W}$, as a low-precision, low-rank decomposition, $\mathbf{W} \approx \mathbf{Q}+\mathbf{L R}$, with all components $\mathbf{Q}, \mathbf{L}, \mathbf{R}$ in low-precision format. Unlike previous works such as $[13,22,43,44]$, which represent the low-rank factors $\mathbf{L}$ and $\mathbf{R}$ in highprecision (16 or 32-bit floating point), this work extends their representation to low-precision. This further reduces the memory footprint while preserving performance. Alternatively, for the same memory footprint, it allows the target rank $k$ to be higher, thereby capturing the low rank structure with higher fidelity by including more of the higher singular value components. The following paragraph formalizes this as a constrained optimization problem.

For a given quantizer, let $\mathbb{Q}$ denote the set of discrete quantization points in $\mathbb{R}$. For B-bit quantization, the cardinality of $\mathbb{Q}$ satisfies $\log _{2}|\mathbb{Q}| \leq \mathrm{B}$. Consider a matrix $\mathbf{W} \in \mathbb{R}^{n \times d}$. The goal of this work is to obtain an decomposition $\mathbf{W} \approx \mathbf{Q}+\mathbf{L R}$ by approximately solving the minimization problem

$$
\begin{equation*}
\min _{\mathbf{Q}, \mathbf{L}, \mathbf{R}}\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2} \quad \text { subject to } \quad \mathbf{Q} \in \mathbb{Q}_{\mathrm{Q}}^{n \times d}, \mathbf{L} \in \mathbb{Q}_{\mathrm{L}}^{n \times k}, \text { and } \mathbf{R} \in \mathbb{Q}_{\mathrm{R}}^{k \times d} \tag{1}
\end{equation*}
$$

Here, $\mathbb{Q}_{\mathrm{Q}}, \mathbb{Q}_{\mathrm{L}}$ and $\mathbb{Q}_{\mathrm{R}}$ denote the lattice codebooks used to quantize $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$, using $\mathrm{B}_{\mathrm{Q}}, \mathrm{B}_{\mathrm{L}}$ and $\mathrm{B}_{\mathrm{R}}$ bits, respectively. Furthermore, $\mathbf{X} \in \mathbb{R}^{m \times d}$ is a calibration matrix that aims to preserve the Frobenius norm error of the compressed layer output activations. If $\mathbf{W}$ is the first layer's weight matrix, $\mathbf{X}$ includes input embeddings from a calibration dataset, such as a subset of RedPajama [33], with the $i^{\text {th }}$ row representing the $i^{\text {th }}$ datapoint. For intermediate layers, $\mathbf{X}$ contains the input activations, which are the output activations of the preceding layer.

Using the Frobenius norm of the output of a layer as a proxy objective for quantizing the weight matrices of an LLM is a popular strategy, and was used in prior work of Nagel et al. [27] . This proxy objective function is particularly useful for post-training quantization of LLMs because their large size makes it difficult to apply sophisticated compression methods.

## 3 Proposed Algorithm: Calibration-Aware Low-Precision Decomposition with Low Rank Adaptation

This section introduces CALDERA to approximately solve (1) and get a $\mathbf{Q}+\mathbf{L R}$ decomposition of a weight matrix $\mathbf{W}$ using the calibration matrix $\mathbf{X}$. The pseudocode is provided in Alg. 1. It consists of a nested loop for alternately optimizing the variables $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$. Suppose $\mathrm{Q}_{\mathrm{Q}}, \mathrm{Q}_{\mathrm{L}}$ and $\mathrm{Q}_{\mathrm{R}}$, respectively, denote quantizers used for quantizing $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$. For instance, they can refer to uniformly dithered scalar quantizers, as described in App. G.2. Initially, the low-rank actors are set to $\mathbf{0}$, and $\mathbf{W}$ is quantized using the LDLQ quantizer proposed in [3, ยง3.1]. LDLQ is an adaptive quantization method that iteratively quantizes [8] each column of $\mathbf{W}$ using $\mathrm{Q}_{\mathrm{Q}}$ to get $\mathbf{Q}$ as

$$
\begin{equation*}
\mathbf{Q}^{(k)}=\mathrm{Q}_{\mathbf{Q}}\left(\mathbf{W}^{(k)}+\left(\mathbf{W}^{(1: k-1)}-\mathbf{Q}^{(1: k-1)}\right) \mathbf{a}_{k}\right) \tag{2}
\end{equation*}
$$

where $\mathbf{Q}^{(k)}, \mathbf{W}^{(k)}$ denote the $k^{\text {th }}$ column, $\mathbf{W}^{(1: k-1)}$ denotes the first $k$ columns, $\mathrm{Q}_{\mathrm{Q}}$ has a bit-budget $\mathrm{B}_{\mathrm{Q}}$, and $\mathbf{a}_{k} \in \mathbb{R}^{k-1}$ is a learnable sequence of vectors. Update Eq. (2) incorporates linear feedback from already quantized columns, it can be seen that $\mathbf{Q}$ satisfies $\mathbf{Q}=$ $\mathrm{Q}_{\mathrm{Q}}(\mathbf{W}+(\mathbf{W}-\mathbf{Q}) \mathbf{M})$, where the feedback matrix $\mathbf{M}$ is a strictly upper triangular matrix with
columns $\mathbf{a}_{k}$. Defining $\mathbf{H} \triangleq \frac{1}{m} \mathbf{X}^{\top} \mathbf{X}$ to be the (scaled) Hessian of the least squares objective in (1), [3] show that the optimal feedback matrix is the $\mathbf{M}$ obtained from the LDL decomposition of $m \mathbf{H}$, given by $m \mathbf{H}=(\mathbf{M}+\mathbf{I}) \mathbf{D}(\mathbf{M}+\mathbf{I})^{\top}$.

Subsequently, $\mathbf{Q}$ is fixed and the Low-Precision Low-Rank (LPLR) factorization of the residual, $(\mathbf{W}-\mathbf{Q})$, is computed. This is done by the LPLRFActorize submodule (Alg. 2), which is a refined version of the LPLR algorithm proposed in [30]. For a given matrix A, Alg. 2 minimizes

$$
\begin{equation*}
\min _{\mathbf{L}, \mathbf{R}}\left\|(\mathbf{L R}-\mathbf{A}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2} \quad \text { subject to } \quad \mathbf{L} \in \mathbb{Q}_{\mathrm{L}}^{n \times k}, \text { and } \mathbf{R} \in \mathbb{Q}_{\mathrm{R}}^{k \times d} \tag{3}
\end{equation*}
$$

where $Q_{L}$ and $Q_{R}$ use $B_{L}$ and $B_{R}$ bits, respectively. In contrast to [30], the objective in (3) is calibration-data aware. Therefore, the update equations are derived using a rank-constrained regression framework, as described in App. B. Moreover, lines 7 to 14 in LPLRFACTORIZE iteratively refine the estimates of $\mathbf{L}$ and $\mathbf{R}$, and can only yield a smaller Frobenius norm error. The left and right low-rank factor update equations are described as follows.

Initialization: In the absence of quantization constraints, a globally optimal solution to the optimization problem (3) can be found as described later in lemma 4.2. Consequently, the low-rank factors are initialized using rank-constrained regression in lines $2-4$. Since subsequent quantization disrupts optimality of the solution, the factors are iteratively updated to minimize this distortion.

Updating L: To update the left factor $\mathbf{L}$, lines 5 and 9 of Alg. 2 solves $\min _{\mathbf{Z}}\left\|(\mathbf{Z R}-\mathbf{A}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$. For a fixed $\mathbf{R}$, this is a least squares minimization, whose solution is available is closed form as $\grave{\mathbf{L}}=\left(\mathbf{A X}^{\top}\right)\left(\mathbf{R} \mathbf{X}^{\top}\right)^{\dagger}=\mathbf{A H R}^{\top}\left(\mathbf{R H R}{ }^{\top}\right)^{-1}$, as derived in App. C.1.

Updating R: Line 8 of Alg. 2, updates the right factor $\mathbf{R}$ by keeping $\mathbf{L}$ fixed and solving $\min _{\mathbf{Z}}\left\|(\mathbf{L Z}-\mathbf{A}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$. As this is an under-determined linear system, there exist multiple solutions for $\mathbf{Z}$, all attaining the same objective function value. It is shown in App. C. 1 that $\grave{\mathbf{R}}=\mathbf{L}^{\dagger} \mathbf{A H} \mathbf{H}^{\dagger}$ is a solution. The corresponding error is also obtained, which is used in the derivation of Thm. 4.1.

Computational Complexity: A high-level calculation is provided here, and detailed discussions can be found in App. D. It is worthwhile to note that the closed form expressions of $\grave{\mathbf{L}}$ and $\grave{\mathbf{R}}$, which are iteratively quantized, are functions of the Hessian $\mathbf{H}=\frac{1}{m} \mathbf{X}^{\top} \mathbf{X}$. Therefore, $\mathbf{H}$ can be computed offline initially, per LLM, by doing a single forward pass, and subsequently used for all model quantization experiments. For each layer, this pre-processing includes computing $\mathbf{H}$ and its LDL decomposition, along with computing $\mathbf{H} \mathbf{H}^{\dagger}$, requiring a total of $\mathrm{O}\left(m d^{2}+2 d^{3}\right)$ multiplications. Each outer iteration involves an LDLQ quantization. Quantizing the $k^{\text {th }}$ column has complexity $\mathrm{O}(n k)$, since feedback from $k$ already quantized columns need to be incorporated. Hence, quantizing a matrix in $\mathbb{R}^{n \times d}$ entails $\mathrm{O}\left(n^{2}+3 n\right)$ complexity. Moreover, LPLRFACTORIZE requires $\mathrm{O}\left(m^{2}(n+d)\right)$ to initialize, and subsequently, each inner iteration entails $\mathrm{O}(n d k)$. Assuming $n, d \geq m \gg k$, and keeping only the dominant terms, the total complexity of CALDERA, not including the complexity of the pre-processing discussed earlier, is $\mathrm{O}\left(\mathrm{T}_{\text {out }}\left(n^{2}+m^{2}(n+d)+n d k \mathrm{~T}_{\mathrm{in}}\right)\right)$.

Fine tuning via Low-Rank Adaptation: Once the weight matrices of each layer are replaced by its $\mathbf{Q}+\mathbf{L R}$ approximation, the zero-shot performance of (post-training) quantized model can be evaluated. $\S 5$ shows that CALDERA quantized models outperform existing strategies. Additionally, if desired, the low-rank factors $\mathbf{L}$ and $\mathbf{R}$ can be further fine-tuned using low-rank adaptation [13, 16, 22] on a small task-specific dataset. While the initialization of the fine-tuning step has quantized $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$, the fine-tuned factors are represented using 16-bits (BF16 format). Although this leads to a slight increase in the memory footprint, the performance gains from fine-tuning are substantial.

## 4 Approximation Error Analysis

The approximation error upper bounds are derived via a rank-constrained regression framework. Thm. 4.1 below (formally stated and proved in App. C.4) is an informal version of the main theoretical result of this paper, and provides an upper bound on the Frobenius norm error when CALDERA approximates a weight matrix $\mathbf{W}$ is as $\mathbf{W} \approx \mathbf{Q}+\mathbf{L R}$ by solving the optimization problem (1) using Alg. 1. For convenience of analysis, it is assumed that the dynamic range of $\mathrm{Q}_{\mathrm{Q}}$, denoted as $R$, is chosen to be high enough, ensuring it remains unsaturated. Consequently, for a scalar input the quantization error from $Q_{Q}$ has zero mean and bounded variance, given by $\frac{\Delta^{2}}{4}=\frac{R^{2}}{\left(2^{B} Q^{2}-1\right)^{2}}$.

Algorithm 1: CALDERA: Calibration Aware Low-Precision DEcomposition with Low-Rank Adaptation

```
Input: Matrix: $\mathbf{W} \in \mathbb{R}^{n \times d}$, Target rank: $k$, Calibration matrix: $\mathbf{X} \in \mathbb{R}^{m \times d}$, Outer and inner
                iterations: $\mathrm{T}_{\text {out }}, \mathrm{T}_{\text {in }}$, Quantizers: $\mathrm{Q}_{\mathrm{Q}}, \mathrm{Q}_{\mathrm{L}}, \mathrm{Q}_{\mathrm{R}}$, Flag: EnableLoRA, Fine-tune rank: $r$
Output: LPLR decomposition: $\mathbf{Q} \in \mathbb{Q}_{\mathrm{Q}}^{n \times d}, \mathbf{L} \in \mathbb{Q}_{\mathrm{L}}^{n \times k}, \mathbf{R} \in \mathbb{Q}_{\mathrm{R}}^{k \times d}$ s.t.
            $\mathbf{W} \mathbf{X}^{\top} \approx(\mathbf{Q}+\mathbf{L R}) \mathbf{X}^{\top}$
Initialize: $t \leftarrow 0, \mathbf{L}_{0} \leftarrow \mathbf{0}, \mathbf{R}_{0} \leftarrow \mathbf{0}$, MinError $\leftarrow \infty$
while $t<\mathrm{T}_{\text {out }}$ do
    Update $\mathbf{Q}: \mathbf{Q}_{t+1} \leftarrow \operatorname{LDLQ}\left(\mathbf{W}-\mathbf{L}_{t} \mathbf{R}_{t}, \mathrm{Q}_{\mathrm{Q}}\right)$
    Update low-rank factors:

```

![](https://cdn.mathpix.com/cropped/2024_06_04_bc47e5d834c845042b4fg-05.jpg?height=46&width=951&top_left_y=665&top_left_x=430)

```
    if $\left\|\left(\mathbf{Q}_{t+1}+\mathbf{L}_{t+1} \mathbf{R}_{t+1}-\mathbf{W}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}<$ MinError then
        $\mathbf{Q}_{\text {best }} \leftarrow \mathbf{Q}_{t+1}, \mathbf{L}_{\text {best }} \leftarrow \mathbf{L}_{t+1}, \mathbf{R}_{\text {best }} \leftarrow \mathbf{R}_{t+1}$,
            MinError $\leftarrow\left\|\left(\mathbf{Q}_{t+1}+\mathbf{L}_{t+1} \mathbf{R}_{t+1}-\mathbf{W}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$
    end
    $t \leftarrow t+1$
end
if EnableLoRA is TRUE then
    Further Fine-tune top- $r$ singular components of $\mathbf{L}_{\text {best }}$ and $\mathbf{R}_{\text {best }}$ to 16-bit precision using
        Low-Rank Adaptation (LoRA) (as in [13, 16, 22])
end
return $\mathbf{Q}_{\text {best }}, \mathbf{L}_{\text {best }}, \mathbf{R}_{\text {best }}$
```

Theorem 4.1. Approximation error of CAlDERA (Informal) Given $\mathbf{W} \in \mathbb{R}^{n \times d}$ and $\mathbf{X} \in \mathbb{R}^{m \times d}$ with $m \leq d$, let $\mathbf{D}$ be obtained from the LDL decomposition $\mathbf{X}^{\top} \mathbf{X}=m \mathbf{H}=(\mathbf{M}+\mathbf{I}) \mathbf{D}(\mathbf{M}+\mathbf{I})^{\top}$, and $\lambda_{\max }, \lambda_{\min }$ denote the max and min eigenvalues of $\mathbf{H}$. Additionally, let $\mathbf{Q} \triangleq \operatorname{LDLQ}\left(\mathbf{W}, \mathrm{Q}_{\mathrm{Q}}\right)$, where $\mathrm{Q}_{\mathrm{Q}}$ has dynamic range $\mathrm{R}$ and bit-budget $\mathrm{B}_{\mathrm{Q}}$, the quantization error be $\boldsymbol{\eta} \triangleq \mathrm{Q}_{\mathrm{Q}}(\mathbf{Q}+(\mathbf{W}-$ $\mathbf{Q}) \mathbf{M})-(\mathbf{Q}+(\mathbf{W}-\mathbf{Q}) \mathbf{M})$, and $\sigma_{1} \geq \ldots \geq \sigma_{k} \ldots$ be the singular values of $\mathbf{X}(\mathbf{W}-\mathbf{Q})^{\top}$. If the target rank $k$ satisfies $0.25 \lambda_{\min }^{1 / 2}\left(m \sigma_{1}\right)^{-1} \lambda_{\max }^{-3 / 2} \sum_{i>k} \sigma_{i}^{2} \leq k \leq m$, and the dynamic ranges of $\mathrm{Q}_{\mathrm{L}}$ and $\mathrm{Q}_{\mathrm{R}}$ are set as $\mathrm{R}_{\mathrm{L}}=\frac{2 \sigma_{1}}{\sigma_{k} \sqrt{m \lambda_{\min }}}$ and $\mathrm{R}_{\mathrm{R}}=\sigma_{1}$, then $\mathbf{Q}, \mathbf{L}$ and $\mathbf{R}$ returned by Alg. 1 satisfy

$$
\frac{1}{n m} \mathbb{E}\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2} \leq \frac{1}{n} \sum_{i>k} \mathbb{E} \lambda_{i}\left(\boldsymbol{\eta} \mathbf{D} \boldsymbol{\eta}^{\top}\right)+\epsilon \lesssim \frac{4 d \lambda_{\max } \mathrm{R}^{2}}{\pi\left(2^{\mathrm{B}_{\mathrm{Q}}}-1\right)^{2}}\left(1-\frac{k}{2 n}\right)^{2}+\epsilon
$$

while utilizing an average budget of $\frac{1}{2} \log _{2}\left(\frac{k \sigma_{1}^{3}}{m \epsilon \sigma_{k}} \frac{\lambda_{\max }}{\lambda_{\min }} \sqrt{d / n}\right)$ bits per parameter for the low-rank factors $\mathbf{L}$ and $\mathbf{R}$, when $n \approx d$. Here, the expectation is over the stochasticity of the quantizers.

An informal version of the main result is provided here, and the formal version including specific constant values, along with the derivation, can be found in App. C.4. The requirement $m \leq d$ is not restrictive, as $\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}=\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{H}^{1 / 2}\right\|_{\mathrm{F}}^{2}$, i.e., (1) can be rewritten to ensure $m=d$. The approximation error upper bound given by Thm. 4.1 can be directly compared with the result of Chee et al. [3, Thm. 1], which states that for vanilla LDLQ without LPLRFACTORIZE,

$$
\begin{equation*}
\mathbb{E}\left\|(\mathbf{Q}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2} \leq \mathbb{E}\left[\operatorname{Tr}\left(\boldsymbol{\eta} \mathbf{D} \boldsymbol{\eta}^{\top}\right)\right]=\sum_{i=1}^{n} \mathbb{E} \lambda_{i}\left(\boldsymbol{\eta} \mathbf{D} \boldsymbol{\eta}^{\top}\right) \tag{4}
\end{equation*}
$$

Evidently, Alg. 1 yields a smaller error provided, $\sum_{i>k} \mathbb{E} \lambda_{i}\left(\boldsymbol{\eta} \mathbf{D} \boldsymbol{\eta}^{\top}\right)<\sum_{i=1}^{k} \mathbb{E} \lambda_{i}\left(\boldsymbol{\eta} \mathbf{D} \boldsymbol{\eta}^{\top}\right)-\epsilon$, where $\epsilon$ can be chosen to be arbitrarily small. Furthermore, since the expression in Thm. 4.1 consists of two terms, namely, the rank-constrained regression error, which depends on the target rank $k$, and the additive quantization error of $\epsilon$, which is dictated by the bit-budgets used for $\mathbf{L}$ and $\mathbf{R}$, this upper bound can be made arbitrarily small by ensuring that the two terms are approximately equal, i.e., $\mathbb{E}\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$ is upper bounded by $2 \epsilon$. This is apparent in the following regimes:

(i) $k \ll n$ : In this regime, $k$ is treated as a constant as $n$ grows. Then, if the bit-budget $\mathrm{B}_{\mathrm{Q}}$ satisfies

$$
\mathrm{B}_{\mathrm{Q}} \geq \log _{2}\left(2 \mathrm{R}(\pi \epsilon)^{-1 / 2} \sqrt{n m d \lambda_{\max }}+1\right), \quad \text { then } \quad \mathbb{E}\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2} \leq 2 \epsilon
$$

(ii) $k=\mathrm{O}(n)$ : For a fixed $\mathrm{B}_{\mathrm{Q}}$, if $k$ is allowed to grow with dimension $n$, then choosing $k$ to satisfy $k \geq 2 n-\left(2^{\mathrm{B}_{\mathrm{Q}}}-1\right) \mathrm{R}^{-1}(\pi \epsilon)^{1 / 2}\left(m d \lambda_{\max }^{-1 / 2}\right) \sqrt{n} \quad$ ensures $\quad \mathbb{E}\left\|(\mathbf{Q}+\mathbf{L R}-\mathbf{W}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2} \leq 2 \epsilon$.

This implies that the upper bound can be made arbitrarily small by either (i) increasing the bit-budget of the backbone, i.e., $\mathrm{B}_{\mathrm{Q}}$, for a fixed rank $k$, or (ii) increasing the rank $k$ for a fixed $\mathrm{B}_{\mathrm{Q}}$, for example, $\mathrm{B}_{\mathrm{Q}}=2$. Alternatively stated, this provides a tunable knob for controlling the error by trading off the allocated bit-budget between the backbone $\mathbf{Q}$ and the low-rank factors $\mathbf{L}, \mathbf{R}$.

### 4.1 Analysis Outline

In this section, a brief proof sketch is presented, highlighting the major challenges in the proof and how they are addressed. For analysis, $\mathbf{Q}$ is assumed to be updated prior to $\mathbf{L}, \mathbf{R}$ in Alg. 1. However, in practice, the update order is inconsequential, and can be swapped, depending on whichever yields a smaller error. The complete derivation of the approximation error is provided in App. C. A key ingredient of the proof is the solution of the rank-constrained regression problem, defined as,

$$
\begin{equation*}
\min _{\operatorname{rank}(\mathbf{Z}) \leq k}\|\mathbf{X Z}-\mathbf{Y}\|_{\mathrm{F}}^{2} \tag{5}
\end{equation*}
$$

Although this problem is non-convex, it can be solved to global optimality via two SVDs [40]. The following lemma characterizes the solution to the optimization problem in (5).

Lemma 4.2. Given $\mathbf{Y} \in \mathbb{R}^{m \times n}$, and full $\operatorname{rank} \mathbf{X} \in \mathbb{R}^{m \times d}$, where $m \leq d$. Let $\mathbf{X}=\mathbf{U} \widetilde{\boldsymbol{\Sigma}} \mathbf{V}^{\top}$ and

![](https://cdn.mathpix.com/cropped/2024_06_04_bc47e5d834c845042b4fg-06.jpg?height=57&width=1279&top_left_y=1045&top_left_x=366)

$$
\mathbf{Z}_{*} \triangleq \underset{\operatorname{rank}(\mathbf{Z}) \leq k}{\arg \min }\|\mathbf{X Z}-\mathbf{Y}\|_{\mathrm{F}}^{2}=\left(\mathbf{V} \mathbf{I}_{m} \boldsymbol{\Sigma}^{-1} \mathbf{U}_{\mathbf{I}}\right)\left(\mathbf{I}_{k}^{\top} \grave{\boldsymbol{\Sigma}} \mathbf{V}^{\top}\right)
$$

where $\boldsymbol{\Sigma}:=\widetilde{\boldsymbol{\Sigma}} \mathbf{I}_{m} \in \mathbb{R}^{m \times m}$ is a diagonal matrix consisting of the non-zero singular values of $\mathbf{X}$. Moreover, denoting the non-zero singular values of $\mathbf{Y}$ as $\left\{\sigma_{i}(\mathbf{Y})\right\}_{i=1}^{m}$, the optimal value of (7) is

$$
\begin{equation*}
\min _{\operatorname{rank}(\mathbf{Z}) \leq k}\|\mathbf{X Z}-\mathbf{Y}\|_{\mathrm{F}}^{2}=\left\|\mathbf{X Z} \mathbf{Z}_{*}-\mathbf{Y}\right\|_{\mathrm{F}}^{2}=\sum_{i=k+1}^{m} \sigma_{i}^{2}(\mathbf{Y}) \tag{6}
\end{equation*}
$$

The complete lemma (with the case $m>d$ ), and the derivation, are provided in App. B. Using lemma 4.2, the approximation error of LPLRFACTORIZE is analyzed in App. C.3. Specifically,

Algorithm 2: LPLRFACTORIZE(A, $\left.k, \mathbf{X}, \mathrm{Q}_{\mathrm{L}}, \mathrm{Q}_{\mathrm{R}}, T_{\text {in }}\right)$ : LPLR factorization submodule

Input: Matrix: $\mathbf{A} \in \mathbb{R}^{n \times d}$, Target rank: $k$, Calibration matrix: $\mathbf{X} \in \mathbb{R}^{m \times d}$, Iterations: $\mathrm{T}_{\mathrm{in}}$, Quantizers: $\mathrm{Q}_{\mathrm{L}}, \mathrm{Q}_{\mathrm{R}}$

Output: Low precision Low Rank factors: $\mathbf{L} \in \mathbb{Q}^{n \times k}, \mathbf{R} \in \mathbb{Q}^{k \times d}$ s.t. $\mathbf{A X}{ }^{\top} \approx \mathbf{L R X}{ }^{\top}$

Initialize: Iteration counter: $i \leftarrow 0$

Compute SVD of $\mathbf{X}$ as $\mathbf{U} \tilde{\boldsymbol{\Sigma}} \mathbf{V}^{\top}$.

Compute SVD of $\mathbf{U}^{\top} \mathbf{X} \mathbf{A}^{\top}$ as $\mathbf{U} \grave{\boldsymbol{\Sigma}} \grave{\mathbf{V}}^{\top}$

Get right low-rank factor: $\mathbf{R}_{0} \leftarrow \mathrm{Q}_{\mathrm{R}}\left(\mathbf{I}_{k}^{\top} \grave{\boldsymbol{\Sigma}} \grave{\mathbf{V}}^{\top}\right)$

Get left low-rank factor: $\mathbf{L}_{0} \triangleq \mathrm{Q}_{\mathrm{L}}\left(\grave{\mathbf{L}}_{0}\right)$, where $\grave{\mathbf{L}}_{0}=\arg \min _{\mathbf{Z} \in \mathbb{R}^{k \times d}}\left\|\left(\mathbf{Z R} \mathbf{R}_{0}-\mathbf{A}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$

$\mathbf{L}_{\text {best }} \leftarrow \mathbf{L}_{0}, \mathbf{R}_{\text {best }} \leftarrow \mathbf{R}_{0}$, MinError $\leftarrow\left\|\left(\mathbf{L}_{0} \mathbf{R}_{0}-\mathbf{A}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$.

while $i<\mathrm{T}_{\text {in }}$ do

Update right: $\mathbf{R}_{i+1} \leftarrow \mathrm{Q}_{\mathrm{R}}\left(\grave{\mathbf{R}}_{i+1}\right)$, where $\grave{\mathbf{R}}_{i+1}=\arg \min _{\mathbf{Z} \in \mathbb{R}^{k \times d}}\left\|\left(\mathbf{L}_{i} \mathbf{Z}-\mathbf{A}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$

Update left: $\mathbf{L}_{i+1} \leftarrow \mathrm{Q}_{\mathrm{L}}\left(\grave{\mathbf{L}}_{i+1}\right)$, where $\grave{\mathbf{L}}_{i+1}=\arg \min _{\mathbf{Z} \in \mathbb{R}^{n \times k}}\left\|\left(\mathbf{Z R} \mathbf{R}_{i}-\mathbf{A}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$

if $\left\|\left(\mathbf{L}_{i+1} \mathbf{R}_{i+1}-\mathbf{A}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}<$ MinError then

$\mathbf{L}_{\text {best }} \leftarrow \mathbf{L}_{i+1}, \mathbf{R}_{\text {best }} \leftarrow \mathbf{R}_{i+1}$, MinError $\leftarrow\left\|\left(\mathbf{L}_{i+1} \mathbf{R}_{i+1}-\mathbf{A}\right) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$

end

$i \leftarrow i+1$

end

return $\mathbf{L}_{\text {best }}, \mathbf{R}_{\text {best }}$
lemma C. 3 shows that for any input matrix $\mathbf{A}$, Alg. 2 with suitably chosen $\mathrm{B}_{\mathrm{L}}$ and $\mathrm{B}_{\mathrm{R}}$, ensures that $\mathbb{E}\left\|(\mathbf{L R}-\mathbf{A}) \mathbf{X}^{\top}\right\|_{\mathrm{F}}^{2}$, as in (3), can be upper bounded by twice the sum of squared trailing singular values, (ref. (6)). While proving lemma C.3, it is assumed that if $\mathrm{Q}_{\mathrm{L}}$ or $\mathrm{Q}_{\mathrm{R}}$ gets saturated, a trivial output of $\mathbf{L}=\mathbf{0}, \mathbf{R}=\mathbf{0}$ is returned. Therefore, lemmas C. 1 and C. 2 specify choosing the dynamic ranges $R_{R}$ and $R_{L}$ to be sufficiently high so that saturation happens with a very low probability. The proof of Thm. 4.1 is completed by using the LDL decomposition of $m \mathbf{H}$ as proposed in [3], along with an application of Marchenko-Pastur approximation to bound the expected eigenvalues of the quantization error, i.e., $\mathbb{E} \lambda_{i}\left(\boldsymbol{\eta} \boldsymbol{\eta}^{\top}\right)$, yielding the final inequality.

## 5 Numerical Simulations

The efficacy of CALDERA is assessed by using it to compress three popular open source LLMs from Meta AI, namely, LLaMa-2 7B, LLaMa-2 70B [34] and LLaMa-3 8B [26]. The framework is built in PyTorch on top of the QuIP\# [35] and LoftQ [22], and is available at https://github.com/pilancilab/caldera.

Baselines. The full-rank matrix $\mathbf{Q}$, also referred to as the backbone, is quantized to 2-bits using the LDLQ procedure from QuIP [3, 35], employing an E8 lattice quantizer [38]. For CALDERA, which allows even the low-rank factors, $\mathbf{L}$ and $\mathbf{R}$, to be represented in low-precision, the quantization is also performed with an E8 lattice. Prior to running Alg. 1, a randomized Hadamard transform (RHT) is applied to the left and the right of the input weight matrix, as the incoherence pre-processing step, to equalize the magnitude of the entries making them more robust to quantization. In other words, CALDERA decomposition is performed on $\widetilde{\mathbf{W}} \triangleq \mathbf{H}_{\mathrm{L}}^{\top} \mathbf{W} \mathbf{H}_{\mathrm{R}}$, where $\mathbf{H}_{\mathrm{L}}$ and $\mathbf{H}_{\mathrm{R}}$ are Hadamard matrices, right-multiplied by a diagonal matrix with i.i.id. $\{ \pm 1\}$ entries. In addition, the Hessian matrix obtained from the calibration data is substituted by $\widetilde{\mathbf{H}} \triangleq \mathbf{H}_{\mathrm{R}}^{\top} \mathbf{H} \mathbf{H}_{\mathrm{R}}$. As described in [3], this improves the quantization error incurred by LDLQ. Further details are provided in App. E.2.

Metrics. The performance of CALDERA is evaluated using perplexity on the test splits of the Wikitext2 [25] and C4 [6] datasets, as well as task-specific goodness-of-fit metrics such as zeroshot accuracy for sequence classification. Specifically, zero-shot accuracy was measured on the Winogrande [19], RTE [1, 39], PiQA [2], ARC-Easy, and ARC-Challenge [4] tasks. App. E. 3 provides more details regarding these benchmarks. Perplexity was measured using a sequence length equal to the model's maximum context length, i.e., 4096 for LLaMa-2, and 8192 for LLaMa-3. Zeroshot experiments were performed using EleutherAI's Language Model Evaluation Harness [9].

### 5.1 Zero-shot Results

Tables 1 and 2 report the perplexities and accuracies for CALDERA with varying target rank $(k)$ of $\mathbf{L}$ and $\mathbf{R}$. A smaller value is better for perplexity, which is defined as the $\exp (\cdot)$ of the training objective, while zero-shot accuracies are reported as percentages. Per-parameter bit budgets range from 2.1 (e.g., rank-64 factors in 4 -bit precision) to 2.4 bits (e.g., rank-64 factors in half precision or rank-256 factors in 4-bit precision). For comparison, the $\mathbf{Q}+\mathbf{L R}$ decomposition of weight matrices found in the QuIP\# codebase was performed on each model. For the sake of direct comparison, finetuning of the diagonal matrices in RHT was omitted. As QuIP\# does not support quantized factors, $\mathbf{L}$ and $\mathbf{R}$ are rank-64 in order to ensure that the per-parameter bit-budget remains in the $2-2.4$ range. As another baseline comparison, each model is quantized using QuIP\# without any low-rank factors. Results for the unquantized LLaMa-2 7B, LLaMa-2 70B, and LLaMa-3 8B models are also provided.

For all but the LLaMa-2 70B model, the rank-256 CALDERA decomposition with 4-bit factors had the lowest perplexity and highest accuracy. As CALDERA supports quantizing low-rank factors with minimal performance loss, more singular components can be captured compared to using halfprecision factors while employing the same number of bits. Consequently, the low-rank factors can regain the performance that was compromised when the backbone $\mathbf{Q}$ was quantized to 2 bits. Since zero-shot experiments have some inherent randomness and low-rank regularization effects [32], the zero-shot accuracies reported here are not as directly indicative of quantization performance as the perplexity results. In addition, $\S 5.3$, demonstrates that degradation in zero-shot accuracy can be recovered via LoRA fine-tuning. It is worthwhile to note these results substantiate the claims of [17],

Table 1: Zero-shot perplexities (denoted by $\downarrow$ ) and accuracies ( $\uparrow$ ) for LLaMa-2. $\mathrm{B}_{\mathrm{Q}}=2$ bits throughout.

| Method | Rank | $\mathrm{B}_{\mathrm{L}}\left(=\mathrm{B}_{\mathrm{R}}\right)$ | Avg Bits | Wiki2 $\downarrow$ | $\mathrm{C} 4 \downarrow$ | Wino $\uparrow$ | RTE $\uparrow$ | PiQA $\uparrow$ | ArcE $\uparrow$ | ArcC $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CALDERA (7B) | 64 | 16 | 2.4 | 7.36 | 9.47 | 64.6 | 66.4 | 73.7 | 60.8 | 31.7 |
| CALDERA (7B) | 64 | 4 | 2.1 | 7.37 | 9.74 | 63.7 | 62.1 | 72.3 | 60.9 | 31.7 |
| CALDERA (7B) | 128 | 4 | 2.2 | 6.76 | 8.83 | 63.8 | 59.9 | 75.1 | $\mathbf{6 5 . 1}$ | $\mathbf{3 4 . 6}$ |
| CALDERA (7B) | 256 | 4 | 2.4 | $\mathbf{6 . 1 9}$ | $\mathbf{8 . 1 4}$ | $\mathbf{6 6 . 0}$ | 60.6 | $\mathbf{7 5 . 6}$ | 63.6 | 34.0 |
| QuIP\# (7B, No FT) | 64 | 16 | 2.4 | 7.73 | 10.0 | 63.1 | $\mathbf{6 6 . 8}$ | 71.7 | 63.2 | 31.7 |
| QuIP\# (7B, No FT) | 0 | - | 2 | 8.23 | 10.8 | 61.7 | 57.8 | 69.6 | 61.2 | 29.9 |
| CALDERA (70B) | 64 | 16 | 2.2 | $\mathbf{4 . 5 0}$ | $\mathbf{6 . 3 8}$ | $\mathbf{7 5 . 4}$ | $\mathbf{7 1 . 7}$ | $\mathbf{7 9 . 2}$ | 71.8 | $\mathbf{4 3 . 9}$ |
| CALDERA (70B) | 128 | 4 | 2.1 | 5.07 | 7.10 | 72.9 | 62.1 | 78.0 | $\mathbf{7 3 . 2}$ | $\mathbf{4 3 . 9}$ |
| QuIP\# (70B, No FT) | 0 | - | 2 | 5.37 | 7.51 | 72.3 | 47.6 | 77.7 | 68.8 | 40.9 |
| Unquantized (7B) | 0 | - | 16 | 5.12 | 6.63 | 67.3 | 63.2 | 78.5 | 69.3 | 40.0 |
| Unquantized (70B) | 0 | - | 16 | 3.12 | 4.97 | 77.0 | 67.9 | 81.1 | 77.7 | 51.1 |

Table 2: Zero-shot perplexities (denoted by $\downarrow$ ) and accuracies ( $\uparrow$ ) for LLaMa-3 8B. $\mathrm{B}_{\mathrm{Q}}=2$ bits throughout.

| Method | Rank | $\mathrm{B}_{\mathrm{L}}\left(=\mathrm{B}_{\mathrm{R}}\right)$ | Avg Bits | Wiki2 $\downarrow$ | $\mathrm{C} 4 \downarrow$ | Wino $\uparrow$ | RTE $\uparrow$ | PiQA $\uparrow$ | ArcE $\uparrow$ | ArcC $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CALDERA | 64 | 16 | 2.4 | 9.22 | 10.5 | 68.9 | 63.9 | 72.9 | 69.9 | 36.5 |
| CALDERA | 64 | 4 | 2.1 | 10.6 | 11.8 | 66.9 | 58.5 | 71.8 | 68.2 | 34.3 |
| CALDERA | 128 | 4 | 2.2 | 9.21 | 10.5 | 67.6 | $\mathbf{6 9 . 7}$ | 74.4 | 71.8 | 36.3 |
| CALDERA | 256 | 4 | 2.4 | $\mathbf{8 . 2 2}$ | $\mathbf{9 . 5 6}$ | $\mathbf{6 9 . 7}$ | 65.0 | $\mathbf{7 5 . 1}$ | $\mathbf{7 3 . 2}$ | $\mathbf{4 0 . 0}$ |
| QuIP\# (No FT) | 64 | 16 | 2.4 | 10.9 | 11.8 | 66.5 | 57.0 | 69.6 | 63.8 | 31.0 |
| QuIP\# (No FT) | 0 | - | 2 | 13.8 | 15.6 | 63.2 | 52.7 | 67.6 | 57.6 | 28.2 |
| Unquantized | 0 | - | 16 | 5.54 | 7.01 | 73.5 | 68.6 | 79.7 | 80.1 | 50.2 |

which report that low-bit quantization of LLaMa-3 8B, significantly deteriorates model performance across various post-training quantization techniques, more so than with the LLaMa-2 series.

### 5.2 Fine-tuning of Randomized Hadamard Transform (RHT) Parameters

As CALDERA presents a general optimization framework for matrix decompositions of the form $\mathbf{Q}+\mathbf{L R}$, it can easily be extended with additional heuristics to improve performance. This section serves as a proof of concept, by examining one such heuristic: Fine-tuning of randomized Hadamard transform parameters. This technique, proposed in QuIP\# [35], involves fine-tuning the diagonal Rademacher matrices with $\pm 1$ entries in the RHT to minimize the cross-entropy loss between the output of the original and quantized models on the calibration dataset. Subsequently, RHT finetuning is performed on the models quantized using CALDERA in $\$ 5.1 .{ }^{1}$ Details on specific finetuning hyperparameters can be found in App. E.4.

Table 3: Zero-shot perplexities and accuracies for LLaMa-2 7B, with end-to-end fine-tuning of randomized

![](https://cdn.mathpix.com/cropped/2024_06_04_bc47e5d834c845042b4fg-08.jpg?height=43&width=951&top_left_y=1930&top_left_x=370)

| Method | Rank | $\mathrm{B}_{\mathrm{L}}\left(=\mathrm{B}_{\mathrm{R}}\right)$ | Avg Bits | Wiki2 $\downarrow$ | $\mathrm{C} 4 \downarrow$ | Wino $\uparrow$ | RTE $\uparrow$ | PiQA $\uparrow$ | ArcE $\uparrow$ | ArcC $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CALDERA | 64 | 16 | 2.4 | 6.22 | 8.23 | 64.2 | 63.2 | 76.1 | 63.4 | 34.7 |
| CALDERA | 64 | 4 | 2.1 | 6.30 | 8.32 | 64.6 | 65.7 | 75.4 | 63.3 | 35.4 |
| CALDERA | 128 | 4 | 2.2 | 6.09 | 8.06 | 65.1 | 61.0 | $\mathbf{7 6 . 5}$ | $\mathbf{6 5 . 1}$ | 35.6 |
| CALDERA | 256 | 4 | 2.4 | $\mathbf{5 . 8 4}$ | $\mathbf{7 . 7 5}$ | $\mathbf{6 5 . 7}$ | 60.6 | $\mathbf{7 6 . 5}$ | 64.6 | $\mathbf{3 5 . 9}$ |
| QuIP\#* $^{*}$ | 64 | 16 | 2.4 | 6.32 | 8.31 | 64.9 | $\mathbf{6 6 . 4}$ | 75.0 | $\mathbf{6 5 . 2}$ | 34.5 |
| QuIP\# $^{*}$ | 0 | - | 2 | 6.58 | 8.62 | 64.4 | 53.4 | 75.0 | 64.8 | 34.0 |

[^0]Table 4: Zero-shot perplexities and accuracies for LLaMa-3 8B, with end-to-end fine-tuning of randomized Hadamard transform parameters. $\mathrm{B}_{\mathrm{Q}}=2$ bits throughout. ${ }^{*}$ See Footnote 1.

| Method | Rank | $\mathrm{B}_{\mathrm{L}}\left(=\mathrm{B}_{\mathrm{R}}\right)$ | Avg Bits | Wiki2 $\downarrow$ | $\mathrm{C} 4 \downarrow$ | Wino $\uparrow$ | RTE $\uparrow$ | PiQA $\uparrow$ | ArcE $\uparrow$ | ArcC $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CALDERA | 64 | 16 | 2.4 | 7.63 | 8.9 | $\mathbf{7 0 . 3}$ | $\mathbf{7 0 . 8}$ | 75.4 | 72.4 | 39.0 |
| CALDERA | 64 | 4 | 2.1 | 8.06 | 9.34 | 69.5 | 64.3 | 76.0 | 71.5 | 40.0 |
| CALDERA | 128 | 4 | 2.2 | 7.76 | 9.02 | 69.4 | 63.9 | 76.0 | $\mathbf{7 3 . 7}$ | 41.8 |
| CALDERA | 256 | 4 | 2.4 | $\mathbf{7 . 3 4}$ | $\mathbf{8 . 6 8}$ | $\mathbf{7 0 . 3}$ | 70.4 | $\mathbf{7 6 . 5}$ | $\mathbf{7 3 . 6}$ | $\mathbf{4 2 . 3}$ |
| QuIP\#* $^{2} \mathbf{6 4}$ | 16 | 2.4 | 7.92 | 9.15 | 68.4 | 58.1 | 74.9 | 72.3 | 40.4 |  |
| QuIP\# $^{*}$ | 0 | - | 2 | 8.44 | 9.75 | 67.5 | 57.8 | 72.9 | 67.6 | 37.3 |

Perplexity and zero-shot results in Tables 3 and 4 match the trends in $\S 5.1$, i.e., CALDERA with rank-256 factors typically performs best, with the exception of RTE. In addition, perplexities are substantially lower than without the fine-tuning of randomized Hadamard transform parameters.

### 5.3 Low Rank Adaptation (LoRA) Fine-tuning Results

Once the $\mathbf{Q}+\mathbf{L R}$ decomposition with target rank $k$ is obtained, and $k$ takes values 64,128 and 256, fine-tuning the top $r(\leq k)$ singular components on a task-specific dataset can recover the performance lost due to quantization. Throughout all experiments in Table $5, r=64$ is chosen and those singular components are fine-tuned to 16 -bit precision, i.e., $\mathrm{BF} 16$ format. In other words, the approximation is written as $\mathbf{W} \approx \mathbf{Q}+\mathbf{L}_{1} \mathbf{R}_{1}+\mathbf{L}_{2} \mathbf{R}_{2}$, where $\mathbf{L}_{1} \in \mathbb{R}^{n \times r}, \mathbf{L}_{2} \in \mathbb{R}^{n \times(k-r)}$, $\mathbf{R}_{1} \in \mathbb{R}^{r \times d}, \mathbf{R}_{2} \in \mathbb{R}^{(k-r) \times d}, \mathbf{L}=\left[\mathbf{L}_{1} \mid \mathbf{L}_{2}\right], \mathbf{R}^{\top}=\left[\mathbf{R}_{1}^{\top} \mid \mathbf{R}_{2}^{\top}\right]$. The value of $r$ is set to 64 and $\mathbf{L}_{2}, \mathbf{R}_{2}$ are fined-tuned to $\mathbf{L}_{\mathrm{bf} 16}, \mathbf{R}_{\mathrm{bf16}}$ using low-rank adaptation similar to [13, 16, 22]. Doing this significantly on a small task-specific dataset like WikiText2, RTE, or Winogrande, can noticeably boost the zero-shot accuracy, as can be seen from Table $5 .{ }^{2}$

Table 5: CALDERA fine-tuning results for LLaMa-2 7B and LLaMa-3 8B. $B_{L}, B_{R}$ are the bit-budgets of $\mathbf{L}$ and $\mathbf{R}$ for the low-rank initialization. Rank-64 fine-tuned factors are represented in $\mathrm{BF} 16$ precision.

|  |  |  |  |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Method | Rank | $\mathrm{B}_{\mathrm{Q}}$ | $\mathrm{B}_{\mathrm{L}}\left(=\mathrm{B}_{\mathrm{R}}\right)$ | RHT FT | Avg Bits | Wiki2 $\downarrow$ | RTE $\uparrow$ | Wino $\uparrow$ | Wiki2 $\downarrow$ | RTE $\uparrow$ | Wino $\uparrow$ |
| CALDERA | 64 | 2 | 16 | No | 2.4 | 6.06 | 82.31 | 84.06 | 7.91 | 84.48 | 85.56 |
| CALDERA | 64 | 2 | 16 | Yes | 2.4 | 5.89 | 85.19 | 85.32 | 7.88 | 86.28 | 88.16 |
| CALDERA | 64 | 2 | 4 | No | 2.4 | 6.01 | 81.23 | 84.06 | 8.33 | 85.56 | 88.40 |
| CALDERA | 64 | 2 | 4 | Yes | 2.4 | 5.91 | 85.56 | 83.42 | 7.96 | $\mathbf{8 7 . 0 0}$ | 88.40 |
| CALDERA | 128 | 2 | 4 | No | 2.5 | 5.84 | 83.75 | 85.32 | 7.84 | 84.84 | 88.63 |
| CALDERA | 128 | 2 | 4 | Yes | 2.5 | 5.77 | 84.12 | 85.00 | 7.68 | 86.64 | 88.00 |
| CALDERA | 256 | 2 | 4 | No | 2.7 | 5.61 | 83.75 | $\mathbf{8 5 . 4}$ | $\mathbf{7 . 4 4}$ | 86.28 | 88.08 |
| CALDERA | 256 | 2 | 4 | Yes | 2.7 | $\mathbf{5 . 5 5}$ | $\mathbf{8 6 . 2 8}$ | 84.93 | $\mathbf{7 . 4 4}$ | 85.20 | $\mathbf{8 9 . 1 9}$ |
| LoftQ | 64 | 2 | 16 | - | 2.4 | 7.85 | - | - | - | - | - |
| LoftQ | 64 | 2.5 | 16 | - | 2.9 | 5.78 | - | - | - | - | - |
| LQ-LoRA | 64 | 2.75 | 8 | - | 2.95 | 5.67 | - | 72.4 | - | - | - |

Experimental details can be found in App. E.4. For each dataset, ten checkpoints are saved during the course of fine-tuning, and the best test performance is reported in Table 5. For datasets where test labels are not available, evaluation performance is reported instead.

For comparison, results from the LoftQ [22] and LQ-LoRA [13] papers are also reported, where available. As these papers were published before the release of LLaMa-3, only LLaMa-2 results are available. ${ }^{3}$ In each case, CALDERA achieves better performance at a lower bit budget.[^1]

## 6 Conclusions

In this work, the problem of obtaining a low-precision and low-rank decomposition of an LLM weight matrix was considered. A $\mathbf{Q}+\mathbf{L R}$ decomposition efficiently captures the high singular components of the weight matrix with sufficient fidelity, while coarsely compressing the less significant moderate-to-low singular components. An optimization-theoretically motivated algorithm was proposed to obtain this decomposition, which iteratively optimized the quantized backbone $\mathbf{Q}$ and the low-rank factors $\mathbf{L}, \mathbf{R}$. Additionally, it was shown that $\mathbf{L}$ and $\mathbf{R}$ can be efficiently fine-tuned using low-rank adaptation to boost the zero-shot performance of the quantized model. By utilizing a rankconstrained regression framework, an upper bound was established on the approximation error of the algorithm, and it was shown that this upper bound can be significantly smaller than prior bounds in the literature. Finally, the proposed method was empirically evaluated by compressing the LlaMA family of LLMs in the challenging sub-2.5 bits per parameter regime. The proposed approach can also be used to complement existing compression strategies; thereby making it efficient to distribute compressed LLMs and deploy them on regular consumer hardware, making them more accessible to researchers.

## Acknowledgements

This work was supported in part by the National Science Foundation (NSF) under Grant DMS2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164.

## References

[1] L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini. The Fifth PASCAL Recognizing Textual Entailment Challenge, 2009.

[2] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.

[3] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. QuIP: 2-Bit Quantization of Large Language Models With Guarantees. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[4] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457v1, 2018.

[5] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=0UIFPHEgJU.

[6] J. Dodge, M. Sap, A. Marasoviฤ, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus, 2021.

[7] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh. Extreme Compression of Large Language Models via Additive Quantization, 2024. URL https://arxiv.org/abs/2401.06118.

[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. OPTQ: Accurate Quantization for Generative Pre-trained Transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.

[9] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac'h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.

[10] G. H. Golub and C. F. van Loan. Matrix Computations. JHU Press, fourth edition, 2013. ISBN 1421407949 9781421407944. URL http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm.

[11] R. Gray and T. Stockham. Dithered quantizers. IEEE Transactions on Information Theory, 39 (3):805-812, 1993. doi: $10.1109 / 18.256489$.

[12] S. Gugger, L. Debut, T. Wolf, P. Schmid, Z. Mueller, S. Mangrulkar, M. Sun, and B. Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022.

[13] H. Guo, P. Greengard, E. P. Xing, and Y. Kim. LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. arxiv:2311.12023, 2023. URL https://arxiv.org/abs/2311.12023.

[14] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217-288, 2011. doi: 10.1137/090771806. URL https://doi.org/10.1137/090771806.

[15] Y.-C. Hsu, T. Hua, S. Chang, Q. Lou, Y. Shen, and H. Jin. Language model compression with weighted low-rank factorization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5.

[16] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.

[17] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu, and M. Magno. How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study, 2024.

[18] A. Kaushal, T. Vaidhya, and I. Rish. LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression, 2023.

[19] S. Keisuke, L. B. Ronan, B. Chandra, and C. Yejin. WinoGrande: An Adversarial Winograd Schema Challenge at Scale, 2019.

[20] A. Krishnamoorthy and D. Menon. Matrix inversion using cholesky decomposition. In 2013 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA), pages $70-72,2013$.

[21] H. J. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, page 552-561. AAAI Press, 2012. ISBN 9781577355601.

[22] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. LoftQ: LoRAFine-Tuning-Aware Quantization for Large Language Models. arxiv:2310.08659, 2023. URL https://arxiv.org/abs/2310.08659.

[23] S.-Y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024.

[24] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.

[25] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer Sentinel Mixture Models, 2016.

[26] Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-07.

[27] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. Up or Down? Adaptive Rounding for Post-Training Quantization. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 7197-7206, 2020.

[28] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models, 2020.

[29] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining, KDD '20, page 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703.

[30] R. Saha, V. Srivastava, and M. Pilanci. Matrix Compression via Randomized Low Rank and Low Precision Factorization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=rxsCTtkqA9.

[31] L. Schuchman. Dither Signals and Their Effect on Quantization Noise. IEEE Transactions on Communication Technology, 12(4):162-165, 1964. doi: 10.1109/TCOM.1964.1088973.

[32] P. Sharma, J. T. Ash, and D. Misra. The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction, 2023.

[33] Together Computer. Redpajama: an open dataset for training large language models, October 2023. URL https://github.com/togethercomputer/RedPajama-Data.

[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov,

P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023.

[35] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. D. Sa. QuIP\#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks, 2024.

[36] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144-160, 2019. doi: 10.1137/18M1183480. URL https://doi.org/10.1137/18M1183480.

[37] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi: 10.1017/9781108231596.

[38] M. Viazovska. The sphere packing problem in dimension 8. Annals of Mathematics, 185(3), May 2017. ISSN 0003-486X. doi: 10.4007/annals.2017.185.3.7. URL http://dx.doi.org/10.4007/annals.2017.185.3.7.

[39] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, 2019. In the Proceedings of ICLR.

[40] S. Xiang, Y. Zhu, X. Shen, and J. Ye. Optimal exact least squares rank minimization. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12, page 480-488, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450314626. doi: 10.1145/2339530.2339609. URL https://doi.org/10.1145/2339530.2339609.

[41] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. In Proceedings of the 40th International Conference on Machine Learning, 2023.

[42] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation, 2023.

[43] Z. Yuan, Y. Shang, Y. Song, Q. Wu, Y. Yan, and G. Sun. ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models, 2023.

[44] C. Zhang, J. Cheng, G. A. Constantinides, and Y. Zhao. LQER: Low-Rank Quantization Error Reconstruction for LLMs, 2024.
