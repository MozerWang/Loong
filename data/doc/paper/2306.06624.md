# RestGPT: Connecting Large Language Models with Real-World RESTful APIs 

Yifan Song ${ }^{1}$, Weimin Xiong ${ }^{1}$, Dawei Zhu ${ }^{1}$, Wenhao $\mathbf{W u}^{1}$, Han Qian ${ }^{2}$, Mingbo Song ${ }^{2}$<br>Hailiang Huang ${ }^{2}$, Cheng Li $^{3}$, Ke Wang ${ }^{3}$, Rong Yao ${ }^{3}$, Ye Tian ${ }^{3}$, Sujian $\mathbf{L i}^{1 *}$<br>${ }^{1}$ School of Computer Science, Peking University<br>${ }^{2}$ School of Electronics Engineering and Computer Science, Peking University<br>${ }^{3}$ Huawei Technologies<br>\{yfsong, lisujian\}@pku.edu.cn<br>https://restgpt.github.io


#### Abstract

Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI.


## 1 Introduction

Large language models (LLMs), such as GPT-3 [1] and ChatGPT [2], have shown various emergent abilities, including in-context learning [1, 3], reasoning [4, 5], and step-by-step planning [6, 7]. In pursuit of advancing the capabilities of LLMs for practical applications, an ongoing research direction is investigating the incorporation of external tools/APIs to enhance the functionality of LLMs 8, 9, 10, 11]. This endeavor has yielded successful integration of diverse tools, including search engines and other foundational models, with LLMs [12, 13, 14].

Despite significant progresses, we find that existing API-augmented LLMs are still in the experimental stage and have yet to fully meet the demands of real-world user instructions. As shown in Table 1., current methods are limited to connect with a small number of specially designed tools/APIs [11, 12, 15]. For example, Chameleon [12] designs a set of 15 tools, such as table verbalizer and image captioner. Additionally, the absence of a standardized API design specification obstructs the scalability of previous endeavors. Thus, the potential for connecting LLMs with a diverse range of real-world APIs, like RESTful APIs, remains under-explored and challenging. Furthermore, when dealing with a complex instruction in real scenario, it is necessary to decompose it into smaller subtasks and accomplish them by employing a mix of various APIs. As a result, it becomes essential for[^0]

| Model | API/Tool Use |  |  | Framework |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Num. | Extensibility | Schema | Planning | Planning Form | Feedback | Plug-n-Play |
| ReAct | 3 | - | Specialized | Online | Natural Lang. | $\checkmark$ | $\checkmark$ |
| Toolformer | 5 | - | Specialized | $x$ | - | $x$ | $x$ |
| Visual ChatGPT | 22 | - | Specialized | $x$ | - | Human | $\checkmark$ |
| ViperGPT | 11 | - | Python func. | Offline | Program | $x$ | $\checkmark$ |
| HuggingGPT | $24^{1}$ | + | HuggingFace | Offline | Natural Lang. | $x$ | $\checkmark$ |
| API-Bank | 53 | - | Specialized | $x$ | - | Human | $\checkmark$ |
| Chameleon | 15 | - | Specialized | Offline | Natural Lang. | $x$ | $\checkmark$ |
| Gorilla | $1645^{\dagger}$ | + | JSON | $x$ | - | $x$ | $x$ |
| GPT4Tools | 31 | - | Specialized | $x$ | - | Human | $x$ |
| RestGPT (ours) | $100+$ | ++ | RESTful | Online | Coarse-to-Fine | $\bar{\checkmark}$ | $\checkmark$ |

Table 1: A comparison of work that augments LLMs with API/tool usage. ${ }^{\dagger}$ denotes API selection with retrieval.

API-augmented LLMs to have robust planning and decision-making capabilities to effectively tackle real-world tasks [9]. Nonetheless, existing techniques, either offline introspective plan-then-execute methods [9, 13, 12] or the ReAct framework [16], encounter challenges in effectively adapting API feedback and generating viable plans.

In this work, we delve into a more realistic scenario by connecting LLMs with real-world RESTful APIs, aiming at fulfilling practical user instructions. RESTful is the de facto standard for web service development [17], which utilizes HTTP methods (e.g., GET, POST) and URIs to manipulate resources. RESTful API development typically adheres to the OpenAPI Specification (OAS) [18], which describes the operations, parameters, and response schemas of each API endpoint. Therefore, our resulting framework can connect with any RESTful application and offer standardized API development processes, thereby enabling enhanced extensibility compared to previous approaches. However, connecting LLMs with RESTful APIs also brings practical challenges. First, calling real-world APIs may give rise to a multitude of unforeseen situations, necessitating the framework to exhibit strong robustness and conduct reasonable planning. Second, the parameters and responses of RESTful APIs often follow specific formats, leading to difficulty in API invoking and response parsing.

To tackle the limitations of previous methods and the practical challenges associated with RESTful APIs, we propose RestGPT, a LLM-based framework connecting with RESTful APIs to handle complex instructions. RestGPT comprises three main modules: a Planner, an API Selector, and an Executor. The core of each module is prompting an LLM. Unlike prior work that uses static or ReAct style planning which lacks flexibility in realistic scenarios, RestGPT adopts an iterative coarse-to-fine online planning mechanism. Given a complicated instruction, the planner generates a sub-task for current task in the format of natural language. Subsequently, the API selector maps the coarse high-level sub-task to finer API calling plan, forming a coarse-to-fine task planning. The executor, responsible for invoking RESTful APIs and get execution results, is further divided it into two sub-modules: a Caller and a response Parser. The caller organizes API call parameters based on the API plan and API documentation, while the parser utilizes the response schema defined in OAS to generate Python code to parse responses. Once receiving the execution results of the API plan, the planner performs online planning for the subsequent sub-task in the next step. Through the integration of the three modules, our method RestGPT shows superior extensibility and flexibility in mastering RESTful APIs.

To evaluate the performance of RestGPT in utilizing RESTful APIs, we introduce RestBench, a human-annotated benchmark consisting of two realistic scenarios, TMDB movie database and Spotify music player. For each scenario, we collect diverse real-world user instructions that require the utilization of multiple APIs to complete. Based on the RestBench, we conduct comprehensive experiments to investigate the performance of RestGPT across different dimensions. The experimental results demonstrate that RestGPT exhibits robust capabilities in handling complex user instructions and has significant advantages in task planning, API understanding, and response parsing.[^1]

Our contributions can be summarized as follows:

1. For the first time, we attempt to connect large language models with RESTful APIs, enabling the resulting framework to be compatible with existing real-world applications while also providing powerful extensibility.
2. We propose RestGPT, a coarse-to-fine online planning framework that effectively handles the practical challenges associated with connecting LLMs with RESTful APIs, including API understanding, planning, and API response parsing.
3. To evaluate the performance of RestGPT, we build a human-annotated benchmark, RestBench, which comprises two practical scenarios. Experimental results show the capability of RestGPT to effectively utilize a number of RESTful APIs to accomplish complex instructions.

## 2 Background

### 2.1 Tool-Augmented Language Models

The emergence of recent powerful LLMs has enabled artificial intelligence systems to match human skills in utilizing tools [8, 9]. To enhance the performance of LLMs in accessing up-to-date information and carrying out precise mathematical reasoning, early work leverages simple tools like web search engines and calculators, such as ReAct [16], Toolformer [11], and ART [19]. Another line of research has focused on equipping LLMs to coordinate with external models for complex AI tasks, exemplified by HuggingGPT [13], ViperGPT [20], Visual ChatGPT [14] and Chameleon [12]. Recently, some work study how to enable open-sourced LLMs, such as LLaMa, to perform API usage [21, 15, 22]. Additionally, API-Bank [23] provides a systematic benchmark to showcase the efficacy of LLMs using tools to respond to human instructions.

Despite the notable advancements in incorporating tools for large language models, previous methods have exhibited certain limitations, most notably their restricted support for a limited number of specially designed APIs [12] and their inferior planning methods [9, 24, 12]. We compare RestGPT with other tool-augmented language models in Table 1. As shown, our work stands out by supporting for over 100 RESTful APIs. Furthermore, compared with most previous approaches adopt static offline planning which cannot interact with APIs and utilize feedback to adjust the plan, we employ a coarse-to-fine online planning framework with feedback, facilitating more flexible planning for complex instructions. Our work shares the similar spirit of AutoGPT, an autonomous agent capable of accomplishing complex tasks with numerous tools. While AutoGPT relies on developers to ensure compatibility with various applications, RestGPT can be integrated with any RESTful API-based applications in a plug-and-play fashion.

### 2.2 RESTful APIs

RESTful APIs have become a popular way to expose functionalities and data of web services to client applications [25, 17]. RESTful APIs also provide a standard for integrating external systems together with using a simple yet powerful interface. There are millions of RESTful APIs available on Internet, such as Spotify, Twitter, Gmail, etc. RESTful APIs are based on the REST architectural style, which emphasizes a client-server communication via stateless HTTP requests, including GET, POST, etc, where resources are identified by self-descriptive URIs [25]. The response of RESTful APIs are always structured in JSON format and contain various information. Thus, LLMs connected with RESTful APIs must possess a strong ability to extract the required information from the response.

OpenAPI Specification (OAS, or Swagger) [18], has been widely adopted as a standard for defining RESTful APIs. OAS is a structured documentation file which describes the endpoints, operations, parameters, response schemas, and other details of an API endpoint, providing a clear interface for our method to use the APIs.

![](https://cdn.mathpix.com/cropped/2024_06_04_bb60900547f00dfe3a19g-04.jpg?height=509&width=1334&top_left_y=236&top_left_x=401)

Figure 1: Overview of RestGPT. The planner, API selector, executor collaborate to form the coarseto-fine online planning framework. The caller and response parser in the executor provides robust execution of the RESTful API calling plan.

## 3 RestGPT

### 3.1 RestGPT Architecture

As demonstrated in Figure 1, RestGPT is composed of three main modules: a Planner $\mathcal{P}$, an API Selector $\mathcal{S}$ and an Executor $\mathcal{E}$. The planner decomposes each user instruction into several sub-tasks, while the API selector selects APIs to address each sub-task. The executor, consisting of a Caller and a response Parser, performs RESTful API calls and extracts useful information from the JSON response to form the execution result. The core of each component is an LLM with the corresponding prompt and in-context examples describing the function of the component.

One of the challenges in connecting LLMs with a vast number of APIs is to ensure that the framework is able to fully understand the API documents with a limited context window size of LLMs. As depicted in Figure 1. we designate different modules to read distinct parts of the OpenAPI Specification (OAS). This strategy allows us to leverage OAS information to its fullest potentials when working with RESTful APIs. Specifically, the API selector reads the endpoint descriptions of all APIs to select a proper API for solving the current sub-task. Then, the caller uses the detailed documents of the API within the API plan to generate the correct API calling parameters and request body. Lastly, the parser is developed to make use of the response schema within OAS to generate the parsing code for information extraction.

### 3.2 Coarse-to-fine Online Planning

To fully exploit the planning and decision making capabilities of LLMs and enable our method to dynamically adjust the plan to changing circumstances when accomplishing real-world user instructions, we propose a coarse-to-fine online planning mechanism in RestGPT.

The workflow of RestGPT can be characterized as an iterative "plan and execution" loop. During the planning stage, the planner and API selector collaborate to accomplish an instruction through iteratively decomposing it into suitable natural language sub-tasks and corresponding APIs. In each step $t$, the planner $\mathcal{P}$ leverages commonsense knowledge to generate a natural language (NL) sub-task $p_{t}$ based on the user instruction $q$, previous NL plans $\left(p_{1}, \ldots, p_{t-1}\right)$, and execution results $\left(r_{1}, \ldots, r_{t-1}\right)$, thereby constructing a high-level NL plan. Then, the API selector $\mathcal{S}$ reads the descriptions of available API endpoints to select appropriate APIs and construct the finer API plan $a_{t}$, which may contain a single or multiple API calls to solve the current NL plan $p_{t}$. Then the executor $\mathcal{E}$ executes the API plan $a_{t}$ and gets the execution result $r_{t}$ for current step. This process can be formulated as:

$$
\begin{align*}
\text { NL Plan: } p_{t} & \leftarrow \mathcal{P}\left(q ; p_{1}, r_{1} \ldots, p_{t-1}, r_{t-1}\right), \\
\text { API Plan: } a_{t} & \leftarrow \mathcal{S}\left(p_{t} ; r_{1}, \ldots, r_{t-1}\right),  \tag{1}\\
\text { Exec. Res.: } r_{t} & \leftarrow \mathcal{E}\left(a_{t} ; r_{1}, \ldots, r_{t-1}\right)
\end{align*}
$$

In this way, the planner and API selector are dedicated to NL sub-task planning and API selection, respectively, effectively utilizing the large language model's abilities of planning and text comprehension.

Alongside the "plan and execution" loop, we design two special states, "continual" and "end", for the planner to monitor the execution result from the executor. Specifically, if the planner finds that the current executor's output $r_{t}$ has not completed the present NL sub-task $p_{t}$, it will output a "continue" signal and provide a special NL plan $p_{t+1}$ to the API selector, instructing it to continue fulfilling the plan $p_{t}$. In such cases, the API selector will re-generate a new API plan based on the original NL plan $p_{t}$, new NL plan $p_{t+1}$, previous API plan $a_{t}$ and execution result $r_{t}$. This process is described as:

$$
\begin{align*}
\text { API Plan: } a_{t+1} & \leftarrow \mathcal{S}\left(p_{t}, p_{t+1} ; r_{1}, \ldots, r_{t-1} ; a_{t}, r_{t}\right) \\
\text { Exec. Res.: } r_{t+1} & \leftarrow \mathcal{E}\left(a_{t+1} ; r_{1}, \ldots, r_{t-1}, r_{t}\right) \tag{2}
\end{align*}
$$

If the planner assesses that the user's request has been completed, it will give the termination signal "end" and output the final result. With such a design, our method achieves a more flexible online planning which is capable of handling various situations encountered in real-world scenarios.

The planner, API selector, and executor collaborate to form RestGPT's coarse-to-fine online planning framework. This framework significantly enhances the ability to decompose tasks and select appropriate APIs, providing the model with the flexibility to effectively tackle user instructions.

### 3.3 API Plan Execution

Once an API calling plan is generated, the next step is to execute it. The executor $\mathcal{E}$ consists of a caller and a response parser. The caller should read the API documents carefully and generate correct parameters or request body for the API call. Due to the constraints of maximum context length, we filter API documents and only preserve APIs appearing in current API plan $a_{t}$. Given the generated parameters and request body, we use Requests Python library to call the RESTful API. Besides, to guide the response parser to extract information from the API response, the caller also generates a response description and output instruction for the response parser. Figure 2 presents an example output of the caller.

RESTful APIs typically return a JSON formatted

![](https://cdn.mathpix.com/cropped/2024_06_04_bb60900547f00dfe3a19g-05.jpg?height=450&width=680&top_left_y=1214&top_left_x=1075)

Figure 2: Example output of the caller. response with much redundant information. The executor needs to extract the required information from the response and return it to the planner. However, the response may sometimes have a complex structure or be lengthy, making it difficult to extract important information via directly prompting the LLMs. To address this problem, we make use of the response schema defined in the OAS. Specifically, we utilize the coding capability of LLM to generate Python parsing code based on the provided schema and output instructions generated by the caller. Next, the Python code is executed to get the final result. If there are no execution exceptions or errors, the output is returned. Otherwise, the LLM is prompted to parse the response directly as a backup.

## 4 RestBench

To assess the effectiveness of RestGPT in processing complex user instructions through RESTful APIs, we introduce RestBench, a high-quality human annotated dataset comprising of two real-world scenarios. Existing researches have proposed several benchmarks for the evaluation of tool/API augmented LLMs [23, 21, 9]. However, these benchmarks primarily focus on simple tasks that can be accomplished using a single API. We hope RestBench can facilitate the exploration on utilizing multiple APIs to address real-world user instructions.

| Scenario | Num. <br> APIs | Len. of Solution Path |  | Avg. <br> Len. | Total |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | 1 | 2 | 3 | 4 |  |  |  |
| TMDB | 54 | 5 | 66 | 27 | 2 | 2.3 | 100 |
| Spotify | 40 | 8 | 18 | 22 | 9 | 2.6 | 57 |

Table 2: Statistics of RestBench test set. We report the number of instructions with different lengths of solution path.

### 4.1 Scenarios and APIs

We select two common real-world scenarios: TMDB movie database and Spotify music player. The main consideration is to evaluate the capabilities of RestGPT: (1) augmenting LLMs with external specialized domain database via RESTful APIs; (2) connecting LLMs with RESTful APIs to autonomously control real-world applications. TMDB offers official RESTful APIs encompassing the information of movies, TVs, actors, and images. Spotify music player provides API endpoints to retrieve content metadata, receive recommendations, create and manage playlists, and control playback. For these two scenarios, we filter out 54 and 40 commonly used APIs respectively and obtain the corresponding OpenAPI Specifications to build RestBench.

### 4.2 Dataset Collection

High-quality instructions generally satisfy two crucial aspects: (1) to reflect a wide range of real user needs; (2) to cover different levels of complexity to fully study the reasoning and planning ability of our method. To achieve these goals, we adopt a bottom-up instruction collection approach. We employ 6 experts that work on NLP research to brainstorm instructions for different combinations of APIs. Along with the instructions, the experts need to annotate the gold API solution path for each instruction. To guarantee the quality of the instructions, we employ two additional experts to thoroughly verify the solvability of each instruction and correctness of the corresponding solution path. Ultimately, we annotate 10 instruction-solution pairs for each scenario as the development set, and 100 pairs for TMDB and 57 pairs for Spotify as the test set. Though the data scale is not large, these instructions are typical of the frequently raised user

| Instruction: $\quad$ TMDB |
| :--- |
| Who is the director of today's most trending movie? |
| Gold Solution Path: |
| 1. GET /trending/\{media_type\}/\{time_window\} |
| 2. GET /movie/\{movie_id\}/credits |
| $\quad \triangleright$ Spotify |
| Instruction: |
| Make me a playlist containing three songs of Mariah |
| Carey and name it 'Love Mariah’ |
| Gold Solution Path: |
| 1. GET /search |
| 2. GET /me |
| 3. POST /users/\{user_id\}/playlists |
| 4. POST /playlists/\{playlist_id\}/tracks |

requests. Moreover, different from prior work

Table 3: Example instructions and the corresponding gold solution paths of RestBench. which uses LLMs to get API calling procedure, we utilize human labeled API solution paths for evaluation. Table 3 presents example instructions of the two scenarios. The statistics of RestBench are shown in Table 2

### 4.3 Evaluation Metrics

Since some user requests are time-dependent (see the TMDB example in Table 3, it is impractical to annotate a fixed ground-truth answer for each instruction, whereas, the API solution paths for most instructions remain consistent. If the model-generated API call path contains the gold API call path as a subsequence (with the elements not necessarily being contiguous), we think that the model has generated a correct path. To further evaluate the model's performance, we rely on human evaluation to determine if the model result successfully fulfills the user query. We calculate the proportion of correct paths and successful query completions as metrics, i.e., Correct Path Rate and Success Rate. Moreover, the number of actual API calls can be utilized to measure the planning efficiency of different methods. Given the length of gold solutions, we further define $\Delta$ Solution Len. as the

| Model | TMDB |  |  |  | Spotify |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Success\% | CP\% | $\Delta$ Solution Len. |  | Success\% | CP\% | $\Delta$ Solution Len. |
| Offline [9] | 29.0 | 33.0 | +1.52 | 14.5 | 36.4 | +1.10 |  |
| DEPS [7] | 38.0 | 43.0 | +1.20 |  | 19.3 | 43.8 | +1.74 |
| ReAct [16] | 44.0 | 57.0 | +0.76 |  | 54.5 | 49.1 | +0.31 |
| Reflexion [26] | 52.0 | 59.0 | +1.37 | 59.6 | 61.4 | +1.68 |  |
| RestGPT | $\mathbf{7 5 . 0}$ | $\mathbf{7 9 . 0}$ | $\mathbf{+ 0 . 5 5}$ | $\mathbf{7 2 . 7}$ | $\mathbf{7 4 . 5}$ | +0.25 |  |
| w/o Planner | 44.0 | 57.0 | +0.76 | 54.5 | 49.1 | +0.31 |  |
| w/o Parser | 46.0 | 53.0 | +0.60 |  | 47.3 | 52.7 | $+\mathbf{0 . 2 4}$ |
| RestGPT (ChatGPT) | 68.0 | 65.0 | +0.72 | 69.1 | 72.3 | +0.28 |  |
| RestGPT (Llama2-13B) | 0.0 | 0.0 | - | 0.0 | 0.0 | - |  |
| RestGPT (Vicuna-13B) | 9.0 | 15.0 | +1.21 | 12.7 | 20.6 | +1.52 |  |

Table 4: Success rate (\%), Correct Path rate (CP, \%), and $\Delta$ Solution Length on two scenarios of RestBench. The best results are in boldface. $\dagger$ RestGPT w/o planner is equivalent with ReAct equipped with our proposed executor.

mean number of additional API calls required to successfully execute an instruction:

$$
\Delta \text { Solution Len. }=\frac{1}{N_{s}} \sum_{i=0}^{N}\left(L_{\text {real }}^{i}-L_{\text {gold }}^{i}\right) \cdot \mathbb{I}(i, \text { success })
$$

where $N_{s}$ is the number of successfully accomplished instructions, $L_{\text {real }}^{i}$ and $L_{\text {gold }}^{i}$ are the actually and gold number of API calls for the $i$-th instruction respectively, $\mathbb{I}(i$, success) denotes whether the $i$-th instruction is successfully completed.

## 5 Experiments

### 5.1 Experimental Setup

We compare RestGPT with four recent baselines, including offline introspective method [9] used in HuggingGPT [13] and Chameleon [12], DEPS [7], ReAct [16] and Reflexion [26]. Since some methods are not originally designed for tool/API usage, we reproduce them and add the API executor proposed in Section 3.3 to make them able to call RESTful APIs. The maximum steps for DEPS is set to 10 and the maximum trials for Reflexion is set to 2 .

To showcase the planning and API calling capabilities of our method, we implement two ablation variants of RestGPT. The first variant involves removing the planner and allowing the API selector to directly choose APIs in a ReAct style. This approach can be seen as ReAct equipped with our proposed executor. The second one is to replace the schema-based response parser with an LLM that directly reads and extracts the required information from the JSON response.

In our experiments, we employ text-davinci-003 from OpenAI as the LLM for RestGPT and all baselines. The decoding temperature is set to 0 for the most deterministic generation.

### 5.2 Main Results

Table 4 shows the performance of RestGPT and baselines on two scenarios. Our approach outperforms all other methods in both scenarios, achieving a success rate of $75 \%$ on the movie database and over $70 \%$ on the music player. Note that in most cases, the correct path rate is slightly higher than success rate, indicating that the method may generate correct API calling plan but fail to execute it. RestGPT also stands out with its minimal solution length, showcasing the superior planning ability of the coarse-to-fine online planning mechanism.

Ablation experiments on coarse-to-fine planning and schema-based parser show both mechanisms are conductive to the model performance. Particularly, when removing the planner, the performance degrades significantly, indicating that current LLMs are unable to simultaneously conduct planning, API understanding and selection. Thus, the coarse-to-fine planning mechanism plays a crucial role in

![](https://cdn.mathpix.com/cropped/2024_06_04_bb60900547f00dfe3a19g-08.jpg?height=206&width=1241&top_left_y=247&top_left_x=431)

Figure 3: Error breakdown of RestGPT on RestBench. Error types are categorized by the module where the error occurred.
![](https://cdn.mathpix.com/cropped/2024_06_04_bb60900547f00dfe3a19g-08.jpg?height=408&width=1314&top_left_y=598&top_left_x=400)

Figure 4: Scaling ability of RestGPT. (a) (b) Scaling curves of the gold solution path on TMDB and Spotify. The length of gold API solution path indicates the complexity of the instruction. (c) Scaling curves of the number of APIs on TMDB scenario.

our framework. The ablation results without parser demonstrates that the schema-based parser enables LLMs to better comprehend and parse the real-world API responses with complicated structure.

To investigate the performance of our method with different base LLMs, we implement RestGPT with ChatGPT (gpt-3.5-turbo-0301), Llama2-13B (Llama-2-13b-chat-hf), and Vicuna-13B (vicuna13b-v1.5). As shown in Table4, the performance of ChatGPT is slightly worse than text-davinci-003. Interestingly, we have tried all official checkpoints of Llama2-13B, but none of them were able to comprehend the prompt and generate valid plans. In contrast, Vicuna-13B, which is fine-tuned from Llama2 on user-shared conversations, can accomplish some simple instructions. This result indicates that by fine-tuning LLMs on ChatGPT-generated data, the model can acquire the ability to understand and follow complicate prompts.

### 5.3 Error Analysis

To further investigate the effectiveness of different modules in RestGPT, we conduct error analysis. In Figure 3 , we classify errors based on the module in which they occur. We discover that the majority of errors occur during the planning stage, i.e., within the planner (purple) and API selector (blue). The planner sometimes loses track of its intended objective after multiple rounds of execution, resulting in early exit before completing the instruction. For the API selector, it may either select incorrect APIs or hallucinate to make up in-path parameters. This error analysis highlights the insufficient planning and decision-making capabilities of LLMs.

Compared with text-davinci-003, ChatGPT tends to make more errors in the planning stage, leading to slightly worse performance on both scenarios. More specifically, we find that ChatGPT is often too verbose and tend to continue planning even after the user instruction has been fulfilled. This behavior can be attributed to the fact that ChatGPT is trained specifically for conversational interactions, which encourages it to generate more lengthy responses.

### 5.4 Scaling Curves

In this section, we aim to demonstrate the scaling ability of RestGPT on two dimensions: scaling the difficulty of the tasks and scaling the number of APIs.

For each instruction in RestBench, the length of gold solution path indicates the complexity of the instruction. We calculate the success rate of models on instructions with varying complexities.

![](https://cdn.mathpix.com/cropped/2024_06_04_bb60900547f00dfe3a19g-09.jpg?height=572&width=1393&top_left_y=234&top_left_x=363)

Figure 5: Case study of three methods, (a) Offline [9, 13, 12], (b) ReAct [16], and (c) RestGPT. For offline method, we only show the generated plan. For ReAct and RestGPT, we omit the detailed execution process of the executor.

As depicted in Figure 4 (a) (b), the success rate of all methods decreases as the complexity of the instruction increases. Notably, when the gold path length is 4 , all baselines struggle to complete the task in both scenarios. In contrast, our proposed RestGPT can still achieve a success rate of over $40 \%$, showing its superior performance in planning and API calling.

Before conducting experiments on scaling the number of APIs, we handpicked 10 APIs from TMDB and created a small test set comprising 15 instructions. All 15 instructions can be resolved using the selected 10 APIs. Then, we increasingly expanded the number of APIs and introduced additional noise APIs sourced from the official TMDB APIs. The results are shown in Figure 4 (c). As the number of noise APIs increases, the performance of all baseline methods deteriorates due to their inferior planning and reasoning. However, our method almost remains unaffected. These results effectively demonstrate the strong extensibility of our proposed RestGPT.

### 5.5 Case Study

In Figure 5, we conduct a case study to compare the planning ability of RestGPT with the offline planning [9. 12] and ReAct [16] framework. Firstly, we observe the offline method is unable to solve most user instructions. As depicted in Figure 5 (a), the planner not only selects the wrong API (step 2), but also ignores the dependencies between APIs and used the parameter "user_id" before obtaining it (step 4). Regarding ReAct which generates chain-of-thought and actions in an interleaved manner, we find that current LLMs have a limited ability to simultaneously conduct planning, API understanding and selection. As shown in Figure 5(b), the planner of ReAct generates a sub-task that is difficult to solve (step 2) and also ignores the dependencies between different APIs (step 3). Due to the inferior planning, it consumes 6 API calls to complete the task. In contrast, RestGPT employs a planner to generate high-level NL sub-tasks and an API selector to choose appropriate APIs to solve the sub-task. Notably, in step 3, the planner assesses the playlist that has not been successfully created and generate "continue" signal with further instructions for the API selector. Our method accomplishes the instruction with only 4 API calls. The coarse-to-fine online planning framework of RestGPT fully exploits the LLMs' planning and document understanding capabilities, providing the model with the flexibility to tackle complex user requests.

## 6 Conclusion

In this paper, we explore the scenarios of connecting current large language models (LLMs) with real-world applications via RESTful APIs. To overcome the limitations of existing approaches and tackle the challenges in integrating LLMs with RESTful APIs, we propose RestGPT, an approach that leverages LLMs to complete complex user instructions. Our method features a coarse-to-fine online planning mechanism to enable more flexible planning and API selection. Furthermore, to handle the complex scenario of calling RESTful APIs, we designed a specialized API executor to
formulate parameters and parse API responses. To assess the performance of our method, we build a high-quality dataset, RestBench, which consists of human-annotated instructions from two realistic scenarios. Extensive experiments demonstrate that RestGPT achieves impressive results in complex tasks and exhibits strong robustness, which paves a new way towards AGI. In the future, we aim to delve into a broader range of intricate tasks, thoroughly examining the immense potential of RestGPT across both academic and industrial domains.

## References

[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[2] OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt

[3] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.

[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.

[5] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.

[6] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.

[7] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.

[8] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.

[9] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023 .

[10] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022 .

[11] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

[12] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023.

[13] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.

[14] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[15] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752, 2023.

[16] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.

[17] Li Li, Wu Chou, Wei Zhou, and Min Luo. Design patterns and extensibility of rest api for networking applications. IEEE Transactions on Network and Service Management, 13(1):154-167, 2016.

[18] SmartBear. Swagger, 2023. URLhttps://swagger.io/

[19] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.

[20] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.

[21] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.

[22] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.

[23] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023.

[24] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.

[25] Mark Masse. REST API design rulebook: designing consistent RESTful web service interfaces. " O'Reilly Media, Inc.", 2011.

[26] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.

![](https://cdn.mathpix.com/cropped/2024_06_04_bb60900547f00dfe3a19g-12.jpg?height=854&width=829&top_left_y=240&top_left_x=648)

Figure 6: A RESTful API from TMDB.
