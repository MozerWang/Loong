# MediTron-70B: Scaling Medical Pretraining for Large Language Models 

Zeming Chen $^{1} \quad$ Alejandro Hernández Cano ${ }^{1 \ddagger}$ Angelika Romanou ${ }^{1 \ddagger}$ Antoine Bonnet ${ }^{1 \ddagger}$<br>Kyle Matoba $^{1,2 \ddagger} \quad$ Francesco Salvi $^{1} \quad$ Matteo Pagliardini ${ }^{1} \quad$ Simin Fan $^{1} \quad$ Andreas Köpf $^{3}$<br>Amirkeivan Mohtashami ${ }^{1} \quad$ Alexandre Sallinen ${ }^{1} \quad$ Alireza Sakhaeirad ${ }^{1} \quad$ Vinitra Swamy $^{1}$<br>Igor Krawczuk $^{1}$ Deniz Bayazit ${ }^{1}$ Axel Marmet ${ }^{1} \quad$ Syrielle Montariol $^{1}$<br>Mary-Anne Hartley ${ }^{1,4}$ Martin Jaggi ${ }^{1 \dagger} \quad$ Antoine Bosselut ${ }^{1 \dagger}$<br>${ }^{1}$ EPFL $\quad{ }^{2}$ Idiap Research Institute $\quad{ }^{3}$ Open Assistant $\quad{ }^{4}$ Yale<br>\{zeming.chen, antoine.bosselut\}@epfl.ch


#### Abstract

Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closedsource (e.g., PaLM, GPT-4) or limited in scale ( $\leq$ 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEdiTRon: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a $6 \%$ absolute performance gain over the best public baseline in its parameter class and 3\% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEdiTRon-70B outperforms GPT-3.5 and Med-PaLM and is within 5\% of GPT-4 and $10 \%$ of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs.


![](https://cdn.mathpix.com/cropped/2024_06_04_ade956008c157567bd85g-01.jpg?height=521&width=1183&top_left_y=1783&top_left_x=468)

Figure 1: MediTron-70B's performance on MedQA MediTron-70B achieves an accuracy of $70.2 \%$ on USMLE-style questions in the MedQA (4 options) dataset.[^0]

## Safety Advisory

While MediTron is designed to encode medical knowledge from sources of high-quality evidence, it is not yet adapted to deliver this knowledge appropriately, safely, or within professional actionable constraints. We recommend against deploying MEDITrON in medical applications without extensive use-case alignment, as well as additional testing, specifically including randomized controlled trials in real-world practice settings.

## 1 Introduction

Medicine is deeply rooted in knowledge, and recalling evidence is key to guiding standards in clinical decision-making. However, while 'Evidence-based medicine' (EBM) is now synonymous with quality care, it requires expertise that is not universally available. Thus, ensuring equitable access to standardized medical knowledge is an ongoing priority across all domains of medicine. Recent advances in large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Almazrouei et al., 2023; Touvron et al., 2023b; OpenAI, 2023b; Chowdhery et al., 2022) have the potential to revolutionize access to medical evidence. Today, the largest LLMs have tens or hundreds of billions of parameters (Bommasani et al., 2021; Hoffmann et al., 2022; Kaplan et al., 2020) and are trained on enormous pretraining corpora (Raffel et al., 2019; Gao et al., 2020; Together AI, 2023; Soldaini et al., 2023). This unprecedented scale has enabled emergent properties in LLMs that are core traits of human decision-making: step-by-step chain-of-thought reasoning, coherent communication, and contextual interpretation (Bubeck et al., 2023; Wei et al., 2023; Wang et al., 2023).

Until recently, LLMs have been developed and evaluated for generalist tasks, principally using data collected from diverse internet sources with varying levels of quality in terms of domain-specific evidence (Rozière et al., 2023). This approach, while generally very powerful, hampers task-specific performance, including in the medical domain. Several newer task-specific models, trained on more carefully curated datasets, have repeatedly out-performed generalist models (Wu et al., 2023b; Yue et al., 2023; Rozière et al., 2023; Azerbayev et al., 2023), revealing the potential of balancing quality with quantity with regards to pretraining data. A promising method for achieving this equilibrium is to use general-purpose LLMs and then continue training on more selective domain-specific data. These systems acquire a combination of both natural and domain-specific language understanding and generation skills (Gururangan et al., 2020). In the medical domain, this approach has only been reported for models below 13B parameters (Lee et al., 2020; Gu et al., 2021; Peng et al., 2023; Wu et al., 2023a). At larger scales (i.e., $\geq$ 70B-parameters), prior studies have only explored the scope of instruction-tuning (M42-Health) or parameter-efficient finetuning (Toma et al., 2023).

In this work, we present MEDITRon-7B and 70B, a pair of generative LLMs for medical reasoning, adapted from Llama-2 (Touvron et al., 2023b) through continued pretraining on carefully curated high-quality medical data sources: PubMed Central (PMC) and PubMed open-access research papers (collected through the S2ORC corpus, Lo et al., 2020), PubMed abstracts (from non-open-access papers) in S2ORC, and a unique set of diverse medical guidelines from the internet, covering multiple countries, regions, hospitals, and international organizations. To enable training, we extend Nvidia's Megatron-LM distributed training library to support the Llama-2 architecture.

We evaluate MediTron on four medical reasoning benchmarks using both in-context learning (providing examples during prompting, i.e., within the context window) and task-specific finetuning. The benchmarks comprise two medical examination question banks, MedQA (from the United States Medical Licensing Examination, Jin et al., 2020), and MedMCQA (a Multi-Subject Multi-Choice Dataset for the Medical domain, Pal et al., 2022), PubMedQA (biomedical question answering based on PubMed abstracts, Jin et al., 2019), and MMLU-Medical (a medically themed evaluation set from Massive Multitask Language understanding, Hendrycks et al., 2021a). Using in-context learning without fine-tuning, MEdiTRON-7B outperforms several state-of-the-art baselines, showing a $10 \%$ average performance gain over PMC-Llama-7B (a similar LLM adapted from Llama, Touvron et al., 2023a, through continued pretraining on PubMed Central papers), and a 5\% average performance gain over the Llama-2-7B model. After finetuning on task-specific training data, MEDITRON's performance also improves over other finetuned baselines at the same scale, achieving a $5 \%$ (7B) and a $2 \%$ (70B) average performance gain. Finally, finetuning MEDITron-70B to support advanced prompting
strategies such as chain-of-thought and self-consistency further improves over the best baseline by $3 \%$ and the best public baseline by $12 \%$. Overall, MEdiTron achieves strong performance on medical reasoning benchmarks, matching or outperforming state-of-the-art baselines at the same scale.

In summary, we propose an optimized workflow to scale domain-specific pretraining for medical LLMs, incorporating knowledge-based data curation, continued pretraining via a distributed training pipeline, finetuning, few-shot in-context learning, and advanced inference methods such as chainof-thought reasoning and self-consistency. We release the curated training corpus, the distributed training library ${ }^{2}$, and the MEDITRON models (7B and 70B) ${ }^{3}$ with and without fine-tuning to the public to ensure access for real-world evaluation and to facilitate similar efforts in other domains.

## 2 Medical Training Data

MediTron 's domain-adaptive pre-training corpus GAP-REPLAY combines 48.1B tokens from four datasets; Clinical Guidelines: a new dataset of $46 \mathrm{~K}$ clinical practice guidelines from various healthcare-related sources, Paper Abstracts: openly available abstracts from 16.1M closed-access PubMed and PubMed Central papers, Medical Papers: full-text articles extracted from 5M publicly available PubMed and PubMed Central papers, and a Replay dataset: general domain data distilled to compose $1 \%$ of the entire corpus.

### 2.1 Clinical Guidelines

Clinical practice guidelines (CPGs) are rigorously researched frameworks designed to guide healthcare practitioners and patients in making evidence-based decisions regarding diagnosis, treatment, and management (Berg et al., 1997). They are compiled through a systematic process of collaborative consensus between experts to establish recommendations from the latest evidence on best practices that would maximize benefit in light of practical concerns such as available resources and context. As a super-synthesis of meta-analyses, they sit atop the 'evidence pyramid' and form the basis of actionable evidence-based practice (Burns et al., 2011). CPGs are produced at various geographic and organizational granularities, ranging from global to hospital-level initiatives directed by international professional medical associations to informal consortia, regional or national governmental bodies to individual NGOs and hospitals.

Our GUIDELINES pre-training corpus comprises 46,469 guideline articles from 16 globally recognized sources for clinician and patient-directed guidance across high and low-resource settings, multiple medical domains (internal medicine, pediatrics, oncology, infectious disease, etc.), and various geographic granularities. The full list of sources used, along with the descriptive statistics of each source, can be found in Table 9. We publicly release ${ }^{4}$ a subset of 35,733 articles from the GUIDELINES corpus, extracted from 8 of 16 sources allowing content redistribution, namely CCO, CDC, CMA, ICRC, NICE, SPOR, WHO and WikiDoc. For all 16 sources, we release our web scrapers and pre-processing code.

Collection and processing We employed pragmatic selection criteria, seeking CPGs that were: (1) open-access, (2) systematically formatted with homogenous textual structure (i.e., in a format in which automated processes could be deployed without excessive risk of misaligning textual sequences), (3) in the language predominantly represented by the pre-training corpus of Llama (i.e., English), and (4) covering a breadth of medical sub-domains, audiences (clinician, nurse, patient), and resource settings (high, low, and humanitarian response settings).

After extracting the raw text from each source, we cleaned data to exclude irrelevant or repetitive content that did not contribute to the textual content, such as URLs, references, figures, table delimiters, and ill-formatted characters. Additionally, the text was standardized to a unified format with indicated section headers, homogenous space separating paragraphs, and normalized lists. Finally, all samples were deduplicated using title matching, and articles that were too short or not English were filtered out.[^1]

| Dataset | Number of samples |  |  | Number of tokens |  |
| :--- | ---: | ---: | ---: | ---: | ---: |
|  | Train | Validation |  | Train | Validation |
| Clinical Guidelines | $41 \mathrm{~K}$ | $2284(5 \%)$ |  | $107 \mathrm{M}$ | $6 \mathrm{M}(5 \%)$ |
| PubMed Abstracts | $15.7 \mathrm{M}$ | $487 \mathrm{~K}(3 \%)$ |  | $5.48 \mathrm{~B}$ | $170 \mathrm{M}(3 \%)$ |
| PubMed Papers | $4.9 \mathrm{M}$ | $142 \mathrm{~K}(3 \%)$ |  | $40.7 \mathrm{~B}$ | $1.23 \mathrm{~B}(3 \%)$ |
| Experience Replay | $494 \mathrm{~K}$ | $0(0 \%)$ |  | $420 \mathrm{M}$ | $0(0 \%)$ |
| Total | $21.1 \mathrm{M}$ | $631 \mathrm{~K}$ |  | $46.7 \mathrm{~B}$ | $1.4 \mathrm{~B}$ |

Table 1: GAP-Replay data mixture statistics. The size of both training and validation sets of the GAP-REPLAY pre-training mixture. For each set, we give the total number of samples and the total number of tokens belonging to each dataset. The portion of each dataset allocated to the validation set (relative to the training set) is given as a percentage.

Content The GUIDELINES corpus comprises a broad range of contexts. For instance, the geographic scope ranges from global (WHO) to national (CDC, NICE) and regional (Ontario, Melbourne) to institutional (ICRC, Mayo Clinic). The corpus also represents health care concerns from high(Ontario, Melbourne), low- (WHO), and volatile- (ICRC) resource settings. GUIDELINES also contains a range of technical and conversational vocabulary with target audiences of clinicians or patients (or both), and is sometimes highly specialized within a theme (cancer, pediatrics, infectious disease). The peer review processes also ranged from UN bodies (WHO), institutional review boards (ICRC), professional associations (AAFP) to publicly crowdsourced knowledge bases (WikiDoc).

### 2.2 PubMed Papers \& Abstracts

Adapting a large language model to the health domain requires vast amounts of biomedical textual data. As the largest public corpus of medical research papers, PubMed was chosen to form the backbone of MediTron's pre-training mix. From the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020), which aggregates papers from hundreds of academic publishers and digital archives into a unified source, we collected 4.47M full-text papers from the PubMed Central Open Access Subset (National Library of Medicine, 2003-2023). We added 444,521 open-access full-text PubMed papers that are not found in the PubMed Central archive. Finally, we collected 16,209,047 PubMed and PubMed Central abstracts for which full-text versions are unavailable in S2ORC. The knowledge cutoff for all papers and abstracts in the corpus is August 2023.

Pre-processing PubMed For all full-text articles, we removed the metadata information and references, namely the authors, bibliography, acknowledgments, tables, and figures, and kept only the main text of each paper. Using automatic annotations from S2ORC, we identified inline citations, section headers, figures, and tables within the text using special tokens to allow for higher flexibility in downstream tasks. To promote the use of accurate citations by the model, we formatted all in-text references with a similar methodology to the Galactica model (Taylor et al., 2022). We replaced the paper citations with the special token [BIB_REF] and formatted them with the referenced paper's title, truncated to a maximum of 12 words, and the main author's last name. Similarly, we wrapped in-text figure and table references with the special token [FIG_REF] and formatted them with the figure number and the truncated figure caption. Finally, we wrapped all mathematical formulas using the special tokens [FORMULA]. We additionally removed URLs and references and normalized whitespace between paragraphs. To promote hierarchical structure learning, we indicate section headers with '\#' for main sections and '\#\#' for subsections. We also prepend the paper title to the main body. We performed the same formatting procedure described above for both abstracts and full-text articles. We deduplicated articles and abstracts based on PubMed and PubMed Central IDs and filtered out non-English content. Additional details on our pre-processing procedure are given in Appendix B.2.

### 2.3 Experience Replay

Experience replay refers to the process of including data from old, previously seen tasks when training on new tasks. Distilling replay data into the training mixture has been shown to overcome catastrophic forgetting, a phenomenon where a model incorporating out-of-distribution data forgets
its previous training (Sun et al., 2020b). To promote the retention of knowledge acquired by the pre-trained Llama-2 model, we included general domain data into GAP-REPLAY that consists of the $1 \%$ of the mixture. We used a randomly selected subset of 420 million tokens from the RedPajama dataset, an open-access equivalent to the original Llama-2 pre-training corpus (Together AI, 2023). This dataset contains a mixture of the Falcon refined web corpus (Penedo et al., 2023), the StarCoder dataset (Li et al., 2023), and Wikipedia, ArXiv, books, and StackExchange.

## 3 Engineering

Training LLMs at scale presents an important engineering challenge. The large model parameter size and pretraining token count require a framework for large-scale distributed training that can harness the power of multiple GPUs across many computation nodes. To distribute the training within a cluster, we developed the Megatron-LLM distributed training library (Cano et al., 2023), which extends Nvidia's Megatron-LM (Shoeybi et al., 2019; Narayanan et al., 2021) to support the training of three popular open-source LLMs that have recently been released: Llama, Falcon, and Llama-2. We use it to pretrain and finetune all MEDiTron models. The library supports several forms of complementary parallelism for distributed training, including Data Parallelism (DP - different GPUs process different subsets of the batches), Pipeline Parallelism (PP - different GPUs process different layers), Tensor Parallelism (TP - different GPUs process different subtensors for matrix multiplication). The library also includes activation recomputation to reduce memory usage at the expense of increased computation times, sequence parallelism to exploit further the coordinate-wise independence of batch norm and dropout operations (see (Korthikanti et al., 2022)), fused operations, and other modern primitives to help increase training throughput.

Natively, Megatron-LM's language modeling is oriented around a GPT-like architecture. We extended its functionalities to support the Llama (Touvron et al., 2023a), Llama-2 (Touvron et al., 2023b), and Falcon (Almazrouei et al., 2023) models. We integrate necessary new architecture features such as the rotary position embedding (Chen et al., 2023), grouped-query attention (Ainslie et al., 2023), the parallel attention/MLP in the transformer layer of Falcon-40B, and the unbinding of the word embedding and the next-token prediction classifier weights used in Llama. We also added support for FlashAttention (Dao et al., 2022) and FlashAttention-2 (Dao, 2023) for more efficient inference and long-context decoding.

Hardware The MediTron models are trained on an in-house cluster with 16 nodes, each with 8 Nvidia A100 80GB GPUs. The nodes are equipped with $2 \times$ AMD EPYC 7543 32-Core Processors and $512 \mathrm{~GB}$ of RAM. The large parameter size of models requires distributed training across many GPUs and computation nodes, making network efficiency paramount. The 16 nodes used for training are connected via RDMA over Converged Ethernet. The 8 Nvidia A100 80GB GPUs in each node are connected by NVLink and NVSwitch with a single Nvidia ConnectX-6 DX network card. ${ }^{5}$ We expect relatively low inter-node bandwidth to relatively disadvantageous forms of parallelism, such as pipeline parallelism, which relies upon communicating activation values across nodes.

Model Parallelism Narayanan et al. (2021) prescribe that tensor parallelism equal to the number of GPUs per node should be used, which is 8 in our cluster. We empirically found this to be correct across every parallelization configuration considered and do not analyze it further. For our largest training run using a 70 billion parameter model, we use a pipeline parallelism (PP) factor of 8 . With a total of 128 GPUs in our cluster, we get a data parallelism (DP) of 2 ( $=128$ / TP / PP). We use a micro-batch size of 2 and a global-batch size of 512. Although one would prefer larger batch sizes in general for greater pipeline parallelism, we observe negative impacts from a discretization problem: raising the micro-batch size from 2 to 4 simply requires too much memory that must be compensated by less pipeline parallelism. We note that Narayanan et al. (2021, Figure 13) also shows that on a similar-sized problem with a similar number of GPUs, with (TP, PP) $\in\{(2,32),(4,16),(8,8),(16,4),(32,2)\}$, $\mathrm{TP}=\mathrm{PP}=8$ is also observed to deliver the highest per-GPU flops. Fundamentally, we do find that $3 \mathrm{D}$ model parallelism is necessary for the efficient training of models of this scale in the sense that TP, PP, and DP are all greater than one.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_ade956008c157567bd85g-06.jpg?height=604&width=1399&top_left_y=232&top_left_x=360)

Figure 2: MediTron. The complete pipeline for continued pretraining, supervised finetuning, and evaluation of MEDITRON-7B and MEDITRON-70B.

## 4 Modeling

### 4.1 Pretraining

To adapt the Llama-2 (Touvron et al., 2023b) language model to the medical domain, we start with the process of continued pretraining on the GAP-REPLAY data mixture we build in Section 2. This mixture contains papers from PubMed and PubMed Central (PMC), abstracts from PubMed, medical guidelines published and used by different regions, hospitals, and health organizations, as well as experience replay data (see Table 1 ).

Training Details We adopt most pretraining settings and model architecture from the Llama-2 paper (Touvron et al., 2023b). For optimization, we use the AdamW optimizer with a cosine learning rate scheduler. For the model architecture, we inherit the standard transformer architecture, the use of RMSNorm, the SwiGLU activation function, and rotary positional embeddings directly from the implementation of Llama. We use group-query attention (GQA) introduced by Llama-2, and a context length of 2048 for the 7B model and 4096 for the 70B model.

For the pretraining run with Llama-2-70B, we achieve a throughput of 40,200 tokens/second. This amounts to $1.6884 \times 10^{16}$ bfloat16 flop/second and represents roughly $42.3 \%$ of the theoretical peak flops of 128 A100 GPUs, which is $128 \times\left(312 \times 10^{12}\right)=3.9936 \times 10^{16}$ flops. This is in line with existing runs of comparable size. For instance, Narayanan et al. (2021, Table 1) shows a model flops utilization (MFU) of $45 \%$ for a 76B parameter GPT-3, and Mangrulkar et al. (2023) gives an MFU of $45.5 \%$ on a Llama-2 finetuning task similar to ours.

Hyperparameters and Tokenization The parameters for the AdamW optimizer are as follows: $\beta_{1}=0.9, \beta_{2}=0.95$, eps $=10^{-5}$. The cosine learning rate schedule uses 2000 steps for warmup and decays the final learning rate to $10 \%$ of the maximum learning rate. We use $1.5 \times 10^{-4}$ as the learning rate for the $70 \mathrm{~B}$ model and $3 \times 10^{-4}$ for the $7 \mathrm{~B}$ and $13 \mathrm{~B}$ models. The weight decay is set to 0.1 , and the gradient clipping is set to 1.0. We inherit the tokenizer from Llama and use the bytepair encoding algorithm (BPE) implemented with SentencePiece. The total vocabulary size is $32 k$ tokens. Extra tokens are added to incorporate the new tokens we introduced for the pretraining data preprocessing. See Section 2.2 and Appendix B. 2 for more details.

### 4.2 Supervised Finetuning

To evaluate the downstream performance of our MEDITRON models on common medical reasoning benchmarks, we individually finetune the pretrained model on each benchmark's training set. For example, we finetune the model on the MedMCQA training set and evaluate it on the MedMCQA test set. Since MMLU does not have a training set, we evaluate the model finetuned on MedMCQA

| Dataset | Instruction |
| :--- | :--- |
| MedQA | You are a medical doctor taking the US Medical Licensing Examination. You need to demonstrate <br> your understanding of basic and clinical science, medical knowledge, and mechanisms underlying <br> health, disease, patient care, and modes of therapy. Show your ability to apply the knowledge essential <br> for medical practice. For the following multiple-choice question, select one correct answer from A to <br> E. Base your answer on the current and standard practices referenced in medical guidelines. |
| PubMedQA | As an expert doctor in clinical science and medical knowledge, can you tell me if the following <br> statement is correct? Answer yes, no, or maybe. |
| MedMCQA | You are a medical doctor answering real-world medical entrance exam questions. Based on your <br> understanding of basic and clinical science, medical knowledge, and mechanisms underlying health, <br> disease, patient care, and modes of therapy, answer the following multiple-choice question. Select one <br> correct answer from A to D. Base your answer on the current and standard practices referenced in <br> medical guidelines. |

Table 2: Medical task instructions. The instruction used for each benchmark for in-context learning and finetuning. Because MMLU-Medical does not provide training data, we evaluate MEDITRON models finetuned on MedMCQA on MMLU-Medical. Thus, the instruction for MMLU-Medical is identical to the one used for MedMCQA.

for out-of-distribution inference. For instruction finetuning, we manually write expressive and clear instructions for each training set. We list these instructions in Table 2.

Implementation We follow OpenAI's ChatML format (OpenAI, 2023a) to format the instruction data. ChatML documents consist of a series of messages, starting with a special token <|im_start|>, followed by the role of messenger (i.e., the "user" or the "assistant"), a new line, and then the message itself. The message is then suffixed with a second special token: <| im_end | >. We adopt ChatML's format for constructing the input prompt for the model. During training, we only compute the loss with respect to the response tokens (including $<\mid$ im_start $\mid>$ and $<\mid$ im_end $\mid>)$.

When preprocessing the input data, we keep each document separate and insert pad tokens $<\mathrm{PAD}>$ at the end of each text and mask out the loss on padding tokens. An example prompt format for task-specific-finetuning on MedQA is as follows:

$<$ im_start $\mid>$ system

You are a medical doctor answering real-world medical entrance exam questions. Based on your understanding of basic and clinical science, medical knowledge, and mechanisms underlying health, disease, patient care, and modes of therapy, answer the following multiplechoice question. Select one correct answer from A to D. Base your answer on the current and standard practices referenced in medical guidelines. $<\mid$ im_end $\mid>$

$<\mid$ im_start $\mid>$ question

Question: Which of the following ultrasound findings has the highest association with aneuploidy?

Options:

(A) Choroid plexus cyst

(B) Nuchal translucency

(C) Cystic hygroma

(D) Single umbilical artery $<\mid$ im_end $\mid>$

$<$ |im_start $\mid>$ answer

A finetuned MediTron model needs to predict (C) Cystic hygroma as the answer for this prompt.

Hyperparameters The finetuning process uses the AdamW optimizer, with $\beta_{1}=0.9, \beta_{2}=0.95$, and eps $=1 \times 10^{-5}$. We use a cosine learning rate schedule with a $10 \%$ warmup ratio and decay the final learning rate down to $10 \%$ of the peak learning rate. Following Llama2-chat (Touvron et al., 2023b), we use a learning rate of $2 \times 10^{-5}$, a weight decay of 0.1 , and a batch size of 64 . We finetune the model for 3 epochs for all the finetuning runs.

| Dataset | \# Train Samples | \# Test Samples | Format | \# Choices |
| :--- | ---: | ---: | :--- | ---: |
| MedQA | 10,178 | 1,273 | Question + Answer | 5 |
| MedQA-4-option | $0^{\dagger}$ | 1,273 | Question + Answer | 4 |
| PubMedQA | 200,000 | 500 | Abstract + Question + Answer | 3 |
| MedMCQA | 159,669 | 4,183 | Question + Answer | 4 |
| MMLU-Medical | 0 | 1,862 | Question + Answer | 4 |

Table 3: Medical benchmark datasets. In this table, we summarize the major details of each benchmark we use to evaluate MediTron. We report the number of train and test questions, the format of the questions, and the number of choices for each benchmark. Note that all benchmarks are multiple-choice question-answering tasks. ${ }^{\dagger}$ For MedQA-4-option, we train on the 5-option variant and evaluate on the 4 -option setting.

### 4.3 Inference

We apply several different inference methods to elicit answers from the resulting model from continued pretraining or instruction tuning.

Top Token Selection (Top-Token): For tasks with a single-label answer, such as Multiple-choice or Boolean QA, we follow the HELM implementation (Liang et al., 2023) of the Open LLM benchmark (Beeching et al., 2023). In particular, we rely on a text generation engine to generate the next token output and gather the probability from the model for each word in the vocabulary. We select the token with the maximum log probability as the model's generated answer and then compare the model answer to the text of the expected answer to determine the accuracy. For models finetuned on the downstream task, we pass the question directly to the model as input. For the pretrained model, we perform in-context learning (Xie et al., 2022) and provide the model with few-shot demonstrations as part of the input. For both in-context learning and direct generation from a finetuned model, we append the instruction of each benchmark in front of the question for answer generation.

Chain-of-Thought (CoT): CoT, introduced by Wei et al. (2023), enables an LLM to condition its generation on its intermediate reasoning steps when answering multi-step problems, thereby augmenting the LLM's reasoning ability on complex problems such as math word problems. We apply zero-shot CoT prompting to the models finetuned on medical data since we only finetune on zero-shot CoT training samples. In the case of zero-shot CoT, we add the phrase "Let's think step-by-step" at the end of the question following Kojima et al. (2023).

Self-consistency CoT (SC-CoT): Wang et al. (2023) found that sampling multiple reasoning traces and answers from the model and selecting the final answer through majority voting can significantly improve large language model performance on multiple-choice question-answering benchmarks. We apply SC-CoT prompting using a decoding temperature of 0.8 , sample 5 generations, extract the answer options from each generation, and use majority voting to select the final prediction.

## 5 Medical Benchmarks

Following previous works on developing medical LLMs and evaluation methods (Wu et al., 2023a; Singhal et al., 2023a,b), we selected four commonly used medical benchmarks, which are MedQA, MedMCQA, PubMedQA, and MMLU-Medical.

MedQA: The MedQA (Jin et al., 2020) dataset consists of questions in the style of the US Medical License Exam (USMLE). MedQA is a challenging benchmark due to its combination of different medical knowledge (patient profile, disease symptoms, drug dosage requirements, etc.) that needs to be contextualized for the questions to be answered correctly. The training set consists of 10178 samples, and the test set has 1273 questions. MedQA was compiled with a choice of four (MedQA4-option) or five possible answers, so we finetuned the models on the original 5-option dataset and tested it on both the 5 and 4 -option questions (MedQA-4-option) to have comparable results with existing evaluations of medical LLMs. This dataset does not include any long explanatory answers, so to finetune a model for chain-of-thought reasoning, we used a training set of questions in the distribution of MedQA that provides human-written explanations.

| Model | Accuracy $(\uparrow)$ |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | MMLU-Medical | PubMedQA | MedMCQA | MedQA | MedQA-4-Option | Avg |
| MPT-7B | $23.5_{ \pm 0.93}$ | $43.9_{ \pm 21.9}$ | $32.1_{ \pm 0.91}$ | $22.5_{ \pm 0.59}$ | $27.6_{ \pm 1.57}$ | 29.9 |
| Falcon-7B | $26.1_{ \pm 0.51}$ | $52.8_{ \pm 44.2}$ | $27.3_{ \pm 1.53}$ | $19.6_{ \pm 1.86}$ | $25.3_{ \pm 1.63}$ | 30.2 |
| Llama-2-7B | $41.4_{ \pm 0.24}$ | $49.1_{ \pm 51.1}$ | $37.9_{ \pm 1.16}$ | $29.1 \pm 0.90$ | $35.4_{ \pm 4.27}$ | 38.6 |
| PMC-Llama-7B | $26.2_{ \pm 1.27}$ | $57.0_{ \pm 20.6}$ | $27.4_{ \pm 5.91}$ | $21.6_{ \pm 0.32}$ | $27.8_{ \pm 0.86}$ | 32.0 |
| MEDITRON-7B | $42.3_{ \pm 2.37}$ | $69.3_{ \pm 15.1}$ | $36.3_{ \pm 1.38}$ | $28.7_{ \pm 0.81}$ | $37.4_{ \pm 3.27}$ | 42.8 |
| Llama-2-70B | $71.3+0.87$ | $72.8+7.34$ | $52.4_{ \pm 0.21}$ | $49.0_{ \pm 0.85}$ | $58.4_{ \pm 0.95}$ | 60.8 |
| MediTRon-70B | $71.5_{ \pm 0.67}$ | $79.8_{ \pm 0.46}$ | $53.3_{ \pm 0.51}$ | $52.0_{ \pm 1.21}$ | $59.8 \pm 0.24$ | 63.3 |

Table 4: Few-shot Learning results of raw MEDiTron models against open-source pretrained baselines. This table shows the main few-shot learning results of MEDITRON on downstream medical tasks against other open-source pretrained models. Our models (MediTron-7B and MediTron70B) are continue-pretrained raw models with no additional supervised finetuning on task-specific training sets. For the 7B models, we apply 3 -shot in-context learning with 3 demonstrations randomly sampled from each benchmark's training set because the maximum context window size is limited to 2048 tokens. For the 70B models, we use 5-shot in-context learning. We report the average accuracy across three random seeds used for sampling random demonstrations.

MedMCQA: The MedMCQA (Pal et al., 2022) dataset consists of more than 194k 4-option multiple-choice questions from the Indian medical entrance examinations (AIIMS/NEET). This dataset covers $2.4 \mathrm{k}$ healthcare topics and 21 medical subjects. The training set contains $187 \mathrm{k}$ samples, and the validation set has 4183 questions. Because the test set of MedMCQA does not provide the answer keys to the general public, we follow Wu et al. (2023a) and use the validation set to report evaluations. For hyperparameter tuning, we randomly split the training set into new train/validation splits. For both single-answer and chain-of-thought training data, we also remove all the samples with "None" as the explanation, resulting in 159,669 training samples.

PubMedQA: The PubMedQA (Jin et al., 2019) dataset consists of 200k artificially created multiplechoice QA samples and 1k samples QA labeled by experts. Given a PubMed abstract as context and a question, the model needs to predict a yes, no, or maybe answer. We follow the reasoning-required evaluation setting where the model is given a question together with a PubMed abstract as context. Out of the $1 \mathrm{k}$ expert-labeled samples, we use the 500 test samples for evaluation following Singhal et al. (2023a)'s setting. Because the size of the other 500 training samples is relatively small, we use the 200k artificially labeled examples as the training data to finetune our models.

MMLU-Medical: The MMLU dataset (Hendrycks et al., 2021b) includes exam questions from 57 subjects (e.g., STEM, social sciences, etc.). Each MMLU subject contains four-option multiplechoice questions and their respective answer. We selected the nine subjects that are most relevant to medical and clinical knowledge: high school biology, college biology, college medicine, professional medicine, medical genetics, virology, clinical knowledge, nutrition, and anatomy, and we concatenate them into one medical-related benchmark: MMLU-Medical. The total number of questions in MMLU-Medical is 1862. Note that MMLU does not provide any training data. Therefore, we used MedMCQA's training data (four-answer options, the same as MMLU-Medical) to finetune our models and evaluate the generalization performance from MedMCQA to MMLU-Medical.

## 6 Main Results

### 6.1 Pretrained Model Evaluation

Setup: For the benchmarks that provide publicly available training sets, i.e., PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022), and MedQA (Jin et al., 2020), we randomly sample few-shot demonstrations from the training data using three different random seeds (3-shot for 7B models and 5 -shot for 70B models). We report the average accuracy across three random seeds. As baselines, we compare the raw MEDITRon models to other pretrained models. Our first baselines are the Llama-2 models (7B and 70B) without any continued pretraining, as it allows us to control for the effect of our continued pretraining. For MEDITRon-7B, we additionally run comparisons with PMC-Llama-7B

| Model | Accuracy $(\uparrow)$ |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | MMLU-Medical | PubMedQA | MedMCQA | MedQA | MedQA-4-Option | $\operatorname{Avg}$ |
| Top Token Selection |  |  |  |  |  |  |
| Mistral-7B* | 55.8 | 17.8 | 40.2 | 32.4 | 41.1 | 37.5 |
| Zephyr-7B- $\beta^{*}$ | 63.3 | 46.0 | 43.0 | 42.8 | 48.5 | 48.7 |
| PMC-Llama-7B | 59.7 | 59.2 | 57.6 | 42.4 | 49.2 | 53.6 |
| Llama-2-7B | 56.3 | 61.8 | 54.4 | 44.0 | 49.6 | 53.2 |
| MEDITRON-7B | 55.6 | 74.4 | 59.2 | 47.9 | 52.0 | $\underline{57.5}$ |
| Clinical-Camel-70B* | 65.7 | 67.0 | $46 . \overline{7}$ | 50.8 | $\overline{56.8}$ | 57.4 |
| Med42-70B* | 74.5 | 61.2 | 59.2 | 59.1 | 63.9 | 63.6 |
| Llama-2-70B | 74.7 | 78.0 | 62.7 | 59.2 | 61.3 | 67.2 |
| MEDITRON-70B | 73.6 | 80.0 | 65.1 | 60.7 | 65.4 | 69.0 |
| Chain-of-thought |  |  |  |  |  |  |
| Llama-2-70B | 76.7 | 79.8 | 62.1 | 60.8 | 63.9 | 68.7 |
| MEDITRON-70B | 74.9 | 81.0 | 63.2 | 61.5 | 67.8 | $69.7 \quad$ |
| Self-consistency Chain-of-thought |  |  |  |  |  |  |
| Llama-2-70B | 77.9 | 80.0 | 62.6 | 61.5 | 63.8 | 69.2 |
| MEDITRON-70B | 77.6 | 81.6  | 66.0 | 64.4 | 70.2 | 72.0 |

Table 5: Main results of MediTron against open-source baselines. This table shows the main results of MEDITRON's downstream medical task performance against other best-performing opensource medical models measured by accuracy. Our models (MEdiTron-7B and MEDiTron-70B), the Llama-2 models (7B and 70B), and PMC-Llama-7B are individually finetuned on PubMedQA, MedMCQA, and MedQA training sets. The baselines with *, i.e., Mistral-7B (instruct version), Zephyr-7B- $\beta$, Med42-70B, and Clinical-Camel-70B are instruction-tuned, so we do not perform further finetuning on the training sets and use the out-of-box model for inference. The inference modes consist of (1) top-token selection based on probability, (2) zero-shot chain-of-thought prompting, and (3) self-consistency chain-of-thought prompting ( 5 branches with 0.8 temperature). According to Tian et al. (2023), the passing score for humans on MedQA is 60.0.

(Wu et al., 2023a), a medical LLM adapted from Llama through continued pretraining on PubMed Central papers. We also select general-purpose pretrained models that perform well in open-source reasoning benchmarks as baselines, including Falcon-7B (Almazrouei et al., 2023) and MPT-7B (MosaicML NLP Team, 2023).

Results: In Table 4, we observe that at the 7B-scale, MediTron-7B with in-context learning outperforms other pretrained baselines. A potential reason for the improved performance is that MEDITRon-7B uses Llama-2 as a backbone model, which already achieves much higher average performance than other pretrained baselines. However, we show that continued pretraining on medical data brings additional benefits and further improves Llama-2's performance on the medical benchmarks. In particular, MEdITRON-7B shows much higher performance on PubMedQA than the base model ( $20 \%$ increase). At the 70B scale, the base model Llama-2-70B and MEDITRON-70B's performances increase significantly compared to the 7B models, with MEDITRON-70B outperforming the base model on all benchmarks. At the 7B scale, we observe that MediTRon-7B does not perform as well as the base model on the most difficult benchmark, MedQA (though the difference is within the margin of error). However, At the 70B scale, we see that MEDiTron-70B outperforms the base Llama-2 by 3\%. Overall, we show that MEDITRON models, particularly At the 70B scale, already demonstrate decent reasoning ability on medical tasks even before finetuning for a particular task. More specifically, for PubMedQA, the in-context learning performance ( $79.8 \%$ ) is only $0.2 \%$ behind the model finetuned on non-chain-of-thought PubMedQA training data $(80.0 \%)$.

### 6.2 Finetuned Model Evaluation

Setup: For the benchmarks that provide publicly available training sets, we conduct supervised finetuning individually on each training set and evaluate on the corresponding test sets. Both PubMedQA and MedMCQA provide reasoning traces (long answers or explanations) for chain-ofthought. For MedQA, which does not provide reasoning traces, we use a separate training set that

![](https://cdn.mathpix.com/cropped/2024_06_04_ade956008c157567bd85g-11.jpg?height=545&width=1065&top_left_y=253&top_left_x=519)

Figure 3: Main results of MediTron against commercial LLMs. We compare MEdiTRon-70B's performance on four medical benchmarks (PubMedQA, MedMCQA, MedQA, MedQA-4-option) against commercial LLMs that have much larger parameter counts. We focus on GPT-3.5 (175B), GPT-4, Med-PaLM (540B), and Med-PaLM-2 (540B). The results of these commercial LLMs are directly taken from the associated papers (Nori et al., 2023; Singhal et al., 2023a,b). Note that MedPaLM does not report its performance on MedQA, and MedPaLM-2 does not report its performance on MedQA-4-option.

provides a human-written explanation for each question. ${ }^{6}$ We train with the format where the answer is concatenated to the explanation. For MMLU-Medical (Hendrycks et al., 2021b), which does not contain a training set, we test the model trained on MedMCQA instead since both datasets have the four-option answer format (with A, B, C, D). For the MedQA-4-option test set, we directly evaluate the model trained on the MedQA training set with five options.

We evaluate MEDiTron models finetuned on each individual benchmark's training set against Llama-2 (7 and 70B) and PMC-Llama-7B (also finetuned on each benchmark's training sets). We then include 4 instruction-tuned models as public baselines: Mistral-7B-instruct (Jiang et al., 2023) and Zephyr-7B- $\beta$ (Tunstall et al., 2023) for as 7B-scale baselines, and Clinical-Camel-70B (Toma et al., 2023) and Med42-70B (M42-Health) as 70B-scale baseline. Clinical-Camel-70B is a Llama2 70B variant tuned using QLoRA (Dettmers et al., 2023) on multi-turn dialogues transformed from conversations, clinical articles, and medical task data. Med42-70B is instruction-tuned on medical tasks, but the training details are not publicly released. We do not further finetune the public baselines on the task-specific training sets because they are already instruction-tuned. Finally, we compare MEdiTron-70B against commercial LLMs, including GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023b), Med-PaLM (Singhal et al., 2023a), and Med-PaLM-2 (Singhal et al., 2023b). These LLMs are pretrained or tuned on large-scale, high-quality, proprietary corpora and instruction data. They are also significantly larger than MEdiTRon (i.e., 175B, 540B). Note that only MediTRon, Llama-2, and PMC-Llama-7B models are finetuned on the training sets. Because Med42 (M42-Health) and Clinical-Camel (Toma et al., 2023) have already been tuned on these datasets as part of their initial instruction-tuning, we exclude them from further supervised finetuning.

Results: We report the performance of MEDITRon and related baselines in both the 7B and 70B parameter scales. Table 5 shows all the performance measured in terms of accuracy ( $\uparrow$ ). At the 7B scale, we first compare with Llama-2-7B and PMC-Llama-7B, which are finetuned in the same manner as MediTron-7B. The results show that MediTron-7B outperforms these two baselines by an average of $4 \%$. Compared to the state-of-the-art instruction-tuned models Mistral (Jiang et al., 2023) and Zephyr- $\beta$ (Tunstall et al., 2023), MEDITRON achieves significant performance gains on all benchmarks except MMLU-Medical, particularly on PubMedQA, with more than a $10 \%$ increase. Overall, MediTron-7B achieves the best PubMedQA performance with 74.4\% accuracy, the best MedMCQA performance with $59.2 \%$ accuracy, and the best performance on both MedQA and MedQA-4-option with $47.9 \%$ and $52.0 \%$ accuracy, respectively. At 70B scale, we compare with[^3]![](https://cdn.mathpix.com/cropped/2024_06_04_ade956008c157567bd85g-12.jpg?height=560&width=1370&top_left_y=240&top_left_x=366)

Figure 4: Training and validation loss during continued pretraining of the MEDITRON-70B model. We report the training and validation loss of the 70B MEDITRON model across the number of processed tokens during the pretraining run.

Llama-2-70B (finetuned exactly like MediTron-70B) and two other medical LLMs, both of which are instruction-tuned for medical tasks from Llama-2-70B. On average, MEDITron-70B improves over all three baseline models with an $11.6 \%$ gain over Clinical-Camel-70B, a $5.4 \%$ performance gain over Med42-70B, and a 1.8\% performance gain over the finetuned Llama-2-70B.

Next, we apply chain-of-thought (CoT) and self-consistency chain-of-thought (SC-CoT) to investigate if they can further improve our model's performance. CoT improves MediTron-70B's average performance by $0.7 \%$, and SC-CoT improves the performance by $3 \%$. Although the finetuned Llama2-70B's performance also improves through CoT and SC-CoT, MEdITRON-70B maintains and extends its advantage by outperforming Llama-2 (by $1.9 \%$ with CoT and $2.8 \%$ with SC-CoT). Overall, with SC-CoT, MEdiTRON-70B achieves the highest accuracy on average (72.0\%) and on all the benchmarks except MMLU-Medical ( $81.6 \%$ with PubMedQA, $66.0 \%$ with MedMCQA, $64.4 \%$ with MedQA, and $70.2 \%$ with MedQA-4-option). Interestingly, MediTron-70B with the three inference modes all surpass the human passing score, 60.0, for MedQA (Tian et al., 2023).

MediTron vs. Commercial LLMs: We also compare MEdiTron's performance to commercial LLMs. These models often have a massive parameter count ( $>100 \mathrm{~B}$ ). We focus on four popular LLMs: GPT-3.5 (i.e., text-davinci-003, (Ouyang et al., 2022)), GPT-4 (OpenAI, 2023b; Nori et al., 2023), MedPaLM-540B (Singhal et al., 2023a), and MedPaLM-2-540B (Singhal et al., 2023b). In Figure 3, we show that MEdiTRON-70B outperforms the GPT-3.5 model on all benchmarks despite the latter having 175B parameters. On PubMedQA, MediTron-70B outperforms MedPaLM and GPT-4, and its performance is only $0.2 \%$ behind the state-of-the-art model, Med-PaLM2. On MedMCQA and MedQA (5-option and 4-option), MediTRon-70B's performance falls between Med-PaLM and the SOTA performance (GPT-4 and Med-PaLM-2). ${ }^{7}$ Overall, we show that MEDITRON-70B's performance on medical reasoning tasks is competitive with commercial LLMs with significantly larger parameter sizes.

## 7 Analysis

### 7.1 Impact of Continued Pretraining

During the continued pretraining process, we closely monitor the learning quality of the model. We report the language modeling losses of training and validation in Figure 4, indicating that both losses decrease as the model consumes more tokens and the model learns effectively without overfitting. To monitor MEDITRon's downstream performance during the pretraining process, we also conduct intermediate evaluations on the $5 \mathrm{k}, 10 \mathrm{k}$, and $15 \mathrm{k}$ iteration checkpoints. We evaluated each medical benchmark in a 5 -shot in-context learning setting. We provided five demonstrations randomly sampled[^4]

|  |  | Accuracy $(\uparrow)$ |  |  |  |  |  |
| ---: | ---: | ---: | :---: | :---: | :---: | :---: | :---: |
| Iteration | \# Tokens | MMLU-Medical | PubMedQA | MedMCQA | MedQA | MedQA-4-Option | Avg |
| 0 (Llama-2) | $0 \mathrm{~B}$ | $71.3_{ \pm 0.87}$ | $72.8_{ \pm 7.34}$ | $52.4_{ \pm 0.21}$ | $49.0_{ \pm 0.85}$ | $58.4_{ \pm 0.95}$ | 60.8 |
| 5,000 | $10 \mathrm{~B}$ | $70.2_{ \pm 1.13}$ | $79.2_{ \pm 3.81}$ | $51.0_{ \pm 0.48}$ | $48.4_{ \pm 0.86}$ | $57.3_{ \pm 1.21}$ | 61.2 |
| 10,000 | $21 \mathrm{~B}$ | $70.0_{ \pm 0.85}$ | $77.8_{ \pm 4.96}$ | $52.3_{ \pm 0.91}$ | $49.8_{ \pm 0.71}$ | $57.0_{ \pm 1.06}$ | 61.4 |
| 15,000 | $31 \mathrm{~B}$ | $70.8_{ \pm 0.42}$ | $78.9_{ \pm 5.02}$ | $51.3_{ \pm 0.95}$ | $48.9_{ \pm 0.79}$ | $57.7_{ \pm 0.79}$ | 61.5 |
| 23,000 | $48 \mathrm{~B}$ | $\mathbf{7 1 . 5}_{ \pm 0.67}$ | $\mathbf{7 9 . 8}_{ \pm 0.46}$ | $\mathbf{5 3 . 3}_{ \pm 0.51}$ | $\mathbf{5 2 . 0}_{ \pm 1.21}$ | $\mathbf{5 9 . 8}_{ \pm 0.24}$ | $\mathbf{6 3 . 3}$ |

Table 6: In-context learning performance of intermediate MediTron-70B checkpoints. We monitor the pretraining process through intermediate evaluations of the downstream tasks using in-context learning. Without any finetuning, we provide the model five demonstrations sampled from the training data as a part of the prompt and generate the model's answer. The average performance increases consistently as the iteration number increases, though this varies across benchmarks. We report the average accuracy across three random seeds used for sampling random demonstrations.

| Name | \# Tokens | Description |
| :--- | ---: | :--- |
| PMC (2.2) | 39.2B | Only publicly accessible PubMed papers directly from the PubMed <br> Central portion of the S2ORC collection. |
| PMC + Replay (2.3) | 37.5B | Combines PMC with 400 million tokens sampled from the 1 trillion <br> RedPajama <br> PMC training corpus for experience replay in the general domain. |
| PMC Upsampled (B.4) | 41.4B | Filters out the animal studies, preprints, and retracted documents in PMC, <br> and weigh each paper according to a set of predefined quality criteria <br> such as publication type, recency, and number of citations. Higher- <br> quality and practice-ready papers are upsampled to appear more fre- <br> quently in the pretraining corpus. |
| PMC + Replay + Code | 39.5B | Mix PMC + Replay with 10B or 2B tokens of code data from the Star- <br> Coder training corpus. We create this mixture to study the impact of <br> including code data in the pretraining corpus on the model's downstream <br> reasoning performance. |
| GAP + Replay (2.1) | 46.8B | GAP contains PMC, PubMed abstracts, and medical guidelines and is <br> mixed with the 400 million replay tokens from RedPajama. This is the <br> data mixture chosen for MEDITRon's continued pretraining. |

Table 7: Different data mixtures for continued pretraining trial runs. In this table, we summarize the details of five different data mixtures we use for continued pretraining trial runs.

from each benchmark's training data with associated instructions from Table 2. We used top-token generation as the inference method used to get the model's prediction for each multiple-choice question-answer pair. Table 6 reports the in-context learning performance for these intermediate checkpoints. We observe that the intermediate performance fluctuates between different checkpoints. However, the average performance grows consistently across iterations, and the final checkpoint achieves the best performance. We note that on certain individual datasets, the model's performance drops in the intermediate checkpoints relative to the seed Llama-2 model, demonstrating the benefit of large-scale continual pretraining.

### 7.2 Data Mixture Ablation

Multiple prior works show that the content of pretraining data can significantly impact the pretraining and downstream performance of the model (Xie et al., 2023; Du et al., 2022; Penedo et al., 2023; Longpre et al., 2023). Thus, in this ablation study, we analyze the impact of different distributions of the training corpus on the model's downstream medical reasoning ability. Based on prior assumptions, we conduct continued pretraining of the Llama2-7B model on several data mixtures. The list of data mixtures and their details are shown in Table 7. We assess the downstream performance of the trial models by evaluating the finetuned models on the training sets of PubMedQA, MedMCQA, and MedQA. The setup for the supervised finetuning is the same as that described in Section 6.2. The results are displayed in Table 8, and all reported metrics are measured in terms of accuracy ( $\uparrow$ ). We now discuss the findings from the trial-run experiments.

Replay tokens are beneficial for downstream performance. Experience replay with tokens from the general domain improves the model's performance on all benchmarks except MedMCQA. On average, PMC + Replay increases the performance by $1.6 \%$ compared to PMC results. We

|  | Accuracy ( $\uparrow$ ) |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Mixture | MMLU-Medical | PubMedQA | MedMCQA | MedQA | Avg |
| PMC-Llama-7B | 56.4 | 59.2 | 57.6 | 42.4 | 53.9 |
| Llama-2-7B | 53.7 | 61.8 | 54.4 | 44.0 | 53.5 |
| PMC | 55.6 | 62.8 | 54.5 | 45.4 | 54.6 |
| PMC + Replay | $\mathbf{5 6 . 4}$ | 63.2 | 58.1 | 46.9 | 56.2 |
| PMC Upsampled | 55.2 | 61.6 | 57.2 | 44.9 | 54.7 |
| PMC + Replay + Code (10B) | 55.8 | 58.0 | 47.2 | 35.1 | 49.0 |
| PMC + Replay + Code (2B) | 54.1 | 64.2 | 58.0 | 45.8 | 55.5 |
| GAP + Replay | 54.2 | $\mathbf{7 4 . 4}$ | $\mathbf{5 9 . 2}$ | $\mathbf{4 7 . 9}$ | $\mathbf{5 8 . 9}$ |

Table 8: Performance comparison of different trial-runs on 7B models. We analyze which pretraining data mixture yields the best performance on downstream medical benchmarks. For each data mixture, we first do continued pretraining from the base Llama-2-7B model. Next, we finetune the pretrained model on individual medical tasks' training sets and evaluate using their corresponding test sets. Note that for MMLU-Medical, we use the model finetuned on MedMCQA since both have 4 options. For inference, we select the token with the maximum log probability.

conclude that adding replay data to the training corpus for continued pretraining benefits the model's downstream performance. Based on this observation, we add the same $400 \mathrm{M}$ replay tokens to the final training data mixture (GAP + Replay) for our pretraining runs.

Upsampling the medical papers leads to weaker downstream performance. Comparing the upsampled version of PMC to the full PMC corpus, the model's performance on MedMCQA increases, but the performance on MedQA decreases, making this mixture weaker than PMC + Replay. Although showing a weaker performance, there may be other potential benefits of an upsampled version of PMC, such as allowing the model to generate content that is more clinic-ready or reducing the model's tendency to generate content that is not tested on human subjects. However, in the scope of this preliminary analysis of data mixture, we omit additional evaluations since they would require expert-level opinions that are hard to collect.

Adding code does not improve the performance. There has been some speculation that training on code could improve the model's ability to perform reasoning tasks (Chen et al., 2021). However, at this model scale, we find that adding code decreases the overall performance on medical benchmarks, with the PMC-Replay mixture slightly outperforming the 2B-Code addition ( $+0.6 \%$ ) and greatly outperforming the 10B-Code addition by $5.7 \%$. Thus, in this setting, where no explicit reasoning (e.g., mathematical reasoning) is required from the model, we decide against using code in the final pre-training mixture.

GAP mixture is better than PubMed only. The GAP mixture adds PubMed abstracts and medical guidelines to the PMC corpus. Here, we compare GAP + Replay with PMC + Replay, the latter outperforming the former by $2.8 \%$ on average. This mixture leads to the best average performance and is chosen for MEDITRON's continued pretraining.

## 8 Related Work

Medical Large Language Models. Developing large language models in the medical domain and supporting biomedical and clinical tasks has been an ongoing effort. Early works on adapting pretrained language models to the medical domain focused on pretraining encoder-only models (e.g., BERT) with large-scale biomedical corpora such as the PubMed Central articles and PubMed abstracts (Gu et al., 2021; Lee et al., 2020). Further approaches used links between documents (Yasunaga et al., 2022b) and knowledge graphs (Yasunaga et al., 2022a) to improve model performance. As large autoregressive generative models became more popular and delivered improved performances, decoder-only architectures such as GPT (Radford and Narasimhan, 2018) and Llama (Touvron et al., 2023a) were used to pretrain medical LLMs on medical domain text data (Stanford CRFM; Wu et al., 2023a). With the recent trend of scaling up pretraining data size and model parameter size, multiple studies explored the benefit of scaling up on medical tasks. GatorTronGPT (Peng et al., 2023) is a GPT-3-like (Brown et al., 2020) model with 20B parameters pretrained on 227B words of mixed clinical and English text. Clinical-Camel (Toma et al., 2023) adapted from the Llama-2-70B (Touvron
et al., 2023b) model using QLoRA (Dettmers et al., 2023) training on medical data. Singhal et al. (2023a) and Singhal et al. (2023b) study the medical reasoning ability of Flan-PaLM and PaLM-2, both with 540B parameter sizes. PaLM-2 achieves state-of-the-art performance on the major medical benchmarks. Our work scales up full-parameter medical domain pretraining to 70B parameters. Our evaluations show that our model outperforms previous pretrained language models and is competitive with Flan-PaLM and PaLM-2.

Continued Pretraining. Early studies on pretrained language models show that continued pretraining in a specific domain is beneficial for downstream task performance (Hoang et al., 2019; Alsentzer et al., 2019; Chakrabarty et al., 2019; Lee et al., 2020; Gu et al., 2021). Several studies found that continued pretraining of a language model on the unlabeled data of a given task improves the models' end-task performance (Howard and Ruder, 2018; Phang et al., 2019; Sun et al., 2020a). Gururangan et al. (2020) performed a comprehensive study exploring the benefit of continued pretraining on multiple domains for the BERT (Devlin et al., 2019) class of models and showed that a second phase of in-domain pretraining and adapting to the task's unlabeled data improved the performance on downstream domain-specific tasks. Additional benefits of continued pretraining also include improved zero-shot and few-shot promptability (Wu et al., 2022). In the medical domain, the most similar work to ours is PMC-Llama (Wu et al., 2023a), which adapts the Llama model through continued pretraining on PubMed Central papers and medical textbooks. In contrast to prior works, MEDITRON studies the benefit of continued pretraining at the 70B scale and shows that expanding the domain-specific pretraining data brings significant performance gain on downstream tasks.

## 9 Conclusion

We release MEDITRon, a suite of domain-adapted medical LLMs that demonstrate high-level medical reasoning and improved domain-specific benchmark performance. Through continued pretraining on carefully curated high-quality medical resources, including a novel set of clinical guidelines, MEDITRON shows improved performance over all the state-of-the-art baselines at matched scale on clinical reasoning benchmarks, coming within $10 \%$ performance of state-of-the-art commercial LLMs that are $8 \times$ larger. Importantly, MEDITRON outperforms all open-source generalist and medical LLMs on all medical benchmarks. We make our models (at both 7B and 70B scale), tools required for curating the training corpus, and our distributed training library available as an open resource. This not only ensures access to real-world evaluation but also enables further fine-tuning and the development of instruction-based models, among other efforts. By providing these resources openly, we aim to help unlock the transformative potential of openly shared models in enhancing medical research, improving patient care, and fostering innovation across various health-related fields.

Safety Advisory. While MediTron is designed to encode medical knowledge from sources of high-quality evidence, it is not yet adapted to deliver this knowledge appropriately, safely, or within professional actionable constraints. We recommend against deploying MediTron in medical applications without extensive use-case alignment, as well as additional testing, specifically including randomized controlled trials in real-world practice settings. While we do not view MEdiTron as being ready for real-world use in its current form, we release MEDITRON to the research community to promote work on the safety of language models in medical applications. Our work represents the largest open-source model adapted for the medical domain, trained on a large and diverse medical pretraining corpus. We hope these resources will enable the research community to more comprehensively study large language models for the medical domain.

## Acknowledgements

We are extremely grateful to the EPFL Research Computing Platform Cluster team and the EPFL School of Computer and Communication Sciences for providing the computing resources for this project. We are especially grateful to Khadidja Malleck, Ed Bugnion, Jim Larus, Anna Fontcuberta i Morral, and Rüdiger Urbanke for their support in organizing the resources for this project. We also thank the IT team, Yoann Moulin and Emmanuel Jaep, for their technical support on the cluster, and Marcel Salathé, Jacques Fellay, and François Fleuret for providing feedback on earlier versions of this draft. We also thank Katie Link and Lewis Tunstall from HuggingFace for their support.

The availability of open-access clinical practice guidelines (CPG) was critical to this work, and we thank all the societies listed in Table 9. A broader representation of geography, medical specialties, and contexts (especially low-resource settings) could be achieved through more standardized CPG formatting practices to ensure reliable textual extraction (e.g., releasing .txt or .html versions with structured content). We encourage the CPG community to continue to make these documents available (open-access with permissive licenses for incorporation into large language models) and easily usable.

Kyle Matoba is supported by SNSF grant number FNS-188758 "CORTI". Amirkeivan Mohtashami is supported by SNSF grant number 200020_200342. Alexandre Sallinen is supported by the Science and Technology for Humanitarian Action Challenges (HAC) program from the Engineering for Humanitarian Action (EHA) initiative, a partnership between the ICRC, EPFL, and ETH Zurich. EHA initiatives are managed jointly by the ICRC, EPFL EssentialTech Centre, and ETH Zurich's ETH4D. Antoine Bosselut gratefully acknowledges the support of the Swiss National Science Foundation (No. 215390), Innosuisse (PFFS-21-29), the EPFL Science Seed Fund, the EPFL Center for Imaging, Sony Group Corporation, and the Allen Institute for AI.

## References

[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv e-prints, page arXiv:2305.13245.

[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.

[3] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72-78, Minneapolis, Minnesota, USA. Association for Computational Linguistics.

[4] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics.

[5] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.

[6] Alfred O. Berg, David Atkins, and William Tierney. 1997. Clinical practice guidelines in practice and education. Journal of General Internal Medicine, 12(S2).

[7] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258.

[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.

[9] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.

[10] Patricia B. Burns, Rod J. Rohrich, and Kevin C. Chung. 2011. The levels of evidence and their role in evidence-based medicine. Plastic and Reconstructive Surgery, 128(1):305-310.

[11] Alejandro Hernández Cano, Matteo Pagliardini, Andreas Köpf, Kyle Matoba, Amirkeivan Mohtashami, Olivia Simin Fan, Axel Marmet, Deniz Bayazit, Igor Krawczuk, Zeming Chen, Francesco Salvi, Antoine Bosselut, and Martin Jaggi. 2023. epfLLM Megatron-LLM. https://github . com/epfLLM/Megatron-LLM.

[12] Tuhin Chakrabarty, Christopher Hidey, and Kathy McKeown. 2019. IMHO fine-tuning improves claim detection. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 558-563, Minneapolis, Minnesota. Association for Computational Linguistics.

[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.

[14] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. Arxiv.

[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

[16] Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning.

[17] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.

[18] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms.

[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics.

[20] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. ACM.

[21] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022. GLaM: Efficient scaling of language models with mixture-of-experts. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5547-5569. PMLR.

[22] Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027.

[23] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3(1):1-23.

[24] Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. 2023. Continual pre-training of large language models: How to (re)warm your model?

[25] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.

[26] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3309-3326, Dublin, Ireland. Association for Computational Linguistics.

[27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding.

[28] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding.

[29] Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, and Yejin Choi. 2019. Efficient adaptation of pretrained transformers for abstractive summarization. ArXiv, abs/1906.00138.

[30] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.

[31] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics.

[32] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.

[33] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

[34] Hongpeng Jin, Wenqi Wei, Xuyu Wang, Wenbin Zhang, and Yanzhao Wu. 2023. Rethinking learning rate tuning in the era of large language models. arXiv preprint arXiv:2309.08859.

[35] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567-2577, Hong Kong, China. Association for Computational Linguistics.

[36] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models.

[37] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners.

[38] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Reducing Activation Recomputation in Large Transformer Models. Arxiv.

[39] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240.

[40] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you!

[41] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic evaluation of language models.

[42] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.

[43] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.

[44] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. 2023. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, and toxicity.

[45] M42-Health. Med42 - clinical large language model. https://huggingface.co/ m42-health/med42-70b. Accessed: 2023-11-05.

[46] Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2023. At which training stage does code data help llms reasoning?

[47] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384-1403, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

[48] Sourab Mangrulkar, Sylvain Gugger, Lewis Tunstall, and Philipp Schmid. 2023. Finetuning Llama $270 b$ using PyTorch FSDP. https://huggingface.co/blog/ ram-efficient-pytorch-fsdp. Accessed 2023-11-02.

[49] Bethesda (MD): National Library of Medicine. 2003-2023. PMC Open Access Subset. https : //www.ncbi.nlm.nih.gov/pmc/tools/openftlist/. Accessed on 12/10/2023.

[50] MosaicML NLP Team. 2023. Introducing mpt-7b: A new standard for open-source, commercially usable llms. Accessed: 2023-05-05.

[51] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021. Efficient large-scale language model training on GPU clusters using Megatron-LM. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '21, New York, NY, USA. Association for Computing Machinery.

[52] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems.

[53] Jesutofunmi A. Omiye, Jenna C. Lester, Simon Spichak, Veronica Rotemberg, and Roxana Daneshjou. 2023. Large language models propagate race-based medicine. npj Digital Medicine, 6(1).

[54] OpenAI. 2023a. Chatml. https://github.com/openai/openai-python/blob/ main / chatml.md. Accessed 2023-11-02.

[55] OpenAI. 2023b. Gpt-4 technical report.

[56] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.

[57] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248-260. PMLR.

[58] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only.

[59] Cheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa, Cheryl Martin, Mona G Flores, Ying Zhang, Tanja Magoc, Gloria Lipori, Duane A Mitchell, Naykky S Ospina, Mustafa M Ahmed, William R Hogan, Elizabeth A Shenkman, Yi Guo, Jiang Bian, and Yonghui Wu. 2023. A study of generative large language model for medical research and healthcare.

[60] Jason Phang, Thibault Févry, and Samuel R. Bowman. 2019. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks.

[61] Alec Radford and Karthik Narasimhan. 2018. Improving language understanding by generative pre-training.

[62] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.

[63] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.

[64] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv e-prints, page arXiv:1909.08053.

[65] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Agüera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2023a. Large language models encode clinical knowledge. Nature, 620(7972):172-180.

[66] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023b. Towards expert-level medical question answering with large language models.

[67] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Khyathi Chandu, Jennifer Dumas, Li Lucy, Xinxi Lyu, Ian Magnusson, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2023. Dolma: An Open Corpus of 3 Trillion Tokens for Language Model Pretraining Research. Technical report, Allen Institute for AI. Released under ImpACT License as Medium Risk artifact, https://github.com/allenai/dolma.

[68] MosaicML Stanford CRFM. Biomedlm. https://huggingface.co/stanford-crfm/ BioMedLM. Accessed: 2023-11-05.

[69] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2020a. How to fine-tune bert for text classification?

[70] Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2020b. Distill and replay for continual language learning. In International Conference on Computational Linguistics.

[71] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science.

[72] Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C. Comeau, Rezarta Islamaj, Aadit Kapoor, Xin Gao, and Zhiyong Lu. 2023. Opportunities and challenges for chatgpt and large language models in biomedicine and health.

[73] Together AI. 2023. Redpajama: An open source recipe to reproduce llama training dataset. https : //github.com/togethercomputer/RedPajama-Data.

[74] Augustin Toma, Patrick R. Lawler, Jimmy Ba, Rahul G. Krishnan, Barry B. Rubin, and Bo Wang. 2023. Clinical camel: An open expert-level medical language model with dialogue-based knowledge encoding.

[75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.

[76] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.

[77] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of $1 \mathrm{~m}$ alignment.

[78] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.

[79] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.

[80] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023a. Pmc-llama: Towards building open-source language models for medicine.

[81] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023b. Bloomberggpt: A large language model for finance.

[82] Yanzhao Wu, Ling Liu, Juhyun Bae, Ka-Ho Chow, Arun Iyengar, Calton Pu, Wenqi Wei, Lei Yu, and Qi Zhang. 2019. Demystifying learning rate policies for high accuracy training of deep neural networks. In 2019 IEEE International conference on big data (Big Data), pages 1971-1980. IEEE.

[83] Zhaofeng Wu, Robert L. Logan IV au2, Pete Walsh, Akshita Bhagia, Dirk Groeneveld, Sameer Singh, and Iz Beltagy. 2022. Continued pretraining for better zero- and few-shot promptability.

[84] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu. 2023. Doremi: Optimizing data mixtures speeds up language model pretraining.

[85] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference.

[86] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang, and Jure Leskovec. 2022a. Deep bidirectional language-knowledge graph pretraining. In Neural Information Processing Systems (NeurIPS).

[87] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. 2022b. Linkbert: Pretraining language models with document links.

[88] Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, and Fangzhou Yao. 2023. Fedjudge: Federated legal large language model.

[89] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.
