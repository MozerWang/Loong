# More Centralized Training, Still DecentralIZED EXECUTION: MULTI-AGENT CONDITIONAL POLICY FACTORIZATION 

Jiangxing Wang<br>School of Computer Science<br>Peking University<br>jiangxiw@stu.pku.edu.cn

Deheng Ye<br>Tencent AI Lab<br>dericye@tencent.com

Zongqing $\mathbf{L u}^{\dagger}$<br>Peking University<br>BAAI<br>zongqing.lu@pku.edu.cn


#### Abstract

In cooperative multi-agent reinforcement learning (MARL), combining value decomposition with actor-critic enables agents to learn stochastic policies, which are more suitable for the partially observable environment. Given the goal of learning local policies that enable decentralized execution, agents are commonly assumed to be independent of each other, even in centralized training. However, such an assumption may prohibit agents from learning the optimal joint policy. To address this problem, we explicitly take the dependency among agents into centralized training. Although this leads to the optimal joint policy, it may not be factorized for decentralized execution. Nevertheless, we theoretically show that from such a joint policy, we can always derive another joint policy that achieves the same optimality but can be factorized for decentralized execution. To this end, we propose multi-agent conditional policy factorization (MACPF), which takes more centralized training but still enables decentralized execution. We empirically verify MACPF in various cooperative MARL tasks and demonstrate that MACPF achieves better performance or faster convergence than baselines. Our code is available at https://github.com/PKU-RL/FOP-DMAC-MACPF.


## 1 INTRODUCTION

The cooperative multi-agent reinforcement learning (MARL) problem has attracted the attention of many researchers as it is a well-abstracted model for many real-world problems, such as traffic signal control (Wang et al., 2021a) and autonomous warehouse (Zhou et al., 2021). In a cooperative MARL problem, we aim to train a group of agents that can cooperate to achieve a common goal. Such a common goal is often defined by a global reward function that is shared among all agents. If centralized control is allowed, such a problem can be viewed as a single-agent reinforcement learning problem with an enormous action space. Based on this intuition, Kraemer \& Banerjee (2016) proposed the centralized training with decentralized execution (CTDE) framework to overcome the non-stationarity of MARL. In the CTDE framework, a centralized value function is learned to guide the update of each agent's local policy, which enables decentralized execution.

With a centralized value function, there are different ways to guide the learning of the local policy of each agent. One line of research, called value decomposition (Sunehag et al., 2018), obtains local policy by factorizing this centralized value function into the utility function of each agent. In order to ensure that the update of local policies can indeed bring the improvement of joint policy, Individual-Global-Max (IGM) is introduced to guarantee the consistency between joint and local policies. Based on the different interpretations of IGM, various MARL algorithms have been proposed, such as VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), and QPLEX (Wang et al., 2020a). IGM only specifies the relationship between optimal local actions and optimal joint action, which is often used to learn deterministic policies. In order to learn stochastic policies, which are more suitable for the partially observable environment, recent studies (Su et al., 2021; Wang et al., 2020b; Zhang et al., 2021; Su \& Lu, 2022) combine the idea of[^0]value decomposition with actor-critic. While most of these decomposed actor-critic methods do not guarantee optimality, FOP (Zhang et al., 2021) introduces Individual-Global-Optimal (IGO) for the optimal joint policy learning in terms of maximum-entropy objective and derives the corresponding way of value decomposition. It is proved that factorized local policies of FOP converge to the global optimum, given that IGO is satisfied.

The essence of IGO is for all agents to be independent of each other during both training and execution. However, we find this requirement dramatically reduces the expressiveness of the joint policy, making the learning algorithm fail to converge to the global optimal joint policy, even in some simple scenarios. As centralized training is allowed, a natural way to address this issue is to factorize the joint policy based on the chain rule (Schum, 2001), such that the dependency among agents' policies is explicitly considered, and the full expressiveness of the joint policy can be achieved. By incorporating such a joint policy factorization into the soft policy iteration (Haarnoja et al., 2018), we can obtain an optimal joint policy without the IGO condition. Though optimal, a joint policy induced by such a learning method may not be decomposed into independent local policies, thus decentralized execution is not fulfilled, which is the limitation of many previous works that consider dependency among agents (Bertsekas, 2019; Fu et al., 2022).

To fulfill decentralized execution, we first theoretically show that for such a dependent joint policy, there always exists another independent joint policy that achieves the same expected return but can be decomposed into independent local policies. To learn the optimal joint policy while preserving decentralized execution, we propose multi-agent conditional policy factorization (MACPF), where we represent the dependent local policy by combining an independent local policy and a dependency policy correction. The dependent local policies factorize the optimal joint policy, while the independent local policies constitute their independent counterpart that enables decentralized execution. We evaluate MACPF in several tasks, including matrix game (Rashid et al., 2020), SMAC (Samvelyan et al., 2019), and MPE (Lowe et al., 2017). Empirically, MACPF consistently outperforms its base method, i.e., FOP, and achieves better performance or faster convergence than other baselines. By ablation, we verify that the independent local policies can indeed obtain the same level of performance as the dependent local policies.

## 2 PRELIMINARIES

### 2.1 Multi-AGENT MARKov DeCision ProceSS

In cooperative MARL, we often formulate the problem as a multi-agent Markov decision process (MDP) (Boutilier, 1996). A multi-agent MDP can be defined by a tuple $\langle I, S, A, P, r, \gamma, N\rangle$. $N$ is the number of agents, $I=\{1,2 \ldots, N\}$ is the set of agents, $S$ is the set of states, and $A=A_{1} \times \cdots \times A_{N}$ is the joint action space, where $A_{i}$ is the individual action space for each agent $i$. For the rigorousness of proof, we assume full observability such that at each state $s \in S$, each agent $i$ receives state $s$, chooses an action $a_{i} \in A_{i}$, and all actions form a joint action $\boldsymbol{a} \in A$. The state transitions to the next state $s^{\prime}$ upon $\boldsymbol{a}$ according to the transition function $P\left(s^{\prime} \mid s, \boldsymbol{a}\right): S \times A \times S \rightarrow[0,1]$, and all agents receive a shared reward $r(s, \boldsymbol{a}): S \times A \rightarrow \mathbb{R}$. The objective is to learn a local policy $\pi_{i}\left(a_{i} \mid \mathrm{s}\right)$ for each agent such that they can cooperate to maximize the expected cumulative discounted return, $\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]$, where $\gamma \in[0,1)$ is the discount factor. In CTDE, from a centralized perspective, a group of local policies can be viewed as a joint policy $\pi_{\mathrm{jt}}(\boldsymbol{a} \mid \mathrm{s})$. For this joint policy, we can define the joint state-action value function $Q_{\mathrm{jt}}\left(\mathrm{s}_{t}, \boldsymbol{a}_{t}\right)=\mathbb{E}_{\mathrm{s}_{t+1: \infty}, \boldsymbol{a}_{t+1: \infty}}\left[\sum_{k=0}^{\infty} \gamma^{t} r_{t+k} \mid \mathrm{s}_{t}, \boldsymbol{a}_{t}\right]$. Note that although we assume full observability for the rigorousness of proof, we use the trajectory of each agent $\tau_{i} \in \mathcal{T}_{i}:\left(Y \times A_{i}\right)^{*}$ to replace state $s$ as its policy input to settle the partial observability in practice, where $Y$ is the observation space.

### 2.2 FOP

FOP (Zhang et al., 2021) is one of the state-of-the-art CTDE methods for cooperative MARL, which extends value decomposition to learning stochastic policy. In FOP, the joint policy is decomposed into independent local policies based on Individual-Global-Optimal (IGO), which can be stated as:

$$
\begin{equation*}
\pi_{\mathrm{jt}}(\boldsymbol{a} \mid \mathrm{s})=\prod_{i=1}^{N} \pi_{i}\left(a_{i} \mid \mathrm{s}\right) \tag{1}
\end{equation*}
$$

As all policies are learned by maximum-entropy RL (Haarnoja et al., 2018), i.e., $\pi_{i}\left(a_{i} \mid \mathrm{s}\right)=$ $\exp \left(\frac{1}{\alpha_{i}}\left(Q_{i}\left(\mathrm{~s}, a_{i}\right)-V_{i}(\mathrm{~s})\right)\right)$, IGO immediately implies a specific way of value decomposition:

$$
\begin{equation*}
Q_{\mathrm{jt}}(\mathrm{s}, \boldsymbol{a})=\sum_{i=1}^{N} \frac{\alpha}{\alpha_{i}}\left[Q_{i}\left(\mathrm{~s}, a_{i}\right)-V_{i}(\mathrm{~s})\right]+V_{\mathrm{jt}}(\mathrm{s}) \tag{2}
\end{equation*}
$$

Unlike IGM, which is used to learn deterministic local policies and naturally avoids the dependency of agents, IGO assumes agents are independent of each other in both training and execution. Although IGO advances FOP to learn stochastic policies, such an assumption can be problematic even in some simple scenarios and prevent learning the optimal joint policy.

### 2.3 PRoblemATIC IGO

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-03.jpg?height=368&width=1306&top_left_y=816&top_left_x=407)

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-03.jpg?height=298&width=391&top_left_y=824&top_left_x=409)

(a) centralized control

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-03.jpg?height=301&width=396&top_left_y=820&top_left_x=862)

(b) FOP

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-03.jpg?height=298&width=376&top_left_y=824&top_left_x=1319)

(c) dependency

Figure 1: Sampled trajectories from the learned policy: (a) centralized control; (b) FOP, where IGO is assumed; (c) considering dependency during training.

As stated in soft Q-learning (Haarnoja et al., 2018), one goal of maximum-entropy RL is to learn an optimal maximum-entropy policy that captures multiple modes of near-optimal behavior. Since FOP can be seen as the extension of maximum-entropy RL in multi-agent settings, it is natural to assume that FOP can also learn a multi-modal joint policy in multi-agent settings. However, as shown in the following example, such a desired property of maximum-entropy RL is not inherited in FOP due to the IGO condition.

We extend the single-agent multi-goal environment used in soft Q-learning (Haarnoja et al., 2018) to its multi-agent variant to illustrate the problem of IGO. In this environment, we want to control a 2D point mass to reach one of four symmetrically placed goals, as illustrated in Figure 1. The reward is defined as a mixture of Gaussians, with means placed at the goal positions. Unlike the original environment, this $2 \mathrm{D}$ point mass is now jointly controlled by two agents, and it can only move when these two agents select the same moving direction; otherwise, it will stay where it is. As shown in Figure 1a, when centralized control is allowed, multi-agent training degenerates to single-agent training, and the desired multi-modal policy can be learned. However, as shown in Figure 1b, FOP struggles to learn any meaningful joint policy for the multi-agent setting. One possible explanation is that, since IGO is assumed in FOP, the local policy of each agent is always independent of each other during training, and the expressiveness of joint policy is dramatically reduced. Therefore, when two agents have to coordinate to make decisions, they may fail to reach an agreement and eventually behave in a less meaningful way due to the limited expressiveness of joint policy. To solve this problem, we propose to consider dependency among agents in MARL algorithms to enrich the expressiveness of joint policy. As shown in Figure 1c, the learned joint policy can once again capture multiple modes of near-optimal behavior when the dependency is considered. Details of this algorithm will be discussed in the next section.

## 3 METHOD

To overcome the aforementioned problem of IGO, we propose multi-agent conditional policy factorization (MACPF). In MACPF, we introduce dependency among agents during centralized training to ensure the optimality of the joint policy without the need for IGO. This joint policy consists of dependent local policies, which take the actions of other agents as input, and we use this joint policy as the behavior policy to interact with the environment during training. In order to fulfill decentralized
execution, independent local policies are obtained from these dependent local policies such that the joint policy resulting from these independent local policies is equivalent to the behavior policy in terms of expected return.

### 3.1 CONDITIONAL FACTORIZED SOFT POLICY ITERATION

Like FOP, we also use maximum-entropy RL (Ziebart, 2010) to bridge policy and state-action value function for each agent. Additionally, it will also be used to introduce dependency among agents. For each local policy, we take the actions of other agents as its input and define it as follows:

$$
\begin{align*}
& \pi_{i}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)=\exp \left(\frac{1}{\alpha_{i}}\left(Q_{i}\left(\mathrm{~s}, a_{<i}, a_{i}\right)-V_{i}\left(\mathrm{~s}, a_{<i}\right)\right)\right)  \tag{3}\\
& V_{i}\left(\mathrm{~s}, a_{<i}\right):=\alpha_{i} \sum_{a_{i}} \exp \left(\frac{1}{\alpha_{i}} Q_{i}\left(\mathrm{~s}, a_{<i}, a_{i}\right)\right) \tag{4}
\end{align*}
$$

where $a_{<i}$ represents the joint action of all agents whose indices are smaller than agent $i$. We then can get the relationship between the joint policy and local policies based on the chain rule factorization of joint probability:

$$
\begin{equation*}
\pi_{\mathrm{jt}}(\boldsymbol{a} \mid \mathrm{s})=\prod_{i=1}^{N} \pi_{i}\left(a_{i} \mid \mathrm{s}, a_{<i}\right) \tag{5}
\end{equation*}
$$

The full expressiveness of the joint policy can be guaranteed by (5) as it is no longer restricted by the IGO condition. From (5), together with $\pi_{\mathrm{jt}}(\boldsymbol{a} \mid \mathrm{s})=\exp \left(\frac{1}{\alpha}\left(Q_{\mathrm{jt}}(\mathrm{s}, \boldsymbol{a})-V_{\mathrm{jt}}(\mathrm{s})\right)\right)$, we have the $Q_{\mathrm{jt}}$ factorization as:

$$
\begin{equation*}
Q_{\mathrm{jt}}(\mathrm{s}, \boldsymbol{a})=\sum_{i=1}^{N} \frac{\alpha}{\alpha_{i}}\left[Q_{i}\left(\mathrm{~s}, a_{<i}, a_{i}\right)-V_{i}\left(\mathrm{~s}, a_{<i}\right)\right]+V_{\mathrm{jt}}(\mathrm{s}) \tag{6}
\end{equation*}
$$

Note that in maximum-entropy RL, we can easily compute $V$ by $Q$. From (6), we introduce conditional factorized soft policy iteration and prove its convergence to the optimal joint policy in the following theorem.

Theorem 1 (Conditional Factorized Soft Policy Iteration). For any joint policy $\pi_{\mathrm{jt}}$, if we repeatedly apply joint soft policy evaluation and individual conditional soft policy improvement from $\pi_{i} \in \Pi_{i}$. Then the joint policy $\pi_{\mathrm{jt}}(\boldsymbol{a} \mid \mathrm{s})=\prod_{i=1}^{N} \pi_{i}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)$ converges to $\pi_{\mathrm{jt}}^{*}$, such that $Q_{\mathrm{jt}}^{\pi_{\mathrm{jt}}^{*}}(\mathrm{~s}, \boldsymbol{a}) \geq Q_{\mathrm{jt}}^{\pi_{\mathrm{jt}}}(\mathrm{s}, \boldsymbol{a})$ for all $\pi_{\mathrm{jt}}$, assuming $|A|<\infty$.

Proof. See Appendix A.

### 3.2 Independent JOINT POLICY

Using the conditional factorized soft policy iteration, we are able to get the optimal joint policy. However, such a joint policy requires dependent local policies, which are incapable of decentralized execution. To fulfill decentralized execution, we have to obtain independent local policies.

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-04.jpg?height=173&width=349&top_left_y=2060&top_left_x=411)

(a) dependent joint policy $\pi_{\mathrm{jt}}^{\mathrm{dep}}$

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-04.jpg?height=174&width=306&top_left_y=2057&top_left_x=885)

(b) independent joint policy $\pi_{\mathrm{jt}}^{\mathrm{ind}}$

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-04.jpg?height=176&width=374&top_left_y=2059&top_left_x=1296)

(c) dependency correction $b_{\mathrm{jt}}^{\text {dep }}$

Figure 2: A dependent joint policy and its independent counterpart

Consider the joint policy shown in Figure 2a. This joint policy, called dependent joint policy $\pi_{\mathrm{jt}}^{\mathrm{dep}}$, involves dependency among agents and thus cannot be factorized into two independent local policies. However, one may notice that this policy can be decomposed as the combination of an independent joint policy $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ that involves no dependency among agents, as shown in Figure 2b, and a dependency
policy correction $b_{\mathrm{jt}}^{\mathrm{dep}}$, as shown in Figure 2c. More importantly, since we use the Boltzmann distribution of joint $Q$-values as the joint policy, the equivalence of probabilities of two joint actions also indicates that their joint $\mathrm{Q}$-values are the same,

$$
\begin{equation*}
\pi_{\mathrm{jt}}^{\mathrm{dep}}(A, A)=\pi_{\mathrm{jt}}^{\mathrm{dep}}(B, B) \Rightarrow Q_{\mathrm{jt}}(A, A)=Q_{\mathrm{jt}}(B, B) \tag{7}
\end{equation*}
$$

Therefore, in Table 2, the expected return of the independent joint policy $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ will be the same as the dependent joint policy $\pi_{\mathrm{jt}}^{\mathrm{dep}}$

$$
\begin{align*}
\mathbb{E}_{\pi_{\mathrm{jt}}^{\mathrm{dep}}}\left[Q_{\mathrm{jt}}\right] & =\pi_{\mathrm{jt}}^{\mathrm{dep}}(A, A) * Q_{\mathrm{jt}}(A, A)+\pi_{\mathrm{jt}}^{\mathrm{dep}}(B, B) * Q_{\mathrm{jt}}(B, B)  \tag{8}\\
& =\pi_{\mathrm{jt}}^{\mathrm{ind}}(A, A) * Q_{\mathrm{jt}}(A, A)=\mathbb{E}_{\pi_{\mathrm{jt}}^{\mathrm{jnd}}}\left[Q_{\mathrm{jt}}\right] \tag{9}
\end{align*}
$$

Formally, we have the following theorem.

Theorem 2. For any dependent joint policy $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ that involves dependency among agents, there exists an independent joint policy $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ that does not involve dependency among agents, such that $V_{\pi_{\mathrm{jt}}^{\mathrm{dep}}}(s)=V_{\pi_{\mathrm{jt}}^{\mathrm{ind}}}(s)$ for any state $s \in S$.

Proof. See Appendix B.

Note that the independent counterpart of the optimal dependent joint policy may not be directly learned by FOP, as shown in Figure 1. Therefore, we need to explicitly learn the optimal dependent joint policy to obtain its independent counterpart.

### 3.3 MACPF FRAMEWORK

With Theorem 1 and 2, we are ready to present the learning framework of MACPF, as illustrated in Figure 3, for simultaneously learning the dependent joint policy and its independent counterpart.

In MACPF, each agent $i$ has an independent local policy $\pi_{i}^{\text {ind }}\left(a_{i} \mid \mathrm{s} ; \theta_{i}\right)$ parameterized by $\theta_{i}$ and a dependency policy correction $b_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i} ; \phi_{i}\right)$ parameterized by $\phi_{i}$, which together constitute a dependent local policy $\pi_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)^{1}$. So, we have:

$$
\begin{align*}
& \pi_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)=\pi_{i}^{\mathrm{ind}}\left(a_{i} \mid \mathrm{s} ; \theta_{i}\right)+b_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i} ; \phi_{i}\right)  \tag{10}\\
& \pi_{\mathrm{jt}}^{\mathrm{dep}}(\mathrm{s}, \boldsymbol{a})=\prod_{i=1}^{N} \pi_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)  \tag{11}\\
& \pi_{\mathrm{jt}}^{\mathrm{ind}}(\mathrm{s}, \boldsymbol{a})=\prod_{i=1}^{N} \pi_{i}^{\mathrm{ind}}\left(a_{i} \mid \mathrm{s} ; \theta_{i}\right) \tag{12}
\end{align*}
$$

Similarly, each agent $i$ also has an independent local critic $Q_{i}^{\text {ind }}\left(a_{i} \mid \mathrm{s} ; \psi_{i}\right)$ parameterized by $\psi_{i}$ and a dependency critic correction $c_{i}^{\text {dep }}\left(a_{i} \mid \mathrm{s}, a_{<i} ; \omega_{i}\right)$ parameterized by $\omega_{i}$, which together constitute a dependent local critic $Q_{i}^{\text {dep }}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)$. Given all $Q_{i}^{\text {ind }}$ and $Q_{i}^{\text {dep }}$, we use a mixer network, $\operatorname{Mixer}(\cdot ; \Theta)$ parameterized by $\Theta$, to get $Q_{\mathrm{jt}}^{\mathrm{dep}}$ and $Q_{\mathrm{jt}}^{\mathrm{ind}}$ as follows,

$$
\begin{align*}
& Q_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)=Q_{i}^{\mathrm{ind}}\left(a_{i} \mid \mathrm{s} ; \psi_{i}\right)+c_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i} ; \omega_{i}\right)  \tag{13}\\
& Q_{\mathrm{jt}}^{\mathrm{dep}}(\mathrm{s}, \boldsymbol{a})=\operatorname{Mixer}\left(\left[Q_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)\right]_{i=1}^{N}, \mathrm{~s} ; \Theta\right)  \tag{14}\\
& Q_{\mathrm{jt}}^{\mathrm{ind}}(\mathrm{s}, \boldsymbol{a})=\operatorname{Mixer}\left(\left[Q_{i}^{\mathrm{ind}}\left(a_{i} \mid \mathrm{s} ; \psi_{i}\right)\right]_{i=1}^{N}, \mathrm{~s} ; \Theta\right) \tag{15}
\end{align*}
$$

$Q_{i}^{\text {dep }}, Q_{i}^{\text {ind }}$, and Mixer are learned by minimizing the TD error,

$$
\begin{align*}
\mathcal{L}^{\mathrm{dep}}\left(\left[\omega_{i}\right]_{i=1}^{N}, \Theta\right) & =\mathbb{E}_{\mathcal{D}}\left[\left(Q_{\mathrm{jt}}^{\mathrm{dep}}(\mathrm{s}, \boldsymbol{a})-\left(r+\gamma\left(\hat{Q}_{\mathrm{jt}}^{\mathrm{dep}}\left(\mathrm{s}^{\prime}, \boldsymbol{a}^{\prime}\right)-\alpha \log \pi_{\mathrm{jt}}^{\mathrm{dep}}\left(\boldsymbol{a}^{\prime} \mid \mathrm{s}^{\prime}\right)\right)\right)\right)^{2}\right]  \tag{16}\\
\mathcal{L}^{\mathrm{ind}}\left(\left[\psi_{i}\right]_{i=1}^{N}, \Theta\right) & =\mathbb{E}_{\mathcal{D}}\left[\left(Q_{\mathrm{jt}}^{\mathrm{ind}}(\mathrm{s}, \boldsymbol{a})-\left(r+\gamma\left(\hat{Q}_{\mathrm{jt}}^{\mathrm{ind}}\left(\mathrm{s}^{\prime}, \boldsymbol{a}^{\prime}\right)-\alpha \log \pi_{\mathrm{jt}}^{\mathrm{ind}}\left(\boldsymbol{a}^{\prime} \mid \mathrm{s}^{\prime}\right)\right)\right)\right)^{2}\right] \tag{17}
\end{align*}
$$[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-06.jpg?height=410&width=1309&top_left_y=272&top_left_x=408)

Figure 3: Learning framework of MACPF, where each agent $i$ has four modules: an independent local policy $\pi_{i}^{\text {ind }}\left(\cdot ; \theta_{i}\right)$, a dependency policy correction $b_{i}^{\text {dep }}\left(\cdot ; \phi_{i}\right)$, an independent local critic $Q_{i}^{\text {ind }}\left(\cdot ; \psi_{i}\right)$, and a dependency critic correction $c_{i}^{\mathrm{dep}}\left(\cdot, \omega_{i}\right)$.

where $\mathcal{D}$ is the replay buffer collected by $\pi_{\mathrm{jt}}^{\mathrm{dep}}, \hat{Q}$ is the target network, and $\boldsymbol{a}^{\prime}$ is sampled from the current $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ and $\pi_{\mathrm{jt}}^{\mathrm{ind}}$, respectively. To ensure the independent joint policy $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ has the same performance as $\pi_{\mathrm{jt}}^{\mathrm{dep}}$, the same batch sampled from $\mathcal{D}$ is used to compute both $\mathcal{L}^{\mathrm{dep}}$ and $\mathcal{L}^{\mathrm{ind}}$. It is worth noting that the gradient of $\mathcal{L}^{\text {dep }}$ only updates $\left[c_{i}^{\text {dep }}\right]_{i=1}^{N}$, while the gradient of $\mathcal{L}^{\text {ind }}$ only updates $\left[Q_{i}^{\text {ind }}\right]_{i=1}^{N}$. Then, $\pi_{i}^{\text {dep }}$ and $\pi_{i}^{\text {ind }}$ are updated by minimizing KL-divergence as follows,

$$
\begin{align*}
& \mathcal{J}^{\mathrm{dep}}\left(\phi_{i}\right)=\mathbb{E}_{\mathcal{D}, a_{<i} \sim \pi_{<i}^{\mathrm{dep}}, a_{i} \sim \pi_{i}^{\mathrm{dep}}}\left[\alpha_{i} \log \pi_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)-Q_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)\right]  \tag{18}\\
& \mathcal{J}^{\mathrm{ind}}\left(\theta_{i}\right)=\mathbb{E}_{\mathcal{D}, a_{i} \sim \pi_{i}^{\mathrm{ind}}}\left[\alpha_{i} \log \pi_{i}^{\mathrm{ind}}\left(a_{i} \mid \mathrm{s} ; \theta_{i}\right)-Q_{i}^{\mathrm{ind}}\left(a_{i} \mid \mathrm{s} ; \psi_{i}\right)\right] \tag{19}
\end{align*}
$$

Similarly, the gradient of $\mathcal{J}^{\text {dep }}$ only updates $b_{i}^{\text {dep }}$ and the gradient of $\mathcal{J}^{\text {ind }}$ only updates $\pi_{i}^{\text {ind }}$. For computing $\mathcal{J}^{\text {dep }}, a_{<i}$ is sampled from their current policies $\pi_{<i}^{\text {dep }}$.

The purpose of learning $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ is to enable decentralized execution while achieving the same performance as $\pi_{\mathrm{jt}}^{\mathrm{dep}}$. Therefore, a certain level of coupling has to be assured between $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ and $\pi_{\mathrm{jt}}^{\mathrm{dep}}$. First, motivated by Figure 2, we constitute the dependent policy as a combination of an independent policy and a dependency policy correction, similarly for the local critic. Second, as aforementioned, the replay buffer $\mathcal{D}$ is collected by $\pi_{\mathrm{jt}}^{\mathrm{dep}}$, which implies $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ is the behavior policy and the learning of $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ is offline. Third, we use the same Mixer to compute $Q_{\mathrm{jt}}^{\mathrm{dep}}$ and $Q_{\mathrm{jt}}^{\mathrm{ind}}$. The performance comparison between $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ and $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ will be studied by experiments.

## 4 RELATED WORK

Multi-agent policy gradient. In multi-agent policy gradient, a centralized value function is usually learned to evaluate current joint policy and guide the update of each local policy. Most multi-agent policy gradient methods can be considered as an extension of policy gradient from RL to MARL. For example, MAPPDG (Lowe et al., 2017) extends DDPG (Lillicrap et al., 2015), PS-TRPO(Gupta et al., 2017) and MATRPO (Kuba et al., 2021) extend TRPO (Schulman et al., 2015), and MAPPO (Yu et al., 2021) extends PPO (Schulman et al., 2017). Some methods additionally address multi-agent credit assignment by policy gradient, e.g., counterfactual policy gradient (Foerster et al., 2018) or difference rewards policy gradient (Castellini et al., 2021; Li et al., 2022).

Value decomposition. Instead of providing gradients for local policies, in value decomposition, the centralized value function, usually a joint $\mathrm{Q}$-function, is directly decomposed into local utility functions. Many methods have been proposed as different interpretations of Individual-GlobalMaximum (IGM), which indicates the consistency between optimal local actions and optimal joint action. VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018) give sufficient conditions for IGM by additivity and monotonicity, respectively. QTRAN (Son et al., 2019) transforms IGM into optimization constraints, while QPLEX (Wang et al., 2020a) takes advantage of duplex dueling architecture to guarantee IGM. Recent studies (Su et al., 2021; Wang et al., 2020b; Zhang et al., 2021; $\mathrm{Su} \& \mathrm{Lu}, 2022$ ) combine value decomposition with policy gradient to learn stochastic policies, which

| $a_{2}$ | $A$ | $B$ | $C$ | $D$ |
| :---: | :---: | :---: | :---: | :---: |
| $A$ | 8 | -20 | -20 | -20 |
| $B$ | -12 | 0 | 0 | -20 |
| $C$ | -12 | 0 | 0 | -20 |
| $D$ | -12 | -12 | -12 | 8 |

(a) payoff matrix

![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-07.jpg?height=306&width=407&top_left_y=281&top_left_x=843)

(b) learning curves

| $a_{2}$ | $A$ | $B$ | $C$ | $D$ |
| :---: | :---: | :---: | :---: | :---: |
| $a_{1}$ |  |  |  |  |
| $A$ | $\mathbf{0 . 5}$ | 0 | 0 | 0 |
| $B$ | 0 | 0 | 0 | 0 |
| $C$ | 0 | 0 | 0 | 0 |
| $D$ | 0 | 0 | 0 | $\mathbf{0 . 5}$ |

(c) $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ of MACPF

Figure 4: A matrix game that has two optimal joint actions: (a) payoff matrix; (b) learning curves of different methods; (c) the learned dependent joint policy of MACPF.

are more desirable in partially observable environments. However, most research in this category does not guarantee optimality, while our method enables agents to learn the optimal joint policy.

Coordination graph. In coordination graph (Guestrin et al., 2002) methods (Böhmer et al., 2020; Wang et al., 2021b; Yang et al., 2022), the interactions between agents are considered as part of value decomposition. Specifically, the joint Q-function is decomposed into the combination of utility functions and payoff functions. The introduction of payoff functions increases the expressiveness of the joint $\mathrm{Q}$-function and considers at least pair-wise dependency among agents, which is similar to our algorithm, where the complete dependency is considered. However, to get the joint action with the maximum $\mathrm{Q}$-value, communication between agents is required in execution in coordination graph methods, while our method still fulfills fully decentralized execution.

Coordinated exploration. One of the benefits of considering dependency is coordinated exploration. From this perspective, our method might be seen as a relative of coordinated exploration methods (Mahajan et al., 2019; Iqbal \& Sha, 2019; Zheng et al., 2021). In MAVEN (Mahajan et al., 2019), a shared latent variable is used to promote committed, temporally extended exploration. In EMC (Zheng et al., 2021), the intrinsic reward based on the prediction error of individual Q-values is used to induce coordinated exploration. It is worth noting that our method does not conflict with coordinated exploration methods and can be used simultaneously as our method is a base cooperative MARL algorithm. However, such a combination is beyond the scope of this paper.

## 5 EXPERIMENTS

In this section, we evaluate MACPF in three different scenarios. One is a simple yet challenging matrix game, which we use to verify whether MACPF can indeed converge to the optimal joint policy. Then, we evaluate MACPF on two popular cooperative MARL scenarios: StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) and MPE (Lowe et al., 2017), comparing it against QMIX (Rashid et al., 2018), QPLEX (Wang et al., 2020a), FOP (Zhang et al., 2021), and MAPPO (Yu et al., 2021). More details about experiments and hyperparameters are included in Appendix C. All results are presented using the mean and standard deviation of five runs with different random seeds. In SMAC experiments, for visual clarity, we plot the curves with the moving average of a window size of five and half standard deviation.

### 5.1 MATRIX GAME

In this matrix game, we have two agents. Each can pick one of the four actions and get a reward based on the payoff matrix depicted in Figure 4a. Unlike the non-monotonic matrix game in QTRAN (Son et al., 2019), where there is only one optimal joint action, we have two optimal joint actions in this game, making this scenario much more challenging for many cooperative MARL algorithms.

As shown in Figure 4b, general value decomposition methods, QMIX, QPLEX, and FOP, fail to learn the optimal coordinated strategy in most cases. The same negative result can also be observed for MAPPO. For general MARL algorithms, since agents are fully independent of each other when making decisions, they may fail to converge to the optimal joint action, which eventually leads to a suboptimal joint policy. As shown in Figure 4b, QMIX and MAPPO fail to converge to the optimal policy but find a suboptimal policy in all the seeds, while QPLEX, QTRAN, and FOP find the optima by chance (i.e., $60 \%$ for QPLEX, $20 \%$ for QTRAN, and $40 \%$ for FOP). This is because, in QMIX, the
![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-08.jpg?height=666&width=1312&top_left_y=260&top_left_x=406)

Figure 5: Learning curves of all the methods in six maps of SMAC, where the unit of $\mathrm{x}$-axis is $1 \mathrm{M}$ timesteps and $\mathrm{y}$-axis represents the win rate of each map.

mixer network is purely a function of state and the input utility functions that are fully independent of each other. Thus it considers no dependency at all and cannot solve this game where dependency has to be considered. For QPLEX and FOP, since the joint action is considered as the input of their mixer network, the dependency among agents may be implicitly considered, which leads to the case where they can find the optima by chance. However, since the dependency is not considered explicitly, there is also a possibility that the mixer network misinterprets the dependency, which makes QPLEX and FOP sometimes find even worse policies than QMIX ( $20 \%$ for QPLEX and $40 \%$ for FOP). For QTRAN, it always finds at least the suboptimal policy in all the seeds. However, its optimality largely relies on the learning of its $V_{\mathrm{jt}}$, which is very unstable, so it also only finds the optima by chance.

For the dependent joint policy $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ of MACPF, the local policy of the second agent depends on the action of the first agent. As a result, we can see from Figure $4 \mathrm{~b}$ that $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ (denoted as MACPF_DEP) always converges to the highest return. We also notice that in Figure $4 \mathrm{c}, \pi_{\mathrm{jt}}^{\mathrm{dep}}$ indeed captures two optimal joint actions. Unlike QMIX, QPLEX, and FOP, the mixer network in MACPF is a function of state and the input utility functions $Q_{i}^{\mathrm{dep}}\left(a_{i} \mid \mathrm{s}, a_{<i}\right)$ that are properly dependent on each other, so the dependency among agents is explicitly considered. More importantly, the learned independent joint policy $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ of MACPF, denoted as MACPF in Figure $4 \mathrm{~b}$, always converges to the optimal joint policy. Note that in the rest of this section, the performance of MACPF is achieved by the learned $\pi_{\mathrm{jt}}^{\mathrm{ind}}$, unless stated otherwise.

### 5.2 SMAC

Further, we evaluate MACPF on SMAC. Maps used in our experiment include two hard maps (8m_vs_9m, 10m_vs_11m), and two super-hard maps (MMM2, corridor). We also consider two challenging customized maps (8m_vs_9m_myopic, 10m_vs_11m_myopic), where the sight range of each agent is reduced from 9 to 6 , and the information of allies is removed from the observation of agents. These changes are adopted to increase the difficulty of coordination in the original maps. Results are shown in Figure 5. In general, MACPF outperforms the baselines in all six maps. In hard maps, MACPF outperforms the baselines mostly in convergence speed, while in super-hard maps, MACPF outperforms other algorithms in either convergence speed or performance. Especially in corridor, when other value decomposition algorithms fail to learn any meaningful joint policies, MACPF obtains a winning rate of almost $70 \%$. In the two more challenging maps, the margin between MACPF and the baselines becomes much larger than that in the original maps. These results show that MACPF can better handle complex cooperative tasks and learn coordinated strategies by introducing dependency among agents even when the task requires stronger coordination.

We compare MACFP with the baselines in 18 maps totally. Their final performance is summarized in Appendix D. The win rate of MACFP is higher than or equivalent to the best baseline in 16 out of 18 maps, while QMIX, QPLEX, MAPPO, and FOP are respectively 7/18, 8/18, 9/18, and 5/18.
![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-09.jpg?height=284&width=1392&top_left_y=264&top_left_x=366)

Figure 6: Performance of $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ and $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ during training in four maps of SMAC, where the unit of $\mathrm{x}$-axis is $1 \mathrm{M}$ timesteps and y-axis represents the win rate of each map.
![](https://cdn.mathpix.com/cropped/2024_06_04_ea570f57f780255786ebg-09.jpg?height=274&width=1354&top_left_y=633&top_left_x=385)

Figure 7: Ablation study in four maps of SMAC, where the unit of $x$-axis is $1 \mathrm{M}$ timesteps and $\mathrm{y}$-axis represents the win rate of each map.

Dependent and Independent Joint Policy. As discussed in Section 3.2, the learned independent joint policy of MACPF should not only enable decentralized execution but also match the performance of dependent joint policy, as verified in the matrix game. What about complex environments like SMAC? As shown in Figure 6, we track the evaluation result of both $\pi_{\mathrm{jt}}^{\mathrm{ind}}$ and $\pi_{\mathrm{jt}}^{\mathrm{dep}}$ during training. As we can see, their performance stays at the same level throughout training.

Ablation Study. Without learning a dependent joint policy to interact with the environment, our algorithm degenerates to FOP. However, since our factorization of $Q_{\mathrm{jt}}$ is induced from the chain rule factorization of joint probability (5), we use a mixer network different from FOP (the reason is discussed and verified in Appendix E). Here we present an ablation study to further show that the improvement of MACPF is indeed induced by introducing the dependency among agents. In Figure 7, MACPF_CONTROL represents an algorithm where all other perspectives are the same as MACPF, except no dependent joint policy is learned. As shown in Figure 7, MACPF outperforms MACPF_CONTROL in all four maps, demonstrating that the performance improvement is indeed achieved by introducing the dependency among agents.

### 5.3 MPE

We further evaluate MACPF on three MPE tasks, including simple spread, formation control, and line control (Agarwal et al., 2020). As shown in Table 1, MACPF outperforms the baselines in all three tasks. A large margin can be observed in simple spread, while only a minor difference can be observed in the other two. This result may indicate that these MPE tasks are not challenging enough for strong MARL algorithms.

Table 1: Average rewards per episode on three MPE tasks.

| Tasks Algorithms | MACPF | QMIX | QPLEX | FOP | MAPPO |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Simple Spread | $\mathbf{- 1 1 8 . 2 4} \pm 2.74$ | $-145.93 \pm 21.09$ | $-122.50 \pm 2.58$ | $-125.19 \pm 5.42$ | $-166.75 \pm 23.44$ |
| Formation Control | $\mathbf{- 1 5 . 7 9} \pm 0.16$ | $-16.11 \pm 0.30$ | $-16.10 \pm 0.28$ | $-15.84 \pm 0.19$ | $-21.71 \pm 1.69$ |
| Line Control | $\mathbf{- 1 9 . 6 0} \pm 0.33$ | $-20.12 \pm 0.21$ | $-20.17 \pm 0.26$ | $-19.78 \pm 0.27$ | $-24.47 \pm 2.54$ |

## 6 CONCLUSION

We have proposed MACPF, where dependency among agents is introduced to enable more centralized training. By conditional factorized soft policy iteration, we show that dependent local policies provably converge to the optimum. To fulfill decentralized execution, we represent dependent local policies as a combination of independent local policies and dependency policy corrections, such that independent local policies can achieve the same level of expected return as dependent ones. Empirically, we show that MACPF can obtain the optimal joint policy in a simple yet challenging matrix game while baselines fail and MACPF also outperforms the baselines in SMAC and MPE.

## ACKNOWLEDGMENTS

This work was supported in part by NSFC under grant 62250068 and Tencent AI Lab.

## REFERENCES

Akshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative behavior in multi-agent teams. In International Conference on Autonomous Agents and MultiAgent Systems, 2020.

Dimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning. arXiv preprint arXiv:1910.00120, 2019.

Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International Conference on Machine Learning, 2020.

Craig Boutilier. Planning, learning and coordination in multiagent decision processes. In TARK, volume 96, pp. 195-210, 1996.

Jacopo Castellini, Sam Devlin, Frans A Oliehoek, and Rahul Savani. Difference rewards policy gradients. In International Conference on Autonomous Agents and MultiAgent Systems, 2021.

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI conference on artificial intelligence, 2018.

Wei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. Revisiting some common practices in cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 68636877. PMLR, 2022.

Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In International Conference on Machine Learning, 2002.

Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems, 2017.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, 2018.

Shariq Iqbal and Fei Sha. Coordinated exploration via intrinsic rewards for multi-agent reinforcement learning. arXiv preprint arXiv:1905.12127, 2019.

Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82-94, 2016.

Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021.

Yueheng Li, Guangming Xie, and Zongqing Lu. Difference advantage estimation for multi-agent policy gradients. In International Conference on Machine Learning, 2022.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.

Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 2017.

Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational exploration. Advances in Neural Information Processing Systems, 2019.

James R Munkres. Topology, volume 2. Prentice Hall Upper Saddle River, 2000.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, 2018.

Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 2020.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. arXiv preprint arXiv:2007.12322, 2019.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

David A Schum. The Evidential Foundations of Probabilistic Reasoning. Northwestern University Press, 2001.

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, 2019.

Jianyu Su, Stephen Adams, and Peter A Beling. Value-decomposition multi-agent actor-critics. In AAAI Conference on Artificial Intelligence, 2021.

Kefan $\mathrm{Su}$ and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In International Conference on Machine Learning, 2022.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In International Conference on Autonomous Agents and MultiAgent Systems, 2018.

Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.

Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. In International Conference on Learning Representations, 2020a.

Tong Wang, Jiahua Cao, and Azhar Hussain. Adaptive traffic signal control for large-scale scenario with cooperative group-based multi-agent reinforcement learning. Transportation research part $C$ : emerging technologies, 125:103046, 2021a.

Tonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, and Chongjie Zhang. Contextaware sparse deep coordination graphs. arXiv preprint arXiv:2106.02886, 2021b.

Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-agent decomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020b.

Qianlan Yang, Weijun Dong, Zhizhou Ren, Jianhao Wang, Tonghan Wang, and Chongjie Zhang. Self-organized polynomial-time coordination graphs. In International Conference on Machine Learning, 2022.

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.

Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint policy of maximum-entropy multi-agent reinforcement learning. In International Conference on Machine Learning, 2021.

Lulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-driven exploration. Advances in Neural Information Processing Systems, 2021.

Tong Zhou, Dunbing Tang, Haihua Zhu, and Zequn Zhang. Multi-agent reinforcement learning for online scheduling in smart factories. Robotics and Computer-Integrated Manufacturing, 72: $102202,2021$.

Brian D Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. Carnegie Mellon University, 2010.
