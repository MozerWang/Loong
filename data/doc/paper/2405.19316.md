# Robust Preference Optimization through Reward Model Distillation 

Adam Fisch* Jacob Eisenstein* Vicky Zayats* Alekh Agarwal<br>Ahmad Beirami Chirag Nagpal Pete Shaw Jonathan Berant*<br>Google DeepMind


#### Abstract

Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.


## 1 Introduction

Language model (LM) post-training (or alignment) aims to steer language model policies towards responses that agree with human preferences. Early state-of-the-art approaches have focused on reward learning from human feedback. In this paradigm, preference annotations are used to train reward models, which then guide the optimization of the language model policy through online reinforcement learning (an approach broadly referred to as RLHF). Recent research on offline "Direct Preference Optimization" [DPO; 23] and extensions thereof [3; 31], however, has demonstrated that it is also possible to directly optimize policies on the preference data, which bypasses the need for a separate reward model-and its offline nature also leads to faster, and simpler, training frameworks.

While this direct approach to preference optimization is attractive in terms of its simplicity and efficiency, it also raises important questions about the effectiveness and robustness of the resulting policies-as well as the broader utility of using an explicit reward model. In this paper, we argue that explicit reward modeling can, in fact, offer substantial practical advantages that are not captured by DPO's formulation. In particular, we theoretically show that relying solely on the preference data can be a precarious strategy, with few natural brakes in place to prevent policies trained under the DPO objective from careening off towards degenerate policies when the preference data exhibits certain idiosyncratic properties. On the other hand, explicit reward models can easily be regularized[^0]and understood—regardless of whether they are Bradley-Terry models [4], margin-based ranking models [40], or simply any other kind of function that correlates well with human preferences [31; 17].

Taking a step back from pure direct preference optimization, we propose a method that merges the best of both worlds: an efficient reward model distillation algorithm that (i) operates effectively in the offline setting, (ii) makes minimal assumptions about the true, optimal reward we aim to maximize, and (iii) demonstrates greater robustness to the specific distribution of prompt/response data used for policy alignment. Drawing inspiration from prior knowledge distillation techniques [14; 26; 35; 10], we leverage the same change of variables trick employed in DPO to express the language model policy in terms of its implicit reward model [23]. We then train the policy to match our desired, explicit reward via an $L_{2}$ loss that directly regresses the pairwise differences in target rewards for any two generation pairs $\left(x, y_{1}\right)$ and $\left(x, y_{2}\right)$. We theoretically establish the equivalence between optimizing this simple distillation loss over a sufficiently diverse offline dataset of unlabeled examples and optimizing the traditional online RLHF objective with reinforcement learning.

Our reward model distillation approach, however, is not immune to some of the same challenges facing DPO-style learning of policies. In particular, reward model distillation requires having a reliable reward model—but having a reliable reward requires having a reliable method for extracting a reward model from a potentially noisy preference dataset. To address the uncertainty surrounding the "right" reward model, we introduce a pessimistic extension to our approach. This extension aims to maximize the worst-case improvement of our model across a plausible family of reward models (e.g., those sufficiently consistent with annotated preference data). This strategy aligns with that of existing work in conservative offline reinforcement learning [5; 16]. Interestingly, we derive that this pessimistic objective can be equivalently expressed and optimized by adding a simple additional KL-divergence regularization to the original distillation objective.

Empirically, we find that reward model distillation, particularly pessimistic reward model distillation, leads to similar performance to prior direct preference optimization methods in settings where the preference datasets used are unbiased, but significantly better performance in settings where the preference datasets are biased, when compared to DPO and the Identity Preference Optimization (IPO) framework of [3], which was introduced as a more robust alternative to DPO. To further support these empirical observations, we provide an extensive theoretical analysis that both (i) sheds more light on the degenerative tendencies of DPO and issues inherent to its objective, and (ii) highlights relative advantages of our explicitly regularized approaches.

## 2 Preliminaries

We begin with a brief review of Direct Preference Optimization (DPO) [23] and its analysis. Proofs of all theoretical results provided here, and in the rest of the paper, are deferred to Appendix A.

### 2.1 The preference alignment problem

Let $x$ be an input prompt, and let $y \sim \pi_{\theta}(\cdot \mid x)$ be the language model policy $\pi_{\theta}$ 's response to $x$. Given some reward function $r^{*}(x, y)$ and another reference policy $\pi_{\text {ref }}(y \mid x)$, the goal of alignment is to solve for the "aligned" policy $\pi_{\theta^{*}}(y \mid x)$ that maximizes the following RLHF objective, i.e.,

$$
\begin{equation*}
\pi_{\theta^{*}}(y \mid x)=\underset{\pi_{\theta}}{\operatorname{argmax}} \mathbb{E}_{\mu(x)}\left[\mathbb{E}_{\pi_{\theta}(y \mid x)}\left[r^{*}(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}(\cdot \mid x) \| \pi_{\mathrm{ref}}(\cdot \mid x)\right]\right] \tag{1}
\end{equation*}
$$

where $\mu(x)$ is a fixed distribution over prompts, and the KL-divergence term prevents the aligned policy from being dramatically different from the anchoring reference policy, $\pi_{\mathrm{ref}}(y \mid x)$. Here, the reward function $r^{*}$ is typically not known in advance, but rather inferred from collected human preference data in the form of $\left(x, y^{w}, y^{\ell}\right)$, where $x$ is the prompt, $y^{w}$ is the "winning", or preferred, response, and $y^{\ell}$ is the "losing", or dispreferred, response. A common approach is to assume that pairs $\left(y_{1}, y_{2}\right)$ follow a Bradley-Terry model [4], under which the probability that $y_{1}$ is preferred to $y_{2}$ given the reward function $r^{*}$ and prompt $x$ is $p^{*}\left(y_{1} \succ y_{2} \mid x\right)=\sigma\left(r^{*}\left(x, y_{1}\right)-r^{*}\left(x, y_{2}\right)\right)$, where $\sigma(\cdot)$ is the sigmoid function and $\succ$ denotes preference. Under this model, we can use the preference data $\left(x, y^{w}, y^{\ell}\right) \sim \mathcal{D}_{\text {pref }}$ to estimate $r^{*}$ via maximum likelihood estimation, i.e.,

$$
\begin{equation*}
\hat{r} \in \underset{r}{\operatorname{argmin}} \mathbb{E}_{\left(y^{w}, y^{\ell}, x\right) \sim \mathcal{D}_{\text {pref }}}\left[-\log \sigma\left(r\left(x, y^{w}\right)-r_{\phi}\left(x, y^{\ell}\right)\right)\right] \tag{2}
\end{equation*}
$$

With $\hat{r}$ in hand, Eq. (1) can be optimized using standard reinforcement learning algorithms [27; 29; 6].

### 2.2 Direct preference optimization

DPO is a simple approach for offline policy optimization that uses preferences to directly align the language model policy, without training an intermediate reward model. Specifically, DPO leverages the fact that the optimal solution to the KL-constrained objective in (1) takes the form [15]

$$
\begin{equation*}
\pi_{\theta^{*}}(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r^{*}(x, y)\right) \tag{3}
\end{equation*}
$$

where $Z(x)=\sum_{y} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r^{*}(x, y)\right)$ is the partition function. DPO reparameterizes the true reward function $r^{*}$ in terms of the optimal policy $\pi_{\theta^{*}}$ that it induces, i.e.,

$$
\begin{equation*}
r^{*}(x, y)=\beta \log \left(\frac{\pi_{\theta^{*}}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}\right)+\beta \log Z(x) \tag{4}
\end{equation*}
$$

Under the Bradley-Terry model, the likelihood that $y_{1} \succ y_{2}$ can then be written as

$$
\begin{equation*}
p^{*}\left(y_{1} \succ y_{2} \mid x\right)=\sigma\left(\beta \log \frac{\pi_{\theta^{*}}\left(y_{1}\right) \pi_{\mathrm{ref}}\left(y_{2}\right)}{\pi_{\theta^{*}}\left(y_{2}\right) \pi_{\mathrm{ref}}\left(y_{1}\right)}\right) \tag{5}
\end{equation*}
$$

where now $\pi_{\theta^{*}}$ can be directly estimated on $\mathcal{D}_{\text {pref }}$ following the objective in (2), in place of the intermediate reward model $\hat{r}$, i.e., $\pi_{\hat{\theta}}(y \mid x) \in \operatorname{argmin}_{\pi_{\theta}} \mathcal{L}_{\mathrm{dpo}}\left(\pi_{\theta} ; \mathcal{D}_{\text {pref }}\right)$ where

$$
\begin{equation*}
\mathcal{L}_{\mathrm{dpo}}\left(\pi_{\theta} ; \mathcal{D}_{\text {pref }}\right)=\mathbb{E}_{\left(y^{w}, y^{\ell}, x\right) \sim \mathcal{D}_{\mathrm{pref}}}\left[-\log \sigma\left(\beta \log \frac{\pi_{\theta^{*}}\left(y^{w}\right) \pi_{\mathrm{ref}}\left(y^{\ell}\right)}{\pi_{\theta^{*}}\left(y^{\ell}\right) \pi_{\mathrm{ref}}\left(y^{w}\right)}\right)\right] \tag{6}
\end{equation*}
$$

### 2.3 Pitfalls of direct preference optimization

As argued in [3], the Bradley-Terry assumption that DPO strongly relies on for maximum likelihood estimation is sensitive to the underlying preference data. Specifically, if we have any two responses $y_{1}$ and $y_{2}$ where $p^{*}\left(y_{1} \succ y_{2} \mid x\right)=1$, then the Bradley-Terry model dictates that $r^{*}\left(y_{1}\right)-r^{*}\left(y_{2}\right)=+\infty$, and therefore $\pi_{\theta^{*}}\left(y_{2} \mid x\right)=0$ for any finite KL-regularization strength $\beta$.

We can illustrate this phenomenon on a broader level with the following example.

Assumption 1. Suppose we are given a preference dataset of (context-free) pairs $\mathcal{D}_{\text {pref }}=$ $\left\{\left(y_{i}^{w}, y_{i}^{\ell}\right)\right\}_{i=1}^{n}$, the pairs $\left(y_{i}^{w}, y_{i}^{\ell}\right)$ are mutually disjoint in both the elements. Further suppose that we optimize the DPO objective on $\mathcal{D}_{\text {pref }}$ with a single parameter $\theta_{y}$ for each $y$.

Proposition 1. Under Assumption 1, for any $\left(y, y^{\prime}\right)$ such that $y=y_{i}^{w}$ and $y^{\prime}=y_{i}^{\ell}$ for some $i$, we have $\frac{\pi_{\theta^{*}}(y) \pi_{\mathrm{ref}}\left(y^{\prime}\right)}{\pi_{\theta^{*}}\left(y^{\prime}\right) \pi_{\mathrm{ref}}(y)} \rightarrow \infty$, for all global minimizers $\pi_{\theta^{*}}$ of the DPO objective in (6), for any $\beta>0$.

Corollary 1. Under Assumption 1, further assume that $0<\pi_{\mathrm{ref}}(y)<1$ for all $y$. Then $\pi_{\theta^{*}}$ is a global minimizer of the DPO objective in (6) iff $\pi_{\theta^{*}}\left(\mathcal{C}\left(y^{\ell}\right)^{c}\right) \rightarrow 1$ with $\pi_{\theta^{*}}\left(y_{i}^{w}\right)>0 \forall i \in[n]$, where $\mathcal{C}\left(y^{\ell}\right)^{c}$ is the complement of the set of all responses $y$ that appear as a dispreferred $y_{i}^{\ell}$ for any $i \in[n]$.

Additional analysis of the training dynamics of DPO is also provided in $\S 5$. A significant, and nonobvious, implication of Corollary 1 is that the set of global optima of the DPO loss also includes policies that can shift nearly all probability mass to responses that never even appear in the training setand even assign near zero probability to all of the training data responses that do in fact correspond to winning generations, $y^{w}$, a phenomenon that has been observed empirically [e.g., 20]. Stated differently, Corollary 1 implies that any $\theta^{*}$ merely satisfying $\pi_{\theta^{*}}\left(y_{i}^{\ell}\right)=0$ with $\pi_{\theta^{*}}\left(y_{i}^{w}\right)>0 \forall i \in[n]$ is a global minimizer of the DPO objective in this setting. Though simplistic, the scenario in Assumption 1 is closer to reality than might first be appreciated: in many practical situations we can almost always expect the finite-sample preference data to contain one (or at most a few) preference annotations per example $\left(x, y_{1}, y_{2}\right)$, while the policies $\pi_{\theta}$ can have billions of parameters $(\gg n)$. Of course, this issue can also be viewed as a classic instance of overfitting-with the additional caveat that as opposed to overpredicting responses within the training set, we might overfit to almost never producing anything like the "good" responses that do appear within the training set. Furthermore, without additional regularization (beyond $\beta$ ), we can expect this degeneration to easily happen in typical preference datasets.

## 3 Uncertainty-aware reward model distillation

As discussed in the previous section, a core issue in preference optimization is that the true preference distribution $p^{*}\left(y_{1} \succ y_{2} \mid x\right)$ is not known. Attempting to infer it from finite-sample preference data (that may further be biased or out-of-distribution with respect to the target domain) can then result in a failure to learn reasonable policies. In this section, we now propose an inherently regularized approach to direct preference optimization that uses uncertainty-aware reward model distillation.

### 3.1 Reward model distillation

Suppose for the moment that the reward function $r^{*}$ was in fact known, and did not have to be inferred from sampled preference data. Under this setting, we can then define an efficient offline optimization procedure that is similar in spirit to DPO, but no longer relies directly on a preference dataset. Concretely, given unlabeled samples $\left(x, y_{1}, y_{2}\right) \sim \rho$ (where the number of samples can be potentially unlimited), we can define a simple "distillation" loss, $\mathcal{L}_{\text {distill }}\left(r^{*}, \pi_{\theta}\right)$, as follows:

$$
\begin{equation*}
\mathcal{L}_{\text {distill }}\left(r^{*}, \pi_{\theta} ; \rho\right)=\mathbb{E}_{\rho\left(x, y_{1}, y_{2}\right)}\left[\left(r^{*}\left(x, y_{1}\right)-r^{*}\left(x, y_{2}\right)-\beta \log \frac{\pi_{\theta}\left(y_{1} \mid x\right) \pi_{\mathrm{ref}}\left(y_{2} \mid x\right)}{\pi_{\theta}\left(y_{2} \mid x\right) \pi_{\mathrm{ref}}\left(y_{1} \mid x\right)}\right)^{2}\right] \tag{7}
\end{equation*}
$$

Intuitively, the distillation loss seeks to exactly match differences in reward model scores across all generation pairs $\left(x, y_{1}, y_{2}\right)$. It is then easy to see that under the Bradley-Terry model, this is equivalent to matching the strength of the preference relationship, $y_{1} \succ y_{2}$. Furthermore, by only matching differences, we can still conveniently ignore the $\log$ partition term, $\log Z(x)$, in the implicit reward formulation for $\pi_{\theta}$ as shown in (4), as it is constant across different $y$ for any given $x$. Finally, similar to the motivation in DPO, we can show that minimizing $\mathcal{L}_{\text {distill }}\left(r^{*}, \pi_{\theta} ; \rho\right)$ indeed results in an optimally aligned policy $\pi_{\theta^{*}}$, as long as the data distribution $\rho$ has sufficient support.

Theorem 1. Let $\mathcal{Y}$ denote the set of all possible responses for any model $\pi_{\theta}$. Assume that $\operatorname{supp}\left(\pi_{\mathrm{ref}}(y \mid x)\right)=\mathcal{Y}$, i.e., the reference policy may generate any outcome with non-zero probability. Further, let $\operatorname{supp}\left(\rho\left(x, y_{1}, y_{2}\right)\right)=\operatorname{supp}(\mu(x)) \times \mathcal{Y} \times \mathcal{Y}$. Let $\pi_{\theta^{*}}(y \mid x) \in \operatorname{argmin}_{\pi_{\theta}} \mathcal{L}_{\text {distill }}\left(r^{*}, \pi_{\theta} ; \rho\right)$ be a minimizer over all possible policies, of the implicit reward distillation loss in (7), for which $r^{*}(x, y)$ is assumed to be deterministic, and finite everywhere. Then for any $\beta>0, \pi_{\theta^{*}}$ also maximizes the alignment objective in (1).

The above result holds for a broad class of data distributions $\rho\left(x, y_{1}, y_{2}\right)$, and makes no assumptions on $r^{*}$ (e.g., it is no longer necessary for it to be defined using a Bradley-Terry model). In fact, this result can also be seen as strict generalization of the IPO framework of [3] when taking $r^{*}(x, y) \triangleq$ $\mathbf{1}\left\{y=y_{w}\right\}$, if labeled pairs $\left(x, y_{w}, y_{l}\right)$ are provided instead of the unlabeled pairs $\left(x, y_{1}, y_{2}\right)$.

Of course, the true reward $r^{*}$ is usually not known in practice. Still, as in standard RLHF, we can go about constructing good proxies by using the preference data to identify plausible target reward models $r_{\text {tgt }}$-further guided by any amount of regularization and inductive bias that we desire. A natural choice is to first learn $r_{\text {tgt }}$ on the preference data $\mathcal{D}_{\text {pref }}$ using standard methods, and then reuse $\mathcal{D}_{\text {pref }}$ to distill $\pi_{\theta}$, which is similar to classical settings in teacher-based model distillation [14; 26]. Furthermore, as $r_{\text {tgt }}$ is a real-valued model, at a bare minimum it is guaranteed to induce a regularized Bradley-Terry preference distribution $p_{\text {tgt }}\left(y_{1} \succ y_{2} \mid x\right)>0, \forall x, y_{1}, y_{2} \in \mathcal{X} \times \mathcal{Y} \times \mathcal{Y}$, and thereby avoid some of the degeneracies identified in $\S 2.3$ for the maximum likelihood estimate under DPO.

### 3.2 Pessimistic reward model distillation

Choosing a single reward model $r_{\text {tgt }}$ for anchoring the LM policy can naturally still lead to degenerate behavior if $r_{\text {tgt }}$ is a poor approximation of the true $r^{*}$ that accurately reflects human preferences. However, we can easily extend our framework to handle uncertainty in the right target reward function by defining a confidence set of $k \geq 1$ plausible target reward models, $\mathcal{S}=\left\{r_{\mathrm{tgt}}^{1}, \ldots, r_{\mathrm{tgt}}^{k}\right\}$, and training $\pi_{\theta^{*}}(y \mid x)$ to maximize the following "pessimistic" form of the objective in (1):

$$
\begin{equation*}
\max _{\pi_{\theta}} \min _{r_{\mathrm{tgt}}^{i} \in \mathcal{S}} \mathbb{E}_{\mu(x)}[\underbrace{\mathbb{E}_{\pi_{\theta}(y \mid x)}\left[r_{\mathrm{tgt}}^{i}(x, y)\right]-\mathbb{E}_{\pi_{\mathrm{ref}}(y \mid x)}\left[r_{\mathrm{tgt}}^{i}(x, y)\right]}_{\text {advantage over baseline policy }}-\beta \mathbb{D}_{\mathrm{KL}}\left(\pi_{\theta}(\cdot \mid x) \| \pi_{\mathrm{ref}}(\cdot \mid x)\right)] \tag{8}
\end{equation*}
$$

In this pessimistic objective we are no longer optimizing $\pi_{\theta}$ for a single reward, but optimizing $\pi_{\theta}$ to produce generations that are scored favorably on average, even by the worst-case reward model in the set $\mathcal{S}$, relative to the generations of the baseline policy $\pi_{\text {ref. }}{ }^{1}$ When the set $\mathcal{S}=\left\{r^{*}\right\}$ consists of only the ground-truth reward, the objective (8) is equivalent to standard RLHF (1), up to a constant offset independent of $\theta$. More generally, whenever $\mathcal{S}$ includes a good proxy $\widetilde{r}$ for $r^{*}$, the pessimistic advantage evaluation ensures that the the policy $\pi_{\theta}^{*}$ that maximizes eq. (8) still has a large advantage over $\pi_{\text {ref }}$ under all $r \in \mathcal{S}$, including $\widetilde{r}$. This use of pessimism to handle uncertainty in the knowledge of the true reward is related to similar techniques in the offline RL literature $[16 ; 5]$.

![](https://cdn.mathpix.com/cropped/2024_06_04_074b0bd3362eaf0c4f73g-05.jpg?height=220&width=658&top_left_y=253&top_left_x=1075)

Figure 1: A toy illustration of Theorem 2, which states that the optimal $\pi_{\theta^{*}}$ for (8) is the policy in $\mathcal{P}_{\beta}(\mathcal{S})$ with the lowest forward-KL from $\pi_{\mathrm{SFT}}$. The set $\mathcal{P}_{\beta}(\mathcal{S})$ contains a (potentially infinite) set of policies $\pi_{1}, \pi_{2}, \ldots$ corresponding to target reward models. Here, $\pi_{\mathrm{SFT}}$ assigns equal mass to $y^{w}$ and $y^{\ell}, \pi_{\text {MLE }}$ is the MLE solution for the DPO objective, which puts all probability mass on $y^{w}$, and $\pi_{3}$ is the policy in $\mathcal{P}_{\beta}(\mathcal{S})$ with lowest forward-KL.

For the objective to be meaningful, the set $\mathcal{S}$ has to be chosen carefully. When $\mathcal{S}$ is small, it might not include any good proxy for $r^{*}$. Conversely, if $\mathcal{S}$ is too rich, it forces $\pi_{\theta^{*}}$ to be nearly identical to $\pi_{\text {ref }}$, since any deviations from $\pi_{\text {ref }}$ might be penalized by some reward model in $\mathcal{S}$. Consequently, we want to design $\mathcal{S}$ to be the smallest possible set which contains a reasonable approximation to $r^{*}$.

To optimize (8), it turns out that we can formulate it as an equivalent constrained offline optimization problem, that we will show to conveniently admit a similar loss form as (7).

Theorem 2 (Pessimistic distillation). Define the constrained minimizer

$$
\begin{equation*}
\pi_{\theta^{*}}(y \mid x) \in \underset{\pi_{\theta} \in \mathcal{P}_{\beta}(\mathcal{S})}{\operatorname{argmin}} \beta \mathbb{E}_{\mu(x)} \mathbb{D}_{\mathrm{KL}}\left(\pi_{\mathrm{ref}}(\cdot \mid x) \| \pi_{\theta}(\cdot \mid x)\right) \tag{9}
\end{equation*}
$$

where $\mathcal{P}_{\beta}(\mathcal{S})$ is the set of all possible policies with implicit reward models that are consistent with any target reward model $r_{\mathrm{tgt}}^{i} \in \mathcal{S}$, i.e., $\mathcal{P}_{\beta}(\mathcal{S}) \triangleq\left\{\pi_{\theta_{i}}\right\}_{i=1}^{|\mathcal{S}|}$ where $\pi_{\theta_{i}} \propto \pi_{\mathrm{ref}}(y \mid x) \exp \frac{1}{\beta} r_{\mathrm{tgt}}^{i}(x, y)$. Then for any $\beta>0, \pi_{\theta^{*}}$ also maximizes the pessimistic alignment objective in (8).

To unpack this result, Theorem 2 stipulates that the $\pi_{\theta}$ that maximizes the pessimistic objective in (8) is the policy in $\mathcal{P}_{\beta}(\mathcal{S})$ that is closest in forward KL-divergence to $\pi_{\text {ref }}$ (see Figure 1). ${ }^{2}$ In addition, this policy also maximizes the expected reward of one of the $r_{\mathrm{tgt}}^{i} \in \mathcal{S}$ (minus the additional weighted reverse KL-divergence penalty term). Intuitively, the forward KL-divergence term serves the role of biasing the model towards optimizing for reward models that are similar to the implicit reward that $\pi_{\text {ref }}$ already maximizes. Otherwise, there might exist a target reward model $r_{\mathrm{tgt}}^{i} \in \mathcal{S}$ for which the advantage of $\pi_{\theta}$ relative to $\pi_{\text {ref }}$ will be low, or even negative (a solution that we would like to avoid).

### 3.2.1 Optimization

The constraint in (9) can then be relaxed and approximately optimized by introducing an objective with a Lagrangian-style penalty with strength $\alpha>0$ on a form of distillation loss as (7), i.e.,

$$
\begin{equation*}
\min _{\pi_{\theta}} \beta \mathbb{E}_{\mu(x)} \mathbb{D}_{\mathrm{KL}}\left(\pi_{\mathrm{ref}}(y \mid x) \| \pi_{\theta}(y \mid x)\right)+\alpha \min _{r_{\mathrm{tgt}}^{i} \in \mathcal{S}} \mathcal{L}_{\mathrm{distill}}\left(r_{\mathrm{tgt}}^{i}, \pi_{\theta} ; \rho\right) \tag{10}
\end{equation*}
$$

where for convenience we divide by $\alpha$ and instead optimize ${ }^{3}$

$$
\begin{equation*}
\mathcal{L}_{\text {pdistill }}\left(\mathcal{S}, \pi_{\theta} ; \rho\right)=\min _{r_{\mathrm{tgt}}^{i} \in \mathcal{S}} \mathcal{L}_{\text {distill }}\left(r_{\text {tgt }}^{i}, \pi_{\theta} ; \rho\right)+\gamma \mathbb{E}_{\mu(x)} \mathbb{D}_{\mathrm{KL}}\left(\pi_{\mathrm{ref}}(\cdot \mid x) \| \pi_{\theta}(\cdot \mid x)\right) \tag{11}
\end{equation*}
$$

where $\gamma=\beta / \alpha$. In reality, minimizing (11) for $\gamma>0$ is equivalent to solving the constrained optimization problem in (9) with an implicitly larger set of possible reward models $\mathcal{S}_{\gamma} \supseteq \mathcal{S}$ indexed by $\gamma$. More specifically, $\mathcal{S}_{\gamma}$ also contains all reward models $\tilde{r}$ that are approximately consistent with the anchoring reward models $r_{\mathrm{tgt}}^{i}$ contained in $\mathcal{S}$, as the following result states.[^1]

Proposition 2 (Soft pessimistic distillation). Assume the same conditions as Theorem 1. Then for any $0<\gamma<\infty$, there exists $a \lambda \geq 0$ such that $\pi_{\theta^{*}}(y \mid x) \in \operatorname{argmin}_{\pi_{\theta}} \mathcal{L}_{\text {pdistill }}\left(\mathcal{S}, \pi_{\theta} ; \rho\right)$, where $\pi_{\theta^{*}}$ is a minimizer over all possible policies, is a solution to (9) for the effective reward model set

$$
\begin{equation*}
\mathcal{S}_{\gamma}=\bigcup_{r_{\mathrm{tgt}}^{i} \in \mathcal{S}}\left\{\tilde{r}: \mathbb{E}_{\rho\left(x, y_{1}, y_{2}\right)}\left[\left(r_{\mathrm{tgt}}^{i}\left(x, y_{1}\right)-r_{\mathrm{tgt}}^{i}\left(x, y_{2}\right)-\tilde{r}\left(x, y_{1}\right)+\tilde{r}\left(x, y_{2}\right)\right)^{2}\right] \leq \lambda\right\} \tag{12}
\end{equation*}
$$

As a result, optimizing (11) even when using the singleton $\mathcal{S}=\left\{r_{\text {tgt }}\right\}$ yields an implicitly pessimistic objective, in which the pessimism is over all reward models $\tilde{r}$ that are consistent up to $\lambda$ with $r_{\text {tgt }}$.

### 3.3 Pessimistic DPO

We can also observe that Proposition 2 can be leveraged to obtain an alternative, implicitly pessimistic, objective that uses DPO directly instead of distillation. Consider the following regularized DPO loss:

$$
\begin{equation*}
\mathcal{L}_{\text {pdpo }}\left(\pi_{\theta} ; \mathcal{D}_{\text {pref }}\right)=\mathcal{L}_{\text {dpo }}\left(\pi_{\theta} ; \mathcal{D}_{\text {pref }}\right)+\gamma \mathbb{E}_{\mu(x)} \mathbb{D}_{\mathrm{KL}}\left(\pi_{\text {ref }}(y \mid x) \| \pi_{\theta}(y \mid x)\right) \tag{13}
\end{equation*}
$$

Following a similar analysis as in Proposition 2, we can derive that this implicitly corresponds to maximizing the pessimistic objective in (8) for the reward model set

$$
\begin{equation*}
\mathcal{S}_{\gamma}=\left\{r_{\pi_{\theta}}: \mathcal{L}_{\mathrm{dpo}}\left(\pi_{\theta} ; \mathcal{D}_{\mathrm{pref}}\right) \leq \min _{\pi_{\theta}^{\prime}} \mathcal{L}_{\mathrm{dpo}}\left(\pi_{\theta}^{\prime} ; \mathcal{D}_{\mathrm{pref}}\right)+\lambda\right\} \tag{14}
\end{equation*}
$$

where $r_{\pi_{\theta}}(x, y) \triangleq \beta \log \pi_{\theta}(y \mid x) / \pi_{\text {ref }}(y \mid x)+\beta \log Z(x)$ is the implicit reward model defined by $\pi_{\theta} . \mathcal{S}_{\gamma}$ then corresponds to the set of reward models $r_{\pi_{\theta}}$ that are all approximate minimizers of the DPO loss. This not only includes the MLE, but also all other estimators that obtain nearly the same loss. In principle, this can be expected to help ameliorate some of the issues of §2.3: since driving the reward to $\pm \infty$ only marginally decreases the $\mathcal{L}_{\mathrm{dpo}}$ loss past a certain point, the set $\mathcal{S}$ will also include finite reward functions $\left|r_{\pi_{\theta}}(x, y)\right|<\infty$ for any $\gamma>0$. These rewards would then be preferred if they induce a policy with a smaller (forward) KL-divergence to $\pi_{\mathrm{ref}}$ than the degenerate, infinite rewards.

## 4 Experimental results

The main motivation for reward distillation and pessimism is to increase alignment robustness in challenging settings where it is difficult to learn good policies directly from the preference data. To demonstrate the effectiveness of our approach, we run experiments on the popular TL;DR summarization task $[29 ; 32]$, in which we simulate a scenario where the preference data has a spurious correlation between the length of a summary and whether or not it is preferred. ${ }^{4}$

### 4.1 Experimental setup

We first train an "oracle" reward model on the TL;DR preference data training set [29] and relabel all preference pairs with this oracle. This enables us to use the oracle reward model for evaluation, without worrying about the gap to true human preferences. After relabeling, longer responses (where longer is defined as $y_{1}$ having at least $10 \%$ more tokens than $y_{2}$ ) are preferred in $61 \%$ of the examples.

To test the effect of a spurious correlation on preference-based policy optimization, we select as a training set $30 \mathrm{~K}$ examples from the relabeled data such that the longer output is preferred in $\rho$ fraction of examples, with $\rho \in\{0.2,0.3,0.4,0.5,0.6,0.7,0.8\}$. Each such training set is denoted $\mathcal{D}_{\rho}$. At each $\mathcal{D}_{\rho}$, we compare our approach to DPO [23] and IPO [3], which are currently the most commonly used offline alignment methods. We test the following variants of distillation and pessimism:

- Distilled DPO (d-DPO): Trains a reward model $r_{\rho}$ on $\mathcal{D}_{\rho}$, and then optimizes $\mathcal{L}_{\text {distill }}\left(r_{\rho}, \pi_{\theta} ; \rho\right)$.
- Pessimistic DPO (p-DPO): A pessimistic version of DPO as described in $\S 3.3$, trained on $\mathcal{D}_{\rho}$.
- Pessimistic Distilled DPO (pd-DPO): Combines the above two by training a reward model $r_{\rho}$ on $\mathcal{D}_{\rho}$ and optimizing the pessimistic distillation objective (Eq. (11)) with confidence set $\mathcal{S}=\left\{r_{\text {tgt }}\right\}$.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_074b0bd3362eaf0c4f73g-07.jpg?height=480&width=1230&top_left_y=253&top_left_x=445)

Figure 2: Main results, showing the advantage in oracle reward compared to the initial finetuned policy. Errorbars correspond to bootstrap $95 \%$ confidence intervals for finite sample variance. Ensemble DPO (e-DPO) is significantly better than DPO and IPO in the challenging setup where shorter responses are preferred $(\rho \leq 0.5)$, and is generally the best-performing method overall in this regime. Distilled DPO (d-DPO) performs best when longer responses are preferred $(\rho>0.6$ ).

- Pessimistic Ensemble DPO (e-DPO): To create ensembles of reward models, we subsample from each $\mathcal{D}_{\rho}$ five preference datasets, $\mathcal{D}_{\rho, b}$, at $b \in \mathcal{B}=\{0.2,0.4,0.5,0.6,0.8\}$, such that the fraction of pairs where the longer response is preferred is $b$, and train reward models $r_{\rho, b}$ on those subsets. Consequently, sensitivity to length should vary across ensemble members. We then apply the same procedure as pd-DPO above, with a confidence set $\mathcal{S}_{\rho}=\left\{r_{\rho, b}\right\}_{b=1}^{\mathcal{B}}$.

All reward models and policies are initialized from Palm-2-XS [2]. Policies also go through a supervised finetuning step on human-written summaries from the original TL;DR training set [32] prior to alignment, and we term this policy $\pi_{\mathrm{SFT}}$. We evaluate performance by sampling summaries for test set prompts, evaluating the average reward according to the oracle reward model, and computing the advantage in average reward compared to $\pi_{\mathrm{SFT}}$ (before alignment). We train policies for $10^{4}$ steps with batch size 16 and learning rate $10^{-6}$, and reward models for $3 k$ steps with batch size 64 and learning rate $4 \times 10^{-6}$. We use the validation set for model selection during policy training and to choose the following hyperparameters. For all DPO variants, we sweep over $\beta \in\{.01, .1,1,3,10,30,100\}$. For IPO, we sweep over $\tau \in\{0.01,0.1,1,3,5,10,25\}$. For all pessimistic methods we anneal $\gamma=\alpha / \beta$ from $10^{-4}$ to $10^{-2}$ linearly during the $10 \mathrm{k}$ training steps.

### 4.2 Results

We present the results of our experiment in Figure 2. As can be seen in the plot, the more challenging setting is when $\rho<0.5$, which corresponds to a sample of preference annotations in which shorter outputs are generally preferred. This distribution shift is more difficult because as mentioned the oracle reward model (trained on human annotations) has a bias in favor of longer outputs [28]. Nevertheless we get sizable improvements compared to the reference policy $\pi_{\mathrm{SFT}}$ for all length bias values.

All approaches that invoke distillation (d-DPO, e-DPO, dp-DPO) outperform IPO and DPO ( $p<.01$ by a Wald test) for $\rho \leq 0.5$, where shorter responses are preferred. Pessimistic ensemble DPO (e-DPO) performs particularly well in these settings, generally outperforming all methods that use a single reward model. When longer responses are preferred ( $\rho>0.6$ ), single reward distillation (d-DPO) leads to the highest performance, significantly outperforming both DPO and IPO ( $p<.01$ by a Wald test). Interestingly, p-DPO does not provide empirical benefits relative to the distillation based methods, indicating that the distillation loss itself is quite important. For the effect of hyper-parameter selection, see Figure D.1. In DPO-based methods, the optimal value of $\beta$ is inversely correlated with the bias; in IPO the same holds for the $\tau$ hyperparameter.

To better understand the utility of reward ensembles in e-DPO, in particular when $\rho<0.5$, we examine the role of each reward model in the ensemble across different biases. Specifically, given the final e-DPO policy per length bias, for each example we identify the reward model $r_{\rho, b}$ that best matches the implicit reward of this policy, i.e., for which reward model is $\mathcal{L}_{\text {distill }}$ minimized on that example (see Eq. (7) and (11)). We find that when the policy is trained on data where shorter preference are preferred ( $\rho<.5)$, the reward model that best matches the policy often has the opposite
bias ( $b$ is high), and vice versa. Thus, the success of e-DPO may be explained by its ability to distill from reward models that do not suffer from the bias in the policy training data, which is particularly helpful when $\rho \leq .5$ as this bias is also not shared by the oracle RM. We provide the full distribution over reward models for all $\rho$ and $\beta$ in App. C. Overall, these results demonstrate the efficacy of training a policy by distilling from a reward model in the presence of distribution shifts, and that a careful design of an ensemble to mitigate spurious correlations can lead to further performance gains. ${ }^{5}$

## 5 Theoretical analysis

This section characterizes problems with the DPO objective and solutions offered by pessimistic DPO and distillation, focusing on the simplified scenario in which we optimize with respect to a single preference pairs $\left(y^{w}, y^{\ell}\right)$. Once again, all proofs are deferred to Appendix A.

In its Lagrangian formulation, pessimistic DPO adds a forward KL term to the DPO objective (§3.3). For the sake of analysis, we assume that the preference annotations are sampled from the reference distribution, $\mu(x) \times \pi_{\mathrm{ref}}(y \mid x) \times \pi_{\mathrm{ref}}(y \mid x)$. Then a finite-sample approximation of the forward $\mathrm{KL}$ term is $\hat{\Omega}(\Theta):=\sum_{\left(y^{w}, y^{\ell}\right) \in \mathcal{D}_{\text {Pref }}}-\left(\log \pi_{\theta}\left(y^{\ell}\right)+\log \pi_{\theta}\left(y^{w}\right)\right)$. By applying this finite-sample approximation, $p-D P O$ has a finite optimum, unlike DPO, as shown in Proposition 1. Note that this analysis is limited in two ways: (1) as mentioned, we compute the KL term over the completions in the preference data; (2) we directly optimize the probability ratios $\psi_{w}=\pi_{\theta}\left(y^{w}\right) / \pi_{\text {ref }}\left(y^{w}\right)$ and $\psi_{\ell}=\pi_{\theta}\left(y^{\ell}\right) / \pi_{\text {ref }}\left(y^{\ell}\right)$, rather than optimizing them jointly through the parameters. For sufficiently expressive $\pi_{\theta}$, however, this approximation captures the behavior of the two algorithms reasonably well.

Proposition 3. Let $\hat{\mathcal{L}}_{\text {pdpo }}$ represent a finite-sample approximation to $\mathcal{L}_{\mathrm{pdpo}}$ with the empirical forward $K L$ term $\hat{\Omega}(\Theta)$. For a fixed $\hat{\pi}_{\theta}\left(y_{i}^{w}\right)$ and $\alpha>1$, the $\operatorname{argmin}_{\pi_{\theta}\left(y^{\ell}\right)} \hat{\mathcal{L}}_{\text {pdpo }}$ is $\min \left(1-\hat{\pi}_{\theta}\left(y_{i}^{w}\right), \hat{\pi}_{\theta}\left(y_{i}^{\ell}\right)\right)$, with $\log \hat{\pi}_{\theta}\left(y_{i}^{\ell}\right)=-\frac{1}{\beta} \log (\alpha-1)+\log \hat{\pi}_{\theta}\left(y_{i}^{w}\right)+\log \frac{\pi_{\mathrm{ref}}\left(y_{i}^{\ell}\right)}{\pi_{\mathrm{ref}}\left(y_{i}^{w}\right)}$.

The optimum in Proposition 3 corresponds to $\log \psi_{w} / \psi_{\ell}=\beta^{-1} \log (\alpha-1)$. Recall that IPO seeks to assign a constant value to this ratio by minimizing $\left(\log \frac{\psi_{w}}{\psi_{\ell}}-\tau^{-1}\right)^{2}$; the (unconstrained) optima are identical for $\tau^{-1}:=\beta^{-1} \log (\alpha-1)$, but the loss surfaces are different (see Appendix B). DPO sets $\pi_{\theta}\left(y_{i}^{\ell}\right) \rightarrow 0$, as shown in Corollary 1 ; this is due not only to competition from $\pi_{\theta}\left(y_{i}^{w}\right)$ but from DPO penalizing positive probability on $y_{i}^{\ell}$. Analysis of the distilled loss gives a similar result:

Proposition 4. For any fixed $\hat{\pi}_{\theta}\left(y_{i}^{w}\right)$ and $\beta>0$, the argmin of the distilled DPO objective (eq. (7)) is $\min \left(1-\hat{\pi}_{\theta}\left(y_{i}^{w}\right), \hat{\pi}_{\theta}\left(y_{i}^{\ell}\right)\right.$, with $\log \hat{\pi}_{\theta}\left(y_{i}^{\ell}\right)=\frac{1}{\beta}\left(r_{t}\left(x, y_{i}^{\ell}\right)-r_{t}\left(x, y_{i}^{w}\right)\right)+\log \hat{\pi}_{\theta}\left(y_{i}^{w}\right)+\log \frac{\pi_{\mathrm{ref}}\left(y_{i}^{\ell}\right)}{\pi_{\mathrm{ref}}\left(y_{i}^{w}\right)}$.

While the setting is simplistic, the results are comforting: here the additional regularization effects of both distillation and pessimism (in the case of p-DPO) clearly help to avoid degenerate optima.

Why DPO can drive $\pi\left(y^{w}\right)$ to zero. In $\S 2.3$ we pointed out a peculiarity of the DPO global optima: in certain cases, it can include policies where $\pi\left(y^{w}\right)$ may be nearly 0 for all $y^{w}$ in the training set. This undesirable behavior has also been observed in practice $[20 ; 22 ; 30]$. For intuition on why this may happen, consider the simplified case where the policy is a bag-of-words model, $\pi_{\theta}(y) \propto \exp (c(y) \cdot \theta)$ for $c(y)$ representing a vector of counts in $y$ and $\theta_{i}$ representing the unnormalized log-probability of token $i$. Then we can formally show that DPO optimization monotonically decreases an upper bound on the probability of the preferred completion, $\tilde{\pi}_{\theta^{(t-1)}}\left(y^{w}\right) \geq \tilde{\pi}_{\theta^{(t)}}\left(y^{w}\right) \geq \pi_{\theta^{(t)}}\left(y^{w}\right)$.

Proposition 5. Let $y^{w}, y^{\ell} \in \mathcal{V}^{n}$ be preferred vs. dispreferred outputs of length $n$, respectively, with $\pi_{\mathrm{ref}}\left(y^{w}\right), \pi_{\mathrm{ref}}\left(y^{\ell}\right)>0$ and corresponding count vectors $c\left(y^{w}\right), c\left(y^{\ell}\right)$. Let $\log \pi_{\theta}(y)=c(y)$. $\theta-n Z(\theta)$ for $Z(\theta)=\log \sum_{i}^{\mathcal{V}} e^{\theta_{i}}$, with upper bound $\log \tilde{\pi}_{\theta}(y)=c(y) \cdot \theta-n \max _{j} \theta_{j}$. Let $\theta^{(t)}$ represent the parameters of $\pi$ after $t$ steps of gradient descent on $\mathcal{L}_{\mathrm{dpo}}\left(\left\{y^{\ell}, y^{w}, x\right\}\right)$, with $\theta^{(0)}=0$. Then $\pi_{\theta^{(t)}}\left(y^{w}\right) \leq \tilde{\pi}_{\theta^{(t)}}\left(y^{w}\right) \leq \tilde{\pi}_{\theta^{(t-1)}}\left(y^{w}\right)$ for all $t$.

Where does the probability mass go? If $\pi_{\theta(t)}\left(y^{w}\right)$ decreases in $t$, what other strings become more probable? In the following proposition, we show that under the bag-of-words model, DPO[^3]optimization moves probability mass away from $y^{w}$ to sequences that contain only the tokens that maximize the difference between $y^{w}$ and $y^{\ell}$. This is a concrete example of the type of undesirable optima described in $\S 2.3$, now shown here to be realizable.

Proposition 6. Let $y^{w}$ and $y^{\ell}$ be preferred / dispreferred outputs of length $n$. Let $\Delta=c\left(y^{w}\right)-c\left(y^{\ell}\right)$ be the difference in unigram counts. Let $\hat{y}=[i, i, \ldots, i]$, for $i \in \arg \max \Delta$, with $\|c(\hat{y})\|_{1}=n$. Then $\pi_{\theta^{(t)}}\left(y^{w}\right)-\pi_{\theta^{(t)}}(\hat{y})=\tau(t) k$ for some $k \leq 0$ and some non-decreasing $\tau: \mathbb{Z}_{+} \rightarrow \mathbb{R}_{+}$.

We have $k=0$ when $c\left(y^{w}\right)=c(\hat{y})$, and $k \ll 0$ when $\left\|c\left(y^{w}\right)\right\|_{2} \ll\|c(\hat{y})\|_{2}=n$ (dense $\left.c\left(y^{w}\right)\right)$ and $\|\Delta\|_{2}=\|\Delta\|_{\infty}$ (sparse $\Delta$ ). This implies that when $y^{w}$ and $y^{\ell}$ are similar, $\pi_{\theta}\left(y^{w}\right)$ will degrade more rapidly. Early stopping will therefore tradeoff between reaching the degenerate solution on such cases, and underfitting other cases in which $y^{w}$ and $y^{\ell}$ are more distinct.

## 6 Related work

Recent work in offline alignment has focused on DPO [23] as a simpler alternative for aligning language models from preference data. Subsequent work has identified issues with DPO, including weak regularization [3] and a tendency to decrease the probability of winning generations during training [20]. Other methods have explored various avenues for improvement. These include analyzing the impact of noise on DPO alignment [11], proposing to update the reference policy during training [12], and suggesting a variant of IPO with a per-context margin [1]. Additional research has focused on token-level alignment methods [38; 22] and on developing a unified view of various offline alignment methods [31]. This work builds upon several these findings, and provides further analysis, as well as a solution based on pessimism and reward distillation.

While offline alignment methods are popular, recent evidence suggests that online alignment methods such as RLHF [6; 29], may lead to more favorable outcomes [13; 30; 8; 34]. Notably, Zhu et al. [41] proposed iterative data smoothing, which uses a trained model to softly label data during RLHF. Whether online or offline, however, policies are still succeptible to overfitting to certain degenerate phenomena. To this end, reward ensembles have been widely investigated recently as a mechanism for tackling reward hacking in RLHF $[9 ; 7 ; 39 ; 25]$, and in the context of multi-objective optimization [19; 24]. We use an ensemble of rewards to represent the uncertainty with respect to reward models that are suitable given preference data. Moskovitz et al. [19] focus on "composite" rewards, with the goal of achieving high task reward while ensuring that every individual component is above some threshold-also by applying a Lagrangian relaxation. In this work, we also consider multiple reward models, but we only focus on cases where there is no known, obvious reward decomposition.

Finally, the question of using a small amount of offline data to learn high-quality policies, instead of online access to reward feedback, has been widely studied in the offline reinforcement learning (RL) literature. The predominant approach here is to use pessimism, that is, to learn a policy with the highest reward under all plausible environment models consistent with the data, with an extensive theoretical $[18 ; 37 ; 33]$ and empirical $[16 ; 5 ; 36]$ body of supporting work. The key insight in this literature is that without pessimism, the RL algorithm learns undesirable behaviors which are not explicitly ruled out in the training data, and pessimism provides a robust way of preventing such undesirable extrapolations, while still preserving generalization within the support of the data.

## 7 Conclusion

LM alignment is crucial for deploying safe and helpful assistants, but is difficult due to lack of access to perfect preference oracles. We presented a thorough theoretical analysis of some of the degeneracies that DPO is susceptible to when learning from sampled human preference data. Furthermore, our findings suggest that explicit reward modeling remains a powerful vehicle for introducing regularization into post-training. By distilling the reward assigned by a single, explicit reward model-or a family of explicit reward models-directly into the implicit reward maximized by our policies using offline data, we demonstrated that we can achieve improved robustness to variations in preference dataset quality, while maintaining the simplicity of the DPO framework.

Limitations. The empirical results in the paper are based on one dataset and form of distribution shift. For deeper understanding of pessimism and ensembling, additional settings should be explored. The theoretical aspects of the paper are sometimes based on restrictive assumptions and simplifications. Nonetheless, they provide potential explanations for phenomena observed in real-world settings.

Broader impact. We introduce new ideas to the active field of research on preference-based posttraining, which we hope will help facilitate the alignment of large models, and improve understanding of current approaches-ultimately supporting the development of capable and reliable AI systems.

Acknowledgements. We thank Alexander D'Amour and Chris Dyer for helpful comments and feedback on the manuscript. This research also benefited from discussions with Victor Veitch, Mandar Joshi, Kenton Lee, Kristina Toutanova, David Gaddy, Dheeru Dua, Yuan Zhang, Tianze Shi, and Anastasios Angelopoulos.

## References

[1] Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. arXiv preprint arXiv:2402.10571, 2024.

[2] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.

[3] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 4447-4455. PMLR, 2024.

[4] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952. ISSN 00063444. URL http://www.jstor.org/stable/2334029.

[5] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning, pages 3852-3878. PMLR, 2022.

[6] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

[7] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.

[8] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024.

[9] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? Reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023.

[10] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1607-1616. PMLR, 10-15 Jul 2018. URL https://proceedings . mlr.press/v80/furlanello18a.html.

[11] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment performance of generative language models. arXiv preprint arXiv:2404.09824, 2024.

[12] Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. Learn your reference model for real good alignment. arXiv preprint arXiv:2404.09656, 2024.

[13] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024.

[14] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv . org/abs/1503.02531.

[15] Tomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed as bayesian inference. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1083-1091, 2022.

[16] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33: $1179-1191,2020$.

[17] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

[18] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy reinforcement learning without great exploration. Advances in neural information processing systems, 33:1264-1274, 2020.

[19] Ted Moskovitz, Aaditya K Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D Dragan, and Stephen McAleer. Confronting reward model overoptimization with constrained rlhf. arXiv preprint arXiv:2310.04373, 2023.

[20] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024.

[21] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.

[22] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $q^{*}$ : Your language model is secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.

[23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

[24] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36, 2024.

[25] Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. WARM: On the benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024.

[26] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In In Proceedings of ICLR, 2015.

[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[28] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.

[29] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.

[30] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024.

[31] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024.

[32] Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to learn automatic summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu, editors, Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL https://aclanthology.org/W17-4508.

[33] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellmanconsistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683-6694, 2021.

[34] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is DPO superior to PPO for LLM alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024.

[35] Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L. Yuille. Training deep neural networks in generations: a more tolerant teacher educates better students. In Proceedings of the ThirtyThird AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'19/IAAI'19/EAAI'19. AAAI Press, 2019. ISBN 978-1-57735-8091. doi: 10.1609/aaai.v33i01.33015628. URL https://doi.org/10.1609/aaai.v33i01. 33015628 .

[36] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:28954-28967, 2021.

[37] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. Advances in neural information processing systems, $34: 13626-13640,2021$.

[38] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Tokenlevel direct preference optimization. arXiv preprint arXiv:2404.11999, 2024.

[39] Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, and Huaimin Wang. Uncertainty-penalized reinforcement learning from human feedback with diverse reward lora ensembles. arXiv preprint arXiv:2401.00243, 2023.

[40] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023 .

[41] Banghua Zhu, Michael I Jordan, and Jiantao Jiao. Iterative data smoothing: Mitigating reward overfitting and overoptimization in rlhf. arXiv preprint arXiv:2401.16335, 2024.
