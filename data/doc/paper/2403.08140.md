# BAGEL: Bootstrapping Agents by Guiding Exploration with Language 

Shikhar Murty $^{\dagger \star}$ Christopher D. Manning ${ }^{\dagger}$ Peter Shaw ${ }^{\ddagger}$ Mandar Joshi ${ }^{\ddagger}$ Kenton Lee ${ }^{\ddagger}$


#### Abstract

Following natural language instructions by executing actions in digital environments (e.g. webbrowsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these roundtrips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over retrieved demonstrations, and find improvements of over 2-13\% absolute on ToolQA and MiniWob++, with up to $13 \times$ reduction in execution failures.


## 1. Introduction

In recent years, large language models (LLMs) have shown strong performance on a broad range of language understanding tasks, making them powerful tools for controlling policies in digital environments such as web browsers Yao et al. 2022, Kim et al., 2023). Such grounded language understanding tasks are fundamentally challenging for LMs in environments with ambiguous dynamics. For instance, even inputting a date into a text box could require either simply typing or a complex interaction using a drop-down date picker. An LM cannot know this a-priori without in-depth knowledge about the website.

One common way to provide such knowledge to LM agents[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_0d0a139940b839d4482dg-01.jpg?height=658&width=707&top_left_y=625&top_left_x=1121)

Figure 1. (Top) Given a seed set of explored trajectories, BAGEL constructs synthetic demonstrations via an iterative round-trip procedure between two $\mathrm{LM}$ components: a zero-shot $\mathrm{LM}$ agent that generates trajectories and an LM labeler that generates instructions for these trajectories. (Bottom) Given an instruction at test time, we retrieve synthetic demonstrations with similar instructions, to use as in-context exemplars to adapt the base agent.

is via expert demonstrations that provide information about mapping instructions to action sequences, recovering from errors, and reasoning traces (Yao et al., 2022; Sun et al., 2023, Kim et al. 2023; Sodhi et al. 2023). Of course, collecting human demonstrations for every new environment is laborious and requires knowing possible user instructions a priori. Moreover, as agents scale to complex tasks with hundreds of actions, human supervision will become increasingly infeasible to obtain. Instead of relying on human demonstrations for training LM agents, could we instead use exploration and environment feedback to automatically collect a large number of synthetic demonstrations?

Prior work has shown the effectiveness of collecting synthetic demonstrations by retroactively labeling trajectories from embodied agents (Sumers et al. 2023). In this scenario, the environments dynamics are assumed to be well understood by the agent; the synthetic demonstrations only serve to connect agent behavior with human language. However, we observe the opposite challenge with digital agents in our
setting-grounding instructions is relatively easy due to the highly textual environment, but zero-shot digital agents typically are not exposed to any environment dynamics before they are directly used to follow instructions.

Our method, termed BAGEL (Bootstrapping Agents by Guiding Exploration with Language), uses an iterative procedure to relabel a seed set of trajectories obtained from unconditioned exploration (Figure 1). Intuitively, BAGEL operates by progressively shifting the distribution of trajectories towards those that can be well-described via natural language, using two noisy LM components: an LM labeler takes a trajectory and relabels it with a (potentially unnatural) instruction, and a zero-shot LM policy maps the instruction back into a refined trajectory (Figure 2. By performing these round trips iteratively, BAGEL converts trajectories from random exploration into meaningful trajectories that are executable, without requiring a trained base agent or significant information about possible instructions. While both the re-labeling and instruction-following processes are imperfect, round-trips between these components work in harmony to reduce any noise. Once an instruction, trajectory pair reaches a threshold score under a demonstration filter (another prompted LM), the generated synthetic demonstration is added into a buffer. BAGEL demonstrations can be used for both in-context learning or finetuning, and serve as a drop-in replacement for expert demonstrations. Here, we follow the former strategy along with a simple retrieval augmented generation procedure-given a user instruction at test time, we retrieve the most relevant demonstrations based on instruction embeddings, and feed that into the agent's prompt to serve as in-context exemplars.

We experiment with BAGEL on two domains, by using a prompted LM (similar to ReAct, Yao et al. 2022) as our base policy and find significant improvements with no human supervision. In MiniWoB++ (Shi et al., 2017, Liu et al. 2018), an agent follows instructions on diverse webinterfaces ranging from booking flights to replying to emails, given an HTML state, by issuing a sequence of mouse and keyboard operations to interact with DOM objects. Using BAGEL for test-time adaptation, we find an improvement of over $13 \%$ compared to the base LM policy. Next, we evaluate on ToolQA (Zhuang et al., 2023), a collection of question answering tasks over 8 domains, where answering each question requires chaining together multiple tools such as SQL, text retrievers, graph tools, python interpreters and calculators. Here, we find an improvement of $2 \%$ over the base LM policy. Further analysis reveals the various positive effects of conditioning on our synthetic demonstration beyond improved accuracy, including up to $13 \times$ reduction in execution failures due to better understanding of environment dynamics. By carefully using LM priors to shape random exploration, our method serves as a tool for automated discovery of use cases in complex environments.

## 2. Background

Given a natural language instruction $g$, our agent interacts with the environment by taking a sequence of actions $\left\{a_{1}, a_{2}, \ldots, a_{T}\right\}$, where each $a_{t}$ is issued in response to an environment observation $o_{t}$. The entire interaction with the environment is captured as a trajectory $\tau=\left\{o_{1}, a_{1}, o_{2}, \ldots, o_{T}, a_{T}, o_{T+1}\right\}$.

We define an agent as a language conditioned policy $\pi\left(a_{t} \mid\right.$ $\left.\tau_{<t}, g\right)$ where $\tau_{<t}=\left\{o_{1}, a_{1}, o_{2}, \ldots, o_{t}\right\}$ refers to the trajectory until time-step $t$. Such policies are typically trained via imitation learning and optional RL finetuning, where a large set of expert curated instruction-trajectory pairs are required for imitation learning, and a suitably shaped reward signal is needed for RL finetuning (Branavan et al. 2009; Chaplot et al., 2018; Misra et al., 2017). For our setup, both observations and actions can be expressed as natural language strings. The agent policy $\pi$ can then be cast into an autoregressive LM that assigns probabilities to action strings given string descriptions of the previous actions and observations. Thus, recent work focuses on directly using LLMs as policies, by using prompts along with in-context human demonstrations (Yao et al., 2022; Shinn et al., 2023, Sun et al., 2023, Kim et al. 2023, among others).

Executing Action Strings. Similar to prior work that uses LMs to generate action strings (Huang et al., 2022, Logeswaran et al. 2022), we assume access to an environment specific low-level controller that maps action strings to a low-level command (e.g. a web-driver action or an API call), which can be directly executed to change the environment.

## 3. BAGEL

BAGEL generates synthetic demonstrations via exploration, as illustrated in Figure 2. First, we describe the various model components in $\$ 3.1$, and then describe the overall procedure in $\$ 3.2$.

### 3.1. Model Components

In order to generate synthetic demonstrations, we model different aspects of the joint distribution over instructions and trajectories. Every component is implemented by the same underlying LM, but with different prompts. Every component is also implicitly dependent on a given environment, although this is omitted in the notation for simplicity. All prompts used can be found in Appendix B.

Exploration Policy. The exploration policy, $\pi_{\text {explore }}\left(a_{t} \mid\right.$ $\tau_{<t}$, selects an action without conditioning on any instruction. The prompt used is similar to that of ReAct Yao et al., 2022). We can sample from the resulting distribution over trajectories, $p_{\text {explore }}(\tau)$, by sampling actions from $\pi_{\text {explore }}$

![](https://cdn.mathpix.com/cropped/2024_06_04_0d0a139940b839d4482dg-03.jpg?height=881&width=1396&top_left_y=215&top_left_x=321)

Figure 2. BAGEL generates synthetic demonstrations by exploring the environment. Shown here is an example from the MiniWob++ choose-date task. First, we generate an initial trajectory by sampling actions without conditioning on any natural language instruction. Then, we alternate between generating an instruction given a trajectory, and generating a trajectory given an instruction. The process aims to converge towards a trajectory that accurately satisfies a natural language instruction, and aims to recover from errors in labeling or instruction following from earlier rounds (see example). Once an instruction and trajectory pair satisfies a filtering criteria, it is added to the set of synthetic demonstrations. Alternatively, BAGEL can be initialized by first sampling an instruction, as described in $\$ 3.2$

until the episode completes or a "finish" action is generated. We can increase the entropy of $\pi_{\text {explore }}$ with a configurable temperature parameter.

Trajectory Labeler. The trajectory labeler, $p_{\text {label }}(g \mid \tau)$, is prompted to generate an instruction, $g$, that corresponds to a given trajectory, $\tau$.

Instruction Following Policy. Unlike the exploration policy, the instruction following policy, $\pi_{\text {agent }}\left(a_{t} \mid \tau_{<t}, g\right)$, selects actions conditioned on an instruction, $g$. We sample from the resulting distribution over trajectories, $p_{\text {agent }}(\tau \mid g)$, by choosing actions according to $\pi_{\text {agent }}$ until the episode completes or a "finish" action is generated. This component is also implemented using a ReAct based prompt.

Demonstration Filter. Given a synthetic demonstration $(g, \tau)$, the demonstration filter makes a binary judgement $s(g, \tau) \in\{0,1\}$, based on how well $\tau$ corresponds to the instruction $g$.

Instruction Generator Finally, as an alternative to the exploration policy (see $\$ 3.2$ ) we can instead use an instructor generator to initialize exploration. This model defines a distribution over instructions, $p_{\text {instruct }}(g)$, based on a prompt that elicits plausible instructions based on the initial observation from the environment, and the action space.

### 3.2. Generating Demonstrations

Initial Exploration We consider and compare two different variations of BAGEL: trajectory-first and instructionfirst exploration. For trajectory-first exploration, we first sample a trajectory $\tau^{0} \sim p_{\text {explore }}(\cdot)$ with the exploration policy. For instruction-first exploration, we first sample an instruction $g^{0} \sim p_{\text {instruct }}(\cdot)$ with the instruction generator.

Iterative Refinement Trajectories sampled from $p_{\text {explore }}$ may not correspond to any reasonable instruction, and, similarly, there may be no feasible trajectory that satisfies instructions sampled from $p_{\text {instruct }}$. Our iterative re-labeling procedure aims to find an instruction and trajectory pair where the trajectory satisfies the instruction, without sacrificing the diversity of the initial exploration. The process alternates between sampling instructions and trajectories:

$$
\begin{align*}
g^{t} & \sim p_{\text {label }}\left(\cdot \mid \tau^{t}\right)  \tag{1}\\
\tau^{t+1} & \sim p_{\text {agent }}\left(\cdot \mid g^{t}\right) \tag{2}
\end{align*}
$$

We perform these iterative updates until we find a pair where
$s\left(g^{t}, \tau^{t}\right)=1$ or a maximum number of steps is reached. If we are successful, the demonstration $\left(g^{t}, \tau^{t}\right)$ is added to the set of synthetic demonstrations, $\mathcal{M}$. The overall procedure is repeated to collect multiple demonstrations.

### 3.3. Discussion

Guiding Trajectory Distribution with LM Components. To better understand how the LM labeler and policy shape the distribution of trajectories, we consider how this distribution evolves over the course of multiple iterations. Let $p_{k}(\tau)$ be the distribution over trajectories and $p_{k}(g)$ be the distribution over instructions, after $k$ iterations. For $k>0$ :

$$
\begin{align*}
p_{k}(\tau) & =\sum_{g^{\prime}} p_{\text {agent }}\left(\tau \mid g^{\prime}\right) \cdot p_{k-1}\left(g^{\prime}\right)  \tag{3}\\
p_{k-1}\left(g^{\prime}\right) & =\sum_{\tau^{\prime}} p_{\text {label }}\left(\tau^{\prime} \mid g^{\prime}\right) \cdot p_{k-1}\left(\tau^{\prime}\right) \tag{4}
\end{align*}
$$

Combining these, we obtain

$$
\begin{equation*}
p_{k}(\tau)=\sum_{\tau^{\prime}, g^{\prime}} p_{k-1}\left(\tau^{\prime}\right) \cdot \underbrace{p_{\text {label }}\left(g^{\prime} \mid \tau^{\prime}\right) \cdot p_{\text {agent }}\left(\tau \mid g^{\prime}\right)}_{\text {environment and LM constraints }} \tag{5}
\end{equation*}
$$

Thus, we shape the distribution of trajectories from the previous marginal $p_{k-1}$ based on the criteria that they can be assigned a concrete string $g^{\prime}$, and are executable in the environment. These soft constraints work together to ensure that (1) trajectories can be described in terms of some feasible instruction in the environment, and (2) the trajectories themselves correspond to valid environment dynamics.

Connection to Hindsight Experience Replay. Hindsight Experience Replay (HER, Andrychowicz et al., 2017) is a popular approach for training language conditioned policies. Given some goal $g$, HER converts an unsuccessful trajectory $\tau$ into positive examples by replacing $g$ with some hindsight goal $g^{\prime}$. That is, HER uses a relabeling function to map $\tau$ to a new goal $g^{\prime}$, resulting in a positive demonstration $\left(g^{\prime}, \tau\right)$, that is used to update the policy.

Since the original implementation of HER considers settings where the goal space is the raw environment observation space, applying HER to natural language instructionfollowing requires access to a learnt relabeling function to map observations to language instructions. Such relabeling functions typically map only the final observation $o_{T}$ to the instruction via pre-trained captioning models Xiao et al. 2022, Cideron et al., 2020; Sumers et al., 2023) that operate on trajectories from trained agents. In BAGEL, we use the full trajectory for relabeling and use an iterative relabeling procedure to reduce noise from zero-shot components.

## 4. Inference

We use synthetic demonstrations from BAGEL to adapt LM agents via retrieval augmented generation, and leave finetuning for future work. Concretely, given a test instruction $g_{\text {test }}$, we retrieve top- $k$ most relevant demonstrations in the demonstration set $\mathcal{M}$, pre-pending these to the context window of our agent as in-context examples. More concretely, we use dual encoder retrieval, similar to Lee et al. (2019), using a T5-XXL (Raffel et al., 2020) embedding model. We first compute a vector embedding $f_{\theta}(g)$ for each instruction $g \in \mathcal{M}$, and then find the top- $k$ demonstrations based on scores $f_{\theta}(g)^{\top} f_{\theta}\left(g_{\text {test }}\right)$. More details can be found in Appendix A.

## 5. Datasets

Our experiments are based on two environments, MiniWoB++ (Shi et al., 2017; Liu et al., 2018) and ToolQA (Zhuang et al. 2023).

### 5.1. MiniWoB++

MiniWoB++ is a collection of tasks consisting of web interfaces with a shared action space of mouse and keyboard actions. In our setup, actions are specified in natural language (Type Bob in the name text box, Click on the datepicker, Clear text on Destination). The low-level controller that maps action strings into a Selenium API call is implemented via a separate zero-shot prompted LM (see Appendix Cfor details). Each task consists of a script to generate variations of the task with a templated instruction, where each variation is controlled via a random seed.

Evaluation. We follow Shaw et al. (2023) for evaluating agents on MiniWoB++, by mapping the raw MiniWoB++ reward from $[-1,1]$ to $[0,1]$. For each web interface, we report the mean score over 50 random seeds. Starting with the set of 55 MiniWoB++ tasks used in prior work on applying LM agents to this domain (Gur et al., 2023, Kim et al. 2023; Sun et al. 2023), we evaluate on the hardest 10 tasks where the zero-shot agent has an average reward of less than 0.95 , to perform a more targeted evaluation of BAGEL to domains that are hard for zero-shot agents.

### 5.2. ToolQA

ToolQA is a tool augmented question-answering environment over 8 domains, where questions can be answered by chaining calls to multiple tools including text retrievers, databases, SQL interpreter, calculator etc. Each tool can be called according to a set of pre-defined methods (see Appendix B. 2 for the full action space for the policy and corresponding tool methods). The observation space is the string output from the most recent tool call (the first obser-

![](https://cdn.mathpix.com/cropped/2024_06_04_0d0a139940b839d4482dg-05.jpg?height=455&width=1678&top_left_y=214&top_left_x=188)

Figure 3. Results across MiniWoB++ and ToolQA, broken down by domain. We compare using demonstrations obtained via BAGEL (blue) with a zero-shot ReAct baseline (green) with no synthetic demonstrations. For MiniWob++, we use the Trajectory-First variant for exploration, and for ToolQA, we use Instruction-First. Overall, using BAGEL demonstrations leads to improvements on both datasets.

vation is hard-coded as a "System prompt"). Each action corresponds to a specific tool call expressed in language (Load the Airbnb Database, Calculate 3+7), and the lowlevel controller is implemented by post-processing strings into tool methods. The episode terminates when the policy chooses the Finish with Answer action e.g. Finish with Answer: 300 , where 300 is taken as the predicted answer.

Evaluation. Following prior work on questionanswering (Rajpurkar et al., 2016, 2018, Joshi et al. 2017), we compute the F1 score of the final (free-form) model output from the Finish with Answer tool call against ground-truth answers.

## 6. Experimental Setup

### 6.1. Baselines and Ablations

Zero-shot. As our first baseline, we use the zero-shot policy $\pi_{\text {base }}$ directly at test time.

Non-iterative Ablations. Similar in spirit to Sumers et al. (2023), in BAGEL (trajectory-first, no itrs), explored trajectories $\tau^{0}$ are labeled using $p_{\text {label }}$ and resulting demonstrations $\left(\mathrm{g}, \tau^{0}\right.$ ) are included in $\mathcal{M}$ if the score $s(g, \tau)=1$. Similarly, in BAGEL (instruction first, no itrs), synthetic instructions sampled from the instruction generator (see $\$ 3.1$ ) are converted into trajectories using $p_{\text {agent }}$, and the resulting demonstration $\left(g^{0}, \tau\right)$ is added to $\mathcal{M}$, if $s\left(g^{0}, \tau\right)=1$. This baseline captures a simple way to use LMs to construct synthetic demonstrations via a sample-then-filter approach: prompt an LM to generate possible instructions given the first observation from the environment, create trajectories based on these, and filter based on another criterion. In general, we expect exploration using the instruction generator to work poorly in settings where the $\mathrm{LM}$ cannot predict potential instructions from just the first observation (e.g. it might hard to generate candidate instructions solely from the landing page of the website without further interaction).

### 6.2. Implementation Details

We evaluate all baselines and variants of BAGEL on MiniWoB++ and ToolQA. For MiniWoB++, we start with sampling 60 trajectories in the exploration phase for trajectoryfirst variants of BAGEL, and sample 60 synthetic goals for instruction-first variants. For ToolQA, we sample 200 trajectories for BAGEL (trajectory-first), and 200 synthetic goals for BAGEL (instruction-first).

We use an instruction tuned PaLM-2 (Anil et al. 2023) as the base $\mathrm{LM}$ for all our experiments. We set the max episode length $T$ to 15 for all datasets and models. We also set $T_{\text {iter }}$ to 5 , when performing multiple iterations in BAGEL. In addition to using ReAct prompting, we use a simple "resampling" procedure to recover from issuing syntactically incorrect actions-if an action causes the environment to return an Exception (such as incorrectly invoking a tool, or typing on an element that cannot be typed on), we sample another action rom the agent with the Exception message appended to its context. We keep re-sampling until it chooses a syntactically correct action, or terminate the episode if the agent is unable to fix an erroneous action in $m=5$ steps.

## 7. Main Results

Figure 3 compares the zero-shot baseline with agents augmented with BAGEL demonstrations. We find that using synthetic demonstrations as in-context exemplars, retrieved based on instruction relevance, lead to significant boosts in performance compared to the zero-shot agent. For the best variant of BAGEL, we find improvements of over $13 \%$ points on MiniWoB++, and over $2 \%$ on ToolQA. For MiniWoB++, our improvements are particularly strong ( $20 \%$ absolute) on choose-date, tic-tac-toe, and use-autocomplete. Solving these tasks successfully requires learning environment dynamics (e.g. Figure 1) which is enabled by BAGEL demonstrations. We isolate the source of these improvements from synthetic in-context exemplars in $\$ 8.1$. Further-

| Dataset | Zero-Shot | instruction-first |  |  | trajectory-first |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | No-itrs | Full |  | No-itrs | Full |
| MiniWoB++ | 46.8 | 52.0 | 56.0 |  | 53.0 | $\mathbf{6 1 . 0}$ |
| ToolQA | 40.9 | 38.8 | $\mathbf{4 3 . 3}$ |  | 40.9 | 42.2 |

Table 1. Ablations showing the effect of multiple rounds of relabeling in BAGEL. Multiple iterations improve performance for both instruction-first and trajectory-first variants.

more, trajectory-first exploration significantly outperforms instruction-first on MiniWoB++, which we posit is due to the LM prior being misaligned with the distribution over possible instructions on MiniWoB++.

Finally, Table 1 shows that iterative re-labeling always improves performance over non-iterative baselines. Multiple iterations of round trips improves average reward by $4-8 \%$ on MiniWoB++ and 1.3-4.5\% on ToolQA.

## 8. Analysis

To understand how BAGEL demonstrations improve agent performance, we first look at confounders from in-context learning ( $\$ 8.1$ ), and then study the impact of synthetic demonstrations on execution failures ( $\$ 88$ ). Next, we analyze the correctness ( $\$ 8.3$ ) and diversity ( $\$ 8.4$ ) of BAGEL's demonstrations to identify areas for further improvements.

### 8.1. In-context Learning with Synthetic Demonstrations

In-context exemplars can provide a range of useful learning signal to LM agents, ranging from simply providing examples of valid action trajectories or relevant natural language instructions in isolation, to providing rich information about the conditional $p(\tau \mid g)$ (how to map relevant instructions into action sequences). Indeed, for some text classification tasks, Min et al. (2022) find that improvements from incontext learning may be explained in terms of the former i.e. examples of the label space and input text. To better understand how synthetic demonstrations help in our setting, we report results from two ablations. First, we provide the model with randomly chosen demonstrations instead of using the retriever (Random). Next, we shuffle demonstrations so that trajectories are paired with randomly chosen instruction within the set of retrieved examples (Shuffled).

Results. Table 2 reports results of these ablations. First, Shuffled improves performance over the zero-shot baseline, suggesting that some of the improvements come from providing examples of valid action trajectories in the domain in line with findings in Min et al. (2022). Ours records a further improvement of $0.8 \%$ over Shuffled, which suggests that the agent is able to use signal about the conditional to

| Method | Accuracy |
| :--- | :---: |
| Zero-shot | 40.9 |
| Random | 38.0 |
| Shuffled | 41.4 |
| Ours | $\mathbf{4 2 . 2}$ |

Table 2. Ablations showing the effect of various sources of information in synthetic demonstrations to agent performance.

| Task | Zero-Shot $(\downarrow)$ | +BAGEL $(\downarrow)$ |
| :--- | :---: | :---: |
| choose-date | 1.3 | $\mathbf{0 . 1}$ |
| book-flight | 3.0 | $\mathbf{0 . 6}$ |
| ToolQA (average) | 3.0 | $\mathbf{1 . 9}$ |

Table 3. Average number of execution failures for tasks in MiniWoB++ and ToolQA. We find that using synthetic demonstrations reduces execution failures.

improve decision making.

### 8.2. Synthetic demonstrations reduce execution failures

As mentioned in $\$ 6.2$, in our implementation, LM agents recover from execution failures using a simple re-sampling procedure-when the agent generates an invalid action (such as attempting to Type on a checkbox element or calling a tool with incorrect syntax), we re-prompt it with the error message produced by the environment, until it produces a valid action. Of course, such re-sampling can be costly at inference time due to multiple calls to the LM. Table 3 reports the average execution failures for tasks with re-sampling on MiniWoB++ and ToolQA. We note a considerable reduction in average re-sampling with BAGEL, due to a better understanding of environment dynamics, in turn leading to faster inference.

### 8.3. Correctness of Synthetic Demonstrations

One way to identify the scope for improvements in our method is to manually verify the correctness of demonstrations. We filter demonstrations which, upon execution, do not achieve the corresponding instruction. Using these filtered demonstrations improves performance further by $7 \%$ absolute on all 10 tasks from MiniWoB++.

### 8.4. Diversity of Synthetic Demonstrations

To better understand the distribution of synthetic demonstrations, we manually bucket demonstrations for social-media and email-inbox-all into semantic clusters- for socialmedia these clusters include \{Retweet, Like, Share, ...\} and for email-inbox-all we have clusters such as \{Forward, Delete, Star, Reply, ...\}. For ToolQA, we cluster demonstrations based on the set of tools invoked in the demonstration.
![](https://cdn.mathpix.com/cropped/2024_06_04_0d0a139940b839d4482dg-07.jpg?height=432&width=1646&top_left_y=212&top_left_x=183)

Figure 4. Distribution of demonstrations over semantic categories for MiniWob++ environments, social-media and email-inbox-all, and ToolQA. While BAGEL prefers certain modes, overall we find that these demonstrations cover a diverse range of actions.

We plot the number of demonstrations in each cluster in Figure 4 We note that while this distribution tends to be skewed towards specific modes (e.g. $\{$ graph $\}$ for ToolQA, $\{\mathrm{Star}\}$ for email-inbox), there exists a long tail that covers a broad range of possible use cases in the environment. Nevertheless, improving diversity during exploration remains a failure mode for BAGEL which we expand on next. Finally, we provide some examples of BAGEL demonstrations in Table 4 along with their corresponding semantic category.

### 8.5. Error Analysis

We conclude with a discussion of failure modes of our aproach using the domains book-flight, search-engine, and SciRex as case studies.

Handling Long-Horizon Planning. We note that bookflight is the most complex environment in MiniWoB++, with longer trajectories of lengths 8-20, and the zero-shot policy performs poorly on this environment (average reward of $5 \%$ ). While using BAGEL demonstrations improves this to $15 \%$, we hypothesize that further improvements would require better handling of long range plans, such as with hierarchical planning (Sodhi et al., 2023; Jiang et al., 2019).

Improving Diversity. We hypothesize that improving diversity among seed trajectories would lead to further improvements across the board. For instance, for book-flight, all BAGEL demonstrations correspond to booking flights in December, while the test distribution is more uniform.

Reducing Mismatch with Test Instructions. On SciRex, all models fail to produce even a single correct answer. Here, we find that in the absence of any knowledge about user instructions at test-time, BAGEL demonstrations tend to create questions with more descriptive answers and trajectories with generic queries (See Table 4 for an example) while test instructions requires retrieving specific numbers from scientific documents by querying for specific topics. Similarly, on search-engine, we note a modest improvement of only $5 \%$. Here, we find that while BAGEL demonstrations cover a variety of instructions like Search for cat and navigate to the third page of search results, Search for cars, then visit the second search result, the model fails on test instructions like Enter [term] then find and click the 9th search result that requires keeping track of the number of search results per page, and navigating to the correct page. While our goal is to build fully unsupervised agents, methods that use sparse information about test-time instructions could help drive performance further.

## 9. Related Work

Instruction-Following Digital Agents. Building agents that navigate the digital world is a long standing goal of AI and language understanding (Allen et al., 2007, Branavan et al. 2009). However, most prior work relies on expert demonstrations (Liu et al., 2018; Humphreys et al., 2022; Furuta et al., 2023) with an appropriately shaped reward (Branavan et al., 2009, Liu et al. 2018). Here, we assume no access to demonstrations or a reward function, and use pretrained components to bootstrap synthetic demonstrations.

LMs for Decision Making. Pre-trained LMs are increasingly being used for sequential decision making tasks such as robotic manipulation (Ahn et al., 2022, Liang et al. 2023), instruction-following (Yao et al., 2022; Kim et al., 2023; Sun et al., 2023, Lù et al., 2024), and tool-use (Parisi et al. 2022). While some of these approaches finetune LMs based on human demonstrations (Nakano et al. 2021), others use human demonstrations in their prompt for in-context learning and adaptation (Yao et al., 2022, Kim et al., 2023, Sun et al. 2023). We use no human supervision or reward and adapt LM agents purely using synthetic demonstrations. Another line of work uses LM priors in RL to improve exploration (Du et al., 2023), deal with large action spaces (Yao et al. 2020), or as proxy reward functions (Kwon et al., 2023). In the same tradition, BAGEL bootstraps a learning signal in the form of synthetic demonstrations by combining several LM components without RL.

| Instruction | Trajectory |
| :---: | :---: |
| MiniWoB++ |  |
| Find the email by Trixi and reply to them with the text "Maecenas <br> eu massa" $\{$ Reply $\}$ | Move Mouse to Trixi $\rightarrow$ Click on an email-thread $\rightarrow$ Click on the <br> reply button $\rightarrow$ Type 'Maecenas eu massa' on the textarea with id <br> 'reply-text' $\rightarrow$ Click on the span with id 'send-reply' |
| Find the email by Darcy and forward it to Dionis $\{$ Forward $\}$ | Click on Darcy, the sender of an email thread. $\rightarrow$ Click on 'for- <br> ward' button $\rightarrow$ Type Dionis on the to field $\rightarrow$ Click on the 'send' <br> button |
| Retweet Gallegos's post $\{$ Retweet $\}$ | Move Mouse to Pretium,. Ullamcorper. $\rightarrow$ Click on retweet <br> element with id 101 |
| Like tweet by @ leonie and share tweet by @livia \{Like, Share $\}$ | Click on the like element with ID 41. $\rightarrow$ Click on share-113 |
| ToolQA |  |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0d0a139940b839d4482dg-08.jpg?height=119&width=829&top_left_y=735&top_left_x=201) | Retrieve passages related to David's plans this weekend $\rightarrow$ Finish <br> with answer: On the evening of September 16th, 2022, David will <br> be attending a Blind Date Night event at The Press Lounge. |
| Who is affiliated with both nicolas christin and giulia fanti? <br> \{Python, Graph $\}$ | Load DBLP $\rightarrow$ Check neighbours of node Giulia Fanti in graph <br> AuthorNet $\rightarrow$ Check neighbours of node Nicolas Christin in <br> graph AuthorNet $\rightarrow$ Evaluate python code: listl $=[$ 'Wanzheng <br> Zhu', 'Rohan Bansal', 'Zachary Weinberg', 'Nicolas Christin', <br> 'Suma Bhat', 'Hongyu Gong']; list2=['Wanzheng Zhu', 'Rohan <br> Bansal', 'Zachary Weinberg', 'Suma Bhat', 'Hongyu Gong', 'Giu- <br> lia Fanti']; ans set(list1) \& set(list2) $\rightarrow$ Finish with answer: <br> \{'Hongyu Gong', 'Rohan Bansal', 'Wanzheng Zhu', 'Zachary <br> Weinberg', 'Suma Bhat'\} |
| What are the top 5 airbnb options with price $<900$, availability <br> $>260$ and at least 40 reviews $\{$ Database, SQL \} | Load database airbnb $\rightarrow$ Filter database according to price $<$ <br> 900, availability $365>260$, number of reviews $>40 \rightarrow$ Interpret <br> SQLite query: SELECT FROM airbnb_data ORDER BY num- <br> ber_of_reviews DESC LIMIT $5 \rightarrow$ Finish with answer: ['High-end <br> doorman bldg in the LES', 'THE BEST DEAL ON THE HUD- <br> SON RIVER!!', 'Heart of Williamsburg, Brooklyn!', 'Beautiful <br> $\&$ Tranquil Oasis in a Great Location', 'Sunny/Cozy 1BD'] |
| What are the different approaches for computing graph similarity? <br> \{RetrieveSciRex\} | Retrieve passages from ML papers related to graph similarity $\rightarrow$ <br> Finish with answer: The different approaches to computing graph <br> similarity are graph kernels, graph features and graph convolu- <br> tional neural networks (CNNs). |

Table 4. Example demonstrations obtained via BAGEL for MiniWoB++ (top) and ToolQA (bottom). We also provide the semantic category for these demonstrations, and report the distribution of these categories in Figure 4

Self-training for Language Models. A recent line of work uses LM-generated data for finetuning the same LM, in settings where external verifiers may be used to filter generated data (Singh et al., 2023, Gulcehre et al., 2023). While we also use data generated from an LM for adaptation, unlike these approaches, environment interactions form a critical part of the learning signal and we also do not use external verifiers for filtering data.

## 10. Conclusion

There is a growing interesting in grounding LMs to the real world, by building helpful assistants that execute openended instructions in digital environments. The complexity of such sequential tasks makes collecting expert demonstrations tedious, and so, further progress towards building such agents requires new methods for bootstrapping a learning signal with minimal human supervision. To this end, we introduce BAGEL, a method for constructing synthetic demonstrations for instruction following agents. These demonstrations are constructed by iteratively relabeling an initial seed set of trajectories or instructions, where both relabeling and exploration is driven by a language model. Experiments on two different domains show that using BAGEL demonstrations as in-context exemplars leads to considerable improvements ranging from $2-13 \%$, as well as significant reductions in execution failures.

## 11. Impact Statement

In this paper, we evaluated models only in offline environments. Responsibly deploying models online carries potential risks, and it would be important to verify and constrain model behaviour to not cause harm (e.g. violating
terms of service). Further research related to secure model deployment should take into account problems such as spam detection, privacy preservation etc.

## Acknowledgements

SM was partly funded by a gift from Apple Inc. CM is a fellow in the CIFAR Learning in Machines and Brains program. We thank David Gaddy, Anna Goldie, Luke Vilnis, Tianze Shi, Jonathan Berant, Kristina Toutanova, Raphael Hoffman, and members of Google DeepMind and the Stanford NLP Group for helpful discussions and comments.

## References

Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022 .

Allen, J., Chambers, N., Ferguson, G., Galescu, L., Jung, H., Swift, M., and Taysom, W. Plow: a collaborative task learning agent. In Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2, AAAI'07, pp. 1514-1519. AAAI Press, 2007. ISBN 9781577353232.

Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.

Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Branavan, S., Chen, H., Zettlemoyer, L., and Barzilay, R. Reinforcement learning for mapping instructions to actions. In Su, K.-Y., Su, J., Wiebe, J., and Li, H. (eds.), Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 82-90, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL https://aclanthology.org/P09-1010.

Chaplot, D. S., Sathyendra, K. M., Pasumarthi, R. K., Rajagopal, D., and Salakhutdinov, R. Gated-attention architectures for task-oriented language grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.

Cideron, G., Seurin, M., Strub, F., and Pietquin, O. Higher: Improving instruction following with hindsight generation for experience replay. In 2020 IEEE Symposium
Series on Computational Intelligence (SSCI), pp. 225232. IEEE, 2020.

Du, Y., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel, P., Gupta, A., and Andreas, J. Guiding pretraining in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692, 2023.

Furuta, H., Nachum, O., Lee, K.-H., Matsuo, Y., Gu, S. S., and Gur, I. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023.

Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.

Gur, I., Nachum, O., Miao, Y., Safdari, M., Huang, A., Chowdhery, A., Narang, S., Fiedel, N., and Faust, A. Understanding HTML with large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 2803-2821, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.185. URL https:// aclanthology.org/2023.findings-emnlp.185.

Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022.

Humphreys, P. C., Raposo, D., Pohlen, T., Thornton, G., Chhaparia, R., Muldal, A., Abramson, J., Georgiev, P., Santoro, A., and Lillicrap, T. A data-driven approach for learning to control computers. In International Conference on Machine Learning, pp. 9466-9482. PMLR, 2022.

Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019 .

Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Barzilay, R. and Kan, M.-Y. (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10. 18653/v1/P17-1147. URL https://aclanthology. org/P17-1147.

Kim, G., Baldi, P., and McAleer, S. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.

Kwon, M., Xie, S. M., Bullard, K., and Sadigh, D. Reward design with language models. In The Eleventh International Conference on Learning Representations, 2023. URLhttps://openreview.net/forum?id= 10 uNUgI5Kl

Lee, K., Chang, M.-W., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6086-6096, 2019 .

Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. IEEE, 2023.

Liu, E. Z., Guu, K., Pasupat, P., Shi, T., and Liang, P. Reinforcement learning on web interfaces using workflowguided exploration. In International Conference on Learning Representations, 2018.

Logeswaran, L., Fu, Y., Lee, M., and Lee, H. Few-shot subgoal planning with language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5493-5506, 2022.

Lù, X. H., Kasner, Z., and Reddy, S. Weblinx: Realworld website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930, 2024.

Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.

Misra, D., Langford, J., and Artzi, Y. Mapping instructions and visual observations to actions with reinforcement learning. arXiv preprint arXiv:1704.08795, 2017.

Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

Parisi, A., Zhao, Y., and Fiedel, N. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21: $1-67,2020$.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Su, J., Duh, K., and Carreras, X. (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https: //aclanthology.org/D16-1264

Rajpurkar, P., Jia, R., and Liang, P. Know what you don't know: Unanswerable questions for SQuAD. In Gurevych, I. and Miyao, Y. (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology.org/P18-2124.

Shaw, P., Joshi, M., Cohan, J., Berant, J., Pasupat, P., Hu, H., Khandelwal, U., Lee, K., and Toutanova, K. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. arXiv preprint arXiv:2306.00245, 2023.

Shi, T., Karpathy, A., Fan, L., Hernandez, J., and Liang, P. World of bits: An open-domain platform for webbased agents. In International Conference on Machine Learning, pp. 3135-3144. PMLR, 2017.

Shinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.

Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.

Sodhi, P., Branavan, S., and McDonald, R. Heap: Hierarchical policies for web actions using llms. arXiv preprint arXiv:2310.03720, 2023.

Sumers, T., Marino, K., Ahuja, A., Fergus, R., and Dasgupta, I. Distilling internet-scale vision-language models into embodied agents. 2023.

Sun, H., Zhuang, Y., Kong, L., Dai, B., and Zhang, C. Adaplanner: Adaptive planning from feedback with language models. arXiv preprint arXiv:2305.16653, 2023.

Xiao, T., Chan, H., Sermanet, P., Wahid, A., Brohan, A., Hausman, K., Levine, S., and Tompson, J. Robotic skill acquisition via instruction augmentation with visionlanguage models. arXiv preprint arXiv:2211.11736, 2022.

Yao, S., Rao, R., Hausknecht, M., and Narasimhan, K. Keep CALM and explore: Language models for action generation in text-based games. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736-8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704. URL https: //aclanthology.org/2020.emnlp-main.704.

Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022.

Zhuang, Y., Yu, Y., Wang, K., Sun, H., and Zhang, C. Toolqa: A dataset for llm question answering with external tools. arXiv preprint arXiv:2306.13304, 2023.
