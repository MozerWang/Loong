# WHAT DOES THE KNOWLEDGE NEURON THESIS HAVE TO DO WITH KNOWLEDGE? 

Jingcheng Niu ${ }^{14} \quad$ Andrew Liu ${ }^{2} \quad$ Zining Zhu ${ }^{134} \quad$ Gerald Penn ${ }^{14}$<br>niu@cs.toronto.edu a254liu@uwaterloo.ca zzhu41@ stevens.edu gpenn@cs.toronto.edu<br>${ }^{1}$ University of Toronto, ${ }^{2}$ University of Waterloo, ${ }^{3}$ Stevens Institute of Technology, ${ }^{4}$ Vector Institute


#### Abstract

We reassess the Knowledge Neuron (KN) Thesis: an interpretation of the mechanism underlying the ability of large language models to recall facts from a training corpus. This nascent thesis proposes that facts are recalled from the training corpus through the MLP weights in a manner resembling key-value memory, implying in effect that "knowledge" is stored in the network. Furthermore, by modifying the MLP modules, one can control the language model's generation of factual information. The plausibility of the $\mathrm{KN}$ thesis has been demonstrated by the success of $\mathrm{KN}$-inspired model editing methods (Dai et al. 2022; Meng et al., 2022).

We find that this thesis is, at best, an oversimplification. Not only have we found that we can edit the expression of certain linguistic phenomena using the same model editing methods but, through a more comprehensive evaluation, we have found that the $\mathrm{KN}$ thesis does not adequately explain the process of factual expression. While it is possible to argue that the MLP weights store complex patterns that are interpretable both syntactically and semantically, these patterns do not constitute "knowledge." To gain a more comprehensive understanding of the knowledge representation process, we must look beyond the MLP weights and explore recent models' complex layer structures and attention mechanisms.


## 1 INTRODUCTION

Recent research has highlighted the remarkable ability of large pretrained language models (PLMs) to recall facts from a training corpus (Petroni et al. 2019). The underlying mechanism by which this information is stored and retrieved within PLMs, however, remains a subject of intensive investigation. The Knowledge Neuron (KN) Thesis has been recently proposed as a novel framework for interpreting language models (LMs) (Dai et al. 2022; Meng et al., 2022; 2023). This thesis suggests that LMs operate akin to key-value memories, recalling facts from the training corpus through the multi-layer perceptron (MLP) weights. Therefore, a significant implication of the KN thesis is that factual information generation by LMs can be controlled by modifying the MLP modules. Should this manipulation of factual information recall become feasible, it could lead to the development of language models that are more controllable, interpretable, and factually aligned.

The plausibility of the $\mathrm{KN}$ thesis is demonstrated by the success of $\mathrm{KN}$-inspired model-editing methods. Dai et al. (2022) argued that relational facts can be localised to a handful of 2-5 MLP neurons. They then developed a method to identify these neurons using a search algorithm based on an integral of gradients. By manipulating the activation of these identified neurons (KN edit), they managed to alter the model's response to fill-in-the-blank cloze tasks and generate counterfactual information without additional fine-tuning. In a parallel approach, Meng et al. (2022) proposed a more intricate model wherein factual recall occurs in two critical locations, each incorporating a different module. In this model, the mid-layer MLP retrieves the fact, and an attention module copies it into the output response at the topmost layer. Despite this proposed two-step process, their proposed model editing method, Rank-One Model Editing (ROME), only modifies MLP weights, much as KN edit only modifies MLP activations without editing attention modules.

While the efficacy of these model editing methods has been showcased in simple fill-in-the-blank cloze tasks, the appraisal of such achievements mainly rests on basic paraphrasing of the prompts, as

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-02.jpg?height=292&width=1179&top_left_y=228&top_left_x=468)

Figure 1: Syntactic phenomena can be located and edited using existing model editing methods. The integrated gradient of singular determiner (this, that) and plural determiner (these, those) form two distinct groups. Erasing these neurons leads to output probability changes.

outlined by Yao et al. (2023), who introduced an additional assessment metric, portability, finding that model-editing methods to date lack robustness. Their performance is halved when evaluated with the portability measure. Building on this, we introduce two new metrics. First, a successful edit must demonstrate symmetry within bijective relationships (e.g., with the assertion Ottawa is the capital of Canada, the reciprocal Canada's capital is Ottawa should also hold valid). Second, a successful edit must extend to synonym usage (e.g., a dentist treats a toothache and a dentist treats tooth pain should be considered equivalent). Our evaluation shows that existing model-editing methods are even less robust under these two new criteria.

It is practically impossible to exhaustively assess factual model-editing methods due to the difficulty in systematically dealing with counterfactual data. The potential counterfactual replacements for Canada's capital are seemingly endless. Thus, beyond the introduction of the two new evaluation criteria above, we propose the evaluation of model-editing methods using syntactic constructions. We have determined that the $\mathrm{KN}$ thesis applies just as reliably to syntactic phenomena (as illustrated in Figure 11. Unlike many facts, syntactic phenomena can provide rigorously defined targets for editing through the use of so-called minimal pairs. As a result, in this paper, we re-evaluate the $\mathrm{KN}$ thesis by expanding the scope of our assessment to include more complex factual patterns and syntactic phenomena. This also speaks to a long-standing debate regarding the formal $v s$. functional competence of language models (Mahowald et al. 2024) - an LM's ability to follow linguistic rules and patterns vs. its ability to apply language in the real-world (see \$2.3). If we edit a model's expression of facts and linguistic phenomena using the same approach, this could indicate that both the formal and functional competencies of an LM are governed by the same underlying mechanisms.

Within the context of Dai et al.'s (2022) KN framework, KN edit's efficacy is unsatisfactory. Editing the $\mathrm{KN}$ activations has only limited impact on categorical predictions. The effect of $\mathrm{KN}$ edit is only apparent in the shifts in the output probability distributions of tokens. The patterns that the method localises also appeal to superficial cues such as word co-occurrence frequencies. We also find several critical shortcomings in the ROME framework. LMs process both linguistic and factual information in phases, but the exact task distribution between the MLP and attention modules appears to be more idiosyncratic than initially theorized (Meng et al. 2022). ROME model editing only superficially alters token association patterns, in a manner that is inconsistent across the various expressions that may attend the same underlying knowledge. As a result, whatever is being manipulated reflects none of the traditional tautologies that have been associated with "knowledge," as that term has been understood in philosophy since the time of Aristotle. When implemented on syntactic constructions, furthermore, the influence of ROME's editing is limited only to the word altered and no pivot that preserves any reasonable standard of syntactic paraphrase, such as substitutability salva veritate, is forthcoming. Furthermore, ROME fails under our newly proposed symmetry and synonymy criteria.

We therefore argue for the position that the feed-forward MLP modules of the transformer model do not store knowledge, but rather complex "token expression patterns." These token expression patterns can often be interpreted linguistically, but the information that they express does not fit into linguistically or factually defined categories. A key-value, memory-based view of the language model is overly simplistic in explaining the remarkable ability of recent PLM's formal, and perhaps even functional, competence. We need to investigate the rich layer and attentive structure of recent PLMs more to arrive at a better understanding of their underlying mechanics.

In the following sections, we will first provide an overview of the $\mathrm{KN}$ thesis ( $\$ 22$. Then we will evaluate two practices inspired by it: Dai et al.'s (2022) KN edit framework ( $\$ 3$ and Meng et al.'s (2022) ROME framework ( $\$ 4$. Finally, we will conclude the paper with a discussion ( $\$ 5$.[^0]

## 2 The KNOWLEdGe NeUron ThESIS

Geva et al. (2021) were among the first to propose that the MLP modules in a transformer model behave like key-value memories. A typical MLP module in recent transformer-based PLMs has two layers. They argue that the first layer corresponds to keys, and the second layer, to values ${ }^{2}$ They found that each key neuron is triggered by human-interpretable shallow input patterns such as periods of time that end with the letter " $a$." Then, the corresponding value neurons distorted the next-token output probability, until a final distribution is generated.

The KN thesis emerged as a result of this important discovery. Dai et al. (2022) coined the term knowledge neuron and ambitiously claimed that the keys and values within MLP modules not only capture simple patterns but also store "knowledge." They formulate an item of fact, such as Canada's capital is Ottawa, as a 3-tuple $(s, t, r)$, consisting of the source ( $s$, Canada), the target ( $t$, Ottawa) and the relation ( $r$, capital) between them. The authors asserted that this tuple can be localized to a small group of MLP neurons typically found in the topmost layers of the language model, which they identified by analysing the magnitude of the integrals of gradients among prompts. To support their claim, they conducted model-editing experiments. By suppressing the KNs (setting their activations to zero), they observed a decrease in the probability of generating the correct original target $(t)$, while other tokens remained largely unaffected, demonstrating a "minimally invasive surgery." Meng et al. (2022) proposed a refinement of Dai et al.'s (2022) model. They employed a causal mediation method (Finlayson et al. 2021) to form a more intricate version of the $\mathrm{KN}$ thesis. They argue that the factual association process happens at two locations: a mid-layer MLP recalls the fact from memory, and the topmost layer's attention model copies that information to the final output.

There were similar investigations of neurons prior to the KN thesis. Durrani et al. (2020) observed the neurons of an auxiliary probing model that was trained on BERT embeddings, not the neurons of BERT itself. Therefore, their analysis faced an all-too-common dilemma for probing: did they find insights about the language models or artefacts of the fine-tuning process (Hewitt \& Liang, 2019)? Finlayson et al. (2021) used causal mediation analysis to study subject-verb agreement in GPT and XLNet (Yang et al. 2019). In particular, they observed a difference in ratios between the verb with the correct inflection and one with the incorrect inflection. They then modify the prompt, see the probability change and reason about the internal mechanisms of the model for expressing subjectverb agreement. They concluded that the upper-middle layers are more relevant to the expression and that there are various levels of overlap between the top $5 \%$ neurons used to express agreement. These insights, however, just as with previous probing work, are still purely observational and largely preoccupied with layers and network depth. They are able to observe many characteristics of the process, but still cannot cannot provide a satisfactory understanding of how it happens.

More recently, there has been interest in utilizing large language models (LLMs) to gain insight into the differing functionalities of individual neurons. Despite its title's strident claim that neurons in LMs can be "explained," Bills et al. (2023) clarify that their model "explains correlations, not mechanisms." From a knowledge-representation standpoint, their evaluation of LLM explanations is also entirely observational. When Huang et al. (2023) reassessed the validity of these explanations, even the most confident ones had high error rates and little to no causal effects on the interventions that use the explanations. The LLM interpretation of LMs is still immature.

### 2.1 EVALUATING THE KN THESIS: AN OVERVIEW

The effectiveness of a model-editing algorithm is customarily evaluated across three dimensions (Yao et al., 2023): (1) reliability: whether the model can successfully change its output from $t$ to $t^{*}$ (also referred to as an efficacy score by Meng et al. (2022)); (2) generality: whether the effect is applicable to rephrased relations; and, (3) locality: whether the edit impacts unrelated relations. Yao et al. (2023) stress, however, that the assessment of generality is often constrained to simple paraphrasing. This is typically done by developing multiple templates for a specific relation. For instance, the relation capital can be structured as both "The capital of [s] is [t]." and "[s]'s capital is [t]." Previous evaluations (Elazar et al., 2021; Meng et al., 2022, 2023) prematurely announced success when a model, edited on a first template, could be generalized to a second template. Thus, Yao et al. (2023) recommended extending the assessment parameters by introducing the concept of[^1]portability. For example, having changed Watts Humphrey's alma mater from Trinity College to Harvard University, the model should return Boston instead of Dublin when asked about the city where Watts Humphrey received his university education. It was apparent that model-editing methods present a markedly lower level of portability than generality ( $50 \%$ versus $90 \%$ ). The evaluation of portability, on the other hand, requires new data annotation, which can be costly.

Extending Yao et al. (2023), we attempt a more comprehensive evaluation of model editing of factual association with two extra criteria: bijective symmetry and synonymous invariance. Bijective symmetry does not require new data collection and we can obtain data automatically from previous corpora. For a bijection relation such as capital or capital of, we should see the model generalise $\left(s, t \rightarrow t^{*}, r\right)$ to $\left(t^{*}, s, r^{-1}\right)$. For example, if we change the capital of Canada to Rome, then the model should also agree that Rome is the capital of Canada. Similarly, an effective edit should also be able to generalise across synonyms. If the model knows that a dentist treats toothaches, it should also know that they also treat tooth pain. Prior work (Elazar et al. 2021) only used synonym replacement on rephrasing the relation prompts - we extend it to the source and the target.

Several others have already questioned the validity of the KN thesis. Hase et al. (2023) identified discrepancies between the results of causal tracing and the effects of ROME editing. They concluded that a mechanistic understanding reveals insights on the consequences of model editing. To the best of our knowledge, we are the first to comprehensively evaluate the $\mathrm{KN}$ thesis using rigorously defined syntactic phenomena. We consider three: determiner-noun agreement, subject-verb agreement, and gender and number agreement across anaphoric chains.

### 2.2 EvaluATING THE KN THESIS ON SYntactiC PHENOMENA

Edit pairs for syntactic phenomena, by contrast, can be systematically extracted through the formation of "minimal pairs." For a grammatical sentence that expresses a linguistic phenomenon, we can construct an ungrammatical sentence that minimally differs from the original sentence in respect of one feature of grammatical acceptability. For example, the phrase this student can be changed to the ungrammatical counterpart, *this students. The BLiMP corpus (Warstadt et al. 2020) is one of the most comprehensive and extensively utilised collections of such minimal pairs.

We therefore propose to systematically evaluate the effect of model-editing methods using syntactically differentiated prompts. We define a similar 3-tuple $(s, t, p)$ that contains the source $(s)$, the target $(t)$ and the syntactic phenomenon $(p)$. Take the phenomenon determiner-noun agreement as an example. In a grammatical sample sentence from a minimal pair, $s$ is the tokens that are condition the expression of the target (the determiner), and $t$ is the tokens that differ within the pair (the noun). The ungrammatical target $t^{*}$, is the noun in the opposite form. We then intervene with model editing, and observe whether the model assigns a higher probability to $t$ than $t^{*}$.

### 2.3 EdITING SYntACTIC PHENOMENA \& THE “FORMAL VS FUNCTIONAL" DISTINCTION

If we can successfully edit facts as well as syntactic phenomena using the same model-editing methods to the same degree, then it stands to reason that the model follows a unified underlying mechanism for both factual and syntactic information. Choosing the correct city (the Space Needle is in Seattle/*Rome) would be no different than choosing the correct verb form (the apple is/*are red).

Mahowald et al. (2024) refers to a distinction between the formal and functional competence of a language model: formal means "knowledge of linguistic rules and patterns," and functional refers to "understanding and using language in the world." Syntactic phenomena pertain to formal competence, and facts pertain to functional competence, respectively. NLP researchers sometimes informally use the terms syntax and semantics to refer to this distinction. BLiMP even refers to anaphoric gender agreement as morphological. Jawahar et al. (2019) and Tenney et al. (2019) believe that syntactic information is located in lower layers in BERT than semantic information, because syntactic information is more "shallow." Dai et al. (2022) appear to agree with this assertion in claiming that factual information is located in the upper layers. Meng et al. (2022), however, claim that factual information is located in the middle. This contradiction may support Niu et al.'s (2022) assertion that layers are not the best explanatory device of the distribution of these types of information in LMs. We explore here the possibility that no dividing line exists at all between the mechanisms through which a language model processes information related to these two types of competence.

## 3 Localising Syntactic PHENOMENA In LANGUAGE ModeLS

We put the $\mathrm{KN}$ thesis to the test under the $\mathrm{KN}$-edit framework by asking three questions: (1) can we localise linguistic phenomena using the same KN-edit method; (2) how do the levels of localisation compare to each other; and (3) are these localisations strong enough to support the $\mathrm{KN}$ thesis? 3

### 3.1 METHODS: SEARCHING FOR KNS OF SYntactic PHENOMENA

For each prompt, we calculate an integral-of-gradient attribution score $\alpha_{i}^{(l)}$ for the $i$-th intermediate neuron on the $l$-th layer $\left(w_{i}^{(l)}\right)$. Then, for a syntactic phenomenon with the source-target pair $(s, t, p)$, we find the neurons that have an attribution score greater or equal to $\pi=20 \%$ of the maximum attribution score shared among at least $\tau \%$ of its prompts. We start from $\tau=70 \%$ and adjust it by an increment or decrement of $5 \%$ until the number of neurons is within the range of $[2,5]$.

Neuron Attribution Score Given an input prompt $x$, we follow Dai et al. (2022) and use the integral of gradients to calculate the neuron attribution score:

$$
\begin{equation*}
\alpha_{i}^{(l)}=\bar{w}_{i}^{(l)} \int_{\gamma=0}^{1} \frac{\partial P_{x}\left(\gamma \bar{w}_{i}^{(l)}\right)}{\partial w_{i}^{(l)}} d \gamma, P_{x}\left(\hat{w}_{i}^{(l)}\right)=p\left(y \mid x, w_{i}^{(l)}=\hat{w}_{i}^{(l)}\right) \tag{1}
\end{equation*}
$$

where $P_{x}\left(\hat{w}_{i}^{(l)}\right)$ denotes the probability distribution of the token $y$ when changing the neuron $w_{i}^{(l)}$, $\mathrm{s}$ value to $\hat{w}_{i}^{(l)}$, and $\frac{\partial P_{x}\left(\alpha \bar{w}_{i}^{(l)}\right)}{\partial w_{i}^{(l)}}$ denotes the gradient of the model with respect to the activation $w_{i}^{(l)}$. We will see a more salient gradient when the neuron inflicts a greater change on the output probability.

Measuring the Level of Localisation We use three metrics to measure the level of localisation: (1) the number of identified neurons $(|\mathrm{KN}|$ ) using the initial threshold setting ( $\tau=70 \%)$, (2) the final threshold $\tau$ to obtain 2-5 KNs, and, (3) a similarity score among all the token attribution patterns.

Both of Dai et al.'s (2022) measures $(|\mathrm{KN}|$ and $\tau$ ) depend on adjusting the two threshold hyperparameters, $\pi$ and $\tau$. Here, we propose a non-parametric measure using a generalised $n$-sample similarity measure $\left(R_{1}^{2}\right)$ that measures the correlation of all the attribution patterns:

$$
\begin{equation*}
Y=\left[y_{1} \ldots y_{n}\right], y_{i}=\frac{s_{i}}{\left\|s_{i}\right\|}, Y=U S V^{\top}=\sum_{k=1}^{n} \sigma_{k} u_{k} v_{k}^{\top}, R^{2}=\frac{\sigma_{1}^{2}-1}{n-1} \tag{2}
\end{equation*}
$$

We first normalise and concatenate each attribution pattern $s_{i}$ for each prompt $x_{i}$ in the dataset into $Y$. Then, we can calculate the similarity/correlation among all $n$ patterns by conducting a singular value decomposition (SVD) and using the square of the first singular value $\sigma_{1}^{2}$. We then normalise this measure to the range $[0,1]$ so that the similarity between $n$ parallel vectors will be $R_{1}^{2}=1$, and $n$ orthogonal vectors will get $R_{1}^{2}=0$.

### 3.2 RESULTS \& FINDINGS

Finding 1: We can localise the grammatical number of determiners to just two neurons, just like factual information. The BLiMP paradigm determiner_noun_agreement_2 (DNA.2) contains 1000 sentence pairs with exactly one demonstrative determiner (this, that, these, those) agreeing with an adjacent noun, e.g., Carl cures those/*that horses. The determiner those is $t$, that is $t^{*}$ and[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-06.jpg?height=336&width=702&top_left_y=282&top_left_x=362)

(a) Effect of suppressing the singular neuron $w_{2096}^{(10)}$

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-06.jpg?height=288&width=699&top_left_y=325&top_left_x=366)

(b) Effect of suppressing the plural neuron $w_{1094}^{(9)}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-06.jpg?height=296&width=697&top_left_y=321&top_left_x=1061)

Figure 3: Suppressing the number neuron's (singular: $w_{2096}^{(10)}$; plural: $\left.w_{1094}^{(9)}\right)$ effect across numberexpressing prenominal modifiers. Significant $(p<0.05)$ changes are highlighted in red. The three sections in the plots are, from left to right, plural, singular and neutral modifiers.

the noun horses is $s$. A noun may appear in multiple sentence pairs. Among the paradigm's 1000 sentence pairs, we identified 283 unique Det-N pairs $\left(s, t, t^{*}, r\right)$.

Attribution Score Patterns The attribution score of neurons shows a highly consistent pattern that can be interpreted linguistically. We calculated the average attribution scores of all the prompts that contains each one of the determiners. Figure $2 \mathrm{a}$ shows a selection of the average attribution scores. The colour block in the $i$ th column and $j$ th row shows the attribution score $\alpha_{i}^{(j)}$. As we can see, a common neuron $\left(w_{2096}^{(10)}\right)$ has a high average attribution score for both of the singular determiners this and that, and another common neuron $\left(w_{1094}^{(9)}\right)$ lights up for the plural determiners these and those 4

This pattern is not only shown in aggregate. For each Det-N pair, we use the 1000 sentences in the paradigm as templates to create the prompts needed for a $\mathrm{KN}$ search. For each sentence, we replace the sentence's determiner and noun with the Det-N's determiner and noun. We then obtain 1000 sentences with different contexts but the same determiners and nouns. Then, we run a KN search on these 1000 sentences. When we look into each individual Det-N pair, the two neurons are identified as $\mathrm{KNs}$ in the vast majority of the pairs. As shown in Figure $2 \mathrm{~b}, w_{2096}^{(10)}$ appeared in $93 \%$ of the pairs with this and $75 \%$ of the pairs with that. The plural neuron appeared in $100 \%$ of pairs with these or those. More importantly, these neurons were not identified as KNs in pairs with the opposite grammatical numbers. Figure 2b shows an excerpt of the results (full results in Appendix B.2).

Effects of Suppressing the "Number Neuron" Do these two neurons correspond to grammatical number? We suppress each neuron (setting activation to 0 ) and compute the pre- and post-edit model's output probability of various number-expressing prenominal modifiers across all prompts with singular/plural nouns. Appendix B.1 explains the prenominal modifier selection process. Figure 3 shows the average effect of suppressing the identified $\operatorname{KNs}\left(\frac{p \text { (post-edit })-p \text { (pre-edit })}{\min (p(\text { post-edit) }) p \text { (pre-edit }))}\right)$.

The result of suppressing the plural neuron is pronounced (Figure 3 b). This intervention leads to a significant reduction in probability across all plural modifiers, a notable increase for the majority of singular modifiers, but a limited impact for modifiers that do not express number agreement. Therefore, erasing the activation of the plural neuron causes a decrease in the expression of determiner-noun agreement for plural modifiers. Although this $\mathrm{KN}$ search is solely based on these four demonstrative determiners, we observed that it generalizes to other determiners (one, $a$, an, every; two, both; multiple, several, various) and even adjectives (single, unique, sole). This effect is statistically significant. By treating the preand post-edit probabilities as two separate groups, a Student's (1908) $t$-test reveals significance when the modifiers are highlighted in red in

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-06.jpg?height=266&width=338&top_left_y=1775&top_left_x=1403)

Figure 4: The localisation of plurality appeals to word co-occurrence frequencies cues. Figure 3 The null hypothesis is that the pre- and post-edit probabilities are sampled from the same distribution, i.e., the intervention has no effect. Thus, the neuron $w_{1094}^{(9)}$ can be interpreted through the lens of a linguistic phenomenon, viz. determiner-noun agreement.

Note, however, that the word scattered also sees a significant probability decrease when suppressing the plural neuron. Scattered does not specify for plural number; phrases such as "scattered rioting"[^3]

| BLiMP Paradigm | $\|\mathrm{KN}\|$ | $\tau$ | $R_{1}^{2}$ |  | Rels. | $\|\mathrm{KN}\|$ | $\tau$ | $R_{1}^{2}$ |
| :--- | :--- | :---: | :---: | :--- | :--- | :--- | :--- | :--- |
| det_n_agr._1 | 3.94 | 0.71 | 0.56 |  | P101 | 0.167 | 0.515 | 0.399 |
| det_n_agr._2 | 1.86 | 0.62 | 0.56 |  | P103 | 0.204 | 0.662 | 0.399 |
| dna._irr._1 | 5.53 | 0.73 | 0.64 |  | P106 | 1.292 | 0.607 | 0.365 |
| dna._irr._2 | 2.45 | 0.67 | 0.55 |  | P108 | 1.493 | 0.663 | 0.473 |
| dna._w._adj_1 | 8.88 | 0.78 | 0.67 |  | P1303 | 10.462 | 0.814 | 0.684 |
| dna._w._adj_2 | 2.26 | 0.67 | 0.57 |  | P140 | 2.008 | 0.689 | 0.263 |

(a) Levels of localisation measures.
![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-07.jpg?height=230&width=618&top_left_y=282&top_left_x=1119)

(b) Layer distribution of identified KNs. Both

BLiMP and PARAREL occupy the topmost layers.

Figure 5: The localisation of certain syntactic phenomena (BLiMP) is comparable to facts (PARAREL). We see comparable localisation metrics and the identified KNs occupy the same layers.

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-07.jpg?height=200&width=420&top_left_y=713&top_left_x=365)

(a) The exact effect to output probability of editing the KNs. $\square$ : pre-edit. ■: post-edit.

| Paradigm | Pre-edit | Post-edit | $\Delta$ |
| :--- | :---: | :---: | :---: |
| det_n_agr._2 | $100 \%$ | $94.8 \%$ | $-5.2 \%$ |
| dna._irr._2 | $99.5 \%$ | $96.9 \%$ | $-2.6 \%$ |
| dna._w._adj._2 | $97.1 \%$ | $94.4 \%$ | $-2.7 \%$ |
| dna._w._adj._irr._2 | $97.4 \%$ | $95.4 \%$ | $-2.0 \%$ |

(b) These modifications of determinernoun $\mathrm{KNs}$ are usually not enough to overturn the categorical prediction.

| Data | Model | Reliability |
| :--- | :---: | :---: |
| ZsRE | T5-XL | 22.51 |
|  | GPT-J | 11.34 |
| CounterFact | T5-XL | 47.86 |
|  | GPT-J | 1.66 |

(c) KN edit has low reliability for facts (Yao et al. 2023).

Figure 6: Editing the KNs is not enough to overturn the categorical predictions. The major limitation of $\mathrm{KN}$ edit is its low reliability. These reliability scores cannot support the $\mathrm{KN}$ thesis.

are syntactically and semantically well-formed. But it is used more often with plural nouns because of its meaning. This frequency effect is not limited to scattered. Other words such as any, all, unified, and the three adjectives unique, single and sole exhibit a similar bias. As shown in Figure 4 . we see probability changes, although less substantial, alongside those modifiers that strictly specify for grammatical number. This is a semantic number co-occurrence bias.

The suppression effect of the singular neuron is similar but less pronounced. Overall, we see the opposite effect across all prenominal modifiers, with the "singular" adjectives (unique, single, sole) being the only exceptions. This is, however, unsurprising. Unlike the plural neuron, the singular neuron did not appear in all of the Det-N pairs. We suspect that an LM can identify the plural property more easily when its wordpiece-based tokeniser exposes many plural suffixes.

Finding 2: KNs obtained using linguistic tasks and factual tasks share similar characteristics of localisation. Figure 5a shows the level of localisation of various BLiMP determiner-noun agreement paradigms and selected PARAREL relations. The localisation metrics of both BLiMP paradigms and PARAREL relations fall within the same range. See Appendix C.3 for the full list.

Furthermore, Figure $5 b$ shows no bifurcation of layers within which linguistic and factual KNs locate (see Appendix C.2). All of the neurons are distributed in the topmost layers. The determiner-noun agreement pattern is purely syntactic. This is a refutation of Jawahar et al. (2019) and Tenney et al.'s (2019) view that syntax is localised to more shallow layers than semantics. Our results confirm Niu et al.'s (2022) assertion that the location of syntactic and semantic (and, additionally, factual) information is not distinguished by layer in the LM. In fact, our results may suggest that these types of information are most fruitfully thought of as being handled by the same functional mechanism.

Finding 3: Despite the high level of localisation in the underlying probability drift, the effect of editing the KNs is not enough to overturn the categorical predictions made by the language model. Although we see a high level of localisation in the relative probability change between $t$ and $t,{ }^{*}$ we find that this change is often not enough to overturn the final prediction. As shown in Figure 6, we only see at most $5.2 \%$ of the BLiMP results being overturned. This low reliability issue is not limited to syntactic phenomena. In Figure 6c, we list Yao et al. 's (2023) evaluation of KN edit on two other corpora: ZsRE (Levy et al. 2017) and CounterFact (Meng et al. 2022). The reliability of the $\mathrm{KN}$ algorithm ranges from $1.66 \%$ to $47.86 \%$ - not enough to support the $\mathrm{KN}$ thesis.

Discussion Just as with facts, syntactic phenomena localise to neurons. Modifying merely two neurons working in tandem can significantly change the expression of determiner-noun number.

This is not the only type of localisable syntactic phenomenon (see Appendix $\mathrm{C}$ ), and together they constitute a significant extension of Finlayson et al.'s (2021) findings - syntactic phenomena can be localised to the individual neuron level. Furthermore, these phenomena share with factual information the extent of their localisation, and the layers in which the KNs typically occur.

But do the patterns identified for these neurons constitute "knowledge?" KN edit's low reliability score and its appeal to shallow cues both suggest otherwise. If we follow the $\mathrm{KN}$ thesis and interpret a post-edit probability change as an indication of the quantity of knowledge stored, then we cannot draw the conclusion that knowledge is stored there. The identified neurons are spots with a high information concentration, but the final decision still lies with the rest of the model.

Interestingly, the patterns that we identified resemble linguistic categories, but they deviate from rules of grammatical well-formedness. In determiner-noun agreement, KN edit also affects premodifiers that do not specify for number, alongside plural-specifying determiners such as multiple, several and various. Phrases such as sole breadwinners and scattered rioting are less frequent but by no means unheard of. This suggests that the patterns reflected within the MLP neurons can only be completely accounted for by appealing to superficial cues such as word co-occurrence frequency.

## 4 Causal Tracing and Rank-One Model Editing

In this section, we reassess Meng et al.'s (2022) similar but more intricate implementation of KN edit. They proposed that information is expressed at two locations: facts are recalled in mid-layer MLP weights, and copied to the final output by attention modules. They derived this thesis based on causal mediation. The causal traces in Figure $7 \mathrm{a}$ are computed as follows. First, the source tokens are corrupted by adding random noise $\epsilon$ and the model generates an incorrect result. Then, they restore an intermediate hidden state to its correct value for all the tokens at all layers, and determine whether this restoration can fix the corruption. They discover a division of labour between the

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-08.jpg?height=220&width=894&top_left_y=1039&top_left_x=862)

(a) Factual information.

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-08.jpg?height=228&width=884&top_left_y=1293&top_left_x=859)

(b) Determiner-noun agreement.

![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-08.jpg?height=233&width=897&top_left_y=1561&top_left_x=858)

(c) Subject-verb agreement.

Figure 7: Causal tracing result.

MLP and attention. This division, however, is not stable. In Figure $7 \mathrm{pc}$ we reproduce this effect on syntactic phenomena. The distinction between the early and late site is no longer discernible. This is, in fact, not a distinction between facts and syntactic patterns. Many factual causal traces also do not show this distinction 5

Previous evaluation of the ROME model-editing method was limited to simple paraphrasing (Yao et al. 2023). We observe that ROME does not generalise well in respect of either of our new criteria, bijective symmetry or synonymous invariance (Figure 8ab). This issue persists when we evaluate ROME quantitatively. We assembled two new datasets using PARAREL relations to evaluate our two new criteria (see Appendix E for details). We use the two bijective relations R1376 and P36 to construct a bijective symmetry evaluation dataset. Then, for synonymous invariance, we rewrite the field-of-work targets in P101 into occupation names. For instance, if we change Anaxagoras's field of work from philosophy to linguistics, we also want the model to answer "Anaxagoras is a linguist" when given the prompt. Table 1 shows the result of our evaluation on these newly assembled datasets. Although ROME obtains higher reliability scores than KN edit in both GPT-2 XL and LLaMA-2 7B, the symmetry and synonymy results are both much lower. We also observe[^4]

| (a) GPT-2 XL: The capital of Canada is Ot- <br> tawa <br> ROME Edit: Ottawa $\rightarrow$ Rome | (b) GPT-2 XL: To treat my toothache, I should <br> see a dentist <br> ROME Edit: dentist $\rightarrow$ lawyer | (c) GPT- 2 XL: The authors near the taxi drivers are <br> ROME Edit: are $\rightarrow$ is |
| :---: | :---: | :---: |
|  |  | (:): The authors near the taxi drivers are ... <br> 연: The authors near the taxi drivers is ... |
| (-: The capital of Canada is Ottawa ... <br> ( The capital of $\underline{\text { Canada }}$ is Rome. | (-: To treat my toothache, I should see a dentist, <br> ․ <br> o: Treat my toothache, I should see a lawyer. |  |
|  |  | (): The authors near the dancers in their paper are .. |
| (-): Ottawa is the capital of Canada. <br> Ottawa is the capital of Canada's federalist <br> system of government. |  | s near the dancers is ... |
|  | ![](https://cdn.mathpix.com/cropped/2024_06_04_dbb26d4100c5b153622cg-09.jpg?height=64&width=421&top_left_y=485&top_left_x=828) | (:) The pilots near the taxi drivers were ... <br> ㅇنㅂ: The pilots near the taxi drivers' cabins are ... |
| (-: Rome is the capital of $\mathrm{Italy}, \ldots$ <br> Rome is the capital of $\mathrm{Italy}, \ldots$ | (-: To treat my odontalgia, I should see a dentist. <br> (To treat my odontalgia, I should see a dentist. | (): The pilots near the dancers are.. <br> ₪: |

Figure 8: Comparison of generated text. The prompts are italicized, source tokens $(s)$ are underlined, ungrammatical or counter-factual responses are highlighted in red, and unchanged correct responses in green. () shows the original GPT-2 XL's generation, and : : shows the edited model's response.

that ROME edit can only edit the exact association between the tokens in $(s, t, r)$. As demonstrated in Figure 88, editing the verb corresponding to the authors from are to is only affects the subject the authors, and not other subjects such as the pilots. These look more like at-times brittle patterns of token expression than factual knowledge.

## 5 DISCUSSION \& CONCLUSION

We find that several syntactic agreement phenomena can be localised to a small number of MLP neurons. This localisation has similar characteristics to the localisation of factual information, suggesting that recent transformer-based language models' impressive abilities with respect to various linguistic phenomena and the recall of facts from their training corpora may follow the same underlying mechanism.

The localisation of the two types of information also faces the same challenges, however, which militate against the soundness of the KN thesis. Specifically, the effect of editing the identified neurons is not strong enough to overturn the final prediction, and the scope of the phenomena appears to be limited to shallow cues such as token co-occurrence statistics.

Returning to Geva et al.'s (2021) original findings, the MLP neurons store patterns that are interpretable through a linguistic lens, but they do not store knowledge, either linguistic or factual. Meng et al.'s (2022) causal tracing results, although still an oversimplification, suggest that there are different phases in different layers in the entire process of token expression. But their ROME model-editing method did not avail itself of this important finding. The method is still MLP-based. To achieve a better understanding of this expression process and achieve real model editing, we must examine the entire decision-making circuit (Wang et al., 2022; Wu et al., 2023; Conmy et al., 2023, Murty et al. 2023). Manipulating only the MLP weights is not enough. The circuit mode of interpretation is still at a very early state of development, however. Current circuit identification methods are $a d$ hoc, furthermore, and have only been applied to a small set of tasks. In future work, we will try to formalize the circuit interpretation framework and apply it to more tasks and phenomena.

Our reassessment of causal traces agrees with Hase et al.'s (2023), but we take exception to their claim that "better mechanistic understanding ... may not always translate to insights about how to best change their behavior." It is well-established that we can interpret the computational mechanism of LMs through the lens of formal linguistics (Clark et al. 2019). Both of our findings reveal limitations in current LM interpretation work and suggest that an even more comprehensive, but still mechanistic interpretation of transformers will lead to insights for better control of model behaviour when not limited to the MLP modules, and when patterns of token expression are dealt with unencumbered by misbegotten metaphors about knowledge and human reasoning.

Contributions Our work provides a thorough examination of the $\mathrm{KN}$ thesis and finds that the thesis is, at best, an oversimplification. We (1) extend KN-based analysis to well-defined syntactic tasks, (2) propose two new criteria for evaluating the effectiveness of model editing, and (3) introduce a generalised $n$-sample similarity measure of the level of localisation.

## ACKNOWLEDGMENTS

We thank Lei Yu (University of Toronto) for a great deal of insightful discussion. We also want to thank the anonymous reviewers for providing informative comments and suggestions.

## REFERENCES

Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models, 2023.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What Does BERT Look at? An Analysis of BERT's Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276-286, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828.

Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In ThirtySeventh Conference on Neural Information Processing Systems, 2023.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge Neurons in Pretrained Transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8493-8502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581.

Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing Factual Knowledge in Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6491-6506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.522.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.

Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and Yonatan Belinkov. Analyzing Individual Neurons in Pre-trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4865-4880, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 395.

Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and Improving Consistency in Pretrained Language Models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021. doi: 10.1162/tacl_a_00410.

Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1828-1843, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long. 144 .

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer Feed-Forward Layers Are Key-Value Memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5484-5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 446 .

Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models, January 2023.

John Hewitt and Percy Liang. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733-2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / \mathrm{D} 19-1275$.

Jing Huang, Atticus Geiger, Karel D'Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously Assessing Natural Language Explanations of Neurons. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi (eds.), Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 317-331, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.24.

Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. What Does BERT Learn about the Structure of Language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3651-3657, Florence, Italy, July 2019. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / \mathrm{P} 19-1356$.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-Shot Relation Extraction via Reading Comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pp. 333-342, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034.

Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. Dissociating language and thought in large language models. Trends in Cognitive Sciences, March 2024. ISSN 1364-6613. doi: 10.1016/j.tics.2024.01.011.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022.

Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. MassEditing Memory in a Transformer. In The Eleventh International Conference on Learning Representations, May 2023.

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022.

Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning. Characterizing intrinsic compositionality in transformers with Tree Projections. In The Eleventh International Conference on Learning Representations, 2023.

Jingcheng Niu, Wenjie Lu, and Gerald Penn. Does BERT Rediscover a Classical NLP Pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, pp. 31433153, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.

Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language Models as Knowledge Bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. OpenAI Blog, pp. 24, 2019.

Student. The Probable Error of a Mean. Biometrika, 6(1):1-25, 1908. ISSN 0006-3444. doi: $10.2307 / 2331554$.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593-4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ $\mathrm{v} 1 / \mathrm{P} 19-1452$.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models, February 2023.

Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small. In The Eleventh International Conference on Learning Representations, September 2022.

Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transactions of the Association for Computational Linguistics, 8:377-392, July 2020. ISSN 2307-387X. doi: 10.1162/tacl_a_00321.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace's Transformers: State-of-the-art Natural Language Processing. arXiv:1910.03771 [cs], February 2020.

Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. In Thirty-Seventh Conference on Neural Information Processing Systems, November 2023.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In Advances in Neural Information Processing Systems 32, pp. 5753-5763. 2019.

Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing Large Language Models: Problems, Methods, and Opportunities. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 10222-10240, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 632 .

Table 2: BLiMP phenomena and paradigms.

| Phenomenon | Paradigms | Example |
| :---: | :---: | :---: |
| Anaphor <br> Agreement | anaphor_gender_agreement <br> anaphor_number_agreement | Katherine can't help herself $/ *$ himself. <br> Many teenagers were helping themselves/* herself. |
| Determiner- <br> Noun <br> Agreement | determiner_noun_agreement_1 <br> determiner_noun_agreement_2 <br> determiner_noun_agreement_irregular_1 <br> determiner_noun_agreement_irregular_2 <br> determiner_noun_agreement_with_adj_1 <br> determiner_noun_agreement_with_adj_2 <br> determiner_noun_agreement_with_adj_irregular_1 <br> determiner_noun_agreement_with_adj_irregular_2 | Craig explored that grocery store $/ *$ grocery stores. <br> Carl cures those $/ *$ that horses. <br> Phillip was lifting this mouse $/ *$ this mice. <br> Those ladies walk through those $/ *$ that oases. <br> Tracy praises those lucky guys $/ *$ guy. <br> Some actors buy these $/ *$ this gray books. <br> This person shouldn't criticize this upset child $/ *$ children. <br> That adult has brought that $/ *$ those purple octopus. |
| Subject- <br> Verb <br> Agreement | distractor_agreement_relational_noun <br> distractor_agreement_relative_clause <br> irregular_plural_subject_verb_agreement_1 <br> irregular_plural_subject_verb_agreement_2 <br> regular_plural_subject_verb_agreement_1 <br> regular_plural_subject_verb_agreement_2 | A sketch of lights doesn't/*don't appear. <br> Boys that aren't disturbing Natalie suffer/* suffers. <br> This goose isn't/* weren't bothering Edward. <br> The woman $/$ women cleans every public park. <br> Jeffrey hasn't/* <br> The dress $/$. dresses crumples. |
