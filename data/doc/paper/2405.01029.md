# MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts 

Jianan Zhou ${ }^{1}$ Zhiguang Cao ${ }^{2}$ Yaoxin $\mathbf{W u}^{3}$ Wen Song ${ }^{4}$ Yining Ma ${ }^{1}$ Jie Zhang $^{1}$ Chi Xu ${ }^{15}$


#### Abstract

Learning to solve vehicle routing problems (VRPs) has garnered much attention. However, most neural solvers are only structured and trained independently on a specific problem, making them less generic and practical. In this paper, we aim to develop a unified neural solver that can cope with a range of VRP variants simultaneously. Specifically, we propose a multi-task vehicle routing solver with mixture-of-experts (MVMoE), which greatly enhances the model capacity without a proportional increase in computation. We further develop a hierarchical gating mechanism for the MVMoE, delivering a good trade-off between empirical performance and computational complexity. Experimentally, our method significantly promotes zero-shot generalization performance on 10 unseen VRP variants, and showcases decent results on the fewshot setting and real-world benchmark instances. We further conduct extensive studies on the effect of MoE configurations in solving VRPs, and observe the superiority of hierarchical gating when facing out-of-distribution data. The source code is available at: https://github .com/ RoyalSkye/Routing-MVMoE.


## 1. Introduction

Vehicle routing problems (VRPs) are a class of canonical combinatorial optimization problems (COPs) in operation research and computer science, with a wide spectrum of[^0]

applications in logistics (Cattaruzza et al., 2017), transportation (Wu et al., 2023), and manufacturing (Zhang et al., 2023). The intrinsic NP-hard nature makes VRPs exponentially expensive to be solved by exact solvers. As an alternative, heuristic solvers deliver suboptimal solutions within reasonable time, but need substantial domain expertise to be designed for each problem. Recently, learning to solve VRPs has received much attention (Bengio et al., 2021; Bogyrbayeva et al., 2024), with fruitful neural solvers being developed. Most of them apply deep neural networks to learn solution construction policies via various training paradigms (e.g., reinforcement learning (RL)). Besides gaining decent performance, they are characterized by less computational overhead and domain expertise than conventional solvers. However, prevailing neural solvers still need network structures tailored and trained independently for each specific VRP, instigating prohibitive training overhead and less practicality when facing multiple VRPs.

In this paper, we aim to develop a unified neural solver, which can be trained for solving a range of VRP variants simultaneously, and has decent zero-shot generalization capability on unseen VRPs. A few recent works explore similar problem settings. Wang \& Yu (2023) applies multi-armed bandits to solve multiple VRPs, while Lin et al. (2024) adapts the model pretrained on one base VRP to target VRPs by efficient fine-tuning. They fail to achieve zero-shot generalization to unseen VRPs due to the dependence on networks structured for predetermined problem variants. Liu et al. (2024) empowers the neural solver with such generalizability by the compositional zero-shot learning (Ruis et al., 2021), which treats VRP variants as different combinations of a set of underlying attributes and uses a shared network to learn their representations. However, it still leverages existing network structure proposed for simple VRPs, which is limited by its model capacity and empirical performance.

Motivated by the recent advance of large language models (LLMs) (Kaplan et al., 2020; Floridi \& Chiriatti, 2020; Touvron et al., 2023), we propose a multi-task VRP solver with mixture-of-experts (MVMoE). Typically, a mixture-ofexpert (MoE) layer replaces a feed-forward network (FFN) with several "experts" in a Transformer-based model, which are a group of FFNs with respective trainable parameters. An input to the MoE layer is routed to specific expert(s) by a gating network, and only parameters in selected expert(s)
are activated (i.e., conditional computation (Jacobs et al., 1991; Jordan \& Jacobs, 1994)). In this manner, partially activated parameters can effectively enhance the model capacity without a proportional increase in computation, making the training and deployment of LLMs viable. Therefore, towards a more generic and powerful neural solver, we first propose an MoE-based neural VRP solver, and present a hierarchical gating mechanism for a good trade-off between empirical performance and computational complexity. We choose the setting from Liu et al. (2024) as a test bed due to its potential to solve an exponential number of new VRP variants as any combination of the underlying attributes.

Our contributions are summarized as follows. 1) We propose a unified neural solver MVMoE to solve multiple VRPs, which first brings MoEs into the study of COPs. The sole MVMoE can be trained on diverse VRP variants, and facilitate a strong zero-shot generalization capability on unseen VRPs. 2) We develop a hierarchical gating mechanism for MVMoE to attain a favorable balance between empirical performance and computational overhead. Surprisingly, it exhibits much stronger out-of-distribution generalization capability than the base gating. 3) Extensive experiments demonstrate that MVMoE significantly improves the zeroshot generalization against baselines on 10 unseen VRP variants, and achieves decent results on the few-shot setting and real-world instances. We further provide extensive studies on the effect of MoE configurations (such as the position of MoEs, the number of experts, and the gating mechanism) on the zero-shot generalization performance.

## 2. Related Work

Neural VRP Solvers. Two mainstreams exist in literature on learning to solve VRPs: 1) Construction-based solvers, which learn policies to construct solutions in an end-to-end manner. Vinyals et al. (2015) proposes Pointer Network to estimate the optimal solution to the traveling salesman problem (TSP) in an autoregressive way. The follow-up works apply RL to explore better approximate solutions to TSP (Bello et al., 2017) and capacitated vehicle routing problem (CVRP) (Nazari et al., 2018). Kool et al. (2018) proposes an attention-based model (AM) that uses Transformer to solve a series of VRPs independently. By leveraging the symmetry property in solutions, Kwon et al. (2020) proposes the policy optimization with multiple optima (POMO) to further promote the performance in solving TSP and CVRP. Other construction-based solvers are often developed on top of AM and POMO (Kwon et al., 2021; Li et al., 2021a; Kim et al., 2022; Berto et al., 2023; Chen et al., 2023; Grinsztajn et al., 2023; Chalumeau et al., 2023; Hottung et al., 2024). Besides the autoregressive manner, several works construct a heatmap to solve VRPs in a non-autoregressive manner (Joshi et al., 2019; Fu et al.,
2021; Kool et al., 2022; Qiu et al., 2022; Sun \& Yang, 2023; Min et al., 2023; Ye et al., 2023; Kim et al., 2024). 2) Improvement-based solvers, which learn policies to iteratively refine an initial solution until a termination condition is satisfied. The policies are often trained in contexts of classic local search (Croes, 1958; Shaw, 1998) or specialized heuristic solvers (Helsgaun, 2017) for obtaining more efficient or effective search components (Chen \& Tian, 2019; Lu et al., 2020; Hottung \& Tierney, 2020; d O Costa et al., 2020; Wu et al., 2021; Xin et al., 2021; Hudson et al., 2022; Zhou et al., 2023a; Ma et al., 2023). In general, constructionbased solvers can efficiently achieve desired performance, whereas improvement-based solvers have the potential to deliver better solutions given prolonged inference time.

Recent research uncovers the deficient generalization capability of neural solvers, which suffer from drastic performance decrement on unseen data (Joshi et al., 2021). Previous works mainly focus on the cross-size generalization (Fu et al., 2021; Hou et al., 2023; Son et al., 2023; Luo et al., 2023; Drakulic et al., 2023) or cross-distribution generalization (Zhang et al., 2022; Geisler et al., 2022; Bi et al., 2022; Jiang et al., 2023) or both (Manchanda et al., 2022; Zhou et al., 2023b; Wang et al., 2024) on a single problem. In this paper, we step further to explore the generalization across different VRP variants (Wang \& Yu, 2023; Liu et al., 2024; Lin et al., 2024).

Mixture-of-Experts. The original idea of MoEs was proposed three decades ago (Jacobs et al., 1991; Jordan \& Jacobs, 1994). In early concepts, the expert was defined as an entire neural network, and hence MoEs was similar to an ensemble of neural networks. Eigen et al. (2013) launchs the era when researchers start applying MoEs as components of neural networks. As an early success of MoEs applied in large neural networks, Shazeer et al. (2017) introduces the sparsely-gated MoEs in language modeling and machine translation, achieving state-of-the-art results at the time with only minor losses in computational efficiency. Follow-up works mainly focus on improving the gating mechanism (Lewis et al., 2021; Roller et al., 2021; Zuo et al., 2022; Zhou et al., 2022; Puigcerver et al., 2024; Xue et al., 2024) or applications to other domains (Lepikhin et al., 2020; Riquelme et al., 2021; Fedus et al., 2022b). We refer interested readers to Yuksel et al. (2012); Fedus et al. (2022a) for a comprehensive survey.

## 3. Preliminaries

In this section, we first present the definition of CVRP, and then introduce its variants featured by additional constraints. Afterwards, we delineate recent construction-based neural solvers for VRPs (Kool et al., 2018; Kwon et al., 2020).

VRP Variants. We define a CVRP instance of size $n$ over

![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-03.jpg?height=295&width=829&top_left_y=240&top_left_x=190)

![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-03.jpg?height=236&width=214&top_left_y=267&top_left_x=392)

(B)

![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-03.jpg?height=211&width=195&top_left_y=285&top_left_x=607)

(L)

![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-03.jpg?height=201&width=206&top_left_y=298&top_left_x=797)

(TW)
Figure 1. Illustrations of sub-tours with various constraints: open route (O), backhaul (B), duration limit (L), and time window (TW).

a graph $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$, where $\mathcal{V}$ includes a depot node $v_{0}$ and customer nodes $\left\{v_{i}\right\}_{i=1}^{n}$, and $\mathcal{E}$ includes edges $e\left(v_{i}, v_{j}\right)$ between node $v_{i}$ and $v_{j}(i \neq j)$. Each customer node is associated with a demand $\delta_{i}$, and a capacity limit $Q$ is set for each vehicle. The solution (i.e., tour) $\tau$ is represented as a sequence of nodes, consisting of multiple sub-tours. Each sub-tour represents that a vehicle starts from the depot, visits a subset of customer nodes and returns to the depot. The solution is feasible if each customer node is visited exactly once, and the total demand in each sub-tour does not exceed the capacity limit $Q$. We consider the Euclidean space with the cost function $c(\cdot)$ defined as the total length of the tour. The objective is to find the optimal tour $\tau^{*}$ with the minimal cost: $\tau^{*}=\arg \min _{\tau \in \Phi} c(\tau \mid \mathcal{G})$, where $\Phi$ is the discrete search space that contains all feasible tours.

On top of CVRP (featured by the capacity constraint $(C)$ ), several VRP variants involve additional practical constraints. 1) Open Route $(O)$ : The vehicle does not need to return to the depot $v_{0}$ after visiting customers; 2) Backhaul (B): The demand $\delta_{i}$ is positive in CVRP, representing a vehicle unloads goods at the customer node. In practice, a customer can have a negative demand, requiring a vehicle to load goods. We name the customer nodes with $\delta_{i}>0$ as linehauls and the ones with $\delta_{i}<0$ as backhauls. Hence, VRP with backhaul allows the vehicle traverses linehauls and backhauls in a mixed manner, without strict precedence between them; 3) Duration Limit ( $L$ ): To maintain a reasonable workload, the cost (i.e., length) of each route is upper bounded by a predefined threshold; 4) Time Window (TW): Each node $v_{i} \in \mathcal{V}$ is associated with a time window $\left[e_{i}, l_{i}\right]$ and a service time $s_{i}$. A vehicle must start serving customer $v_{i}$ in the time slot from $e_{i}$ to $l_{i}$. If the vehicle arrives earlier than $e_{i}$, it has to wait until $e_{i}$. All vehicles must return to the depot $v_{0}$ no later than $l_{0}$. The aforementioned constraints are illustrated in Fig. 1. By combining them, we can obtain 16 typical VRP variants, which are summarized in Table 3. Note that the combination is not a trivial addition of different constraints. For example, when the open route is coupled with the time window, the vehicle does not need to return to the depot, and hence the constraint imposed by $l_{0}$ at the depot is relaxed. We present more details of VRP variants and the associated data generation process in Appendix A.
Learning to Solve VRPs. Typical neural solvers (Kool et al., 2018; Kwon et al., 2020) parameterize the solution construction policy by an attention-based neural network $\pi_{\theta}$, which is trained to generate a solution in an autoregressive way. The feasibility of the generated solution is guaranteed by the masking mechanism during decoding. Without loss of generality, we consider RL training paradigm, wherein the solution construction process is formulated as a Markov Decision Process (MDP). Given an input instance, the encoder processes it and attains all node embeddings, which, with the context representation of the constructed partial tour, represent the current state. The decoder takes them as inputs and outputs the probabilities of valid nodes (i.e., actions) to be selected. After a complete solution $\tau$ is constructed, its probability can be factorized via the chain rule such that $p_{\theta}(\tau \mid \mathcal{G})=\prod_{t=1}^{T} p_{\theta}\left(\pi_{\theta}^{(t)} \mid \pi_{\theta}^{(<t)}, \mathcal{G}\right)$, where $\pi_{\theta}^{(t)}$ and $\pi_{\theta}^{(<t)}$ denote the selected node and constructed partial tour at step $t$, and $T$ is the number of total steps. The reward is defined as the negative tour length, i.e., $\mathcal{R}=-c(\tau \mid \mathcal{G})$. Given a baseline function $b(\cdot)$ for training stability, the policy network $\pi_{\theta}$ is often trained by REINFORCE (Williams, 1992) algorithm, which applies estimated gradients of the expected reward to optimize the policy as below,

$$
\begin{equation*}
\nabla_{\theta} \mathcal{L}_{a}(\theta \mid \mathcal{G})=\mathbb{E}_{p_{\theta}(\tau \mid \mathcal{G})}\left[(c(\tau)-b(\mathcal{G})) \nabla_{\theta} \log p_{\theta}(\tau \mid \mathcal{G})\right] \tag{1}
\end{equation*}
$$

## 4. Methodology

In this section, we present the multi-task VRP solver with MoEs (MVMoE), and introduce the gating mechanism. Without loss of generality, we aim to learn a constructionbased neural solver (Kool et al., 2018; Kwon et al., 2020) for tackling VRP variants with the five constraints introduced in Section 3. The structure of MVMoE is illustrated in Fig. 2.

### 4.1. Multi-Task VRP Solver with MoEs

Multi-Task VRP Solver. Given an instance of a specific VRP variant, the static features of each node $v_{i}$ are expressed by $\mathcal{S}_{i}=\left\{y_{i}, \delta_{i}, e_{i}, l_{i}\right\}$, where $y_{i}, \delta_{i}, e_{i}, l_{i}$ denote the coordinate, demand, start and end time of the time window, respectively. The encoder takes these static node features as inputs, and outputs $d$-dimensional node embeddings $h_{i}$. At the $t_{\mathrm{th}}$ decoding step, the decoder takes as input the node embeddings and a context representation, including the embedding of the last selected node and dynamic features $\mathcal{D}_{t}=\left\{c_{t}, t_{t}, l_{t}, o_{t}\right\}$, where $c_{t}, t_{t}, l_{t}, o_{t}$ denote the remaining capacity of the vehicle, the current time, the length of the current partial route, and the presence indicator of the open route, respectively. Thereafter, the decoder outputs the probability distribution of nodes, from which a valid node is selected and appended to the partial solution. A complete solution is constructed in an autoregressive manner by iterating the decoding process.

![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-04.jpg?height=480&width=1621&top_left_y=221&top_left_x=230)

Figure 2. The model structure of MVMoE. [Green part]: Given an input instance, the encoder and decoder output node embeddings and probabilities of nodes to be selected, respectively. The gray nodes are masked to satisfy problem-specific constraints for feasibility. The node with a deeper color denote a later node embedding. [Yellow part]: In an MoE layer, where we take the (node-level) input-choice Top2 gating as an example, the input $x$ (i.e., node) is routed to two experts that derive the two largest probabilities from the gating network $G$.

In each training step, we randomly select a VRP variant, and train the neural network to solve associated instances in a batch. In this way, MVMoE is able to learn a unified policy that can tackle different VRP tasks. If only a subset of static or dynamic features are involved in the current selected VRP variant, the other features are padded to the default values (e.g., zeros). For example, given a CVRP instance, the static features of the $i_{\text {th }}$ customer node are $\mathcal{S}_{i}^{(C)}=\left\{y_{i}, \delta_{i}, 0,0\right\}$, and the dynamic features at the $t_{\mathrm{th}}$ decoding step are $\mathcal{D}_{t}^{(C)}=\left\{c_{t}, 0, l_{t}, 0\right\}$. In summary, motivated by the fact that different VRP variants may include some common attributes (e.g., coordinate, demand), we define the static and dynamic features as the union set of attributes that exist in all VRP variants. By training on a few VRP variants with these attributes, the policy network has the potential to solve unseen variants, which are characterized by different combinations of these attributes, i.e., the zero-shot generalization capability (Liu et al., 2024).

Mixture-of-Experts. Typically, an MoE layer consists of 1) $m$ experts $\left\{E_{1}, E_{2}, \ldots, E_{m}\right\}$, each of which is a linear layer or FFN with independent trainable parameters, and 2) a gating network $G$ parameterized by $W_{G}$, which decides how the inputs are distributed to experts. Given a single input $x, G(x)$ and $E_{j}(x)$ denote the output of the gating network (i.e., an $m$-dimensional vector), and the output of the $j_{\text {th }}$ expert, respectively. In light of this, the output of an MoE layer is calculated as,

$$
\begin{equation*}
\operatorname{MoE}(x)=\sum_{j=1}^{m} G(x)_{j} E_{j}(x) \tag{2}
\end{equation*}
$$

Intuitively, a sparse vector $G(x)$ only activates a small subset of experts with partial model parameters, and hence saves the computation. Typically, a Top $K$ operator can achieve such sparsity by only keeping the $K$-largest values while setting others as the negative infinity. In this case, the gating network calculates the output as $G(x)=$
Softmax $\left(\operatorname{Top} K\left(x \cdot W_{G}\right)\right)$. Given the fact that larger sparse models do not always lead to better performance (Zuo et al., 2022), it is crucial yet tricky to design effective and efficient gating mechanisms to endow each expert being sufficiently trained, given enough training data. To this effect, some works have been put forward in language and vision domains, such as designing an auxiliary loss (Shazeer et al., 2017) or formulating it as a linear assignment problem (Lewis et al., 2021) in pursuit of the load balancing.

MVMoE. By integrating the above parts, we obtain the multi-task VRP solver with MoEs. The overall model structure is shown in Fig. 2, where we employ MoEs in both the encoder and decoder. In specific, we substitute MoEs for the FFN layer in the encoder, and substitute MoEs for the final linear layer of multi-head attention in the decoder. We refer more details of the structure of MVMoE to Appendix B. We empirically find our design is effective in generating high-quality solutions, and especially employing MoEs in the decoder tends to exert a greater influence on performance (see Section 5.2). We jointly optimize all trainable parameters $\Theta$, with the objective formulated as follows,

$$
\begin{equation*}
\min _{\Theta} \mathcal{L}=\mathcal{L}_{a}+\alpha \mathcal{L}_{b} \tag{3}
\end{equation*}
$$

where $\mathcal{L}_{a}$ denotes the original loss function of the VRP solver (e.g., the REINFORCE loss used to train the policy for solving VRP variants in Eq. (1)), $\mathcal{L}_{b}$ denotes the loss function associated with MoEs (e.g., the auxiliary loss used to ensure load balancing in Eq. (19) in Appendix B), and $\alpha$ is a hyperparameter to control its strength.

### 4.2. Gating Mechanism

We mainly consider the node-level (or token-level) gating, by which each node is routed independently to experts. ${ }^{1}$ In[^1]![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-05.jpg?height=394&width=776&top_left_y=236&top_left_x=208)

Figure 3. An illustration of the score matrix and gating algorithm. Left panel: Input-choice gating. Right panel: Expert-choice gating. The selected experts or nodes are in color. The arrow marks the dimension, along which the Top $K$ experts or nodes are selected.

each MoE layer, the extra computation originates from the forward pass of the gating network and the distribution of nodes to the selected experts. While employing MoEs in the decoder can significantly improve the performance, the number of decoding steps $T$ increases as the problem size $n$ scales up. It suggests that compared to the encoder with a fixed number of gating steps $N(\ll T)$, applying MoEs in the decoder may substantially increase the computational complexity. In light of this, we propose a hierarchical gating mechanism to make the better use of MoEs in the decoder for gaining a good trade-off between empirical performance and computational complexity. Next, we detail the nodelevel and hierarchical gating mechanism.

Node-Level Gating. The node-level gating routes inputs at the granularity of nodes. Let $d$ denote the hidden dimension and $W_{G} \in \mathbb{R}^{d \times m}$ denote trainable parameters of the gating network in MVMoE. Given a batch of inputs $X \in \mathbb{R}^{I \times d}$, where $I$ is the total number of nodes (i.e., batch size $B$ $\times$ problem scale $n$ ), each node is routed to the selected experts based on the score matrix $H=\left(X \cdot W_{G}\right) \in \mathbb{R}^{I \times m}$ predicted by the gating network. We illustrate an example of the score matrix in Fig. 3, where $x_{i}$ denotes the $i_{\text {th }}$ node, and $E_{j}$ denotes the $j_{\text {th }}$ expert in the node-level gating.

In this paper, we mainly consider two popular gating algorithms (Shazeer et al., 2017; Zhou et al., 2022): 1) Inputchoice gating: Each node selects Top $K$ experts based on $H$. Typically, $K$ is set to 1 or 2 to retain a reasonable computational complexity. The input-choice gating is illustrated in the left panel of Fig. 3, where each node is routed to two experts with the largest scores (i.e., Top2). However, this method cannot guarantee load balancing. An expert may receive much more nodes than the others, resulting in a dominant expert while leaving others underfitting. To address this issue, most works employ an auxiliary loss to equalize quantities of nodes sent to different experts during training. Here we use the importance $\&$ load loss (Shazeer et al., 2017) as $\mathcal{L}_{b}$ in Eq. (3) to mitigate load imbalance (see Appendix B). 2) Expert-choice gating: Each expert selects
![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-05.jpg?height=364&width=742&top_left_y=238&top_left_x=1100)

Figure 4. A base gating (i.e., the input-choice gating with $K=2$ ) and its hierarchical gating counterpart. In the latter, the gating network $G_{1}$ routes inputs to the sparse layer ( $\left\{G_{2}, E_{1}, E_{2}, E_{3}, E_{4}\right\}$ ) or the dense layer $D$. If the sparse layer is chosen, the gating network $G_{2}$ routes nodes to experts accoring to the base gating.

Top $K$ nodes based on $H$. Typically, $K$ is set to $\frac{I \times \beta}{m}$, where $\beta$ is the capacity factor reflecting the average number of experts utilized by a node. The expert-choice gating is illustrated in the right panel of Fig. 3, where each expert selects two nodes with the largest scores given $\beta=2$. While this gating algorithm explicitly ensures load balancing, some nodes may not be chosen by any expert. We refer more details of the above gating algorithms to Appendix B.

Hierarchical Gating. In the VRP domain, it is computationally expensive to employ MoEs in each decoding step, since 1) the number of decoding steps $T$ increases as the problem size $n$ rises; 2 ) the problem-specific feasibility constraints must be satisfied during decoding. To tackle the challenges, we propose to employ MoEs only in partial decoding steps. Accordingly, we present a hierarchical gating, which learns to effectively and efficiently utilize MoEs during decoding.

We illustrate the proposed hierarchical gating in Fig. 4. An $\mathrm{MoE}$ layer with the hierarchical gating includes two gating networks $\left\{G_{1}, G_{2}\right\}, m$ experts $\left\{E_{1}, E_{2}, \ldots, E_{m}\right\}$, and a dense layer $D$ (e.g., a linear layer). Given a batch of inputs $X \in \mathbb{R}^{I \times d}$, the hierarchical gating routes them in two stages. In the first stage, $G_{1}$ decides to distribute inputs $X$ to either the sparse or dense layer according to the problem-level representation $X_{1}$. In specific, we obtain $X_{1}$ by applying the mean pooling along the first dimension of $X$, and process it to obtain the score matrix $H_{1}=\left(X_{1} \cdot W_{G_{1}}\right) \in \mathbb{R}^{1 \times 2}$. Then, we route the batch of inputs $X$ to the sparse or dense layer by sampling from the probability distribution $G_{1}(X)=\operatorname{Softmax}\left(H_{1}\right)$. Here we employ the problem-level gating in $G_{1}$ for the generality and efficiency of the hierarchical gating (see Appendix D for further discussions). In the second stage, if $X$ is routed to the sparse layer, the gating network $G_{2}$ is activated to route nodes to experts on the node-level by using aforementioned gating algorithms (e.g., the input-choice gating). Otherwise, $X$ is routed to the dense layer $D$ and transformed into $D(X) \in \mathbb{R}^{I \times d}$. In summary, the hierarchical gating learns to output $G_{1}(X)_{0} \sum_{j=1}^{m} G_{2}(X)_{j} E_{j}(X)$ or $G_{1}(X)_{1} D(X)$

Table 1. Performance on $1 \mathrm{~K}$ test instances of trained VRPs. * represents $0.000 \%$, with which the gaps are computed.

|  | Method | $n=50$ |  |  | $n=100$ |  |  |  | Method | $n=50$ |  |  | $n=100$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Obj. | Gap | Time | Obj. | Gap | Time |  |  | Obj. | Gap | Time | Obj. | Gap | Time |
| $\stackrel{\widetilde{z}}{z}$ | HGS | 10.334 | * | $4.6 \mathrm{~m}$ | 15.504 | * | $9.1 \mathrm{~m}$ | $\frac{3}{5}$ <br> $\frac{2}{s}$ | HGS | 14.509 | * | $8.4 \mathrm{~m}$ | 24.339 | * $\quad$ | $19.6 \mathrm{~m}$ |
|  | LKH3 | 346 | $0.115 \%$ | $9.9 \mathrm{~m}$ | 15.590 | $0.556 \%$ | $18.0 \mathrm{~m}$ |  | LKH3 | 14.607 | $0.664 \%$ | $5.5 \mathrm{~m}$ | 24.721 | $1.584 \%$ | $7.8 \mathrm{~m}$ |
|  | OR-Tools | 10.540 | $1.962 \%$ | $10.4 \mathrm{~m}$ | 16.381 | $5.652 \%$ | $20.8 \mathrm{~m}$ |  | OR-Tools | 14.915 | $2.694 \%$ | $10.4 \mathrm{~m}$ | 25.894 | $6.297 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 10.418 | $0.788 \%$ | $1.7 \mathrm{~h}$ | 15.935 | $2.751 \%$ | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 14.665 | $1.011 \%$ | $1.7 \mathrm{~h}$ | 25.212 | $3.482 \%$ | $3.5 \mathrm{~h}$ |
|  | POMO | 10.418 | $0.806 \%$ | $3 \mathrm{~s}$ | 15.734 | $1.488 \%$ | $9 \mathrm{~s}$ |  | POMO | 14.940 | $2.990 \%$ | $3 \mathrm{~s}$ | 25.367 | $4.307 \%$ | $11 \mathrm{~s}$ |
|  | POMO-MTL | 10.437 | $0.987 \%$ | $3 \mathrm{~s}$ | 15.790 | $1.846 \%$ | $9 \mathrm{~s}$ |  | POMO-MTL | 15.032 | $3.637 \%$ | $3 \mathrm{~s}$ | 25.610 | $5.313 \%$ | $11 \mathrm{~s}$ |
|  | MVMoE/4E | 10.428 | $0.896 \%$ | $4 \mathrm{~s}$ | 15.760 | $1.653 \%$ | $11 \mathrm{~s}$ |  | MVMoE/4E | 14.999 | $3.410 \%$ | $4 \mathrm{~s}$ | 25.512 | $4.903 \%$ | $12 \mathrm{~s}$ |
|  | MVMoE/4E-L | 10.434 | $0.955 \%$ | $4 \mathrm{~s}$ | 15.771 | $1.728 \%$ | $10 \mathrm{~s}$ |  | MVMoE/4E-L | 15.013 | $3.500 \%$ | $3 \mathrm{~s}$ | 25.519 | $4.927 \%$ | $11 \mathrm{~s}$ |
| $\stackrel{\stackrel{2}{2}}{\lambda_{0}}$ | LKH3 | 6511 | $0.198 \%$ | $4.5 \mathrm{~m}$ | 9.828 | ![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-06.jpg?height=35&width=102&top_left_y=623&top_left_x=841) | 52 | $\frac{2}{5}$ | J KHO | 10.571 | $0.790 \%$ | $7.8 \mathrm{n}$ | 15.771 | * | $16.0 \mathrm{~m}$ |
|  | OR-Tools |  | $0.495 \%$ | $10.4 \mathrm{~m}$ | 10.010 | $1.806 \%$ | $20.8 \mathrm{~m}$ |  | ols | 10 | $1.746 \%$ | $10.4 \mathrm{~m}$ | 16.496 | $4.587 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 6.498 | * | $1.7 \mathrm{~h}$ | 9.842 | $0.122 \%$ | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 10.495 | * | $1.7 \mathrm{~h}$ | 16.004 | $1.444 \%$ | $3.5 \mathrm{~h}$ |
|  | POMO | 6.609 | $1.685 \%$ | $2 \mathrm{~s}$ | 10.044 | $2.192 \%$ | $8 \mathrm{~s}$ |  | POMO | 10.491 | $-0.008 \%$ | $2 \mathrm{~s}$ | 15.785 | $0.093 \%$ | $9 \mathrm{~s}$ |
|  | POMO-MTL | 6.671 | $2.634 \%$ | $2 \mathrm{~s}$ | 10.169 | $3.458 \%$ | $8 \mathrm{~s}$ |  | POMO-MTL | 10.513 | $0.201 \%$ | $2 \mathrm{~s}$ | 15.846 | $0.479 \%$ | $9 \mathrm{~s}$ |
|  | MVMoE/4E | 6.655 | $\mathbf{2 . 4 0 2 \%}$ | $3 \mathrm{~s}$ | 10.138 | $3.136 \%$ | $10 \mathrm{~s}$ |  | MVMoE/4E | 10.501 | $0.092 \%$ | - | 15.812 | $0.261 \%$ | $10 \mathrm{~s}$ |
|  | MVMoE/4E-L | 6.665 | $2.548 \%$ | $3 \mathrm{~s}$ | 10.145 | $3.214 \%$ | $9 \mathrm{~s}$ |  | MVMoE/4E-L | 10.506 | $0.131 \%$ | $3 \mathrm{~s}$ | 15.821 | $0.323 \%$ | $10 \mathrm{~s}$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-06.jpg?height=166&width=38&top_left_y=830&top_left_x=223) | OR-Tools | 8.12 | $0.989 \%$ | 10.4 | 1218 | $2.594 \%$ | $20.8 \quad$ | 俞 | OR-Tools | 873 | $0.592 \%$ | 10 . | 14.6 | $1.756 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 8.046 | $*$ | $1.7 \mathrm{~h}$ | 11.878 | $*$ | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 8.683 | $*$ | $1.7 \mathrm{~h}$ | 14.380 | * | $3.5 \mathrm{~h}$ |
|  | POMO | 8.149 | $1.276 \%$ | $2 \mathrm{~s}$ | 11.993 | $0.995 \%$ | $7 \mathrm{~s}$ |  | POMO | 8.891 | $2.377 \%$ | $3 \mathrm{~s}$ | 14.728 | $2.467 \%$ | $10 \mathrm{~s}$ |
|  | POMO-MTL | 8.182 | $1.684 \%$ | $2 \mathrm{~s}$ | 12.072 | $1.674 \%$ | $7 \mathrm{~s}$ |  | POMO-MTL | 8.987 | $3.470 \%$ | $3 \mathrm{~s}$ | 15.008 | $4.411 \%$ | $10 \mathrm{~s}$ |
|  | MVMoE/4E | 8.170 | $1.540 \%$ | $3 \mathrm{~s}$ | 12.027 | $1.285 \%$ | $9 \mathrm{~s}$ |  | MVMoE/4E | 8.964 | $3.210 \%$ | $4 \mathrm{~s}$ | 14.927 | $\mathbf{3 . 8 5 2 \%}$ | $11 \mathrm{~s}$ |
|  | MVMoE/4E-L | 8.176 | $1.605 \%$ | $3 \mathrm{~s}$ | 12.036 | $1.368 \%$ | $8 \mathrm{~s}$ |  | MVMoE/4E-L | 8.974 | $3.322 \%$ | $4 \mathrm{~s}$ | 14.940 | $3.941 \%$ | $10 \mathrm{~s}$ |

based on both problem-level and node-level representations.

Overall, the hierarchical gating improves the computational efficiency with a minor loss on the empirical performance. To balance the efficiency and performance of MVMoE, we use the base gating in the encoder and its hierarchical gating counterpart in the decoder. Note that the hierarchical gating is applicable to different gating algorithms, such as the input-choice gating (Shazeer et al., 2017) and expert-choice gating (Zhou et al., 2022). We also explore a more advanced gating algorithm (Puigcerver et al., 2024) for reducing the number of routed nodes and thus the computational complexity. But its empirical performance is unsatisfactory in the VRP domain (see Section 5.3).

## 5. Experiments

In this section, we empirically verify the superiority of the proposed MVMoE, and provide insights into the application of MoEs to solve VRPs. We consider 16 VRP variants with five constraints. Due to page limit, we present more experimental results in Appendix C. All experiments are conducted on a machine with NVIDIA Ampere A100-80GB GPU cards and AMD EPYC 7513 CPU at $2.6 \mathrm{GHz}$.

Baselines. Traditional solvers: We employ HGS (Vidal, 2022) to solve CVRP and VRPTW instances with default hyperparameters (i.e., the maximum number of iterations without improvement is 20000). We run LKH3 (Helsgaun, 2017) to solve CVRP, OVRP, VRPL and VRPTW instances with 10000 trails and 1 run. OR-Tools (Furnon \& Perron, 2023) is an open source solver for complex optimization problems. It is more versatile than LKH and HGS, and can solve all 16 VRP variants considered in this paper. We use the parallel cheapest insertion as the first solution strategy, and use the guided local search as the local search strategy in OR-Tools. For $n=50 / 100$, we set the search time limit as 20 s/40s to solve an instance, and also provide its results given 200s/400s (i.e., OR-Tools (x10)). For all traditional solvers, we use them to solve 32 instances in parallel on 32 CPU cores following Kool et al. (2018).

Neural solvers: We compare our method to POMO (Kwon et al., 2020) and POMO-MTL (Liu et al., 2024). While POMO is trained on each single VRP, POMO-MTL is trained on multiple VRPs by multi-task learning. Note that POMO-MTL is the dense model counterpart of MVMoE, which is structured by dense layers (e.g., FFNs) rather than sparse MoEs. In specific, POMO-MTL and MVMoE/4E possess $1.25 \mathrm{M}$ and $3.68 \mathrm{M}$ parameters, but they activate $\mathrm{a}$ similar number of parameters for each single input.

Training. We follow most setups in (Kwon et al., 2020). 1) For all neural solvers: Adam optimizer is used with the learning rate of $1 e-4$, the weight decay of $1 e-6$, and the batch size of 128. The model is trained for 5000 epochs, with each containing 20000 training instances (i.e., $100 \mathrm{M}$ training instances in total). The learning rate is decayed by 10 for the last $10 \%$ training instances. We consider two problem scales $n \in\{50,100\}$ during training, according to (Liu et al., 2024). 2) For multi-task solvers: The training problem set includes CVRP, OVRP, VRPB, VRPL, VRPTW, and OVRPTW (see Appendix C. 1 for further discussions). In each batch of training, we randomly sample a problem from the set and generate its instances. Please refer to Appendix A for details of the generation procedure. 3) For our method: We employ $m=4$ experts with $K=\beta=2$ in each MoE layer, and set the the weight $\alpha$ of the auxiliary loss $\mathcal{L}_{b}$ as 0.01 . The default gating mechanism of MVMOE/4E is the node-level input-choice gating in both the encoder

Table 2. Zero-shot generalization on $1 \mathrm{~K}$ test instances of unseen VRPs. * represents $0.000 \%$, with which the gaps are computed.

| Method |  | $n=50$ |  |  | $n=100$ |  |  | Method |  | $n=50$ |  |  | $n=100$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Obj. | Gap | Time | Obj. | Gap | Time |  |  | Obj. | Gap | Time | Obj. | Gap | Time |
| $\frac{\infty}{2}$ | OR-Tools | 5.764 | $0.332 \%$ | $10.4 \mathrm{~m}$ | 8.522 | $1.852 \%$ | $20.8 \mathrm{~m}$ | $\vec{a}$ <br> $\stackrel{2}{2}$ | OR-Tools | 6.522 | $0.480 \%$ | $10.4 \mathrm{~m}$ | 9.966 | $1.783 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 5.745 | * | $1.7 \mathrm{~h}$ | 8.365 | * | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 6.490 | * | $1.7 \mathrm{~h}$ | 9.790 | * | $3.5 \mathrm{~h}$ |
|  | POMO-MTL | 6.116 | $6.430 \%$ | $2 \mathrm{~s}$ | 8.979 | $7.335 \%$ | $8 \mathrm{~s}$ |  | POMO-MTL | 6.668 | $2.734 \%$ | $2 \mathrm{~s}$ | 10.126 | $3.441 \%$ | $9 \mathrm{~s}$ |
|  | MVMoE/4E | 6.092 | $5.999 \%$ | $3 \mathrm{~s}$ | 8.959 | $7.088 \%$ | $9 \mathrm{~s}$ |  | $\mathrm{MVMoE} / 4 \mathrm{E}$ | 6.650 | $2.454 \%$ | $3 \mathrm{~s}$ | 10.097 | $3.148 \%$ | $10 \mathrm{~s}$ |
|  | $\mathrm{MVMoE} / 4 \mathrm{E}-\mathrm{L}$ | 6.122 | $6.522 \%$ | $3 \mathrm{~s}$ | 8.972 | $7.243 \%$ | $9 \mathrm{~s}$ |  | MVMoE/4E-L | 6.659 | $2.597 \%$ | $3 \mathrm{~s}$ | 10.106 | $3.244 \%$ | $9 \mathrm{~s}$ |
| $\stackrel{\rightharpoonup}{a}$ <br> $\stackrel{a}{s}$ | OR-Tools | 8.131 | $1.254 \%$ | $10.4 \mathrm{~m}$ | 12.095 | $2.586 \%$ | $20.8 \mathrm{~m}$ | 3 <br> $\frac{m}{2}$ <br> $\frac{2}{2}$ <br> $>$ | OR-Tools | 15.053 | $1.857 \%$ | $10.4 \mathrm{~m}$ | 26.217 | $2.858 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 8.029 | * | $1.7 \mathrm{~h}$ | 11.790 | * | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 14.771 | * | $1.7 \mathrm{~h}$ | 25.496 | $*$ | $3.5 \mathrm{~h}$ |
|  | POMO-MTL | 8.188 | $1.971 \%$ | $2 \mathrm{~s}$ | 11.998 | $1.793 \%$ | $8 \mathrm{~s}$ |  | POMO-MTL | 16.055 | $8.841 \%$ | $3 \mathrm{~s}$ | 27.319 | $7.413 \%$ | $10 \mathrm{~s}$ |
|  | MVMoE/4E | 8.172 | $1.776 \%$ | $3 \mathrm{~s}$ | 11.945 | $1.346 \%$ | $9 \mathrm{~s}$ |  | $\mathrm{MVMoE} / 4 \mathrm{E}$ | 16.022 | $8.600 \%$ | $4 \mathrm{~s}$ | 27.236 | $7.078 \%$ | $11 \mathrm{~s}$ |
|  | MVMoE/4E-L | 8.180 | $1.872 \%$ | $3 \mathrm{~s}$ | 11.960 | $1.473 \%$ | $9 \mathrm{~s}$ |  | MVMoE/4E-L | 16.041 | $8.745 \%$ | $4 \mathrm{~s}$ | 27.265 | $7.190 \%$ | $10 \mathrm{~s}$ |
| 3 <br> $\underset{d}{z}$ <br> $\underset{z}{3}$ | OR-Tools | 14.815 | $1.432 \%$ | $10.4 \mathrm{~m}$ | 25.823 | $2.534 \%$ | $20.8 \mathrm{~m}$ | 0 <br> 2 <br> 2 <br> 2 | OR-Tools | 5.771 | $0.549 \%$ | $10.4 \mathrm{~m}$ | 8.555 | $2.459 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 14.598 | $*$ | $1.7 \mathrm{~h}$ | 25.195 | * | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 5.739 | $*$ | $1.7 \mathrm{~h}$ | 8.348 | * | $3.5 \mathrm{~h}$ |
|  | POMO-MTL | 14.961 | $2.586 \%$ | $3 \mathrm{~s}$ | 25.619 | $1.920 \%$ | $12 \mathrm{~s}$ |  | POMO-MTL | 6.104 | $6.306 \%$ | $2 \mathrm{~s}$ | 8.961 | $7.343 \%$ | $8 \mathrm{~s}$ |
|  | MVMoE/4E | 14.937 | $2.421 \%$ | $4 \mathrm{~s}$ | 25.514 | $1.471 \%$ | $13 \mathrm{~s}$ |  | MVMoE/4E | 6.076 | $5.843 \%$ | $3 \mathrm{~s}$ | 8.942 | $7.115 \%$ | $9 \mathrm{~s}$ |
|  | MVMoE/4E-L | 14.953 | $2.535 \%$ | $4 \mathrm{~s}$ | 25.529 | $1.545 \%$ | $12 \mathrm{~s}$ |  | MVMoE/4E-L | 6.104 | $6.310 \%$ | $3 \mathrm{~s}$ | 8.957 | $7.300 \%$ | $9 \mathrm{~s}$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-07.jpg?height=145&width=44&top_left_y=836&top_left_x=217) | OR-Tools | 8.758 | $0.927 \%$ | $10.4 \mathrm{~m}$ | 14.713 | $2.268 \%$ | $20.8 \mathrm{~m}$ | 3 <br> $\frac{3}{2}$ <br> $\frac{a}{3}$ <br> 0 | OR-Tools | 8.728 | $0.656 \%$ | $10.4 \mathrm{~m}$ | 14.535 | $1.779 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 8.675 | * | $1.7 \mathrm{~h}$ | 14.384 | * | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 8.669 | * | $1.7 \mathrm{~h}$ | 14.279 | * | $3.5 \mathrm{~h}$ |
|  | POMO-MTL | 9.514 | $9.628 \%$ | $3 \mathrm{~s}$ | 15.879 | $10.453 \%$ | $10 \mathrm{~s}$ |  | POMO-MTL | 8.987 | $3.633 \%$ | $3 \mathrm{~s}$ | 14.896 | $4.374 \%$ | $11 \mathrm{~s}$ |
|  | $\mathrm{MVMoE} / 4 \mathrm{E}$ | 9.486 | $9.308 \%$ | $4 \mathrm{~s}$ | 15.808 | $9.948 \%$ | $11 \mathrm{~s}$ |  | MVMoE/4E | 8.966 | $3.396 \%$ | $4 \mathrm{~s}$ | 14.828 | $3.903 \%$ | $12 \mathrm{~s}$ |
|  | $\mathrm{MVMoE} / 4 \mathrm{E}-\mathrm{L}$ | 9.515 | $9.630 \%$ | $3 \mathrm{~s}$ | 15.841 | $10.188 \%$ | $10 \mathrm{~s}$ |  | MVMoE/4E-L | 8.974 | $3.488 \%$ | $4 \mathrm{~s}$ | 14.839 | $3.971 \%$ | $10 \mathrm{~s}$ |
| 3 <br> $\underset{n}{n}$ <br> $\stackrel{2}{a}$ <br> $>$ | OR-Tools | 14.890 | $1.402 \%$ | $10.4 \mathrm{~m}$ | 25.979 | $2.518 \%$ | $20.8 \mathrm{~m}$ | 3 <br> $\underset{a}{m}$ <br> $\stackrel{a}{a}$ <br> $\vec{r}$ | OR-Tools | 8.729 | $0.624 \%$ | $10.4 \mathrm{~m}$ | 14.496 | $1.724 \%$ | $20.8 \mathrm{~m}$ |
|  | OR-Tools (x10) | 14.677 | $*$ | $1.7 \mathrm{~h}$ | 25.342 | * | $3.5 \mathrm{~h}$ |  | OR-Tools (x10) | 8.673 | $*$ | $1.7 \mathrm{~h}$ | 14.250 | $*$ | $3.5 \mathrm{~h}$ |
|  | POMO-MTL | 15.980 | $9.035 \%$ | $3 \mathrm{~s}$ | 27.247 | $7.746 \%$ | $11 \mathrm{~s}$ |  | POMO-MTL | 9.532 | $9.851 \%$ | $3 \mathrm{~s}$ | 15.738 | $10.498 \%$ | $10 \mathrm{~s}$ |
|  | $\mathrm{MVMoE} / 4 \mathrm{E}$ | 15.945 | $8.775 \%$ | $4 s$ | 27.142 | $7.332 \%$ | $12 \mathrm{~s}$ |  | $\mathrm{MVMoE} / 4 \mathrm{E}$ | 9.503 | $9.516 \%$ | $4 \mathrm{~s}$ | 15.671 | $10.009 \%$ | $11 \mathrm{~s}$ |
|  | MVMoE/4E-L | 15.963 | $8.915 \%$ | $4 \mathrm{~s}$ | 27.177 | $7.473 \%$ | $11 \mathrm{~s}$ |  | MVMoE/4E-L | 9.518 | $9.682 \%$ | $4 \mathrm{~s}$ | 15.706 | $10.263 \%$ | $10 \mathrm{~s}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-07.jpg?height=386&width=784&top_left_y=1178&top_left_x=194)

Figure 5. Few-shot generalization on unseen VRPs.

and decoder layers. MVMoE/4E-L is a computationally light version that replaces the input-choice gating with its hierarchical gating counterpart in the decoder.

Inference. For all neural solvers, we use greedy rollout with x8 instance augmentation following Kwon et al. (2020). We report the average results (i.e., objective values and gaps) over the test dataset that contains $1 \mathrm{~K}$ instances, and the total time to solve the entire test dataset. The gaps are computed with respect to the results of the best-performing traditional VRP solvers (i.e., * in Tables 1 and 2).

### 5.1. Empirical Results

Performance on Trained VRPs. We evaluate all methods on 6 trained VRPs and gather all results in Table 1. The single-task neural solver (i.e., POMO) achieves better performance than multi-task neural solvers on each single problem, since it is restructured and retrained on each VRP independently. However, its average performance over all trained
VRPs is quite inferior as shown in Table 4 in Appendix C, since each trained POMO is overfitted to a specific VRP. For example, the average performance of POMO solely trained on CVRP is $16.815 \%$, while POMO-MTL and MVMoE/4E achieve $2.102 \%$ and $1.925 \%$, respectively. Notably, our neural solvers consistently outperform POMO-MTL. MV$\mathrm{MoE} / 4 \mathrm{E}$ performs slightly better than $\mathrm{MVMoE} / 4 \mathrm{E}-\mathrm{L}$ at the expense of more computation. Despite that, MVMoE/4E-L exhibits stronger out-of-distribution generalization capability than MVMoE/4E (see Tables 7 and 8 in Appendix C).

Generalization on Unseen VRPs. We evaluate multi-task solvers on 10 unseen VRP variants. 1) Zero-shot generalization: We directly test the trained solvers on unseen VRPs. The results in Table 2 reveal that the proposed MVMoE significantly outperforms POMO-MTL across all VRP variants. 2) Few-shot generalization: We also consider the few-shot setting on $n=50$, where a trained solver is fine-tuned on the target VRP using $10 \mathrm{~K}$ instances $(0.01 \%$ of total training instances) in each epoch. Without loss of generality, we conduct experiments on VRPBLTW and OVRPBLTW following the training setups. The results in Fig. 5 showcase MVMoE generalizes more favorably than POMO-MTL.

### 5.2. Ablation on MoEs

Here we explore the effect of different MoE settings on the zero-shot generalization of neural solvers, and provide insights on how to effectively apply MoEs to solve VRPs. Due to the fast convergence, we reduce the number of epochs to 2500 on VRPs of the size $n=50$, while leaving other setups unchanged. We set MVMoE/4E as the default baseline,
![](https://cdn.mathpix.com/cropped/2024_06_04_746914156a07050e0d78g-08.jpg?height=342&width=1706&top_left_y=230&top_left_x=182)

Figure 6. Left three panels: The effect of MoE settings on the average zero-shot generalization performance - (a) the position of MoEs; (b) the number of experts; (c) the gating mechanism. Right two panels: Further analyses - (d) average zero-shot generalization performance of each method employing various gating algorithms in the decoder; (e) training efficiency of each gating algorithm.

and ablate on different components of MoEs below.

Position of MoEs. We consider three positions to apply MoEs in neural solvers: 1) Raw feature processing (Raw): The linear layer, which projects raw features into initial embeddings, are replaced by MoEs. 2) Encoder (Enc): The FFN in an encoder layer is replaced by MoEs. Typically, MoEs are widely used in every-two or last-two layers (i.e., every or last two layers with even indices $\ell \in[0, N-$ 1]) (Riquelme et al., 2021). Besides, we further attempt to use MoEs in all encoder layers. 3) Decoder (Dec): The final linear layer of the multi-head attention is replaced by MoEs in the decoder. We show the average performance over 10 unseen VRPs in Fig. 6(a). The results reveal that applying MoEs at the shallow layer (e.g., Raw) may worsen the model performance, while using MoEs in all encoder layers (Enc_All) or decoder (Dec) can benefit the zero-shot generalization. Therefore, in this paper, we employ MoEs in both encoder and decoder to pursue a strong unified model architecture to solve various VRPs.

Number of Experts. We increase the number of experts in each MoE layer to 8 and 16, and compare the derived MVMoE/8E/16E models to MVMoE/4E. We first train all models using the same number (50M) of instances. After that, we also train MVMoE/8E/16E with more data and computation to explore potential better results, based on the scaling laws (Kaplan et al., 2020). In specific, we provide $\mathrm{MVMoE} / 8 \mathrm{E} / 16 \mathrm{E}$ with more data by using larger batch sizes, which linearly scale up against the number of experts (i.e., MVMoE/4E/8E/16E are trained on 50M/100M/200M instances with batch sizes 128/256/512, respectively). The results in Fig. 6(b) show that increasing the number of experts with more training data further unleashes the power of MVMoE, indicating the efficacy of MoEs in solving VRPs.

Gating Mechanism. We investigate the effect of different gating levels and algorithms, including three levels (i.e., node-level, instance-level and problem-level) and three algorithms (i.e., input-choice, expert-choice and random gatings), with their details presented in Appendix B. As shown in Fig. 6(c), the node-level input-choice gating performs the best, while the node-level expert-choice gating performs the worst. Interestingly, we observe that the expert-choice gating in the decoder makes MVMoE hard to be optimized. It may suggest that each gating algorithm could have its most suitable position to serve MoEs. However, after an attempt to tune this configuration (i.e., by using MoEs only in the encoder), its performance is still inferior to the baseline, with an average gap of $7.190 \%$ on unseen VRPs.

### 5.3. Additional Results

We further provide experiments and discussions on more advanced gating algorithms, training efficiency, benchmark performance, and scalability. We refer readers to more empirical results (e.g., sensitivity analyses) in Appendix C.

Advanced Gating. Besides the input-choice and expertchoice gating algorithms evaluated above, we further consider soft MoEs (Puigcerver et al., 2024), which is a recent advanced gating algorithm. Specifically, it performs an implicit soft assignment by distributing $K$ slots (i.e., convex combinations of all inputs) to each expert, rather than a hard assignment between inputs and experts as done by the conventional sparse and discrete gating networks. Since only $K$ (e.g., 1 or 2 ) slots are distributed to each expert, it can save much computation. We train MVMoE on $n=50$ by using node-level soft MoEs in the decoder, following training setups. We also show the result of employing heuristic (random) hierarchical gating in the decoder. However, their results are unsatisfactory as shown in Fig. 6(d).

Training Efficiency. Fig. 6(e) shows the training time of employing each gating algorithm in the decoder, combining with their results reported in Fig. 6(d), demonstrating the efficacy of the proposed hierarchical gating in reducing the training overhead with only minor losses in performance.

Benchmark Performance. We further evaluate the out-ofdistribution (OOD) generalization performance of all neural solvers on CVRPLIB benchmark instances. Detailed results can be found in Tables 7 and 8 in Appendix C. Surprisingly, we observe that $\mathrm{MVMoE} / 4 \mathrm{E}$ performs poorly on large-scale
instances (e.g., $n>500$ ). It may be caused by the generalization issue of sparse MoEs when transferring to new distributions or domains, which is still an open question in the MoE literature (Fedus et al., 2022a). In contrast, MVMoE/4E-L mostly outperforms MVMoE/4E, demonstrating more favourable potential of the hierarchical gating in promoting the OOD generalization capability. It is worth noting that all neural solvers are only trained on the simple uniformly distributed instances with the size $n=100$. Embracing more varied problem sizes (cross-size) and attribute distributions (cross-distribution) into the multi-task training (cross-problem) may further consolidate their performance.

Scalability. Given that supervised learning based approaches appear to be more scalable than RL-based approaches in the current literature, we try to build upon a more scalable method, i.e., LEHD (Luo et al., 2023). Concretely, we train a dense model LEHD and a light sparse model with 4 experts LEHD/4E-L on CVRP. The training setups are kept the same as Luo et al. (2023), except that we train all models for only 20 epochs for the training efficiency. We use the hierarchical MoE in each decoder layer of LEHD/4E-L. The results are shown in Table 8, which demonstrates the potential of $\mathrm{MoE}$ as a general idea that can further benefit recent scalable methods. Moreover, during the solution construction process, recent works (Drakulic et al., 2023; Gao et al., 2023) typically constrain the search space within a neighborhood of the currently selected node, which is shown to be effective in handling large-scale instances. Integrating MVMoE with these simple yet effective techniques may further improve large-scale performance.

## 6. Conclusion

Targeting a more generic and powerful neural solver for solving VRPs, we propose a multi-task vehicle routing solver with MoEs (MVMoE), which can solve a range of VRPs concurrently, even in a zero-shot manner. We provide valuable insights on how to apply MoEs in neural VRP solvers, and propose an effective and efficient hierarchical gating mechanism. Empirically, MVMoE demonstrates strong generalization capability on zero-shot, few-shot settings, and real-world benchmark. Despite this paper presents the first attempt towards a large VRP model, the scale of parameters is still far less than LLMs. We leave 1) the development of scalable MoE-based models in solving large-scale VRPs, 2) the venture of generic representations for different problems, 3) the exploration of interpretability of gating mechanisms (Nguyen et al., 2023; 2024), and 4) the investigation of scaling laws in MoEs (Krajewski et al., 2024) to the future work. We hope our work benefit the COP community in developing large optimization (or foundation) models ${ }^{2}$.[^2]

## Acknowledgements

This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-RP-2022-031), the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant, the National Natural Science Foundation of China (Grant 62102228), and the Natural Science Foundation of Shandong Province (Grant ZR2021QF063). We would like to thank the anonymous reviewers and (S)ACs of ICML 2024 for their constructive comments and dedicated service to the community. Jianan Zhou would like to personally express deep gratitude to his grandfather, Jinlong Hu, for his meticulous care and love during last 26 years. Eternal easy rest in sweet slumber.

## Impact Statements

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## References

Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. In ICLR Workshop Track, 2017.

Bengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial optimization: a methodological tour d'horizon. European Journal of Operational Research, 290(2):405-421, 2021.

Berto, F., Hua, C., Park, J., Kim, M., Kim, H., Son, J., Kim, H., Kim, J., and Park, J. RL4CO: a unified reinforcement learning for combinatorial optimization library. In NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023.

Bi, J., Ma, Y., Wang, J., Cao, Z., Chen, J., Sun, Y., and Chee, Y. M. Learning generalizable models for vehicle routing problems via knowledge distillation. In NeurIPS, 2022.

Bogyrbayeva, A., Meraliyev, M., Mustakhov, T., and Dauletbayev, B. Machine learning to solve vehicle routing problems: A survey. IEEE Transactions on Intelligent Transportation Systems, 2024.

Boisvert, L., Verhaeghe, H., and Cappart, Q. Towards a generic representation of cominatorial problems for learning-based approaches. arXiv preprint arXiv:2403.06026, 2024.

Cattaruzza, D., Absi, N., Feillet, D., and González-Feliu, J. Vehicle routing problems for city logistics. EURO Journal on Transportation and Logistics, 6(1):51-79, 2017.

Chalumeau, F., Surana, S., Bonnet, C., Grinsztajn, N., Pretorius, A., Laterre, A., and Barrett, T. D. Combinatorial optimization with policy adaptation using latent space search. In NeurIPS, 2023.

Chen, J., Zhang, Z., Cao, Z., Wu, Y., Ma, Y., Ye, T., and Wang, J. Neural multi-objective combinatorial optimization with diversity enhancement. In NeurIPS, 2023.

Chen, X. and Tian, Y. Learning to perform local rewriting for combinatorial optimization. In NeurIPS, volume 32, 2019 .

Chen, Z., Deng, Y., Wu, Y., Gu, Q., and Li, Y. Towards understanding the mixture-of-experts layer in deep learning. In NeurIPS, volume 35, pp. 23049-23062, 2022.

Croes, G. A. A method for solving traveling-salesman problems. Operations research, 6(6):791-812, 1958.

d O Costa, P. R., Rhuggenaath, J., Zhang, Y., and Akcay, A. Learning 2-opt heuristics for the traveling salesman problem via deep reinforcement learning. In Asian Conference on Machine Learning, pp. 465-480. PMLR, 2020.

Drakulic, D., Michel, S., Mai, F., Sors, A., and Andreoli, J.M. BQ-NCO: Bisimulation quotienting for generalizable neural combinatorial optimization. In NeurIPS, 2023.

Eigen, D., Ranzato, M., and Sutskever, I. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.

Fedus, W., Dean, J., and Zoph, B. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022a.

Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022b.

Floridi, L. and Chiriatti, M. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681-694, 2020 .

Fu, Z.-H., Qiu, K.-B., and Zha, H. Generalize a small pretrained model to arbitrarily large tsp instances. In $A A A I$, volume 35, pp. 7474-7482, 2021.

Furnon, V. and Perron, L. Or-tools routing library, 2023. URLhttps://developers.google.com/ optimization/routing.

Gao, C., Shang, H., Xue, K., Li, D., and Qian, C. Towards generalizable neural solvers for vehicle routing problems via ensemble with transferrable local policy. arXiv preprint arXiv:2308.14104, 2023.
Geisler, S., Sommer, J., Schuchardt, J., Bojchevski, A., and Günnemann, S. Generalization of neural combinatorial solvers through the lens of adversarial robustness. In ICLR, 2022.

Grinsztajn, N., Furelos-Blanco, D., Surana, S., Bonnet, C., and Barrett, T. D. Winner takes it all: Training performant RL populations for combinatorial optimization. In NeurIPS, 2023.

Helsgaun, K. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling salesman and vehicle routing problems. Roskilde: Roskilde University, pp. $24-50,2017$.

Hottung, A. and Tierney, K. Neural large neighborhood search for the capacitated vehicle routing problem. In European Conference on Artificial Intelligence, pp. 443$450,2020$.

Hottung, A., Mahajan, M., and Tierney, K. PolyNet: Learning diverse solution strategies for neural combinatorial optimization. arXiv preprint arXiv:2402.14048, 2024.

Hou, Q., Yang, J., Su, Y., Wang, X., and Deng, Y. Generalize learned heuristics to solve large-scale vehicle routing problems in real-time. In ICLR, 2023.

Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.

Hudson, B., Li, Q., Malencia, M., and Prorok, A. Graph neural network guided local search for the traveling salesperson problem. In ICLR, 2022.

Ismail, A. A., Arik, S. O., Yoon, J., Taly, A., Feizi, S., and Pfister, T. Interpretable mixture of experts. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.

Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural computation, 3(1):79-87, 1991 .

Jiang, Y., Cao, Z., Wu, Y., Song, W., and Zhang, J. Ensemble-based deep reinforcement learning for vehicle routing problems under distribution shift. In NeurIPS, 2023.

Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2): $181-214,1994$.

Joshi, C. K., Laurent, T., and Bresson, X. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.

Joshi, C. K., Cappart, Q., Rousseau, L.-M., and Laurent, T. Learning tsp requires rethinking generalization. In International Conference on Principles and Practice of Constraint Programming, 2021.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

Kim, M., Park, J., and Park, J. Sym-NCO: Leveraging symmetricity for neural combinatorial optimization. In NeurIPS, 2022.

Kim, M., Choi, S., Son, J., Kim, H., Park, J., and Bengio, Y. Ant colony sampling with gflownets for combinatorial optimization. arXiv preprint arXiv:2403.07041, 2024.

Kool, W., van Hoof, H., and Welling, M. Attention, learn to solve routing problems! In ICLR, 2018.

Kool, W., van Hoof, H., Gromicho, J., and Welling, M. Deep policy dynamic programming for vehicle routing problems. In International conference on integration of constraint programming, artificial intelligence, and operations research, pp. 190-213, 2022.

Krajewski, J., Ludziejewski, J., Adamczewski, K., Pióro, M., Krutul, M., Antoniak, S., Ciebiera, K., Król, K., Odrzygóźdź, T., Sankowski, P., et al. Scaling laws for fine-grained mixture of experts. arXiv preprint arXiv:2402.07871, 2024.

Kwon, Y.-D., Choo, J., Kim, B., Yoon, I., Gwon, Y., and Min, S. POMO: Policy optimization with multiple optima for reinforcement learning. In NeurIPS, volume 33, pp. 21188-21198, 2020.

Kwon, Y.-D., Choo, J., Yoon, I., Park, M., Park, D., and Gwon, Y. Matrix encoding networks for neural combinatorial optimization. In NeurIPS, volume 34, pp. 5138$5149,2021$.

Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. Base layers: Simplifying training of large, sparse models. In ICML, pp. 6265-6274. PMLR, 2021.

Li, J., Ma, Y., Gao, R., Cao, Z., Lim, A., Song, W., and Zhang, J. Deep reinforcement learning for solving the heterogeneous capacitated vehicle routing problem. IEEE Transactions on Cybernetics, 52(12):13572-13585, 2021a.
Li, S., Yan, Z., and Wu, C. Learning to delegate for largescale vehicle routing. In NeurIPS, volume 34, pp. 2619826211, $2021 b$.

Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, pp. 4582-4597, 2021.

Lin, Z., Wu, Y., Zhou, B., Cao, Z., Song, W., Zhang, Y., and Senthilnath, J. Cross-problem learning for solving vehicle routing problems. In IJCAI, 2024.

Liu, F., Lin, X., Zhang, Q., Tong, X., and Yuan, M. Multi-task learning for routing problem with cross-problem zero-shot generalization. arXiv preprint arXiv:2402.16891, 2024.

Lu, H., Zhang, X., and Yang, S. A learning-based iterative method for solving vehicle routing problems. In ICLR, 2020.

Luo, F., Lin, X., Liu, F., Zhang, Q., and Wang, Z. Neural combinatorial optimization with heavy decoder: Toward large scale generalization. In NeurIPS, 2023.

Ma, Y., Cao, Z., and Chee, Y. M. Learning to search feasible and infeasible regions of routing problems with flexible neural k-opt. In NeurIPS, 2023.

Manchanda, S., Michel, S., Drakulic, D., and Andreoli, J.-M. On the generalization of neural combinatorial optimization heuristics. In ECML PKDD, 2022.

Min, Y., Bai, Y., and Gomes, C. P. Unsupervised learning for solving the travelling salesman problem. In NeurIPS, 2023 .

Nazari, M., Oroojlooy, A., Snyder, L., and Takác, M. Reinforcement learning for solving the vehicle routing problem. In NeurIPS, volume 31, 2018.

Nguyen, H., Nguyen, T., and Ho, N. Demystifying softmax gating function in gaussian mixture of experts. In NeurIPS, 2023.

Nguyen, H., Akbarian, P., Yan, F., and Ho, N. Statistical perspective of top-k sparse softmax gating mixture of experts. In ICLR, 2024.

Puigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N. From sparse to soft mixtures of experts. In ICLR, 2024.

Qiu, R., Sun, Z., and Yang, Y. DIMES: A differentiable meta solver for combinatorial optimization problems. In NeurIPS, 2022.

Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., Keysers, D., and Houlsby, N. Scaling vision with sparse mixture of experts. In NeurIPS, volume 34, pp. 8583-8595, 2021.

Roller, S., Sukhbaatar, S., Weston, J., et al. Hash layers for large sparse models. In NeurIPS, volume 34, pp. $17555-17566,2021$.

Ruis, F., Burghouts, G., and Bucur, D. Independent prototype propagation for zero-shot compositionality. NeurIPS, 34:10641-10653, 2021.

Shaw, P. Using constraint programming and local search methods to solve vehicle routing problems. In International conference on principles and practice of constraint programming, pp. 417-431. Springer, 1998.

Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017.

Solomon, M. M. Algorithms for the vehicle routing and scheduling problems with time window constraints. $O p$ erations research, 35(2):254-265, 1987.

Son, J., Kim, M., Kim, H., and Park, J. Meta-SAGE: Scale meta-learning scheduled adaptation with guided exploration for mitigating scale shift on combinatorial optimization. In ICML, 2023.

Sun, Z. and Yang, Y. DIFUSCO: Graph-based diffusion solvers for combinatorial optimization. In NeurIPS, 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Uchoa, E., Pecin, D., Pessoa, A., Poggi, M., Vidal, T., and Subramanian, A. New benchmark instances for the capacitated vehicle routing problem. European Journal of Operational Research, 257(3):845-858, 2017.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In NeurIPS, volume 30, 2017.

Vidal, T. Hybrid genetic search for the cvrp: Open-source implementation and swap* neighborhood. Computers \& Operations Research, 140:105643, 2022.

Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In NeurIPS, volume 28, 2015.

Wang, C. and Yu, T. Efficient training of multi-task neural solver with multi-armed bandits. arXiv preprint arXiv:2305.06361, 2023.

Wang, C., Yu, Z., McAleer, S., Yu, T., and Yang, Y. ASP: Learn a universal neural solver! IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.
Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256, 1992.

Wu, Y., Song, W., Cao, Z., Zhang, J., and Lim, A. Learning improvement heuristics for solving routing problems. IEEE transactions on neural networks and learning systems, 33(9):5057-5069, 2021.

Wu, Y., Zhou, J., Xia, Y., Zhang, X., Cao, Z., and Zhang, J. Neural airport ground handling. IEEE Transactions on Intelligent Transportation Systems, 2023.

Xin, L., Song, W., Cao, Z., and Zhang, J. NeuroLKH: Combining deep learning model with lin-kernighan-helsgaun heuristic for solving the traveling salesman problem. In NeurIPS, volume 34, pp. 7472-7483, 2021.

Xue, F., Zheng, Z., Fu, Y., Ni, J., Zheng, Z., Zhou, W., and You, Y. OpenMoE: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024.

Ye, H., Wang, J., Cao, Z., Liang, H., and Li, Y. DeepACO: Neural-enhanced ant systems for combinatorial optimization. In NeurIPS, 2023.

Yuksel, S. E., Wilson, J. N., and Gader, P. D. Twenty years of mixture of experts. IEEE transactions on neural networks and learning systems, 23(8):1177-1193, 2012.

Zhang, C., Wu, Y., Ma, Y., Song, W., Le, Z., Cao, Z., and Zhang, J. A review on learning to solve combinatorial optimisation problems in manufacturing. IET Collaborative Intelligent Manufacturing, 5(1):e12072, 2023.

Zhang, Z., Zhang, Z., Wang, X., and Zhu, W. Learning to solve travelling salesman problem with hardness-adaptive curriculum. In AAAI, 2022.

Zhou, J., Wu, Y., Cao, Z., Song, W., Zhang, J., and Chen, Z. Learning large neighborhood search for vehicle routing in airport ground handling. IEEE Transactions on Knowledge and Data Engineering, 2023a.

Zhou, J., Wu, Y., Song, W., Cao, Z., and Zhang, J. Towards omni-generalizable neural methods for vehicle routing problems. In ICML, pp. 42769-42789. PMLR, 2023b.

Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A. M., Le, Q. V., Laudon, J., et al. Mixture-of-experts with expert choice routing. In NeurIPS, volume 35, pp. $7103-7114,2022$.

Zuo, S., Liu, X., Jiao, J., Kim, Y. J., Hassan, H., Zhang, R., Gao, J., and Zhao, T. Taming sparsely activated transformer with stochastic experts. In ICLR, 2022.
