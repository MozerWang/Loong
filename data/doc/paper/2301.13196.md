# Looped Transformers as Programmable Computers 

Angeliki Giannou ${ }^{w *}$, Shashank Rajput ${ }^{w *}$, Jy-yong Sohn ${ }^{w}$,<br>Kangwook Lee ${ }^{w}$, Jason D. Lee $^{p}$, Dimitris Papailiopoulos ${ }^{w}$<br>${ }^{p}$ Princeton University<br>${ }^{w}$ University of Wisconsin-Madison

January 31, 2023


#### Abstract

We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.


## 1 Introduction

Transformers (TFs) have become a popular choice for a wide range of machine learning tasks, achieving state-of-the-art results in fields such as natural language processing and computer vision [Vaswani et al., 2017, Khan et al., 2022, Yuan et al., 2021, Dosovitskiy et al., 2020]. One key reason for their success is their ability to capture higher-order relationships and long-range dependencies across tokens, through attention. This allows TFs to model contextual information and makes them effective in tasks such as machine translation and language modeling, where they have consistently outperformed other methods [Vaswani et al., 2017, Kenton and Toutanova, 2019].

Language models with billions of parameters, such as GPT-3 (175B parameters Brown et al. [2020]) and PaLM (540B parameters Chowdhery et al. [2022]), have achieved state-of-the-art[^0]performance on many natural language processing tasks. Interestingly, some of these large language models (LLMs) can also perform in-context learning, adapting to and performing a specific task, on-the-fly, based on a brief prompt and a few examples. The ability to perform in-context learning (ICL) arises without explicit training for it, and allows these large models to efficiently perform new tasks without requiring weight updates.

Surprisingly, through in-context learning LLMs can perform algorithmic tasks and reasoning, as demonstrated in several works including Nye et al. [2021], Wei et al. [2022c], Lewkowycz et al. [2022], Wei et al. [2022b], Zhou et al. [2022], Dasgupta et al. [2022], Chung et al. [2022]. For example, Zhou et al. [2022] showed that LLMs can successfully perform addition on unseen examples when prompted with a multidigit addition algorithm and a few examples of addition. These results suggest that LLMs can apply algorithmic principles and perform pre-instructed commands on a given input at inference time, as if interpreting natural language as code.

Constructive arguments have demonstrated that Transformers can simulate Turing Machines with enough depth or recursive links between attention layers Pérez et al. [2021], Pérez et al. [2019], Wei et al. [2022a]. This demonstrates the potential of transformer networks to precisely follow algorithmic instructions specified by the input. Yet, these constructions are more generalized and do not provide insight into how to create Transformers that can carry out particular algorithmic tasks, or compile programs in a higher-level programming language.

More specialized designs can however allow TFs to execute higher level programs. For example, in Weiss et al. [2021], the authors design a computational model and a programming language that maps simple selection and aggregation commands on indexed input tokens. This language can be used to create several interesting algorithms, such as counting tokens, sorting, creating histograms, and recognizing Dyck- $k$ languages. Programs written in Restricted Access Sequence Processing Language (RASP) can then be mapped into transformer networks, which typically scale in size with the size of the program.

Another line of research has demonstrated methods for selecting the weights of a Transformer model to function as an optimization algorithm for learning linear regression models on-the-fly, performing implicit training at inference time when given training data as input [Akyürek et al., 2022, von Oswald et al., 2022]. These methods typically require a number of layers proportional to the number of iterations of the learning algorithm and are limited to a small set of loss functions and models.

The ability to program transformer models to emulate the abstract computation of a Turing Machine, the specialized commands of languages like RASP, and the specific algorithms of incontext learning, highlights the potential for transformer networks as versatile programmable computers. Our research aims to explore this promising prospect, uncovering how the mechanics of attention can enable the emulation of a general-purpose computer inspired by instruction-set architectures.

Our Contributions: In this paper, we demonstrate that transformer networks can simulate complex algorithms and programs by hardcoding them with specific weights and placing them in a loop. We do this by reverse engineering attention to emulate basic computing blocks, such as edit operations on the input sequence, nonlinear functions, function calls, program counters and conditional branches. Our paper demonstrates the importance of using a single loop or recursion to connect the transformer's output sequence back to its input, avoiding the need for a deep model.

We accomplish this by designing a transformer that can execute programs written in a generalized version of a single instruction, known as SUBLEQ(A,B,C), i.e., SUBtract and branch if Less-than or EQual to zero. SUBLEQ is a single instruction language, defining a one-instruction set computer (OISC, pronounced "whisk"). SUBLEQ consists of 3 memory address operands and when executed it subtracts the value at memory address $\mathrm{A}$ from the value at memory address $\mathrm{B}$, and stores the result in B. If the result in B is less than or equal to zero, the execution jumps to address C, otherwise it proceeds to the next instruction. Programs written in SUBLEQ language use only this command, yet this single instruction is capable of defining a universal computer [Mavaddat and Parhami, 1988, Esolangs].

We construct explicit transformers that implement SUBLEQ-like programs, of a more flexible single instruction which we call FLEQ which takes the form

$$
\begin{aligned}
& \operatorname{mem}[c]=f_{m}(\operatorname{mem}[a], \operatorname{mem}[b]) \\
& \text { if mem }[\text { flag }] \leq 0 \\
& \quad \text { goto instruction } p
\end{aligned}
$$

where $f_{m}$ can be selected from a set of functions (matrix multiplication/non-linear functions/polynomials/etc), which we can hardcode into the network. The depth of a looped transformer that can execute $\mathrm{FLEQ}$ programs is not dependent on the depth of the program or the number of lines of code, but rather on the depth required to implement a single $F L E Q$ instruction, which is constant. This is achieved by

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-03.jpg?height=477&width=811&top_left_y=716&top_left_x=1058)

Figure 1: A sketch of the looped transformer architecture, where the input sequence stores the commands, memory where the data is read/written from, and a scratchpad where intermediate results are stored. The input is processed by the network and the output is used as the new input, allowing the network to iteratively update an implicit state and perform complex computations. running the transformer in cycles over the input sequence, similar to how a CPU operates.

Using this framework, we demonstrate the ability to emulate a variety of functions at inference time, including a basic calculator, a basic linear algebra library (matrix transpose, multiplication, inversion, power iteration) and an in-context learning algorithm that implements backpropagation on implicit fully-connected networks. The input sequence, or the prompt, acts as a punchcard that includes the program in the form of instructions that the transformer needs to execute, while providing space for storing and processing the variables used in the program. The transformer networks used to execute these programs are all of depth smaller or equal to thirteen, and the exact weight matrices for all these models are provided. The following informal theorem summarizes our main findings:

Theorem 1 (Informal). There exists a looped transformer with less than 13 layers that can emulate a general purpose computer (see Sec. 5), a basic calculator (see Sec. 7), numerical linear algebra methods, such as approximate matrix inverse and power iteration (see Sec. 8), and in-context learning algorithms, such as SGD, on neural networks (See Sec. 9).

The precise size of the transformers constructed in this paper is also summarized in Section 1.

|  | \# Layers | \# Heads | Formal Statement |
| :---: | :---: | :---: | :---: |
| SUBLEQ | 9 | 2 | Lemma. 4 |
| Matrix Inversion | 13 | 1 | Lemma. 12 |
| Power Iteration | 13 | 1 | Lemma. 13 |
| SGD | 13 | 1 | Lemma. 15 |

Table 1: Looped transformer sizes required to successfully emulate the functionalities of a one instruction set computer (OISC), perform basic calculations, run numerical linear algebra algorithms, and incontext learning using Stochastic Gradient Descent on a neural network. The width of these networks depends on the complexity of the functions implemented, and typically range from $O$ (log(length_input) + embedding_dimension) to at most polynomial in the approximation error required when implementing arbitrary loss functions for in-context learning.

Our research highlights the flexibility of the attention mechanism and the importance of even a single loop making it possible to design models that can emulate complex iterative algorithms and execute general programs. It further demonstrates the ability of transformer models to efficiently perform complex mathematical and algorithmic tasks. It is conceivable that modern transformers, such as GPT-3, utilize similar internal subroutines when performing various tasks. In a way, these models may possess the ability to elicit a specific skill or algorithm, akin to a function call, when given in-context examples and instructions. However, this hypothesis should be taken with caution, as the way we design our constructions shares no similarities with how real-world language models are trained.

We hope that our study will encourage further research into the potential of attention mechanisms, and the ability of language models to execute algorithmic instructions. Our proposed designs can aid in determining the minimal transformer network size required to perform specific algorithmic tasks. Additionally, we hope that our findings will contribute to the development of methods to enhance the capabilities of trained language models by utilizing smaller, reverse-engineered transformer networks for specific algorithmic tasks

## 2 Prior Work

Our work is inspired by the recent results on the expressive power of Transformer networks and their in-context learning capabilities.

In [Pérez et al., 2021, Pérez et al., 2019, Wei et al., 2022a] the authors explore the computational properties of Transformers establishing that they are Turing complete, meaning that they can simulate a Turing machine. The constructions typically require high/infinite precision (apart from that of Wei et al. [2022a]), and recursion around attention layers. In Yun et al. [2019], the authors prove that given access to sufficient width/depth TFs can act as universal sequence to sequence approximators.

In Weiss et al. [2021], the authors propose a computational model for the transformer-encoder in the form of a domain-specific language called the Restricted Access Sequence Processing Language (RASP). The model maps the basic components of a TF encoder into simple primitives. Examples of tasks that could be learned by a Transformer are provided, and the maximum number of heads and layers necessary to encode a task in a transformer are analyzed.

In a recent and related work, Lindner et al. [2023] suggests using transformer networks as programmable units and introduces a compiler called Tracr which utilizes RASP. However, the expressivity limitations and unclear Turing completeness of the language are discussed in Weiss et al. [2021], Merrill et al. [2022], Lindner et al. [2023]. Our approach, in contrast, demonstrates the potential of transformer networks to serve as universal computers, enabling the implementation of arbitrary nonlinear functions and emulating iterative, non-linear algorithms. Furthermore, our framework allows the depth of our transformers to not scale in proportion to the lines of code that they execute, allowing the implementation of iterative algorithms, expanding the potential applications.

In Garg et al. [2022] the authors demonstrate that standard Transformers (e.g., GPT-2) can be trained from scratch to perform in-context learning of linear functions and more complex model classes, such as two-layer neural networks, with performance that matches or exceeds task-specific learning algorithms. A useful element of their analysis is the fact that language is completely removed from the picture, and they perform all operations on the level of vector embeddings. This allows a higher abstraction level than using language as an input, and in fact is what also allows us to obtain our derivations.

Motivated by the above experimental work, in Akyürek et al. [2022], the authors investigate the hypothesis that TF-based in-context learners emulate standard learning algorithms implicitly at inference time. The authors provide evidence for this hypothesis by constructing transformers that implement SGD for linear models, showing that trained in-context learners closely match the predictors computed by these algorithms.

In a similar vein, von Oswald et al. [2022] argues that training Transformers on auto-regressive tasks is closely related to gradient-based meta-learning formulations. The authors also provide a hard-coded weight construction showing the equivalence between data transformations induced by a single linear self-attention layer and gradient descent on a regression loss. The authors empirically show that when training linear attention TFs on simple regression tasks, the models learned by GD and Transformers have intriguing similarities.

In Liu et al. [2022], the authors test the hypothesis that TFs can perform algorithmic reasoning using fewer layers than the number of reasoning steps, in the context of finite automata. The authors characterized "shortcut solutions" that allow shallow Transformer models to exactly replicate the computation of an automaton on an input sequence, and showed that these solutions can be learned through standard training methods. As is expected this hypothesis is only true for a certain family of automata, as the general existence of shortcut solutions would imply the collapse of complexity classes that are widely believed not to be identical.

Other experimental studies have utilized recursion in transformer architectures in a similar manner to our constructions, although in our case we only utilize a single recursive link that feeds the output of the transformer back as an input [Hutchins et al., 2022, Shen et al., 2022, Dehghani et al., 2018].

## 3 Preliminaries

The transformer architecture. Our work follows a similar problem setting as previous studies (e.g. Yun et al. [2019], Garg et al. [2022], Akyürek et al. [2022], von Oswald et al. [2022]) in which the input sequence consists of $d$-dimensional embedding vectors rather than tokens. This
simplifies our results without sacrificing generality, as an embedding layer can map tokens to the desired vector constructions.

The input to each layer, $\mathrm{X} \in \mathbb{R}^{d \times n}$, is a vector representation of a sequence of $n$ tokens, where each token is a $d$-dimensional column. In this paper, the terms "token" and "column" may be used interchangeably.

A transformer layer outputs $f(\mathbf{X})$, where $f$ is defined as follows:

$$
\begin{align*}
\operatorname{Attn}(\mathbf{X}) & =\mathbf{X}+\sum_{i=1}^{H} \mathbf{V}^{i} \mathbf{X} \sigma_{\mathbf{S}}\left(\mathbf{X}^{\top} \mathbf{K}^{i \top} \mathbf{Q}^{i} \mathbf{X}\right)  \tag{1a}\\
f(\mathbf{X}) & =\operatorname{Attn}(\mathbf{X})+\mathbf{W}_{2} \operatorname{ReLU}\left(\mathbf{W}_{1} \operatorname{Attn}(\mathbf{X})+\mathbf{b}_{1} \mathbf{1}_{n}^{\top}\right)+\mathbf{b}_{2} \mathbf{1}_{n}^{\top} \tag{1b}
\end{align*}
$$

where $\sigma_{\mathrm{S}}$ is the softmax function applied on the columns of the input matrix, i.e.,

$$
\left[\sigma_{\mathrm{S}}(\mathbf{X}, \lambda)\right]_{i, j}=\frac{e^{\lambda X_{i, j}}}{\sum_{k=1}^{n} e^{\lambda X_{k, j}}}
$$

where $\lambda \geq 0$ is the temperature parameter, $\operatorname{ReLU}(x)=x \cdot 1_{x>0}$ is the $\operatorname{ReLU}$ activation, and $\mathbf{1}_{n}$ is the all ones vector of length $n$. We refer to the $\mathbf{K}, \mathbf{Q}$, and $\mathbf{V}$ matrices as the key, query, and value matrices respectively ${ }^{1}$; the superscript $i$ that appears on the weight matrices indicates those corresponding to the $i$-th attention head.Consistent with previous literature, the first equation Eq. (1a) represents the attention layer. We refer to the combination of attention and ReLU layers as a single transformer layer.

Iterative computation through a simple loop. In the following sections, we utilize TF networks with multiple transformer layers. Let us refer to the output of such a multilayer $\mathrm{TF}$ as $\mathrm{TF}(\mathbf{W} ; \mathbf{X})$, where for simplicity $\mathbf{W}$ is the collection of all weight matrices required to define such a multi-layer TF.

We use our constructions recursively, and feed the output back as an input sequence, allowing the network to perform iterative computation through a simple fixed-point like iteration. This recursive transformer is similar to past work on adding recursion to TF networks. We refer to these simple recursive TFs as Looped Transformers.

Feeding the output back to its input is similar to how a

Algorithm 1

Looped Transformer

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-06.jpg?height=52&width=282&top_left_y=1855&top_left_x=1363)
\mathbf { X } \leftarrow \operatorname { T F } ( \mathbf { W } ; \mathbf { X } )
end for

``` traditional computer processes machine code, where it continually reads/writes data in memory, by executing one instruction at a time. The input sequence $\mathbf{X}$ includes the instructions and memory. Similar to how a CPU processes each line of code in a program, the transformer network processes parts of the input sequence to perform complex computations. Like a CPU, the TF acts as a self-contained computational unit. The use of loops in this process is analogous to how CPUs operate using cycles.
\footnotetext{
${ }^{1}$ We'd like to note that typically the weight matrices are denoted as $\mathbf{W}_{Q}, \mathbf{W}_{K}, \mathbf{W}_{V}$ but to make notation cleaner, we use instead $\mathbf{Q}, \mathbf{K}, \mathbf{V}$.
}

While the analogy between TFs and CPUs can be entertaining, there are also many differences in implementation. It is important to keep these differences in mind and not rely too heavily on the analogy. The results obtained from using TFs as computational units do not require the analogy to be valid.

To be able to build compute boxes out of a TF network, it is crucial to format the input sequence $\mathbf{X}$ in a way that separates memory, a cache-like scratchpad, and commands.

Input sequence format. The input to our transformer network has the following abstract form:

$$
\begin{equation*}
\mathbf{X}=\left[\right] \tag{2}
\end{equation*}
$$

where $\mathrm{S}$ represents the portion of the input that serves as a "scratchpad," $\mathrm{M}$ represents the portion that acts as memory that can be read from and written to, and $\mathbf{C}$ represents the portion that contains the commands provided by the user. The $\mathbf{p}_{1}, \ldots, \mathbf{p}_{n}$ are positional encodings for the $n$ columns, which will be described in more detail in the following paragraph, and will be used as pointers to data and instructions. The structure of our input sequence bares similarities to that of Wei et al. [2022a], Akyürek et al. [2022] that also use scratchspace, and have a separate part for the input data.

Scratchpad. The scratchpad is a crucial component of our constructions. This is the central location where the inputs and outputs of all computation are recorded. It is perhaps useful to think of this as an analogue to a CPU's cache memory. It functions as a temporary workspace where data is copied, transformed, and manipulated in order to perform a wide variety of operations, ranging from simple arithmetic to more complex tasks such as matrix inversion. Regardless of the specific computation that is performed, the data necessary for the operation is always transferred from the memory to the scratchpad, and once the computation is completed, the data is transferred back to the memory. This allows the TF to perform the necessary calculations in a designated area, separate from other parts of the input sequence.

Memory. All the compute boxes we create require memory to perform specific actions. The memory component of the input sequence serves as a storage location for data. This data can take various forms, including scalars, vectors, and matrices, and is subject to manipulation through various operations. When computation is needed, the data is first copied from the memory to the scratchpad, where it is updated and transformed as necessary. Once the computation is complete, the updated data is then returned and copied back to the memory for future use or reference. In this way, the memory serves as a central repository for all relevant data, allowing it to be accessed and manipulated as needed.

Commands. Our framework implements a set of commands within a transformer network; these serve as instructions that guide the internal functioning of the transformer, similar to a low-level programming language. These commands include indicators for memory locations and operation directives, allowing the TF to execute complex computations and tasks in a consecutive and organized manner.

\section*{4 Building Transformer Blocks towards General Computation}

To build general compute boxes using transformer networks, specialized compute blocks are required. These blocks will be assembled to create the desired end functionality. In this section, we highlight various operations that transformer layers can perform. These operations will serve the building blocks to create more complex routines and algorithms. These operations are designed to be interoperable with each other, leveraging the ability of attention to perform various tasks, such as producing approximate permutation matrices and approximating general functions through sigmoid activations.

In the following sections, we focus on the fundamental components necessary to emulate a general-purpose computer, re-
![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-08.jpg?height=596&width=894&top_left_y=362&top_left_x=974)

Figure 2: A sketch of the three transformer blocks used as building blocks to implement a small instruction-set computer. These blocks handle edits in the input sequence (such as moving or copying from one block to another), keep track of the program counter, and execute a program counter jump if a specified condition is met. serving the examination of how attention can replicate sigmoid-based functions in the sections that follow.

\subsection*{4.1 Positional Encodings, Program Counter, and Data Pointers}

To aid the transformer in locating the position of each token, each column of $\mathbf{X}$ is appended with positional encodings that is based on the column index. In this case, similar to Wei et al. [2022a], the positional encodings is the binary representation of the column index, which is appended to each column to keep the encoding dimension low, i.e., logarithmic in the sequence length. This approach to using positional encodings is slightly different from the typical method of adding them to the encodings of the input sequence. However, in this case, appending them as suffixes to the encodings allows for cleaner arguments and constructions.

In particular, the encoding for token/column indexed by $i$ is a $\log (n)$-dimensional $\pm 1$ binary vector $\mathbf{p}_{i} \in \pm 1^{\log (n)}$, where $n$ is the length of the input sequence. Using the standard binary representation of an integer $i$, meaning $i=\sum_{k=0}^{\log (n)-1} 2^{k} \cdot b_{k}$, the positional encoding vector $\mathbf{p}_{i}$ is set to -1 at index $j$ if the binary representation of $i$ has 0 at the $j$-th index, i.e., $b_{i}=0$, otherwise it is +1 . As a result, we have $\mathbf{p}_{i}^{T} \mathbf{p}_{i}=\log (n)$ and by Cauchy-Schwarz inequality, $\mathbf{p}_{i}^{T} \mathbf{p}_{j}<\left|\mathbf{p}_{i}\right|\left|\mathbf{p}_{j}\right|=\sqrt{\log (n)} \sqrt{\log (n)}=\log (n)$ whenever $i \neq j$, since $\mathbf{p}_{i}, \mathbf{p}_{j}$ differ in at least one coordinate.

In the applications presented, the transformer often needs to execute iterative algorithms or go through a sequence of commands. To achieve this, we utilize a program counter that iterates through the commands. The counter contains the encoding of the location where the next command is stored. Additionally, a command may have data pointers that point to the location of the data the command needs to read and write to. Both the program counter and data pointers utilize the same
positional encodings as discussed in the previous paragraph. Using binary vectors as positional encodings allows us to easily increment the program counter by 1 (or any other amount) using the feed forward ReLU layers in the transformer architecture (1). This is formalized in the following lemma, for the proof see Lemma 16.

Lemma 1. Given two d-dimensional binary vectors representing two non-negative integers, there exists a 1-hidden layer feedforward network with ReLU activation, containing 8d activations in the hidden layer and d neurons in the output layer, that can output the binary vector representation of their sum, as long as the sum is less than $2^{d+1}$.

Our positional encoding scheme can also be used to point to specific data locations for reading or writing, as discussed in the following section. This is achieved by using the same binary vectors as positional encodings for both the program counter and data pointers. Furthermore, this technique for pointing to specific data locations enables the transformer to effectively read and write from/to data during the execution of the algorithm or sequence of commands that is build to implement.

\section*{4.2 read / write: Copying Data/Instructions to/from the Scratchpad}

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-09.jpg?height=502&width=1111&top_left_y=1180&top_left_x=499)

Figure 3: A sketch of the read operation. Arrows show command blocks being copied from the part of the input that is allocated to commands to the scratchpad. Typically an instruction is another set of pointers. Positional encodings and counters are used for tracking what is copied where.

As previously stated, the scratchpad serves as a temporary memory for storing all information needed for computation. This includes copying commands and data to it, performing computation, and writing results back to memory. This process has similarities with the copy/write mechanism developed in Akyürek et al. [2022].

The following lemma states that the command pointed to by the program counter or the data from a location specified in the current command can be copied to the scratchpad for further computation. The location of the program counter is conventionally placed right below the contents of the scratchpad, but it can be changed arbitrarily. Keeping it in a specific location throughout the entire computation helps retain a good organization of the construction.

Lemma 2 (read). A transformer with one layer, one head, and width of $O(\log n+d)$, where $d$ is the dimension of the data vectors and $n$ is the length of the input, can read data/command vectors
from the input to the scratchpad from the location pointed to by the position embedding vector in the scratchpad.

Proof. Consider a simplified input where the scratchpad only has one column, and we have positional encodings, denoted as $\mathbf{p}_{i}$, that point to the location where data or commands should be copied from. In this case, the operation we want to perform is as follows:

$$
\mathbf{X}=\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{i} & \cdots \\
\boldsymbol{v}_{1} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
1 & 0 & \ldots & 0 & \ldots
\end{array}\right] \rightarrow\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{i} & \cdots \\
\boldsymbol{v}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
1 & 0 & \ldots & 0 & \cdots
\end{array}\right]
$$

which moves data/command embedding vector $\boldsymbol{v}_{i}$ from the memory/command part of the input to the scratchpad. The first row contains the data to be read, the second row has the data written in the scratchpad, the third row contains the program counter, the fourth row contains the positional encodings, the fifth row is used by for temporary storage and the last row is just a bit that indicates whether the column is in the scratchpad or not.

We use the following key and query matrices: $\mathbf{K}=\mathbf{Q}=\left[\begin{array}{llllll}\mathbf{0} & \mathbf{0} & \mathbf{I} & \mathbf{I} & \mathbf{0} & 0\end{array}\right]$, so that the key and query become equal to $\mathbf{K X}=\mathbf{Q X}=\left[\begin{array}{lllll}\mathbf{p}_{i} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots\end{array}\right]$, and hence,

$$
(\mathbf{K X})^{\top} \mathbf{Q X}=\left[\begin{array}{ccc}
\mathbf{p}_{i}^{\top} \mathbf{p}_{i} & \mathbf{p}_{i}^{\top} \mathbf{p}_{2} & \cdots \\
\mathbf{p}_{2}^{\top} \mathbf{p}_{i} & \mathbf{p}_{2}^{\top} \mathbf{p}_{2} & \cdots \\
\vdots & \vdots & \vdots \\
\mathbf{p}_{i}^{\top} \mathbf{p}_{i} & \mathbf{p}_{i}^{\top} \mathbf{p}_{2} & \cdots \\
\vdots & \vdots & \vdots
\end{array}\right]
$$

Recall that $\mathbf{p}_{i}$ is a $\log (n)$-dimensional $\pm 1$ vector such that $\mathbf{p}_{i}^{T} \mathbf{p}_{i}=\log (n)$ and each $\mathbf{p}_{i}^{T} \mathbf{p}_{j} \leq$ $\log (n)-1$ for $j \neq i$. We show in the appendix that if we apply the softmax with temperature $\lambda \geq \log \frac{n^{3}}{\epsilon}$, we have $\sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)$ to be an $n \times n$ matrix of the following form

$$
\left[\begin{array}{ccccccc}
\frac{1}{2} & 0 & 0 & \cdots & \frac{1}{2} & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
\frac{1}{2} & 0 & 0 & \cdots & \frac{1}{2} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 0 & \cdots & 1
\end{array}\right]+\epsilon \mathrm{M}=\left[\begin{array}{llllll}
\frac{\boldsymbol{e}_{1}+\boldsymbol{e}_{i}}{2} & \boldsymbol{e}_{2} & \boldsymbol{e}_{3} & \cdots & \frac{\boldsymbol{e}_{1}+\boldsymbol{e}_{i}}{2} & \cdots
\end{array}\right]+\epsilon \mathrm{M}
$$

where $\boldsymbol{e}_{i}$ is the $i$ th column of the identity matrix, $\|\mathbf{M}\| \leq 1$, and $\epsilon$ is as defined in Appendix B. For the purpose of the proof, we ignore the error term $\epsilon \mathrm{M}$, because it can be reduced arbitrarily by increasing the temperature (it can be made precisely equal to 0 , if we consider hardmax instead of softmax), and overall does not limit us from deriving arbitrarily small error bounds.

Next we set the output and value weight matrices as follows

$$
\mathbf{V}=\left[\begin{array}{llllll}
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & 0 \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & 0 \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & 0 \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & 0 \\
\mathbf{I} & \mathbf{I} & \mathbf{0} & \mathbf{0} & \mathbf{0} & 0 \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & 0
\end{array}\right]
$$

Using this, the output of the head is

$$
\mathbf{X}+\mathbf{V X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{i} & \cdots \\
\boldsymbol{v}_{1} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\frac{v_{1}+\boldsymbol{v}_{i}}{2} & \boldsymbol{v}_{2} & \cdots & \frac{v_{1}+\boldsymbol{v}_{i}}{2} & \cdots \\
\hline 1 & 0 & \cdots & 0 & \cdots
\end{array}\right]
$$

Each column above has the following form:

$$
\left[\begin{array}{c}
\boldsymbol{v}_{\text {orig }}^{0} \\
\boldsymbol{v}_{\text {orig }}^{1} \\
\boldsymbol{v}_{\text {orig }} \\
\mathbf{p}^{(0)} \\
\mathbf{p}^{(1)} \\
\boldsymbol{v}_{\text {new }} \\
b
\end{array}\right],
$$

where $\boldsymbol{v}_{\text {orig }}^{(0)}$ and $\boldsymbol{v}_{\text {orig }}^{(1)}$ are the original value vectors (present in the top two row blocks) contained in that column, $\mathbf{p}^{(0)}$ and $\mathbf{p}^{(1)}$ are the corresponding embeddings of each column, $\boldsymbol{v}_{\text {new }}$ is the new value, and $b$ is the bit indicating whether the column is part of the scratchpad or not.

The feedforward layers have the following form:

$$
\begin{aligned}
& \boldsymbol{v}_{\text {orig }}^{(1)}:=\boldsymbol{v}_{\text {orig }}^{(1)}+\operatorname{ReLU}\left(C(b-1) \mathbf{1}+2 \boldsymbol{v}_{\text {new }}-2 \boldsymbol{v}_{\text {orig }}^{(1)}\right)-\operatorname{ReLU}\left(C(b-1) \mathbf{1}-2 \boldsymbol{v}_{\text {new }}+2 \boldsymbol{v}_{\text {orig }}^{(1)}\right) \\
& \boldsymbol{v}_{\text {new }}:=\boldsymbol{v}_{\text {new }}-\operatorname{ReLU}\left(\boldsymbol{v}_{\text {new }}\right)+\operatorname{ReLU}\left(-\boldsymbol{v}_{\text {new }}\right)=\mathbf{0}
\end{aligned}
$$

where $C$ is a large positive constant. The first equation is performing the operation of subtracting $\boldsymbol{v}_{\text {new }}$ from $\boldsymbol{v}_{\text {orig }}$ but only when the sum and difference of $C(b-1) \boldsymbol{1}$ and $\boldsymbol{v}_{\text {new }}$ are positive, otherwise the subtraction does not occur. The second equation is resetting the value of $\boldsymbol{v}_{\text {new }}$ to zero after it has been copied to $\boldsymbol{v}_{\text {orig }}$, where $\operatorname{ReLU}\left(-\boldsymbol{v}_{\text {new }}\right)$ is the rectified linear unit (ReLU) applied to the negative of $\boldsymbol{v}_{\text {new }}$.

It can be verified that the output of the feedforward layers would then be the desired result

$$
\mathbf{X}=\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{i} & \cdots \\
\boldsymbol{v}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
1 & 0 & \ldots & 0 & \cdots
\end{array}\right]
$$

The next lemma explains that the vector $\boldsymbol{v}$ stored in the scratchpad can be copied to a designated location in memory, as specified within the scratchpad itself. This allows for the transfer of data from the scratchpad to a specific location in memory for further use or storage.

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-12.jpg?height=507&width=1116&top_left_y=554&top_left_x=510)

Figure 4: A sketch of the write operation. Arrows show data blocks being copied from the scratchpad to a designated location in the part of the input allocated for memory. Positional encodings are used for tracking the destination location and ensuring data is written at the correct memory location.

Lemma 3 (write). A transformer network with a single layer, one head, and width $O(\log n+d)$, where $d$ is the dimension of the data vectors and $n$ is the length of the input, can effectively write a data vector stored in the scratchpad to a specific location in the input, as designated by a positional encoding vector in the scratchpad.

Proof. We want to achieve the following operation

$$
\mathbf{X}=\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{i} & \cdots \\
\boldsymbol{v}_{1} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
1 & 0 & \ldots & 0 & \ldots
\end{array}\right] \rightarrow\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{1} & \cdots \\
\boldsymbol{v}_{1} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
1 & 0 & \ldots & 0 & \cdots
\end{array}\right]
$$

The construction for this is identical to the one for read (see the proof of Lemma 2), except that the feedforward layers are outputting the following:

$$
\begin{aligned}
& \boldsymbol{v}_{\text {orig }}^{(0)}:=\boldsymbol{v}_{\text {orig }}^{(0)}+\operatorname{ReLU}\left(-C b \mathbf{1}+2 \boldsymbol{v}_{\text {new }}-2 \boldsymbol{v}_{\text {orig }}^{(0)}\right)+\operatorname{ReLU}\left(-C b \mathbf{1}-2 \boldsymbol{v}_{\text {new }}+2 \boldsymbol{v}_{\text {orig }}^{(0)}\right) \\
& \boldsymbol{v}_{\text {new }}:=\boldsymbol{v}_{\text {new }}-\operatorname{ReLU}\left(\boldsymbol{v}_{\text {new }}\right)+\operatorname{ReLU}\left(-\boldsymbol{v}_{\text {new }}\right)=\mathbf{0}
\end{aligned}
$$

where $C$ is a large positive constant. The first equation updates the value of a vector $\boldsymbol{v}_{\text {orig }}$ in memory with the value of a vector $\boldsymbol{v}_{\text {new }}$ from the scratchpad. The second equation is resetting the
new vector in the scratchpad to zero. It can be verified that the output of the feedforward layers would be

$$
\mathbf{X}=\left[\begin{array}{c|cccc}
\mathbf{0} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{1} & \cdots \\
\boldsymbol{v}_{1} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{p}_{i} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
\mathbf{0} & \mathbf{p}_{2} & \cdots & \mathbf{p}_{i} & \cdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \cdots \\
1 & 0 & \ldots & 0 & \ldots
\end{array}\right]
$$

\section*{4.3 if $\langle c o n d i t i o n\rangle$ then goto 〈instruction〉: Conditional branching}

In this subsection, we will implement a conditional branching instruction that evaluates a condition and sets the program counter to a specified location if the condition is true, or increments the program counter by 1 if the condition is false. The form of the command is as follows: if $\operatorname{mem}[a] \leq 0$, then goto $i$, where mem $[a]$ is a value of some location in the memory part of the input sequence. This command has two parts: evaluating the inequality and modifying the program counter accordingly.

The first thing we do is read from mem $[a]$, as described in the previous subsection. Then, we evaluate the inequality. Let us say that "flag" is the truth value of the inequality. Since we assume that for such conditional branching command, mem $[a]$ contains an integer, the following ReLU network can be used to compute the flag:

$$
\begin{equation*}
\text { flag }=1-\operatorname{ReLU}(\operatorname{mem}[a])+\operatorname{ReLU}(\operatorname{mem}[a]-1) \tag{3}
\end{equation*}
$$

In Section 5.1, we consider mem $[a]$ to be vectors contain the binary $\pm 1$ representation of integers. There we use 2's complement convention to represent negative integers. Let the vector be $\left[b_{N} \ldots b_{1}\right]$, where $b_{N}$ is the most significant bit and $b_{1}$ the least significant. As we explain in that section, the sign of $b_{N}$ indicates whether the integer is negative or positive (The number is negative if $b_{N}=+1$ and non-negative otherwise). Hence, the flag is 1 if $b_{N}=+1$ or if all the bits are -1 (which is the case when mem $[a]$ represents the integer 0 ).

$$
\begin{equation*}
\text { flag }=\operatorname{ReLU}\left(b_{N}\right)+\operatorname{ReLU}\left(1+N-\sum_{i=1}^{N} b_{i}\right) \tag{4}
\end{equation*}
$$

Let the current Program Counter be $\mathbf{p}_{\mathrm{PC}}$, which points to a given command. Thus, if flag is 1 , we want the program counter to "jump" and become $\mathbf{p}_{i}$, else if flag is 0 the program counter will be incremented by one, and set to be $\mathbf{p}_{\mathrm{PC}+1}$.

Consider that the simplified input currently has the following scratchpad

$$
\left[\begin{array}{ccccc}
* & * & \ldots & * & * \\
\text { flag } & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{\mathrm{PC}} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{i} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$
where ${ }^{\prime} *^{\prime}$ are inconsequential values. The incremented pointer, $\mathbf{p}_{\mathrm{PC}+1}$, can be computed using the pointer incrementing operation that we described in the Subsection 4.1, using one feedforward layer of (1b).Then,

$$
\mathbf{p}_{\text {next }}=2 \operatorname{ReLU}\left(\mathbf{p}_{\mathrm{PC}+1}-\mathbf{1} \text { flag }\right)+2 \operatorname{ReLU}\left(\mathbf{p}_{i}-\mathbf{1}(1-\mathrm{flag})\right)-1
$$

where 1 is the all ones vector. Notice that we can implement this with just the feed forward layers of Eq. (1b). To account for the residual connection we can add the expression $-\operatorname{ReLU}\left(\mathbf{p}_{\mathrm{PC}}\right)+$ $\operatorname{ReLU}\left(-\mathbf{p}_{\mathrm{PC}}\right)$ in the equation above.

Hence, this entire operation requires 3 feed forward layers of Eq. (1b), and hence 2 transformer layers. Note that to ensure that the attention layer of the transformer do not modify the input, we simply set the $\mathbf{V}$ matrix to zero in (1a).

\section*{5 Emulating a Generalized One-instruction Set Computer}

\subsection*{5.1 A SUBLEQ Transformer}

Mavaddat and Parhami [1988] showed that there exists an instruction such that any computer program can be translated to a program consisting of instantiation of this single instructions. A variant of such an instruction is SUBLEQ, where different registers, or memory locations are accessed. The way that SUBLEQ works is simple. It accesses two registers in memory, takes the difference of their contents and stores it back to one of the registers, and then if the result is negative it jumps to a different predefined line of code, or continues on the next instruction from the current line of code. ${ }^{2}$ A computer that is built to execute SUBLEQ programs is called an One-Instruction Set Computer, and is a universal computer, i.e., it is Turing Complete, if given access to infinite memory.

```

Algorithm 2 SUBLEQ $(a, b, c)$
$\operatorname{mem}[b]=\operatorname{mem}[b]-\operatorname{mem}[a]$
if mem $[b] \leq 0$ then
goto instruction $c$
else goto next instruction
end if

```

The following describes the construction of a looped transformer that can execute a program written in a specific set of instructions. The transformer keeps track of the lines of code, memory locations, and a program counter, using the memory part of the input as memory registers and the command part as lines of code/instructions. The scratchpad is used to record the additions and pointers involved in each instruction, and the read, write, and conditional branch operations are utilized.
\footnotetext{
${ }^{2}$ This version of the SUBLEQ instruction is a slightly restricted version of the original instruction; here we separate the memory / registers from the instructions. We show that this restriction does not make our version computationally less powerful by proving in Appendix C that our version is also Turing Complete.
}

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-15.jpg?height=177&width=957&top_left_y=288&top_left_x=581)

Figure 5: Graphical representation of the building blocks necessary to implement the OISC instruction. The first two blocks transfer the data/command to the scratchpad, the second and third implement the substraction and store the result, while the last one implements the if goto command that completes the instruction.

Lemma 4. There exists a looped transformer architecture that can run SUBLEQ programs. This architecture has nine layers, two heads, and a width of $O(\log (n)+N)$, where $n$ is the length of the input sequence that is proportional to the length of the program and memory used by the emulated OISC, and $N$ is the number of bits we use to store each integer. The integers are considered to be in the range $\left[-2^{N-1}+1,2^{N-1}-1\right]$

Before we present our construction some observations are in place.

The importance of loops. The use of a loop outside the transformer is crucial as it allows the computer to keep track of the program counter and execute the instructions in the correct order. Without this loop, the size of the transformer would have to scale with the number of lines of code, making the implementation impractical. Note that the overall complexity of running a SUBLEQ program is going to scale with the number of lines of code, which is to be expected given standard complexity theoretic assumptions on the circuit depth of functions. Note however that the depth of the looped transfromer itself does not scale with the size of the program.

Can we avoid the logarithmic width scaling? Finally note, that the width of the transformer scales logarithmically with the length of the program, and memory used. This is a side-effect of the bit-complexity of our positional encodings, and could be overcome by considering higher bit-complexity.

OISC as a basis for a more flexible attention-based computer. The following construction describes an implementation of a fully functioning one-instruction set computer (OISC) using a transformer architecture. The memory stores integers and the instructions are executed in a sequential manner. The key to this construction is the reverse engineering of the attention mechanism to perform read/write operations and taking full advantage of each piece of the transformer architecture, including the feedforward layers. This implementation serves as the foundation for a more general attention-based computer presented in the next subsection, where the subtraction of two contents of memory can be replaced with a general function, allowing for the implementation of arbitrary iterative algorithms.

Proof of Lemma 4. Looking at Algorithm 2, note that each instruction can be specified by just 3 indices, $a, b$, and $c$. Since we use binary representation of indices to form positional encodings and pointers, each of these indices can be represented by a $\log n$ dimensional vector. We represent
each instruction by simply concatenating these embedding vectors to form a $3 \log n$ dimensional vector as follows:

$$
\mathbf{c}=\left[\begin{array}{l}
\mathbf{p}_{a} \\
\mathbf{p}_{b} \\
\mathbf{p}_{c}
\end{array}\right]
$$

The input then takes the following form:

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-16.jpg?height=432&width=1463&top_left_y=543&top_left_x=255)

where $\mathbf{c}_{i} \in \mathbb{R}^{3 \log (n)}, \mathbf{M} \in \mathbb{R}^{N \times m}$ and $\mathbf{X} \in \mathbb{R}^{(8 \log (n)+3 N+1) \times n}$. The first $s$ columns constitute the scratchpad, the next $m$ constitute the memory section, and the last $n-m-s$ columns contain the instructions.

The program counter, $\mathbf{p}_{\mathrm{PC}}$ points to the next instruction that is to be executed, and hence it is initialized to the first instruction as $\mathbf{p}_{\mathrm{PC}}:=\mathbf{p}_{s+m+1}$. The contents of the memory section are $N$ dimensional $\pm 1$ binary vectors which represent the corresponding integers. We follow the 2's complement convention to represent the integers, described as follows. Let's say the bits representing an integer are $b_{N}, \ldots, b_{1}$, with $b_{N}$ being the most significant bit. Then,

1. If $b_{N}=-1$, then the integer is considered positive with the value $\sum_{i=1}^{N-1} 2^{i-1} \frac{b_{i}+1}{2}$.

2. If $b_{N}=+1$, then the integer is considered negative with the value $-2^{N-1}+\sum_{i=1}^{N-1} 2^{i-1} \frac{b_{i}+1}{2}$.

Step 1 - Read the instruction $\mathrm{c}_{\mathrm{PC}}$. The first thing to do is to read and copy the instruction pointed to by $\mathbf{p}_{\mathrm{PC}}$ in the scratchpad. The current instruction is located at column index PC, and is pointed to by the current program counter $\mathbf{p}_{\mathrm{PC}}$. The instruction, $\mathbf{c}_{\mathrm{PC}}$ consists of three pointers, each of length $\log n$. In particular we copy the elements at the location $(1: 3 \log (n), \mathrm{PC})$ to the location $(3 \log (n)+4: 6 \log (n)+3,1)$. This can be done using the read operation as described in Section 4.2. Hence, after this operation, the input looks as follows:

$$
\mathbf{X}=\left[\begin{array}{cc|c|ccccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{c}_{1} & \mathbf{c}_{2} & \ldots & \mathbf{c}_{n-m-s} & \mathbf{c}_{\mathrm{EOF}} \\
\mathbf{0} & \mathbf{0} & \mathbf{M} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{c}_{\mathrm{PC}} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{\mathrm{PC}} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{p}_{2: s} & \mathbf{p}_{s+1: s+m} & \mathbf{p}_{s+m+1} & \mathbf{p}_{s+m+2} & \ldots & \mathbf{p}_{n-1} & \mathbf{p}_{n} \\
1 & 1_{2: s} & 0_{s+1: s+m} & 0_{s+m+1} & 0_{s+m+2} & \ldots & 0_{n-1} & 0_{n}
\end{array}\right]
$$

$$
=\left[\begin{array}{cc|c|ccccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{c}_{1} & \mathbf{c}_{2} & \ldots & \mathbf{c}_{n-m-s-1} & \mathbf{c}_{\mathrm{EOFE}} \\
\mathbf{0} & \mathbf{0} & \mathbf{M} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{a} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{b} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{c} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{\mathrm{PC}} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{p}_{2: s} & \mathbf{p}_{s+1: s+m} & \mathbf{p}_{s+m+1} & \mathbf{p}_{s+m+2} & \ldots & \mathbf{p}_{n-1} & \mathbf{p}_{n} \\
1 & 1_{2: s} & 0_{s+1: s+m} & 0_{s+m+1} & 0_{s+m+2} & \ldots & 0_{n-1} & 0_{n}
\end{array}\right]
$$

This step can be done in one layer.

Step 2 - Read the data required by the instruction. We need to read the data that the columns $a, b$ contain. To do so, we again use the read operation on the pointers $\mathbf{p}_{a}, \mathbf{p}_{b}$. Note that we need two heads for this operation, one each for reading $a$ and $b$. The resulting output sequence looks like

$$
\mathbf{X}=\left[\begin{array}{cc|c|ccccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{c}_{1} & \mathbf{c}_{2} & \ldots & \mathbf{c}_{n-m-s-1} & \mathbf{c}_{\mathrm{EOF}}  \tag{6}\\
\mathbf{0} & \mathbf{0} & \mathbf{M} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\operatorname{mem}[a] & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\operatorname{mem}[b] & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{a} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{b} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{c} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{\mathrm{PC}} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{p}_{2: s} & \mathbf{p}_{s+1: s+m} & \mathbf{p}_{s+m+1} & \mathbf{p}_{s+m+2} & \ldots & \mathbf{p}_{n-1} & \mathbf{p}_{n} \\
1 & 1_{2: s} & 0_{s+1: s+m} & 0_{s+m+1} & 0_{s+m+2} & \ldots & 0_{n-1} & 0_{n}
\end{array}\right]
$$

This step can be done in one layer.

Step 3 - Perform subtraction. Let $\boldsymbol{x}$ denote a column of the input $\mathbf{X}$. Let it have the following structure:

$$
\boldsymbol{x}=\left[\begin{array}{c}
* \\
* \\
\boldsymbol{b}_{r} \\
\boldsymbol{b}_{s} \\
* \\
* \\
* \\
* \\
*
\end{array}\right],
$$

where each entry above represents the corresponding column element of the matrix $\mathbf{X}$ in (6). Thus, $\boldsymbol{b}_{r}=\operatorname{mem}[a], \boldsymbol{b}_{s}=\operatorname{mem}[b]$ for the first column, and $\boldsymbol{b}_{r}=\boldsymbol{b}_{s}=\mathbf{0}$ otherwise.

Hence, to perform $\boldsymbol{b}_{s-r}$, we first need to compute the binary representation of $-r$, which is $\boldsymbol{b}_{-r}$, and then simply add it to $\boldsymbol{b}_{s}$. To compute $\boldsymbol{b}_{-r}$, which is the 2 's complement of $\boldsymbol{b}_{r}$, we just need to flip the bits of $\boldsymbol{b}_{r}$ and add 1. Bit flipping a $\pm 1$ bit can be done with a neuron simply as $b_{\text {flipped }}=2 * \operatorname{ReLU}(-b)-1$. For adding 1, we can use Lemma 16. Hence, each of these operations can be done using 1 ReLU layer of width $O(N)$, and so we need 2 transformer layers to perform this (Here we make the intermediate attention layers become the identity mapping by setting their value matrices to $\mathbf{0}$ ). Finally, we need one more ReLU layer to add $\boldsymbol{b}_{s}$, $\boldsymbol{b}_{-r}$, hence bringing the total to 3 transformer layers.

This results in the following:

$$
\mathbf{X}=\left[\begin{array}{cc|c|ccccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{c}_{1} & \mathbf{c}_{2} & \ldots & \mathbf{c}_{n-m-s-1} & \mathbf{c}_{\mathrm{E} O \mathrm{~F}} \\
\mathbf{0} & \mathbf{0} & \mathbf{M} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\operatorname{mem}[b]-\operatorname{mem}[a] & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{a} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{b} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{c} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{\mathrm{PC}} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{p}_{2: s} & \mathbf{p}_{s+1: s+m} & \mathbf{p}_{s+m+1} & \mathbf{p}_{s+m+2} & \ldots & \mathbf{p}_{n-1} & \mathbf{p}_{n} \\
1 & 1_{2: s} & 0_{s+1: s+m} & 0_{s+m+1} & 0_{s+m+2} & \ldots & 0_{n-1} & 0_{n}
\end{array}\right]
$$

Note that since this can be done in the feedforward layers of the previous step, this does not require an additional layer.

Step 4 - Write the result back to memory. Writing mem $[b]-\operatorname{mem}[a]$ back to location $b$ can be done using the pointer $\mathbf{p}_{b}$ and the set of embeddings and applying the write operation described in Section 4.2. This operation requires one layer.

Step 5 - Conditional branching. We first use Eq. (4) as described in Section 4.3 to create the flag, which is 1 if mem $[b]-\operatorname{mem}[a] \leq 0$ and 0 otherwise. This can be done using the Eq. (1b) of the transformer. Thus, we have

$$
\mathbf{X}=\left[\begin{array}{cc|c|ccccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{c}_{1} & \mathbf{c}_{2} & \ldots & \mathbf{c}_{n-m-s-1} & \mathbf{c}_{\mathrm{EOF}}  \tag{7}\\
\mathbf{0} & \mathbf{0} & \mathbf{M} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\text { flag } & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \\
\mathbf{p}_{a} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{b} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{c} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{\mathrm{PC}} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{p}_{2: s} & \mathbf{p}_{s+1: s+m} & \mathbf{p}_{s+m+1} & \mathbf{p}_{s+m+2} & \ldots & \mathbf{p}_{n-1} & \mathbf{p}_{n} \\
1 & 1_{2: s} & 0_{s+1: s+m} & 0_{s+m+1} & 0_{s+m+2} & \ldots & 0_{n-1} & 0_{n}
\end{array}\right]
$$

This operation requires one layer.

Next we use the construction described in Section 4.3 to choose, depending on the value of the flag, whether we want to increment the current program counter or we want to jump in the command $c$. Similar to Section 4.3, this step needs 2 layers of transformers.

Step 6 - Error Correction. Note that some of the steps above we incur some error while reading and writing due to the fact that we are using softmax instead of hardmax. This error can be made arbitrarily small by increasing the temperature of the softmax. In this step, we push the error down to zero. Note that all the elements of $\mathbf{X}$ can only be one of $\{-1,0,1\}$, with some additive error from reads and writes as explained before. Assume that the temperature is set high enough that the error is at most $\epsilon<0.5$. Then, a noisy bit $b$ can be fixed using the following ReLU:

$$
\begin{aligned}
b_{\text {noiseless }}= & \frac{1}{1-2 \epsilon}(\operatorname{ReLU}(b+1-\epsilon)-\operatorname{ReLU}(b+\epsilon)) \\
& +\frac{1}{1-2 \epsilon}(\operatorname{ReLU}(b-\epsilon)-\operatorname{ReLU}(b-1+\epsilon))-1
\end{aligned}
$$

This operation can be done with a single layer of transformer.

Step 7 - Program Termination. The special command $\mathbf{c}_{\mathrm{EOF}}$ is used to signal the end of a program to the transformer. This command is made up of three encodings: $\mathbf{p}_{s+1}, \mathbf{p}_{s+2}$, and $\mathbf{p}_{n}$. The first encoding, $\mathbf{p}_{s+1}$, points to the first entry in the memory, which we hard-code to contain the value 0 . The second encoding, $\mathbf{p}_{s+2}$, points to the second entry in the memory, which is hard-codeded to contain the value -1 . The third encoding, $\mathbf{p}_{n}$, points to itself, signaling the end of the program and preventing further execution of commands. Hence, on executing this command, the next command pointer is set to point to this command again. This ensures that the transformer maintains the final state of the input.
- For this, we ensure that the last instruction in each program is $\mathbf{c}_{\mathrm{EOF}}$, and that mem $[s+1]=0$ and $\operatorname{mem}[s+2]=-1$.
- For this case $a=s+1, b=s+2$, and $c=n$.
- The memory is updated with the value $\operatorname{mem}[b]=\operatorname{mem}[b]-\operatorname{mem}[a]$. Since $\operatorname{mem}[a]=0$ here, the memory remains unchanged.
- Since mem $[b] \leq 0$ here, the branch is always true and thus the pointer for the next instruction is again set to point to $\mathbf{c}_{\mathrm{EOF}}$.

\subsection*{5.2 FLEQ: A More Flexible Attention-based Computer}

In this section, we introduce FLEQ, a generalization of SUBLEQ that defines a more flexible reduced-instruction set computer. This implied set of additional instructions is based on a more advanced version of SUBLEQ that allows for the implementation of multiple functions within the same transformer network. This is achieved by generalizing the previous OISC construction to include not just addition of registers, but any function from a set of $M$ predefined functions implementable by a transformer network. In the following, we use the term FLEQ to refer interchangably to the instruction, the language, and the attention-based computer it defines.

The design of FLEQ allows for the implementation of complex and sophisticated algorithms by generating more general functions beyond simple subtraction, such as matrix multiplication, computation of square roots, activation functions, etc. This not only increases the flexibility of the system, but also makes it possible to implement nonlinear computations, linear algebra calculations, and iterative optimization algorithms for in-context learning while containing the length of the corresponding programs.

Definition 1. Let $\mathcal{T}_{i}$ be a transformer network of the form (1) with $l_{i}$-layers, $h_{i}$-heads and dimensionality $r$. We call this a "transformer-based function block" if it implements a function $f(\mathbf{A}, \mathbf{B})$ where the input and output sequence format is assumed to be the following: $\mathbf{A} \in \mathbb{R}^{d_{h} \times d_{w}}$ is assumed to be provided in the first set of d columns (columns 1 to d) and $\mathbf{B} \in \mathbb{R}^{d_{h} \times d_{w}}$ the second set of $d$ columns (columns $d+1$ to $2 d$ ); after passing the input through the $l_{i}$ layers, the output of $f(\mathbf{A}, \mathbf{B}) \in \mathbb{R}^{d_{h} \times d_{w}}$ is stored in the third d columns (columns $2 d+1$ to $3 d$ ), where d is the maximum size that the input could have and it is a constant that we determine. Note that $d_{h}, d_{w} \leq d$. Finally, the sequence length of the block is $s \geq 3 d$. Similarly to $d$, $s$ is a predetermined constant.

The parameters $\mathbf{A}, \mathbf{B}$ can be scalars, vectors or matrices as long as they can fit within a $d \times d$ matrix. Hence, the above definition is minimally restrictive, with the only main constraint being the input and output locations. More details about the input and output requirements will be explained towards the end of this subsection.

Theorem 2. Given $M$ different transformer-based function blocks $\mathcal{T}_{1}, \cdots, \mathcal{T}_{M}$, there exists a transformer $\mathcal{T}$ of the form (1) with number of layers $9+\max \left\{l_{1}, \cdots, l_{M}\right\}$, a number of $\sum_{i=1}^{M} h_{i}$ heads, and dimensionality $O(M d+\log n)$ such that running it recurrently $T$ times can run $T$ instructions of any program where each instruction is $\operatorname{FLEQ}\left(a, b, c, m\right.$, flag, $\left.p, d_{h}, d_{w}\right)$, and executes the following:

$$
\begin{equation*}
\operatorname{mem}[c]=f_{m}(\operatorname{mem}[a], \operatorname{mem}[b]) \quad ; \quad \text { if mem }[\text { flag }] \leq 0 \text { goto instruction } p \tag{8}
\end{equation*}
$$

Here $n$ is the total length of the program and we assume that mem $[f$ lag] is an integer. The parameters $d_{h}, d_{w}$ are explained in Remark 1 below.

Remark 1. Note that, the transformer $\mathcal{T}$ contains $M$ transformer-based function blocks and each one may use different input parameters. We thus define with $d$ the max length that each of the parameters $\mathbf{A}, \mathbf{B}, \mathbf{C}$ (stored in locations $a, b, c$ ) as in Definition 1 can have; this is a global constant and it is fixed for all the different instances that we can create. Now, $d_{h}, d_{w}$ refer to the maximum dimension that the parameters can have in a specific instance of the transformer $\mathcal{T}$; the rest of the columns $d-d_{w}$ and rows $d-d_{h}$ are set to zero.

The proof of this theorem can be found in Appendix D. Below we explain some of our design choices.

Execution cycle of the unified attention-based computer. In each iteration of the looped transformer, one instruction is fetched from the set of instructions in the input according to the program counter. The instruction is then copied to the scratchpad. Depending on the function to be implemented, a different function block location is used to locally record the results of that
function. Once the result is calculated, it is copied back to a specified memory location provided by the instruction. The execution cycle is similar to the one-instruction set computer (OISC) in the previous section, with the main difference being that for each instruction, we can choose from a pre-selected list of functions that take inputs in the form of arbitrary arrays of numbers, such as matrices, vectors, and scalars.

The format of the input sequence. In Fig. 6, we illustrate the input $\mathrm{X}$ to our looped transformer, which can execute a program written as a series of FLEQ instructions. Note that $\mathbf{X}$ is divided into three sections: Scratchpad, Memory, and Instructions. As in the left bottom part of Fig. 6, we allocate a separate part of the scratchpad for each of the $M$ functions that are internally implemented by the transformer. For example, if we have matrix multiplication and element-wise square root as two functions, we would allocate a different function block for each one.

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-21.jpg?height=504&width=1131&top_left_y=911&top_left_x=497)

Figure 6: The structure of input $\mathbf{X}$, to execute FLEQ commands.

This design may not be the most efficient, but our goal is to demonstrate the possibilities of looped transformers. Additionally, since the number of different functions is typically small in the applications we have in mind, the design does not significantly increase in size. The choice to reserve different function blocks for each predefined function is for convenience, as it allows for separate treatment of functions without worrying about potentially overlapping results. We believe that a design with a single function block is feasible, but it would significantly complicate the rest of the transformer construction.

Instruction format. The instruction in Theorem 2 is essentially a composition of the following two components: the function call to $f_{m}$ and the conditional branching (if ... goto ...). The instruction, located at the top right side of Fig. 6 contains the following components:

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-22.jpg?height=420&width=1442&top_left_y=294&top_left_x=409)

The goal of each positional encoding vector in Eq. (9) is to point to the corresponding space of the input where each component required by the instruction is located. To be specific, $\mathbf{p}_{a}$ and $\mathbf{p}_{b}$ point to the locations that the inputs $a$ and $b$ are located, $\mathbf{p}_{c}$ points to the location to which we will record the final result of the function $f_{m}$. Similarly, $\mathbf{p}_{m}$ points to the function block in the scratchpad that the intermediate computations required for $f_{m}$ are recording, $\mathbf{p}_{\text {flag }}$ points to the variable that we check if it is non-positive (the result is used for conditional branching), and $\mathbf{p}_{p}$ points to the address of the line of code that we would jump if the variable in pointed by $\mathbf{p}_{\text {flag }}$ is non-positive.

Execute a function; Jump to command. Recall that the first four parameters $(a, b, c, m)$ of FLEQ, as well as the last two $\left(d_{h}, d_{w}\right)$ are related to the implementation of the function block, while the other two (flag, $p$ ) are related with the conditional branching. Since there is no overlap between the two components of each instruction, it is possible to use each of these components independently. By having a fixed location flag ${ }_{0}$ where mem[flag $]$ is always set to 1 , we can have the simpler command $\operatorname{FLE} Q\left(a, b, c, m\right.$, flag $\left._{0}, p, d_{h}, d_{w}\right)$ which implements

$$
\operatorname{mem}[c]=f_{m}(\operatorname{mem}[a], \operatorname{mem}[b])
$$

Further, by having fixed locations $a_{0}, b_{0}, c_{0}$ which are not used elsewhere in the program, and hence inconsequential, we can have the simpler command FLEQ $\left(a_{0}, b_{0}, c_{0}, m\right.$, flag, $\left.p, d_{h}, d_{w}\right)$ which implements

$$
\text { if mem }[\text { flag }] \leq 0 \text { goto instruction } p
$$

Using this, we get the following corollary:

Corollary 1. The Unified Attention Based Computer presented in Theorem 2 can run programs where each instruction can be either of the following two simple instructions:
- $\operatorname{mem}[c]=f_{m}(\operatorname{mem}[a], \operatorname{mem}[b])$
- if mem $[$ flag $] \leq 0$ goto instruction $p$

Format of Transformer-Based Function Blocks. Recall that each function block is located at the bottom left part of the input $\mathbf{X}$, as shown in Fig. 6. Each transformer-based function block is expected to operate using the following format of the input:
- The number of rows in the input is $r$, while the number of columns is $s$ and $s \geq 3 d$. Here $s$ will dictate the total maximum number of columns that any transformer-based function block needs to operate. The reason that $s$ might be larger than $3 d$ has to do with the fact that some blocks may need some extra scratchpad space to perform some calculations.
- The function block specifies the dimensions of input and output. Say they are $d_{h} \times d_{w}$, where $d_{h}, d_{w} \leq d$. These will be part of the instruction which calls this function inside the FLEQ framework, as in (9).
- Suppose each function block has two inputs $\left(\mathbf{A} \in \mathbb{R}^{d_{h} \times d_{w}}\right.$ and $\mathbf{B} \in \mathbb{R}^{d_{h} \times d_{w}}$ ) and one output $f(\mathbf{A}, \mathbf{B})=\mathbf{C} \in \mathbb{R}^{d_{h} \times d_{w}}$. As in (10), the function block is divided into four parts: (1) the first input $\mathbf{A}$ is placed in the first $d_{h}$ rows and the first $d_{w}$ columns, (2) the second input $\mathbf{B}$ is placed in the first $d_{h}$ rows and the columns $d+1: d+d_{w}$, (3) the output $f(\mathbf{A}, \mathbf{B})=\mathbf{C}$ is in the first $d_{h}$ rows and the columns $2 d+1: 2 d+d_{w}$ columns and 4) the rest $s-3 d$ column used as scratchpad space for performing necessary calculations. Note that the unused columns are set to zero.
- The last $r-d_{h}$ rows can be used by the transformer-based function block in any way, e.g., to store any additional positional encodings.

We put the format of the input of each transformer-based function block in (10). The first input $\mathbf{A}=\left[\boldsymbol{z}_{a}^{1}, \cdots, \boldsymbol{z}_{a}^{d_{w}}\right]$ of the function is zero padded and stored in the first $d$ columns. Similarly, the second input $\mathbf{B}=\left[\boldsymbol{z}_{b}^{1}, \cdots, \boldsymbol{z}_{b}^{d_{w}}\right]$ is stored in the next $d$ columns. The output/result of the function block $\mathbf{C}=\left[\boldsymbol{z}_{c}^{1}, \cdots, \boldsymbol{z}_{c}^{d_{w}}\right]$ is located in the next $d$ columns while we have some extra $s-3 d$ columns which can be used as scratchpad.

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-23.jpg?height=208&width=1309&top_left_y=1430&top_left_x=511)

Let us consider the case where we wish to multiply a matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$, with a vector $\mathbf{b} \in \mathbb{R}^{d \times 1}$. The resulting output matrix would look as follows:

$$
\left[\begin{array}{ll|ll}
\mathbf{A} \mid \mathbf{b} & \mathbf{0} \mid \mathbf{A}^{\top} \mathbf{b} & \mathbf{0} \mid \mathbf{0}
\end{array}\right]
$$

Computational concerns: Do we need full attention? In our construction, the computational complexity of each layer depends on the number of embedding vectors that each part of the input has to attend to. Typically, this is quite sparse, as only a few of them need global attention. In our specific construction, only the columns within the scratchpad require global attention. By focusing only on these columns, we can reduce the computational complexity of the attention mechanism from $O\left(n^{2} d\right)$ to $O(n d)$, where $\mathrm{n}$ is the number of input sequences, $d$ is the dimension of the embedding vectors.

This reduction in computational complexity is achieved by limiting the attention mechanism to only the columns within the scratchpad, which helps to improve the overall efficiency of the model. Additionally, since the computational complexity grows linearly with the number of input sequences, rather than quadratically, it enables us to scale the model to handle larger input sequences.

\section*{6 Functions in the Unified Template Form}

In this section, we demonstrate how to implement a variety of nonlinear functions and basic linear algebra operations using transformers. These techniques will be crucial in the construction of iterative algorithms in the following sections. Each transformer-based function block in this section fits in our unified template in terms of input/output parameters' locations. We note here that each transformer-based function block might have its own positional encodings used to transfer the output in the correct place or perform some read/write operations and they are part of the design of the block.

\subsection*{6.1 Encoding Non-linear Functions within the Attention Mechanism}

One key ingredient of our constructions is encoding various functions within the attention mechanism. We do this by forcing the softmax to act as a sigmoid function and by storing multiple coefficients in the query and value weight matrices. As far as we know, this is the first work that shows how general non-linear functions can be emulated by attention layers. This allows us to create linear combinations of sigmoids that can be accessed by an indicator vector in the input. Our analysis is based on the result of Barron [1993] which we present below.

Definition 2. Let $\Gamma_{C, B}$ be the set of functions defined in a bounded domain $B, f: B \rightarrow \mathbb{R}, B \subseteq \mathbb{R}^{d}$ with a proper extension to $\mathbb{R}^{d}$ such that they have $C$ bounded Fourier integral, i.e., $\int \sup _{x \in B}|w \cdot x| F(d w) \leq C$ holds where $F(d w)$ is the magnitude of the Fourier distribution.

Definition 3. Given $\tau>0, C>0$ and a bounded set $B$, let

$$
G_{\sigma, \tau}=\left\{\gamma \sigma\left(\tau\left(\mathbf{a}^{T} \mathbf{x}+b\right)\right):|\gamma| \leq 2 C,\|\mathbf{a}\|_{B} \leq 1,|b| \leq 1\right\}
$$

where $\|\mathbf{a}\|_{B}=\sup _{\mathbf{x} \in B}\left\{\mathbf{x}^{T} \mathbf{a}\right\}$ and $\sigma$ is the sigmoid function, i.e., $\sigma(x)=\frac{1}{1+e^{-x}}$.

Theorem 3 (Theorem 3 in Barron [1993]). Every function $f \in \Gamma_{C, B}$ with $f(0)=0$ and can be approximated by a linear combination of sigmoids $f_{i} \in G_{\sigma, \tau}, i=1, \ldots m$. If $\tau \geq m^{1 / 2} \ln m$ the error scales as

$$
\left|f(\mathbf{x})-\sum_{i=1}^{m} f_{i}(\mathbf{x})\right| \leq O\left(\frac{1}{m^{1 / 2}}\right), \mathbf{x} \in B
$$

To encode $N$ different functions, we use the index $j \in[N]$ and write $c_{j i}, \mathbf{a}_{j i}$ for the coefficients of the sigmoids that approximate them or

$$
f_{j}(\mathbf{x})=\sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) \text { for } j=1, \ldots, N
$$

We here note that the terms $\tau, b$ can be incorporated in the term $\mathbf{a}_{i j}$ by adding an extra coefficient of 1 in $\mathrm{x}$ and multiplying everything with $\tau$.

We are now able to present the lemma on approximating functions using transformer blocks, in a format that is consistent with the FLEQ design outlined in the previous section.

Lemma 5. Fix $\epsilon>0$ and consider an input of the form

$$
\mathbf{X}=\left[\begin{array}{cc|cc|cc}
\boldsymbol{e} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right]
$$

where $d$ is chosen according to the $F L E Q$ construction from the previous section and $N$ is the number of functions we encode $. \boldsymbol{e}=\mathbf{e}_{j} \in \mathbb{R}^{N}$ is an indicator vector signifying the function we wish to execute. Then there exists a transformer-based function block with 3 layers, $m$ heads and dimensionality $r=2 \log (d)+d+1=O(d)$ such that

$$
f(\mathbf{X})=\left[\begin{array}{cc|cc|cc}
* & * & * & * & \sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right)+\epsilon & * \\
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right]
$$

where $*$ denoted inconsequential values that will be ignored downstream. This implies that arbitrary function $g \in \Gamma_{C, B}$ can be well approximated by attention layers.

Remark 2. Notice that in this case we don't use any extra scratchpad space and thus $s=3 d$; however if this function block was to be used with another one that needs $s>3 d$ scratchpad space, we would simply zero pad the input of Lemma 5 and ignore these columns. The same holds for the rest of the transformer-based function blocks and we will not mention it from now on.

In the expression $\sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right)$, the number head is equal to the number of terms we need. We show in the appendix that we can actually encode these $m$ terms in the dimension of the transformer architecture with just one head (See Corollary 6). The choice of which result to use can depend on the specific design and can affect both accuracy and efficiency of the implemented transformer network.

The proof of this Lemma is given in Appendix A.2.

\subsection*{6.2 Matrix Transposition and Multiplication by Linearizing the Softmax}

We assume that a $d \times d$ matrix $\mathbf{A}$ in the input $\mathbf{X}$ is represented by a sequence of length $d$, and each of these $d$ columns has $d$ rows. While this representation has the advantage that it is well suited for the matrix multiplication operation (as we will see in the next sub-section), a vectorized form of the matrix is more suited to create transpose. This is how we implement the transpose; we first vectorize the matrix $\mathbf{A}$, then with a fixed permutation of the columns we create its vectorized version of a transpose.

Lemma 6. Fix $\epsilon>0$ and consider an input of the following form

$$
\mathbf{X}=\left[\begin{array}{c|c|c|cc}
\mathbf{A} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \mathbf{P}_{2}^{\prime} & \mathbf{P}_{3}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$
where $\mathbf{A} \in \mathbb{R}^{d \times d}$; then there exists transformer-based function block with 4 layers, 1 head and dimensionality $r=2 d+2 \log d=O(d)$ that outputs the following matrix

$$
\mathbf{X}=\left[\begin{array}{c|c|c|cc}
\mathbf{A}^{\prime} & \mathbf{A}^{\prime} & \mathbf{A}^{\prime} & \ldots & \mathbf{A}^{\prime} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \mathbf{P}_{2}^{\prime} & \mathbf{P}_{3}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$

where $\mathbf{A}^{\prime}=\mathbf{A}^{\top}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$. The error $\epsilon$ depends on the choice of the temperature $\lambda$, as it is a consequence of the read/write operations.

In order for matrix multiplication to fit in our unified template, we need to show for example for the result of $\mathbf{A}^{\top} \mathbf{B}$, where $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$ with $k, m, n<d$ we can achieve the following:

$$
\left[\begin{array}{cc|cc|cc}
\mathbf{A} & \mathbf{0} & \mathbf{B} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right] \rightarrow\left[\begin{array}{cc|cc|cc}
* & * & * & * & \mathbf{A}^{\top} \mathbf{B} & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$

The idea we leverage is the linearization of the softmax, i.e., for a column vector $\boldsymbol{z}=[\boldsymbol{x}, C]$ for some large constant $C$ we have that

$$
\sigma_{\mathrm{S}}(\boldsymbol{z})=[\boldsymbol{x}+\epsilon, *]
$$

The error $\epsilon$ is controlled by the constant $C$.

Lemma 7. Let $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$; then for any $\epsilon>0$ there exists a transformer-based function block with 2 layers, 1 head and dimensionality $r=O(d)$ that outputs the multiplication $\mathbf{A}^{\top} \mathbf{B}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$.

The implementation of $\mathbf{B}^{\top} \mathbf{A}, \mathbf{A}^{\top} \mathbf{A}$ and $\mathbf{B}^{\top} \mathbf{B}$ are simple corollaries of the lemma presented above and we will freely use them in the subsequent sections. In Appendix A.2, we provide the exact form of the input $\mathbf{X}$ for implementing matrix transposition/multiplication, as well as the proof of the corresponding Lemmas.

\subsection*{6.3 Advantage of attention over fully-connected networks}

It is possible to implement the functions and overall lexicographic functionality presented in previous sections using fully connected networks, as they are also universal function approximators. However, it is easy to demonstrate a depth separation between attention-based networks and fully connected networks. For example, to compute simple functions like polynomials of $x\left(e . g ., x^{2}\right)$, a ReLU network with a depth proportional to $\log (1 / \epsilon)$ is required, where $\epsilon$ is the quality of approximation, e.g., as showed in [Perekrestenko et al., 2018]. In contrast, we have shown how $x^{2}$ can be implemented in essentially 2 layers. This simple depth separation argument highlights the constant vs scaling depth required for several functionalities in fully connected networks versus attention-based networks. It is important to note that although these constructions are easy to demonstrate their existence, constructing them is not straightforward. In this work, we provide hardcoded attention layers that precisely do that, making it easier to implement these functionalities in practice.

\section*{7 A Basic Calculator}

We show that the FLEQ transformer introduced in Section 5.2, can be used to build a simple calculator. This transformer consists of six transformer-based function blocks that implement addition, substraction, multiplication, percentage, division and square root. The formal statement is written as below.

Theorem 4. There exists a transformer with 12 layers, $m$ heads and dimensionality $O(\log n)$ that uses the Unified Attention Based Computer framework in Section 5.2 to implement a calculator which can perform addition, subtraction, multiplication, and computing the inverse, square root and percentage. For computing the inverse and square root, the operand needs to be in the range $\left[-e^{O(m)},-\tilde{\Omega}\left(\frac{1}{\sqrt{m}}\right)\right] \cup\left[\tilde{\Omega}\left(\frac{1}{\sqrt{m}}\right), e^{O(m)}\right]$ and $\left[0, O\left(m^{2}\right)\right]$ respectively, and the returned output is correct up to an error of $O(1 / \sqrt{m})$ and $O(1 / m)$ respectively. Here, $n$ is the number of operations to be performed.

Remark 3. In the proof of this theorem, we use Lemma 5 to approximate the square root and the inversion function. That lemma provides error guarantees in terms of the number of heads m. We prove Corollary 6 in the appendix which provides equivalent error guarantees, but where the error decreases with the dimension $d$ of the transformer. Depending on the design choices of the transformer, either of the results can be used, and the calculator's error guarantee will also change accordingly.

We show how one can implement a calculator in our FLEQ framework in Algorithm 3.

```

Algorithm 3 A sample program for executing a basic calculator functionality. The following
algorithm performs $\frac{\sqrt{1 /(((a+b)-c) \cdot d)}}{100}$
Require: $\operatorname{mem}[p]=a, \operatorname{mem}[q]=b, \operatorname{mem}[r]=c, \operatorname{mem}[s]=d$.
$\triangle$ The location of the inputs.
$\operatorname{mem}[t]=f_{\text {add }}(\operatorname{mem}[p], \operatorname{mem}[q])$
$\operatorname{mem}[t]=f_{\text {sub }}(\operatorname{mem}[t], \operatorname{mem}[r])$
$\operatorname{mem}[t]=f_{\operatorname{mul}}(\operatorname{mem}[t], \operatorname{mem}[s])$
$\operatorname{mem}[t]=f_{\text {inv }}(\operatorname{mem}[t])$
$\operatorname{mem}[t]=f_{\text {sqrt }}(\operatorname{mem}[t])$
$\triangleright \operatorname{mem}[t]=a+b$.
$\operatorname{mem}[t]=f_{\text {perc }}(\operatorname{mem}[t])$
$\triangleright \operatorname{mem}[t]=(a+b)-c$.
$\triangleright \operatorname{mem}[t]=((a+b)-c) * d$.
$\triangleright \operatorname{mem}[t]=1 /((a+b)-c) * d$.
$\triangleright \operatorname{mem}[t]=\sqrt{1 /((a+b)-c) * d}$.
$\triangleright \operatorname{mem}[t]=\frac{\sqrt{1 /((a+b)-c) * d}}{100}$.

```

Looking at the algorithm, it is clear that for proving the theorem above, it is sufficient to implement the 6 functions (addition, subtraction, multiplication, inversion, square root and percentage) using the transformer-based function blocks defined in Definition 1. We start with two lemmas, which can be proved by constructing transformers that add and subtract in a similar way to the OISC transformer constructed in Section 5.1.

Lemma 8 (addition). There exists a transformer-based function block with 3 layers, 1 head and dimensionality $O(1)$ which can implement $f(a, b)=a+b$.

Proof. Consider the input in the form of Eq. (10)

$$
\mathbf{X}=\left[\begin{array}{cc|cc|cc}
a & \mathbf{0} & b & \mathbf{0} & 0 & \mathbf{0}  \tag{11}\\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{p}_{2: d} & \mathbf{p}_{d+1} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
1 & \mathbf{0} & 0 & \mathbf{0} & 0 & \mathbf{0}
\end{array}\right]
$$

We can perform the following transformation

$$
\begin{align*}
& {\left[\begin{array}{ll|ll|ll}
a & \mathbf{0} & b & \mathbf{0} & 0 & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right] \rightarrow\left[\begin{array}{ll|ll|ll}
a & \mathbf{0} & b & \mathbf{0} & 0 & \mathbf{0} \\
a & \mathbf{0} & b & \mathbf{0} & 0 & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]}  \tag{12}\\
& \rightarrow\left[\begin{array}{ll|ll|ll}
a & \mathbf{0} & 0 & \mathbf{0} & 0 & \mathbf{0} \\
0 & \mathbf{0} & b & \mathbf{0} & 0 & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]  \tag{13}\\
& \rightarrow\left[\begin{array}{cc|cc|cc}
a+b & \mathbf{0} & 0 & \mathbf{0} & 0 & \mathbf{0} \\
0 & \mathbf{0} & 0 & \mathbf{0} & 0 & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]  \tag{14}\\
& \rightarrow\left[\begin{array}{cc|cc|cc}
a+b & \mathbf{0} & 0 & \mathbf{0} & a+b & \mathbf{0} \\
0 & \mathbf{0} & b & \mathbf{0} & 0 & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right] \tag{15}
\end{align*}
$$

The first and second step are implemented with one feed-forward layer each. The third step with the Section 4.2. We have ignored the last three rows since we don't change them and we only use them for the last step.

Lemma 9 (subtraction). There exists a transformer-based function block with 3 layers, 1 head and dimensionality $O(1)$ which can implement $f(a, b)=a-b$.

This lemma can be proved in the exact same way as the previous one. In addition, we can use the theory presented in Lemma 7 to get the following corollaries:

Corollary 2 (multiplication). There exists a transformer-based function block with 2 layers, 1 head and dimensionality $O(d)$ which can implement $f(a, b)=a b$.

Corollary 3 (percentage). There exists a transformer-based function block with 2 layers, 1 head and dimensionality $O(1)$ which can implement $f(a)=a / 100=a * 0.01$.

To implement inversion function, we introduce the following lemma.

Lemma 10. Given $\epsilon, \delta \in[0,1]$, and $C \geq 1$ there exists a function $f$ of the form $f(x)=$ $\sum_{i=1}^{m} c_{i} \sigma\left(w_{i} x+b_{i}\right)$, where $\sigma$ is the sigmoid function, such that

$$
\forall x \in[\delta, C],\left|f(x)-\frac{1}{x}\right| \leq \epsilon
$$

as long as $d=\Omega\left(\frac{\log (1 /(\epsilon \delta))}{\epsilon \delta}+\log C\right)$.

We can use this lemma along with the result presented in Lemma 5 to get the following corollary:

Corollary 4 (inversion). There exists a transformer-based function block with 3 layers and $m$ heads which can implement $f(a)=\frac{1}{a}$ up to error $\tilde{O}\left(\frac{1}{\sqrt{m}}\right)$ for all $a \in\left[\tilde{\Omega}\left(\frac{1}{\sqrt{m}}\right), \tilde{O}\left(e^{m}\right)\right]$.

Note that using Corollary 2 (multiplication) and Corollary 4 (inversion), the operation of division can be implemented as well. Next, we move on to showing the way of implementing square root.

Lemma 11. Given $\epsilon \in[0,1]$, and $C \geq 1$ there exists a function $f$ of the form $f(x)=\sum_{i=1}^{m} c_{i} \sigma\left(w_{i} x+\right.$ $b_{i}$ ), where $\sigma$ is the sigmoid function such that

$$
\forall x \in[0, C],|f(x)-\sqrt{x}| \leq \epsilon
$$

as long as $m=\Omega\left(\frac{\sqrt{C}}{\epsilon}\right)$.

We can use this lemma along with the result presented in Lemma 5 to get the following corollary:

Corollary 5 (sqrt). There exists a transformer-based function block with 3 layers and $m$ heads which can implement $f(a)=\sqrt{a}$ up to error $O(1 / m)$ for all $a \in\left[0, O\left(m^{2}\right)\right]$.

The functions $f: x \rightarrow \frac{1}{x}$ (inversion) and $f: x \rightarrow \sqrt{x}$ (square root) since they can be approximated by sums of sigmoids, they can directly be encoded in the standard transformer-based function block form through Lemma 5.

What other functions can our calculator implement? We have included some of the most commonly used operations in calculators in our construction, but it can be extended to include a wider variety of operations such as algebraic and trigonometric functions. When implementing these functions within our transformer architecture, there are typically two choices that can be made. One option is to approximate the target function $f(x)$ using sigmoids. Another option is to use an iterative numerical algorithm where the next output $y$ is calculated based on the previous output $y$ and the goal is to minimize the difference between the calculated output and the target function $f(x)$. This algorithm takes the form $y_{k+1}=g\left(y_{k}\right)$, where $g$ is typically an algebraic function. The desired accuracy is achieved when the difference between the calculated output and target function is less than or equal to a certain tolerance $\epsilon$.

\section*{8 Linear Algebra}

In Section 6, we demonstrated the implementation of matrix transpose and matrix multiplication as transformer-based function blocks. Utilizing these implementations, we proceed to execute two iterative algorithms for determining the inverse of a matrix through the Newton-Raphson Method and identifying the eigenvector corresponding to the maximum eigenvalue through the Power Iteration method.

Linear algebra using Transformers In the study conducted by Charton [2021], the author implemented some standard matrix method operations using a transformer-based architecture. Four distinct encoding schemes were proposed and applied to nine different operations, ranging from matrix multiplication to eigenvalue decomposition. We find that the size of the networks in Charton [2021] is comparable to that of ours.

As an example we compare the required network size of ours and Charton [2021], for the task of transposing a matrix of size $30 \times 30$ : our construction uses a transformer with 1 layer, 1 head and width of 168 , while the transformer in Charton [2021] has 1 layer, 8 heads and width of 256 . Notice that the number of layers, heads and width reported above may seem different with Lemma 6; however, in the proof of Lemma 6 we first vectorize the matrix (1 layer), then we implement the fixed permutation using Lemma 3 (1 layer) and finally we use another 2 layers to bring back the matrix in its original representation. If the matrix is given to us, as in Charton [2021], in its transposed form then we only need one layer and the two sets of encodings to perform the fixed permutation. Since the maximum size of the matrix is $30 \times 30$, the sequence length is $n=30^{2}$ and thus the size of each of the encodings will be 10 , leading to an input with width $2 \cdot 10+1=21$. This will lead to a total width of 168 , due to the ReLU layer in Lemma 16, for adding two binary vectors, having a width eight times the input's width.

We intend to further investigate our constructions, by implementing them and evaluating the errors involved as a function of the constants used in the proof of Lemma 7 and the temperature in Lemma 2, in future work.

Matrix Inversion. We can use the Unified Attention Based Computer to write a program for Matrix Inversion using the functions for matrix multiplications and a function for subtraction. We do so by implementing Newton's algorithm for matrix inversion using our unified framework. The pseudo code for the algorithm is as follows:

```

Algorithm 4 Pseudocode for running Newton's algorithm for Matrix inversion for $T$ iterations.
$\mathbf{X}_{-T}=\epsilon \mathbf{A}$
for $i=-T, \ldots, 0$ do
$\mathbf{X}_{i+1}=\mathbf{X}_{i}\left(2 \mathbf{I}-\mathbf{A} \mathbf{X}_{i}\right)$
end for

```

Lemma 12. Consider a matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$, then for any $\epsilon>0$ there exists a transformer with 13 layers, 1 head and dimensionality $r=O(d)$ that emulates Algorithm 4 with output $\mathbf{X}_{1}^{\text {(transf })}$ that satisfies $\left\|\mathbf{X}_{1}^{\text {(transf }}-\mathbf{X}_{1}\right\| \leq \epsilon$.

Proof. The proof of this lemma is the code using the FLEQ instruction provided below ( Algorithm 5). Let $f_{\text {mul }}, f_{\text {sub }}$ and $f_{\text {transp }}$ be the functions that implement multiplication, substraction and transpose respectively. Then, the following code runs Newton's algorithm for matrix inversion.

```

Algorithm 5 Program to compute the approximate inverse using our Unified Attention Based
Computer
Require: $\operatorname{mem}[a]=\mathbf{A}$. $\quad \triangleright$ This is the location of the input.
Require: $\operatorname{mem}[p]=2 \mathbf{I}$, mem $[x]=\epsilon \mathbf{I}$, $\operatorname{mem}[y]=\mathbf{0}$, mem $[q]=-1 . \quad \triangleright$ Constants.
Require: $\operatorname{mem}[t]=-T$. $\quad \triangleright$ Iteration counter, $i$ initialized as $i:=-T$.
$\operatorname{mem}[x]=f_{\text {mul }}(\operatorname{mem}[x], \operatorname{mem}[a]) . \quad \triangleright$ Initializes the result, $\mathbf{X}_{-T}:=\epsilon \mathbf{A}$.
$\operatorname{mem}[a]=f_{\text {transp }}(\operatorname{mem}[a], \operatorname{mem}[y]) \quad \triangleright$ Transpose $\mathbf{A}$.
$\operatorname{mem}[y]=f_{\mathrm{mul}}(\operatorname{mem}[a], \operatorname{mem}[x]) . \quad \triangleright$ First sub-step of Newton's algorithm, $\mathbf{Y}:=\mathbf{A} \mathbf{X}_{i}$
$\operatorname{mem}[y]=f_{\text {sub }}(\operatorname{mem}[p], \operatorname{mem}[y]) . \quad \triangleright$ Second sub-step of Newton's algorithm, $\mathbf{Y}:=2 \mathbf{I}-\mathbf{Y}$
$\operatorname{mem}[y]=f_{\text {transp }}(\operatorname{mem}[y], \operatorname{mem}[q]) . \quad \triangleright$ Transpose of $\mathbf{Y}$.
$\operatorname{mem}[x]=f_{\text {mul }}(\operatorname{mem}[x], \operatorname{mem}[y]) . \quad \triangleright$ Updating the result, $\mathbf{X}_{i+1}:=\mathbf{X}_{i} \mathbf{Y}$
$\operatorname{mem}[t]=f_{\text {sub }}(\operatorname{mem}[t]$, mem $[q]) . \quad \triangleright$ Increment counter, $i:=i+1$.
if mem $[t] \leq 0$ goto instruction $3 . \quad \triangleright$ Keep looping back as long as $i \leq 0$.
EOF.
$\triangleright$ End of File command.

```

Power Iteration. The Power Iteration algorithm (Algorithm 6) is used for finding the dominant eigenvalue, the one that has the maximum absolute value, and corresponding eigenvector of a diagonalizable matrix. The algorithm starts with an initial approximation of the eigenvector and converges linearly to the eigenvector associated with the dominant eigenvalue; below we provide the pseudocode.

```

Algorithm 6 Power Iteration
Input: $\mathbf{A}, T$
Initialize $b_{0}=1$
for $k=0, \ldots, T-1$ do
$\mathbf{b}_{k+1}=\mathbf{A b}_{k}$
end for
$\mathbf{b}=\frac{\mathbf{b}_{T}}{\left\|\mathbf{b}_{T}\right\|}$

```

The last step in the algorithm above needs a normalization by the norm of $\mathbf{b}_{T}$. While we can compute $\left\|\mathbf{b}_{T}\right\|^{2}$ easily and precisely using the matrix multiplication function block (since $\left\|\mathbf{b}_{T}\right\|^{2}=\mathbf{b}_{T}^{\top} \mathbf{b}_{T}$ ), computing the norm and taking its inverse using the function block from Section 7 would induce error. Hence, we use the following Newton's algorithm that converges quadratically.

```

Algorithm 7 Newton's algorithm to compute inverse square root: $1 / \sqrt{S}$
Input: $S$
Initialize $x_{0}=1$
for $k=0, \ldots, T$ do
$x_{k+1}=x_{k}\left(\frac{3}{2}-\frac{S}{2} x_{k}^{2}\right)$
end for

```

Lemma 13. Consider a matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$, then for any $\epsilon>0$ there exists a transformer with 13 layers, 1 head and dimensionality $r=O(d)$ that emulates Algorithm 6 for $T=O(\log 1 / \epsilon)$ iterations with output $\mathbf{b}_{T+1}^{(\text {transf }}$ that satisfies $\left\|\mathbf{b}_{T+1}^{\text {(transf })}-\mathbf{b}_{T+1}\right\| \leq \epsilon$.

Proof. The proof consists of translating each step of the pseudocode for Algorithm 6 and Algorithm 7 to commands of our unified framework.

```

Algorithm 8 Program to simulate Power Iteration using our Unified Attention Based Computer
Require: $\operatorname{mem}[a]=\mathbf{A}$, mem $[b]=\mathbf{1}$, mem $[$ inv_norm $]=1 . \quad \triangleright$ Location of matrix and initialization.
Require: $\operatorname{mem}[q]=1, \operatorname{mem}[p]=0, \operatorname{mem}[r]=0.5, \operatorname{mem}[s]=1.5 \quad \triangleright$ Constants.
Require: $\operatorname{mem}\left[t_{1}\right]=\operatorname{mem}\left[t_{2}\right]=-T+1$,
mem $[a]=f_{\text {transp }}(\operatorname{mem}[a], \operatorname{mem}[p]) . \quad \triangleright$ Transpose of $\mathbf{A}$.
$\operatorname{mem}[b]=f_{\text {mul }}(\operatorname{mem}[a]$, mem $[b]) . \quad \triangleright$ Inner product: $\mathbf{A b}_{k}$.
$\operatorname{mem}[t]=f_{\text {add }}\left(\operatorname{mem}\left[t_{1}\right], \operatorname{mem}[q]\right) . \quad \triangleright$ Increment counter, $i:=i+1$.
if mem $\left[t_{1}\right] \leq 0$ goto instruction 2 . $\quad \triangleright$ Keep looping back as long as $i \leq 0$.
mem $[$ norm_square $]=f_{\text {mul }}(\operatorname{mem}[b]$, mem $[b])$. $\quad \triangleright$ Calculate $\left\|\mathbf{b}_{T}\right\|^{2}$.
Code for Algorithm 7 begins.
mem $[\mathrm{y}]=f_{\text {mul }}($ mem[inv_norm], mem[inv_norm] $) . \quad \triangleright$ Calculate $x_{k}^{2}$.
mem $[\mathrm{y}]=f_{\text {mul }}($ mem[norm_square], mem[y] $) . \quad \triangleright$ Calculate $S x_{k}^{2}$.
$\operatorname{mem}[\mathrm{y}]=f_{\text {mul }}(\operatorname{mem}[r]$, mem $[\mathrm{y}]) . \quad \triangleright$ Calculate $S x_{k}^{2} / 2$.
$\operatorname{mem}[\mathrm{y}]=f_{\text {sub }}(\operatorname{mem}[s], \operatorname{mem}[\mathrm{y}]) . \quad \triangleright$ Calculate $\left(3-S x_{k}^{2}\right) / 2$.
mem[inv_norm] $=f_{\text {mul }}\left(\right.$ mem[inv_norm], mem[y]). $\quad \triangleright$ Update $x_{k+1}:=x_{k}\left(3-S x_{k}^{2}\right) / 2$.
$\operatorname{mem}\left[t_{2}\right]=f_{\text {add }}\left(\operatorname{mem}\left[t_{2}\right], \operatorname{mem}[q]\right) . \quad \triangleright$ Increment counter, $j:=j+1$.
if mem $\left[t_{2}\right] \leq 0$ goto instruction $6 . \quad \triangleright$ Keep looping back as long as $j \leq 0$.
Code for Algorithm 7 ends.
13: mem $[b]=f_{\text {mul }}($ mem $[b]$, mem[inv_norm $\left.]\right) . \quad \triangleright \mathbf{b}:=\mathbf{b}_{T} /\left\|\mathbf{b}_{T}\right\|$.
14: EOF. $\triangleright$ End of File command.

```

What other numerical linear algebra algorithms can transformers implement? The algorithms presented above serve as proof of concept for the potential to build small linear algebra libraries using our transformer construction. As demonstrated, the size of the looped transformer is constant regardless of the depth. To implement iterative numerical algorithms, additional functions can be incorporated into our architecture. For instance, QR decomposition, Gauss-Seidel, Arnoldi iteration, or Lanczos algorithm can be implemented. While we have not included detailed code for these specific algorithms, the above examples should provide sufficient insight on how to do so.

\section*{9 Emulating Learning Algorithms at Inference Time}

In this section we demonstrate the ability of our unified template to emulate Stochastic Gradient Descent (SGD). We begin by examining the case of linear models, before extending our results to the implementation of the backpropagation algorithm for two layer neural networks. Utilizing this as a "function" which we call at each step, we demonstrate the application of SGD in updating the implicit weights of a model.

Our work demonstrates that looped transformers can effectively perform in-context learning for a wide range of models and achieve high levels of accuracy, given access to a sufficient number of inference calls/loops. Previous research, such as Akyürek et al. [2022] and Garg et al. [2022], has limited in-context learning to a single inference call of a transformer model deeper than ours, which restricts the types of models that can be learned and the level of accuracy that can be achieved. To implement complex iterative programs like SGD, either a looped structure transformer or one that grows in size with the program's depth is required, unless widely believed complexity conjectures are falsified. Additionally, this is the first work to show that transformers can implement SGD on more general loss functions and models beyond linear regression.

Stochastic Gradient Descent in linear models. In Algorithm 9 we provide the program for running SGD in linear models; that is we perform updates of the form: $\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta \sum_{i=1}^{\mathcal{D}}\left(\mathbf{w}^{\top} \mathbf{x}_{i}-\right.$ $\left.y_{i}\right) \mathbf{x}_{i}$, where $\mathbf{w}$ is the weight vector, $\left(\mathbf{x}_{i}, y_{i}\right)$ is the feature-label pair of the $i$-th data point, and $\eta$ is the step-size. The program iterates through the $\mathcal{D}$ data points that the user gives and cycles back to the first point after one pass is completed. The step-size is given as input by the user.

Lemma 14. Let $\epsilon>0$, there exists a transformer with 13 layers, 1 head and dimensionality $O(\log (\mathcal{D})+d)$ that uses the Unified Attention Based Computer framework in Section 5.2 to implement $T$ iterations of $S G D$ on a weight vector $\mathbf{w} \in \mathbb{R}^{d}$, over a set of $\mathcal{D}$ data points $\left(\mathbf{x}_{i}, y_{i}\right) \in$ $\mathbb{R}^{d+1}, i=1, \ldots, \mathcal{D}$ with error up to $\epsilon$. The step size is given as a parameter to the program.

Remark 4. The error is controlled by two parameters: the temperature $\lambda$ and the constants used in the proof of Lemma 7. Implementing arbitrary loss functions $f$ and thus updates of the form $\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta \sum_{i=1}^{\mathcal{D}} f^{\prime}\left(\mathbf{w}^{\top} \mathbf{x}_{i}-y_{i}\right) \mathbf{x}_{i}$ would introduce an extra error as a result of Barron's theorem (Theorem 3) applied in Lemma 5. Specifically, we would need in general poly(TD) heads, in order to ensure control over this approximation error. However, if the derivative $f^{\prime}(x)$ of the loss function $f(x)$ is a sum of sigmoids, the number of heads will be equal to the number of sigmoids required, and there will be no error associated with this aspect of the construction.

```

Algorithm 9 Program to simulate SGD using our Unified Attention Based Computer
Require: $\operatorname{mem}[w]=\mathbf{w}, \operatorname{mem}[\eta]=\eta$
$>$ Location of the weight and step-size.
Require: $\operatorname{mem}\left[x_{0}+i-1\right]=\mathbf{x}_{i}, i=1, \ldots, \mathcal{D}$. $\quad \triangleright$ Location of the data points.
Require: $\operatorname{mem}\left[y_{0}+i-1\right]=y_{i}, i=1, \ldots, \mathcal{D}$.
$\triangle$ Location of the labels.
Require: $\mathbf{p}_{x_{*}}=x_{0}$
$\triangleright \mathbf{p}_{x_{*}}$ is a pointer to the first data.
Require: $\mathbf{p}_{y_{*}}=y_{0}$.
$\triangleright \mathbf{p}_{y_{*}}$ is a pointer to the first label.
Require: $\mathbf{p}_{\mathrm{PC}}=$ instr $_{1}$. $\quad \triangleright$ Program Counter points to first instruction.
Require: $\operatorname{mem}[q]=1, \operatorname{mem}[p]=0, \operatorname{mem}[z]=n$.
$\triangleright$ Constants
Require: $\operatorname{mem}[j]=-\mathcal{D}$.
$\triangle$ Within epoch iteration counter initialized to $-n$.
Require: $\operatorname{mem}[k]=-T$.
$\triangleright$ Epoch counter initialized to $-T$.
$\left(\operatorname{instr}_{1}\right) \quad \operatorname{mem}[t e m p]=f_{\operatorname{mul}}\left(\operatorname{mem}\left[\mathbf{p}_{x_{*}}\right], \operatorname{mem}[w]\right)$
$\triangleright$ Inner product: $\mathbf{w}^{\top} \mathbf{x}_{i}$.
$\left(\right.$ instr $\left._{2}\right) \quad \operatorname{mem}[t e m p]=f_{\text {sub }}\left(\operatorname{mem}[t e m p], \operatorname{mem}\left[\mathbf{p}_{y_{*}}\right]\right)$.
$\left(\operatorname{instr}_{3}\right) \quad \operatorname{mem}[t e m p]=f_{\operatorname{mul}}\left(\operatorname{mem}\left[\mathbf{p}_{x_{*}}\right]\right.$, mem $[$ temp $\left.]\right)$.
$\operatorname{mem}[$ temp $]=f_{\text {mul }}(\operatorname{mem}[$ temp $], \operatorname{mem}[\eta])$.
$\operatorname{mem}[w]=f_{\text {sub }}(\operatorname{mem}[w], \operatorname{mem}[t e m p])$.
$\operatorname{mem}\left[\right.$ instr $\left._{1}\right]=f_{\text {incr_pointer }}\left(\right.$ mem $\left[\right.$ instr $\left.\left._{1}\right]\right)$.
$\operatorname{mem}\left[\right.$ instr $\left._{2}\right]=f_{\text {incr_pointer }}\left(\right.$ mem $\left[\right.$ instr $\left.\left._{2}\right]\right)$.
$\operatorname{mem}\left[\right.$ instr $\left._{3}\right]=f_{\text {incr pointer }}\left(\operatorname{mem}\left[\right.\right.$ instr $\left.\left._{3}\right]\right)$.
$\operatorname{mem}[j]=f_{\text {add }}(\operatorname{mem}[j], \operatorname{mem}[q])$.
if $\operatorname{mem}[j] \leq 0$ goto 1 .
$\operatorname{mem}[j]=-\mathcal{D}$.
$\operatorname{mem}\left[\right.$ instr $\left._{1}\right]=f_{\text {reset_pointer }}\left(\right.$ mem $\left[\right.$ instr $\left.\left._{1}\right], x_{0}\right)$.

```
![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-34.jpg?height=41&width=650&top_left_y=1324&top_left_x=320)
```

    $\operatorname{mem}\left[\right.$ instr $\left._{3}\right]=f_{\text {reset_pointer }}\left(\mathrm{mem}\left[\mathrm{instr}_{3}\right], x_{0}\right)$.
    $\operatorname{mem}[k]=f_{\text {add }}(\operatorname{mem}[k], \operatorname{mem}[q])$.
    if mem $[k]<0$ goto $1 . \quad>$ Cycle back to the first data point.
    EOF. $\triangleright$ End of File command.
    ```

The following will detail the essential procedures for implementing the Stochastic Gradient Descent algorithm. We employ three pointers, namely $\mathbf{p}_{\mathrm{PC}}, \mathbf{p}_{x_{*}}$ and $\mathbf{p}_{y_{*}}$ in our algorithm. The first one, referred to as program counter, is used to iterate through the commands; after one pass over all data points is completed, the program counter is reset to the first instruction (line 16), until $T$ full passes have been completed. The second and third ones, referred to as data and label pointer respectively, iterate through the features and labels one by one. The increment of the pointer $\mathbf{p}_{x_{*}}$ needs to occur in both instructions 1 and 3 , as to in the next iteration they have been updated from $\operatorname{instr}_{i}\left(\mathbf{p}_{x_{*}}, w\right.$, temp $) \rightarrow \operatorname{instr}_{i}\left(\mathbf{p}_{x_{*}}+1, w\right.$, temp $), i=1,3$. The same holds for the pointer $\mathbf{p}_{y_{*}}$ in line 7. Finally, we reset the two pointers in lines 13,14 to cycle back in the first feature and label.

To enhance understanding, we note that lines 6-8 modify the instructions themselves; instead of doing this we could have $\mathcal{D}$ copies of the lines $1-3$, each one with parameters pointers of a different (feature,label) pair. In that case the number of instructions would have been $7 \mathcal{D}$.

Notice that the functions $f_{\text {incr_pointer }}$ and $f_{\text {reset_pointer }}$ can be directly implemented using Lemma 16 .

Backpropagation and SGD. We will now generalize the result of Lemma 14 to two layer neural networks with non-linear activation functions; we demonstrate in Algorithm 12 how this can be achieved if the activation function is the sigmoid function.

Closest to this section is the work of Akyürek et al. [2022], where the authors prove that constant number of layers is needed to perform one step SGD in linear models, using decoder only transformer architecture.

```

Algorithm 10 Backpropagation
Loss function: $J(x)=\frac{1}{2} x^{2}$
Input: $\mathbf{W}_{1} \in \mathbb{R}^{m \times d}, \mathbf{b}_{1} \in \mathbb{R}^{m}, \mathbf{W}_{2} \in \mathbb{R}^{m \times 1}, \mathbf{b}_{2} \in \mathbb{R} \mathbf{x} \in \mathbb{R}^{d}, y \in \mathbb{R}$
: Compute $\boldsymbol{z}=\mathbf{W}_{1} \mathbf{x}+\mathbf{b}_{1}$.
Compute $\boldsymbol{a}=\sigma(\boldsymbol{z})$.
Compute $o=\mathbf{W}_{2} \boldsymbol{a}+\mathbf{b}_{2}$.
Compute $\delta_{2}=(o-y)$.
Compute $\delta_{1}=\sigma^{\prime}(\boldsymbol{z}) \odot \mathbf{W}_{2}(o-y)$.
Compute $\frac{\partial J}{\partial \mathrm{W}_{2}}=\delta_{2} \boldsymbol{a}^{\top}$.
Compute $\frac{\partial J}{\partial \mathbf{b}_{2}}=\delta_{2}$.
Compute $\frac{\partial J}{\partial \mathbf{W}_{1}}=\delta_{1} \mathbf{x}^{\top}$.
Compute $\frac{\partial J}{\partial \mathbf{b}_{1}}=\delta_{1}$.

```

Lemma 15. Let $\epsilon>0$, there exists a transformer with 13 layers, 1 head and dimensionality $O(\log (\mathcal{D})+d)$ that uses the Unified Attention Based Computer framework in Section 5.2 to implement $T$ iterations of $S G D$ on a two layer neural network, over a set of $\mathcal{D}$ data points $\left(\mathbf{x}_{i}, y_{i}\right) \in \mathbb{R}^{d+1}, i=1, \ldots, \mathcal{D}$ with error up to $\epsilon$. The step size is given as a parameter to the program.

Remark 5. The program we provide in Algorithm 11 is implemented as an independent function, which we call multiple times. Specifically, in line 1 of Algorithm 12 we call the algorithm for backpropagation at each iteration with a different data point. In terms of our construction, this translates to different instructions which will be in total $O(\mathcal{D})$, each one with parameters pointers to a different data point. However, as in Algorithm 9 the utilization of a pointer that changes the instructions themselves, would result in a program of constant length; we did not do this in order to contain the total length of the program.

Remark 6. If we want to account for different activation functions we can use Lemma 5 to express the activation function and its derivative as sums of sigmoids. The number of heads would need to be in that case poly $(T \mathcal{D})$ to ensure control over the error induced by the approximation.

```

Algorithm 11 Program to simulate Backpropagation for two layer Neural Networks
Input: $\mathbf{p}_{w_{1}}, \mathbf{p}_{w_{2}}, \mathbf{p}_{b_{1}}, \mathbf{p}_{b_{2}}$
$\triangleright$ Pointers to weights and biases.
Input: $\mathbf{p}_{x}, \mathbf{p}_{y}$
$\triangleright$ Pointer to data point and label.
Input: $\eta$.
$\triangleright$ Pointer to step size.
Require: $\operatorname{mem}[q]=1, \operatorname{mem}[p]=0, \operatorname{mem}[r]=-1, \operatorname{mem}[m]=m$.
$\triangle$ Constants.
Require: $\operatorname{mem}[k]=1$.
$\triangleright$ Iteration counter, $k:=1$.
Require: $\mathbf{p}_{z}=z_{\mathrm{T}}^{1}$.
$\triangleright$ Pointer for $z$.
Require: $\mathbf{p}_{\delta}=\delta_{1, \mathrm{~T}}^{1}$.
$\triangleright$ Pointer for $\delta_{1}$.
$:\left(\operatorname{instr}_{1}\right) \operatorname{mem}[$ temp $]=f_{\text {trans }}\left(\operatorname{mem}\left[\mathbf{p}_{w_{1}}\right], \operatorname{mem}[p]\right)$.
$\triangleright$ Create $\mathbf{W}_{1}^{\top}$.
$\operatorname{mem}[z]=f_{\operatorname{mul}}\left(\operatorname{mem}[t e m p], \operatorname{mem}\left[\mathbf{p}_{x}\right]\right)$.
$\operatorname{mem}[z]=f_{\text {add }}\left(\operatorname{mem}[z], \operatorname{mem}\left[\mathbf{p}_{b_{1}}\right]\right)$.
$\operatorname{mem}[a]=f_{\text {sigmoids }}(\operatorname{mem}[z], \operatorname{mem}[q])$.
$\operatorname{mem}[$ temp $]=f_{\text {trans }}\left(\operatorname{mem}\left[\mathbf{p}_{w_{2}}\right], \operatorname{mem}[p]\right)$.
$\operatorname{mem}[o]=f_{\operatorname{mul}}(\operatorname{mem}[$ temp $]$, mem $[a])$.
$\operatorname{mem}[o]=f_{\text {add }}\left(\operatorname{mem}[o], \operatorname{mem}\left[\mathbf{p}_{b_{2}}\right]\right)$.
$\operatorname{mem}\left[\delta_{2}\right]=f_{\text {sub }}\left(\operatorname{mem}[o], \operatorname{mem}\left[\mathbf{p}_{y}\right]\right)$
Multiply: $\mathbf{W}_{1} \mathbf{x}$.
$\operatorname{mem}\left[\delta_{1}\right]=f_{\operatorname{mul}}\left(\operatorname{mem}\left[\mathbf{p}_{w_{2}}\right], \operatorname{mem}\left[\delta_{2}\right]\right)$.
$\triangleright$ Add the bias: Compute $\boldsymbol{z}$.
$\triangleright$ Compute $\boldsymbol{a}=\sigma(\boldsymbol{z})$.
$\triangleright$ Create $\mathbf{W}_{2}^{\top}$.
$\triangleright$ Multiply: $\mathbf{W}_{2} \boldsymbol{a}$.
$\triangleright$ Add bias: Compute $o$.
$\triangleright$ Compute $\delta_{2}$.
$\triangleright$ Multiply $\mathbf{W}_{2} \delta_{2}$.
$\operatorname{mem}[$ flag $]=f_{\text {sub }}(\operatorname{mem}[k], \operatorname{mem}[m])$.
$\triangleright$ Create $k-m$.
$\operatorname{mem}\left[\mathbf{p}_{z}\right]=f_{\text {trans }}(\operatorname{mem}[z], \operatorname{mem}[p])$
$\triangleright$ Store $\boldsymbol{z}$ to consecutive memory cells.
$\operatorname{mem}\left[\mathbf{p}_{\delta}\right]=f_{\text {trans }}\left(\operatorname{mem}\left[\delta_{1}\right], \operatorname{mem}[p]\right)$.
$\triangleright$ Store $\delta_{1}$ to consecutive memory cells.
if mem $[$ flag $] \leq 0$ goto 20 . $\quad \triangleright$ If we iterated all the elements goto next command.
$\left(\operatorname{instr}_{14}\right) \operatorname{mem}\left[\right.$ temp $\left.p^{\prime}\right]=f_{\text {sigmoids }}\left(\operatorname{mem}[p], \operatorname{mem}\left[\mathbf{p}_{z}\right]\right)$.
$\triangleright$ Create $\sigma\left(z_{i}\right)$.
$\operatorname{mem}\left[\right.$ temp $\left.p^{\prime \prime}\right]=f_{\text {sub }}\left(\operatorname{mem}[q], \operatorname{mem}\left[t e m p^{\prime}\right]\right)$.
$\triangleright$ Create $1-\sigma\left(z_{i}\right)$.
$\operatorname{mem}\left[\right.$ temp $\left.p^{\prime}\right]=f_{\operatorname{mul}}\left(\operatorname{mem}\left[\right.\right.$ temp $\left.p^{\prime}\right], \operatorname{mem}\left[\right.$ temp $\left.\left.p^{\prime \prime}\right]\right)$.
$\left(\operatorname{instr}_{17}\right) \operatorname{mem}\left[\mathbf{p}_{\delta}\right]=f_{\text {mul }}\left(\operatorname{mem}\left[\operatorname{tem} p^{\prime}\right], \operatorname{mem}\left[\mathbf{p}_{\delta}\right]\right)$.
mem $\left[\right.$ instr $\left._{14}\right]=f_{\text {incr_pointer }}\left(\right.$ mem $\left[\right.$ instr $\left.\left._{14}\right]\right)$.
$\operatorname{mem}\left[\right.$ instr $\left._{17}\right]=f_{\text {incr_pointer }}\left(\right.$ mem $\left[\right.$ instr $\left.\left._{17}\right]\right)$.
$\triangleright$ Create $\sigma^{\prime}\left(z_{i}\right)=\sigma\left(z_{i}\right)\left(1-\sigma\left(z_{i}\right)\right)$.
20: $\operatorname{mem}[k]=f_{\text {add }}(\operatorname{mem}[k]$, mem $[q])$.
$\triangleright$ Create $\sigma^{\prime}\left(z_{i}\right)\left(\mathbf{W}_{2}\right)_{i}(o-y)$.
$\triangleright$ Point to next element of $z$.
$\triangleright$ Point to next element of $\delta_{1}$.
$\triangleright$ Increment counter, $k:=k+1$.
If mem $[p] \leq 0$ goto 13 .
$\triangleright$ Loop back.
mem $\left[\right.$ instr $\left._{1}\right]=f_{\text {reset_pointer }}\left(\operatorname{mem}\left[\operatorname{instr}_{14}\right], z_{\top}^{1}\right)$.
$\triangleright$ Reset pointer.
$\operatorname{mem}\left[\operatorname{instr}_{15}\right]=f_{\text {reset_pointer }}\left(\operatorname{mem}\left[\right.\right.$ instr $\left.\left._{15}\right], \delta_{1, \mathrm{~T}}^{1}\right)$.
$\triangleright$ Reset pointer.
mem $\left[\mathrm{grad} W_{2}\right]=f_{\text {mul }}\left(\operatorname{mem}\left[\delta_{2}\right]\right.$, mem $\left.[a]\right)$.
$\triangleright$ Create $\frac{\partial J}{\partial \mathrm{W}_{2}}$.
$\operatorname{mem}\left[g r a d \_b_{2}\right]=f_{\text {mul }}\left(\operatorname{mem}\left[\delta_{2}\right], \operatorname{mem}[q]\right)$.
$\triangleright$ Create $\frac{\partial J_{2}}{\partial \mathbf{b}_{2}}$.
mem $\left[g r a d \_W_{1}\right]=f_{\text {mul }}\left(\operatorname{mem}\left[\delta_{1}\right], \operatorname{mem}\left[\mathbf{p}_{x}\right]\right)$.
$\triangleright$ Create $\frac{\partial J}{\partial W_{1}}$.
$\operatorname{mem}\left[g r a d \_b_{1}\right]=f_{\text {mul }}\left(\operatorname{mem}\left[\delta_{1}\right], \operatorname{mem}[q]\right)$.
$\triangle$ Create $\frac{\partial J}{\partial \mathbf{b}_{1}}$.
$\operatorname{mem}[$ temp $]=f_{\mathrm{mul}}\left(\operatorname{mem}\left[\operatorname{grad}_{W_{2}}\right], \operatorname{mem}[\eta]\right)$.
$\triangleright$ Multiply with step-size.
$\operatorname{mem}\left[\mathbf{p}_{w_{2}}\right]=f_{\text {sub }}\left(\operatorname{mem}\left[\mathbf{p}_{w_{2}}\right], \operatorname{mem}[t e m p]\right)$.
$\triangleright$ Update $\mathbf{W}_{2}$.
$\operatorname{mem}[$ temp $]=f_{\operatorname{mul}}\left(\operatorname{mem}\left[g r a d_{W_{1}}\right], \operatorname{mem}[\eta]\right)$.
$\triangleright$ Multiply with step-size.
$\operatorname{mem}\left[\mathbf{p}_{w_{1}}\right]=f_{\text {sub }}\left(\operatorname{mem}\left[\mathbf{p}_{w_{1}}\right], \operatorname{mem}[\operatorname{temp}]\right)$
$\triangleright$ Update $\mathbf{W}_{1}$.
$\operatorname{mem}[$ temp $]=f_{\text {mul }}\left(\operatorname{mem}\left[\mathrm{grad}_{b_{2}}\right], \operatorname{mem}[\eta]\right)$.
$\triangleright$ Multiply with step-size.
$\operatorname{mem}\left[\mathbf{p}_{b_{2}}\right]=f_{\text {sub }}\left(\operatorname{mem}\left[\mathbf{p}_{b_{2}}\right], \operatorname{mem}[\right.$ temp $\left.]\right)$.
$\triangleright$ Update $\mathbf{b}_{2}$.
$\operatorname{mem}[$ temp $]=f_{\text {mul }}\left(\operatorname{mem}\left[\mathrm{grad}_{b_{1}}\right], \operatorname{mem}[\eta]\right)$.
$\triangleright$ Multiply with step-size.
$\operatorname{mem}\left[\mathbf{p}_{b_{1}}\right]=f_{\text {sub }}\left(\operatorname{mem}\left[\mathbf{p}_{b_{1}}\right], \operatorname{mem}[t e m p]\right)$.
$\triangle$ Update $\mathbf{b}_{1}$.

```

```

Algorithm 12 Program to simulate SGD using our Unified Attention Based Computer
Require: $\operatorname{mem}\left[w_{1}\right]=\mathbf{W}_{1}$, mem $\left[w_{2}\right]=\mathbf{W}_{2}$.
$\triangleright$ Location weights and biases.
Require: $\operatorname{mem}\left[b_{1}\right]=\mathbf{b}_{1}$, mem $\left[b_{2}\right]=\mathbf{b}_{2}$.
Require: $\operatorname{mem}\left[x_{0}+i-1\right]=\mathbf{x}_{i}, i=1, \ldots, \mathcal{D}$.
Require: $\operatorname{mem}\left[y_{0}+i-1\right]=y_{i}, i=1, \ldots, \mathcal{D}$.
Require: $\operatorname{mem}[z]=\boldsymbol{e}$.
Require: $\mathbf{p}_{x_{*}}=x_{0}$.
Require: $\mathbf{p}_{y_{*}}=y_{0}$.
Require: $\mathbf{p}_{\mathrm{PC}}=$ instr $_{1}$.
Require: $\operatorname{mem}[q]=1, \operatorname{mem}[p]=0, \operatorname{mem}[z]=n$.
Require: $\operatorname{mem}[j]=-\mathcal{D}$.
Require: $\operatorname{mem}[k]=-T$.
Backpropagation $\left(w_{1}, w_{2}, b_{1}, b_{2}, \mathbf{p}_{x_{*}}, \mathbf{p}_{y_{*}}\right)$
$\operatorname{mem}[j]=f_{\text {add }}(\operatorname{mem}[j], \operatorname{mem}[q])$.
$\mathbf{p}_{x_{*}}=f_{\text {incr_pointer }}\left(\mathbf{p}_{x_{*}}\right)$.
$\mathbf{p}_{y_{*}}=f_{\text {incr_pointer }}\left(\mathbf{p}_{y_{*}}\right)$
if mem $[j] \leq 0$ goto 1 .
$\operatorname{mem}[j]=-\mathcal{D}$.
$\mathbf{p}_{x_{*}}=f_{\text {reset_pointer }}\left(\mathbf{p}_{x_{*}}, x_{0}\right)$.
$\mathbf{p}_{y_{*}}=f_{\text {reset_pointer }}\left(\mathbf{p}_{y_{*}}, y_{0}\right)$.
$\operatorname{mem}\left[\right.$ instr $\left._{3}\right]=f_{\text {reset_pointer }}\left(\operatorname{mem}\left[\right.\right.$ instr $\left.\left._{3}\right], x_{0}\right)$.
$\operatorname{mem}[k]=f_{\text {add }}(\operatorname{mem}[k], \operatorname{mem}[q])$.
if mem $[k] \leq 0$ goto 1 .
$\triangleright$ Location of biases.
EOF.
$\triangleright$ Location of the data points.
$\triangleright$ Location of the labels.
$\triangleright$ Indicator for the choice of loss function
$\triangleright \mathbf{p}_{x_{*}}$ is a pointer to the first data.
$\triangleright \mathbf{p}_{y_{*}}$ is a pointer to the first label.
$\triangleright$ Program Counter points to first instruction.
$\triangle$ Constants.
$\triangleright$ Within epoch iteration counter initialized to $-n$.
$\triangleright$ Epoch counter initialized to $-T$.
$\triangleright$ Perform one step of SGD using Backpropagation
$\triangleright$ Increment within epoch iteration counter by 1 .
$\triangleright$ Show to next data point.
$\triangle$ Show to next label.
$\triangleright$ Cycle back until all data points are iterated.
$\triangleright$ Reset counter.
$\triangleright$ Reset pointer.
$\triangleright$ Reset pointer.
$\triangleright$ Reset pointer.
$\triangleright$ Increment epoch counter by 1 .
$\triangleright$ Cycle back to the first data point.
$\triangleright$ End of File command.

```

Generalizing to arbitrary depth. Our algorithm above is designed to emulate backpropagation on a neural network that contains only one hidden layer. However, it is important to note that this construction can be generalized to networks of arbitrary depth, with the caveat that the length of the code will scale with the number of layers in the network. This is because each line of code in our algorithm represents one cycle of the looped transformer, and the number of cycles required is directly proportional to the depth of the network. It's important to note that the number of cycles of the looped transformer will be equal to the depth of the network. So the cost of this algorithm is proportional to looping the transformer network as many times as the depth of the network. This means that as the network becomes deeper, the computational cost of training it using our algorithm will also increase.

\section*{10 Conclusion and Open Problems}

In this paper, we have shown that transformer networks can be used as universal computers by programming them with specific weights and placing them in a loop. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, such as lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. We construct a one-instruction set computer (OISC) and use it to map iterative algorithms to programs that can be executed by a transformer network. Our results include constant-depth transformers that
emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention.

Our study sheds light on the versatility of the attention mechanism and how even a single loop can enable the creation of models that can mimic complex iterative algorithms and execute general programs. Our findings also reveal the ability of transformer models to effectively perform intricate mathematical and algorithmic tasks. It is possible that advanced transformer models like GPT-3 use similar internal subroutines when given in-context examples and instructions. In a sense, these models may have the ability to call upon a specific skill or algorithm, similar to a function call, when given contextual examples and instructions. The unique aspect of this is that the programming language of transformers is in natural language, rather than traditional code. This opens up the possibility of using natural language commands to control and program these models, further expanding their potential as programmable computers.

In conclusion, there are several open problems that warrant further exploration in the field of programmable computers using transformer networks. One of the most intriguing possibilities is the potential to fuse hardcoded models with larger pretrained transformers, in order to harness the strengths of both. Additionally, as our constructions currently do not take into account the language aspect of the input, it would be interesting to investigate ways to tokenize input commands in order to map them to natural language. Another promising avenue for research is the potential for model distillation, in which larger networks could learn the skills performed by these looped transformers. Additionally, experimental validation through the creation of even smaller networks, trained on input-output pairs as well as internal representations, could provide further insight into the capabilities of these designs. Finally considering what architecture changes would make the above designs easier to implement and train, could lead to new insights in the field.

\section*{References}

Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.

A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930-945, 1993. doi: 10.1109/18.256500.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

François Charton. Linear algebra with transformers. arXiv preprint arXiv:2112.01898, 2021.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.

Esolangs. Subleq. URL https://esolangs.org/wiki/Subleq.

Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn incontext? a case study of simple function classes. In Advances in Neural Information Processing Systems, 2022.

DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. arXiv preprint arXiv:2203.07852, 2022.

Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186, 2019.

Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s): $1-41,2022$.

Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.

David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability. arXiv preprint arXiv:2301.05062, 2023 .

Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.

Farhad Mavaddat and Behrooz Parhami. Urisc: the ultimate reduced instruction set computer. International Journal of Electrical Engineering Education, 25(4):327-334, 1988.

William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 2022 .

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. 2021.

Dmytro Perekrestenko, Philipp Grohs, Dennis Elbrächter, and Helmut Bölcskei. The universal approximation power of finite-width deep relu networks. arXiv preprint arXiv:1806.01528, 2018 .

Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):1-35, 2021. URL http: / jmlr.org/papers/v22/ 20-302.html.

Jorge Pérez, Javier Marinković, and Pablo Barceló. On the turing completeness of modern neural network architectures, 2019. URL https://arxiv.org/abs/1901.03429.

Zhiqiang Shen, Zechun Liu, and Eric Xing. Sliced recursive transformer. In European Conference on Computer Vision, pages 727-744. Springer, 2022.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022.

Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. Advances on Neural Information Processing Systems (NeurIPS), 2022a.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022c.

Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pages 11080-11090. PMLR, 2021.

Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages $558-567,2021$.

Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2019.

Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.

\section*{A Ommited proofs}

\section*{A. 1 Addition of pointers.}

Lemma 16. There exists a 1-hidden layer feedforward, ReLU network, with 8d activations in the hidden layer and $d$ neurons in the output layer that when given two $d$-dimensional binary vectors representing two non-negative integers, can output the binary vector representation of their sum, as long as the sum is less than $2^{d+1}$.

Proof. For the purpose of explaining this proof, we use the $\{0,1\}^{d}$ binary representation of the integers, instead of the $\{ \pm 1\}^{d}$ binary representation. However, since the conversion of a bit between the two representations can be done easily using simple affine transformation, the proof will also work for the $\{ \pm 1\}^{d}$ binary representation.

Let the two integers be $a, b$ and let $c:=a+b$. We assume that $c<2^{d}$. Futher, let $a_{1}$ be the least significant bit of $a, a_{d}$ the most significant, and $a_{i}$ be the $i$-th most significant bit, and similarly for $b$ and $c$. Further, let $a_{[i]}$ represent the integer formed by considering only the least $i$ significant bits of $a$.

Note that $c_{i}$ is only dependent on the least $i$ bits of $a$ and $b$, and not on the more significant bits of $a$ or $b$. In particular, $c_{i}$ only depends on $a_{[i]}+b_{[i]}$. Define $s:=a_{[i]}+b_{[i]}$, and note that $c_{i}=s_{i}$. Further note that $s<2^{i+1}$ and hence can be represented in $i+1$ bits. Then, whenever $c_{i}=1$, there can be two cases: $\left(s_{i+1}=1, s_{i}=1\right)$; or $\left(s_{i+1}=0, s_{i}=1\right)$. This can be equivalently written as $c_{i}=1$ iff $s \in\left[2^{i-1}, 2^{i}-1\right] \cup\left[3 \cdot 2^{i-1}, 2^{i+1}-1\right]$. This can be computed by the following ReLU:

$$
\begin{aligned}
c_{i}= & \left(\operatorname{ReLU}\left(s-2^{i-1}+1\right)-\operatorname{ReLU}\left(s-2^{i-1}\right)\right)+\left(\operatorname{ReLU}\left(2^{i}-s\right)-\operatorname{ReLU}\left(2^{i}-s-1\right)\right)-1 \\
& +\left(\operatorname{ReLU}\left(s-3 \cdot 2^{i-1}+1\right)-\operatorname{ReLU}\left(s-3 \cdot 2^{i-1}\right)\right)
\end{aligned}
$$

Thus, each bit of $c$ can be computed using 6 neurons. Hence, computing the entire sum needs $8 d$ activations, as to substract the residual.

\section*{A. 2 Non-linear functions as sum of sigmoids}

Lemma 17. Consider an input of the form

$$
\mathbf{X}=\left[\begin{array}{cccccc}
\boldsymbol{e} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right] \in \mathbb{R}^{N+d_{x} \times 3 d}
$$

where d is chosen, $N$ is the number of functions we encode and $d_{x}$ is the dimension of $\mathbf{x} . \boldsymbol{e}=\mathbf{e}_{j}$ an indicator vector of the function we want to choose. Then there exists a transformer-based function block with 3 layers, $m$ heads and dimensionality $O(d)$ such that

$$
f(\mathbf{X})=\left[\begin{array}{cccccc}
* & * & * & * & \sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) & * \\
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right]
$$
where $*$ denoted inconsequential values that will be ignored downstream.

Proof. The first thing we do is to move the $\mathrm{x}$ to the second row block, as follows:

$\mathbf{X}=\left[\begin{array}{cccccc}\boldsymbol{e} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\ \mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\ 0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}\end{array}\right] \rightarrow\left[\begin{array}{cccccc}\boldsymbol{e} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\ \mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\ 0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}\end{array}\right]$

This can be done using a ReLU feedforward layer that performs this using the last row of the input as the indicator bit for the column containing $\mathbf{x}$.

Then we want to create the following transformation

$$
\left[\begin{array}{cccccc}
\boldsymbol{e} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right] \rightarrow\left[\begin{array}{cccccc}
* & * & * & * & \sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) & * \\
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right]
$$

The proof follows that of Lemma 5. We again ignore the last three rows by setting the corresponding rows in the key, query and values weight matrices to be zero. Let

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-43.jpg?height=134&width=1142&top_left_y=1277&top_left_x=486)

We note that for the purpose of this proof, each $\mathbf{a}_{i}$ has one extra element at the end equal to $-\log (3 d-1)$, while the vectors $\mathbf{x}$ will have the last element equal to one. Then we will have

$$
\begin{aligned}
& \sigma_{\mathbf{S}}\left(\left(\mathbf{K}^{i} \mathbf{X}\right)^{T}\left(\mathbf{Q}^{i} \mathbf{X}\right)\right)=\left[\begin{array}{cc}
\mathbf{a}_{j i}^{\top} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0}
\end{array}\right]\left[\begin{array}{llllll}
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right] \\
& =\left[\begin{array}{cccccc}
0 & 0 & \mathrm{a}_{j i}^{\top} \mathrm{x} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right] \\
& =\left[\begin{array}{cccccc}
* & * & \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) & * & * & * \\
* & * & * & * & * & * \\
* & * & * & * & * & * \\
* & * & * & * & * & * \\
* & * & * & * & * & * \\
* & * & * & * & * & *
\end{array}\right]
\end{aligned}
$$
since $\mathbf{a}_{j i}^{\top} \mathbf{x}=\mathbf{a}_{j i}^{\top} \mathbf{x}-\log 3 d-1$ and thus $e^{\mathbf{a}_{j i}^{\top} \mathbf{x}} /\left(3 d-1+e^{\mathbf{a}_{j i}^{\top} \mathbf{x}}\right)=\sigma\left(\mathbf{a}_{j i}^{\top} \mathbf{x}\right)$ with a slight abuse of notation over the inner product $\mathbf{a}_{j i}^{\top} \mathbf{x}$ to account for the extra corrections bias term. Thus,

$$
\mathbf{V X} \sigma_{\mathrm{S}}\left((\mathbf{K X})^{T}(\mathbf{Q X})\right)=\left[\begin{array}{cccccc}
* & * & c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) & * & * & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$

By summing over all heads and adding the residual we get

$$
\left[\begin{array}{cccccc}
* & * & \sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) & * & * & * \\
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right]
$$

Finally, we use an extra layer similarly to Lemma 3 to write the result in the desired output. Hence, we get

$$
\left[\begin{array}{cccccc}
* & * & * & * & \sum_{i=1}^{m} c_{j i} \sigma\left(\mathbf{x}^{T} \mathbf{a}_{j i}\right) & * \\
\mathbf{0} & \mathbf{0} & \mathbf{x} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{p}_{1} & \mathbf{p}_{2: d} & \mathbf{0} & \mathbf{p}_{d+2: 2 d} & \mathbf{p}_{2 d+1} & \mathbf{p}_{2 d+2: 3 d} \\
0 & 0_{2: d} & 1 & 0_{d+2: 2 d} & 0 & 0_{2 d+2: 3 d}
\end{array}\right]
$$

However, we have another way of controlling the input, which is by the size of each attention mechanism, that is directly controlled by the dimension $d$ of the embedding.

Lemma 18. Consider an input of the form

$$
\mathbf{X}=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x}  \tag{16}\\
0 & \ldots & 0 \\
1-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

where $\boldsymbol{e}_{i}$ is the one hot vector with 1 in the $i-$ th position and $\boldsymbol{x} \in \mathbb{R}^{d}$. Let $m$ be the number of sigmoids we need to represent a function, then there exists a one layer transformer with 1 head such that

$$
\operatorname{Attn}(\mathbf{X})=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x}  \tag{17}\\
\sigma\left(\mathbf{a}_{1}^{\top} \boldsymbol{x}\right) & \ldots & \sigma\left(\mathbf{a}_{m}^{\top} \boldsymbol{x}\right) \\
\mathbf{1}-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

Proof. Let

$$
\mathbf{K}=\left[\begin{array}{cccc}
\mathbf{0}^{\top} & 0 & \mathbf{0}^{\top} & \boldsymbol{e}_{1}^{\top}  \tag{18}\\
\vdots & \vdots & \vdots & \vdots \\
\mathbf{0}^{\top} & 0 & \mathbf{0}^{\top} & \boldsymbol{e}_{m}^{\top}
\end{array}\right], \mathbf{Q}=\left[\begin{array}{cccc}
\mathbf{a}_{1}^{\top} & 0 & -C \boldsymbol{e}_{1}^{\top} & \mathbf{0}^{\top} \\
\vdots & \vdots & \vdots & \vdots \\
\mathbf{a}_{m}^{\top} & 0 & -C \boldsymbol{e}_{m}^{\top} & \mathbf{0}^{\top} \\
\mathbf{0}^{\top} & 0 & \mathbf{0}^{\top} & \mathbf{0}^{\top}
\end{array}\right]
$$

Hence,

$$
\mathbf{K X}=\mathbf{I}_{d}, \mathbf{Q} \mathbf{X}=\left[\begin{array}{cccc}
\mathbf{a}_{1}^{\top} \boldsymbol{x} & -C+\mathbf{a}_{1}^{\top} \boldsymbol{x} & \ldots & -C+\mathbf{a}_{1}^{\top} \boldsymbol{x}  \tag{19}\\
-C+\mathbf{a}_{2}^{\top} \boldsymbol{x} & \mathbf{a}_{2}^{\top} \boldsymbol{x} & \ldots & -C+\mathbf{a}_{2}^{\top} \boldsymbol{x} \\
\vdots & \vdots & \vdots & \vdots \\
-C+\mathbf{a}_{m}^{\top} \boldsymbol{x} & -C+\mathbf{a}_{m}^{\top} \boldsymbol{x} & \ldots & \mathbf{a}_{m}^{\top} \boldsymbol{x} \\
0 & 0 & \ldots & 0
\end{array}\right]
$$

After applying softmax we get,

$$
\sigma_{s}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right) \approx\left[\begin{array}{cccc}
\sigma\left(\mathbf{a}_{1}^{\top} \boldsymbol{x}\right) & 0 & \ldots & 0 \\
0 & \sigma\left(\mathbf{a}_{2}^{\top} \boldsymbol{x}\right) & \ldots & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \ldots & \sigma\left(\mathbf{a}_{m}^{\top} \boldsymbol{x}\right) \\
* & 0 & \ldots & *
\end{array}\right]
$$

for large enough $C$. Next we set

$$
\mathbf{V}=\left[\begin{array}{cccccc}
\mathbf{0} & 0 & \mathbf{0} & 0 & \ldots & 0 \\
\mathbf{0} & 0 & \mathbf{0} & c_{1} & \ldots & c_{m} \\
\mathbf{0} & 0 & \mathbf{0} & 0 & \ldots & 0 \\
\mathbf{0} & 0 & \mathbf{0} & 0 & \ldots & 0
\end{array}\right]
$$

thus resulting in

$$
\mathbf{V X}=\left[\begin{array}{ccccc}
0 & 0 & \ldots & 0 & 0 \\
c_{1} & c_{2} & \ldots & c_{m} & 0 \\
0 & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & \ldots & 0 & 0
\end{array}\right]
$$

Hence, we get

$$
\mathbf{V X} \sigma_{s}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{ccc}
0 & \ldots & 0 \\
c_{1} \sigma\left(\mathbf{a}_{1}^{\top} \boldsymbol{x}\right) & \ldots & c_{m} \sigma\left(\mathbf{a}_{m}^{\top} \boldsymbol{x}\right) \\
0 & \ldots & 0 \\
\vdots & \vdots & \vdots \\
0 & \ldots & 0 \\
0 & \cdots & 0
\end{array}\right]
$$

and

$$
\mathbf{X}+\mathbf{V X} \sigma_{s}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x} \\
c_{1} \sigma\left(\mathbf{a}_{1}^{\top} \boldsymbol{x}\right) & \ldots & c_{m} \sigma\left(\mathbf{a}_{m}^{\top} \boldsymbol{x}\right) \\
\mathbf{1 - \boldsymbol { e } _ { 1 }} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

Corollary 6. Consider an input of the form

$$
\mathbf{X}=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \mathbf{0}  \tag{20}\\
0 & \ldots & 0 \\
\mathbf{1}-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

where $m$ is the number of sigmoids we use and $\boldsymbol{e}_{i}$ is an indicator vector and $\mathrm{x} \in \mathbb{R}^{d}$; then there exists a 3 layer transformer with 1 head such that

$$
\operatorname{Attn}(\mathbf{X})=\left[\begin{array}{ccc}
\sum_{i=1}^{m} \sigma\left(\mathbf{a}_{i}^{\top} \boldsymbol{x}\right) & \ldots & \sum_{i=1}^{m} \sigma\left(\mathbf{a}_{i}^{\top} \boldsymbol{x}\right)  \tag{21}\\
\mathbf{0} & \ldots & \mathbf{0}
\end{array}\right]
$$

Proof. Given the input

$$
\mathbf{X}=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \mathbf{0}  \tag{22}\\
0 & \ldots & 0 \\
\mathbf{1}-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

we set the query and key matrices as follows:

$$
\mathbf{K}=\mathbf{Q}=\left[\begin{array}{llll}
\mathbf{0}^{\top} & 0 & \mathbf{1} & \mathbf{1}
\end{array}\right]
$$

Then, we get

$$
(\mathbf{K X})^{\top} \mathbf{Q X}=\left[\begin{array}{ccc}
d & \ldots & d \\
\vdots & \ldots & \vdots \\
d & \ldots & d
\end{array}\right]
$$

Setting the value matrix to

$$
\left[\begin{array}{cccc}
d \mathbf{I} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$

we get

$$
\mathbf{V X} \sigma_{\mathrm{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x} \\
0 & \ldots & 0 \\
\mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \ldots & \mathbf{0}
\end{array}\right]
$$

Hence, the output of the attention layer is:

$$
\mathbf{X}+\mathbf{V X} \sigma_{\mathrm{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{ccc}
2 \boldsymbol{x} & \ldots & \boldsymbol{x} \\
0 & \ldots & 0 \\
\mathbf{1}-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

Note that using the embeddings in the last rows and a feedforward network can be used to produce the following

$$
\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x} \\
0 & \ldots & 0 \\
1-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

Now, passing this into the transformer of Lemma 18 will result in

$$
\operatorname{Attn}(\mathbf{X})=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x}  \tag{23}\\
c_{1} \sigma\left(\mathbf{a}_{1}^{\top} \boldsymbol{x}\right) & \ldots & c_{m} \sigma\left(\mathbf{a}_{m}^{\top} \boldsymbol{x}\right) \\
\mathbf{1 - \boldsymbol { e } _ { 1 }} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

For the third layer, we set the key and query matrices as follows

$$
\mathbf{K}=\mathbf{Q}=\left[\begin{array}{llll}
\mathbf{0}^{\top} & 0 & 1 & 1
\end{array}\right]
$$

Then, we get

$$
(\mathbf{K X})^{\top} \mathbf{Q} \mathbf{X}=\left[\begin{array}{ccc}
m & \ldots & m \\
\vdots & \ldots & \vdots \\
m & \ldots & m
\end{array}\right]
$$

Setting the value matrix to

$$
\left[\begin{array}{cccc}
0 & 0 & 0 & 0 \\
0 & m & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

we get

$$
\mathbf{V X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{ccc}
\mathbf{0} & \ldots & \mathbf{0} \\
\sum_{i=1}^{m} c_{i} \sigma\left(\mathbf{a}_{i}^{\top} \boldsymbol{x}\right) & \ldots & \sum_{i=1}^{m} c_{i} \sigma\left(\mathbf{a}_{i}^{\top} \boldsymbol{x}\right) \\
\mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \ldots & \mathbf{0}
\end{array}\right]
$$

Hence, the output of the attention layer is:

$$
\mathbf{X}+\mathbf{V X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q} \mathbf{X}\right)=\left[\begin{array}{ccc}
\boldsymbol{x} & \ldots & \boldsymbol{x} \\
\sum_{i=1}^{m} c_{i} \sigma\left(\mathbf{a}_{i}^{\top} \boldsymbol{x}\right) & \ldots & \sum_{i=1}^{m} c_{i} \sigma\left(\mathbf{a}_{i}^{\top} \boldsymbol{x}\right) \\
\mathbf{1}-\boldsymbol{e}_{1} & \ldots & \mathbf{1}-\boldsymbol{e}_{m} \\
\boldsymbol{e}_{1} & \ldots & \boldsymbol{e}_{m}
\end{array}\right]
$$

Finally, the feedforward layers can be used to move the results to the first row.

\section*{A. 3 Matrix Transposition}

Lemma 19. Fix $\epsilon>0$ and consider an input of the following form

$$
\mathbf{X}=\left[\begin{array}{c|c|c|cc}
\mathbf{A} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \mathbf{P}_{2}^{\prime} & \mathbf{P}_{3}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$

where $\mathbf{A} \in \mathbb{R}^{d \times d}$; then there exists transformer-based function block with 4 layers, 1 head and dimensionality $r=2 d+2 \log d=O(d)$ that outputs the following matrix

$$
\mathbf{X}=\left[\begin{array}{c|c|c|cc}
\mathbf{A}^{\prime} & \mathbf{A}^{\prime} & \mathbf{A}^{\prime} & \ldots & \mathbf{A}^{\prime} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \mathbf{P}_{2}^{\prime} & \mathbf{P}_{3}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$

where $\mathbf{A}^{\prime}=\mathbf{A}^{\top}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$.

Proof. We can vectorize the matrix $\mathbf{A}$ into a $d^{2}$ dimensional vector using the attention mechanism, as shown in Eq. (24). Notice that once we have the matrix in this form we can implement its transpose with a fixed permutation of the columns of the matrix to get the vectorized form of $\mathbf{A}^{\top}$. Once we have the transpose in vector form, we matricize it back to get the matrix transform using the attention mechanism. We explain the details of this process below:

Vectorization: We assume that the input is of the following form, where $\mathbf{A}$ is the matrix to be vectorized.

$$
\mathbf{X}=\left[\begin{array}{cccc}
\mathbf{A} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \mathbf{P}_{2}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$

Here, $\mathbf{P}_{i}^{\prime}$ represents a matrix of $d$ columns, where each column is $\mathbf{p}_{i}$.

The first layer uses the $\mathbf{p}_{1: d}$ encodings to make $d$ copies of the matrix $\mathbf{A}$, as follows:

$$
\mathbf{X}=\left[\begin{array}{cccc}
\mathbf{A} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{A} & \mathbf{A} & \ldots & \mathbf{A} \\
\mathbf{p}_{1: d} & \mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \mathbf{P}_{2}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$

The feed forward part of the second layer then uses the encodings $\mathbf{p}_{i}^{\prime}$ to vectorize the matrix in the second row block as follows:

$$
\begin{equation*}
\mathbf{X}=\left[\right] \tag{24}
\end{equation*}
$$

This is achieved, by explicitly defining a neural network that keeps the $i-$ th row if the corresponding encoding is $\mathbf{P}_{i}^{\prime}$ and place it in the $d+1$ row.

Transposition in the vector form: Once we have the matrix vectorized as the second row block of the scratchpad, the following key and query matrices

$$
\mathbf{K}=\left[\begin{array}{llll}
0 & 0 & \mathrm{I} & \mathbf{0} \\
0 & 0 & 0 & \mathrm{I}
\end{array}\right], \mathbf{Q}=\left[\begin{array}{llll}
0 & 0 & 0 & \mathrm{I} \\
0 & 0 & \mathrm{I} & 0
\end{array}\right]
$$

results in the head outputting the following, which is the vectorized form of $\mathbf{A}^{\top}$ (in the second row block)

$$
\mathbf{X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top}(\mathbf{Q X})\right)=\left[\right]
$$

Then, using the following value matrix gives

$$
\begin{aligned}
& \mathrm{V}=\left[\begin{array}{llll}
0 & 0 & 0 & 0 \\
0 & \mathrm{I} & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right] \\
& \mathbf{V X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top}(\mathbf{Q X})\right)=\left[\begin{array}{cccc}
\mathbf{0} & & \ldots & \mathbf{0} \\
{\left[\begin{array}{ccc}
A_{(1,1)} & \ldots & A_{(d, 1)} \\
\mathbf{0} & \ldots & \mathbf{0}
\end{array}\right]} & \ldots \\
& \mathbf{0} & \ldots & {\left[\begin{array}{ccc}
A_{(1, d)} & \ldots & A_{(d, d)} \\
\mathbf{0} & \ldots & \mathbf{0}
\end{array}\right]} \\
& \mathbf{0} & \ldots & \mathbf{0} \\
& & \mathbf{0}
\end{array}\right]
\end{aligned}
$$

Adding back the $\mathbf{X}$ (see (1)), results in

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-49.jpg?height=283&width=1452&top_left_y=1618&top_left_x=331)

Using the feedforward layers and the encodings $\mathbf{P}_{i}^{\prime}$, we get

$$
\mathbf{X}=\left[\right]
$$

Using an attention layer and the first row of encodings, we get

$$
\mathbf{X}=\left[\begin{array}{ccc}
\mathbf{A}^{\top} & \ldots & \mathbf{A}^{\top} \\
\mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{p}_{1: d} & \ldots & \mathbf{p}_{1: d} \\
\mathbf{P}_{1}^{\prime} & \ldots & \mathbf{P}_{d}^{\prime}
\end{array}\right]
$$

\section*{A. 4 Matrix Multiplication by Linearizing the Softmax}

We will show how we can implement matrix multiplication so that it will fit our unified template. To do so, we need to show for example for the result of $\mathbf{A}^{\top} \mathbf{B}$, where $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$ with $k, m, n<d$ we can achieve the following:

$$
\left[\begin{array}{cc|cc|c}
\mathrm{A} & \mathbf{0} & \mathbf{B} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right] \rightarrow\left[\begin{array}{cc|cc|cc}
* & * & * & * & \mathbf{A}^{\top} \mathbf{B} & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$

Lemma 20. Let $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$; then for any $\epsilon>0$ there exists a transformer-based function block with 2 layers, 1 head and dimensionality $r=O(d)$ that outputs the multiplication $\mathbf{A}^{T} \mathbf{B}^{T}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$.

Corollary 7. Let $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$; then for any $\epsilon>0$ there exists a transformer-based function block with 2 layers, 1 head and dimensionality $r=O(d)$ that outputs the multiplication $\mathbf{B}^{\top} \mathbf{A}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$.

Corollary 8. Let $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$; then for any $\epsilon>0$ there exists a transformer-based function block with 2 layers, 1 head and dimensionality $r=O(d)$ that outputs the multiplication $\mathbf{B}^{\top} \mathbf{B}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$.

Corollary 9. Let $\mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$; then for any $\epsilon>0$ there exists a transformer-based function block with 2 layers, 1 head and dimensionality $r=O(d)$ that outputs the multiplication $\mathbf{A}^{\top} \mathbf{A}+\epsilon \mathbf{M}$, for some $\|\mathbf{M}\| \leq 1$.

We will prove just the first of these results and the rest are a simple corollary of it.

Proof. Let $\mathbf{M} \in \mathbb{R}^{2 d \times 2 d}, \mathbf{A} \in \mathbb{R}^{k \times m}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$ be the following matrices:

$$
\mathbf{M}=\left[\begin{array}{cccc}
\mathrm{A} & 0 & \mathrm{~B} & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

The zeros pad the rows and columns to ensure that the matrix $M$ is $2 d \times 2 d$. Then, consider the input matrix to be of the following form:

$$
\mathbf{X}=\left[\begin{array}{ccc}
\mathbf{M} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{1 1} \\
\mathbf{I} & \mathbf{0} & \mathbf{0} \\
& \mathbf{p}^{(1)} & \\
& \mathbf{p}^{(2)} & \\
\mathbf{0} & \mathbf{1}^{T} & \mathbf{0}
\end{array}\right]
$$

where $1 \in \mathbb{R}^{2 d}$ is the all ones vector. The identity matrix $I$ and the all ones matrix $11^{\top}$ are part of the design of the input and they are always fixed. For now we ignore the encodings and the last row, by setting the corresponding rows of the key,query and value weight matrices to be zero. These rows will be used to copy the output to the place that we want.

Focusing on the rest of the rows, we set the key and query weight matrices to be

$$
\mathbf{K}=\mathbf{I}, \mathbf{Q}=\left[\begin{array}{ccc}
c \mathbf{I} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & C \mathbf{I} \\
\mathbf{0} & \mathbf{I} & \mathbf{0}
\end{array}\right], \mathbf{V}=\left[\begin{array}{ccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & n e^{C} \mathcal{D}_{d} \\
\mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$

where $\mathcal{D}_{d} \in \mathbb{R}^{2 d \times 2 d}$ is the diagonal matrix with the first $d$ diagonal elements 1 , and the rest 0 . Thus we have

$$
\begin{align*}
& (\mathbf{K X})^{\top} \mathbf{Q X}=\left[\begin{array}{ccc}
\mathbf{M} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{1 1} \\
\mathbf{I} & \mathbf{0} & \mathbf{0}
\end{array}\right]^{\top}\left[\begin{array}{ccc}
c \mathbf{M} & \mathbf{0} & \mathbf{0} \\
C \mathbf{I} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{1 1}^{\top} & \mathbf{0}
\end{array}\right]  \tag{25}\\
& =\left[\begin{array}{cccc}
c \mathbf{M}^{\top} \mathbf{M} & \mathbf{1 1} \\
C 11^{\top} & \mathbf{0} & \\
& \mathbf{0} & \mathbf{0} & \\
& \mathbf{0}
\end{array}\right] \tag{26}
\end{align*}
$$

Each of the first $2 d$ columns above looks as follows

$$
\left[\begin{array}{llllll}
c z_{1 i} & c z_{2 i} & \ldots & c z_{n i} & C \mathbf{1}^{\top} & \mathbf{0}
\end{array}\right]
$$

After we apply the softmax $\sigma_{s}$ per column, we get

$$
\sigma_{s}\left(c z_{i j}\right)=\frac{e^{c z_{i j}}}{\sum_{j=1}^{n} e^{c z_{i j}}+n\left(e^{C}+1\right)}
$$

where $n=2 d, z_{i j}$ is the $(i, j)$ element of the matrix $\mathbf{M}^{\top} \mathbf{M}$. Let $\ell(\cdot)$ be the transformation above then we have

$$
\begin{aligned}
& \mathbf{V X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)=\left[\begin{array}{ccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} \\
n e^{C} \mathcal{D}_{d} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]\left[\begin{array}{ccc}
\ell\left(c \mathbf{M}^{\top} \mathbf{M}\right) & * & * \\
* & * & * \\
* & * & *
\end{array}\right] \\
& =\left[\begin{array}{ccc}
\mathbf{0} & \mathbf{0} & \mathbf{0} \\
n e^{C} \mathcal{D}_{d} \ell\left(c \mathbf{M}^{\top} \mathbf{M}\right) & * & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right] \\
& \approx\left[\begin{array}{ccc}
\mathbf{0} & \mathbf{0} & 0 \\
11^{\top}+c \mathbf{M}^{\top} \mathbf{M} & * & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0}
\end{array}\right]
\end{aligned}
$$

and by adding back the residual we have

$$
\mathbf{X}=\left[\begin{array}{ccc}
\mathbf{M} & \mathbf{0} & \mathbf{0} \\
\mathbf{1 1} 1^{\top}+c \mathbf{M}^{\top} \mathbf{M} & * & * \\
\mathbf{I} & \mathbf{0} & \mathbf{0}
\end{array}\right]
$$
for small enough $c$ and large enough $C$. This is because

$$
\begin{aligned}
n e^{C} \frac{e^{c x_{i j}}}{\sum_{j=1}^{n} e^{c x_{i j}}+n\left(e^{C}+1\right)} & =e^{c x_{i j}} \frac{1}{1+\sum_{j=1}^{n} e^{c x_{i j}-C-\log n}+n} \\
& =\left(1+c x_{i j}+O\left(\left(c x_{i j}\right)^{2}\right)\right)\left(1-e^{c x_{i j}-C-\log n}+O\left(e^{2\left(c x_{i j}-C-\log n\right)}\right)\right) \\
& =\left(1+c x_{i j}+O\left(\left(c x_{i j}\right)^{2}\right)\right)\left(1-e^{c x_{i j}-C-\log n}\right) \\
& \approx\left(1+c x_{i j}\right)
\end{aligned}
$$

We now use the feedforward layers to perform the following transform

$$
\begin{aligned}
\mathbf{X} & =\left[\begin{array}{ccc}
* & * & * \\
\mathbf{M}^{\top} \mathbf{M} & * & * \\
* & * & *
\end{array}\right] \\
& =\left[\begin{array}{ccccc}
* & * & * & * & * \\
\mathbf{A}^{\top} \mathbf{A} & \mathbf{0} & \mathbf{A}^{\top} \mathbf{B} & \mathbf{0} & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & * \\
\mathbf{B}^{\top} \mathbf{A} & \mathbf{0} & \mathbf{B}^{\top} \mathbf{B} & \mathbf{0} & * \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0} & * \\
* & * & * & * & *
\end{array}\right]
\end{aligned}
$$

Now if $\mathbf{p}^{(1)}=\left[\begin{array}{lllll}\mathbf{0} & \mathbf{0} & \mathbf{p}_{2 d+1: 2 d+n} & \mathbf{0} & \mathbf{0}\end{array}\right]$ and $\mathbf{p}^{(2)}=\left[\begin{array}{lllll}\mathbf{p}_{1: n} & \mathbf{p}_{n+1: d} & \mathbf{0} & \mathbf{p}_{d+n+1: 2 d} & \mathbf{p}_{2 d: 3 d}\end{array}\right]$ we can copy $\mathbf{A}^{\top} \mathbf{B}$ to the desired place using Lemma 2.

\section*{B Error Analysis}

In all of this section we assume that each element of the input matrix $\mathbf{X}$ has values $v_{i}$ bounded by some constant $G$, i.e., $\left|v_{i}\right| \leq G$.

The error in the read/ write operation. The positional encodings as we have already mentioned have the following properties: $\mathbf{p}_{i}$ is an $\log (n)$ dimensional $\pm 1$ vector which is the binary representation of $i$ with -1 in the place of 0 . Hence, we have $\mathbf{p}_{i}^{\top} \mathbf{p}_{i}=\log (n)$ and each $\mathbf{p}_{i}^{\top} \mathbf{p}_{j}<\log (n)$ for $i \neq j$.

Each time a copy is implemented from one column to another, we create a permutation matrix (a matrix of zeros and ones) which then multiplies the input matrix $\mathbf{X} \in \mathbb{R}^{d \times n}$ from the right and results in permutations of the column space. We thus focus on just one column of the $n \times n$ matrix that is created after we apply the softmax. Let $\mathbf{z}$ be this column of the matrix, ideally we want to output in one position 1 and in the rest 0 . In the place that we want to output 1 , say the $a-$ th position, we have the inner product $\mathbf{z}_{a}=\mathbf{p}_{i}^{\top} \mathbf{p}_{i}$ for some $i \in[n]$. The rest of the elements in the same column would be $\mathbf{z}_{b} \leq \mathbf{p}_{i}^{\top} \mathbf{p}_{j}$ for $i \neq j$ and $a \neq b$. Then,

$$
\begin{aligned}
{\left[\sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)\right]_{i, i} } & =\frac{e^{\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{i}}}{e^{\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{i}}+\sum_{j \neq i} e^{\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{j}}} \\
& =\frac{1}{1+\sum_{j \neq i} e^{\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{j}} / e^{\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{i}}}
\end{aligned}
$$

Since $\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{j}<\lambda \mathbf{p}_{i}^{\top} \mathbf{p}_{i}-\lambda$ for $i \neq j$, we have that

$$
\begin{aligned}
{\left[\sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)\right]_{i, i} } & \geq \frac{1}{1+n e^{-\lambda}} \\
& \geq \frac{1}{1+e^{\log n-\lambda}} \\
& \geq 1-\frac{e^{\log n-\lambda}}{1+e^{\log n-\lambda}} \\
& \geq 1-e^{\log n-\lambda}
\end{aligned}
$$

Thus, for $i \neq j,\left[\sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q} \mathbf{X}\right)\right]_{i, j} \leq e^{\log n-\lambda}$. This implies that there exist $\epsilon_{i}, i=1, \ldots, n$ such that

$$
\begin{aligned}
& \mathbf{z}_{a}=1-\varepsilon_{a}, \text { for some } \varepsilon_{a} \leq e^{\log n-\lambda} \\
& \mathbf{z}_{b}=\varepsilon_{b} \text { for } b \neq a \text { and for some } \varepsilon_{b} \leq e^{\log n-\lambda}
\end{aligned}
$$

Hence, we have that

$$
\mathbf{z}=\mathbf{z}^{*}+\varepsilon
$$

where $\mathbf{z}^{*}$ is the targeted vector and $\varepsilon$ is the vector containing the errors $\varepsilon_{a}, \varepsilon_{b}$.

Now let $\mathbf{x}_{i}$ be the $i$-th row of the input matrix $\mathbf{X}$, then we have

$$
\begin{aligned}
\mathbf{X z} & =\mathbf{X z}^{*}+\mathbf{X} \varepsilon \\
& =\mathbf{X z}^{*}+\left[\begin{array}{c}
\left\langle\mathbf{x}_{1}, \varepsilon\right\rangle \\
\vdots \\
\left\langle\mathbf{x}_{d}, \varepsilon\right\rangle
\end{array}\right]
\end{aligned}
$$

In the general case that all the columns will change, let $\mathbf{P}=\sigma_{\mathrm{S}}\left((\mathbf{K X})^{\top} \mathbf{Q X}\right)$ and $\mathbf{P}^{*}$ be the targeted matrix then we have that

$$
\mathbf{X P}=\mathbf{X P} \mathbf{P}^{*}+\mathbf{X E}
$$

where $\mathbf{E}=\left[\begin{array}{lll}\varepsilon_{1} & \ldots & \varepsilon_{n}\end{array}\right]$ is the matrix containing all the errors and so

$$
\begin{aligned}
\left\|\mathbf{X P}-\mathbf{X P}^{*}\right\| & =\max _{1 \leq j \leq n} \sum_{i=1}^{d}\left|\left\langle\mathbf{x}_{i}, \varepsilon_{j}\right\rangle\right| \\
& \leq G n^{2} d e^{\log n-\lambda} \\
& \leq e^{\log G d n^{3}-\lambda}
\end{aligned}
$$

Thus, if $\lambda>\log \frac{G d n^{3}}{\epsilon}$ we have that

$$
\left\|\mathbf{X P}-\mathbf{X P}^{*}\right\| \leq \epsilon
$$

The error in Matrix Multiplication . This error has already been calculated in Appendix A.4, however we explicitly define it here as follows:

$$
\begin{aligned}
n e^{C} \frac{e^{c x_{i j}}}{\sum_{j=1}^{n} e^{c x_{i j}}+n\left(e^{C}+1\right)} & =e^{c x_{i j}} \frac{1}{1+\sum_{j=1}^{n} e^{c x_{i j}-C-\log n}+n} \\
& =\left(1+c x_{i j}+O\left(\left(c x_{i j}\right)^{2}\right)\right)\left(1-e^{c x_{i j}-C-\log n}+O\left(e^{2\left(c x_{i j}-C-\log n\right)}\right)\right)
\end{aligned}
$$

Let $c=\frac{\epsilon_{1}}{C_{1} G}$ for some constant $C_{1}$ and $C=\log \frac{C_{2}}{\epsilon_{2}}$ for some $C_{2}$ then we have

$$
\begin{aligned}
A & =n e^{C} \frac{e^{c x_{i j}}}{\sum_{j=1}^{n} e^{c x_{i j}}+n\left(e^{C}+1\right)} \\
& =e^{c x_{i j}} \frac{1}{1+\sum_{j=1}^{n} e^{c x_{i j}-C-\log n}+n} \\
& =\left(1+c x_{i j}+\frac{\epsilon_{1}^{2} x_{i j}^{2}}{G^{2}}\right)\left(1-\frac{e^{c x_{i j} \epsilon_{2}}}{n}+\frac{e^{2 c x_{i j}} \epsilon_{2}^{2}}{n^{2}}\right) \\
& =\left(1+c x_{i j}\right)\left(1-\frac{e^{c x_{i j} \epsilon_{2}}}{n}+\frac{e^{2 c x_{i j}} \epsilon_{2}^{2}}{n^{2}}\right)+\frac{\epsilon_{1}^{2} x_{i j}^{2}}{G^{2}}\left(1-\frac{e^{c x_{i j}} \epsilon_{2}}{n}+\frac{e^{2 c x_{i j}} \epsilon_{2}^{2}}{n^{2}}\right)
\end{aligned}
$$

Thus,

$$
\begin{aligned}
\left|A-\left(1+c x_{i j}\right)\right| & =\left|-\left(1+c x_{i j}\right) \frac{e^{c x_{i j} \epsilon_{2}}}{n}+\frac{e^{2 c x_{i j}} \epsilon_{2}^{2}}{n^{2}}+\frac{\epsilon_{1}^{2} x_{i j}^{2}}{G^{2}}\left(1-\frac{e^{c x_{i j}} \epsilon_{2}}{n}+\frac{e^{2 c x_{i j}} \epsilon_{2}^{2}}{n^{2}}\right)\right| \\
& \leq \epsilon_{1}^{2}\left(\frac{e^{\epsilon_{1} / C_{1}} \epsilon_{2}}{n}+2 \frac{e^{2 \epsilon_{1} / C_{1}} \epsilon_{2}^{2}}{n^{2}}\right)+\frac{e^{\epsilon_{1} / C_{1}} \epsilon_{2}}{n}\left(1+\frac{\epsilon_{1}}{C_{1}}\right) \\
& \leq 4 \frac{e^{\epsilon_{1} / C_{1}} \epsilon_{2}}{n}
\end{aligned}
$$

Hence if $\epsilon_{2}=\epsilon / 4$ and $\epsilon_{1}=C_{1} \log (n \epsilon)$ we have that the total error is less than $\epsilon$.

Function approximation. The error in Lemma 5 is an immediate consequence of Theorem 3 and it is proportional to $1 / \sqrt{m}$, where $m$ is the number of heads we are using.

Accumulation of error after $T$ operations. Fix an $\epsilon>0$ and assume that in the $t$-th iteration the input is $\mathbf{X}_{t}=\mathbf{X}_{t}^{*}+\epsilon_{t} \mathbf{M}_{t}$, where $\mathbf{X}_{t}^{*}$ is the ideal input $0<\epsilon_{t}<\frac{t \epsilon}{T}$ and $\mathbf{M}_{t}$ is a matrix such that $\left\|\mathbf{M}_{t}\right\| \leq 1$, we will show that $\mathbf{X}_{t+1}=\mathbf{X}_{t+1}^{*}+\epsilon_{t+1} \mathbf{M}_{t+1}$, where $\mathbf{X}_{t+1}^{*}$ is the ideal input, $0<\epsilon_{t+1}<\frac{(t+1) \epsilon}{T}$ and $\mathbf{M}_{t+1}$ is a matrix such that $\left\|\mathbf{M}_{t+1}\right\| \leq 1$.
- Matrix Multiplication with a matrix $\mathbf{A},\|\mathbf{A}\| \leq 1^{3}$ will have the following result:

$$
\mathbf{A} \mathbf{X}_{t}+\epsilon^{\prime}=\mathbf{A} \mathbf{X}_{t}^{*}+\epsilon_{t} \mathbf{A} \mathbf{M}_{t}+\epsilon^{\prime} \mathbf{M}^{\prime}=\mathbf{X}_{t+1}^{*}+\left(\epsilon_{t}+\epsilon^{\prime}\right) \mathbf{M}_{t+1}
$$
\footnotetext{
${ }^{3}$ Notice that this can be assumed without loss of generality, since we can normalize all the errors with the maximum norm of a matrix to the power of $T$.
}
where $\epsilon^{\prime}$ is controlled by the constants we use in the design of the function block and $\mathrm{M}_{t+1}$ is some matrix with $\left\|\mathbf{M}_{t+1}\right\| \leq 1$. If now $\epsilon^{\prime}<\frac{\epsilon}{T}$, our claim follows.
- Read/Write operations will result to an error of

$$
\mathbf{X}_{t} \mathbf{P}=\mathbf{X}_{t} \mathbf{P}^{*}+\epsilon^{\prime} \mathbf{M}^{\prime}=\mathbf{X}_{t}^{*} \mathbf{P}^{*}+\epsilon_{t} \mathbf{M}_{t} \mathbf{P}^{*}+\epsilon^{\prime} \mathbf{M}^{\prime}
$$

Notice that as before, since $\left\|\mathbf{M}^{\prime}\right\| \leq 1$ and $\left\|\mathbf{M}_{t} \mathbf{P}^{*}\right\| \leq 1$ and thus we have $\mathbf{X}_{t+1}=\mathbf{X}_{t} \mathbf{P}=$ $\mathbf{X}_{t+1}^{*}+\epsilon_{t+1} \mathbf{M}_{t+1}$, where $\epsilon_{t+1}=\epsilon_{t}+\epsilon^{\prime}$. Again if $\epsilon^{\prime} \leq \frac{\epsilon}{T}$ the result follows.
- The result for function approximation follows in a similar way.

\section*{C subleq is Turing Complete}

In this section, we show that our slightly restricted version of the original SUBLEQ instruction [Mavaddat and Parhami, 1988] is indeed also Turing complete. To do this, we will utilize Minsky machines, which are also Turing complete. A Minksy machine comprises of registers and a list of instructions, where each instruction can be either of the following two instructions
- $\operatorname{add}(\mathrm{a}): \operatorname{mem}[a]:=\operatorname{mem}[a]+1$, go to the next instruction.
- sub(a, n): If mem $[a]==0$, go to instruction $n$. Otherwise mem $[a]:=\operatorname{mem}[a]-1$, go to the next instruction.

Given a program written in a language above, we translate it into an equivalent one written in our SUBLEQ language. For this, we initialize three fixed locations / registers $c_{-1}, c_{0}$, and $c_{+1}$ such that $\operatorname{mem}\left[c_{-1}\right]:=-1, \operatorname{mem}\left[c_{0}\right]:=0$, and $\operatorname{mem}\left[c_{+1}\right]:=+1$; as well as an extra register mem $[b]$. We translate the program instruction-by-instruction. Assume that we have translated the first $i-1$ instructions. Let $j-1$ be the index of the last (translated) SUBLEQ instruction, that is, the index of the next SUBLEQ instruction will be $j$. Then, for the $i$-th instruction in the Minsky machine language, we translate it into our language as follows:
- Case 1, The $i$-th instruction of the Minsky machine program is $\operatorname{add}(a)$. This is equivalent to $\operatorname{SUBLE}\left(a, c_{-1}, j+1\right)$, and hence the $j$ instruction in our program will simply be $\operatorname{SUBLEQ}\left(a, c_{-1}, j+1\right)$.
- Case 2, The $i$-th instruction in the Minsky machine program is $\operatorname{sub}(a, n)$. This would be equivalent to the sequence of the following 5 SUBLEQ instructions.

```

Algorithm 13 Translation for $\operatorname{sub}(a, n)$
Instr. $j \quad: \operatorname{SUBLEQ}(b, b, j+1)$
Instr. $j+1: \operatorname{SUBLEQ}(b, a, j+3)$
Instr. $j+2: \operatorname{SUBLE} Q\left(a, c_{+1}, j+5\right)$
Instr. $j+3: \operatorname{SUBLE} Q\left(a, c_{0}, n^{\prime}\right)$
Instr. $j+4: \operatorname{SUBLE} Q\left(a, c_{+1}, j+5\right)$

```

Here $n^{\prime}$ is the index of the translation of the $n$-th instruction of the Minsky machine program. This can be computed as a function of the number of add and sub instructions up to instruction $n$. The correctness of the above can be verified by considering the three cases: $\operatorname{mem}[a] \geq 1$, mem $[a] \leq-1$, and $\operatorname{mem}[a]=0$.

\section*{D Single Instruction Set}

Each instruction consists of the following tuple: $\left(\mathbf{p}_{a}, \mathbf{p}_{b}, \mathbf{p}_{c}, \mathbf{p}_{\text {flag }}, \mathbf{p}_{m}, \mathbf{p}_{p}\right)$, and does the following
1. $\operatorname{mem}[c]=f_{m}(\operatorname{mem}[a], \operatorname{mem}[b])$

2. if $m e m[f l a g]_{(0,0)} \leq 0$ goto instruction $p$

Here, locations $a, b$, and $c$ can contain either scalars, or $d$-dimensional vectors or $d \times d$ matrices, and mem[flag $]_{(0,0)}$ is the 1 -st entry of mem[flag] if it is a vector / matrix, else it is mem[flag] if a scalar.

This can be implemented using the following steps (each may use a separate layer of transformer):

At the beginning of each iteration, the scratchpad starts with storing the pointer to the next instruction $\mathbf{p}_{t}$.

1. Read the command $\left(\mathbf{p}_{a}, \mathbf{p}_{b}, \mathbf{p}_{c}, \mathbf{p}_{\text {flag }}, \mathbf{p}_{p}, \mathbf{p}_{m}\right)$ from the location to the scratchpad.

2. Copy the $d \times d$ data at locations $a, b$ to the scratchpad memory scratchMem (assume the data is $d \times d$ even if actually scalar or vector, the $f_{m}$ implementation will handle that)

3. Copy the data to the $i$-th function row block using the feed forward layer.

4. Once in the correct row block, $f_{m}$ (mem $[a]$, mem $\left.[b]\right)$ is computed

5. Feedforward layers copy back the data from $i$-th row block to the scratchpad memory scratchMem.

6. Write result from scratchpad memory to $\mathbf{p}_{c}$.

7. if mem $[f l a g]_{(0,0)} \leq 0$ store $\mathbf{p}_{p}$ in the scratchpad, else $\mathbf{p}_{t+1}$

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-57.jpg?height=493&width=1114&top_left_y=255&top_left_x=511)

Figure 7: The structure of input $\mathbf{X}$

The structure of the input $\mathbf{X}$ is shown in Fig. 7. It has $n$ columns and $O(M d+\log n)$ rows. It is partitioned into 3 column blocks: the Scratchpad block, the Memory block, and the Instructions block. The Memory block is the storage and is the location where all the variables are stored. Each variable can be either a scalar, vector or matrix, as long as the number of rows in it are no larger than $d$. For example, if a variable is a $d \times d$ matrix, it is stored in $d$ consecutive columns in the block, where each column has length $d$. The address of this variable is the index of its first column in the input $\mathbf{X}$. The Instructions block contains instructions, where each instruction is a vector of the form

$$
\boldsymbol{c}=\left[\begin{array}{c}
\mathbf{p}_{a} \\
\mathbf{p}_{b} \\
\mathbf{p}_{c} \\
\mathbf{p}_{m} \\
\mathbf{p}_{\text {flag }} \\
\mathbf{p}_{p} \\
d_{h} \\
d_{w} \\
b_{\text {mask }}^{(1)} \\
b_{\text {mask }}^{(2)} \\
b_{\text {mask }}^{(3)}
\end{array}\right]
$$

which encodes the following logic:

$$
\operatorname{mem}[c]=f_{m}(\operatorname{mem}[a], \operatorname{mem}[b]) \quad ; \quad \text { if mem }[\text { flag }] \leq 0 \text { goto instruction } p
$$

$\mathbf{p}_{a}, \mathbf{p}_{b}, \mathbf{p}_{c}, \mathbf{p}_{p}$, and $\mathbf{p}_{\text {flag }}$ are all binary $\pm 1$ vectors that point to the locations $a, b, c, p$, and flag respectively. These are simply the binary representations of the integers $a, b, c, p$ and flag, and hence have length $\log _{2} n$ each. Similarly, $\mathbf{p}_{m}$ is the binary vector representation of the integer $m$, and hence has length $\log _{2} M$, where $M$ is the number of functions we implement. The $b_{\text {mask }}$ is mask bit used while writing the output back to memory.

The scratchpad has $s$ columns. The length $s$ depends on the maximum number of columns needed by the function blocks to operate, and can be as low as $O(1)$ for scalar and vector functions, $O(d)$ for matrix functions, and can be as high as $O\left(d^{2}\right)$ if functions like matrix vectorization are one of the $M$ functions. The Scratchpad consists of the following parts:

- The program counter is a row block with $\log _{2} n$ rows and $s$ columns and takes the form:

$$
\left[\begin{array}{llll}
\mathbf{p}_{i} & \mathbf{p}_{i} & \cdots & \mathbf{p}_{i}
\end{array}\right]
$$

This signifies that the current program counter points to the $i$-th instruction. Using this, the $i$-th instruction is read into all the $s$ columns of 'Current Instruction' row block.

- The Current Instruction row block has $O(\log n)$ rows and $s$ columns, and each column initially contains the $i$-th instruction once it is read. Then, the instructions in each column are slightly modified depending on the column index, to read memory blocks pointed to in the instruction. The memory blocks are read into the 'Scratchpad Memory'.

- The Scratchpad Memory is a temporary location where the data is first read into from the Memory column block, before it is moved to the correct function's Function Block, using the function index encoding $\mathbf{p}_{m}$ in the instruction.

- The encodings row block has $O(\log n)$ rows and $n$ columns, and is used to index every column in the input $\mathbf{X}$. It contains the binary $\pm 1$ vector encodings of the column index for each column. The details of this row block are explained later.

- The Function Blocks are custom transformer blocks that can be added in a plug-n-play manner to the Unified Attention Based Computer depending on what 'elementary' functions the user wants the computer to have access to.

$$
\mathbf{X}=\left[\begin{array}{cccc|ccc|ccc}
\mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{z}_{s+1} & \ldots & \mathbf{z}_{m+s} & {\left[\begin{array}{c}
\boldsymbol{c}_{m+s+1} \\
\mathbf{0}
\end{array}\right]} & \ldots & {\left[\begin{array}{c}
\boldsymbol{c}_{n} \\
\mathbf{0}
\end{array}\right]} \\
\hline \mathbf{p}_{t} & \mathbf{p}_{t} & \ldots & \mathbf{p}_{t} & * & \ldots & * & * & \ldots & * \\
\hline \boldsymbol{c}_{t}^{1} & \boldsymbol{c}_{t}^{2} & \ldots & \boldsymbol{c}_{t}^{s} & * & \ldots & * & * & \ldots & * \\
\hline \mathbf{z}_{a_{t}}^{1} & \mathbf{z}_{a_{t}}^{2} & \ldots & \mathbf{z}_{a_{t}}^{s} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{z}_{b_{t}}^{t} & \mathbf{z}_{b_{t}} & \ldots & \mathbf{z}_{b_{t}}^{s} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{z}_{c_{t}}^{t} & \mathbf{z}_{c_{t}} & \ldots & \mathbf{z}_{c_{t}} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\hline \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{p}_{s+1} & \ldots & \mathbf{p}_{m+s} & \mathbf{p}_{m+s+1} & \ldots & \mathbf{p}_{n} \\
\mathbf{p}_{1} & \mathbf{p}_{2} & \ldots & \mathbf{p}_{s} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\hline \hline \mathrm{f}_{1} \text { mem } & \ldots & \ldots & \ldots & * & \ldots & * & \ldots & * \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ldots & * & \\
\mathrm{f}_{M} \text { mem } & \ldots & \ldots & \ldots & * & \ldots & * & \ldots & * &
\end{array}\right]
$$

\section*{D. 1 Step 1}

In this step, we need to copy the $t$-th instruction, pointed to by the program counter $\mathbf{p}_{t}$, to the scratchpad's Current Instruction block. We denote the instruction by $\boldsymbol{c}_{t}$ where

$$
\boldsymbol{c}_{t}=\left[\begin{array}{c}
\mathbf{p}_{a_{t}} \\
\mathbf{p}_{b_{t}} \\
\mathbf{p}_{c_{t}} \\
\mathbf{p}_{\text {flag }_{t}} \\
\mathbf{p}_{p_{t}} \\
\mathbf{p}_{m_{t}} \\
d_{h} \\
d_{w} \\
b_{\text {mask }}^{(1)} \\
b_{\text {mask }}^{(2)} \\
b_{\text {mask }}^{(3)}
\end{array}\right]
$$

For this step, we only consider the following relevant subset of rows of the matrix $\mathbf{X}$ :

$$
\mathbf{X}=\left[\begin{array}{cccc|ccc|ccc}
\mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & * & * & \ldots & \boldsymbol{c}_{m+s+1} & \ldots & \boldsymbol{c}_{n} \\
\hline \mathbf{p}_{t} & \mathbf{p}_{t} & \ldots & \mathbf{p}_{t} & * & \ldots & * & * & \ldots & * \\
\hline \boldsymbol{c}_{t}^{1} & \boldsymbol{c}_{t}^{2} & \ldots & \boldsymbol{c}_{t}^{s} & * & \ldots & * & * & \ldots & * \\
\hline \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{p}_{s+1} & \ldots & \mathbf{p}_{m+s} & \mathbf{p}_{m+s+1} & \ldots & \mathbf{p}_{n}
\end{array}\right]
$$

The other rows will not be used or changed during this operation because we can simply set the corresponding rows of the $\mathbf{K}, \mathbf{V}, \mathbf{Q}$ matrices to 0 for all heads and setting the feed forward layers to also pass the corresponding rows unchanged.

At the beginning of execution of each command, the Current Instruction row block would be empty, so the input would look like

$$
\mathbf{X}=\left[\begin{array}{cccc|ccc|ccc}
* & * & \ldots & * & * & * & \ldots & \boldsymbol{c}_{m+s+1} & \ldots & \boldsymbol{c}_{n} \\
\hline \mathbf{p}_{t} & \mathbf{p}_{t} & \ldots & \mathbf{p}_{t} & * & \ldots & * & * & \ldots & * \\
\hline \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & * \\
\hline \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{p}_{s+1} & \ldots & \mathbf{p}_{m+s} & \mathbf{p}_{m+s+1} & \ldots & \mathbf{p}_{n}
\end{array}\right]
$$

Then, consider an attention head with the following $\mathbf{K}, \mathbf{Q}, \mathbf{V}$ matrices:

$$
\mathbf{K}=\left[\begin{array}{llll}
0 & 0 & 0 & \mathrm{I}
\end{array}\right], \mathrm{Q}=\left[\begin{array}{llll}
0 & \mathrm{I} & 0 & 0
\end{array}\right], \mathrm{V}=\left[\begin{array}{llll}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\mathrm{I} & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

This will result in

$$
\mathbf{X}=\left[\begin{array}{cccc|ccc|ccc}
* & * & \ldots & * & * & * & \ldots & \boldsymbol{c}_{m+s+1} & \ldots & \boldsymbol{c}_{n} \\
\hline \mathbf{p}_{t} & \mathbf{p}_{t} & \ldots & \mathbf{p}_{t} & * & \ldots & * & * & \ldots & * \\
\hline \boldsymbol{c}_{t} & \boldsymbol{c}_{t} & \ldots & \boldsymbol{c}_{t} & * & \ldots & * & * & \ldots & * \\
\hline \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{p}_{s+1} & \ldots & \mathbf{p}_{m+s} & \mathbf{p}_{m+s+1} & \ldots & \mathbf{p}_{n}
\end{array}\right]
$$

We apply Lemma 16 on the row blocks

$$
\left[\begin{array}{cccc|ccc|ccc}
\boldsymbol{c}_{t} & \boldsymbol{c}_{t} & \ldots & \boldsymbol{c}_{t} & * & \ldots & * & * & \ldots & * \\
\mathbf{p}_{1} & \mathbf{p}_{2} & \ldots & \mathbf{p}_{s} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0}
\end{array}\right]
$$

to construct feedforward layers that convert $c_{t}$ to $c_{t}^{i}$, where

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-60.jpg?height=603&width=698&top_left_y=517&top_left_x=700)

Note that the last three elements can be created using the following ReLU:

$b_{\text {mask }}^{(1)}=\operatorname{ReLU}\left(2 d+d_{w}-i+1\right)-\operatorname{ReLU}\left(2 d+d_{w}-i\right)$

$b_{\text {mask }}^{(2)}=\operatorname{ReLU}(i-d)-\operatorname{ReLU}(i-d-1)+\operatorname{ReLU}\left(d+d_{\mathrm{w}}-i+1\right)-\operatorname{ReLU}\left(d+d_{w}-i\right)-1$

$b_{\text {mask }}^{(3)}=\operatorname{ReLU}(i-2 d)-\operatorname{ReLU}(i-2 d-1)+\operatorname{ReLU}\left(2 d+d_{w}-i+1\right)-\operatorname{ReLU}\left(2 d+d_{\mathrm{w}}-i\right)-1$.

At the end of this step, we get the following:

$$
\mathbf{X}=\left[\begin{array}{cccc|ccc|ccc}
\mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & * & * & \ldots & \boldsymbol{c}_{m+s+1} & \ldots & \boldsymbol{c}_{n} \\
\hline \mathbf{p}_{t} & \mathbf{p}_{t} & \ldots & \mathbf{p}_{t} & * & \ldots & * & * & \ldots & * \\
\hline \boldsymbol{c}_{t}^{0} & \boldsymbol{c}_{t}^{1} & \ldots & \boldsymbol{c}_{t}^{s} & * & \ldots & * & * & \ldots & * \\
\hline \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{p}_{s+1} & \ldots & \mathbf{p}_{m+s} & \mathbf{p}_{m+s+1} & \ldots & \mathbf{p}_{n}
\end{array}\right]
$$

\section*{D. 2 Step 2}

Use three heads, one each for $\mathbf{p}_{a}, \mathbf{p}_{b}$ and $\mathbf{p}_{c}$.

Using the vectors $\mathbf{p}_{a_{t}+i}, \mathbf{p}_{b_{t}+i-d}$, and $\mathbf{p}_{c_{t}+i-2 d}$ we copy the data (using one head each and $\mathrm{a}$ similar technique as last step) to get the following in the Scratchpad Memory:

$$
\left[\begin{array}{ccccccccc|ccc|ccc}
\mathbf{z}_{a_{t}} & \ldots & \mathbf{z}_{a_{t}+d} & * & \ldots & * & * & \ldots & * & * & \ldots & * & * & \ldots & * \\
* & \ldots & * & \mathbf{z}_{b_{t}} & \ldots & \mathbf{z}_{b_{t}+d} & * & \ldots & * & * & \ldots & * & * & \ldots & * \\
* & \ldots & * & * & \ldots & * & \mathbf{z}_{c_{t}} & \ldots & \mathbf{z}_{c_{t}+s-2 d} & * & \ldots & * & * & \ldots & *
\end{array}\right]
$$

Using the mask bits at the end of $\boldsymbol{c}_{t}^{i}$, we get

$$
\left[\begin{array}{ccccccccccccc|cc|cc}
\mathbf{z}_{a_{t}} & \ldots & \mathbf{z}_{a_{t}+d_{\mathbf{w}}-1} & \mathbf{0} & \mathbf{z}_{b_{t}} & \ldots & \mathbf{z}_{b_{t}+d_{\mathbf{w}}-1} & \mathbf{0} & \mathbf{z}_{c_{t}} & \ldots & \mathbf{z}_{c_{t}+d_{\mathbf{w}}-1} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \ldots  \tag{27}\\
\mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \ldots \\
\mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \ldots
\end{array}\right]
$$

$$
\begin{aligned}
\mathbf{z}_{i}[1: d]= & \operatorname{ReLU}\left(\mathbf{z}_{i}[1: d]-C\left(1-b_{\text {mask }}^{(1)}\right) \mathbf{1}\right)-\operatorname{ReLU}\left(-\mathbf{z}_{i}[1: d]-C\left(1-b_{\text {mask }}^{(1)}\right) \mathbf{1}\right) \\
& +\operatorname{ReLU}\left(\mathbf{z}_{i}[d+1: 2 d]-C\left(1-b_{\text {mask }}^{(2)} \mathbf{1}\right)-\operatorname{ReLU}\left(-\mathbf{z}_{i}[d+1: 2 d]-C\left(1-b_{\text {mask }}^{(1)}\right) \mathbf{1}\right)\right. \\
& +\operatorname{ReLU}\left(\mathbf{z}_{i}[2 d+1: 3 d]-C\left(1-b_{\text {mask }}^{(1)}\right) \mathbf{1}\right)-\operatorname{ReLU}\left(-\mathbf{z}_{i}[2 d+1: 3 d]-C\left(1-b_{\text {mask }}^{(1)}\right) \mathbf{1}\right)
\end{aligned}
$$

$\mathbf{z}_{i}[d+1: 3 d]=\mathbf{0}$,

where $C$ is a large positive constant.

Using the same mask bits, we also mask the row containing the output data pointers for $c$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-61.jpg?height=96&width=1145&top_left_y=819&top_left_x=490)

\section*{D. 3 Step 3}

The following feedforward ReLU layer can move the data to the correct function blocks:

$$
\begin{aligned}
& \mathrm{f}_{k} \operatorname{mem}\left[1: d_{h}\right]=\left(\operatorname{ReLU}\left(\mathbf{z}\left[1: d_{h}\right]-C\left(\left(1-b_{\text {mask }}^{(1)}-b_{\text {mask }}^{(2)}\right) \mathbf{1}+\log M-\mathbf{p}_{k}^{\top} \mathbf{p}_{m}\right)\right)\right. \\
&\left.\quad-\operatorname{ReLU}\left(-\mathbf{z}\left[1: d_{h}\right]-C\left(\left(1-b_{\text {mask }}^{(1)}-b_{\text {mask }}^{(2)}\right) \mathbf{1}+\log M-\mathbf{p}_{k}^{\top} \mathbf{p}_{m}\right)\right)\right)
\end{aligned}
$$

where $C$ is a large positive constant.

\section*{D. 4 Step 4}

Each of the $M$ functions have their own attention heads, which are constructed to be copies of their transformer based function blocks. The results after the attention are written back into their respective row blocks. Since the row blocks are separate, the feedforward layers of each of the transformer based function blocks also work in parallel to store the final results in the respective row blocks.

\section*{D. 5 Step 5}

Similar to Step 3 we use the following feedforward ReLU layer to move the data from the function block back into the scratchpad memory

$$
\begin{aligned}
\mathbf{z}\left[1: d_{h}\right]=\mathbf{z} & \left.1: d_{h}\right]+\sum_{k=1}^{M}\left(\operatorname{ReLU}\left(\left(\mathrm{f}_{k} \operatorname{mem}\left[1: d_{h}\right]-\mathbf{z}\left[1: d_{h}\right]\right)-C\left(\left(1-b_{\text {mask }}^{(3)}\right) \mathbf{1}+\log M-\mathbf{p}_{k}^{\top} \mathbf{p}_{m}\right)\right)\right. \\
& \left.-\operatorname{ReLU}\left(-\left(\mathrm{f}_{k} \operatorname{mem}\left[1: d_{h}\right]-\mathbf{z}\left[1: d_{h}\right]\right)-C\left(\left(1-b_{\text {mask }}^{(3)}\right) \mathbf{1}+\log M-\mathbf{p}_{k}^{\top} \mathbf{p}_{m}\right)\right)\right)
\end{aligned}
$$

where $C$ is a large positive constant.

\section*{D. 6 Step 6}

For this step we focus on the encoding row block, memory storage row block and the following rows in the input (see (28), (27)):

![](https://cdn.mathpix.com/cropped/2024_06_04_151b53b506629767a45eg-62.jpg?height=407&width=1477&top_left_y=455&top_left_x=324)

We set the Key and Query weight matrices as follows:

$$
\begin{aligned}
& \mathrm{K}=\mathrm{Q}=\left[\begin{array}{l}
0 \\
0 \\
\mathrm{I} \\
\mathrm{I}
\end{array}\right] . \\
& V=\left[\begin{array}{llll}
0 & 0 & 0 & 0 \\
\mathrm{I} & \mathrm{I} & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
\end{aligned}
$$

$\mathbf{V X} \sigma_{\mathbf{S}}\left((\mathbf{K X})^{\top} \mathbf{Q} \mathbf{X}\right)$

$=\left[\begin{array}{ccccc|cccccccc|c}\ldots & \mathbf{0} & \ldots & \mathbf{0} & \ldots & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \ldots \\ \hline \ldots & \frac{d_{c_{t}}^{\mathrm{new}}+\boldsymbol{d}_{c_{t}}}{2} & \ldots & \frac{d_{c_{t}+d_{w}}^{\mathrm{new}}+d_{c_{t}+d_{w}}}{2} & \ldots & \boldsymbol{d}_{0} & \ldots & \boldsymbol{d}_{c_{t}-1} & \frac{d_{c_{t}}^{\mathrm{new}}+d_{c_{t}}}{2} & \ldots & \frac{d_{c_{t}+d_{w}}^{\mathrm{new}}+d_{c_{t}+d_{w}}}{2} & \boldsymbol{d}_{c_{t}+d_{w}+1} & \ldots & \ldots \\ \hline \ldots & \mathbf{0} & \ldots & \mathbf{0} & \ldots & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \ldots \\ \hline \ldots & \mathbf{0} & \ldots & \mathbf{0} & \ldots & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} & \mathbf{0} & \ldots & \ldots\end{array}\right]$

Finally, we use the feedforward layers similar to the proof of Lemma 3 to write back $\left[\boldsymbol{d}_{c_{t}}^{\mathrm{new}} \ldots \boldsymbol{d}_{c_{t}+d_{w}}^{\mathrm{new}}\right]$ to the correct rows.

\section*{D. 7 Step 7}

This step is identical to Section 4.3.

\section*{E Calculator}

Lemma 21. Given two constants $\epsilon, \delta \in[0,1]$, there exists a 1 hidden layer neural network $f$ with threshold activation and d activations in the hidden layer, such that

$$
\forall x \in[-C,-\delta] \cup[\delta, C],\left|f(x)-\frac{1}{x}\right| \leq \epsilon
$$

as long as $d=\Omega\left(\frac{\log (1 /(\epsilon \delta))}{\epsilon \delta}+\log C\right)$.

Proof. We partition $[\delta, C]$ into the following intervals

$$
[\delta, \delta(1+\epsilon \delta)),[\delta(1+\epsilon \delta), \delta(1+\epsilon \delta)(1+\epsilon \delta(1+\epsilon \delta))) \ldots,\left[a_{i}, a_{i}\left(1+\epsilon a_{i}\right)\right), \ldots
$$

that is, if an interval begins at $a$, then it ends at $a(1+\epsilon a)$. Note that for any point $x \in\left[a_{i}, a_{i}\left(1+\epsilon a_{i}\right)\right)$

$$
\begin{aligned}
\left|\frac{1}{x}-\frac{1}{a_{i}}\right| & =\frac{1}{a_{i}}-\frac{1}{x} \\
& <\frac{1}{a_{i}}-\frac{1}{a_{i}\left(1+\epsilon a_{i}\right)} \\
& =\frac{\epsilon}{1+\epsilon a_{i}}<\epsilon .
\end{aligned}
$$

Hence two output activations of the form $\frac{1}{a_{i}} 1_{x \geq a_{i}}-\frac{1}{a_{i}} 1_{x<a_{i}\left(1+\epsilon a_{i}\right)}$ can be used to approximate $\frac{1}{x}$ in $\left[a_{i}, a_{i}\left(1+\epsilon a_{i}\right)\right)$.

Thus, all that remains is to compute the number of such intervals, and using that we get the number of output activations in the hidden layer. Towards that end, if the $i$-th interval begins at $a_{i}$,

$$
a_{i}=a_{i-1}\left(1+\epsilon a_{i-1}\right) \geq a_{i-1}(1+\epsilon \delta)=\delta(1+\epsilon \delta)^{i-2}
$$

Hence,

$$
\forall i \geq 2+\frac{\log 1 /(\epsilon \delta)}{\log (1+\epsilon \delta)}, a_{i} \geq \frac{1}{\epsilon}
$$

Noting that $\log (1+\epsilon \delta)>\frac{\epsilon \delta}{2}$ for $\epsilon, \delta \in[0,1]$, we get that

$$
\forall i \geq 2+\frac{2 \log 1 /(\epsilon \delta)}{\epsilon \delta}, a_{i} \geq 1
$$

Once we have that $a_{i} \geq \frac{1}{\epsilon}$, the number of further partitions needed to reach $C$ would be $O(\log C)$ as shown below:

$$
a_{j}=a_{j-1}\left(1+\epsilon a_{j-1}\right) \geq a_{j-1}\left(1+\epsilon \frac{1}{\epsilon}\right)=2 a_{j-1}
$$

Hence, the total number of partitions needed is $O\left(\frac{\log (1 /(\epsilon \delta))}{\epsilon \delta}+\log C\right)$.

We can similarly approximate $1 / x$ on $[-C,-\delta]$ with the same number of output activations.

Lemma 22. Given $\epsilon \in[0,1]$, there exists a 1 hidden layer neural network $f$ with threshold activation and d activations in the hidden layer, such that

$$
\forall x \in[0, C],|f(x)-\sqrt{x}| \leq \epsilon
$$

as long as $d=\Omega\left(\frac{\sqrt{C}}{\epsilon}\right)$.

Proof. We partition $[0, C]$ into the following intervals

$$
\left.\left[0, \epsilon^{2}\right)\right),\left[\epsilon^{2}, 4 \epsilon^{2}\right) \ldots,\left[i^{2} \epsilon^{2},(i+1)^{2} \epsilon^{2}\right), \ldots
$$

Note that for any point $x \in\left[i^{2} \epsilon^{2},(i+1)^{2} \epsilon^{2}\right)$

$$
\left|\sqrt{x}-\sqrt{i^{2} \epsilon^{2}}\right|<\sqrt{(i+1)^{2} \epsilon^{2}}-\sqrt{i^{2} \epsilon^{2}}=\epsilon
$$

Hence two output activations of the form $i \epsilon 1_{x \geq i^{2} \epsilon^{2}}-i \epsilon 1_{x<(i+1)^{2} \epsilon^{2}}$ can be used to approximate $\sqrt{x}$ in $\left[i^{2} \epsilon^{2},(i+1)^{2} \epsilon^{2}\right)$.

Thus, all that remains is to compute the number of such intervals, and using that we get the number of output activations in the hidden layer. It is easy to see that the total number of intervals needed would be $\frac{\sqrt{C}}{\epsilon}$.```


[^0]:    *Equal contribution. The title of this paper was not created by a transformer, but we can't guarantee the same for this footnote.

