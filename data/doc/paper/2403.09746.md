# PICNIQ: Pairwise Comparisons for Natural Image Quality Assessment 

Nicolas Chahine ${ }^{1,2}$, Sira Ferradans ${ }^{1}$, and Jean Ponce ${ }^{2,3}$<br>${ }^{1}$ DXOMARK, Paris<br>\{nchahine, sferradans\}@dxomark.com<br>${ }^{2}$ Departement d'informatique de l'Ecole normale suprerieure (ENS-PSL, CNRS,<br>Inria<br>\{nicolas.chahine, jean.ponce\}@inria.fr<br>${ }^{3}$ Institute of Mathematical Sciences and Center for Data Science, New York<br>University<br>jean.ponce@ens.fr


#### Abstract

Blind image quality assessment (BIQA) approaches, while promising for automating image quality evaluation, often fall short in real-world scenarios due to their reliance on a generic quality standard applied uniformly across diverse images. This one-size-fits-all approach overlooks the crucial perceptual relationship between image content and quality, leading to a 'domain shift' challenge where a single quality metric inadequately represents various content types. Furthermore, BIQA techniques typically overlook the inherent differences in the human visual system among different observers. In response to these challenges, this paper introduces PICNIQ, an innovative pairwise comparison framework designed to bypass the limitations of conventional BIQA by emphasizing relative, rather than absolute, quality assessment. PICNIQ is specifically designed to assess the quality differences between image pairs. The proposed framework implements a carefully crafted deep learning architecture, a specialized loss function, and a training strategy optimized for sparse comparison settings. By employing psychometric scaling algorithms like TrueSkill, PICNIQ transforms pairwise comparisons into just-objectionable-difference (JOD) quality scores, offering a granular and interpretable measure of image quality. We conduct our research using comparison matrices from the PIQ23 dataset, which are published in this paper. Our extensive experimental analysis showcases PICNIQ's broad applicability and superior performance over existing models, highlighting its potential to set new standards in the field of BIQA.


Keywords: Image quality assessment $\cdot$ Pairwise comparisons $\cdot$ Portrait quality assessment $\cdot$ Deep learning

## 1 Introduction

Smartphones are closing the gaps with professional photography by integrating modern image enhancement technology through sophisticated non-linear
processes [26, 39, 73]. Consequently, smartphone cameras produce images with new realistic distortions that are difficult to model, unlike classical linear imaging systems. This makes traditional image quality assessment (IQA) methods $[3,4,16,29]$, that model digital cameras as linear systems, unreliable for camera tuning [11]. Therefore, in addition to objective IQA measurements, smartphone manufacturers conduct rigorous tuning procedures to optimize the quality of their camera systems [9]. The evaluation process, usually referred to as perceptual quality assessment, consists of shooting and evaluating thousands of use cases, which can be costly, time-consuming, and challenging to reproduce.

As an alternative to perceptual evaluation, blind IQA (BIQA) methods try to mimic human perception $[14,36-38,45,58,61,66]$, promising to deliver universal image quality metrics. Learning-based BIQA methods [59], in particular, have shown marked improvements over their classical counterparts, due to their ability to extract perceptual image quality information. However, existing BIQA solutions often use a one-size-fits-all approach for quality by applying the same concepts to different content, without considering the impact of scene-specific semantics on the quality $[5,49,70]$. Given that image quality varies under different conditions, numerous studies have underscored the significance of integrating semantic information into IQA, tackling a challenge known as domain shift $[5,6,11,12,20,49,50,65,69,70]$. Moreover, some approaches attend to model the uncertainty of image quality predictions $[27,31,32,51,55,70]$, due to the intrinsic inconsistency of the human's perception of quality.

An extra challenge comes from the available IQA datasets. Unlike classical datasets $[25,42,43,47,67]$ that only provide synthetic distortions, not suitable for modern digital camera evaluation, some recent datasets $[11,13,19,53,62,72]$ present authentic in-the-wild distortions, originating from online media sources and smartphone cameras. Most of these datasets are annotated using ratingbased procedures, such as mean opinion scores (MOS), which are commonly known to be noisy and non-deterministic [33]. There is thus a clear need for a quantitative and formal framework to evaluate and compare perceptual judgments objectively. A recent dataset annotated by image quality experts, PIQ23 [5], promises high precision by adopting content-dependent pairwise comparisons to annotate image quality, a method known for enhancing consistency in IQA experiments $[33,41]$.

In tackling the limitations of current BIQA methodologies, such as combining IQA knowledge, limitations on generalization, domain shift, and uncertainty, we present PICNIQ, a BIQA framework purely based on pairwise comparisons. Specifically, instead of direct quality score predictions, our model adopts a pairwise comparison approach, simplifying the task to probabilistic ranking while retaining the ability to interpret quality. Through psychometric scaling algorithms applied to the pairwise comparison graph, we transform the rankings into a granular quality scale, facilitating a robust and interpretative assessment of image quality. Our findings highlight the potential of this approach in addressing the identified challenges in IQA, contributing towards the development
of more reliable and generalized image quality evaluation models. We summarize our contributions as follows:

- PICNIQ, a BIQA model that uses a pairwise comparison mechanism. This approach calculates a quality difference vector between image pairs and utilizes a hub layer [34] for probabilistic ranking while ensuring symmetry in comparisons.
- A pairwise comparison framework that integrates a training strategy, a comparison loss, and a unique inference approach. We demonstrate that using a weighted binary cross-entropy loss is necessary to train in sparse comparison settings. Using psychometric scaling and active sampling algorithms, we define a pipeline to derive quality scores from pairwise rankings.
- Exploring and exploiting the sparse pairwise comparison matrices in the PIQ23 dataset, featuring annotations by image quality experts on a per-scene and per-attribute basis. This contribution exploits the granular insights of quality in the PIQ23 dataset, compared to the previous single image quality scores.
- A comprehensive evaluation of PICNIQ, illustrating its outperformance over existing benchmarks on the PIQ23 dataset. This analysis underscores PICNIQ's capability to generalize over new unseen conditions while overcoming previous challenges in BIQA.


## 2 Related Work

IQA methods fall into two main categories. Full-reference IQA (FR-IQA) techniques rely on evaluating images based on a pristine reference image, while noreference (NR-IQA) methods try to capture image quality without any reference. Blind image quality assessment (BIQA) is a part of NR-IQA where neither the reference image nor the distortion space is known. This section reviews both the key datasets and algorithms of BIQA. We also review approaches that aim to quantify the uncertainty in image quality as well as solve the domain shift problem.

### 2.1 BIQA datasets

Traditional datasets such as LIVE [47], CSIQ [25], TID [42, 43], and BAPPS [67] provide reference images with synthetic distortions. Recent datasets like CLIVE [13], KonIQ10k [19], and PaQ-2-PiQ [62] include real-world images, better reflecting realistic image distortions. Other datasets, such as CID2013 [53], SCPQD2020 [72] and SPAQ [11] expand the spectrum of realistic IQA datasets by annotating smartphone images in controlled settings. Despite their contributions, their reliance on MOS often overlooks the content's impact on quality perception and introduces inconsistencies. Chahine et al.'s PIQ23 [5], focusing on portrait quality assessment with pairwise comparison annotations, addresses these limitations. Its structured approach and precise annotations make it central to our study, providing valuable comparison matrices for further research.

### 2.2 BIQA methods

BIQA algorithms can be grouped into regression-based and ranking-based methods. While both aim to predict quality scores, ranking-based approaches also consider the model's ability to accurately rank images before assigning scores.

Regression-based BIQA Regression-based methods have evolved from classical techniques using hand-crafted features $[14,36,38,45,61]$ to deep learning models employing CNNs [2,21,24] and transformers [15,22,44,60,63]. Self-supervised learning methods based on contrastive learning $[1,46,71]$, have recently been explored for training universal BIQA models.

Ranking-based BIQA Ranking-based methods treat BIQA as a learning-torank problem, utilizing distortion levels, FR-IQA models, and human judgments for relative rankings. Early approaches like RankIQA [28] and DBCNN [68] employ siamese networks pre-trained on synthetically distorted images. Ma et al.'s dipIQ [31] represents an early self-supervised strategy, training on qualitydiscriminable image pairs (DIPs) gathered via multiple FR-IQA measures. The same authors have also trained another probabilistic CNN model [32] on DIPs by treating each FR-IQA model as a separate annotator. Shi et al. [48] combine objective IQA features for binary classification-based preference prediction. $\mathrm{Xu}$ et al. [56] introduce a perception-based method for generating pairwise rankings from synthetic distortions, employing an eigenvalue decomposition (EVD) for unsupervised quality score inference. UNIQUE [70] takes an uncertainty-aware cross-domain approach to BIQA, utilizing a Gaussian prior for quality distribution and training on image pairs from multiple datasets. We aim to extend this approach to train on sparse pairwise comparison data using a model-free uncertainty approach.

### 2.3 Uncertainty-aware BIQA

Acknowledging the inherent noise and uncertainty in human-based image quality judgments, several studies have sought to integrate these aspects into BIQA metrics. Kendall et al. [23] and Duanmu et al. [8] stress the importance of incorporating probabilistic models to enhance decision-making under uncertainty in computer vision and IQA. Techniques like PQR [64], NIMA [51] and Liu et al.'s work [27] employ a label distribution learning approach. Wu et al. [55] proposed the LOCRUE, which combines natural scene statistics (NSS) with a Gaussian process to predict image quality. Besides, multiple ranking-based methods, previously mentioned in this paper [31,32,70], incorporate a probabilistic learning approach, based on the Gaussian distribution assumptions of the Thurstone case V observer model $[7,52]$. These methods are limited due to their reliance on specific data structures, strong prior assumptions, or inability to account for cross-domain IQA. In this paper, we propose a method that accounts for all the previous problems using a model-free pairwise comparison framework.

![](https://cdn.mathpix.com/cropped/2024_06_04_16d2ab5d0a02704b9d0fg-05.jpg?height=591&width=1090&top_left_y=409&top_left_x=520)

Fig. 1: The PICNIQ architecture for image quality comparison. A siamese qualityaware backbone is used to process a pair of images and extract relevant image quality features. The difference between the extracted features, $V$, is computed and then fed into a fully connected (FC) layer within the hub layer. The hub layer processes both $V$ and its negation and outputs their difference to ensure probabilistic symmetry in the comparison. The predicted probability $\mathrm{P}(\mathrm{I}>\mathrm{J})$, is obtained by passing the output of the hub layer through a sigmoid function.

### 2.4 Domain shift challenge in BIQA

IQA datasets often feature diverse annotation strategies and image content, leading to relative and independent quality scales. This diversity presents a 'domain shift' challenge, complicating cross-content learning and generalization [50,65,69,70]. Multitask learning has been proposed as a solution [11,20,50,65], but these efforts often fail to explicitly separate semantic information from quality. Su et al.'s HyperIQA [49] offer a novel approach by using a self-adaptive hypernetwork to implicitly adjust quality predictions based on semantic information. However, HyperIQA does not explicitly highlight the semantic variation between different content, as the hypernetwork is expected to comprehend this information naturally. Chahine et al. [5,6] extend the HyperIQA architecture for better semantic understanding and generalization by forcing the model to predict the category of each image. Our work solves the problem of domain shift intrinsically, by employing a generic framework that can be easily extended to multiple sources.

## 3 PICNIQ

We introduce PICNIQ, a pairwise comparison framework designed to tackle the challenges of combining IQA knowledge, limitations on generalization, domain
shift, and uncertainty in BIQA. PICNIQ aims to standardize high-precision image quality metrics through the effective use of pairwise comparisons. We elaborate on the motivations behind this work in Sec. 3.1. In Sec. 3.2 we formulate the problem of training on image quality pairs. Then, in Sec. 3.3, we define the optimization problem and suggest an adapted loss for sparse comparison settings. PICNIQ's architecture is then described in Sec. 3.4. Additionally, we briefly outline the pairwise matrices collected from the PIQ23 dataset in Sec. 3.5. Finally, we propose a strategy to generate image quality scores from pairwise comparisons in Sec. 3.6. PICNIQ's architecture is presented in Figure 1.

### 3.1 Motivation

Blind image quality assessment (BIQA) confronts significant challenges that undermine its efficacy in digital camera evaluation. Xu et al. [56] categorize these challenges based on the content diversity, the stochastic nature of annotations, and the arbitrary sampling of image distortions. The limited availability of large, annotated IQA datasets, coupled with the subjective nature of crowdsourced evaluations, compromises the development of learning-based IQA methods. These challenges are compounded by the domain shift problem, where the integration of quality knowledge from multiple sources is challenged by inherent biases and inconsistencies within the human visual system.

Both FR-IQA and NR-IQA methods fail to address the demands for highprecision digital camera evaluation. While the first struggle with the absence of pristine reference images, as a result of the complex non-linearities in modern digital cameras, NR-IQA methods often fail to achieve the precision necessary to be considered reliable IQA metrics. On the one hand, the common reliance on mean opinion scores (MOS) for deriving quality metrics is flawed, due to the subjective and chaotic nature of these scores. This subjectivity, alongside factors like annotation guidelines, data collection procedures, visual conditions, and the diverse backgrounds of annotators, complicates the effective handling of domain shifts through direct score prediction methods [54,70]. On the other hand, pairwise comparison models, despite their potential, encounter their own set of obstacles. For instance, their Gaussian assumptions might not hold well in sparse comparison settings. Additionally, they rely on quality scores to derive the comparison probability, which inherits the limitations of MOS in domain shift. Adding to that the random pair selection process and the quality predictions can be significantly impacted. This happens because the image quality is derived from pairwise comparison graphs rather than single comparisons, and there is a limitation on the quality difference allowed to accurately derive comparison probabilities [40,41]. Consequently, most BIQA methods do not adequately tackle the challenges posed above, such as the limited size of IQA datasets, the pairwise comparison constraints, and the domain shifts related to BIQA.

To tackle these challenges, we introduce a pairwise comparison framework designed to capture relative quality differences among image pairs, thereby ensuring adaptability to diverse conditions. Our approach incorporates model-free
uncertainty through empirical Bayes principles and leverages pairwise comparison graphs to derive image quality scores. It bridges the gap between FR-IQA and NR-IQA methods, and promises a more precise and scalable solution for BIQA, addressing the challenges of domain shift and enhancing model generalizability. This strategy marks a significant departure from conventional methodologies and signals a more accurate and adaptable paradigm for BIQA.

### 3.2 Problem formulation

Given a set of images $\left\{I_{1}, I_{2}, \ldots, I_{n}\right\}$, we construct a zero-diagonal pairwise comparison matrix $C$, where each element $c_{i j}$ represents the empirical win count of image $I_{i}$ over $I_{j}$ according to image quality preference. We construct a model $M$, parameterized by $\theta$, which takes as input a pair of images $\left(I_{i}, I_{j}\right)$ and outputs the probability $M_{\theta}\left(I_{i}, I_{j}\right)$ of $I_{i}$ being of higher quality than $I_{j}$. We assume $M_{\theta}$ guarantees probabilistic symmetry, hence:

$$
\begin{equation*}
M_{\theta}\left(I_{j}, I_{i}\right)=1-M_{\theta}\left(I_{i}, I_{j}\right) \tag{1}
\end{equation*}
$$

In contrast to previous pairwise approaches in BIQA, we do not make any assumptions regarding the prior image quality distribution. Instead, we allow the posterior distributions to be predominantly informed by the available data. More precisely, previous methods such as UNIQUE [70] predict a quality score per image and then try to compute the probability based on a Gaussian prior (specifically, Thurstone case V) of the quality distribution. We argue that this is only true when we have a large number of annotators and comparisons in a controlled annotation environment. Moreover, computing probabilities from scores and vice versa can only work for small quality differences, since the normal cumulative distribution function tends towards infinity when the probability tends to 1 [41]. Finally, the quality of an image cannot be determined solely by comparing it to another image, but it should be evaluated in the context of a broader comparison graph of a larger image set [5]. Since we do not assume any prior on the quality distribution, we directly predict the probability that one image is better than the other, supposing the model will learn the intrinsic quality distributions from the pairwise training data. Our approach aligns with the empirical Bayes methodology, where priors are broad or estimated from the data, ensuring that the posterior inference is primarily driven by the empirical data available.

### 3.3 Loss function

Our pairwise training approach adopts a maximum likelihood estimation (MLE) framework, akin to the methodology proposed by Perez et al. (2019) [41]. MLE looks for the model parameters that maximize the probability of observing our dataset $\Omega$. We define the true probability of preferring image $I_{i}$ over $I_{j}$ as $P_{i j}$.

Given $c_{i j}$ occurrences of $I_{i}$ being preferred in $n_{i j}$ comparisons, Perez et al. propose that the likelihood follows a binomial distribution:

$$
\begin{equation*}
L\left(P_{i j} \mid c_{i j}, n_{i j}\right)=\binom{n_{i j}}{c_{i j}} P_{i j}^{c_{i j}}\left(1-P_{i j}\right)^{n_{i j}-c_{i j}} \tag{2}
\end{equation*}
$$

where $\binom{n_{i j}}{c_{i j}}$ is the binomial coefficient. Given that the binomial coefficient is a multiplicative factor, independent of the model parameters $\theta$, we omit this term in the loss function.

We define the model's estimated probability that $I_{i}$ is preferred over $I_{j}$ as $P^{\theta} i j=M_{\theta}\left(I_{i}, I_{j}\right)$, and our objective is to minimize the negative log-likelihood of observing the given preferences across all compared pairs in the dataset $\Omega$ :

$$
\begin{align*}
& \theta^{*} \longleftarrow \arg \min _{\theta}-\frac{1}{N} \log \left(\prod_{i, j \in \Omega} L\left(P_{i j}^{\theta} \mid c_{i j}, n_{i j}\right)\right)  \tag{3}\\
& \quad=-\frac{1}{N} \sum_{i=1}^{n} \sum_{j=1, j \neq i}^{n}\left[c_{i j} \log \left(M_{\theta}\left(I_{i}, I_{j}\right)\right)+\left(n_{i j}-c_{i j}\right) \log \left(1-M_{\theta}\left(I_{i}, I_{j}\right)\right)\right]
\end{align*}
$$

where $N$ is a normalization factor representing the total number of comparisons, ensuring the loss is averaged over all pairs. By defining the empirical probability of preference of $I_{i}$ over $I_{j}$ as $p_{i j}=\frac{c_{i j}}{n_{i j}}$, we can reformulate the empirical risk $l_{\theta}$ as follows:

$$
\begin{equation*}
l_{\theta}=-\frac{1}{N} \sum_{i=1}^{n} \sum_{j=1, j \neq i}^{n} n_{i j}\left[p_{i j} \log M_{\theta}\left(I_{i}, I_{j}\right)+\left(1-p_{i j}\right) \log \left(1-M_{\theta}\left(I_{i}, I_{j}\right)\right)\right] \tag{4}
\end{equation*}
$$

using a weighted binary cross-entropy (BCE) loss, where $n_{i j}$ acts as a weight accounting for the varied number of comparisons between different pairs. This is a critical distinction from the standard BCE loss, which assumes equal weights (i.e., $n_{i j}=$ const) for all comparison instances. The weighted BCE approach is essential for handling datasets in imbalanced or sparse comparison settings, which is not often considered in other BIQA methods.

### 3.4 Model specifications

We structure our model to capture differences in image quality between image pairs, simulating human perceptual comparisons. The proposed model is shown in Figure 1. First, given two input images $I$ and $J$, we extract their image quality features using a siamese quality-aware CNN backbone $B_{\theta}$. We then feed the difference feature vector $V=B_{\theta}(I)-B_{\theta}(J)$ to a fully connected layer $H_{\theta}$, followed by a sigmoid $\sigma$, which gives the likelihood that $I$ has a better quality than $J, M_{\theta}(I, J)=\sigma\left(H_{\theta}(V)\right)$. We ensure the probabilistic symmetry described
![](https://cdn.mathpix.com/cropped/2024_06_04_16d2ab5d0a02704b9d0fg-09.jpg?height=412&width=1204&top_left_y=390&top_left_x=466)

Fig. 2: Comparison matrices from the PIQ23 dataset, indexed by images with sorted JOD scores. These matrices demonstrate varying levels of sparsity and comparison counts, averaging one to three comparisons per pair, primarily among diagonally adjacent pairs. This pattern underscores the non-uniformity in pairwise comparison data, illustrating the complexities a pairwise comparison model must address in training.

in Eq. (1), using a hub layer approach inspired by Mattheakis et al. [34]. The hub layer $H_{\theta}(V)=\frac{1}{2}\left(F_{\theta}(V)-F_{\theta}(-V)\right)$, where $F_{\theta}$ is a fully connected layer, leverages odd properties which when combined with the sigmoid's property $\sigma\left(H_{\theta}(V)\right)+\sigma\left(-H_{\theta}(V)\right)=1$, achieves $M_{\theta}(I, J)=1-M_{\theta}(J, I)$.

In this work, we employ DBCNN's [68] deep dual bilinear architecture as a quality-aware backbone, combined with a simple fully connected hub layer. Since our experiments are limited to PIQ23, we do not explore deeper architectures. However, the PICNIQ framework can integrate any quality-aware backbone. As shown by our experiments in Sec. 4.5, with a simple VGG-16 backbone, PICNIQ achieves state-of-the-art generalization performance on PIQ23.

### 3.5 Pairwise comparison data

As far as we know, the only available annotated BIQA dataset with pairwise comparisons is PIQ23 [5], which is composed of 50 different scenarios, each annotated separately. The quality scores are generated from sparse comparison matrices using a psychometric scaling algorithm such as TrueSkill [18]. The public version of PIQ23 does not include the pairwise comparison matrices, but the authors kindly agreed to give us access to these matrices and conduct our research. The matrices will be made public on the PIQ23's GitHub page: https://github.com/DXOMARK-Research/PIQ2023. The annotations have been conducted by adopting the approach proposed by Mikhailiuk et al. [35], who propose an efficient active pair selection technique combined with TrueSkill to minimize the experiment cost. The collected matrices are typically very sparse (so-called incomplete design) with an emphasis on diagonally neighboring pairs. Figure 2 illustrates some examples from the dataset, showcasing the sparsity and significant difference in the comparison distributions between different scenes. More analysis of the pairwise data is shown in the supplementary material.

### 3.6 Generating quality scores from pairwise comparisons

A pairwise comparison model can either be used in its simplest form, as a comparison tool or to produce a precise image quality scale for large image sets. The process of deriving quality scores from pairwise comparisons begins by using PICNIQ to generate preference probabilities for a group of image pairs. We then fill the comparison matrix, $M$, by multiplying a predefined average number of comparisons, $c$, by the predicted probability of preference for each pair $p_{i j}$, such as $M_{i j}=c \times p_{i j}, M_{j i}=c \times\left(1-p_{i j}\right)$. We proceed by applying a psychometric scaling algorithm, TrueSkill in our case, to translate the comparisons into a coherent scale of quality scores. We could of course employ other approaches like Perez et al. [40], who employ a Gaussian distribution as a prior model for image quality. To assign a quality score to a new image, we propose to compare it against a set of reference images with established scores. This can be achieved by either integrating the new image into an existing comparison matrix and recalculating the scores for all images or by fixing the reference scores and adjusting the new image score to align it with the established scale. By only predicting the probability of preference, PICNIQ allows multiple degrees of freedom over the choices of the scaling algorithm, the prior distribution of image quality, and the integration method of single images. In our experiments, we reproduce the comparison matrix of each scene using PICNIQ's predictions, which is a straightforward approach to evaluating the model on a large set of images. We note that the choice of pairs is as crucial in inference as in training, to ensure consistent scaling. We propose using active sampling techniques such as HodgeRank and ASAP [35,57], for optimal pair selection.

## 4 Experiments

### 4.1 Dataset

We test our method on the generalization split of PIQ23 [6], which consists of 15 out of 50 scenes for testing and the rest for training. This selection accounts for approximately $30 \%$ of the total images, uniformly distributed across the different lighting conditions, encompassing around 1486 out of the 5116 images of PIQ23. Previously reported results [5,6] showcase difficulty in the generalization over this dataset, due to the large diversity in image conditions. We demonstrate the capability of our model on the three attributes presented in PIQ23: details, exposure, and overall. Each attribute includes around 100k+ image pairs for training and $50 \mathrm{k}+$ pairs for testing.

### 4.2 Implementation details

We train PICNIQ on randomly cropped patches of size $1200 \times 1200$. We use Adam stochastic optimization with learning rates between $10^{-7}$ and $10^{-4}$. For smoother training, we adopt different learning rates per module. For instance, we apply a smaller learning rate to the backbone compared to the fully connected
layer. Since we have a large set of pairs over a small number of images (i.e. 100k+ pairs over $3 \mathrm{k}+$ images), we apply a strongly decreasing scheduler to smooth the training and avoid early overfitting. We train for 40 epochs and adopt a learning rate decay factor of 0.05 for every 3 epochs. We use early stopping with a patience of 10 epochs. For the loss choice, we have tested a weighted version of the mean squared error (MSE), the Huber loss, and the binary cross entropy (BCE). While the Huber loss demonstrated better stability in training, the best results have been obtained with the weighted BCE loss. Since the comparison matrices include an important number of noisy single count comparison pairs (Fig. 2), we have tested multiple thresholds on the minimum number of comparisons between each pair, $(1,2,3$, or 4$)$. Our best results have been obtained for thresholds 2 and 3. For the model architecture, we adopt DBCNN's deep dual bilinear backbone [68], pre-trained on KonIQ-10k [19]. It consists of a frozen CNN pre-trained on synthetic data from the Waterloo [30] and PASCAL VOC [10] datasets, combined with a VGG-16 for authentic quality extraction. This work is a proof of concept, and we believe more sophisticated models can deliver better results. However, our preliminary tests with deeper models suggest a need for high parameter optimization and tuning, thus we have kept the results out of the scope of this paper. Inside the hub layer, we adopt a three-layer perceptron (MLP) with a dropout of 0.4 and a GELU activation function [17]. Finally, we ensure pair order neutrality by randomly ordering images in input pairs to prevent preference bias (always choosing the first image as best and vice versa). For memory optimization, we have developed an image caching system, which stacks all images present in a single batch and avoids reserving extra memory for images that are repeated in multiple pairs in the same batch. Our experiments have been performed using PyTorch. We have trained our models with either a configuration of 32 Nvidia V100 or 16 Nvidia A100 GPUs, with a batch size of 4 or 10 pairs respectively. The training time was around 10 to 15 hours, depending on the configuration.

### 4.3 Baseline methods

We have compared PICNIQ with several BIQA models tested on PIQ23: DBCNN [68], HyperIQA [49], MUSIQ [22], SEM-HyperIQA [5] and FHIQA [6]. DB-CNN and two MUSIQ models have been pre-trained on the LIVE Challenge, KonIQ-10k, and PaQ-2-PiQ datasets, respectively. For all HyperIQA variants, only the Resnet50 backbone has been pre-trained on ImageNet without any subsequent IQA pre-training.

### 4.4 Metrics

To evaluate performance, we compute Pearson's linear correlation coefficient (PLCC), Spearman's rank correlation coefficient (SRCC), Kendal's rank correlation (KRCC), the averaged correlations, and the mean absolute error (MAE) between the model outputs and the ground-truth scores. In the PIQ23 dataset, each scene is annotated individually, thus quality scores cannot be merged. Therefore,

Table 1: Performance metrics of various Blind Image Quality Assessment (BIQA) models on PIQ23. The results are presented as Median (Mean $\pm \mathrm{MoE}$ ) for each metric over the test scenes of PIQ23. PICNIQ consistently delivers superior results in all metrics and for all attributes.

| $\#$ | Model $\backslash$ Attribute | Details |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | SRCC $(M \pm M o E)$ | PLCC | KRCC | MAE |
| 1 | DB-CNN (LIVE C) | $0.59(0.53 \pm 0.12)$ | $0.51(0.44 \pm 0.12)$ | $0.45(0.39 \pm 0.09)$ | $0.99(1.02 \pm 0.13)$ |
| 2 | MUSIQ (KonIQ-10k) | $0.71(0.65 \pm 0.11)$ | $0.67(0.68 \pm 0.09)$ | $0.52(0.49 \pm 0.10)$ | $0.88(0.99 \pm 0.15)$ |
| 3 | MUSIQ (PaQ-2-PiQ) | $0.72(0.71 \pm 0.09)$ | $\underline{0.77}(0.73 \pm 0.07)$ | $0.53(0.54 \pm 0.08)$ | $0.90(0.94 \pm 0.17)$ |
| 4 | HyperIQA* | $0.70 \overline{(0.66 \pm 0.10)}$ | $\overline{0.67} \overline{(0.66 \pm 0.09)}$ | $0.50 \overline{(0.49 \pm 0.09)}$ | $0.94(0.94 \pm 0.14)$ |
| 5 | SEM-HyperIQA* | $0.73(0.69 \pm 0.10)$ | $0.65(0.67 \pm 0.06)$ | $\underline{0.55}(0.51 \pm 0.08)$ | $0.88(0.87 \pm 0.08)$ |
| 6 | SEM-HyperIQA-CO* | $\underline{0.75}(0.70 \pm 0.09)$ | $0.71(0.70 \pm 0.08)$ | $\overline{0.55}(0.53 \pm 0.08)$ | $0.85(\overline{0.89 \pm 0.10)}$ |
| 7 | FHIQA* | $0.74(0.69 \pm 0.09)$ | $0.72(0.70 \pm 0.08)$ | $0.55(0.52 \pm 0.08)$ | $0.80(0.95 \pm 0.17)$ |
| 8 | PICNIQ (KonIQ-10k) | $0.83(0.80 \pm 0.06)$ | $0.81(0.81 \pm 0.05)$ | $0.64(0.62 \pm 0.06)$ | $0.72(0.71 \pm 0.09)$ |
| $\#$ | Model $\backslash$ Attribute | Exposure |  |  |  |
|  |  | SRCC | PLCC | $\mathrm{KRCC}$ | MAE |
| 1 | DB-CNN (LIVE C) | $0.69(0.65 \pm 0.11)$ | $0.69(0.67 \pm 0.10)$ | $0.51(0.49 \pm 0.09)$ | $0.91(0.92 \pm 0.12)$ |
| 2 | MUSIQ (KonIQ-10k) | $0.74(0.69 \pm 0.09)$ | $0.70(0.71 \pm 0.08)$ | $0.55(0.52 \pm 0.08)$ | $0.93(\overline{1.02 \pm 0.19)}$ |
| 3 | MUSIQ (PaQ-2-PiQ) | $\mathbf{0 . 7 9}(0.71 \pm 0.09)$ | $\underline{0.78(0.73 \pm 0.08)}$ | $0.59(0.54 \pm 0.08)$ | $0.87(0.92 \pm 0.14)$ |
| 4 | HyperIQA* | $0.69(0.66 \pm 0.10)$ | $\overline{0.68}(0.67 \pm 0.11)$ | $\overline{0.50} \overline{(0.49 \pm 0.09)}$ | $0.86 \overline{(0.93 \pm 0.19)}$ |
| 5 | SEM-HyperIQA* | $0.72(0.65 \pm 0.12)$ | $0.70(0.66 \pm 0.11)$ | $0.53(0.49 \pm 0.10)$ | $0.97(0.96 \pm 0.15)$ |
| 6 | SEM-HyperIQA-CO* | $0.70(0.67 \pm 0.09)$ | $0.70(0.69 \pm 0.09)$ | $0.52(0.51 \pm 0.08)$ | $0.94(1.06 \pm 0.22)$ |
| 7  | FHIQA* | $0.76(0.69 \pm 0.11)$ | $0.71(0.69 \pm 0.10)$ | $0.57(0.52 \pm 0.10)$ | $\underline{0.85(0.93 \pm 0.13)}$ |
| 8 | PICNIQ (KonIQ-10k) | $\underline{0.77}(0.77 \pm 0.05)$ | $0.80(0.78 \pm 0.05)$ | $0.60(0.59 \pm 0.05)$ | $\overline{\mathbf{0 . 7 6}}(0.74 \pm 0.07)$ |


| $\#$ | Model $\backslash$ Attribute | Overall |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | SRCC | PLCC | $\mathrm{KRCC}$ | MAE |
| 1 | DB-CNN (LIVE C) | $0.59(0.57 \pm 0.13)$ | $0.64(0.58 \pm 0.12)$ | $0.43(0.42 \pm 0.10)$ | $1.04(1.08 \pm 0.18)$ |
| 2 | MUSIQ (KonIQ-10k) | $0.76(0.68 \pm 0.10)$ | $0.75(0.70 \pm 0.09)$ | $0.57(0.51 \pm 0.09)$ | $\underline{0.95}(1.06 \pm 0.26)$ |
| 3 | MUSIQ (PaQ-2-PiQ) | $0.74(0.65 \pm 0.14)$ | $0.74(0.66 \pm 0.13)$ | $0.54(0.49 \pm 0.11)$ | $\overline{1.09} \overline{(1.23 \pm 0.29)}$ |
| 4 | HYPERIQA* | $0.74(0.69 \pm 0.08)$ | $0.74(0.71 \pm 0.07)$ | $0.55(0.52 \pm 0.08)$ | $0.99(1.09 \pm 0.17)$ |
| 5 | SEM-HyperIQA* | $0.75(0.68 \pm 0.10)$ | $0.75(0.70 \pm 0.09)$ | $0.56(0.52 \pm 0.09)$ | $1.03(1.10 \pm 0.19)$ |
| 6 | SEM-HyperIQA-CO* | $0.74(0.68 \pm 0.09)$ | $0.74(0.70 \pm 0.08)$ | $0.55(0.51 \pm 0.08)$ | $1.04(1.15 \pm 0.23)$ |
| 7  | FHIQA* | $\underline{0.78}(0.71 \pm 0.09)$ | $\underline{0.78}(0.73 \pm 0.08)$ | $\underline{0.59}(0.54 \pm 0.08)$ | $1.12(1.11 \pm 0.21)$ |
| 8 | PICNIQ (KonIQ-10k) | $\overline{0.81}(\overline{0.75 \pm 0.08})$ | $\overline{0.81}(\overline{0.78 \pm 0.07})$ | $\overline{0.61}(\overline{0.58 \pm 0.0} 8)$ | $0.72(0.74 \pm 0.11)$ |

we calculate the metrics for each scene separately, then aggregate performance on each metric across all scenes through the median, $M_{\text {Med }}=M_{\left(\frac{s}{2}\right)}$ where $s$ denotes the total number of scenes, and $M_{(i)}$ represents the $i$-th smallest scene metric value among the sorted scenes. We also report the mean and the margin of error (MoE) across the scenes. For early stopping, we evaluate the models on median SRCC performance.

### 4.5 Results

To demonstrate the performance of PICNIQ, we draw an extensive quantitative analysis on PIQ23. We report quantitative metrics in Table 1, from which we can draw several observations. First, PICNIQ outperforms all other models across all metrics. While our best performance is achieved for the details and overall attributes, we can still see improvement for exposure where our model delivers consistent results over the median and far better results for the mean and a smaller MoE than competitors. Notably, the distinctively smaller MAE of the quality predictions shows that PICNIQ is better suited to reproduce fine-grained
![](https://cdn.mathpix.com/cropped/2024_06_04_16d2ab5d0a02704b9d0fg-13.jpg?height=586&width=1226&top_left_y=390&top_left_x=466)

Fig. 3: Comparative analysis of IQA models based on the averaged correlation metrics (top - larger is better) and mean absolute error (bottom - smaller is better) across all scenes and for the three attributes of PIQ23. The results showcase the superiority of PICNIQ over previous models in all metrics

![](https://cdn.mathpix.com/cropped/2024_06_04_16d2ab5d0a02704b9d0fg-13.jpg?height=680&width=1203&top_left_y=1194&top_left_x=469)

Fig. 4: The histogram of PICNIQ's predictions on PIQ23 separated by their corresponding ground truth values over 6 bins. We can observe that PICNIQ's predictions are reasonably aligned and calibrated with the ground truth probability, suggesting that the model is increasingly confident in predicting higher probabilities when they are indeed higher. The red dashed line represents the indecisive line with a probability of 0.5 .

quality scales that are aligned with the ground truth, pushing in the right direction to solve the domain shift problem. In other words, while regression-based
models are not able to reproduce the JOD scales of the PIQ23 scenes (reflected by the large MAE), our model, through pairwise comparisons, simplifies the task, leaving the regression part out of the training loop, and delivering precise reproduction of the quality scales (reflected by a much smaller MAE). This approach further shows that a scalable and precise BIQA metric is more likely to be ranking-based since we have access to the score generation as a post-processing step.

To further support our claims, we draw the averaged correlation metrics and the MAE distribution across the 15 test scenes of PIQ23 in Figure 3. From these boxplots, we can notice some outlier scenes where the metrics are far worse than the average, which can reduce the apparent quantitative performance of the models. This justifies our choice of using the median to measure performance, instead of the mean, in Table 1. We can see that PICNIQ outperforms all other models with a stable performance across all test scenes of PIQ23. In partciular, we can see that the performance over the exposure attribute, reported in Tab. 1, does not truly reflect the true performance of PICNIQ: while MUSIQ performs well on paper, a deeper look into the distribution across scenes shows that PICNIQ is significantly more consistent.

Finally, we plot the histograms of PICNIQ's predictions on PIQ23 in Fig. 4. We separate the ground truth probability into 6 different bins. We can observe that the model's predictions align reasonably well with the ground truth probability. More specifically, the model seems to be fairly well-calibrated in the sense that the peaks of the histograms (the most frequently predicted probabilities) are generally aligned with the center of the true probability bins. We can notice that as the true probability increases, the peak of the histogram shifts to the right, suggesting that the model is increasingly confident in predicting higher probabilities when they are indeed higher. The model however still struggles in gray areas, i.e., where the comparisons are close but a preference choice can still be made. For instance, in bins $[017,0.33]$ and $[0.67,0.83]$ the model tends towards an indecisive $[0.4,0.6]$ region especially for the overall attribute. This is probably due to an imbalance in the training dataset towards decisive pairs, where we have more 0s and 1s than middle values. For a better understanding of the training biases, we have included an analysis of the distribution of ground truth probabilities of PIQ23 in the supplementary material.

Globally, the results of our model are significantly better than the previous state-of-the-art, indicating a solid foundation for inspiring future solutions in BIQA for domain shift and domain generalization.

## 5 Conclusion

This paper introduces PICNIQ, a novel pairwise comparison framework, adapted to sparse comparison settings and designed to counter the problem of domain shift and uncertainty in BIQA. Instead of direct quality prediction, PICNIQ is trained to predict the quality differences of input image pairs, which is then combined with psychometric scaling algorithms to generate quality scores. PIC-

NIQ can be used as a quality comparison tool or to generate quality scores for a large set of images. We also explore and exploit the pairwise comparison matrices in the PIQ23 dataset, which are made public for future research in BIQA. Through extensive experiments on the generalization split of PIQ23, we demonstrate the high performance of PICNIQ and the granular quality scales it reproduces. Finally, we note that while PICNIQ employs a simple VGG-16 backbone, it outperforms all other complex architectures, which opens a wide door for future adaptations that exploit deep architectures and self-supervised models. We also believe that our comparison framework has the potential to become a standard for high-precision digital camera quality assessment metrics, encouraging more pairwise comparison adoption in the IQA literature.

## 6 Acknowledgments

This work was funded in part by the French government under the management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton/ENS chair in artificial intelligence and the Inria/NYU collaboration. This work was performed using HPC resources from GENCI-IDRIS (Grant 2023AD011013850). NC was supported in part by a DXOMARK/PRAIRIE CIFRE Fellowship. Certain sections of this document were improved with the assistance of AI, specifically GPT-4 and Grammarly AI.

## References

1. Babu, N.C., Kannan, V., Soundararajan, R.: No reference opinion unaware quality assessment of authentically distorted images. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2459-2468 (2023)
2. Bosse, S., Maniry, D., Müller, K.R., Wiegand, T., Samek, W.: Deep neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on image processing 27(1), 206-219 (2017)
3. Cao, F., Guichard, F., Hornung, H.: Measuring texture sharpness of a digital camera. In: Digital Photography V. vol. 7250, pp. 146-153. SPIE (2009)
4. Chahine, N., Belkarfa, S.: Portrait quality assessment using multi-scale cnn. In: London Imaging Meeting. vol. 2021, pp. 5-10. Society for Imaging Science and Technology (2021)
5. Chahine, N., Calarasanu, S., Garcia-Civiero, D., Cayla, T., Ferradans, S., Ponce, J.: An image quality assessment dataset for portraits. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9968-9978 (2023)
6. Chahine, N., Ferradans, S., Vazquez-Corral, J., Ponce, J.: Generalized portrait quality assessment. arXiv preprint arXiv:2402.09178 (2024)
7. Davidson, R.R., Farquhar, P.H.: A bibliography on the method of paired comparisons. Biometrics pp. 241-252 (1976)
8. Duanmu, Z., Liu, W., Wang, Z., Wang, Z.: Quantifying visual image quality: A bayesian view. Annual Review of Vision Science 7, 437-464 (2021)
9. DXOMARK: https://corp.dxomark.com/
10. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal of computer vision 88, 303-338 (2010)
11. Fang, Y., Zhu, H., Zeng, Y., Ma, K., Wang, Z.: Perceptual quality assessment of smartphone photography. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3677-3686 (2020)
12. Feng, Z., Zhang, K., Jia, S., Chen, B., Wang, S.: Learning from mixed datasets: A monotonic image quality assessment model. Electronics Letters 59(3), e12698 (2023)
13. Ghadiyaram, D., Bovik, A.C.: Massive online crowdsourced study of subjective and objective picture quality. IEEE Transactions on Image Processing 25(1), 372-387 (2015)
14. Ghadiyaram, D., Bovik, A.C.: Perceptual quality prediction on authentically distorted images using a bag of features approach. Journal of vision $\mathbf{1 7}(1), 32-32$ (2017)
15. Golestaneh, S.A., Dadsetan, S., Kitani, K.M.: No-reference image quality assessment via transformers, relative ranking, and self-consistency. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1220-1230 (2022)
16. Gousseau, Y., Roueff, F.: Modeling occlusion and scaling in natural images. Multiscale Modeling \& Simulation 6(1), 105-134 (2007)
17. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016)
18. Herbrich, R., Minka, T., Graepel, T.: Trueskill ${ }^{\text {TM: }}$ a bayesian skill rating system. Advances in neural information processing systems 19 (2006)
19. Hosu, V., Lin, H., Sziranyi, T., Saupe, D.: Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing 29, 4041-4056 (2020)
20. Huang, C.H., Wu, J.L.: Multi-task deep cnn model for no-reference image quality assessment on smartphone camera photos. arXiv preprint arXiv:2008.11961 (2020)
21. Kang, L., Ye, P., Li, Y., Doermann, D.: Convolutional neural networks for noreference image quality assessment. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1733-1740 (2014)
22. Ke, J., Wang, Q., Wang, Y., Milanfar, P., Yang, F.: Musiq: Multi-scale image quality transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5148-5157 (2021)
23. Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems 30 (2017)
24. Kim, J., Lee, S.: Fully deep blind image quality predictor. IEEE Journal of selected topics in signal processing 11(1), 206-220 (2016)
25. Larson, E.C., Chandler, D.M.: Most apparent distortion: full-reference image quality assessment and the role of strategy. Journal of electronic imaging 19(1), 011006 (2010)
26. Li, X., Ren, Y., Jin, X., Lan, C., Wang, X., Zeng, W., Wang, X., Chen, Z.: Diffusion models for image restoration and enhancement-a comprehensive survey. arXiv preprint arXiv:2308.09388 (2023)
27. Liu, A., Wang, J., Liu, J., Su, Y.: Comprehensive image quality assessment via predicting the distribution of opinion score. Multimedia Tools and Applications 78, 24205-24222 (2019)
28. Liu, X., Van De Weijer, J., Bagdanov, A.D.: Rankiqa: Learning from rankings for no-reference image quality assessment. In: Proceedings of the IEEE international conference on computer vision. pp. 1040-1049 (2017)
29. Loebich, C., Wueller, D., Klingen, B., Jaeger, A.: Digital camera resolution measurements using sinusoidal siemens stars. In: Digital Photography III. vol. 6502, pp. 214-224. SPIE (2007)
30. Ma, K., Duanmu, Z., Wu, Q., Wang, Z., Yong, H., Li, H., Zhang, L.: Waterloo exploration database: New challenges for image quality assessment models. IEEE Transactions on Image Processing 26(2), 1004-1016 (2016)
31. Ma, K., Liu, W., Liu, T., Wang, Z., Tao, D.: dipiq: Blind image quality assessment by learning-to-rank discriminable image pairs. IEEE Transactions on Image Processing 26(8), 3951-3964 (2017)
32. Ma, K., Liu, X., Fang, Y., Simoncelli, E.P.: Blind image quality assessment by learning from multiple annotators. In: 2019 IEEE international conference on image processing (ICIP). pp. 2344-2348. IEEE (2019)
33. Mantiuk, R.K., Tomaszewska, A., Mantiuk, R.: Comparison of four subjective methods for image quality assessment. In: Computer graphics forum. vol. 31, pp. 2478-2491. Wiley Online Library (2012)
34. Mattheakis, M., Protopapas, P., Sondak, D., Di Giovanni, M., Kaxiras, E.: Physical symmetries embedded in neural networks. arXiv preprint arXiv:1904.08991 (2019)
35. Mikhailiuk, A., Wilmot, C., Perez-Ortiz, M., Yue, D., Mantiuk, R.K.: Active sampling for pairwise comparisons via approximate message passing and information gain maximization. In: 2020 25th International Conference on Pattern Recognition (ICPR). pp. 2559-2566. IEEE (2021)
36. Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing 21(12), 4695-4708 (2012)
37. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a "completely blind" image quality analyzer. IEEE Signal processing letters 20(3), 209-212 (2012)
38. Moorthy, A.K., Bovik, A.C.: Blind image quality assessment: From natural scene statistics to perceptual quality. IEEE transactions on Image Processing 20(12), $3350-3364$ (2011)
39. Morikawa, C., Kobayashi, M., Satoh, M., Kuroda, Y., Inomata, T., Matsuo, H., Miura, T., Hilaga, M.: Image and video processing on mobile devices: a survey. The Visual Computer 37(12), 2931-2949 (2021)
40. Perez-Ortiz, M., Mantiuk, R.K.: A practical guide and software for analysing pairwise comparison experiments. arXiv preprint arXiv:1712.03686 (2017)
41. Perez-Ortiz, M., Mikhailiuk, A., Zerman, E., Hulusic, V., Valenzise, G., Mantiuk, R.K.: From pairwise comparisons and rating to a unified quality scale. IEEE Transactions on Image Processing 29, 1139-1151 (2019)
42. Ponomarenko, N., Jin, L., Ieremeiev, O., Lukin, V., Egiazarian, K., Astola, J., Vozel, B., Chehdi, K., Carli, M., Battisti, F., et al.: Image database tid2013: Peculiarities, results and perspectives. Signal processing: Image communication 30, $57-77$ (2015)
43. Ponomarenko, N., Lukin, V., Zelensky, A., Egiazarian, K., Carli, M., Battisti, F.: Tid2008-a database for evaluation of full-reference visual quality assessment metrics. Advances of Modern Radioelectronics 10(4), 30-45 (2009)
44. Qin, G., Hu, R., Liu, Y., Zheng, X., Liu, H., Li, X., Zhang, Y.: Data-efficient image quality assessment with attention-panel decoder. arXiv preprint arXiv:2304.04952 (2023)
45. Saad, M.A., Bovik, A.C., Charrier, C.: Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE transactions on Image Processing 21(8), 3339-3352 (2012)
46. Saha, A., Mishra, S., Bovik, A.C.: Re-iqa: Unsupervised learning for image quality assessment in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5846-5855 (2023)
47. Sheikh, H.R., Sabir, M.F., Bovik, A.C.: A statistical evaluation of recent full reference image quality assessment algorithms. IEEE Transactions on image processing 15(11), 3440-3451 (2006)
48. Shi, Y., Niu, Y., Guo, W., Huang, Y., Zhan, J.: Pairwise learning to rank for image quality assessment. IEEE Access 8, 192352-192367 (2020)
49. Su, S., Yan, Q., Zhu, Y., Zhang, C., Ge, X., Sun, J., Zhang, Y.: Blindly assess image quality in the wild guided by a self-adaptive hyper network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3667-3676 (2020)
50. Sun, W., Min, X., Zhai, G., Ma, S.: Blind quality assessment for in-the-wild images via hierarchical feature fusion and iterative mixed database training. arXiv preprint arXiv:2105.14550 (2021)
51. Talebi, H., Milanfar, P.: Nima: Neural image assessment. IEEE transactions on image processing 27(8), 3998-4011 (2018)
52. Thurstone, L.L.: A law of comparative judgment. Psychological review 101(2), 266 (1994)
53. Virtanen, T., Nuutinen, M., Vaahteranoksa, M., Oittinen, P., Häkkinen, J.: Cid2013: A database for evaluating no-reference image quality assessment algorithms. IEEE Transactions on Image Processing 24(1), 390-402 (2014)
54. Wang, Z., Ma, K.: Active fine-tuning from gmad examples improves blind image quality assessment. IEEE Transactions on Pattern Analysis and Machine Intelligence $44(9), 4577-4590$ (2021)
55. Wu, Q., Li, H., Ngan, K.N., Ma, K.: Blind image quality assessment using local consistency aware retriever and uncertainty aware evaluator. IEEE Transactions on Circuits and Systems for Video Technology 28(9), 2078-2089 (2017)
56. Xu, L., Jiang, X.: Blind image quality assessment by pairwise ranking image series. China Communications (2023)
57. Xu, Q., Xiong, J., Chen, X., Huang, Q., Yao, Y.: Hodgerank with information maximization for crowdsourced pairwise ranking aggregation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32 (2018)
58. Xue, W., Zhang, L., Mou, X.: Learning without human scores for blind image quality assessment. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 995-1002 (2013)
59. Yang, J., Lyu, M., Qi, Z., Shi, Y.: Deep learning based image quality assessment: A survey. Procedia Computer Science 221, 1000-1005 (2023)
60. Yang, S., Wu, T., Shi, S., Lao, S., Gong, Y., Cao, M., Wang, J., Yang, Y.: Maniqa: Multi-dimension attention network for no-reference image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1191-1200 (2022)
61. Ye, P., Kumar, J., Kang, L., Doermann, D.: Unsupervised feature learning framework for no-reference image quality assessment. In: 2012 IEEE conference on computer vision and pattern recognition. pp. 1098-1105. IEEE (2012)
62. Ying, Z., Niu, H., Gupta, P., Mahajan, D., Ghadiyaram, D., Bovik, A.: From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3575-3585 (2020)
63. You, J., Korhonen, J.: Transformer for image quality assessment. In: 2021 IEEE International Conference on Image Processing (ICIP). pp. 1389-1393. IEEE (2021)
64. Zeng, H., Zhang, L., Bovik, A.C.: A probabilistic quality representation approach to deep blind image quality prediction. arXiv preprint arXiv:1708.08190 (2017)
65. Zerman, E., Valenzise, G., Dufaux, F.: An extensive performance evaluation of full-reference hdr image quality metrics. Quality and User Experience 2(1), 1-16 (2017)
66. Zhang, L., Zhang, L., Bovik, A.C.: A feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing 24(8), 2579-2591 (2015)
67. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018)
68. Zhang, W., Ma, K., Yan, J., Deng, D., Wang, Z.: Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology 30(1), 36-47 (2018)
69. Zhang, W., Ma, K., Zhai, G., Yang, X.: Learning to blindly assess image quality in the laboratory and wild. In: IEEE International Conference on Image Processing (ICIP). pp. 111-115. IEEE (2020)
70. Zhang, W., Ma, K., Zhai, G., Yang, X.: Uncertainty-aware blind image quality assessment in the laboratory and wild. IEEE Transactions on Image Processing 30, 3474-3486 (2021)
71. Zhao, K., Yuan, K., Sun, M., Li, M., Wen, X.: Quality-aware pre-trained models for blind image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22302-22313 (2023)
72. Zhu, W., Zhai, G., Han, Z., Min, X., Wang, T., Zhang, Z., Yangand, X.: A multiple attributes image quality database for smartphone camera photo quality assessment.

In: 2020 IEEE International Conference on Image Processing (ICIP). pp. 29902994. IEEE (2020)

73. van Zwanenberg, O., Triantaphillidou, S., Jenkin, R., Psarrou, A.: Edge detection techniques for quantifying spatial imaging system performance and image quality. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 0-0 (2019)
