# Evolving Code with A Large Language Model 

Erik Hemberg ${ }^{1^{*}}$, Stephen Moskal ${ }^{1}$ and Una-May O'Reilly ${ }^{1}$<br>$1^{*}$ EECS, MIT CSAIL, 32 Vassar St, Cambridge, 02139, MA, USA.

*Corresponding author(s). E-mail(s): hembergerik@csail.mit.edu; Contributing authors: smoskal@mit.edu; unamay@csail.mit.edu;


#### Abstract

Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM_GP, a formalized LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators radically differ from GP's because they enlist an LLM, using prompting and the LLM's pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM_GP and share its code. By addressing algorithms that range from the formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming.


Keywords: Large Language Models, Genetic Programming, Evolutionary Algorithm, Operators

## 1 Introduction

Large language models (LLMs), along with other Foundational Models, have disrupted conventional expectations of Artificial Intelligence systems. An LLM, with a chatbot or Natural Language API, typically works in the input-output space of natural language, i.e. unstructured text. In a question-answer style, it processes natural language prompts and responds in natural language. Technically speaking, it is a pre-trained transformer model ${ }^{1}$ which has distilled statistical patterns from a massive training set within its massive quantity of numerical parameters and artificial neural architecture. Pre-training is a process which back-propagates errors arising from predictions that complete text sequences which come from massive training data. In many cases the[^0]model is then further fine-tuned on specifically selected data. Finally, a process called Reinforcement Learning with Human Feedback [2] is run to set up prompt-response (or question-answer) capability.

In comparison, Evolutionary Algorithms (EA), inspired by Neo-Darwinian evolution, operate on a population of candidate solutions. Generally, an EA uses operators. It has one operator that initializes a population of candidate solutions, two others that iteratively test each solution and calculate its fitness, one that selects parents by referencing solution fitness, operators that vary parental copies randomly, and one to compose a new population that replaces the old. A basic EA is set up with its operators. Before execution of a run, it is provided with a solution representation and a fitness function. GP is an evolutionary algorithm, one that evolves code. It follows the general algorithmic structure of an EA while it has GP-specific versions of evolutionary operators such as crossover.

Surprisingly, LLMs are able to generate code to solve many software engineering tasks and even program synthesis. And, both GP and LLMs are able to evolve code, i.e. perform program synthesis with an evolutionary-inspired method. The LLM-basis of this claim is backed up by recent research with noteworthy results. In very original work [3-5], open challenges in GP [6,7] are being addressed with LLMs integrated with some sort of Evolutionary Algorithm. For example, an LLM has been enlisted to perform the function of different operators in the OpenELM Library [3], an LLM has been used for code-level neural architecture search [4], and, largely without using a typical GP algorithm, though by using similar evolutionary mechanisms, a large number of GP benchmarks for Automatic Programming have been solved with an LLM-supported procedure [5].

One objective of this paper is describe how an algorithm, with the general algorithmic structure of an EA and evolutionary operators, can use an LLM to evolve code, see Figure 1. We describe how the operators are designed to formulate LLM prompts, task the LLM via the prompts, and process LLM responses, while code is represented as a sequence of text in code syntax. The prompts "task" the LLM to fulfill the purpose of the operator: to initialize candidate solutions, select parents based on their performance, to vary one by mutation or more than one solution by recombination, etc. The description is accompanied by operator and prompt design information and it presents preparatory run steps that are LLM-usage specific. Another objective is to provide an implementation and demonstration of a simple LLM_GP variant. We hope to demystify the approach and provide a hands-on starting point for exploration.

We start, in Section 2, with background on LLMs and code models. In Section 3 we introduce LLM_GP and the design of its LLM-based operators, prompt-functions and LLM-oriented preparatory steps. This helps us, in Section 4, to describe and contrast the relatively small body of current work integrating GP and LLMs. Moving to a hands-on perspective, in Section 5 we provide a simple-to-understand LLM_GP variant within a open-source software package named Tutorial-LLM_GP. Modules of the package contain both the algorithm's prompt functions and its prompts. These are a very sensitive part of using LLMs and frequently draw a lot of curiosity. We use this variant to demonstrate time and financial costs, usage statistics, and errors related to LLM usage. We present a discussion in Section 6 regarding risks of using LLM_GP in

![](https://cdn.mathpix.com/cropped/2024_06_04_4aac975f5c7b1f416258g-03.jpg?height=283&width=1122&top_left_y=1258&top_left_x=341)

Fig. 1: Overview of GP process with LLM operators. Codes are the population. Prompts are different for each LLM operator.

regards to best practices of scientific investigations, present arguments for nonetheless pursuing evolving code with LLM-support, and suggest standards and open questions. Finally, in Section 7 we conclude with a summary of the paper's contributions.

## 2 Background: Large Language Models

This section provides background on Large Language Models and code models.

## Large Language Models

Language models (LM) generatively model the statistical likelihood of a corpus of text [8], implying they can generate text completions using pattern matching between prompt text and text seen during training. This capability makes them extremely useful for natural language tasks such as translation, summarization, or text classification. The capabilities of language models abruptly accelerated with the adoption of transformer architectures [11. Transformer architectures avoid the constraints of prior models implying that they had to be trained serially and now allow vast amounts of computation to be marshaled for training with parallelization. They also leverage training on the task of token sequence unmasking or completion which allows training on massive, public, (and free) unlabeled text corpuses such as the web pages of the Internet and digital libraries. The "Large" in "Large Language Models" typically refers to language models with at least 10B parameters. LLMs catapulted to global attention and wide-spread adoption in 2022 with the introduction of OpenAI's "GPT" series (Generative Pre-trained Transformer, [9-11]) which ranged from 120M to 175B parameters and used training sets that ranged in size from 1B [9] to 300B [10]. Chat-GPT made the biggest impact due it being released with a free and easy-touse question-answer interface. This interface (also provided programmatically via an $\mathrm{API}$ ) accepts from some actor $A$, a natural language text sequence, which functions to $A$ as a query, question or "prompt". $A$ assigns knowledge, meaning, and intention to the text in the prompt. They can provide problem context, problem solution examples, chain of thought reasoning, or other information, thought by them, to aid the LLM's generative pattern matching and completion. As output, the LLM provides a sequence of natural language text, intended to be the ideal generative pattern-based completion to the prompt. This input-output relation $f$ can be denoted simply as $f: \mathcal{T}^{n} \rightarrow \mathcal{T}^{l}$, response $=f($ prompt $\mid \Theta), n, l \in \mathbb{Z}^{+}$where $\mathcal{T}^{i}$ denotes a sequence of text of length $i$ and $\Theta$ is the LLM parameters. It is very complicated to more specifically describe a transformer, let alone a GPT, e.g. [12]. Part 1 of one description, written in a 'pure mathematical' style runs 8 pages, see this report. Note that prompt text is tokenized prior to being input to the LLM so frequently LLM input is also described as tokens. All LLMs have a finite token capacity for a prompt-response pair. Often commercial LLMs charge by the token. An LLM remembers nothing between prompts. To link together an interdependent series of tasks, model responses, and even prior prompts, this information must be aggregated and included in subsequent prompts.

Despite many persuasive examples, LLMs exhibit a number of unresolved issues. These include, for example, the fact that LLMs offer no correctness guarantee. In fact, because they are not retrieving information and instead are pattern-based, they have been documented to return confabulated references, facts and fallacious logic [13]. This is often called "hallucination". Approaches to address hallucination include human verification, automated verification, requesting the model to explain itself, and modifications to the RLHF layer. While a complete and robust successful approach is not yet available, useful and timely progress on this challenge is occurring.

Second training and using an LLM is costly [10]. This cost scales with the number of model parameters and the size of the training data set. When deployed, due to how many parameters a model has, it is also costly to use it. This expensive footprint
impedes fair accessibility [14-17]. As in the case of addressing hallucination, practical approaches are already in progress.

Third, while their human users, i.e. the actors $A$, no longer have to resolve the highly technical or operational choices required to design and train deep neural networks, they must alternatively redirect their focus on setting up the prompt(s) for the task they want the LLM to assist with or handle. In other words, they must focus on prompt engineering. Prompt engineering must consider how to efficiently use the constrained token capacity of the prompt window. It must pack the problem context, historical information assisting with problem solving continuity, specifications of response format, and task structure within the prompt's size limit. This is more challenging to efficiently accomplish if a series of prompts is needed while the LLM remembers nothing between prompts. When prompt engineering must be programmed into a system, even more complicated design decisions have to be made. To date, mainly for human-model interactions, effective prompt engineering techniques have been developed. One basic strategy of many of these approaches is to provide the LLM with contextual information that guides it to attend to a solution-appropriate pattern. Adding facts as context to the prompt is also effective to some extent [18]. Other techniques include few-shot prompting [19], chain-of-thought [20], self-consistency [21, 22], "Tree-of-thoughts" [23] and "Ask-Correct" [5].

Fourth, LLMs may be negatively biased. A seminal paper, on deep networks in general, explained how negative racial, gender, and other biases within a data set could end up being entwined with the functional capabilities of an LLM[24]. Negative biases and the general unpredictable nature of LLM behavior pose a safety issue. With offthe-shelf LLMs, the training set is rarely shared and there are examples where training data has been used without permission[25] or clearly not reviewed. Stricter filtering of training content and better transparency into a training sets data will provide better safety but bias-control is likely to be a longstanding open problem of LLMs.

Fifth, prompts for one problem for one LLM should not be expected to transfer to another LLM. Across LLM, their behavior is inconsistent. Additionally, LLMs that are publicly released with no cost should be treated much like Google's Search Engine with respect to the model makers not releasing any information about their version or updates. See, e.g. [26, 27].

Finally, and arguably most importantly, despite their ease of accepting and responding in natural language, LLMs are not capable of general human intelligence. They are simply token-driven, token-outputting, generative pre-trained transformer models! To emphasize this point, one can find a highly respected community of AI researchers who study deep facets of general intelligence (artificial and human) and who work to correct inaccurate, overblown public perception that LLM's are capable of artificial general intelligence. They show for example, that LLMs trained on "A is B" fail to learn "B is A" [28] and that LLMs cannot perform simple analogical reasoning [29] or mathematics [30]. Tom Dietterich clarifies that LLMs have "pointwise" understanding that allows them to provide appropriate responses to individual queries, while they lack "systematic" understanding- the ability to provide appropriate responses across an entire range of queries or situations. He states "When people
complain that an AI system doesn't 'truly' understand, I think they are often saying that while the system can correctly handle many questions/contexts, it fails on very similar questions/contexts. Such a system cannot be trusted to produce the right behavior, in general." (our italics) [31, 32].

## Code models

Of interest and relevance to genetic programming and other closely related communities such as search-based software engineering, LLMs can perform a number of types of coding tasks. Specific models for coding have been trained. For example, Meta released the Code Llama family of models, with LLMs specifically pre-trained and tuned for coding in August 2023 by Meta [33]. They are free for research and commercial use and there are three models: Meta calls Code Llama a foundational code model, Code Llama - Python is specialized for Python; and Code Llama - Instruct, is fine-tuned for understanding natural language instructions about coding. These models come in different "sizes", i.e. with different number of parameters, with implications on how well they will work (the more parameters the better) and how much compute they use (fewer parameters use less compute). Meta claims that in its own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks [33]. One particular intersection between of the code-model community and Genetic Programming is benchmarks. Code Llama is evaluated with HumanEval, MBPP and APPS, as well as MultiPL-E and GSM8K [33]. Differentiating between research questions to pursued by the LLMs for coding community and research questions to be pursued by the GP community is an open challenge [7].

## 3 A General LLM_GP Algorithm

In Section 3.1 we present a LLM_GP algorithm. In Section 3.2 we describe its LLM based operators. Finally, in Section 3.3 we describe prompts and prompt functions.

### 3.1 LLM_GP Algorithm Overview

LLM_GP is described in Algorithm 1. It is intentionally general - each of its operators uses an LLM; while it is likely that many variants will use fewer. Algorithm 1 exhibits a number of features that make it quite dissimilar to GP. The first difference is that the unit of evolution, code, is represented as a variable-length sequence of text in code syntax. This sequence is executable but it is not a parse-tree.

All evolutionary operators in Algorithm 1 (named in its caption) are intended to use an LLM. They initialize a population of candidate genotypes (code, line 1), execute and evaluate them (lines 3 and 4 ), select parents (line 7), create children with variation (lines 8 and 10), and replace the old population with the old one (line 12). Finally, Algorithm 1 includes an LLM-based operator for designating the run's solution (line 13).

```
Algorithm 1: LLM_GP
Operators $i_{L L M}, e_{L L M}, \phi_{L L M}, s_{L L M}, v_{x o L L M}, v_{m u L L M}, r_{L L M}, b_{L L M}$
    Input : RUN HYPER-PARAMETERS
        $g$ : Generations, $n$ : Population size,
        LLM : LLM
        PROMPT-FUNCTIONS (HYPER-PARAMETERS)
        $\rho_{i}(F \cup T)$ : Prompt for initialization,
        $\rho_{e}(\mathbf{a})$ : Prompt for evaluation,
        $\rho_{\phi}(\mathbf{a})$ : Prompt for fitness measure
        $\rho_{s}(\mathbf{a}$,$) : Prompt for selection,$
        $\rho_{v_{x o}}(\mathbf{a})$ : Prompt for crossover,
        $\rho_{v_{m u}}(\mathbf{a}):$ Prompt for mutation,
        $\rho_{r}(\mathbf{a})$ : Prompt for replacement,
        $\rho_{b}(\mathbf{a})$ : Prompt for picking best solution,
    Return: $P^{*}$ : best solution
    $P \leftarrow i_{\mathrm{LLM}}\left(n, \rho_{i} \leftarrow \rho_{i}(F \cup T)\right)$ //Random initialization of population
    //Iterate over generations
    for $i \in[1, \ldots, g]$ do
        $\mathbf{y} \leftarrow e_{\mathrm{LLM}}\left(\rho_{e} \leftarrow \rho_{e}(P, D, E)\right) / /$ Execute solution
        $\mathbf{f} \leftarrow \phi_{\mathrm{LLM}}\left(\rho_{\phi} \leftarrow \rho_{\phi}(\mathbf{y}, D)\right) / /$ Measure the fitness of the solution
        $P^{\prime} \leftarrow \emptyset / /$ New population
        //Iterate over population
        while $\left|P^{\prime}\right| \leq n$ do
            $\left.p_{k}, p_{l} \leftarrow s_{\mathrm{LLM}}\left(P, \rho_{s} \leftarrow \rho_{s}(P)\right)\right) / /$ Select two parents
            $\left.p_{k}^{\prime}, p_{l}^{\prime} \leftarrow v_{x o L L M}\left(\rho_{v_{x o}} \leftarrow \rho_{v_{x o}}\left(p_{k}, p_{l}\right)\right)\right)$ //Variation with crossover
            //Iterate over children
            for $j \in[1, \ldots, n]$ do
            | $\left.p^{\prime} \leftarrow v_{m u \operatorname{LLM}}\left(\rho_{v_{m u}} \leftarrow \rho_{v_{m u}}\left(F \cup T, p^{\prime}\right)\right)\right) / /$ Variation with mutation
            $P^{\prime} \leftarrow P^{\prime} \cup\left\{p_{k}^{\prime}, p_{l}^{\prime}\right\} / /$ Add to new population
        $\left.P \leftarrow r_{\mathrm{LLM}}\left(\rho_{r} \leftarrow \rho_{r}\left(P^{\prime}\right)\right)\right)$ //Replace the population
    return $P^{*} \leftarrow \mathrm{b}_{\text {LLM }}\left(P, \rho_{b}()\right) / /$ Best solution
```


### 3.2 LLM-based Operators,

LLM-based operators are very different from well established evolutionary operators that are used in GP. A LLM_GP operator takes three steps:

1. Formulate $\rho \leftarrow \rho(\cdot)$ : Compose the prompt via calling the operator's prompt-function.
2. Interface $r=f(\rho \mid \theta)$ : Send the prompt to the LLM and collect the LLM's response. $r \in \mathcal{T}^{m}$
3. Check $r^{\prime}=c(r, \cdot)$. Ensure $r$ is well formed.

Descriptions of LLM-based operators in the package Tutorial-LLM_GP can be found in Appendix B. A description of a LLM-based mutation operator from the package Tutorial-LLM_GP follows:
$V_{\text {muLLM }}$, Mutation Operator

1. Formulate prompt with instructions to alter one parent solution $p$, using elements from primitives.
2. Interface to the LLM to execute the prompt and collect the response.
3. Check response by extracting child solution from response. If it is not properly formatted, return the parent solution $p$.

As we observe in Section 4, not every LLM_GP variant uses an LLM for every operator and none use an LLM for code evaluation. LLM EA variants which evolve types of solutions from a broader set than code, e.g. evolve text, more frequently use an LLM base evaluation (and execution operator).

### 3.3 Prompt-functions and Prompts

A new and significant parameter of each operator is its prompt, $\rho$. A prompt variable or object is a sequence of text decomposed into elements:

```
<\rho> ::= <EXAMPLES><QUERY><PRIMITIVES><RESPONSE_FORMAT>
\rho \text { is a sequence of text } \rho \in \mathcal { T } ^ { m } \text { .}
```

Some prompts contain problem-dependent information. For example the prompts for initialization and mutation include the problem primitives. Some elements in a prompt need to be added at run-time. For example, solutions undergoing crossover or a population pool for selection. Run-time information is denoted in the template using \{\} s. For example, a mutation prompt $\left\langle\rho_{m u}\right\rangle$ template for a symbolic regression problem is shown in Figure 2.

```
<EXAMPLES> ::= {n_samples} examples of mathematical expressions
are: {samples}
<QUERY> ::= Rephrase the mathematical expression {expression} into
a new mathematical expression.
<PRIMITIVES> ::= Use the listed symbols {primitives}.
<RESPONSE_FORMAT> ::= Provide no additional text in response.
Format output in JSON as {{"new_expression": "<expression>"}}
```

Fig. 2: Symbolic Regression mutation prompt template $\rho$

Because prompts may contain problem-dependent and run-time information, they must be created at run-time. This means that the LLM_GP must be provided with prompt-function hyper-parameters (overloading the prompt variable's name $\rho$ ) that return these prompts. We generalize prompt-functions to use a list of arguments a.

The general signature of a prompt-function is $\rho \leftarrow \rho(\mathbf{a})$. The Tutorial-LLM_GP package provides examples of specific signatures for a symbolic regression problem. One function for the mutation operator is shown in Figure 3.

```
def form_prompt_rephrase_mutation(self, expression: str, samples: Optional[
    List [Any]] $=$ None) $\rightarrow$ str:
    if samples is not None:
        n_samples $=\min ($ len(samples), self.n_shots)
        // Randomly sample examples to provide context for the LLM
        sample_input $=$ random.sample(list(samples.keys()), n_samples)
    else
        sample_input $="$,
        n_samples $=0$
    prompt $=$ self.REPHRASE_MUTATION_PROMPT_FEW_SHOT. format (
            expression=expression ,
            constraints=self.constraints,
            samples=sample_input,
            n_samples=n_samples ,
    )
    return prompt
```

Fig. 3: self.REPHRASE_MUTATION_PROMPT_FEW_SHOT uses the prompt template in Figure 2. It is a Python implementation of a function formulating a mutation prompt ( $\rho_{m u}$ ). The arguments a for this function are expression and samples.

This results in the prompt and response of Figure 4.

```
2 examples of mathematical expressions are: ['((x0 + x1) * (x0 -
x1) + 1)', 'x0 + x1 * (1 - 0)']
Rephrase the mathematical expression (x0 * x1) + (1 - 0) into a
new mathematical expression. Use the listed symbols ['*', '+',
'_', 'x0', 'x1', '0', '1'].
Provide no additional text in response. Format output in JSON as
{"new_expression": "<new expression>"}
```

{"new_expression": "(x0 * x1) + 1"}

Fig. 4: Example prompt and response for mutation.

The response is then formatted, see Figure 5. This results in the response format as shown in Figure 5.

Koza, in the online post [34] lays out 5 major preparatory steps to be followed ahead of a GP run. Steps 1 and 2 involve specifying the problem-dependent primitives, Step 3 involves specifying a problem-dependent fitness measure, Step 4 involves specifying run parameters, and Step 5 involves designating the solution of the run ${ }^{2}$.[^1]

```
def format_response_rephrase_mutation(self, response: str, expression: str)
    $\rightarrow$ str:
    $\operatorname{try}$ :
        phenotype $=$ json.loads(response) ["new_expression"]
    except (json.decoder.JSONDecodeError, KeyError, TypeError) as e:
            phenotype $=$ expression
            logging.error(f"\{e\} when formatting response for rephrase mutation
    for $\{$ response $\} "$ )
    return phenotype
```

RESPONSE: \{"new_expression": "(x0 * x1) + 1"\}

INDIVIDUAL (PHENOTYPE) : ( $x 0 * \mathrm{x} 1$ ) + 1

Fig. 5: Example LLM mutation prompt response formatting python code implementation and output.

The nature of the preparatory efforts of Steps 1,2 , and 4 change radically with LLMs. They culminate in a new set of at least 8 hyper-parameters which are the promptfunctions, one per LLM operator, that each return a bespoke prompt, and which each use some set of prompt engineering techniques to express the purpose of the operator they serve. Note the human expertise and effort required to prepare the prompts. The three major preparatory steps ahead of a LLM_GP run, required of a person, are:

```
SPECIFY:
    1. the programming language that will express the candidate
        solutions plus problem-dependent hand-written primitives
        and any primitives built-in to the programming language
        to be used.
```

2. the prompt-functions of all operators implemented using
an LLM.
3. the hyper-parameters for controlling the run, including
the termination criterion, i.e. Run Hyper-Parameters

## Other Considerations

Because Algorithm 1 integrates an LLM, the run-time cost of interfacing with the LLM to execute a prompt and collect a response needs to be considered. It can be broken down into time and money. An indirect cost of integrating the LLM is that of pre-training it. This is considerable and we defer discussion of how to consider the pre-training cost to Section 6. The change in the preparatory steps, particularly Step 2 where the prompt-functions must be designed, changes the nature of human effort toward preparing a GP run.

Also noteworthy is the difference in computational effort between GP and LLM_GP. We usually measure the computational effort of a GP run in terms of its
cost-dominating operator - fitness evaluations. While fitness evaluations, likely external, i.e. not with LLM, are still integral to computational effort in LLM_GP, prompting and token counts also need to be counted. When the token and prompt costs equal or exceed that of fitness evaluation, they should be incorporated into the algorithm's computational effort.

## 4 Related Work

In this section we present existing work at the intersection of EAs and LLMs, with a focus on LLM_GP variants. This area is quite new so we include both peer-reviewed papers and non-peer reviewed papers found on https://arxiv.org at the time of writing. We do not consider work which uses EAs to improve LLMs without using LLM-based operators, .e.g. $[35]$

### 4.1 By Problem Domain

We first group contributions by problem domain. We observe an array of problem domains that we group as 1) Code generation, 2) Neural Architecture Search, 3) Game Design, 4) Prompt Generation. Table 1 identifies every contribution by its authors, citation in this article, title, peer review status, and the problem domains it addresses. We compare with each problem domain below.

## 1. Code Generation

Within code generation are works evolving agent controllers or their reward functions [37, 39], solving program synthesis, symbolic regression and Boolean parity problems $[3,5,38]$, and generating meta-heuristics [36].

Lehman et al. [37] is the earliest paper with an evolutionary operator using an LLM. Within a method called Evolution through Large Models (ELM), it uses an LLM-based mutation operator and a Quality-Diversity technique called MAP-Elites to generate training examples that can be used to fine tune a LM for a particular context: game terrain. EUREKA [39] performs LLM-based variation for the evolution of a reward function for agent-based Reinforcement Learning. The approach uses an LLM create reward function variations based on reflective summarizations of the agent performance. It also allows human interaction and feedback on the reward function.

Attempts to solve the GP community's Program Synthesis Benchmark 2 [46] with LLMs tend to generate programs that semantically resemble the correct answer but that have subtle flaws. Liventsev et al. [5] introduce a unique and startling effective evolutionary approach to program synthesis called SEIDR: Synthesize, Execute, Instruct, Debug and Rank. First, program snippet solutions are LLM-generated (Synthesize). Second, with extra program context, every snippet is externally executed and assigned a fitness (Execution). Next, some solutions are selected based on a top- $k$ ranking (Rank). A repair phase (Instruct) then tries to analyze these failed solutions by considering their performance (Debug). To balance exploitation (repairing current solutions) and exploration (replacing the current solution), a beam-search algorithm is used. The SEIDIR framework outperforms Codex highlighting the power of LLM

Table 1: Overview of work at intersection of LLM and EA. PR indicates official peerreviewed publication.

| Author | Title | Problem | PR |
| :---: | :---: | :---: | :---: |
| Code Evolution |  |  |  |
| Liventsev et al. $[5]$ | Fully autonomous programming with <br> large language models. | Program synthesis | $\bar{\checkmark}$ |
| Zelikman et al. [36] | Self-Taught Optimizer (STOP): Recur- <br> sively Self-Improving Code Generation | Code for Optimization |  |
| Lehman et al. [37] | Evolution through large models. | Code for Agent con- <br> troller |  |
| Bradley et al. [3] | The openelm library: Leveraging progress <br> in language models for novel evolutionary <br> algorithms. | Code for Agent con- <br> troller, <br> Boolean Parity, <br> Program synthesis, <br> Text | $\bar{\checkmark}$ |
| Meyerson et al. [38] | Language Model Crossover: Variation <br> through Few-Shot Prompting | Code for Agent Con- <br> troller, <br> Symbolic Regression, <br> Boolean Parity, <br> Text |  |
| Ma et al. [39] | Eureka: Human-Level Reward Design via <br> Coding Large Language Models | Code for reward function |  |
| Chen et al. [4] | Evoprompting: Language models for <br> code-level neural architecture search. | Code for Neural Archi- <br> tecture Search |  |
| Nasir et al. [40] | Neural architecture search via large lan- <br> guage models and quality-diversity opti- <br> mization. | Code for Neural Archi- <br> tecture Search |  |
| Text Evolution |  |  |  |
| Guo et al. $[41]$ | Connecting Large Language Models with <br> Evolutionary Algorithms Yields Powerful <br> Prompt Optimizers | Prompt Search |  |
| Fernando et al. [42] | Promptbreeder: Self-Referential Self- <br> Improvement Via Prompt Evolution  | Prompt Search |  |
| Xu et al. [43] | Wizardlm: Empowering large language <br> models to follow complex instructions | Data for LLM tuning |  |
| Lanzi and Loiacono [44] | Chatgpt and other large language models <br> as evolutionary engines for online inter- <br> active collaborative game design. | Text for Game design | $\checkmark$ |
| Sudhakaran et al. [45] | MarioGPT: Open-Ended Text2Level <br> Generation through Large Language <br> Models  | Text for Game design |  |

operators and evolutionary approaches. On Python and C++ on the PSB2 benchmark [46] SEIDR outperforms the PushGP baseline and achieves the state-of-the-art result with 19 solved problems out of 25 with under 1000 program executions. While SEIDR is compared to GP in [5], only a simple GP variant is compared and the pre-training effort underlying the LLM is not considered in the comparison.

A variety of typical simple GP problems (e.g. Simple agent controllers, Symbolic Regression, OneMax) are investigated by Bradley et al. [3]. They introduce OpenELM, an open-source Python library for designing evolutionary algorithms that leverage LLMs to generate variation, as well as to assess fitness and measures of diversity. Many of the same problems are investigated by Meyerson et al. [38]. They input text-based genotypes that are either code, plain-text sentences, or equations to the LLM. They then interpret and use the corresponding LLM responses as those genotypes' offspring. Their experiments highlight the versatility of LLM-based crossover and the success of the LLM-based approach.

Finally, a meta-heuristic approach that is not explicitly described as an EA, but that is similar in spirit, and which uses a LLM is introduced by Zelikman et al. [36]. An LLM is used to generate a scaffolding program that is iteratively improved.

## 2. Neural architecture search (NAS)

NAS using a Python code representation is investigated by Chen et al. [4]. They use LLM-based mutation and crossover operators. For prompt engineering they use a few shot learning technique and they fine tune with the sub-population that was not selected. In other NAS work Nasir et al. [40] use an LLM and a Quality-Diversity algorithm to obtain variations of Python code defining a neural architecture. Their system LLMatic create diverse networks that are high-performing.

## 3. Game Design

Game design is another problem domain where EA and LLM operators have been combined. Typically the genotype representation is text. Lanzi and Loiacono [44] present a collaborative game design framework that combines interactive evolution and LLMs to simulate the typical human design process. They use interactive evolution for selection and LLMs for a recombination and variation of ideas. Another game design example is from Sudhakaran et al. [45]. They use a fine-tuned GPT2 model to generate tile-based game levels. It is used for LLM based initialization and variation of prompt text that describes a game level.

## 4. Prompt Generation

Hand-crafted prompts are often sub-optimal. Promptbreeder [42] mutates a population of text-based ask-prompts, evaluates them for fitness on a training set, and repeats this process over multiple generations to evolve task-prompts. In a self-adaptive way, Promptbreeder uses its mutation-prompts to improve the task-prompts. Another work on discrete prompt optimization uses LLM-based evolutionary operators and Differential Evolution to improve a population based on the development set [41]. Finally, $\mathrm{Xu}$ et al. [43] starts with an initial set of instructions that they evolve and instruct the LLM to rewrite step by step into more complex instructions. The frequency of papers evolving prompts is likely due to the non-trivial nature of efficient prompt construction and the easy means with which an LLM handles text.

### 4.2 By Operators, Genotype and LLM designs

Next, across the works, we consider which LLM-based evolutionary operators are used, what genotype representations are evolved, and LLM-specific design decisions. LLMspecific design decisions fall into two classes: guiding the behavior of the LLM, and Prompt Engineering Techniques.

We observe LLM behavior being guided in two ways:

Raising Model Temperature LLMs have a temperature parameter. A low temperature makes prompt completion more deterministic. Raising the temperature leads to more variability in prompt completion. Some approaches raise the LLM temperature to obtain more diverse solutions.

Model Fine tuning Update the LLM parameters based on some data and training procedure, $\Theta^{\prime}=g(X, Y, \Theta)$. Note that when training data is generated by the model and the updated and fine-tuned model is used in an evolutionary run, the algorithm is self-adapting.

We observe selections from the following set of Prompt Engineering Techniques, (see also Section 2).

Zero-Shot A simple predefined prompt $\rho=T, T \in \mathcal{T}$

Template The prompt is a template that is expanded at runtime using run-time information, $\rho=\rho(\mathbf{x})$

Few-shot An extension of Template providing examples of correct responses, $\rho=$ $\rho(\mathbf{x}, X, Y),|X|=|Y|, X, Y \in \mathcal{T}$. Note, this is some times called in-context learning.

Chaining A sequence of LLM calls, $y_{i}=f\left(\rho_{i-1}\left(y_{i-1}\right)\right), y_{0}=f\left(\rho_{0}()\right), i \in \mathbb{Z}^{+}$. For example, chain-of-thought is a type of chaining that does not presume any external environment changes [47].

Summarization A combination of Template and Chaining $(i=2)$ where an individual is first summarized and then provided as input to a template.

Human Interaction A human interacts with the LLM to manipulate the prompts and responses, $\rho^{\prime}, y^{\prime}=H(\rho, f(\rho \mid \Theta))$

Optimization A prompt's content is (externally) optimized according to some function $u, \rho^{\prime}=g(\rho, u(f(\rho \mid \Theta))), u: \mathcal{T} \rightarrow \mathbb{R}, v=u(\rho)$

All papers use the LLM generative capabilities for variation, mutation and crossover. A majority also use it for solution initialization, and a minority use it to measure fitness. This includes a case where the diversity of solutions is measured through string embedding similarity. We observe that a wide variety of prompt engineering techniques are used. The most popular technique is Template use. This seems like an obvious approach to integrating run-time information such as genotypes and fitness scores. Second is raising the temperature which increases solution diversity. In order of popularity, are: 1) Template (8) 2) Changing Temperature (3) 3) Chaining (2) 4) Human Interaction (2) 5) Few-shot (2) 6) Zero-shot (2) 7) Summarization (2) 8) Optimization (1). Table 2 summarizes.

## 5 Demonstration of a simple LLM_GP variant

In Section 5.1 we describe an implementation of LLM_GP within a package and code. In Section 5.2 we describe an experimental setup. In Section 5.3 we describe, analyze and discuss experimental results.

### 5.1 LLM_GP implementation

Section 5.1.1 describes keyLLM_GP elements within the package. Section 5.1.2 describes the package's keyLLM_GP components.

### 5.1.1 The Tutorial-LLM_GP Package

Tutorial-LLM_GP design priority is simplicity and readability in order to help a broad range of learners. It is implemented as a Python package of three modules: Algorithms, Problem Environments and Utilities.

Table 2: Details of LLM operators, genotype representation and prompt engineering for related work on LLMs and EA. Use and prompting refers to LLM model manipulation and prompt engineering techniques.

| Cite $\mathbf{N r}$ | LLM Operators | Genotype |  | Use \& Prompting |
| :---: | :---: | :---: | :---: | :---: |
|  |  | Code Text | Bits |  |
| Code Evolution |  |  |  |  |
| [5] | Mutation, <br> Initialization | $\checkmark$ |  | Template, <br> Changing temperature, <br> Chaining, <br> Summarization |
| $[36]$  | Mutation, <br> Fitness Measure | $\checkmark$ |  | Optimization |
| $\left.\begin{array}{llll}\end{array} 37\right]$ | Mutation | $\checkmark$ |  | Template, <br> Fine Tuning |
| [3] | Mutation, <br> Initialization, <br> Crossover, <br> Fitness Measure (QD) | $\sqrt{ }$ | $\sqrt{ }$ | Template |
| $[38]$ | Mutation, <br> Initialization, <br> Crossover, <br> Fitness Measure (QD) | $\checkmark$ | $\bar{v}$ | Few-shot |
| $[39]$ | Mutation | $\checkmark$ |  | Template, <br> Summarization, <br> Human interaction |
| $[4]$   | Mutation, <br> Initialization, <br> Crossover | $\checkmark$ |  | Fine tuning, <br> Few-shot, <br> Changing temperature |
| $[40]$ | Mutation, <br> Initialization | $\bar{v}$ |  | Zero Shot, <br> Changing Temperature |
| Text Evolution |  |  |  |  |
| [41] | Initialization, <br> Mutation, <br> Crossover | $\checkmark$ |  | Template |
| [42] | Initialization, <br> Mutation | $\checkmark$ |  | Template, <br> Chaining |
| [43] | Mutation | $\checkmark$ |  | Template |
| $[44]$  | Mutation, <br> Initialization, <br> Crossover | $\checkmark$ |  | Template, <br> Human interaction |
| $[45]$ | Mutation, <br> Initialization, <br> Crossover | $\checkmark$ |  | Zero Shot |

Algorithms includes three algorithms, each in a separate file, Evolutionary_Algorithm, Tutorial_GP and Tutorial_LLM_GP. Each algorithm is in a file that imports common data and EA operators.

Problem Environments includes prompt-functions and strings for the prompt template of each LLM-based operator for an example use case - Simplified Symbolic Regression.

Utilities includes utilities for interfacing with LLMs and running and analyzing experiments.

### 5.1.2 Tutorial-LLM_GP Design

Key design features of LLM_GP are:

Its evolutionary unit (field genotype of structure Individual) is a symbolic expression for Symbolic Regression

Its Prompt Engineering uses templates and few-shot learning.

LLM API, the OpenAIInterface, is a class that interfaces with OpenAI's GPT3.5-turbo model. It relies on code provided in OpenAI cookbooks for interacting efficiently with the web API, see Figure 6. It also records the input and output to the API. In addition, it tries to reconnect with exponential back-off when exceptions are thrown from the API.

Extra Error handling for LLM timeouts or incorrectly formatted responses. Exceptions from LLM operations are stored and as a fall-back the default LLM operator behavior is executed. E.g. default phenotype, fitness, random selection and no variation.

Extra Logging generation_history stores each LLM API call and response, as well as statistics regarding number of tokens and response time. These are essential for debugging.

```
// Wrap call in a function that retries with exponential back-off
@retry_with_exponential_backoff
def predict_text_logged(self, prompt: str, temp: float=0.8) -> Dict[str, Any
    ]:
    n_prompt_tokens = 0
    n_completion_tokens = 0
    start_query = time.perf_counter()
    content = "-1"
    message = [{"role": "user", "content": prompt}]
    // Get response from gpt-3.5-turbo
    response = openai. ChatCompletion.create(
        model="gpt-3.5-turbo", messages=message, temperature=temp
    // Logging information
    n_prompt_tokens = response["usage"]["prompt_tokens"]
    n_completion_tokens = response["usage"]["completion_tokens"]
    content = response["choices"][0]["message"]["content"]
    end_query = time.perf_counter()
    response_time = end_query - start_query
    return {
        "prompt": prompt,
        "content": content
        "n_prompt_tokens": n_prompt_tokens,
        "n_completion_tokens": n_completion_tokens,
        "response_time": response_time ,
    }
```

Fig. 6: Example LLM API implementation in Python.

### 5.2 Setup

Experimental resources are listed in Table 3.

For baseline algorithms, we include random GP-like explicit generation of solutions, Random, and Tutorial GP from the package. We also directly prompt the LLM to generate random solutions, a method we call LLM. We explore two LLM_GP variants. One, LLM_GP, uses a LLM in all its evolutionary operators except its fitness measure.

Table 3: Experiment resource descriptions.

| Resource | Description |
| :--- | :--- |
| Operating system | Ubuntu 22.04 LTS |
| RAM | $64 \mathrm{~GB}$ |
| CPU | Intel i7-8700K 3.70GHz |
| Budget | 50 USD |
| Max runtime | 60000 seconds |
| Fitness Evaluations (FE) | 300 |
| LLM version | gpt-3.5-turbo-0613 |
| Token max size $\left(\mathcal{T}^{n}\right)$ | 4,096 |

The second, LLM_GP_Mu_XO, only uses a LLM in its initialization, crossover, and mutation operators.

We use the experimental parameters listed in Table 4.

Table 4: Experiment settings. Note the limits to population size is due to the LLM input and output buffer size, $\mathcal{T}^{n, m}$. Limit to generations is due to LLM query time and budget. LLM operators use Few Shot examples for prompt engineering.

| Parameter | Tutorial GP | LLM_GP_Mu_XO | LLM_GP |
| :---: | :---: | :---: | :---: |
| Runs | 30 |  |  |
| Crossover probability | 0.8 |  |  |
| Mutation probability | 0.2 |  |  |
| Population size | 10 |  |  |
| Generations | 30 |  |  |
| Primitives | $+,-,{ }^{*}, x_{0}, x_{1}, 1,0$ |  |  |
| Solution |  |  |  | {$x_{0}^{2}+x_{1}^{2}$ <br> 0.2 Hold-out, (0.7 Training, 0.3 Testing)} |  |  |
| Exemplar splits |  |  |  |  |  |
| Exemplars | 121 | 10 |  |  |  |  |
| Few shot exemplars | NA | 2 |  |  |  |  |
| Mutation | Subtree | See Appendix C |  |  |  |  |
| Crossover | Subtree | See Appendix C |  |  |  |  |
| Initialization | Ramped-Half-Half | See Appendix C |  |  |  |  |
| Max Depth | 5 |  |  |  |  |  |
| Selection | {Tournament <br> 2 <br> Generational <br> 1} |  | See Appendix C |  |  |  |
| Tournament size |  |  | NA |  |  |  |
| Replacement |  |  | See Appendix C |  |  |  |
| Elite size |  |  | NA |  |  |  |

### 5.3 Analysis

This section analyzes the demonstration. Section 5.3.1 analyzes run duration and cost. Section 5.3.2 analyzes solution size and runtime. Section 5.3.3 analyzes LLM usage. Section 5.3.4 analyzes LLM operation errors.

### 5.3.1 Time \& Cost Analysis

Results for a run is in Table 5. Each run used 300 FEs. LLM operators are orders of magnitude slower and costlier than Tutorial GP. LLM_GP takes the
longest. LLM_GP_Mu_XO is slightly faster due to fewer LLM calls. As expected LLM_GP_Mu_XO takes less time than LLM_GP, due to fewer LLM calls by not using LLM for selection and replacement. Note that Tutorial GP is faster than random due to the caching of fitness evaluations.

Table 5: Cost and runtime (seconds) results for compared methods all solving Simple Symbolic Regression. Average over 30 runs.

| Name | Mean Duration (seconds) | STDEV | Cost (USD) |
| :--- | ---: | ---: | ---: |
| LLM | 837.16 | 416.12 | 2.63 |
| LLM_GP | 1664.30 | 1033.97 | 3.90 |
| LLM_GP_Mu_XO | 743.31 | 508.70 | 1.87 |
| Tutorial GP | 0.10 | 0.08 | 0.00 |
| Random | 0.18 | 0.01 | 0.00 |

### 5.3.2 Size Analysis

Figure 7a shows average solution size over generations of a run. For LLM_GP mean solution size increases. With LLM_GP_Mu_XO mean size increase up to generation 15 and then it stabilizes. Tutorial GP fluctuates and the size is larger. Note, that there is a solution simplification step for the representation for the LLM based methods, but not for Tutorial GP. LLM only has the longest solutions.

Figure $7 \mathrm{~b}$ shows duration of each generation (final generation is the total duration of the experiment). LLM_GP takes the longest, as expected given that it has the most calls to the LLM API. Note that the LLM execution time includes API service restrictions and networking limitations. LLM_GP_Mu_XO has fewer LLM calls, thus has shorter runtime than LLM_GP. The GP runtime is several orders of magnitude lower.

### 5.3.3 LLM Usage Analysis

Figure 8 shows statistics on LLM usage. We observe that LLM_GP, see Figure 8a, has the largest number of prompt tokens, completion tokens and response time. As expected, the LLM based operators for selection and replacement use the most tokens and LLM time.

Initialization, mutation and crossover use fewer tokens and LLM time. In Figure 8b we can more clearly see that these LLM operators also behave as expected regarding number of prompt tokens, number of completion tokens and response time, i.e. initialization has shortest prompt (it asks for an individual) and crossover prompt contains examples and two parents (it asks for two children). Note, the observed linear relationship between completion tokens and response time might be an artifact from the LLM API service.

![](https://cdn.mathpix.com/cropped/2024_06_04_4aac975f5c7b1f416258g-19.jpg?height=872&width=1134&top_left_y=252&top_left_x=421)

(a) Simple Symbolic Regression solution sizes over generations.

![](https://cdn.mathpix.com/cropped/2024_06_04_4aac975f5c7b1f416258g-19.jpg?height=845&width=1130&top_left_y=1211&top_left_x=423)

(b) Duration (s) of each generation.

Fig. 7: Solution size and duration per generations
![](https://cdn.mathpix.com/cropped/2024_06_04_4aac975f5c7b1f416258g-20.jpg?height=1762&width=1038&top_left_y=267&top_left_x=560)

Operation

- Initialize population
- Selection
- Mutation

Replacement

(a) LLM_GP

![](https://cdn.mathpix.com/cropped/2024_06_04_4aac975f5c7b1f416258g-20.jpg?height=797&width=802&top_left_y=1166&top_left_x=570)

(b) LLM_GP_Mu_XO

Fig. 8: LLM operation scatter plots and histograms of number of tokens in query, response (Completion tokens) and response time. Color indicates the LLM operation

![](https://cdn.mathpix.com/cropped/2024_06_04_4aac975f5c7b1f416258g-21.jpg?height=805&width=1264&top_left_y=263&top_left_x=356)

Fig. 9: Error types for each method. X-axis error rate. Y-axis is the error type.

### 5.3.4 LLM Operation Errors

Prompts are important, as well as the LLM that use the prompts for inference. We observe that some LLM based operators generate fewer errors than others. Figure 9 shows the error rate for the different variants and operations. The highest error rate is for selection. Replacement also throw errors. Both of these are most likely due to the prompt size. The crossover also throws errors, this can be from the requirement that the symbols of the parents should be reduced. Note we would expect even more errors if we tried to enforce the standard GP subtree crossover constraint of subtree swaps as well.

The fitness evaluation errors are due to malformed expressions from the LLM that the response formatting did not catch. Mutation results in very few errors. Initialization results in no errors, so this was a robust LLM operator. There are also errors from the LLM API service.

## 6 Discussion

In Section 6.1 we compare LLM_GP and GP. In Section 6.2 we discuss risks and other implications of using an LLM specifically to evolve code. In Section 6.3 we discuss why LLM_GP investigations should continue despite the risks. In Section 6.4 we suggest guidance for conducting LLM_GP investigations. Finally, in Section 6.4 we state some LLM_GP research directions.

### 6.1 LLM_GP vs GP

We summarize GP and LLM_GP differences in Table 6. Both GP and LLM_GP are EAs where the evolving unit is code. Both execute with the same procedural logic and operators of an EA. Both represent code with symbols naming problem primitives. But GP relies upon the code being represented by a structure which allows its execution and explicit, manipulation through variation operators that enforce syntactic correctness. For example, Koza-style GP works with code in a parse tree structure, it executes the tree structure with an internal code interpreter, and it uses sub-tree exchange for crossover. In contrast LLM_GP works with a code snippet that is tokenized and acceptable for input to a LLM. It is able, if desired, to directly encode the snippet using a common programming language While it could task the LLM to execute the code snippet, it very reasonably generally uses an external interpreter. Unlike GP, it forgoes explicit structural manipulation of the code during variation. Instead, it tasks the LLM to recombine, or mutate code by using a prompt, and the only control it exercises over how the operation is accomplished is through the prompt formulation. The LLM - a complex, pre-trained, pattern-based sequence completion system, that almost deceptively seems to understand language meaning while it truly does not, is effectively otherwise a black box [48, 49].

Regardless of the LLM being a black box, when prompted with a variation operators' task, it can demonstrate startling capabilities that allow it to potentially respond, on average, with a code snippet that is a better adaptation than adaptations the equivalent GP operator generates. When these capabilities arise, a LLM_GP run is going to solve a problem with fewer fitness evaluations than GP. The power of the LLM in this variation task capacity has been demonstrated, e.g. with a LLM_GP variant solving more of the GP community's program synthesis benchmarks [5]. The same holds for the LLM_GP's selection operator. In GP, selection depends on a numerical comparison of fitness. In LLM_GP, selection need not follow GP's approach. Selection depends on a prompt that contains the set of selection candidates, other information about them (including fitness). The prompt directs a selection of some subset, based on a criteria expressed in natural language. Then, again, the black box, i.e. the LLM, is used to obtain a response and the LLM_GP algorithm logic has no further control. If and when the LLM, across many operator calls, selects more optimally than a GP selection operator, the LLM_GP run could converge to a solution with fewer fitness evaluations than that of a GP run.

### 6.2 Risks and other Implications of Using an LLM

We have just pointed out the obvious risk posed by LLM_GP's operators: the operator (and human designer) surrenders its explicit control over the details of its behavior. LLM_GP also faces other risks and challenges(some generally raised in Section 2):

- it seems that LLMs handle software engineering and coding better than facets of general intelligence, but the reason why is not known. This conundrum adds to the black box nature of the LLM.
- an algorithm's success depends on prompt composition, while the sensitivity of an LLM to a prompt's composition is unreliable. Sensitivity would have to be

Table 6: Comparison of GP and LLM_GP.

| Basis of Comparison | GP | LLM_GP |
| :---: | :---: | :---: |
| Computational mod- <br> el/environment <br> referenced by the code- <br> evolving system | Program execution model / environ- <br> ment | Program execution model/environ- <br> ment and LLM which is a gen- <br> erative pattern completion system <br> using token/sequence-based pattern- <br> matching with built-in patterns |
| Run of a code-evolving <br> system | A GP run executes procedural soft- <br> ware where the code is data, the <br> operators work on code structure, <br> and the code is bespoke evaluated <br> and assigned numerical fitness. | A LLM_GP run executes procedural <br> software that, among other things, <br> composes text-based NL prompts, <br> sends them as inputs to an LLM , <br> and collects responses. (Prompts and <br> responses at lowest level of descrip- <br> tion are sequences of tokens) |
| Code as desired solution <br> (genotype-phenotype <br> duality) | Genotype/phenotype is a data struc- <br> ture with structural properties, e.g. <br> tree, list, stack, and executability | Code is token sequence with code- <br> snippet meaning, it has no struc- <br> tural properties, and it has implicit <br> pattern-related properties related to <br> the patterns, patter-matching and <br> bias within the LLM |
| Evolutionary Variation | Structural, blind to meaning | Not structural, blind to user beyond <br> prompt content. Internal to LLM it <br> is based on built-in patterns and is a <br> black box. |
| Evolutionary Selec- <br> tion/Replacement | Comparative, based on numeric <br> ranking and fitness represented as a <br> number | Comparative, prompt could include <br> fitness, could task LLM to rank, <br> could include other bases of compar- <br> ison. Blind to use beyond prompt <br> content. Internal to LLM it is based <br> on built-in patterns and is a black <br> box. |
| Code evaluation | Uses bespoke execution environment <br> (supporting run's primitives) on top <br> of a general-purpose program execu- <br> tion environment | Practical implementations will use <br> a general-purpose program execution <br> environment |
| Code Fitness | Numeric-based | Numeric or expressed with natural <br> language |

probed and quantified, for each LLM independently. Further, LLMs lack many facets of general intelligence, while to some degree, they deceptively appear to understand prompts. This leads to the risk of mistakenly assuming understanding when it does not exist.

- all LLMs are fine-tuned to be aligned with goals. They display this bias. Neither evolution or coding are specific with any LLM's goals, but biases relating to evolution and coding emerge from pre-training. Some LLMs are specifically trained for coding: see LLama [33] family, but may not be specifically trained on evolution. The biases of these models are poorly characterized or understood.
- success also depends on the design (and aforementioned bias) of the LLM. A LLM is concisely described as a set of weights $\left(\theta_{L L M}\right)$ that have been pre-trained under design decisions of neural network architectures, training algorithms, and data sets. Precisely predicting the combined impact of all of these design decisions is not practically feasible because an LLM is probabilistic and generative. A LLM can be tested and its test results released to describe its capabilities, but there is no description of its precise behavior with new prompts and tasks.
- LLM pre-training is often out of the hands of LLM users and data sets for training and testing are frequently not well documented or openly shared. A researcher may not be able to ensure that the rote solution (and problem description) to the problem at hand is not within the training data upon which the model has pre-trained.
- when an LLM is used via a model-provider's API, experimental replicability is largely impossible.

Collectively these risks directly and significantly impact the quality of the science that can be conducted with a LLM_GP system. To cap this off, working with LLMs is non-trivial and resource intensive. This prompts a question ....

### 6.3 Why conduct LLM_GP investigations?

It is arguable that, despite a LLM-based EA system being less than ideal for scientific purposes, reports of investigations should be welcome and accepted for publication. A fundamental reason is that an LLM evolves code by drawing upon its pre-training on vast amounts of human-written code. One has to assume that a lot of programming knowledge and practice that code evolution could exploit, is embodied in this treasure trove of correct examples.

Another reason is to honor diverse approaches and recognize that any approach, in its early form, is going to be imperfect but may still be worthy of pushing its limits and improving it. No one knows precisely how LLM technology will advance and it is expedient that we become familiar with its advantages and limitations now - to either drive those advances or be set up for them when they arise.

Yet another reason is that working with LLMs stretches us intellectually and research should dig into their provocative novelty. LLMs offer a new computational paradigm, one working around pattern memory and matching. They do not offer a procedural abstraction. We, the GP community, are interested in the intelligence of Nature. We attend to nature's intelligent artifacts, including humanity and also social, collective, cooperative, competitive aspects of natural systems and organisms. How do the mechanisms of a LLM relate to memory mechanisms within Natural systems? Our community is also captivated by coding. Evolving code could lead to understanding the correspondence between an LLM's capabilities and Nature's mechanisms better. Could pattern completion competence be effectively similar to highly environmentally-sensitive, self-adapted variation operators in the natural world? Might it be a "code-evolving" scholar who discovers parallels between biological evolutionary mechanisms and an LLM? Might LLM_GP variants uncover insights into LLM capabilities that lead to advances in LLM design or usage, or GP approaches? Our community may also encounter answers as to how and why LLMs retrieve code better than language.

We have stated the LLM_GP investigations skirt scientific method boundaries, but argued that there are still strong reasons to conduct them. In this case, how should investigations proceed?

### 6.4 Conducting LLM_GP Investigations

To date there are only 13 reports of LLM_GP of which only two are peer-reviewed. GP standards and norms started from a primitive state, i.e. they didn't exist on Day 1 and still evolve. Therefore, we could assume that committees and publication forums will provide the necessary encouragement for nascent work even with nascent standards. Here we suggest some possible initial standards:

Reporting :

- report the preparatory steps clearly.
- report time and cost of prompting during a run.
- report any biases beyond pre-training.
- probe prompt sensitivity. If possible, also probe different LLMs.
- maintain independent leaderboards on a benchmark for each of the GP and LLM_GP approaches.
- try to pin down and report the exact model version along with its pre-training costs, its training data and its fine-tuning.

Methods :

- check the problem and solution are not in the data set!
- compare an LLM-based approach against other LLM-based approaches when using a community benchmark. Consider whether (or not) it makes sense to compare with GP.
- make well-aligned comparisons (apples to apples, not apples to oranges). GP costs are incurred on different bases from LLM_GP. Fitness evaluations dominate running cost so comparison among GP variants can be on the basis of the same number of fitness evaluations. But LLM_GP runs rely on a pre-trained model. The community needs a way of reconciling this cost when comparing. And, the costs related to prompt response time and token cost have no GP equivalent. The asymmetry remains to be addressed.
- address how much human intelligence has gone into solving the GP problem ahead of the LLM_GP run. How would this differ in the case of GP, has it changed? Is domain information (not evolutionary information) hidden in a prompt?

Integrity :

- be responsible with environmental cost. The budget devoted to investigation has the hidden expense of training an LLM. Multiple investigations with an open LLM could amortize its pre-training expense.
- use the LLM ethically and keep usage aligned with human values.

In the next section we propose some avenues for LLM_GP research.

## Research Questions for LLM_GP

While it is infeasible to list all interesting LLM_GP avenues for future research, we offer:

## Applications :

- How can LLM_GP integrate software engineering domain knowledge?
- How can LLM_GP solve prompt composition or other LLM development and use challenges?
- How can an EA using an LLM, but not necessarily evolving code, solve with different of units of evolution, e.g. strings, images, multi-modal candidates?

Algorithm Variants :

- How can we probe LLM_GP to understand the limits of its literal coding competence and more pragmatic coding competences?
- How can a LLM_GP algorithm integrate design explorations related to cooperation, modularity, reuse, or competition?
- How can a LLM_GP algorithm model biology differently from GP?
- How can a LLM_GP intrinsically, or with guidance, support open-ended evolution?
- What new variants hybridizing GP, LLM_GP and/or another search heuristic are possible and in what respects are they advantageous?
- Is there an elegant multi-objective optimization and many-objective optimization approach with LLM_GP?

Analysis Avenues :

- How well does LLM_GP scale with population size and problem complexity?
- What is a search space in LLM_GP and how can it be characterized with respect to problem difficulty?
- Does an LLM-based approach intrinsically address novelty or quality-diversity? To what extent, if so?
- What is the most accurate computational complexity of LLM_GP?


## 7 Conclusions \& Future Work

This paper probes the novelty around using LLMs to evolve code. It provides clarity: emerging from Algorithm 1 is a sharp description of LLM_GP: it is an evolutionary algorithm that solves code synthesis. Like GP, it uses a set of evolutionary operators. However, its operators for initialization, selection, and variation can interface with an LLM via prompts that are returned from prompt-functions that are part of the hyperparameters to the algorithm. Like GP, its unit of evolution is code, but unlike GP's use of a structure that allows both execution and variation, it represents code with text that is a code snippet. Emerging from an implementation and demonstration of execution, is the message that using an LLM to implement evolutionary operators incurs new costs: in a run, the time to interact with the LLM for each prompt, and the cost of prompting are significant. As well, a hidden, but significant cost is the pre-training of the LLM.

For practitioners, the paper provides a tutorial-level implemented variant of LLM_GP. It shows the reader hands-on prompt function signatures, examples of prompt-functions and prompts and the module that interfaces with an LLM. It explicates design decisions, new hyper-parameters and new preparatory steps. Finally, it contributes a discussion that up front itemizes the different risks and uncertainties arising when using an LLM to evolve code. It then argues nonetheless for pursuing LLM_GP, primarily to not cut off potentially new insights. It offers suggestions on
how to conduct and report LLM_GP investigations and, to end, it offers avenues of potential investigation.

## Statements and Declarations

The authors declare no competing interests.

The authors acknowledge funding for this work under US Government Contract \#FA8075-18-D-0008.

Conceptualization: Erik Hemberg and Una-May O'Reilly; Methodology: Erik Hemberg, Stephen Moskal and Una-May O'Reilly; Formal analysis and investigation: Erik Hemberg; Writing - original draft preparation: Erik Hemberg and Una-May O'Reilly; Writing - review and editing: Erik Hemberg, Stephen Moskal and Una-May O'Reilly; Funding acquisition: Una-May O'Reilly; Resources: Una-May O'Reilly; Supervision: Una-May O'Reilly.

## A Notation \& Definitions

For notation see Table 7 .

## B LLM Operators

## $i_{L L M}$, Initialization:

Formulates random candidate solutions using a prompt incorporating the primitives and instructions for formulating the solutions.

Interfaces to the LLM to execute the prompt and collect the response.

Checks response and reformulates if incorrect.

$e_{L L M}$, Execution

Formulates prompt with instructions to execute a solution in an execution context, i.e. with data inputs.

Interfaces to the LLM to execute the prompt (and evaluate solution within it) and collect the response.

Checks response, returning an empty string if format is incorrect.

Note, LLM are notorious for not being able to compute mathematically so for problems requiring this, this operator is not used.

$\phi_{\text {LLM }}$ Fitness measure

Formulates prompt with instructions to use a fitness measure to assess the quality of the response/output from a prior evaluation of a solution.

Interfaces to the LLM to execute the prompt (and execute the measure on the prior response) and collect the response.

Checks response. An incorrect evaluation returns a default value.

Note, LLM are notorious for not being able to compute mathematically so for problems requiring this, this operator is not used.

## $s_{\text {LLM }}$, Selection

Formulates prompt with instructions to select one solution in prompt over another. The prompt contains the solution and fitness for a list of individuals. The instructions

Table 7: Notation and description. Note for readability we assume the context implies the dimensionality of some parameters. Note some symbols are overloaded and the context clarifies the use.

| Symbol | Description |
| :---: | :---: |
| $\overline{L L M}$ |  |
| A | Actor |
| $\mathbf{x}$ | LLM Input. $\mathbf{x} \in \mathcal{T}$ |
| $\mathbf{y}$ | LLM output. $\mathrm{y} \in \mathcal{T}$ |
| $\Theta_{L L M}$ | LLM parameter and weights, $\Theta_{L L M} \in \mathbb{R}$ |
| $f$ | A Large Language Model is parameterized model that probabilistically outputs a <br> sequence of tokens, $f: \mathcal{T} \times \mathbb{R} \rightarrow \mathcal{T}, \mathbf{y}=f(\mathbf{x} \mid \theta)$. |
| $g$ | A prompt is a function that outputs a sequence of tokens, $g: \mathcal{T} \rightarrow \mathcal{T}, \mathbf{x}^{\prime}=g(\mathbf{x})$ |
| $G P$ |  |
| $\bar{g}$ | generations (iterations), $g \in \mathbb{N}$ |
| $n$ | population size (number of point samples), $n \in \mathbb{N}$ |
| $P^{*}$ | best solution, $P \in \mathcal{X}$ |
| $i$ | initialization function, $i: \mathbb{Z} \geq 0 \rightarrow \mathcal{X}, P=i(n)$ |
| $s$ | A selection function $s: \mathcal{X} \rightarrow \mathcal{X}, P^{\prime}=s(P), P^{\prime} \subseteq P$ |
| $v$ | A variation function $v: \mathcal{X} \rightarrow \mathcal{X}, P^{\prime}=v(P)$ |
| $\rho_{*} \quad$ | Prompt for $*, \rho \in \mathcal{T}$ |
| $c$ | A function for formatting an LLM response, $c: \mathcal{T}^{m} \times \mathcal{T} \rightarrow \mathcal{T}, r^{\prime}=c(r, \cdot)$ |
| $\mathbf{x}$ | A GP solution in the form of an expression tree, $\mathbf{x} \in \mathcal{G}, \mathcal{G}=(\mathcal{N}, \mathcal{E})$ |
| $n$ | A node in a GP tree. A node has a symbol $s$ and an outdegree (arity $a) n=(s, a), s \in$ <br> $\mathcal{S}, a \in \mathbb{Z} \geq$ |
| $s$ | A node symbol, $s \in \mathcal{S}$ |
| $a$ | A node arity, $a \in \mathbb{Z} \geq$ |
| $\|\mathbf{x}\|$ | Number of nodes (tree size) of a GP solution, $\mathbf{x} \in \mathbb{Z} \geq 1$ |
| $E$ | Executable environment, $E \in \mathcal{E}$ |
| $e$ | A function that evaluates a solution $e: \mathcal{P} \times \mathcal{E} \rightarrow \mathcal{Y}, \mathbf{y}=e(\rho, E)$ |
| $\phi$ | A measuring function of solution fitness (quality) $\phi: \mathcal{Y} \times \mathcal{E} \rightarrow \mathbb{R}, \mathbf{f}=\phi(\rho, E)$ |
| $D$ | Data, unlabeled or labeled |
| $r$ | Response from LLM $f$ |
| $\mathbf{a}$ | Prompt function arguments |

are to select $\boldsymbol{k}$ individuals from the list of individuals $\mathbf{p}$. Formally, $\left|\mathbf{p}^{\prime}\right|=\boldsymbol{k}, \mathbf{p}^{\prime} \in \mathbf{p}$. Interfaces to the LLM to execute the prompt and collect the response. Checks response. An incorrect response returns individuals randomly selected with replacement.

Note, the prompt size can be an error issue for this formulation. This is due to the size of an individual and the number of individuals (counted as tokens mathcalT.

$V_{x o L L M}$, Crossover:

Formulates prompt with instructions to recombine two parent solutions and instructions for combining them.

Interfaces to the LLM to execute the prompt and collect the response.

Checks response by extracting two child solutions.

In our implementation the prompt instructs the LLM to generate two new solutions that combine elements of the two solutions but it does not check whether all elements appear across the two new solutions.

$r_{L L M}$, Replacement

Formulates prompt with instructions to rank a set of given solutions and select the best.

Interfaces to the LLM to execute the prompt and collect the response.

Checks response.

$b_{L L M}$, Ranking solutions

Formulates prompt with instructions to rank a set of given solutions and select the best.

Interfaces to the LLM to execute the prompt and collect the response.

Checks response.

## C Prompts

## Prompt implementations

Prompts are in Python syntax. $\{$,$\} indicates variable substitution when formatting a$ string. """ indicates a string.

The primitives are the allowed GP symbols. Few shot samples are the taken from the current population.

## C.0.1 LLM_GP Few shot symbolic regression prompts using Primitives

```
SORT_POPULATION_PROMPT_FEW_SHOT = """ An example of an order is in the
following list: {samples}
Order the elements of the following list: {individuals}
Provide no additional text in response. Format output in JSON as
{{"individuals": ["<element>"]}} """
SELECTION_PROMPT_FEW_SHOT = """ {n_samples} examples of high quality
elements are: {samples}
Select {population_size} elements of high quality from the following
list: {individuals}
Provide no additional text in response. Format output in JSON as
{{"individuals": ["<element>"]}} """
REPLACEMENT_PROMPT_FEW_SHOT = SELECTION_PROMPT_FEW_SHOT
REPHRASE_MUTATION_PROMPT_FEW_SHOT = """ {n_samples} examples of
mathematical expressions are: {samples}
Rephrase the mathematical expression {expression} into a new
mathematical expression. Use the listed symbols {constraints}.
Provide no additional text in response. Format output in JSON as
{{"new_expression": "<new expression>"}} """
CROSSOVER_PROMPT_FEW_SHOT = """ {n_samples} examples of mathematical
expressions are: {samples}
Recombine the mathematical expressions {expression} and create
{n_children} new expressions from the terms. Use only the the existing
expression when creating the new expressions.
Provide no additional text in response. Format output in JSON as
```

```
{{"expressions": ["<expression>"]}} """
EVALUATION_PROMPT_FEW_SHOT = """ {n_samples} examples outputs from
mathematical expressions are: {samples}
Provide the output from the evaluation of the mathematical expression
{expression} on the following list of variables: {exemplars}
Provide no additional text in response. Format output in JSON as
{{"outputs": ["<output>"]}} """
FITNESS_MEASURE_PROMPT_FEW_SHOT = """ {n_samples} examples numerical
quality scores for mathematical expressions are: {samples}
Provide a numerical quality score based on the list of outputs from
expression {expression}: {outputs} When comparing it to the following
list of targets: {targets}
Provide no additional text in response. Format output in JSON as
{{"fitness": "<quality>"}} """
```


## C.0.2 LLM one symbolic regression prompt

```
PROMPT = """ Generate a mathematical expression for the following
variables. {exemplars}
Use the operators: {constraints}.
Provide no additional text in response. Format output in JSON as
{{"expression": "<expression>"}} """
```


## References

[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS'17, pp. 6000-6010. Curran Associates Inc., Red Hook, NY, USA (2017)

[2] Griffith, S., Subramanian, K., Scholz, J., Isbell, C.L., Thomaz, A.L.: Policy shaping: Integrating human feedback with reinforcement learning. Advances in neural information processing systems 26 (2013)

[3] Bradley, H., Fan, H., Galanos, T., Zhou, R., Scott, D., Lehman, J.: The openelm library: Leveraging progress in language models for novel evolutionary algorithms. In: Genetic Programming Theory and Practice XX. Springer, ??? (2024)

[4] Chen, A., Dohan, D.M., So, D.R.: Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838 (2023)

[5] Liventsev, V., Grishina, A., Härmä, A., Moonen, L.: Fully autonomous programming with large language models. arXiv preprint arXiv:2304.10423 (2023)

[6] O'Neill, M., Vanneschi, L., Gustafson, S., Banzhaf, W.: Open issues in genetic programming. Genetic Programming and Evolvable Machines 11, 339-363 (2010)

[7] O'Neill, M., Spector, L.: Automatic programming: The open issue? Genetic Programming and Evolvable Machines 21, 251-262 (2020)

[8] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys 55(9), 1-35 (2023)

[9] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)

[10] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners (2020)

[11] OpenAI: GPT-4 Technical Report (2023)

[12] Phuong, M., Hutter, M.: Formal algorithms for transformers. arXiv preprint arXiv:2207.09238 (2022)

[13] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of hallucination in natural language generation. ACM Comput. Surv. 55(12) (2023) https://doi.org/10.1145/3571730

[14] Strubell, E., Ganesh, A., McCallum, A.: Energy and policy considerations for modern deep learning research. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 13693-13696 (2020)

[15] Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021)

[16] Wu, C.-J., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K., Chang, G., Aga, F., Huang, J., Bai, C., et al.: Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems 4, 795-813 (2022)

[17] Kaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D.: Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12(6), 518-527 (2022)

[18] Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., Sedghi, H.: Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066 (2022)

[19] Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., DwivediYu, J., Joulin, A., Riedel, S., Grave, E.: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 (2022)

[20] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., Zhou, D.: Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022)

[21] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.: Self-Consistency Improves Chain of Thought Reasoning in Language Models (2023)

[22] Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., Chen, W.: Synthetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv preprint arXiv:2302.00618 (2023)

[23] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan, K.: Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601 (2023)

[24] Raji, I.D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., Denton, E.: Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing (2020)

[25] Appel, G., Neelbauer, J., Schweidel, D.: Generative ai has an intellectual property problem. april 07, 2023. Harvard Business Review (2023)

[26] Chen, L., Zaharia, M., Zou, J.: How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009 (2023)

[27] Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.: Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325 (2023)

[28] Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A.C., Korbak, T., Evans, O.: The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288 (2023)

[29] Moskvichev, A., Odouard, V.V., Mitchell, M.: The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain (2023)

[30] Ding, Z., Srinivasan, A., MacNeil, S., Chan, J.: Fluid transformers and creative analogies: Exploring large language models' capacity for augmenting cross-domain analogical creativity. In: Proceedings of the 15th Conference on Creativity and

[31] On Evaluating Understanding and Generalization in the ARC Domain. https: //aiguide.substack.com/p/on-evaluating-understanding-and-generalization. Accessed: 2023-10-27

[32] Connectionists: Chomsky's apple. https://mailman.srv.cs.cmu.edu/pipermail/ connectionists/2023-March/039546.html. Accessed: 2023-10-27

[33] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.: Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023)

[34] Preparatory Steps of Genetic Programming. http://www.genetic-programming. com/gppreparatory.html. Accessed: 2023-10-27

[35] Ling, T., Chen, L., Lai, Y., Liu, H.-L.: Evolutionary Verbalizer Search for Promptbased Few Shot Text Classification (2023)

[36] Zelikman, E., Lorch, E., Mackey, L., Kalai, A.T.: Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation (2023)

[37] Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., Stanley, K.O.: Evolution through large models. arXiv preprint arXiv:2206.08896 (2022)

[38] Meyerson, E., Nelson, M.J., Bradley, H., Moradi, A., Hoover, A.K., Lehman, J.: Language Model Crossover: Variation through Few-Shot Prompting (2023)

[39] Ma, Y.J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., Anandkumar, A.: Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv: Arxiv-2310.12931 (2023)

[40] Nasir, M.U., Earle, S., Togelius, J., James, S.D., Cleghorn, C.W.: Llmatic: Neural architecture search via large language models and quality-diversity optimization. ArXiv abs 2306.01102 (2023)

[41] Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., Yang, Y.: Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers (2023)

[42] Fernando, C., Banarse, D., Michalewski, H., Osindero, S., Rocktäschel, T.: Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution (2023)

[43] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Jiang, D.: Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023)

[44] Lanzi, P.L., Loiacono, D.: Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. arXiv preprint arXiv:2303.02155 (2023)

[45] Sudhakaran, S., González-Duque, M., Glanois, C., Freiberger, M., Najarro, E., Risi, S.: MarioGPT: Open-Ended Text2Level Generation through Large Language Models (2023)

[46] Helmuth, T., Kelly, P.: Applying genetic programming to psb2: the next generation program synthesis benchmark suite. Genetic Programming and Evolvable Machines 23(3), 375-404 (2022)

[47] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.: ReAct: Synergizing Reasoning and Acting in Language Models (2023)

[48] Webson, A., Pavlick, E.: Do prompt-based models really understand the meaning of their prompts? In: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2300-2344. Association for Computational Linguistics, Seattle, United States (2022). https://doi.org/10.18653/v1/2022.naacl-main. 167 . https://aclanthology.org/2022.naacl-main. 167

[49] Lipkin, B., Wong, L., Grand, G., Tenenbaum, J.B.: Evaluating statistical language models as pragmatic reasoners (2023)


[^0]:    ${ }^{1}$ A type of deep neural network, see [1].

[^1]:    ${ }^{2}$ Koza left out specifying the (problem-dependent) input-output examples needed for candidate execution and fitness evaluation.

