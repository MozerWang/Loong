# InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation 

Yaohui Wang Francois Bremond Antitza Dantcheva<br>Inria, Université Côte d'Azur<br>\{yaohui.wang, francois.bremond, antitza.dantcheva\}@inria.fr<br>https://wyhsirius.github.io/InMoDeGAN /

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-01.jpg?height=465&width=839&top_left_y=792&top_left_x=163)

(b)
![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-01.jpg?height=452&width=830&top_left_y=796&top_left_x=1032)

Figure 1: Controllable video generation. InMoDeGAN learns to decompose motion into semantic motion-components. This allows for manipulations in the latent code to invoke motion in generated videos that is human interpretable. Top (a) robot arm moves backwards, bottom (a) robot arm moves to the right. Similarly, in (b) we are animating the face to 'talk' (top) and 'move head' (bottom).


#### Abstract

In this work, we introduce an unconditional video generative model, InMoDeGAN, targeted to (a) generate high quality videos, as well as to (b) allow for interpretation of the latent space. For the latter, we place emphasis on interpreting and manipulating motion. Towards this, we decompose motion into semantic sub-spaces, which allow for control of generated samples. We design the architecture of InMoDeGAN-generator in accordance to proposed Linear Motion Decomposition, which carries the assumption that motion can be represented by a dictionary, with related vectors forming an orthogonal basis in the latent space. Each vector in the basis represents a semantic sub-space. In addition, a Temporal Pyramid Discriminator analyzes videos at different temporal resolutions. Extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the VoxCeleb2-mini and BAIR-robot datasets w.r.t. video quality related to (a). Towards (b) we present experimental results, confirming that decomposed sub-spaces are interpretable and moreover, generated motion is controllable.


## 1. Introduction

Generative Adversarial Networks (GANs) [13] have witnessed remarkable progress in image generation [5, 20, 21,
24, 27, 29, 54, 59]. Both conditional $[5,17,61]$ and unconditional $[21,22,38,33]$ generative models have amassed exceptional capacity in generating realistic, high-quality samples. Most recent advances in image generation have sought to 'dissect' [4] and 'steer' [18] GANs by identifying a correspondence of the 'inner-working' of GANs and semantic concepts in generated images. Inner-working in this context has been represented by neurons [4], as well as by latent representations [18, 39, 47] in pre-trained GANs, whereas semantic concepts have included the attributes gender and age in facial image generation [39], as well as camera pan and color changes in broader settings [18, 12].

Videos signify more complex data, due to the additional temporal dimension. While some research works showed early results in video generation [46,36, 42, 52], related interpretability is yet to be revealed. Such interpretability and hence steerability is of particular interest, as it would render video GANs highly instrumental in a number of downstream applications such as video editing [50] and data augmentation [44, 43]. Motivated by the above, we here consider the following question: Can we control and manipulate the complex visual world created by video GANs?

Contributions In order to answer this new and intricate question, we propose a new interpretable motion decomposing GAN for video generation, which we refer to as InMoDeGAN. In particular, we aim to interpret the latent
space of InMoDeGAN by finding sub-spaces, which are endowed with semantic meanings. Once such sub-spaces have been identified, manipulating the sub-spaces allows for targeted modification of generated videos. Specifically, we here place emphasis on interpreting and modifying motion. We note that the posed research question deviates from current efforts on interpreting appearance [18, 39, 47] in the latent space.

This new problem necessitates an original architecture, streamlined to (a) generate high-quality videos, as only then an analysis of interpretability is meaningful, as well as to (b) allow for analysis of the latent motion representation. Hence, we propose a new interpretable architecture that we design based on the assumption that motion can be decomposed into independent semantic motion-components. Therefore, we define the motion space by a linear combination of semantic motion-components which can reflect 'talking' and 'robot arm moving to the right'. We implement named decomposition via a motion bank in our generator. Once trained, InMoDeGAN allows for the incorporation/elimination of corresponding motion-components in the generated videos by activating/deactivating associated latent directions, see Fig. 1.

Towards (a) generating highly realistic videos, we design a two-stream discriminator, which incorporates an image discriminator, as well as a novel Temporal Pyramid Discriminator (TPD) that contains a number of video discriminators. The latter leverages on a set of temporal resolutions that are related to temporal speed. We show that while our proposed discriminator incorporates 2D ConvNets, it is consistently superior to 3D-discriminators. We evaluate proposed InMoDeGAN on two large datasets, namely VoxCeleb2-mini [30] and BAIR-robot [10]. In extensive qualitative and quantitative evaluation, we show that InMoDeGAN systematically and significantly outperforms state-of-the-art baselines w.r.t. video quality. In addition, we propose an evaluation framework for motion interpretability and proceed to demonstrate that InMoDeGAN is interpretable, as well as steerable. Finally, we provide experiments, where we showcase generation of both, higherresolution, as well as longer videos.

## 2. Related work

Image Generation. Recent image generation methods have witnessed considerable progress [5, 20, 51, 34]. Related to our context, notably StyleGAN [21] and specifically the revised StyleGAN2 [22] constitute currently the state-of-the-art in image generation. The related architecture incorporates modulation based convolutional layers, which re-introduce a latent code at different layers of the network. Alterations of the latent code correspond to particular manipulations in generated images. For example basic operations such as adding a vector, linear interpolation, and crossover in the latent space cause expression transfer, morphing, and style transfer in generated images.
Video Generation. While realistic video generation is the natural sequel of image generation, it entails a number of challenges related to complexity and computation, associated to the simultaneous modeling of appearance, as well as motion. Current video generation can be categorized based on their input data into two types, unconditional and conditional methods.

Unconditional video generation methods seek to map noise to video, directly and in the absence of other constraints. Examples of unconditional methods include VGAN [46], TGAN [36], MoCoGAN [42] and G ${ }^{3}$ AN [52]. VGAN was equipped a two-stream generator to generate foreground and background separately. TGAN firstly generated a set of latent vectors corresponding to each frame and then aimed at transforming them into actual images. MoCoGAN and $\mathrm{G}^{3} \mathrm{AN}$ decomposed the latent representation into motion and content, aiming at controlling both factors. We note that named methods have learned to capture spatio-temporal distribution based on shallow architectures. Such works predominantly focused on improving the quality of generated videos, rather than exploring interpretability of the latent space. While MoCoGAN and $\mathrm{G}^{3}$ AN disentangled content/appearance and motion, no further investigation on underlying semantics was provided. As opposed to that, our main goal in this paper is to gain insight into the latent space, seeking to dissect complex motion into semantic latent sub-spaces.

In contrast to unconditional video generation methods, conditional video generation methods aim at achieving videos of high visual quality, following image-to-image generation. In this context and due to challenges in modeling of high dimensional video data, additional information such as semantic maps [32, 50, 49], human keypoints [19, 55, 48, 7, 58, 49], 3D face mesh [60] and optical flow [26,31] have been exploited to guide appearance and motion generation. We note that given the provided motionprior, in such methods motion cannot be further controlled.

GAN Interpretation. In an effort to open the black box representing GANs, Bau et al. [4, 3] sought to associate neurons in the generator with the encoding of pre-defined visual concepts such as colors, textures and objects. Subsequent works [39, 12, 18, 47] proceeded to explore the interpretability of the latent space, seeking for latent representations corresponding to different semantics in generated images. Linear [39, 18] and non-linear [18] walks in the latent space enabled for semantic concepts in the generated images to be modified.

In this paper, we focus on unconditional video generation. Deviating from previous methods, our evolved architecture allows for high-quality video generation. We prioritize in InMoDeGAN the ability to interpret, control and manipulate motion in generated videos. We do so by instilling a-priori the generator with a motion representation module, which learns interpretable motion-components during training, rather than interpreting a-posteriori a pre-trained generator.

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-03.jpg?height=561&width=1740&top_left_y=237&top_left_x=165)

Figure 2: InMoDeGAN-architecture. InMoDeGAN comprises of a Generator and a two-stream Discriminator. We design the architecture of the Generator based on proposed Linear Motion Decomposition. Specifically, a motion bank is incorporated in the Generator to learn and store a motion dictionary $D$, which contains motion-directions $\left[d_{0}, d_{1}, \ldots, d_{N-1}\right]$. We use an appearance net $G_{A}$ to map appearance noise $z_{a}$ into a latent code $w_{0}$, which serves as the initial latent code of a generated video. A motion net $G_{M}$ maps a sequence of motion noises $\left\{z_{m_{t}}\right\}_{t=1}^{T-1}$ into a sequence $\left\{A_{t}\right\}_{t=1}^{T-1}$, which represent motion magnitudes. Each latent code $w_{t}$ is computed based on Linear Motion Decomposition using $w_{0}, D$ and $A_{t}$. Generated video $V$ is obtained by a synthesis net $G_{S}$ that maps the sequence of latent codes $\left\{w_{t}\right\}_{t=0}^{T-1}$ into an image sequence $\left\{x_{t}\right\}_{t=0}^{T-1}$. Our discriminator comprises an image discriminator $D_{I}$ and a Temporal Pyramid Discriminator (TPD) that contains several video discriminators $D_{V_{i}}$, leveraging different temporal speeds $v_{i}$ to improve generated video quality. While $D_{I}$ accepts as input a randomly sampled image per video, each $D_{V_{i}}$ is accountable for one temporal resolution.

## 3. Method

Our objective is to design an unconditional video generative model, which allows for interpretation of the latent space. While we firstly disentangle the latent space into appearance and motion, crucially, we hasten to interpret and modify the motion space. To do so, we decompose motion into semantic sub-spaces, which allow for control of generated samples.

Towards this objective, we propose in the generator $G$ a Motion bank (see Fig. 2), targeted to (a) generate high quality videos, as well as (b) learn and store semantic components. The architecture of $G$ is based on proposed Linear Motion Decomposition, which carries the assumption that motion can be represented by a dictionary with vectors forming an orthogonal basis. Each vector in the basis represents one semantic component. In addition, we propose a Temporal Pyramid Discriminator (TPD) which contains several video discriminators $D_{V_{i}}$, aiming to learn spatiotemporal distribution from different temporal resolutions.

### 3.1. Linear Motion Decomposition

We formulate unconditional video generation as learning a function $G_{S}$ that maps a sequence of latent codes $S=\left\{w_{t}\right\}_{t=0}^{T-1}, w_{t} \sim \mathcal{W} \subset \mathbb{R}^{N} \forall t$ to a sequence of images $V=\left\{x_{t}\right\}_{t=0}^{T-1}, x_{t} \sim \mathcal{X} \subset \mathbb{R}^{H \times W \times 3}$, such that $G_{S}\left(w_{t}\right)=x_{t}, \forall t \in[0, T-1]$, where $T$ denotes the length of the video. $S$ is obtained by mapping a sequence of noises $Z=\left\{z_{t}\right\}_{t=0}^{T-1}, z_{t} \sim \mathcal{Z} \subset \mathbb{R}^{N}$ into the $\mathcal{W}$ space. However, such mapping jointly learns appearance and motion, rendering $\mathcal{W}$ challenging to be interpreted. With respect to an interpretable $\mathcal{W}$, and in hindsight to our core objective, we propose to decompose motion into linear independent components.

Given a video of high visual quality and spatio-temporal consistency, we assume that motion between consecutive frames follows a transformation $\mathcal{T}_{t \rightarrow(t+1)}$, so that $G_{S}\left(w_{t+1}\right)=\mathcal{T}_{t \rightarrow t+1}\left(G_{S}\left(w_{t}\right)\right)$. Based on the idea of equivariance $[25,9,16]$, an alteration in the latent space causes a corresponding alteration in the output, consequently a transition $\tau_{t \rightarrow t+1}$ affecting the latent space results in $G_{S}\left(\tau_{t \rightarrow t+1}\left(w_{t}\right)\right)=\mathcal{T}_{t \rightarrow t+1}\left(G_{S}\left(w_{t}\right)\right)$.

Recent works [18,39] showed that for a given imagetransformation $\mathcal{T}$ such as shifting and zooming, there exists a vector $d$ in the latent space, which represents the direction of $\mathcal{T}$. By linearly navigating in this direction with a magnitude $\alpha$, a corresponding transformation $\mathcal{T}(G(w))=$ $G(w+\alpha * d)$ is witnessed in generated images.

Therefore, we assume that any transition $\tau_{t \rightarrow t+1}$ associated to $\mathcal{T}_{t \rightarrow t+1}$ can be represented as a composition of motion-directions in a motion dictionary $D=$ $\left[d_{0}, d_{1}, . ., d_{N-1}\right], d_{i} \in \mathbb{R}^{N}$. We constrain these motion directions to form an orthogonal basis, so that

$$
<d_{i}, d_{j}>= \begin{cases}0 & i \neq j  \tag{1}\\ 1 & i=j\end{cases}
$$

If these directions are interpretable, manipulating the magnitude of any direction should inflict a specific semantic change in the output, without affecting other directions. Therefore, in transformation $\mathcal{T}_{t \rightarrow t+1}$, the magnitude $A_{t}=$ $\left[\alpha_{t, 0}, \alpha_{t, 1}, \ldots, \alpha_{t, N-1}\right], \alpha_{t, i} \in \mathbb{R}$ will vary. Each $a_{t, i}$ denotes the magnitude pertained to the $i^{\text {th }}$ direction at time step $t$. Based on this, we define the linear motion decompo-
sition as following

$$
\begin{equation*}
\tau_{t \rightarrow t+1}\left(w_{t}\right)=w_{t}+\sum_{i=0}^{N-1} \alpha_{t, i} d_{i} \tag{2}
\end{equation*}
$$

where the transformation between consecutive frames is indicated as

$$
\begin{align*}
G_{S}\left(w_{t+1}\right) & =\mathcal{T}_{t \rightarrow t+1}\left(G_{S}\left(w_{t}\right)\right) \\
& =G_{S}\left(\tau_{t \rightarrow t+1}\left(w_{t}\right)\right) \\
& =G_{S}\left(w_{t}+\sum_{i=0}^{N-1} \alpha_{t, i} d_{i}\right) \tag{3}
\end{align*}
$$

The general term of $w_{t}$ is hence

$$
\begin{equation*}
w_{t}=w_{0}+\sum_{i=0}^{N-1} \sum_{j=0}^{t-1} \alpha_{j, i} d_{i}, t \in[1, T-1] \tag{4}
\end{equation*}
$$

So far, we have succeeded transferring learning $w_{t}$ from an unknown motion space into learning three variables from three sub-spaces which contain clear meanings, namely initial appearance code $w_{0}$, magnitude sequence $\left\{A_{t}\right\}_{t=1}^{T-1}$, as well as associated motion-directions $\left[d_{0}, d_{1} \ldots d_{N-1}\right]$. We proceed to elaborate on how we implement described linear motion decomposition in our architecture.

### 3.2. Generator

The initial latent code $w_{0}$ serves as a representation of appearance in the first and all following frames of an output video. At the same time, the vector $A_{t}$ represents a set of magnitudes associated to motion directions in a transition and hence is accountable for motion. Taking that into account, we decompose $\mathcal{Z}$ into two separated spaces $\mathcal{Z}_{\mathcal{A}}$ and $\mathcal{Z}_{\mathcal{M}}$, which represent appearance and motion, respectively. Hence $w_{0}$ is generated by mapping an appearance noise $z_{a} \sim \mathcal{Z}_{\mathcal{A}}$ using an appearance net $G_{A}$. $A_{t}$ is mapped from the motion noise $z_{m_{t}} \sim \mathcal{Z}_{\mathcal{M}}$ by a motion net $G_{M}$. In order to ensure temporal consistency in the latent space, we integrate a GRU [8] with its initial code set to be $z_{a}$ prior to the mapping. We note that $G_{A}$ and $G_{M}$ are two different 8 -layer MLPs.

Based on our linear motion decomposition, the motion dictionary $D$ is entitled to an orthogonal basis. We propose to find a matrix, with eigenvectors representing $d_{i}$. More specifically, we pre-define a matrix $M \in \mathbb{R}^{N \times N}$ and devise it trainable, updating it along with the parameters in the generator. $D$ is represented as the transpose of right singular vectors of $M, M=U \Sigma V^{T}$ and $D=V^{T}$. Each $d_{i}$ is an eigenvector of matrix $M^{T} M$ and is learned based on adversarial learning. Once trained, $M$ captures the motion distribution of the training dataset and decomposes it into $N$ independent directions. We show that some directions are interpretable and moreover can be manipulated, which results in related modifications of generated results, see Sec. 4.3. $M$ is initialized randomly and updated with other parameters in $G$ via back-propagation. We refer to $M$ and $D$ jointly as motion bank.

We adapt the architecture proposed by Karras et al. [22] in $G_{S}$. We note that $G_{S}$ serves as a rendering network, which incorporates a sequence of convolutional blocks aiming to up-sample a learned constant into high resolution images. In each block, convolutional layers are modulated by the respective input $w_{t}$, in order to learn different appearances. Each $w_{t}$ is computed according to Eq. 4 and serves as input to $G_{S}$ to generate related frame $x_{t}=G_{S}\left(w_{t}\right)$.

### 3.3. Discriminator

Temporal speed in videos has been a pertinent cue in action recognition $[11,56]$. We note that videos sampled at temporal speeds $v$, which represent temporal resolutions, provide a set of motion features. For this reason, we propose a Temporal Pyramid Discriminator (TPD) that leverages videos of different temporal resolutions in order to ensure high video quality in generation.

Principally, our discriminator follows the two-stream architecture of MoCoGAN [42] and $\mathrm{G}^{3} \mathrm{AN}$ [52]. We have a stream comprising an image discriminator $D_{I}$, as well as a stream incorporating the proposed TPD. While the input of $D_{I}$ is a randomly sampled frame, TPD accepts as input a full video sequence. TPD includes a number of video discriminators $D_{V_{i}}$, each $D_{V_{i}}$ is accountable for one temporal resolution.

Deviating from previous work $[42,52]$, we here propose to leverage 2D ConvNets in $D_{V}$ rather than 3D ConvNets. We apply time to channel (TtoC) to concatenate sampled frames in channel dimension, in order to construct a video sampled at speed $v_{i}$ into an image $V_{i}^{\prime} \in \mathbb{R}^{H \times W \times K}$, where $\frac{K}{3}$ denotes the number of sampled frames. We surprisingly find that such design can substantially improve the visual quality, while ensuring temporal consistency of generated videos. We report experimental results in Sec. 4.2.

### 3.4. Learning

We use non-saturating loss [13] with $\mathcal{R}_{1}$ regularization [28, 22] as our objective function following the setting of StyleGAN2 [22]. The loss of TPD combines the losses of each video discriminator $D_{V_{i}}$ in the pyramid, $\sum_{i=0}^{n-1} \mathcal{L}_{D_{V_{i}}}$. We optimize the network based on the full objective

$$
\begin{equation*}
\min _{G}\left(\lambda \sum_{i=0}^{n-1} \max _{D_{V_{i}}} \mathcal{L}_{D_{V_{i}}}+\max _{D_{I}} \mathcal{L}_{D_{I}}\right) \tag{5}
\end{equation*}
$$

where $n$ is a hyperparameter denoting the number of video discriminators to be used during training. We empirically identify appropriate $n$ values in our two datasets, see Sec. 4.2. $\lambda$ aims to balance the loss between $D_{I}$ and TPD.

## 4. Experiments and Analysis

We present extensive experiments, which include the following. In video quality evaluation, we quantitatively evaluate the ability of InMoDeGAN to generate realistic videos
and compare related results with four state-of-the-art methods for unconditional video generation. We then analyze the effectiveness of the proposed TPD. In addition, we provide an ablation study, which indicates the appropriate number of temporal resolutions for both datasets.

In interpretability evaluation, we aim to discover interpretable directions in the motion dictionary. Towards this, we propose a new evaluation framework that quantifies motion in generated videos based on optical flow. We show that directions in the motion dictionary, based on our proposed framework, are indeed semantically meaningful. Further, we demonstrate that generated videos can be easily modified by manipulating such directions. Notably, our model allows for controllable video generation based on pre-defined trajectories for different directions.

Finally, we conduct further analysis of high resolution generation, linear interpolation and go beyond training data to explore longer video generation.

Implementation details. We implement InMoDeGAN using PyTorch [35]. All experiments are conducted on 8 V100 GPUs (32GB) with total batch size 32 (4 videos per GPU). We use Adam optimizer [23] with a learning rate 0.002 and set $\beta_{1}=0.0, \beta_{2}=0.99$. Dimensions of $z_{a}$ and $z_{m}$ are set to be 512 and 256 , respectively. We pre-define to learn $N=512$ directions in the motion dictionary, the dimension of each direction is set to be 512 . $\lambda$ is set to be 0.5 for all experiments. In TPD, we use four time steps 1,3,5,7 to sample videos on VoxCeleb2-mini and three time steps $1,3,5$ on BAIR-robot. More implementation and training details are described in Sec. C.

### 4.1. Datasets and evaluation metric

we report evaluation results on following two datasets.

VoxCeleb2-mini. We construct a subset of VoxCeleb2 [30], which comprises of over 1 million videos pertaining to 6000 celebrities, talking in different real-world scenarios containing diverse complex motions (e.g., head moving, talking, camera zooming, etc.). As the original dataset includes redundant scenarios, we construct a new subset of 12000 videos, where we randomly select video sequences pertaining to 20 diverse videos per each of the 6000 subjects. We note that videos include large appearance diversity.

BAIR-robot [10]. The dataset incorporates a singleclass and depicts stationary videos of a robot arm moving and pushing a set of objects. We use the training set of this dataset which contains 40000 short videos.

Evaluation metric. We use video FID [15] to quantitatively evaluate visual quality and temporal consistency in generated videos. For the computation, we appropriate ResNeXt-101 [14] pre-trained on Kinetics-400 [6] as feature extractor and take features before last fully connected layer to compute the FID. We randomly sample 10000 videos to compute the values for each experiment.

### 4.2. Video quality evaluation

We firstly compare InMoDeGAN with four state-of-theart methods, namely VGAN, TGAN, MoCoGAN, as well as $\mathrm{G}^{3} \mathrm{AN}$. We generate videos pertained to named methods with spatial resolution of $64 \times 64$ and temporal length of 32 for VGAN and 16 for the other methods. Related FIDs are reported in Tab. 1. InMoDeGAN systematically outperforms other methods w.r.t. video quality by obtaining the lowest FID on both datasets. This is a pertinent prerequisite for latent space interpretation, as only highly realistic videos would allow for a meaningful interpretation. We show generated samples on our project website.

| Method | VoxCeleb2-mini | BAIR-robot |
| :---: | :---: | :---: |
| VGAN [46] | 38.13 | 147.23 |
| TGAN [36] | 23.05 | 120.22 |
| MoCoGAN [42] | 12.69 | 13.68 |
| $\mathrm{G}^{3}$ AN [52] | 3.32 | 1.58 |
| InMoDeGAN | $\mathbf{2 . 3 7}$ | $\mathbf{1 . 3 1}$ |

Table 1: Comparison of InMoDeGAN with four state-of-theart models. InMoDeGAN systematically and significantly outperforms other methods on both datasets w.r.t. FID. The lower FID, the better video quality.

Effectiveness of TPD. We replace the original 3D discriminators in VGAN, TGAN, MoCoGAN, as well as $\mathrm{G}^{3} \mathrm{AN}$ with TPD, maintaining all training configurations as in the previous experiment. We report FIDs related to original and proposed discriminators in all algorithms and both datasets in Tab. 2. We observe that TPD improves the results of all methods significantly and consistently. This confirms that videos sampled with a set of temporal resolutions contain different features, which are beneficial in the discriminator.

On a different but related note, we observe during training that models without image discriminator (VGAN and TGAN) tend to reach mode collapse rapidly on BAIR-robot (high FID in Tab. 2). This is rather surprising, as BAIRrobot constitutes the simpler of the two datasets, comprising videos of a robot arm moving, with a fixed background. The occurrence of very similar scenes might be the reason for the challenging distinguishing of real and fake spatial information in the absence of an image discriminator.

| Method | VoxCeleb2-mini |  | BAIR-robot |  |
| :---: | :---: | :---: | :---: | :---: |
|  | 3D | TPD | 3D | TPD |
| VGAN [46] | 38.13 | 16.33 | 147.23 | 93.71 |
| TGAN [36] | 23.05 | 21.24 | 120.22 | 120.04 |
| MoCoGAN [42] | 12.69 | 7.07 | 13.68 | 3.16 |
| $\mathrm{G}^{3}$ AN [52] | 3.32 | 2.98 | 1.58 | 1.50 |

Table 2: Evaluation of TPD. When replacing the initial 3D discriminator with TPD, the latter significantly and consistently improves the FID of all 4 state-of-art models for the VoxCeleb2-mini and BAIR-robot datasets.

In addition, we conduct an ablation study, seeking to
![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-06.jpg?height=718&width=1694&top_left_y=243&top_left_x=169)

(a) Mean and variance of $A_{\bar{t}}$.
![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-06.jpg?height=310&width=804&top_left_y=580&top_left_x=1039)

(b) Time v.s. $\alpha$

Figure 3: Analysis of $\alpha$. (a) Mean and variance bar charts, indicating top 10 motion-directions with highest values in $A_{\bar{t}}$. (b) Time v.s. $\alpha$. Each figure represents a video sample. We illustrate two samples from BAIR-robot (top) and two from VoxCeleb2-mini (bottom). Top 5 dimensions in $\alpha$ are plotted in different color.

determine the optimal number of temporal resolutions in TPD for both datasets. Associated results are reported in Tab. 3, which suggest that while for VoxCeleb2-mini, which contains complex motion, we achieve the lowest FID on four temporal resolutions, for BAIR-robot, which is simpler w.r.t. occurring motion, three resolutions suffice.

| TPD type | VoxCeleb2-mini | BAIR-robot |
| :---: | :---: | :---: |
| $D_{V_{0}}, D_{V_{1}}, D_{V_{2}}, D_{V_{3}}$ | $\mathbf{2 . 3 7}$ | 1.56 |
| $D_{V_{0}}, D_{V_{1}}, D_{V_{2}}$, | 2.65 | $\mathbf{1 . 3 1}$ |
| $D_{V_{0}}, D_{V_{1}}$ | 2.76 | 1.33 |
| $D_{V_{0}}$ | 2.84 | 1.58 |

Table 3: Ablation study on video discriminators in TPD. Number of video discriminators associated to temporal resolutions. FID is reported for comparison. Lower FID indicates a superior quality of generated videos.

### 4.3. Interpretability evaluation

Above, we have provided experimental proof that InMoDeGAN is able to generate high quality videos. In this section, we focus on discussing, how to leverage those videos to find interpretable directions in the motion dictionary. Towards this, firstly we analyze $\alpha$, seeking to find directions with highest impact.

Then, we present our proposed evaluation framework for quantifying motion, in order to find semantic meaning of such directions. Next, we show generated results based on manipulation of such directions. Finally, we demonstrate that our model allows for controllable generation by navigating in found interpretable directions in pre-defined trajectories.
Do all directions contribute equally? As per Eq. 4, each $\alpha_{j, i}$ indicates the magnitude of $d_{i}$ at time step $j$. We sample 10000 latent codes as evaluation set and compute mean and variance over time, for the full set, in order to obtain $A_{\bar{t}}=\left[\alpha_{\bar{t}, 0}, \alpha_{\bar{t}, 1}, \ldots, \alpha_{\bar{t}, N-1}\right], \alpha_{\bar{t}, i} \in \mathbb{R}$. Fig. 3a shows mean and variance values of the 10 most pertinent dimensions in $A_{\bar{t}}$ for both datasets. We note that for both datasets, $\alpha_{\bar{t}, 511}$ has the largest variance, which indicates that $d_{511}$ leads to the strongest motion variation in generated videos. At the same time, $\alpha_{\bar{t}, 1}$ (BAIR-robot) and $\alpha_{\bar{t}, 0}$ (VoxCeleb2-mini) encompass highest mean values, respectively. Therefore, we have that $d_{1}$ (BAIR-robot) and $d_{0}$ (VoxCeleb2-mini) show high and continuous magnitudes, respectively.

Moreover, we are interested in the course of each $\alpha_{j, i}$ over time, which we portray in Fig. 3b. Specifically, we randomly select two samples per dataset and highlight a set of $\alpha_{0: 15, i}$ in different colors. We have that, while $\alpha_{0: 15,511}$ (in red) has the largest amplitude in both datasets, $\alpha_{0: 15,1}$ (BAIR-robot) and $\alpha_{0: 15,0}$ (VoxCeleb2-mini) (in blue) maintain high but steady values over time, respectively. This supports our findings, as displayed in Fig. 3a.

Based on the above, we conclude that directions in the motion dictionary do not contribute equally in composing motion.

Are motion components interpretable? We here aim to semantically quantify motion directions by a novel framework using optical flow. Firstly, we represent the optical flow according to the Middlebury evaluation [2]. Specifically, we partition the flow into four histogram bins, namely $R_{0}, R_{1}, R_{2}$ and $R_{3}$, to cover the $360^{\circ}$ range of orientation and amplitude, see Fig. 5. While different motion directions are represented by the hue values, motion magnitude is indicated by the brightness. Hence each $R_{i}$ represents a motion

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-07.jpg?height=532&width=1743&top_left_y=257&top_left_x=164)

Figure 4: Directions analysis on BAIR-robot. A generated video sample, related optical flow images (top), activation of only $d_{1}$ (middle), and activation of only $d_{511}$ (bottom). Optical flow images indicate that $d_{1}$ is accountable for moving the robot arm backward, whereas $d_{511}$ for moving it left and right.

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-07.jpg?height=209&width=200&top_left_y=974&top_left_x=190)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-07.jpg?height=203&width=206&top_left_y=972&top_left_x=385)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-07.jpg?height=211&width=200&top_left_y=973&top_left_x=583)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-07.jpg?height=209&width=111&top_left_y=974&top_left_x=844)

(d)
Figure 5: Optical flow quantization. (a) Middlebury colorwheel, (b) $\lambda\left(x_{t, j}\right)$ and $\mathrm{H}$ on the colorwheel, (c) one frame from BAIRrobot and (d) related optical flow.

range. Next, for any given optical flow video, we quantify motion in $R_{i}$ as following.

$$
\begin{equation*}
\phi_{i}=\frac{1}{N_{i}} \sum_{t=0}^{T-1} \sum_{j=0}^{N-1} \frac{\lambda\left(x_{t, j}\right)}{H} \mathbb{1}_{R_{i}}\left(x_{t, j}\right), i \in\{0,1,2,3\} \tag{6}
\end{equation*}
$$

with total motion in the video being computed as

$$
\begin{equation*}
\Phi=\frac{1}{N} \sum_{i=0}^{3} \sum_{t=0}^{T-1} \sum_{j=0}^{N-1} \frac{\lambda\left(x_{t, j}\right)}{H} \mathbb{1}_{R_{i}}\left(x_{t, j}\right) \tag{7}
\end{equation*}
$$

where $x_{t, j}$ denotes the value of the $j^{\text {th }}$ pixel at time step $t$ in an optical flow video, which contains $N$ color pixels in total. $N_{i}$ denotes the total number of color pixels in $R_{i}$. $\lambda\left(x_{t, j}\right)$ measures the distance from $x_{t, j}$ to the center of the colorwheel, whose radius is $H$ (see Fig. 5). A large $\phi_{i}$ indicates a frequent and strong appearance of motion associated to $R_{i}$.

For BAIR-robot, we proceed to evaluate the set of directions $d_{1}, d_{2}, d_{116}$ and $d_{511}$, as they exhibit the highest impact according to Fig. 3a. Our idea is to quantify the motion difference $\Delta \phi_{i}=\phi_{i}^{d_{k}}-\phi_{i}$ in each $R_{i}$, when $d_{k}$ is deactivated (set $\alpha_{k}=0$ ) in original videos.

We sample 1000 videos and deactivate each of the chosen directions, respectively, building an evaluation dataset containing 6000 samples ( 1000 original +5000 deacti- vated). We report averaged $\phi_{i}$ over the full evaluation set for each region in Tab. 4. When $d_{1}$ is deactivated, motion in $R_{0}$ and $R_{3}$ are strongly reduced. Similarly for $d_{511}, \phi_{1}$ and $\phi_{2}$ obtain the largest decline. We note that for some directions motion changes are minor. As $\left(R_{0}, R_{3}\right)$ and $\left(R_{1}, R_{2}\right)$ are opposite regions, $d_{1}$ and $d_{511}$ represent symmetric motions. To illustrate this, we generate samples by only activating $d_{1}$ and only activating $d_{511}$, respectively, while maintaining other directions deactivated. Fig. 4 shows one sample and related optical flow, from which we deduce that the results match our quantitative evaluation, which suggested that $d_{1}$ represents 'robot arm moving back and forth', and $d_{511}$ represents 'robot arm moving left and right'.

|  | $\Delta \phi_{0}$ | $\Delta \phi_{1}$ | $\Delta \phi_{2}$ | $\Delta \phi_{3}$ |
| :---: | :---: | :---: | :---: | :---: |
| $d_{1}$ | $\mathbf{- 0 . 0 0 8}$ | 0.017 | 0.002 | $\mathbf{- 0 . 0 3 3}$ |
| $d_{2}$ | -0.001 | 0.002 | 0.002 | -0.005 |
| $d_{116}$ | 0.000 | -0.001 | 0.001 | 0.000 |
| $d_{511}$ | 0.007 | $\mathbf{- 0 . 0 8 7}$ | $\mathbf{- 0 . 0 5 9}$ | 0.019 |

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-07.jpg?height=41&width=830&top_left_y=1741&top_left_x=1062)
( $R_{0}, R_{1}, R_{2}, R_{3}$ ) caused by deactivating motion-directions.

VoxCeleb2-mini comprises a more complex dataset than BAIR-robot. Related videos contain concurrent global motion (e.g. head moving, camera zooming), as well as local motion (talking). For VoxCeleb2-mini we therefore analyze global and local motion by focusing specifically on head and mouth regions, computing facial semantic maps, and further head-flow and mouth-flow videos for each sample (see Fig. 7). We use the method of Yuet al. [57] to extract facial semantic maps.

For VoxCeleb2-mini we proceed to select the top 4 directions $d_{0}, d_{112}, d_{114}$, and $d_{511}$ from Fig. 3a and sample 1000 videos for evaluation. Deviating from above, we here quantify video motion changes in head $\Delta \Phi_{\text {head }}$ and mouth regions $\Delta \Phi_{\text {mouth }}$, respectively. Tab. 5 shows that while deactivation of $d_{511}$ triggers the largest motion decline in the head region, the deactivation of $d_{0}$ leads to the

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-08.jpg?height=674&width=1747&top_left_y=251&top_left_x=162)

Figure 6: Direction analysis in VoxCeleb2-mini. A generated video sample and associated optical flow images (top), by only activating $d_{0}$ (middle), and by only activating $d_{511}$ (bottom). While $d_{0}$ controls the mouth region, $d_{511}$ controls the head region.

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-08.jpg?height=174&width=336&top_left_y=1065&top_left_x=168)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-08.jpg?height=184&width=57&top_left_y=1068&top_left_x=470)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-08.jpg?height=168&width=158&top_left_y=1071&top_left_x=493)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-08.jpg?height=200&width=200&top_left_y=1060&top_left_x=629)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_7b79a73b3f86ab3d9176g-08.jpg?height=198&width=184&top_left_y=1061&top_left_x=813)

(e)
Figure 7: Global and local motion extraction. (a) Generated image, (b) related optical flow, (c) semantic map, (d) mouth-flow image, and (e) face-flow image based on training with VoxCeleb2mini.

largest decline of mouth-motion. Considering that head movement contributes to mouth movement, we compute $\Delta \Phi_{\text {mouth }}-\Delta \Phi_{\text {head }}$, excluding global from local motion. However, $d_{0}$ still remains highest contribution to mouth motion. Similar to BAIR-robot, we illustrate samples by activating only $d_{0}$, and only $d_{511}$, respectively, in Fig. 6. While $d_{0}$ reflects mouth motion, $d_{511}$ represents head motion. This is conform to our quantitative evaluation.

Therefore, we verify that some directions in our motion dictionary are interpretable. In addition, we are able to control motion by (de-)activating such directions.

|  | $\Delta \Phi_{\text {head }}$ | $\Delta \Phi_{\text {mouth }}$ | $\Delta \Phi_{\text {mouth }}-\Delta \Phi_{\text {head }}$ |
| :---: | :---: | :---: | :---: |
| $d_{0}$ | -0.012 | $\mathbf{- 0 . 0 5 2}$ | $\mathbf{- 0 . 0 4 0}$ |
| $d_{112}$ | -0.001 | -0.005 | -0.005 |
| $d_{114}$ | -0.000 | -0.005 | -0.005 |
| $d_{511}$ | $\mathbf{- 0 . 0 3 6}$ | -0.027 | 0.008 |

Table 5: $\Delta \Phi_{\text {head }}$ and $\Delta \Phi_{\text {mouth }}$ on VoxCeleb2-mini. Motion difference in head and mouth regions induced by deactivation of motion-directions.

As we have already found interpretable directions, we show for BAIR-robot, by providing pre-defined trajectories to $d_{1}$ and $d_{511}$, that we are able to generate videos in a controllable manner. We provide detailed experimental descrip- tion in Sec. B and show results generated results on project website.

### 4.4. Further analysis

We here experiment with linear interpolation in the latent space, see Sec. A. We note that such interpolations are evidence that InMoDeGAN has learned a smooth mapping from the latent space to real videos, rather than memorized training data.

Moreover, we show that our model generalizes well to high-resolution video generation. Towards this, we generate $128 \times 128$ videos, as trained on VoxCeleb2-mini, as well as on UCF101 [40]. In this context, we repeat the interpretability evaluation and observe again interpretable directions related to mouth and head motion. For UCF101, we conduct quantitative evaluation based on a metric proposed by TGANv2 [37]. We report evaluation results of VoxCeleb2-mini $(128 \times 128)$ in Tab. 6 and UCF101 in Tab. 7. Results show that our method outperforms current state-of-the-art on UCF101 by exhibiting lower FID and higher IS.

Finally, we generate longer videos to explore the limit of our model for the VoxCeleb2-mini and BAIR-robot datasets. InMoDeGAN is able to generate videos of framelength beyond training data (16 frames), reaching up to around 32 frames on VoxCeleb2-mini and 45 frames on BAIR-robot, which are highly promising. Generated results are shown on project website and experimental details are described in Sec. A.

## 5. Conclusions

We have presented a novel video generative model, InMoDeGAN, which is aimed at (a) generating high quality videos, as well as (b) allowing for interpretation of the latent space. In extensive evaluation on two datasets, InMoD-

eGAN outperforms quantitatively and qualitatively state-ofthe-art methods w.r.t. visual quality. Crucially, we have shown the ability of InMoDeGAN to decompose motion in semantic sub-spaces, enabling direct manipulation of the motion space. We have showcased that proposed Temporal Pyramid Discriminator, streamlined to analyze videos at different temporal resolutions, while involving only $2 \mathrm{D}$ ConvNets, outperforms 3D counterparts. In further analysis we have explored generation of longer videos, as well as of videos with higher resolution. Future work involves the analysis of our method on more complex human activity datasets, where we intend to investigate the possibility to control motion of each joint in a human body.

## References

[1] Dinesh Acharya, Zhiwu Huang, Danda Pani Paudel, and Luc Van Gool. Towards high resolution video generation with progressive growing of sliced wasserstein gans. arXiv preprint arXiv:1810.02419, 2018.

[2] Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, and Richard Szeliski. A database and evaluation methodology for optical flow. International journal of computer vision, 92(1):1-31, 2011.

[3] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Understanding the role of individual units in a deep neural network. PNAS, 2020.

[4] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, and Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In ICLR, 2019.

[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In ICLR, 2019.

[6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.

[7] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In ICCV, 2019.

[8] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.

[9] Taco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral cnn. arXiv preprint arXiv:1902.04615, 2019.

[10] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. arXiv preprint arXiv:1710.05268, 2017.

[11] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In CVPR, 2019.

[12] Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual definitions of cognitive image properties. In ICCV, pages 5744-5753, 2019.

[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[14] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? In CVPR, 2018.
[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.

[16] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In ICANN. Springer, 2011.

[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-Image Translation with Conditional Adversarial Networks. In CVPR, 2017.

[18] Ali Jahanian, Lucy Chai, and Phillip Isola. On the "steerability" of generative adversarial networks. In ICLR, 2020.

[19] Yunseok Jang, Gunhee Kim, and Yale Song. Video Prediction with Appearance and Motion Conditions. In ICML, 2018.

[20] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.

[21] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.

[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In CVPR, 2020.

[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[24] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photorealistic single image super-resolution using a generative adversarial network. In CVPR, 2017.

[25] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In CVPR, 2015.

[26] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal video prediction from still images. In ECCV, 2018.

[27] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario Fritz. Disentangled person image generation. In CVPR, 2018.

[28] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Which training methods for gans do actually converge? In ICML, 2018.

[29] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.

[30] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. Voxceleb: Large-scale speaker verification in the wild. Computer Science and Language, 2019.

[31] Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Hierarchical video generation from orthogonal information: Optical flow and texture. In $A A A I$, 2018.

[32] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang. Video generation from single semantic label map. arXiv preprint arXiv:1903.04480, 2019.

[33] Taesung Park, Alexei A. Efros, Richard Zhang, and JunYan Zhu. Contrastive learning for unpaired image-to-image translation. In ECCV, 2020.

[34] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2337-2346, 2019.

[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.

[36] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In ICCV, 2017.

[37] Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely: Memoryefficient unsupervised training of high-resolution temporal gan. IJCV, 2020.

[38] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Singan: Learning a generative model from a single natural image. In CVPR, 2019.

[39] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. In CVPR, 2020.

[40] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild. Technical report, CRCV-TR-12-01, November 2012.

[41] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV, 2015.

[42] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion and content for video generation. In CVPR, 2018.

[43] Gül Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zisserman. Synthetic humans for action recognition from unseen viewpoints. CoRR, abs/1912.04070, 2019.

[44] Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid. Learning from synthetic humans. In CVPR, 2017.

[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.

[46] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NIPS, 2016.

[47] Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. arXiv preprint arXiv:2002.03754, 2020.
[48] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert. The pose knows: Video forecasting by generating pose futures. In ICCV, 2017.

[49] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-shot video-to-video synthesis. In NeurIPS, 2019.

[50] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-tovideo synthesis. In NeurIPS, 2018.

[51] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $8798-8807,2018$.

[52] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. G3AN: Disentangling appearance and motion for video generation. In CVPR, 2020.

[53] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. G3AN: Disentangling appearance and motion for video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.

[54] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Finegrained text to image generation with attentional generative adversarial networks. In CVPR, 2018.

[55] Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, and Dahua Lin. Pose guided human video generation. In ECCV, 2018.

[56] Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, and Bolei Zhou. Temporal pyramid network for action recognition. In CVPR, 2020.

[57] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In Proceedings of the European conference on computer vision (ECCV), 2018.

[58] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learning of realistic neural talking head models. In ICCV, 2019.

[59] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In CVPR, 2019.

[60] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris Metaxas. Learning to forecast and refine residual motion for image-to-video generation. In ECCV, 2018.

[61] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycleconsistent adversarial networkss. In ICCV, 2017.
