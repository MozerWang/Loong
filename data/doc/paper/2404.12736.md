# Large Language Model Supply Chain: A Research Agenda 

Shenao Wang<br>shenaowang@hust.edu.cn<br>Huazhong University of Science and Technology<br>Wuhan, China<br>Xinyi Hou<br>xinyihou@hust.edu.cn<br>Huazhong University of Science and Technology<br>Wuhan, China

Yanjie Zhao<br>yanjie_zhao@hust.edu.cn<br>Huazhong University of Science and Technology<br>Wuhan, China<br>Haoyu Wang<br>haoyuwang@hust.edu.cn<br>Huazhong University of Science and Technology<br>Wuhan, China


#### Abstract

The rapid advancements in pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs) ${ }^{1}$ have ushered in a new era of intelligent applications, transforming fields ranging from natural language processing to content generation. The LLM supply chain represents a crucial aspect of the contemporary artificial intelligence landscape. It encompasses the entire lifecycle of pre-trained models, from its initial development and training to its final deployment and application in various domains. This paper presents a comprehensive overview of the LLM supply chain, highlighting its three core elements: 1) the model infrastructure, encompassing datasets and toolchain for training, optimization, and deployment; 2) the model lifecycle, covering training, testing, releasing, and ongoing maintenance; and 3) the downstream application ecosystem, enabling the integration of pre-trained models into a wide range of intelligent applications. However, this rapidly evolving field faces numerous challenges across these key components, including data privacy and security, model interpretability and fairness, infrastructure scalability, and regulatory compliance. Addressing these challenges is essential for harnessing the full potential of LLMs and ensuring their ethical and responsible use. This paper provides a future research agenda for the LLM supply chain, aiming at driving the continued advancement and responsible deployment of these transformative LLMs.


## 1 INTRODUCTION

The rapid advancement of pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs), such as GPT4 [2], Gemini [183], and LLaMA [187], has revolutionized the field of artificial intelligence (AI) and sparked a new era of intelligent applications. These powerful models, trained on vast amounts of data, have demonstrated remarkable capabilities in a wide range of tasks, from natural language processing to multimodal content generation.

As the adoption of LLM continues to grow, the need for a robust and efficient supply chain to support their development, deployment, and maintenance has become increasingly apparent. The LLM supply chain encompasses the entire lifecycle, from model training to testing, releasing, and ongoing maintenance. This complex ecosystem involves various stakeholders, including model[^0]

developers, data providers, and end-users, all of whom must navigate a set of unique challenges to unlock the full potential of these transformative technologies.

In this paper, we present a comprehensive overview of the LLM supply chain, highlighting the key components and the critical challenges that must be addressed to ensure the safe, reliable, and equitable deployment of LLMs. We explore the technical, ethical, and operational aspects of this supply chain, drawing insights from the fields of software engineering, system architecture, security, and data governance. Our goal is to provide a holistic understanding of the LLM supply chain and to identify the most promising research and development opportunities that can drive the future of this rapidly evolving landscape.

## 2 DEFINITION OF LLM SUPPLY CHAIN

Similar to the Open Source Software (OSS) supply chain [93, 164, 204], the LLM supply chain refers to the network of relationships that encompass the development, distribution, and deployment of models. This supply chain includes the upstream model development communities, model repositories, distribution platforms, and app markets, as well as data providers, toolchain/model developers, maintainers, and end-users. As illustrated in Figure 1, this supply chain can be further divided into three key components:

- Fundamental Infrastructure: The LLM supply chain is underpinned by a robust model infrastructure, which includes the curation and management of diverse datasets, and the toolchain that enables efficient model training, optimization, and deployment (such as PyTorch [158], TensorFlow [184] and LangChain [94]);
- Model Lifecycle: The model lifecycle stands as the pivotal nexus within the intricate LLM supply chain ecosystem. This holistic lifecycle, spanning a model's entire process from conception to retirement, serves as the convergence point for the complex interdependencies permeating the supply chain. It not only encompasses the dependencies introduced by model reuse but also intricately intertwines with the dataset and development tools supply chain in the infrastructure layer;
- Downstream Application Ecosystem: Atop the model infrastructure and lifecycle, the LLM supply chain encompasses a vibrant downstream application ecosystem. This ecosystem includes applications and services powered by LLMs, such as GPTs [144], as well as Domain-Specific Models (DSMs), which directly bring the capabilities of these transformative technologies to end-users.

![](https://cdn.mathpix.com/cropped/2024_06_04_63e0e973460068c41146g-02.jpg?height=734&width=1572&top_left_y=278&top_left_x=252)

Figure 1: Definition and Each Component of LLM Supply Chain.

These complex interdependencies and interactions between these components form the backbone of the LLM supply chain. By defining the LLM supply chain in this manner, we can draw insights from the existing research on OSS supply chain and apply them to the unique requirements and complexities of the LLM ecosystem. This holistic understanding will serve as a foundation for the subsequent exploration of the opportunities and challenges within the LLM supply chain.

The rest of this paper is organized as follows. We delve into discussions on model infrastructure, model lifecycle, and the downstream application ecosystem in $\S 3$, ยง 4, and ยง 5, respectively. Each section is structured in the order of vision, challenge, and opportunity. Finally, we conclude the paper in $\S 6$.

## 3 LLM INFRASTRUCTURE

The model infrastructure is a foundational component of the LLM supply chain, encompassing the dataset and toolchain necessary for the training, testing, deployment, and maintenance of LLMs.

### 3.1 Vision: High-quality Dataset

In the evolving landscape of LLMs, the vision for a high-quality dataset within the supply chain embodies a multifaceted commitment to excellence, privacy, and ethical standards [136]. At the heart of this vision lies the recognition that the quality and integrity of datasets are not merely ancillary concerns but are central to the development of models that are both effective and responsible [74, 163]. This vision articulates a future where datasets are meticulously curated to ensure accuracy, relevance, and comprehensive representation of real-world complexities. To achieve this vision, several challenges must be addressed:

Challenge I: Data Cleaning and Curation. The process of data cleaning and curation is a critical step in the development of LLMs, serving as the backbone for ensuring integrity, privacy, and ethical alignment. This step, however, is laden with significant challenges that can compromise the efficacy and safety of LLMs if not addressed with rigor and foresight. The primary obstacles stem from the handling of redundant [83], privacy [22, 23, 88, 126], biased [48, 89, 141, 221, 233], and toxic [148, 199, 233] data in training sets, each of which presents unique challenge. Redundancy in training datasets not only inflates the size of the dataset unnecessarily but also skews the model's learning, leading to efficiency issues [25, 96, 185] and potential overfitting to repetitive data patterns [68, 83, 229]. The potential privacy challenges are twofold: ensuring that personally identifiable information (PII) is not present in the training data [19] and preventing the model from learning to reproduce or infer it from the patterns it is trained on [22,23, 88]. Bias in training data is a well-documented issue that can lead models to perpetuate or even amplify existing prejudices $[16,114,176]$. The challenge lies in identifying and mitigating biases, which are often deeply embedded in the data and reflective of broader societal biases [52,176]. The presence of toxic and harmful content in training datasets poses a significant risk to the safety and reliability of LLMs [15, 148, 199, 233]. Models trained on datasets containing such content may reproduce or even generate harmful outputs, undermining their applicability in diverse contexts. These challenges in data cleaning and curation require sophisticated strategies for mitigation, and this provides some opportunities as discussed below.

- Opportunity: Deduplication. At the forefront of this opportunity is the development of more sophisticated deduplication algorithms. Simple deduplication methods such as MinHash [17] often struggle with the scale and diversity of data typical for LLM training [27]. Advanced deduplication strategies that carefully evaluate which duplicates to remove can ensure that the richness of the data is maintained. There lies a potential in leveraging careful data selection via pre-trained model embeddings, ensuring that training data are both diverse and concise. Innovations in this area could significantly reduce computational overhead and improve model performance.
- Opportunity: Privacy Preserving. The development and implementation of innovative privacy preserving algorithms stand out as a primary opportunity. Current methods such as k-anonymity [179], l-diversity [130], t-closeness [105] and differential privacy $[41,43,155,169,170]$ have set the foundation, yet they often face challenges in balancing privacy with data utility. The need to preserve privacy while ensuring that the dataset remains comprehensive and informative enough to train robust models is still an open problem.
- Opportunity: Bias Mitigation. The first opportunity lies in enhancing methodologies for the detection and correction of biases in datasets. While significant progress has been made [120, 141, 152], there is a continuous need for more sophisticated tools that can identify subtle and complex biases. Another critical opportunity is to strike a balance between removing biases and maintaining the representativeness of datasets. This involves not only the removal of harmful biases but also ensuring that the diversity and richness of human experiences are accurately reflected in LLMs.
- Opportunity: Detoxifying. Cleaning datasets of toxic content requires not only sophisticated detection tools [11, 92, 225] but also a nuanced understanding of the mechanism what constitutes harm $[7,153,200]$, which can vary widely across different cultural and social contexts. Cross-cultural sensitivity presents an opportunity to create guidelines and frameworks that respect cultural differences while identifying universally harmful content.

Challenge II: Avoid Data Poisoning. Data poisoning attacks [150, $165,223]$ pose severe supply chain risks for LLMs, as attackers can degrade model performance or introduce backdoors [1, 108, 111, 212] through corrupted training data, which undermines the integrity and reliability of LLMs. Additionally, supply chain attacks targeting the data storage, processing, or distribution infrastructure can facilitate data poisoning or corruption, potentially compromising the entire model development lifecycle. Avoiding data poisoning in the supply chain of LLMs presents a multifaceted set of challenges, intricately linked with the broader objectives of data cleaning and curation. Crucial opportunities include enhancing data validation, improving provenance tracking, and implementing comprehensive security measures throughout the entire data lifecycle.

- Opportunity: Robust Data Validation. The first line of defense against data poisoning is robust data validation [72, 156], a process that is inherently complex due to the vast scale and heterogeneity of datasets used in LLM training. Effective validation requires sophisticated algorithms capable of detecting anomalies and malicious modifications in the data [127], which is a task that becomes exponentially difficult as the data volume and diversity increase [151, 171]. The opportunity for progress in robust data validation resides in advancing algorithmic solutions that are capable of nuanced detection of subtle and sophisticated data manipulation attempts. These solutions must be scalable enough to manage the expansive datasets characteristic of LLM training, thereby ensuring comprehensive coverage without compromising efficiency.
- Opportunity: Provenance Tracking. Provenance tracking, or the ability to trace the origin and history of each data point, becomes paramount in a landscape where data can be compromised at any stage $[72,162,181,210]$. Implementing such tracking mechanisms involves not only technical solutions [140] but also organizational policies that ensure data sources are reputable and that data handling practices are transparent [49] and secure. However, establishing a provenance tracking system that is both comprehensive and efficient remains an open problem, given the complexity of LLM supply chains and the potential for data to be aggregated from myriad sources [125, 197, 227].
- Opportunity: Securing Data Lifecycle. Ensuring rigorous security measures across the entire data lifecycle is critical to safeguarding against poisoning attacks $[6,166]$. This encompasses not only the protection of data at rest and in transit but also the security of the infrastructure used for data processing and model training [163]. As supply chain attacks can target any component of the system, a holistic security approach that includes regular audits, encryption, access control, and real-time monitoring is essential for identifying and mitigating threats promptly $[6,72]$.

Challenge III: License Management. License management encompasses a range of challenges that are critical to navigate in order to maintain legal and ethical standards. As LLMs require vast amounts of diverse data for training, the risk of copyright infringement, licensing violations, and subsequent legal liabilities intensifies [29, 84, 85, 100]. Recent research [125, 178, 192, 202, 204] has shed light on the complex landscape of dataset copyright and licensing, underscoring the need for further exploration and development of best practices. This need is further complicated by the diversity of data sources and the often opaque legal frameworks governing data use [49]. These challenges also open up some opportunities for further research.

- Opportunity: Complex License Understanding. One of the primary challenges in license management is the complexity and variety of licenses [37, 192-194, 196]. Data sources can range from publicly available datasets with open licenses to proprietary datasets with strict usage restrictions. Each source may come with its own set of legal terms, requiring careful review and understanding to ensure compliance $[9,55,195,202$, 207]. Opportunities in this area could include the automated detection and summarization of key legal terms, providing stakeholders with clear, accessible insights into the permissions, obligations, and restrictions associated with each dataset.
- Opportunity: License Conflict Auditing. Automated license conflict auditing represents another significant opportunity to enhance license management practices [3, 121, 188]. Such systems could potentially streamline the process of verifying compliance with licensing agreements across vast datasets. However, developing these systems faces technical hurdles, including the need for advanced algorithms capable of interpreting and applying the legal nuances of different licenses [190]. Moreover, ensuring the reliability and accuracy of these automated systems is paramount to avoid unintentional violations.


### 3.2 Vision: Robust \& Secure Toolchain

In the realm of LLMs, the development tools and frameworks serve as the cornerstone of innovation, significantly shaping the trajectory of artificial intelligence. This vision, from both a software engineering (SE) and security standpoint, is ambitious yet grounded, aiming to forge a development environment that is robust, scalable, and inherently secure. By weaving together the best practices of SE with advanced security measures, this approach ensures that the toolchain not only enable the crafting of sophisticated models but also safeguard the integrity of the entire supply chain.

From a SE perspective, central to this vision is the seamless incorporation of SE best practices into LLM development tools and frameworks. Modular design principles are prioritized to boost maintainability and scalability, allowing for seamless updates and modifications without impacting the broader system. The vision also encompasses the implementation of continuous integration and deployment (CI/CD) pipelines to streamline the testing and deployment processes, enhancing development speed.

From a Security perspective, a "security by design" philosophy is advocated, embedding security considerations at the onset of the development process. This includes deploying comprehensive code analysis tools for early vulnerability detection and enforcing secure authentication. Beyond individual security practices, a crucial aspect of safeguarding the LLM supply chain involves addressing the security of the development tools' own supply chains. Given the reliance of most LLM systems on a handful of core frameworks, the compromise of any one of these foundational elements could expose a vast array of LLM systems to risk [121, 142, 160]. To mitigate these risks, the vision calls for rigorous security measures at every level of the supply chain for development tools and frameworks. Such measures are essential for preventing the introduction of vulnerabilities into LLM systems through compromised software components or malicious third-party contributions.

Challenge: Dependency and Vulnerability Management. Managing the intricate web of dependencies, including both open-source and commercial components, poses a significant challenge within the supply chain $[67,116,129]$. Supply chain attacks targeting the development infrastructure or code repositories could lead to the injection of vulnerabilities or malicious code [42, 62, 106], potentially compromising the entire lifecycle of model development and deployment. Moreover, vulnerabilities within dependencies or components can propagate through the supply chain [69, 226], adversely affecting the security and reliability of models. Establishing robust dependency management processes, conducting thorough security monitoring, and ensuring supply chain transparency are essential for mitigating risks such as compromises in the LLM development tools supply chain.

- Opportunity: LLM Toolchain Mining. A promising avenue for enhancing supply chain security lies in the opportunity of LLM development toolchain mining. This approach involves the systematic analysis and evaluation [80] of the tools and libraries used in the creation and training of LLMs. The core of this opportunity revolves around the comprehensive mining and auditing of development tools, from code libraries to data processing frameworks used in LLM training. Through the detailed analysis of the toolchain, developers can identify redundancies, inefficiencies, and areas for improvement, paving the way for the development of more streamlined, effective, and secure LLMs. Additionally, this mining process can spur innovation by highlighting gaps or needs within the toolchain, driving the creation of new tools, or the enhancement of existing ones to better serve the evolving demands of LLM development.
- Opportunity: SBOM of LLM Toolchain. The adoption of the Software Bill of Materials (SBOM) of LLM toolchain presents a unique opportunity to achieve unprecedented levels of transparency and security. By meticulously documenting every library, dependency, and third-party component, SBOM enables developers to gain a comprehensive overview of their tools' software ecosystem [174, 175, 208]. This holistic visibility is instrumental in identifying vulnerabilities, outdated components, and non-compliant software elements that could jeopardize the development process and, ultimately, the security of the LLMs themselves. The detailed insights provided by SBOMs pave the way for proactive vulnerability management. Armed with knowledge about every constituent component, development teams can swiftly address security flaws, apply necessary patches, and update components. This preemptive identification and remediation process is crucial in safeguarding the models against potential exploits that could jeopardize their reliability and the security of the systems they operate within.


## 4 LLM LIFECYCLE

In the evolving landscape of LLMs, the vision for the model lifecycle within the supply chain encompasses a holistic and agile approach, from initial development to deployment, maintenance, and updates. This lifecycle is envisioned to be a seamless continuum that not only addresses the inherent challenges but also leverages them as catalysts for innovation and progression in the field.

### 4.1 Vision: Efficient Development \& Training

The vision for developing and training LLMs is a compelling narrative of innovation, inclusivity, and ethical responsibility, aiming to push the boundaries of what these computational behemoths can achieve while grounding their evolution in principles that benefit all of humanity. This vision integrates the cutting edge of technological innovation with an unwavering commitment to ethical principles and operational efficiency. Firstly, the training of LLMs is envisioned to become increasingly efficient and environmentally sustainable. As the computational demands of these models soar, innovative approaches to training such as more efficient algorithms and hardware optimization are prioritized. Another cornerstone of this vision is the seamless integration of ethical considerations and bias mitigation strategies from the outset. This approach ensures that LLMs are developed with a deep understanding of their potential societal impacts, embedding ethical guidelines into the DNA of model development and ensuring that LLMs do not become tools for misinformation, manipulation, or harm.

Challenge: Inner Alignment. As the capabilities of LLMs continue to expand, the necessity of ensuring their alignment [123, 167, 201] with human values and intentions becomes increasingly critical. The concept of inner alignment [71, 167] focuses on ensuring that an LLM's objectives are congruent with the intentions of its
designers during the development and training phase. The pursuit of inner alignment during the development and training phase of LLMs requires a multifaceted strategy. However, inner alignment is complicated by its nuanced failure modes [5, 71, 167], such as proxy, approximate, and suboptimality alignments, each presenting unique challenges in ensuring LLM systems operate as intended. These failure modes underscore the potential divergence between a model's optimized objectives and the overarching goals its designers aim to achieve. To address these issues, methodological approaches such as relaxed adversarial training [71] and partitioning gradients [218] have been proposed. However, the efficacy of such methodologies hinges on the transparency of the LLM system's decision-making processes, which provides opportunities for further research.

- Opportunity: Advancing Interpretability of LLMs. Firstly, the opportunity to advance the methodology of transparency and interpretability in LLMs stands as a critical endeavor [173]. Enhancing transparency involves shedding light on the often opaque decision-making processes of these models, enabling a clearer understanding of how inputs are processed and interpreted to produce outputs. By demystifying the inner workings of LLMs, researchers, and practitioners can gain valuable insights into the operational dynamics of these models, identifying areas where the models' behaviors may not align with expected or desired outcomes $[81,173]$. When developers and users can understand how a model is processing information and arriving at conclusions, they can more effectively detect when the model deviates from the intended behavior. This early detection is invaluable, as it allows for timely interventions to correct course [103, 180], preventing minor misalignments from escalating into more significant issues.
- Opportunity: Enhancing Feedback Mechanisms. The integration of robust feedback mechanisms into LLMs represents a transformative opportunity to enhance their adaptability and alignment with human values over time [131, 154]. By embedding iterative feedback loops within the architecture of LLMs, developers can establish a dynamic process where the models continually learn and adjust from real-world interactions and user feedback. Feedback loops can be particularly beneficial in identifying and correcting biases, misconceptions, or inaccuracies that may emerge in LLM outputs, thereby enhancing the models' trustworthiness and reliability [117, 149, 154]. This process enables LLMs to evolve and adapt in response to changing contexts, user needs, and societal norms, ensuring their ongoing relevance and utility.


### 4.2 Vision: Holistic Testing \& Evaluation

In the complex supply chain of LLMs, the testing and evaluation phase is pivotal, serving as the final arbiter of a model's readiness for deployment and its potential impact on users and society at large. The vision for this phase is one of comprehensive rigor, transparency, and adaptability, ensuring that LLMs are not only technologically proficient but also ethically sound and socially beneficial. Specifically, the vision for the testing and evaluation of LLMs is deeply rooted in ensuring these advanced tools are helpful, honest, and harmless [10, 149], aligning with the broader goals of ethical integrity and societal benefit. By rigorously assessing LLMs against these principles, we can foster the development of technologies that are not only revolutionary in their capabilities but also responsible in their deployment. However, realizing such a vision faces the following major challenges:

Challenge I: Helpfulness Testing. Evaluating the helpfulness of LLMs is a critical aspect of ensuring their practical utility and widespread adoption. To this end, researchers have been developing benchmark datasets and tasks that measure LLM performance on capabilities such as question answering [13, 177, 231], task completion [38, 44, 61, 107, 189], and knowledge retrieval [59, 82, 168] across diverse domains. These benchmarks not only test for general knowledge [59, 82, 168] but also probe domain-specific expertise [61, 177, 189], allowing for a comprehensive assessment of an LLM's ability to provide useful and relevant outputs. However, there are still several formidable challenges which highlight not only the complexity inherent in measuring the utility of such models but also underscore the necessity for ongoing refinement in our approaches to evaluation.

- Opportunity: Developing Comprehensive Metrics and Benchmarks. First and foremost, the opportunity to develop more comprehensive metrics and benchmarks provides a pathway to better understand the performance of LLMs [119, 137, 219]. Traditional benchmarks, while useful, often fail to capture the multifaceted nature of tasks LLMs are expected to perform, especially in areas like code generation $[40,119,219]$. The current benchmarks, such as HumanEval [26] and AiXBench [64], provide a starting point but do not sufficiently address the complexities of generating code at the repository or project level $[40,119,219]$. This limitation points to a need for benchmarks that can assess an LLM's ability to understand projectspecific contexts, manage dependencies across multiple files, and ensure consistency within a larger codebase. Developing such metrics requires a deep understanding of the practical tasks users expect LLMs to perform and a thoughtful consideration of how to measure success in those tasks.
- Opportunity: Avoiding Data Contamination. Additionally, the issue of data contamination [35, 134] significantly complicates the evaluation of LLMs. Data contamination occurs when a model is inadvertently exposed to information from the test set during training, leading to inflated performance metrics that do not accurately represent the model's true capabilities [99, 157]. This challenge is particularly acute in domains like code generation $[21,39]$, where the vast amount of publicly available code means that models might "learn" specific solutions during training that they later reproduce during testing. Such instances of data contamination not only overestimate the model's performance but also obscure our understanding of its ability to generate innovative solutions to new problems. Although there have been efforts to quantify and detect data contamination [56, 57, 109, 110], effectively addressing this issue remains a challenge $[8,21,34,146]$. Opportunities in identifying and mitigating the impact of data contamination include the development of novel evaluation frameworks that can detect when a model is reproducing rather than generating solutions [56, 57, 109], and the development of testing
metrics and benchmarks specifically designed to prevent data contamination $[39,75,110]$.

Challenge II: Honesty Testing. As LLMs become increasingly influential in various domains, ensuring their honesty and truthfulness is paramount to building trust and preventing the spread of misinformation. Honesty testing [97, 101, 113, 115, 138, 231] for LLMs involves assessing whether the models can consistently provide information that is not only factually correct but also free from deception or misleading implications. These tests aim to identify instances of hallucinated [77, 228] or fabricated information $[97,115,138]$ in LLM outputs, which can undermine their trustworthiness. Assessing the consistency and coherence of LLM outputs across multiple queries and prompts can reveal potential inconsistencies or contradictions, which may indicate a lack of factual grounding or honesty.

- Opportunity: Hallucination Mitigation. Hallucination mitigation in LLMs is an area of significant concern, with various innovative techniques employed to address the issue [60, 186]. These methods range from retrieval augmented generation $[53,154,191]$ to self-refinement through feedback and reasoning $[78,172]$, each targeting different aspects of hallucination to ensure the accuracy and reliability of LLM outputs. However, there's an open problem in balancing mitigation efforts with the preservation of LLMs' generative capabilities, avoiding over-restriction that could stifle their performance. The development of LLMs with inherent mechanisms to prevent hallucinations is an exciting avenue, potentially leading to inherently more honest models.

Challenge III: Harmlessness Testing. The challenge of harmlessness testing in LLMs is multifaceted, rooted in the need to detect and mitigate a broad spectrum of potential harms. Researchers have been developing benchmarks $[32,54,70,76,139]$ that probe LLMs for the presence of harmful biases, stereotypes, or discriminatory language across various sensitive topics and demographic groups. Furthermore, testing LLMs for potential vulnerabilities to adversarial attacks [159, 198, 230], jailbreaks [28, 36, 122, 232], or misuse [12, 51, 66, 133, 214, 224] by attackers ensures their outputs do not enable harmful actions or security breaches. Yet they are beset with the inherent challenge of predicting and counteracting the myriad ways in which these sophisticated models might be exploited or go awry.

- Opportunity: Detection and Mitigation. Despite these challenges, the domain of harmlessness testing for LLMs presents substantial opportunities to enhance the safety and integrity of LLMs. Developing advanced benchmarks and testing protocols offers a pathway to not only detect but also rectify harmful outputs before they reach end-users, thereby safeguarding public trust in LLM applications. This endeavor encourages the creation of more nuanced and context-aware models, capable of discerning and adapting to the ethical implications of their outputs. Additionally, addressing the risks of adversarial misuse opens avenues for innovative defensive strategies, fortifying LLMs against manipulation and ensuring their outputs remain aligned with ethical standards.


### 4.3 Vision: Collaborative Release \& Sharing

The release and sharing phase represents a pivotal point in the LLM lifecycle, where trained models are packaged for distribution, complete with serialization and documentation detailing their capabilities, limitations, and intended applications. These models are then published to repositories or model hubs like Hugging Face [45], making them accessible for reuse by others through techniques such as feature extraction, fine-tuning, transfer learning, and knowledge distillation [33, 79, 182]. Providing licensing information and metadata is crucial for facilitating responsible adoption and collaboration. However, akin to traditional software supply chains, the reuse of pre-trained models introduces significant supply chain risks that must be carefully managed. The propagation of dependency risks, such as privacy concerns [1, 213], biases [19, 48, 73], hallucinations [118, 228], and vulnerabilities [1], can occur throughout the supply chain during model reuse and adaptation processes. Ensuring the trustworthy and responsible use of these powerful models necessitates comprehensive supply chain risk management strategies to mitigate potential threats and foster transparency, compliance, and accountability.

Challenge I: Model Dependency Analysis. Comprehensive analysis of model dependencies is a crucial first step in mitigating LLM supply chain risks. Existing approaches [91, 112] to dependency analysis include (1) version information analysis; (2) training-code analysis; (3) model file analysis; (4) watermarking and fingerprinting. Analyzing version control systems and model management tools helps track dependencies in model ecosystems, but often fails to fully capture complex interdependencies. Examining the training codebase for dependencies on libraries or datasets is detailed but might not reflect the deployed model accurately. Analyzing binary model files can offer precise insights into model architecture and behavior but is resource-heavy and challenging with encrypted formats. Embedding watermarks in models aids in tracking and provenance but is less effective for third-party models and can impact performance. These methods highlight the intricate challenges of understanding dependencies in LLMs.

- Opportunity: Model Reuse Tracking. Addressing the challenges of dependency tracking in LLMs presents several opportunities for advancing the field. Enhanced algorithms for analyzing version control and model management systems could provide deeper insights into the nuanced interdependencies within model ecosystems. Developing more efficient methods for codebase analysis could reduce computational overhead while offering accurate reflections of model dependencies. Improving techniques for binary model file analysis represents a significant opportunity, which could lead to methods that can effectively navigate obfuscated or encrypted formats, providing a clearer understanding of a model's architecture and behavior. Potential research directions include developing hybrid approaches that combine the strengths of these existing techniques, leveraging advanced code analysis, binary analysis, and machine learning-based methods to enhance dependency detection accuracy and scalability.

Challenge II: Risk Propagation Mitigation. Analyzing the propagation of vulnerabilities in the LLM supply chain introduces considerable challenges. The intricate nature of these models, with
their deep layers and complex dependencies, makes it challenging to track how risks like privacy breaches, bias, hallucination issues, and potential backdoors can permeate through the supply chain. Identifying these risks requires a thorough understanding of the interconnections and the flow of data and configurations across different model components. The absence of standardized methods for documenting these elements further complicates the task, making it difficult to conduct a comprehensive and effective risk assessment and to pinpoint areas where vulnerabilities might be introduced or propagated.

- Opportunity: Developing Model Bill of Materials (MBOM). There lies a significant opportunity to enhance the security and integrity of LLMs through the development of standardized practices for generating and maintaining a Model Bill of Materials (MBOM) for pre-trained models, mirroring the concept of SBOM. Such standardization would improve supply chain transparency, enabling stakeholders to more effectively identify, assess, and mitigate risks. Moreover, fostering collaboration among researchers, industry practitioners, and regulatory bodies can lead to the establishment of robust best practices and guidelines for the responsible release and sharing of models. This collaborative approach would not only enhance the trustworthiness and accountability of LLMs across the supply chain but also ensure that risk mitigation strategies are holistic, timely, and aligned with evolving ethical and security standards, ultimately leading to a safer and more reliable LLM ecosystem.


### 4.4 Vision: Continuous Deploy \& Maintenance

In the rapidly evolving landscape of machine learning, pre-trained models must adapt to changing real-world conditions, emerging data distributions, and novel task requirements to maintain their utility and relevance. The model maintenance and update phase is crucial for ensuring the longevity and continued effectiveness of these powerful models. However, this phase presents several opportunities and challenges that demand rigorous exploration by the research community.

Challenge I: Model Drift. The challenge of identifying and quantifying model drift in LLMs is considerable [30, 102, 132, 147]. LLMs are trained on vast datasets that are supposed to represent the linguistic diversity of their intended application domain. However, as the language evolves or the model is applied to slightly different contexts, the ability to remain relevant and consistent can shift in subtle ways. Recent research [18, 30, 132] emphasizes the need for sophisticated tools that can detect not only overt drifts in language usage but also more nuanced shifts in sentiment, context, or cultural references. These tools must be capable of parsing the complexities of human language, requiring ongoing refinement and adaptation to new linguistic phenomena.

- Opportunity: Model Drift Monitoring. The realm of drift monitoring in LLMs presents a fertile ground for innovation and development. There is a significant opportunity to create and refine tools that can accurately detect and measure drift in various dimensions, from language usage to sentiment and contextual nuances. Furthermore, integrating these drift monitoring tools into the model development and deployment lifecycle can provide ongoing insights into model performance [30, 132], enabling timely adjustments and enhancements. This proactive approach to managing model drift not only ensures the sustained relevance and accuracy of LLMs but also opens new avenues for research in understanding and mitigating the subtleties of language evolution in artificial intelligence.

Challenge II: Continual Learning. Once drift is detected, the next challenge is adapting the model to accommodate this change. A promising aspect of this phase is continual learning [14, 86, 206], the model's ability to learn from new data over time without forgetting previously acquired knowledge. A primary challenge in continual learning is catastrophic forgetting [63, 98], where the model loses its ability to perform tasks it was previously trained on after learning new information. This phenomenon is particularly problematic for LLMs due to their complex architecture and the vast scope of their training data. Recent advancements in research have proposed various strategies to mitigate catastrophic forgetting, such as rehearsal-based methods [4, 20, 65, 161] and regularization-based methods [90, 143, 222]. The foundational concept of rehearsal-based methods is Experience Replay (ER) [161], which involves storing samples from past tasks and reusing them when learning new tasks. This approach simulates the ongoing presence of old data alongside new data, thereby reducing the tendency of the model to forget previously learned information. The core idea behind regularization-based methods is to protect the knowledge acquired from previous tasks by preventing significant updates to the model parameters that are deemed important for those tasks [90]. Despite their conceptual appeal, regularizationbased methods face challenges in practice. They can struggle with long sequences of tasks, as the accumulation of regularization terms may eventually lead to a situation where the model becomes too rigid, hindering its ability to learn new information [24, 47, 98].

- Opportunity: Catastrophic Forgetting Mitigation. The field of mitigating catastrophic forgetting in LLMs is ripe with opportunities, particularly in enhancing and refining the existing strategies. The potential for innovation in rehearsal-based methods extends beyond mere data retention. Advanced data selection algorithms could be developed to identify and store the most representative or crucial samples, thus improving the efficiency of the rehearsal process. In the realm of regularizationbased methods, opportunities abound for creating more dynamic and adaptable regularization techniques. Furthermore, integrating these strategies or exploring hybrid approaches that combine the strengths of rehearsal and regularization could offer new pathways to robust continual learning. By developing methods that dynamically switch or combine strategies based on the task context or learning phase, models could achieve greater flexibility and effectiveness in retaining old knowledge while acquiring new information.


## 5 DOWNSTREAM ECOSYSTEM

The downstream application ecosystem serves as the final stage in the LLM supply chain, embodying the point where the efforts invested in developing, training, and refining these models are translated into practical benefits across different fields. This ecosystem is characterized by a diverse array of applications and services
that leverage pre-trained models to address real-world challenges, driving innovation and efficiency.

### 5.1 Vision: Revolutionary LLM App Store

The concept of an LLM app store (such as GPT Store [145]) represents a transformative vision for the downstream ecosystem of the LLM supply chain. It envisions a centralized platform where developers can publish applications powered by LLMs, and users can discover and access these applications to fulfill a wide array of tasks and objectives. Drawing inspiration from the success of mobile app store [50, 95, 135], the LLM app store aims to replicate this success within the domain by offering a curated, secure, and user-friendly environment for LLM-driven applications. At the core of the LLM app store's vision is the desire to catalyze innovation by lowering the barriers to entry for developers and providing them with a platform to build and deploy LLM-powered applications.

Challenge \& Opportunity: App Store Governance. Creating an LLM app store introduces several challenges, primarily concerning the quality control, compatibility, and ethical considerations of the models hosted. Ensuring that each LLM adheres to a high standard of accuracy, fairness, and security is crucial to maintaining user trust and compliance with regulatory standards. Additionally, the diversity of LLMs in terms of size, functionality, and intended use cases necessitates robust mechanisms for assessing and certifying model compatibility with various platforms and user requirements. Ethical concerns also come to the forefront, as the store must have stringent policies to prevent the dissemination of models that could be used maliciously or propagate bias, misinformation, or harmful content. However, an LLM app store also presents vast opportunities for innovation and value creation. By implementing mechanisms for user engagement, such as ratings and reviews, the store can facilitate a feedback loop that drives the evolution of more sophisticated and user-aligned LLMs, promoting a culture of transparency and accountability within the LLM community.

### 5.2 Vision: Ubiquitous On-device LLMs

The vision for on-device LLM deployment is to bring the power of advanced natural language understanding and generation directly to user devices [104, 124, 217], such as smartphones, tablets, and edge devices. This approach aims to significantly reduce reliance on cloud-based services, enabling faster response times, enhanced privacy, and reduced data transmission costs [216]. By running LLMs locally, users can benefit from real-time, personalized LLM experiences even in offline or low-bandwidth environments, unlocking new possibilities for LLM integration across various industries. Challenge \& Opportunity: Model Compression. The primary challenge in realizing on-device LLMs lies in model compression. Current state-of-the-art LLMs are often massive, requiring substantial computational resources that exceed the capacity of typical consumer devices [217]. Compressing these models without significant loss of effectiveness involves sophisticated techniques such as pruning [128, 211], quantization [87, 209, 215], and knowledge distillation [46, 203]. Each method must be carefully applied to balance the trade-offs between model size, speed, and performance. Additionally, the diverse hardware landscape of user devices presents further challenges in optimizing models for a wide array of processing capabilities, memory sizes, and power constraints. Despite these challenges, model compression presents immense opportunities. Innovations in this space can lead to more accessible and ubiquitous LLM, where powerful language models can operate seamlessly on a broad spectrum of devices. This democratization of LLM can spur a new wave of applications and services that are intelligent, context-aware, and personalized.

### 5.3 Vision: Expert Domain-specific LLMs

The vision for domain-specific LLMs is to create highly specialized models that offer expert-level understanding and generation capabilities within specific fields or industries. Unlike general-purpose LLMs, these models are fine-tuned with domain-specific data, enabling them to offer deeper insights, more accurate predictions, and nuanced understanding tailored to particular professional contexts, such as healthcare [220], law [31], finance [205], or scientific research [58]. This specialization aims to unlock transformative applications in various sectors, providing tools that can augment human expertise, automate complex tasks, and facilitate decisionmaking processes with unprecedented precision and reliability.

Challenge \& Opportunity: Specialized Dataset Collection. The primary challenge in developing domain-specific LLMs lies in gathering high-quality, specialized datasets to train these models. Unlike general-purpose LLMs, domain-specific models require data that encapsulates the depth and breadth of knowledge unique to each field, often necessitating collaboration with domain experts and significant investment in data acquisition and preparation. On the flip side, the opportunities presented by successfully developing domain-specific LLMs are immense and hold the potential to be truly transformative. By overcoming the challenges of data curation and model training, these LLMs can provide unparalleled support in decision-making and operational tasks within specialized fields. In essence, the successful deployment of domain-specific LLMs could introduce new paradigms of efficiency, accuracy, and insight across a myriad of specialized fields, marking a significant leap forward in how industries leverage LLMs.

## 6 CONCLUSION

In this paper, we provide a comprehensive exploration of the LLM supply chain, delving into the intricate phases of model infrastructure, lifecycle, and the downstream application ecosystem. We identified critical challenges at each stage, underscoring the opportunities for future research. In the realm of infrastructure, we highlighted the paramount importance of high-quality datasets and a robust and secure toolchain. The lifecycle of LLMs, marked by phases of development, testing, release, and maintenance, revealed the need for continuous innovation and vigilance to ensure models remain effective, secure, and aligned with ethical standards. The exploration of the downstream application ecosystem, which includes LLM app markets, on-device LLMs, and DSMs, opened a window into the future potential of LLMs across various industries and applications. In conclusion, we believe that the LLM supply chain represents a vibrant and complex ecosystem, and hope that this paper will provide an agenda for future research.

## REFERENCES

[1] Sara Abdali, Richard Anarfi, CJ Barberan, and Jia He. 2024. Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices. arXiv preprint arXiv:2403.12503 (2024).

[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).

[3] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong et al. 2024. A Survey on Data Selection for Language Models. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_63e0e973460068c41146g-09.jpg?height=33&width=252&top_left_y=615&top_left_x=240)

[4] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. 2019. Online continual learning with maximal interfered retrieval. Advances in neural information processing systems 32 (2019).

[5] Eleni Angelou. 2022. Three Scenarios of Pseudo-Alignment. https //www.lesswrong.com/posts/W5nnfgWkCPxDvJMpe/three-scenarios-ofpseudo-alignment. Accessed: 2024-03-28.

[6] Rob Ashmore, Radu Calinescu, and Colin Paterson. 2021. Assuring the machine learning lifecycle: Desiderata, methods, and challenges. ACM Computing Surveys (CSUR) 54, 5 (2021), 1-39.

[7] Ioana Baldini, Dennis Wei, Karthikeyan Natesan Ramamurthy, Mikhail Yurochkin, and Moninder Singh. 2021. Your fairness may vary: Pretrained lan guage model fairness in toxic text classification. arXiv preprint arXiv:2108.01250 (2021).

[8] Simone Balloccu, Patrรญcia Schmidtovรก, Mateusz Lango, and Ondลej Duลกek. 2024 Leak, cheat, repeat: Data contamination and evaluation malpractices in closedsource llms. arXiv preprint arXiv:2402.03927 (2024).

[9] ANN BARCOMB and DIRK RIEHLE. 2022. Open Source License Inconsistencies on GitHub. (2022)

[10] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability and transparency. 610-623.

[11] Dmitriy Bespalov, Sourav Bhabesh, Yi Xiang, Liutong Zhou, and Yanjun Qi. 2023 Towards building a robust toxicity predictor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track). 581-598.

[12] Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, et al. 2023. Purple llama cyberseceval: A secure coding benchmark for language models. arXiv preprint arXiv:2312.04724 (2023).

[13] Ning Bian, Xianpei Han, Bo Chen, and Le Sun. 2021. Benchmarking knowledgeenhanced commonsense question answering via knowledge-to-text transformation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 12574-12582.

[14] Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-Jussa. 2020. Continual lifelong learning in natural language processing: A survey. arXiv preprint arXiv:2012.09823 (2020)

[15] Abeba Birhane, Sanghyun Han, Vishnu Boddeti, Sasha Luccioni, et al. 2024. Into the LAION's Den: Investigating hate in multimodal datasets. Advances in Neural Information Processing Systems 36 (2024)

[16] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021)

[17] Andrei Z Broder. 1997. On the resemblance and containment of documents In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171). IEEE, 21-29

[18] Samuel Broscheit, Quynh Do, and Judith Gaspers. 2022. Distributionally robust finetuning BERT for covariate drift in spoken language understanding In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1970-1985.

[19] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramรจr. 2022. What does it mean for a language model to preserve privacy?. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2280-2292.

[20] Lucas Caccia, Eugene Belilovsky, Massimo Caccia, and Joelle Pineau. 2020 Online learned continual compression with adaptive quantization modules. In International conference on machine learning. PMLR, 1240-1250.

[21] Jialun Cao, Wuqi Zhang, and Shing-Chi Cheung. 2024. Concerned with Data Contamination? Assessing Countermeasures in Code Language Model. arXiv preprint arXiv:2403.16898 (2024)

[22] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. Extract ing training data from diffusion models. In 32nd USENIX Security Symposium
(USENIX Security 23). 5253-5270.

[23] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel HerbertVoss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). 2633-2650.

[24] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European conference on computer vision $(E C C V) .532-547$.

[25] Jou-An Chen, Wei Niu, Bin Ren, Yanzhi Wang, and Xipeng Shen. 2023. Survey: Exploiting data redundancy for optimization of deep learning. Comput. Surveys 55, 10 (2023), 1-38.

[26] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).

[27] Geyao Cheng, Deke Guo, Lailong Luo, Junxu Xia, and Siyuan Gu. 2021. LOFS: A lightweight online file storage strategy for effective data deduplication at network edge. IEEE Transactions on Parallel and Distributed Systems 33, 10 (2021), 2263-2276

[28] Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024. Comprehensive assessment of jailbreak attacks against llms. arXiv preprint arXiv:2402.05668 (2024).

[29] Timothy Chu, Zhao Song, and Chiwun Yang. 2024. How to Protect Copyright Data in Optimization of Large Language Models?. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17871-17879.

[30] MATTEO CITTERIO. 2022. A drift detection framework for large language models. (2022).

[31] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. Chatlaw: Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv:2306.16092 (2023)

[32] Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, et al. 2024. Risk taxonomy, mitigation, and assessment benchmarks of large language model systems. arXiv preprint arXiv:2401.05778 (2024).

[33] James C Davis, Purvish Jajal, Wenxin Jiang, Taylor R Schorlemmer, Nicholas Synovic, and George K Thiruvathukal. 2023. Reusing deep learning models: Challenges and directions in software engineering. In 2023 IEEE John Vincent Atanasoff International Symposium on Modern Computing (fVA). IEEE, 17-30.

[34] Jasper Dekoninck, Mark Niklas Mรผller, Maximilian Baader, Marc Fischer, and Martin Vechev. 2024. Evading Data Contamination Detection for Language Models is (too) Easy. arXiv preprint arXiv:2402.02823 (2024)

[35] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2023. Benchmark probing: Investigating data leakage in large language models. In NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly.

[36] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2024. MASTERKEY: Automated jailbreaking of large language model chatbots. In Proc. ISOC NDSS.

[37] Massimiliano Di Penta, Daniel M German, Yann-Gaรซl Guรฉhรฉneuc, and Giuliano Antoniol. 2010. An exploratory study of the evolution of software licensing. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1. 145-154.

[38] Yihong Dong, Jiazheng Ding, Xue Jiang, Ge Li, Zhuo Li, and Zhi Jin. 2023. Codescore: Evaluating code generation by learning code execution. arXiv preprint arXiv:2301.09043 (2023).

[39] Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, and Ge Li. 2024. Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models. arXiv preprint arXiv:2402.15938 (2024)

[40] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2024. Evaluating Large Language Models in Class-Level Code Generation. In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE). IEEE Computer Society, $865-865$

[41] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. 2024. Flocks of stochastic parrots: Differentially private prompt learning for large language models. Advances in Neural Information Processing Systems 36 (2024).

[42] Ruian Duan, Omar Alrawi, Ranjita Pai Kasturi, Ryan Elder, Brendan Saltaformaggio, and Wenke Lee. 2020. Towards measuring supply chain attacks on package managers for interpreted languages. arXiv preprint arXiv:2002.01139 (2020)

[43] Cynthia Dwork. 2006. Differential privacy. In International colloquium on automata, languages, and programming. Springer, 1-12.

[44] Avia Efrat, Or Honovich, and Omer Levy. 2022. Lmentry: A language model benchmark of elementary language tasks. arXiv preprint arXiv:2211.02069 (2022).

[45] Hugging Face. 2024. Hugging Face. https://huggingface.co/. Accessed: 2024-0328 .

[46] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng Liu. 2021. Compressing visual-linguistic model via knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. $1428-1438$.

[47] Sebastian Farquhar and Yarin Gal. 2018. Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 (2018).

[48] Emilio Ferrara. 2023. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint arXiv:2304.03738 (2023).

[49] Center for Research on Foundation Models (CRFM). 2024. The Foundation Model Transparency Index. https://crfm.stanford.edu/fmti/. Accessed: 2024-03-28.

[50] Bin Fu, Jialiu Lin, Lei Li, Christos Faloutsos, Jason Hong, and Norman Sadeh. 2013. Why people hate your app: Making sense of user feedback in a mobile app store. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. 1276-1284.

[51] Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, and Earlence Fernandes. 2023. Misusing Tools in Large Language Models With Visual Adversarial Examples. arXiv preprint arXiv:2310.03185 (2023).

[52] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2023. Bias and fairness in large language models: A survey. arXiv preprint arXiv:2309.00770 (2023).

[53] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2022. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726 (2022)

[54] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 (2020).

[55] Daniel M German, Yuki Manabe, and Katsuro Inoue. 2010. A sentence-matching method for automatic license identification of source code files. In Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering. $437-446$.

[56] Shahriar Golchin and Mihai Surdeanu. 2023. Data contamination quiz: A tool to detect and estimate contamination in large language models. arXiv preprint arXiv:2311.06233 (2023).

[57] Shahriar Golchin and Mihai Surdeanu. 2023. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493 (2023).

[58] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH) 3, 1 (2021), 1-23.

[59] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, et al. 2024. Xiezhi: An ever updating benchmark for holistic domain knowledge evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 18099-18107.

[60] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hallucinations in large vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 18135-18143.

[61] Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. 2023. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems 36 (2023), 59662-59688.

[62] Wenbo Guo, Zhengzi Xu, Chengwei Liu, Cheng Huang, Yong Fang, and Yang Liu. 2023. An Empirical Study of Malicious Code In PyPI Ecosystem. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 166-177.

[63] Akshat Gupta, Anurag Rao, and Gopala Anumanchipalli. 2024. Model Editing at Scale leads to Gradual and Catastrophic Forgetting. arXiv preprint arXiv:2401.07453 (2024).

[64] Yiyang Hao, Ge Li, Yongqiang Liu, Xiaowei Miao, He Zong, Siyuan Jiang, Yang Liu, and He Wei. 2022. Aixbench: A code generation benchmark dataset. arXiv preprint arXiv:2206.13179 (2022).

[65] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. 2019. Memory efficient experience replay for streaming learning. In 2019 International Conference on Robotics and Automation (ICRA). IEEE, 9769-9776

[66] Julian Hazell. 2023. Spear phishing with large language models. arXiv preprint arXiv:2305.06972 (2023).

[67] Joseph Hejderup, Arie van Deursen, and Georgios Gousios. 2018. Software ecosystem call graph for dependency management. In Proceedings of the 40th International Conference on Soft ware Engineering: New Ideas and Emerging Results. 101-104.

[68] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. 2022. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487 (2022).

[69] Jinchang Hu, Lyuye Zhang, Chengwei Liu, Sen Yang, Song Huang, and Yang Liu. 2023. Empirical Analysis of Vulnerabilities Life Cycle in Golang Ecosystem.
arXiv preprint arXiv:2401.00515 (2023)

[70] Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, et al. 2023. Flames: Benchmarking value alignment of chinese large language models. arXiv preprint arXiv:2311.06899 (2023).

[71] Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019. Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820 (2019).

[72] Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, and Margaret Mitchell. 2021. Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 560-575.

[73] Wiebke Hutiri, Aaron Yi Ding, Fahim Kawsar, and Akhil Mathur. 2023. Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows. ACM Transactions on Software Engineering and Methodology 32, 6 (2023), 1-37.

[74] Abhinav Jain, Hima Patel, Lokesh Nagalapatti, Nitin Gupta, Sameep Mehta, Shanmukha Guttula, Shashank Mujumdar, Shazia Afzal, Ruhi Sharma Mittal, and Vitobha Munigala. 2020. Overview and importance of data quality for machine learning tasks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining. 3561-3562.

[75] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code. arXiv preprint arXiv:2403.07974 (2024)

[76] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems 36 (2024).

[77] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1-38.

[78] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards Mitigating Hallucination in Large Language Models via Self-Reflection. arXiv preprint arXiv:2310.06271 (2023).

[79] Wenxin Jiang, Nicholas Synovic, Matt Hyatt, Taylor R Schorlemmer, Rohan Sethi, Yung-Hsiang Lu, George K Thiruvathukal, and James C Davis. 2023. An empirical study of pre-trained model reuse in the hugging face deep learning model registry. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2463-2475.

[80] Wenxin Jiang, Nicholas Synovic, Rohan Sethi, Aryan Indarapu, Matt Hyatt, Taylor R Schorlemmer, George K Thiruvathukal, and James C Davis. 2022. An empirical study of artifacts and security risks in the pre-trained model supply chain. In Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. 105-114.

[81] Zhongtao Jiang, Yuanzhe Zhang, Zhao Yang, Jun Zhao, and Kang Liu. 2021. Alignment rationale for natural language inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International foint Conference on Natural Language Processing (Volume 1: Long Papers). 5372-5387.

[82] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning. PMLR, 15696-15707.

[83] Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training data mitigates privacy risks in language models. In International Conference on Machine Learning. PMLR, 10697-10707.

[84] Antonia Karamolegkou, Jiaang Li, Li Zhou, and Anders Sรธgaard. 2023. Copyright violations and large language models. arXiv preprint arXiv:2310.13771 (2023).

[85] Jonathan Katzy, Rฤzvan-Mihai Popescu, Arie van Deursen, and Maliheh Izadi. 2024. An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets. arXiv preprint arXiv:2403.15230 (2024).

[86] Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. 2021. Achieving forgetting prevention and knowledge transfer in continual learning. Advances in Neural Information Processing Systems 34 (2021), 22443-22456.

[87] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. 2024. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. Advances in Neural Information Processing Systems 36 (2024).

[88] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. 2024. Propile: Probing privacy leakage in large language models. Advances in Neural Information Processing Systems 36 (2024).

[89] Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano. 2021. Bias out-ofthe-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems 34 (2021), 2611-2624.

[90] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114, 13 (2017), 35213526.

[91] Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. 2023. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329 (2023)

[92] Andreas Kรถpf, Yannic Kilcher, Dimitri von Rรผtte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richรกrd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems 36 (2024).

[93] Piergiorgio Ladisa, Henrik Plate, Matias Martinez, and Olivier Barais. 2022 Taxonomy of attacks on open-source software supply chains. arXiv preprint arXiv:2204.04008 (2022).

[94] LangChain-AI. 2024. LangChain. https://github.com/langchain-ai/langchain. Accessed: 2024-03-28.

[95] Gunwoong Lee and T Santanam Raghu. 2014. Determinants of mobile apps success: Evidence from the app store market. Journal of Management Information Systems 31, 2 (2014), 133-170.

[96] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).

[97] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems 35 (2022), 34586-34599

[98] Timothรฉe Lesort, Andrei Stoian, and David Filliat. 2019. Regularization shortcomings for continual learning. arXiv preprint arXiv:1912.03049 (2019).

[99] Changmao Li and Jeffrey Flanigan. 2024. Task contamination: Language models may not be few-shot anymore. In Proceedings of the AAAI Conference on Artificia Intelligence, Vol. 38. 18471-18480.

[100] Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang Yang Liu, Guoai Xu, Guosheng Xu, and Haoyu Wang. 2024. Digger: Detecting Copyright Content Mis-usage in Large Language Model Training. arXiv preprint arXiv:2401.00676 (2024).

[101] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination evaluation benchmark for large language models In The 2023 Conference on Empirical Methods in Natural Language Processing.

[102] Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viรฉgas Hanspeter Pfister, and Martin Wattenberg. 2024. Measuring and Controlling Persona Drift in Language Model Dialogs. arXiv preprint arXiv:2402.10962 (2024).

[103] Kenneth Li, Oam Patel, Fernanda Viรฉgas, Hanspeter Pfister, and Martin Wat tenberg. 2024. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems 36 (2024).

[104] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024 Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arXiv preprint arXiv:2403.20041 (2024).

[105] Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. 2006. t-closeness: Privacy beyond k-anonymity and l-diversity. In 2007 IEEE 23rd international conference on data engineering. IEEE, 106-115.

[106] Ningke Li, Shenao Wang, Mingxi Feng, Kailong Wang, Meizhen Wang, and Haoyu Wang. 2023. MalWuKong: Towards Fast, Accurate, and Multilingual Detection of Malicious Code Poisoning in OSS Supply Chains. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE) IEEE, 1993-2005.

[107] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852 (2023).

[108] Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, and Jialiang Lu. 2021. Hidden backdoors in human-centric language models. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security. 3123-3140.

[109] Yucheng Li. 2023. Estimating contamination via perplexity: Quantifying memo risation in language model evaluation. arXiv preprint arXiv:2309.10677 (2023).

[110] Yucheng Li, Frank Guerin, and Chenghua Lin. 2024. Latesteval: Addressing data contamination in language model evaluation through dynamic and timesensitive test construction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 18600-18607.

[111] Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, and Yang Liu. 2023. Multi-target backdoor attacks for code pre-trained models. arXiv preprint arXiv:2306.08350 (2023)

[112] Yuanchun Li, Ziqi Zhang, Bingyan Liu, Ziyue Yang, and Yunxin Liu. 2021. Mod elDiff: Testing-based DNN similarity comparison for model reuse detection In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis. 139-151.
[113] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).

[114] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Towards understanding and mitigating social biases in language models. In International Conference on Machine Learning. PMLR, 6565-6576.

[115] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 (2021).

[116] Chengwei Liu, Sen Chen, Lingling Fan, Bihuan Chen, Yang Liu, and Xin Peng. 2022. Demystifying the vulnerability propagation and its evolution via dependency trees in the npm ecosystem. In Proceedings of the 44th International Conference on Software Engineering. 672-684.

[117] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676 (2023)

[118] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. 2024. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 (2024).

[119] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems 36 (2024).

[120] Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, and Soroush Vosoughi. 2022. Quantifying and alleviating political bias in language models. Artificial Intelligence 304 (2022), 103654.

[121] Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. 2024. Datasets for Large Language Models: A Comprehensive Survey. arXiv preprint arXiv:2402.18041 (2024).

[122] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 2023. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860 (2023).

[123] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. arXiv preprint arXiv:2308.05374 (2023).

[124] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. 2024. MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. arXiv preprint arXiv:2402.14905 (2024).

[125] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. 2023. The data provenance initiative: A large scale audit of dataset licensing \& attribution in ai. arXiv preprint arXiv:2310.16787 (2023).

[126] Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-Bรฉguelin. 2023. Analyzing leakage of personally identifiable information in language models. In 2023 IEEE Symposium on Security and Privacy $(S P)$. IEEE, 346-363.

[127] Lucy Ellen Lwakatare, Ellinor Rรฅnge, Ivica Crnkovic, and Jan Bosch. 2021. On the experiences of adopting automated data validation in an industrial machine learning project. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 248-257.

[128] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems 36 (2023), 21702-21720.

[129] Yuxing Ma. 2018. Constructing supply chains in open source software. In Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings. 458-459.

[130] Ashwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakrishnan Venkitasubramaniam. 2007. l-diversity: Privacy beyond k-anonymity. Acm transactions on knowledge discovery from data (tkdd) 1, 1 (2007), 3-es.

[131] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems 36 (2024).

[132] Nishtha Madaan, Adithya Manjunatha, Hrithik Nambiar, Aviral Goel, Harivansh Kumar, Diptikalyan Saha, and Srikanta Bedathur. 2023. DetAIL: a tool to automatically detect and analyze drift in language. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 15767-15773.

[133] Pooria Madani. 2023. Metamorphic Malware Evolution: The Potential and Peril of Large Language Models. In 2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA). IEEE Computer Society, 74-81.

[134] Inbal Magar and Roy Schwartz. 2022. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242 (2022).

[135] William Martin, Federica Sarro, Yue Jia, Yuanyuan Zhang, and Mark Harman. 2016. A survey of app store analysis for software engineering. IEEE transactions on software engineering 43, 9 (2016), 817-847.

[136] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karlaลก, William Gaviria Ro jas, Sudnya Diamos, Greg Diamos, Lynn He, Alicia Parrish, Hannah Rose Kirk, et al. 2024. Dataperf: Benchmarks for data-centric ai development. Advances in Neural Information Processing Systems 36 (2024).

[137] Timothy R McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N Halgamuge. 2024. Inadequacies of large language model benchmarks in the era of generative artificial intelligence. arXiv preprint arXiv:2402.09880 (2024).

[138] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkoy, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023 Generating benchmarks for factuality evaluation of language models. arXiv preprint arXiv:2307.06908 (2023).

[139] Manish Nagireddy, Lamogha Chiazor, Moninder Singh, and Ioana Baldini. 2024 Socialstigmaqa: A benchmark to uncover stigma amplification in generative language models. In Proceedings of the AAAI Conference on Artificial Intelligence Vol. 38. 21454-21462

[140] Mohammad Hossein Namaki, Avrilia Floratou, Fotis Psallidas, Subru Krishnan, Ashvin Agrawal, Yinghui Wu, Yiwen Zhu, and Markus Weimer. 2020. Vamsa Automated provenance tracking in data science scripts. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining. 1542-1551.

[141] Roberto Navigli, Simone Conia, and Bjรถrn Ross. 2023. Biases in large language models: origins, inventory, and discussion. ACM fournal of Data and Information Quality 15, 2 (2023), 1-21.

[142] The Hacker News. 2024. New Hugging Face Vulnerability Exposes AI Models to Supply Chain Attacks. https://thehackernews.com/2024/02/new-huggingface-vulnerability-exposes.html. Accessed: 2024-03-28.

[143] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. 2017. Variational continual learning. arXiv preprint arXiv:1710.10628 (2017).

[144] OpenAI. 2024. GPTs. https://chat.openai.com/gpts. Accessed: 2024-03-28

[145] OpenAI. 2024. Introducing the GPT Store. https://openai.com/blog/introducingthe-gpt-store. Accessed: 2024-03-28.

[146] Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B Hashimoto. 2023. Proving test set contamination in black box language models arXiv preprint arXiv:2310.17623 (2023).

[147] Kurez Oroy and Julia Evan. 2024. Continual Learning with Large Language Models: Adapting to Concept Drift and New Data Streams. Technical Report. EasyChair.

[148] Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. 2021. Probing toxic content in large pre-trained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International foint Conference on Natural Language Processing (Volume 1: Long Papers). 4262-4274.

[149] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022 Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730-27744.

[150] OWASP. 2024. OWASP Top 10 for Large Language Model Applications. https: //owasp.org/www-project-top-10-for-large-language-model-applications/. Accessed: 2024-03-28.

[151] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D Lawrence. 2022. Challenges in deploying machine learning: a survey of case studies. ACM computing surveys 55,6 (2022), 1-29.

[152] Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Reducing gender bias in abusive language detection. arXiv preprint arXiv:1808.07231 (2018).

[153] John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, and Ion Androutsopoulos. 2020. Toxicity detection: Does context really matter? arXiv preprint arXiv:2006.00998 (2020).

154] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qi uyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 (2023).

[155] Richard Plant, Valerio Giuffrida, and Dimitra Gkatzia. 2022. You are what you write: Preserving privacy in the era of large language models. arXiv preprint arXiv:2204.09391 (2022)

[156] Neoklis Polyzotis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven Whang 2019. Data validation for machine learning. Proceedings of machine learning and systems 1 (2019), 334-347.

[157] Mahesh Datta Sai Ponnuru, Likhitha Amasala, and Guna Chaitanya Garikipati. 2024. Unveiling the Veil: A Comprehensive Analysis of Data Contamination in Leading Language Models. (2024)

[158] PyTorch. 2024. PyTorch. https://github.com/pytorch/pytorch. Accessed: 202403-28.

[159] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. 2024. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on Artificial Intelligence Vol. 38. 21527-21536.

[160] The Record. 2024. Thousands of companies using Ray framework exposed to cyberattacks, researchers say. https://therecord.media/thousands-exposed-to- ray-framework-vulnerability. Accessed: 2024-03-28

[161] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. 2019. Experience replay for continual learning. Advances in neural information processing systems 32 (2019)

[162] Lukas Rupprecht, James C Davis, Constantine Arnold, Yaniv Gur, and Deepavali Bhagwat. 2020. Improving reproducibility of data science pipelines through transparent provenance capture. Proceedings of the VLDB Endowment 13, 12 (2020), 3354-3368

[163] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. "Everyone wants to do the model work, not the data work": Data Cascades in High-Stakes AI. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (<conf-loc>, $<$ city $>$ Yokohama</city $>$, <country $>$ Japan</country>, </conf-loc>) (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 39, 15 pages. https://doi.org/10.1145/3411764.3445518

[164] Walt Scacchi, Joseph Feller, Brian Fitzgerald, Scott Hissam, and Karim Lakhani. 2006. Understanding free/open source software development processes. , 95105 pages.

[165] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. 2021. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In International Conference on Machine Learning. PMLR, 9389-9398.

[166] Shreya Shankar, Rolando Garcia, Joseph M Hellerstein, and Aditya G Parameswaran. 2022. Operationalizing machine learning: An interview study. arXiv preprint arXiv:2209.09125 (2022).

[167] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. 2023. Large language model alignment: A survey. arXiv preprint arXiv:2309.15025 (2023).

[168] Dan Shi, Chaobin You, Jiantao Huang, Taihao Li, and Deyi Xiong. 2024. CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 18952-18960.

[169] Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu. 2021. Selective differential privacy for language modeling. arXiv preprint arXiv:2108.12944 (2021).

[170] Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. 2022. Just fine-tune twice: Selective differential privacy for large language models. arXiv preprint arXiv:2204.07667 (2022).

[171] Karthik Shivashankar and Antonio Martini. 2022. Maintainability challenges in ML: A systematic literature review. In 2022 48th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). IEEE, 60-67

[172] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2022. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150 (2022).

[173] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. 2024. Rethinking Interpretability in the Era of Large Language Models. arXiv preprint arXiv:2402.01761 (2024).

[174] Trevor Stalnaker, Nathan Wintersgill, Oscar Chaparro, Massimiliano Di Penta, Daniel M German, and Denys Poshyvanyk. 2024. BOMs Away! Inside the Minds of Stakeholders: A Comprehensive Study of Bills of Materials for Software Systems. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. 1-13

[175] Trevor Wayne Stalnaker. 2023. A Comprehensive Study of Bills of Materials for Software Systems. Ph. D. Dissertation. The College of William and Mary.

[176] Ryan Steed, Swetasudha Panda, Ari Kobren, and Michael Wick. 2022. Upstream mitigation is not all you need: Testing the bias transfer hypothesis in pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 3524-3542.

[177] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 19053-19061.

[178] Zhensu Sun, Xiaoning Du, Fu Song, Mingze Ni, and Li Li. 2022. Coprotector: Protect open-source code against unauthorized training usage with data poisoning. In Proceedings of the ACM Web Conference 2022. 652-660.

[179] Latanya Sweeney. 2002. k-anonymity: A model for protecting privacy. International journal of uncertainty, fuzziness and knowledge-based systems 10, 05 (2002), 557-570.

[180] Zhen Tan, Tianlong Chen, Zhenyu Zhang, and Huan Liu. 2024. Sparsity-guided holistic explanation for llms with interpretable inference-time intervention. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 2161921627 .

[181] MingJie Tang, Saisai Shao, Weiqing Yang, Yanbo Liang, Yongyang Yu, Bikas Saha, and Dongjoon Hyun. 2019. Sac: A system for big data lineage tracking. In 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEE, $1964-1967$

[182] Mina Taraghi, Gianolli Dorcelus, Armstrong Foundjem, Florian Tambon, and Foutse Khomh. 2024. Deep Learning Model Reuse in the HuggingFace Community: Challenges, Benefit and Trends. arXiv preprint arXiv:2401.13177 (2024).

[183] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)

[184] TensorFlow. 2024. TensorFlow. https://github.com/tensorflow/tensorflow. Accessed: 2024-03-28.

[185] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. 2024. D4: Improving llm pretraining via document de-duplication and diversification Advances in Neural Information Processing Systems 36 (2024).

[186] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313 (2024).

[187] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothรฉe Lacroix, Baptiste Roziรจre, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).

[188] Timo Tuunanen, Jussi Koskinen, and Tommi Kรคrkkรคinen. 2009. Automated software license analysis. Automated Software Engineering 16 (2009), 455-490.

[189] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2024. Planbench: An extensible benchmark for evaluat ing large language models on planning and reasoning about change. Advances in Neural Information Processing Systems 36 (2024).

[190] Sander Van Der Burg, Eelco Dolstra, Shane McIntosh, Julius Davies, Daniel M German, and Armijn Hemel. 2014. Tracing software build processes to uncover license compliance inconsistencies. In Proceedings of the 29th ACM/IEEE international conference on Automated software engineering. 731-742.

[191] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987 (2023).

[192] Christopher Vendome. 2015. A large scale study of license usage on github In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 2. IEEE, 772-774

[193] Christopher Vendome, Gabriele Bavota, Massimiliano Di Penta, Mario LinaresVรกsquez, Daniel German, and Denys Poshyvanyk. 2017. License usage and changes: a large-scale study on github. Empirical Software Engineering 22 (2017), 1537-1577.

[194] Christopher Vendome, Mario Linares-Vรกsquez, Gabriele Bavota, Massimiliano Di Penta, Daniel German, and Denys Poshyvanyk. 2015. License usage and changes: a large-scale study of java projects on github. In 2015 IEEE 23rd Inter national Conference on Program Comprehension. IEEE, 218-228.

[195] Christopher Vendome, Mario Linares-Vรกsquez, Gabriele Bavota, Massimiliano Di Penta, Daniel German, and Denys Poshyvanyk. 2017. Machine learning-based detection of open source license exceptions. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE, 118-129.

[196] Christopher Vendome and Denys Poshyvanyk. 2016. Assisting developers with license compliance. In Proceedings of the 38th International Conference on Software Engineering Companion. 811-814.

[197] Jianwu Wang, Daniel Crawl, Shweta Purawat, Mai Nguyen, and Ilkay Altintas. 2015. Big data provenance: Challenges, state of the art and opportunities. In 2015 IEEE international conference on big data (Big Data). IEEE, 2509-2516

[198] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. 2023. Adversarial demonstration attacks on large language models. arXiv preprint arXiv:2305.14950 (2023).

[199] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al 2022. Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 214-229.

[200] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin and Po-Sen Huang. 2021. Challenges in detoxifying language models. arXiv preprint arXiv:2109.07445 (2021)

[201] Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. 2023. Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082 (2023).

[202] Thomas Wolter, Ann Barcomb, Dirk Riehle, and Nikolay Harutyunyan. 2023 Open source license inconsistencies on github. ACM Transactions on Software Engineering and Methodology 32, 5 (2023), 1-23.

[203] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021. One teacher is enough? pre-trained language model distillation from multiple teachers. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_63e0e973460068c41146g-13.jpg?height=38&width=241&top_left_y=2236&top_left_x=243)

[204] Ming-Wei Wu and Ying-Dar Lin. 2001. Open Source software development: An overview. Computer 34, 6 (2001), 33-38.

[205] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann 2023. Bloomberggpt: A large language model for finance. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_63e0e973460068c41146g-13.jpg?height=33&width=241&top_left_y=2404&top_left_x=243)

[206] Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and Gholamreza Haffari. 2021. Pretrained language model in continual learning: A comparative study. In International conference on learning representations.

[207] Yuhao Wu, Yuki Manabe, Tetsuya Kanda, Daniel M German, and Katsuro Inoue. 2017. Analysis of license inconsistency in large collections of open source projects. Empirical Software Engineering 22 (2017), 1194-1222.

[208] Boming Xia, Tingting Bi, Zhenchang Xing, Qinghua Lu, and Liming Zhu. 2023. An empirical study on software bill of materials: Where we stand and the road ahead. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2630-2642.

[209] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, $38087-38099$.

[210] Doris Xin, Hui Miao, Aditya Parameswaran, and Neoklis Polyzotis. 2021. Production machine learning pipelines: Empirical analysis and optimization opportunities. In Proceedings of the 2021 International Conference on Management of Data. 2639-2652.

[211] Runxin Xu, Fuli Luo, Chengyu Wang, Baobao Chang, Jun Huang, Songfang Huang, and Fei Huang. 2022. From dense to sparse: Contrastive pruning for better pre-trained language model compression. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 11547-11555.

[212] Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, Mathias Humbert, Pascal Berrang, and Yang Zhang. 2023. Data poisoning attacks against multimodal encoders. In International Conference on Machine Learning. PMLR, 39299-39313.

[213] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy, explainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024).

[214] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing (2024), 100211.

[215] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems 35 (2022), 27168-27183.

[216] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. Edgemoe: Fast on-device inference of moe-based large language models. arXiv preprint arXiv:2308.14352 (2023).

[217] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. LLM as a System Service on Mobile Devices. arXiv preprint arXiv:2403.11805 (2024).

[218] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023. 6032-6048.

[219] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. 1-12.

[220] Ping Yu, Hua Xu, Xia Hu, and Chao Deng. 2023. Leveraging generative AI and large Language models: a Comprehensive Roadmap for Healthcare Integration. In Healthcare, Vol. 11. MDPI, 2776.

[221] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2024. Large language model as attributed training data generator: A tale of diversity and bias. Advances in Neural Information Processing Systems 36 (2024).

[222] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. 2018. Task agnostic continual learning using online variational bayes. arXiv preprint arXiv:1803.10123 (2018).

[223] Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, and Hang Su. 2023. Text-to-image diffusion models can be easily backdoored through multimodal data poisoning. In Proceedings of the 31st ACM International Conference on Multimedia. 1577-1587.

[224] Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, and Dinghao Wu. 2023. On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused? arXiv preprint arXiv:2310.01581 (2023).

[225] Jiang Zhang, Qiong Wu, Yiming Xu, Cheng Cao, Zheng Du, and Konstantinos Psounis. 2024. Efficient toxic content detection by bootstrapping and distilling large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 21779-21787.

[226] Lyuye Zhang, Chengwei Liu, Sen Chen, Zhengzi Xu, Lingling Fan, Lida Zhao, Yiran Zhang, and Yang Liu. 2023. Mitigating persistence of open-source vulnerabilities in maven ecosystem. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 191-203

[227] Yi Zhang, Zachary Ives, and Dan Roth. 2020. "Who said it, and Why?" Provenance for Natural Language Claims. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4416-4426.

[228] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren's song in the

AI ocean: a survey on hallucination in large language models. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_63e0e973460068c41146g-14.jpg?height=38&width=241&top_left_y=325&top_left_x=243)

[229] Yanjie Zhao, Li Li, Haoyu Wang, Haipeng Cai, Tegawendรฉ F Bissyandรฉ, Jacques Klein, and John Grundy. 2021. On the impact of sample duplication in machinelearning-based android malware detection. ACM Transactions on Software Engineering and Methodology (TOSEM) 30, 3 (2021), 1-38.

[230] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Man Cheung, and Min Lin. 2024. On evaluating adversarial robustness of large visionlanguage models. Advances in Neural Information Processing Systems 36 (2024).
[231] Yiran Zhao, Jinghan Zhang, I Chern, Siyang Gao, Pengfei Liu, Junxian He, et al. 2024. Felm: Benchmarking factuality evaluation of large language models. Advances in Neural Information Processing Systems 36 (2024).

[232] Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, et al. 2024. EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models. arXiv preprint arXiv:2403.12171 (2024)

[233] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. arXiv preprint arXiv:2301.12867 (2023).


[^0]:    ${ }^{1}$ For simplicity in this text, both pre-trained LLMs and LMMs will be collectively referred to as LLMs, and their supply chains will be referred to as the LLM Supply Chain in the subsequent sections.

