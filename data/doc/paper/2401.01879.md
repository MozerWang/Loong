# Theoretical guarantees on the best-of-n alignment policy 

Ahmad Beirami ${ }^{\dagger}$<br>Alekh Agarwal $^{\dagger}$ Jonathan Berant ${ }^{\S}$<br>Chirag Nagpal ${ }^{\dagger}$<br>Ananda Theertha Suresh ${ }^{\dagger}$<br>${ }^{\dagger}$ Google Research<br>${ }^{\S}$ Google DeepMind<br>best-of-n-theoretical-guarantees-paper@google.com

Alexander D'Amour ${ }^{\S}$ Jacob Eisenstein ${ }^{\S}$


#### Abstract

A simple and effective method for the alignment of generative models is the best-of- $n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of- $n$ policy and the base policy is equal to $\log (n)-(n-1) / n$. We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.


## 1 Introduction

Generative language models have shown to be effective general purpose tools to solve various problems. While many problems can be solved in a zero-shot manner, the output from the so-called supervised finetuned (SFT) model may not be outright desirable, e.g., it may violate safety rules. Alignment aims at remedying this issue by further nudging the output to improve a reward function while not drifting too far from the SFT model (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022).

Recently, there has been a proliferation of methods for alignment, which include KL-regularized reinforcement learning (Christiano et al., 2017; Ouyang et al., 2022), controlled decoding (Yang \& Klein, 2021; Mudgal et al., 2023), SLiC (Zhao et al., 2022), direct preference optimization (Rafailov et al., 2023), and best-of- $n$ finetuning (Touvron et al., 2023). At their core, these methods try to (approximately) solve the following constrained optimization problem:

![](https://cdn.mathpix.com/cropped/2024_06_04_087005050d53a937953ag-01.jpg?height=90&width=1151&top_left_y=1988&top_left_x=487)

where $p_{x}$ is a distribution over prompts on which the alignment optimization takes place; $p_{y \mid x}$ denotes a base language model; $r(\boldsymbol{x}, \boldsymbol{y}) \in \mathbb{R}$ represent a scalar reward associated with response $\boldsymbol{y}$ in context $\boldsymbol{x}$; and $\mathrm{KL}\left(q_{y \mid x} \| p_{y \mid x}\right)$ is defined as

$$
\begin{equation*}
\operatorname{KL}\left(q_{\boldsymbol{y} \mid x} \| p_{\boldsymbol{y} \mid \boldsymbol{x}}\right):=E_{\boldsymbol{x} \sim p_{\boldsymbol{x}}} E_{\boldsymbol{y} \sim q_{\boldsymbol{y} \mid \boldsymbol{x}}} \log \frac{q_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})}{p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})} \tag{2}
\end{equation*}
$$

We also define the context-conditional KL divergence as follows:

$$
\begin{equation*}
\mathrm{KL}\left(q_{\boldsymbol{y} \mid \boldsymbol{x}} \| p_{\boldsymbol{y} \mid x} \mid \boldsymbol{x}\right):=E_{\boldsymbol{y} \sim q_{\boldsymbol{y} \mid \boldsymbol{x}}} \log \frac{q_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})}{p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})} \tag{3}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_087005050d53a937953ag-02.jpg?height=686&width=1028&top_left_y=348&top_left_x=516)

Figure 1: The analytical formula $(\log (n)-(n-1) / n)$, Eq. (5), the exact KL divergence, Eq. (6), and the proposed estimator, Eq. (28), for Example 1, illustrating a case where the gap between the analytical formula and the exact $\mathrm{KL}$ divergence is unbounded.

where $\operatorname{KL}\left(q_{y \mid x} \| p_{y \mid x}\right)=E_{x \sim p_{x}} \mathrm{KL}\left(q_{y \mid x} \| p_{y \mid x} \mid x\right)$. Here a small KL divergence between the aligned policy and the SFT policy is desired because it implies that the capabilities of the SFT policy are largely preserved in the aligned policy. To compare different alignment techniques, it is customary to produce tradeoff curves that measure expected reward (or win rate) as a function of $\operatorname{KL}\left(\pi_{y \mid x} \| p_{y \mid x}\right)$ for aligned policy $\pi$. Here, it is desirable to improve the reward with the least drift measured in KL divergence.

Despite all the advancements in alignment, a simple, popular, and well-performing method for alignment remains to be the best-of-n policy (Nakano et al., 2021; Stiennon et al., 2020). In fact, Gao et al. (2023); Mudgal et al. (2023); Eisenstein et al. (2023) show that best-of- $n$ consistently achieves compelling reward-KL tradeoff curves, that even dominate those of KL-regularized reinforcement learning and other more involved alignment policies. Llama 2 (Touvron et al., 2023) uses the best-of- $n$ policy outcomes to further finetune the base model as an effective alignment strategy.

Let $\boldsymbol{x}$ be a given input prompt to the language model. Let $\boldsymbol{y}_{1}, \ldots, \boldsymbol{y}_{n}$ be $n$ i.i.d. samples drawn from $p_{\boldsymbol{y} \mid \boldsymbol{x}}(\cdot \mid \boldsymbol{x})$. The best-of- $n$ strategy selects ${ }^{1}$

$$
\begin{equation*}
\boldsymbol{y}=\boldsymbol{y}_{k^{*}} \quad \text { where } \quad k^{*}:=\arg \max _{k \in[n]} r\left(\boldsymbol{x}, \boldsymbol{y}_{k}\right) \tag{4}
\end{equation*}
$$

This process inherently leads to sampling from a new policy that is aligned to the reward, denoted by $\pi_{y \mid x}^{(n)}$. Notice that $\pi_{y \mid x}^{(1)}=p_{y \mid x}$, and increasing $n$ leads to increasing the expected reward in the outcome of the new policy at the cost of drifting away from the base model. Our goal in this paper is to better understand the best-of- $n$ policy. In particular, we are interested in theoretical guarantees on $\operatorname{KL}\left(\pi_{y \mid x}^{(n)} \| p_{y \mid x}\right)$ for different values of $n$. A commonly used expression in the literature (Stiennon et al., 2020; Hilton \& Gao, 2022 (Accessed on January 3, 2024; Coste et al., 2023; Gao et al., 2023; Go et al., 2023; Scheurer et al., 2023) claims that

$$
\begin{equation*}
\mathrm{KL}\left(\pi_{y \mid x}^{(n)} \| p_{y \mid x}\right) \stackrel{\text { claim }}{=} \widetilde{\mathrm{KL}}_{n}:=\log (n)-(n-1) / n \tag{5}
\end{equation*}
$$[^0]

This formula is commonly used to demonstrate KL-reward tradeoffs for the best-of- $n$ policy. Let us further inspect this formula using a toy example.

Example 1. Consider an unprompted model with $\boldsymbol{x}=\emptyset$ (no input) and binary output, $\boldsymbol{y} \in\{0,1\}$. Let the two outcomes be equiprobable, i.e., $p_{y \mid x}(0)=p_{y \mid x}(1)=\frac{1}{2}$. Further, let $r(0)=0$, and $r(1)=1$, i.e., outcome 1 is more desirable than outcome 0 . Here, we can compute $\pi_{y \mid x}^{(n)}$ in closed form. Specifically, we can see that $\pi_{y \mid x}^{(n)}(0)=\frac{1}{2^{n}}$ and $\pi_{y \mid x}^{(n)}(1)=1-\frac{1}{2^{n}}$. Thus,

$$
\begin{equation*}
K L\left(\pi_{y \mid x}^{(n)} \| p_{y \mid x}\right)=\log (2)-h\left(\frac{1}{2^{n}}\right) \tag{6}
\end{equation*}
$$

where $h(\cdot)$ is the binary entropy function. ${ }^{2}$ We compare the exact closed-form expression for KL divergence with the analytical formula in (5). As can be seen in Figure 1 (and is evident from (6)), the true KL is upper bounded by $\log (2)$ for all $n$, whereas $\widetilde{K L}$ grows unbounded as $n \rightarrow \infty$. In this paper, we also report a new estimator for $K L$ divergence that closely mirrors the true $K L$ divergence.

As we learnt from Example 1, the KL divergence of the best-of- $n$ policy and the base policy may be quite different from the analytical formula used in the literature. In the rest of this paper, we shed some light on this formula, derive bounds on the KL divergence, and propose a new estimator for the KL divergence that better captures the behavior of the KL divergence.

## 2 Derivation of the best-of-n policy

Before we try to bound the KL divergence of the best-of- $n$ policy, we provide a general derivation for the best-of- $n$ policy under two general assumptions. Let $r(\boldsymbol{x}, \boldsymbol{y}) \in \mathbb{R}$ represent the scalar reward of response $\boldsymbol{y}$ in context $\boldsymbol{x}$.

Assumption 1. We assume that the reward $r(\boldsymbol{x}, \boldsymbol{y})$ is unique for all $\boldsymbol{x}, \boldsymbol{y}$.

Assumption 2. Let $\mathcal{Y}^{*}:=\left\{\boldsymbol{y} \mid \max _{\boldsymbol{x} \in \mathcal{X}} p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})>0\right\}$. We assume that the language model is such that $\left|\mathcal{Y}^{*}\right|<\infty$, i.e., there are finite possible outcomes (in each context).

Note that Assumptions 1-2 are fairly non-restrictive and make the presentation clearer, and they may be relaxed with some work and extra notation.

The following result gives the probability mass function (PMF) of the best-of- $n$ policy.

Lemma 1. Under Assumptions 1-2, for all $n \in \mathbb{N}$, the PMF of the best-of-n policy is given by

$$
\begin{equation*}
\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)}(\boldsymbol{y} \mid \boldsymbol{x})=\mathcal{F}(\boldsymbol{y} \mid \boldsymbol{x})^{n}-\mathcal{F}^{-}(\boldsymbol{y} \mid \boldsymbol{x})^{n} \tag{7}
\end{equation*}
$$

where

$$
\begin{align*}
\mathcal{F}(\boldsymbol{y} \mid \boldsymbol{x}) & :=P_{\boldsymbol{z} \sim p_{\boldsymbol{y} \mid x}}[r(\boldsymbol{x}, \boldsymbol{z}) \leq r(\boldsymbol{x}, \boldsymbol{y})]  \tag{8}\\
\mathcal{F}^{-}(\boldsymbol{y} \mid \boldsymbol{x}) & :=P_{\boldsymbol{z} \sim p_{y \mid x}}[r(\boldsymbol{x}, \boldsymbol{z})<r(\boldsymbol{x}, \boldsymbol{y})] \tag{9}
\end{align*}
$$

Proof. Let $\mathcal{Y}_{x}$ be the set of all possible outcomes of the language model, given prompt $\boldsymbol{x}$, i.e., $\mathcal{Y}_{x}:=$ $\left\{\boldsymbol{y} \mid p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})>0\right\}$. Further, let $L_{\boldsymbol{x}}:=\left|\mathcal{Y}_{\boldsymbol{x}}\right|<\infty$ (Assumption 2). We order all possible $L_{x}$ outcomes as[^1]$\left\{\widetilde{\boldsymbol{y}}_{i}\right\}_{i \in\left[L_{\boldsymbol{x}}\right]}$ such that if $r\left(\boldsymbol{x}, \widetilde{\boldsymbol{y}}_{j}\right)>r\left(\boldsymbol{x}, \widetilde{\boldsymbol{y}}_{i}\right)$, then $j>i$. In other words, $\widetilde{\boldsymbol{y}}_{1}$ is the least desirable outcome associated with the lowest reward, and $\widetilde{\boldsymbol{y}}_{L_{x}}$ is the most desirable outcome associated with the highest reward.

First notice that sampling from $p_{\boldsymbol{y} \mid x}$ is akin to sampling $u \sim \mathcal{U}[0,1]$, and returning $\widetilde{\boldsymbol{y}}_{i}$, such that

$$
\begin{equation*}
\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i-1} \mid \boldsymbol{x}\right) \leq u<\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i} \mid \boldsymbol{x}\right) \tag{10}
\end{equation*}
$$

Similarly, sampling from the best-of- $n$ strategy is akin to sampling $u_{1}, \ldots, u_{n} \stackrel{\text { i.i.d. }}{\sim} \mathcal{U}[0,1]$, and returning $\widetilde{\boldsymbol{y}}_{i}$, such that

$$
\begin{equation*}
\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i-1} \mid \boldsymbol{x}\right) \leq \max _{k \in[n]} u_{k}<\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i} \mid \boldsymbol{x}\right) \tag{11}
\end{equation*}
$$

On the other hand, we know that the $\mathrm{CDF}$ of the maximum for all $\tau \in[0,1]$ is given by

$$
\begin{equation*}
P\left[\max _{k \in[n]} u_{k} \leq \tau\right]=\tau^{n} \tag{12}
\end{equation*}
$$

Hence, for all $n \in \mathbb{N}$, the PMF of the best-of- $n$ policy, denoted as $\pi_{y \mid x}^{(n)}$ is given by

$$
\begin{equation*}
\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)}\left(\widetilde{\boldsymbol{y}}_{i} \mid \boldsymbol{x}\right)=\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i} \mid \boldsymbol{x}\right)^{n}-\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i-1} \mid \boldsymbol{x}\right)^{n} \quad \forall i \in\left[L_{\boldsymbol{x}}\right] \tag{13}
\end{equation*}
$$

where $\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{0} \mid \boldsymbol{x}\right):=0$, and

$$
\begin{equation*}
\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i} \mid \boldsymbol{x}\right)=\sum_{l \in[i]} p_{\boldsymbol{y} \mid \boldsymbol{x}}\left(\widetilde{\boldsymbol{y}}_{l} \mid \boldsymbol{x}\right) \tag{14}
\end{equation*}
$$

The proof is completed by noticing that $\mathcal{F}\left(\widetilde{\boldsymbol{y}}_{i-1} \mid \boldsymbol{x}\right)=\mathcal{F}^{-}\left(\widetilde{\boldsymbol{y}}_{i} \mid \boldsymbol{x}\right)$.

Notice that if $n=1$, then $\pi_{y \mid x}^{(1)}\left(\widetilde{\boldsymbol{y}}_{i} \mid x\right)=p_{\boldsymbol{y} \mid \boldsymbol{x}}\left(\widetilde{\boldsymbol{y}}_{i} \mid x\right)$. For any $n$, Lemma 1 gives a closed-form expression for $\pi_{y \mid x}^{(n)}\left(\widetilde{\boldsymbol{y}}_{i} \mid x\right)$, which we will use subsequently to derive theoretical guarantees on $\operatorname{KL}\left(\pi_{y \mid x}^{(n)} \| p_{\boldsymbol{y} \mid x} \mid \boldsymbol{x}\right)$ for any $n$ and $\boldsymbol{x}$.

## 3 Relations between the KL divergence and the analytical formula

Our first result shows that the analytical formula is an upper bound on the (context-dependent) KL divergence. The proofs for this result and several subsequent results are relegated to Appendix A.

Theorem 1. For any $n \in \mathbb{N}$ and any $\boldsymbol{x}$,

$$
\begin{equation*}
K L\left(\pi_{y \mid x}^{(n)} \| p_{y \mid x} \mid x\right) \leq \widetilde{K L}_{n}=\log (n)-\frac{n-1}{n} \tag{15}
\end{equation*}
$$

Corollary 1. For any $n$,

$$
\begin{equation*}
K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}}\right)=E_{\boldsymbol{x} \sim p_{\boldsymbol{x}}} K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) \leq \widetilde{K L}_{n}=\log (n)-\frac{n-1}{n} \tag{16}
\end{equation*}
$$

Proof. This directly follows from Theorem 1.

Remark. Thorem 1 implies that the KL-reward tradeoffs reported in the literature that use the analytical formula (Gao et al., 2023) are conservative and the actual KL-reward tradeoff of the best-of- $n$ policy is in fact even more favorable than what is reported.

In the rest of this section, we characterize the gap between the analytical formula and inspect regimes where the gap may be small or large.

### 3.1 Upper bounds on the gap

Here, we state our main upper bound.

Theorem 2. The gap between the analytical formula and the KL divergence is upper bounded by

$$
\begin{equation*}
\widetilde{K L}_{n}-K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) \leq 2 n(n-1) e^{-H_{2}\left(p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid x\right)} \tag{17}
\end{equation*}
$$

where $\mathrm{H}_{2}\left(p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right)$ is the conditional Rényi entropy of order 2 of the language model given context $\boldsymbol{x}$, and $H_{\alpha}\left(p_{y \mid x}\right)$ is defined as

$$
\begin{equation*}
H_{\alpha}\left(p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right):=\frac{1}{1-\alpha} \log \left(\sum_{y \in \mathcal{Y}^{*}}\left(p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})\right)^{\alpha}\right) \tag{18}
\end{equation*}
$$

Corollary 2. Let $p_{\boldsymbol{y} \mid x}(\boldsymbol{y} \mid \boldsymbol{x}) \leq \delta$ for all $y \in \mathcal{Y}^{*}$. Then, the gap is upper bounded by

$$
\begin{equation*}
\widetilde{K L}_{n}-K L\left(\pi_{y \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) \leq 2 n(n-1) \delta \tag{19}
\end{equation*}
$$

Proof. The proof follows by noticing that $H_{2}\left(p_{y \mid x} \mid \boldsymbol{x}\right) \geq \log (1 / \delta)$ and invoking Theorem 2 .

Hence, if the model outcomes are fairly low probability, making it quite unlikely for them to be redrawn if sampling is done all over again, the analytical formula $\widetilde{\mathrm{KL}}_{n}$ could be relatively accurate, and the gap is bounded above.

### 3.2 Lower bounds on the gap

Here, we characterize cases where the gap may be large. To this end, let us define

$$
\begin{equation*}
\varepsilon_{n}:=p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x}), \quad \text { where } \quad \boldsymbol{y} \sim \pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)}(\cdot \mid \boldsymbol{x}) \tag{20}
\end{equation*}
$$

Note that $\varepsilon_{n}$ is a random function of $\boldsymbol{x}$. In the limit as $n \rightarrow \infty$, we define $\varepsilon_{\infty}:=p_{\boldsymbol{y} \mid \boldsymbol{x}}\left(\boldsymbol{y}_{\max }(\boldsymbol{x}) \mid \boldsymbol{x}\right)$, where $\boldsymbol{y}_{\max }(\boldsymbol{x})=\arg \max _{\boldsymbol{y} \in \mathcal{Y}^{*}} r(\boldsymbol{x}, \boldsymbol{y})$. Note that $\varepsilon_{\infty}$ is a deterministic function of $\boldsymbol{x}$.

Theorem 3. For $n \in \mathbb{N}$, the gap is lower bounded by

$$
\begin{align*}
\widetilde{K L}_{n} & -K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid x} \mid \boldsymbol{x}\right) \\
& \geq\left(1-\left(1-\varepsilon_{\infty}\right)^{n}\right)\left(\log \frac{n \varepsilon_{\infty}}{1-\left(1-\varepsilon_{\infty}\right)^{n}}-\frac{n-1}{n}\right)-(n-1)\left(1-\varepsilon_{\infty}\right)^{n} \log \left(1-\varepsilon_{\infty}\right) \geq 0 \tag{21}
\end{align*}
$$

This theorem immediately also leads to the following result.

Corollary 3. For $n \in \mathbb{N}$, the gap is lower bounded by

$$
\begin{equation*}
\widetilde{K L}{ }_{n}-K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) \geq\left(1-e^{-n \varepsilon_{\infty}}\right) \log \left(n \varepsilon_{\infty}\right)-1 \tag{22}
\end{equation*}
$$

In particular, when $n \varepsilon_{\infty} \gg 1$, then the gap grows unbounded as we already observed in Example 1 .

## 4 Proposed estimator for KL divergence

Motivated by the derivation of the best-of- $n$ policy in Lemma 1, we propose a new estimator for the KL divergence. As a warm-up, first notice the following upper bound:

Lemma 2. For any $n \in \mathbb{N}$ and any $\boldsymbol{x}$,

$$
\begin{equation*}
K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) \leq E_{\boldsymbol{y} \sim \pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)}}\left[\log \left(\frac{1-\left(1-p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})\right)^{n}}{p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})}\right)\right] \tag{23}
\end{equation*}
$$

Proof. The proof follows from:

$$
\begin{align*}
\mathrm{KL}\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) & =E_{\boldsymbol{y} \sim \pi_{y \mid \boldsymbol{x}}^{(n)}}\left[\log \left(\frac{\mathcal{F}(\boldsymbol{y} \mid \boldsymbol{x})^{n}-\mathcal{F}^{-}(\boldsymbol{y} \mid \boldsymbol{x})^{n}}{p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})}\right)\right]  \tag{24}\\
& \leq E_{\boldsymbol{y} \sim \pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)}}\left[\log \left(\frac{1-\left(1-p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})\right)^{n}}{p_{\boldsymbol{y} \mid \boldsymbol{x}}(\boldsymbol{y} \mid \boldsymbol{x})}\right)\right] \tag{25}
\end{align*}
$$

Therefore we may suggest to use the following alternate estimator for KL divergence

$$
\begin{equation*}
\widehat{\mathrm{KL}}_{1}\left(\varepsilon_{n}\right):=\log \left(\frac{1-\left(1-\varepsilon_{n}\right)^{n}}{\varepsilon_{n}}\right) \tag{26}
\end{equation*}
$$

where $\varepsilon_{n}$ is defined in (20). Note that the expected value of $\widehat{\mathrm{KL}}_{1}\left(\varepsilon_{n}\right)$ is an upper bound on the $\mathrm{KL}$ divergence between the best-of- $n$ policy and the base policy. However, this estimator is loose by an additive constant of $(n-1) / n$, especially when $n \varepsilon_{n} \ll 1$.

Here we propose a different estimator to close this gap. To derive the estimator, first notice the following result.

Theorem 4. Recall the definition of $\varepsilon_{\infty}$ in (20). Then,

$$
\begin{align*}
K L\left(\pi_{\boldsymbol{y} \mid \boldsymbol{x}}^{(n)} \| p_{\boldsymbol{y} \mid \boldsymbol{x}} \mid \boldsymbol{x}\right) \leq & \left(1-\varepsilon_{\infty}\right)^{n}\left(\log n+(n-1) \log \left(1-\varepsilon_{\infty}\right)-\frac{n-1}{n}\right) \\
& +\left(1-\left(1-\varepsilon_{\infty}\right)^{n}\right) \log \left(\frac{1-\left(1-\varepsilon_{\infty}\right)^{n}}{\varepsilon_{\infty}}\right) \tag{27}
\end{align*}
$$

Note that Theorem 4 could not be directly used to derive an estimator for the KL divergence because we do not observe $\varepsilon_{\infty}$ when performing the best-of- $n$ policy. Inspired by this result, and given that we can only observe $\varepsilon_{n}$, we put forth the following practical estimator on the KL divergence.

Approximation 1. We propose the following estimator for the KL divergence of the best-of-n policy and the base policy:

$$
\begin{equation*}
\widehat{K L}\left(\varepsilon_{n}\right):=\left(1-\varepsilon_{n}\right)^{n}\left(\log n+(n-1) \log \left(1-\varepsilon_{n}\right)-\frac{n-1}{n}\right)+\left(1-\left(1-\varepsilon_{n}\right)^{n}\right) \log \left(\frac{1-\left(1-\varepsilon_{n}\right)^{n}}{\varepsilon_{n}}\right) \tag{28}
\end{equation*}
$$

In what follows we numerically inspect the proposed estimator in a few scenarios, and compare it with the analytical formula and the exact KL divergence between the best-of- $n$ policy and the base policy.
![](https://cdn.mathpix.com/cropped/2024_06_04_087005050d53a937953ag-07.jpg?height=1062&width=1588&top_left_y=314&top_left_x=236)

Figure 2: The analytical formula $(\log (n)-(n-1) / n)$, Eq. (5), the alternate bound, Eq. (26), the proposed estimator, Eq. (28), and the exact KL divergence, for uniform distributions supported on alphabets of size $L=10,10^{2}, 10^{3}, 10^{4}$ respectively.

The first set of examples, in Figure 2, are uniform distributions over alphabets of varying sizes. Notice that $\varepsilon_{n}=\varepsilon_{\infty}=\frac{1}{L}$ for a uniform distribution, and hence the estimator in Eq. (28) and Eq. (26) are deterministic. As can be seen $K L$ divergence saturates around $n \approx L$. For $\frac{n}{L} \ll 1$, the analytical formula of Eq. (5), $\log (n)-(n-1) / n$, has a small gap with the actual KL divergence (which was also theoretically established in Corollary 2). On the other hand, when $\frac{n}{L} \gg 1$, the gap between $\log (n)-(n-1) / n$ and the actual KL divergence becomes large and unbounded (which was also theoretically establish in Corollary 3). The alternate bound in Eq. (26) captures the behavior of the KL divergence for $\frac{n}{L} \gg 1$ and has a finite gap. However, it has a gap of $(n-1) / n$ for $\frac{n}{L} \ll 1$ as previously discussed. Finally, we also observe that the proposed estimator in Eq. (28) follows the behavior of the true KL divergence closely in all examples.

In the second set of examples, we cherry pick the probability mass function on the outcome to create different behaviors of the KL divergence, shown in Figure 3. In the left panel, the output is supported on an alphabet of size 5 , where the highest reward outcome has a probability of $10^{-4}$ and the rest of the probability mass is uniformly distributed over the rest of the outcomes. Here we see that KL divergence saturates early until the highest reward outcome is discovered with $n \approx 10^{4}$. In the right panel, the output is supported on an alphabet of size 200 , where the highest reward outcome has a probability of $10^{-5}$, the second highest reward outcome has a probability of $10^{-3}$, and the third highest reward outcome has a probability of $10^{-1}$. The rest of the probability mass is uniformly distributed over the rest of the outcomes. Here, the KL divergence starts to saturate until the next high reward is outcome is discovered around $n \approx 10^{3}$ and $n \approx 10^{5}$. As can be seen, the analytical formula in Eq. (5) does not capture the behavior of the KL divergence at all whereas
![](https://cdn.mathpix.com/cropped/2024_06_04_087005050d53a937953ag-08.jpg?height=512&width=1566&top_left_y=324&top_left_x=255)

Figure 3: The analytical formula $(\log (n)-(n-1) / n)$, Eq. (5), the alternate bound, Eq. (26), the proposed estimator, Eq. (28), and the exact KL divergence, for two cherry picked examples. In the left panel, the output is supported on an alphabet of size 5 , where the highest reward outcome has a probability of $10^{-4}$ and the rest of the probability mass is uniformly distributed over the rest of the outcomes. In the right panel, the output is supported on an alphabet of size 200 , where the highest reward outcome has a probability of $10^{-5}$, the second highest reward outcome has a probability of $10^{-3}$, and the third highest reward outcome has a probability of $10^{-1}$. The rest of the probability mass is uniformly distributed over the rest of the outcomes.

the alternate bound in Eq. (26) is much better aligned with the actual behavior. Finally, we observe that the proposed estimator in Eq. (28) closely follows the actual KL divergence. Note that here we have plotted the expected value of the proposed estimator and the alternate estimator, whereas in practice both estimators are subject to variance as well due to the randomness in the value of $\varepsilon_{n}$.

## 5 Conclusion

We studied the best-of- $n$ alignment policy in this paper and derived its probability mass function (Lemma 1). We proved that an analytical formula used in the literature for the KL divergence of the best-of- $n$ policy with the base policy, $\log (n)-(n-1) / n$ in Eq. (5), is false, and only an upper bound on the KL divergence (Theorem 1). We derived bounds on the gap between this formula and the KL divergence where we roughly showed the following: Let $\boldsymbol{y}$ be a draw from the best-of- $n$ policy. Let $\varepsilon_{n}$ be the probability mass of $\boldsymbol{y}$ under the base model. Then, if $n \varepsilon_{n} \ll 1$, the gap between the formula in Eq. (5) and the exact KL divergence is small (Theorem 2); and if $n \varepsilon_{n} \gg 1$, the gap between the two may be large and unbounded (Theorem 3). Finally, to remedy this issue, we proposed a new estimator for the KL divergence (Approximation 1), which we demonstrated to capture the behavior of the KL divergence on several numerical experiments. Further theoretical investigation of the properties of the proposed estimator is a promising direction for future work.

## References

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.

Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 10835-10866. PMLR, 2023.

Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, and Marc Dymetman. Compositional preference models for aligning LMs. arXiv preprint arXiv:2310.13011, 2023.

Jacob Hilton and Leo Gao. Measuring Goodhart's law, April 2022 (Accessed on January 3, 2024). URL https://openai.com/research/measuring-goodharts-law.

Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, HengTze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023 .

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3511-3535, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.276. URL https://aclanthology.org/2021.naacl-main.276.

Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2022.
