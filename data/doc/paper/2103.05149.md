# CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR 

Alex Xiao, Christian Fuegen, Abdelrahman Mohamed

Facebook AI


#### Abstract

Pseudo-labeling is the most adopted method for pre-training automatic speech recognition (ASR) models. However, its performance suffers from the supervised teacher model's degrading quality in low-resource setups and under domain transfer. Inspired by the successes of contrastive representation learning for computer vision and speech applications, and more recently for supervised learning of visual objects, we propose Contrastive Semi-supervised Learning (CSL). CSL eschews directly predicting teacher-generated pseudolabels in favor of utilizing them to select positive and negative examples. In the challenging task of transcribing public social media videos, using CSL reduces the WER by $8 \%$ compared to the standard Cross-Entropy pseudo-labeling (CE-PL) when $10 \mathrm{hr}$ of supervised data is used to annotate $75,000 \mathrm{hr}$ of videos. The WER reduction jumps to $19 \%$ under the ultra low-resource condition of using $1 \mathrm{hr}$ labels for teacher supervision. CSL generalizes much better in out-of-domain conditions, showing up to $17 \%$ WER reduction compared to the best CE-PL pre-trained model.


Index Terms- Pseudo-labeling, contrastive learning

## 1. INTRODUCTION

Driven by the practical need to deploy speech and audio models to new languages and domains, there has been a surge in research work utilizing self-, semi-, and weakly-supervised approaches for speech and audio representation learning achieving impressive results on a wide variety of tasks.

The contrastive loss, which has been utilized extensively in selfsupervised computer vision and speech applications [1, 2, 3, 4], centers around distinguishing one or more positive examples from negative ones given an anchor sample. It has also been utilized for supervised learning of visual objects [5].

Pseudo-labeling is the most adopted approach of semi-supervised learning in speech recognition since the late 1990s and more recently for modern end-to-end neural approaches [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16. A 'teacher' model uses a small amount of supervised data for training, then transcribes a larger volume of unlabeled audio data. A 'student' model then utilizes the teacher-generated pseudo-labels for its training. Depending on the downstream task, the student model can be smaller [17] or larger [15] than the teacher model. A critical ingredient of pseudo-labeling is filtering the teacher-generated data by removing implausible, low-quality hypotheses through confidence filtering [18], which reduces the possibility of the student model copying the teacher's mistakes. The whole pseudo-labeling process stalls when the initial labeled data is not large enough to train a reliable teacher model.

In this paper, we introduce a Contrastive Semi-supervised Learning (CSL) approach that benefits from a contrastive loss for improving the stability of learned speech representations. Precisely, we use a contrastive loss to replace the cross-entropy loss in the standard pseudo-labeling pre-training (CE-PL). Contextual representations of audio segments are optimized to get closer to others with similar pseudo-labels (positive examples) and further away from ones that are different (negative examples). Such soft constraint during learning allows the CSL to be more resilient to teacher labeling noise. To demonstrate its resilience to pseudo-labeling noise, we apply CSL pre-training in a low-resource setup with only $10 \mathrm{hr}$ of labeled data, where it reduces WER by $8 \%$ compared to the standard cross-entropy pseudo-labeling (CE-PL). This WER reduction increase to $19 \%$ with a teacher trained only on $1 \mathrm{hr}$ of labels and $17 \%$ for out-of-domain conditions.

## 2. RELATED WORK

Our paper builds upon previous research work in ASR pre-training using semi-supervised and self-supervised techniques. Pseudolabeling, a semi-supervised method, where a supervised 'teacher' model is used to label a large volume of unlabeled data, is a widely adopted pre-training method for ASR. Similar approaches with slight variations exist in the literature as teacher-student training, knowledge distillation, and self-training. Since the 1990s, pseudolabeling showed improvements to classical HMM/GMM systems as well as hybrid-NN systems [6, 7, 8, 10, 9, 11], and also for end-to-end ASR approaches [12, 13, 14, 15, 16]. Our work inherits the same setup of utilizing a (small) set of supervised data for the teacher model learning similar to pseudo-labeling. However, we introduce a contrastive pre-training loss to deal with low-quality teacher pseudo-labels.

Recently, self-supervised ASR pre-training achieved impressive performance, especially with minimal amounts of labeled data [2, 3, 19, 20, 21, 22, 4, 23]. Given unlabeled audio data, self-supervised methods rely primarily on pretext pre-training tasks that acquire their labels either from the input signal itself, e.g., predicting future frames [20] or via unsupervised means, e.g., feature clustering [4, 23]. Many self-supervised methods boil down to contrasting positive and negative samples for a particular anchor sample. Depending on the downstream application in mind, positive samples could be near-by frames to the current time step or random samples for the same speaker. Negative samples could be further-away frames or samples for a different speaker [2] 4]. After pre-training, a supervised fine-tuning stage optimizes the model performance for one or more downstream tasks, e.g., ASR.

Our approach relies on pseudo-labels from a supervised teacher model for positive and negative example selection. This work draws inspiration from the recent application of contrastive learning for supervised learning of visual object classification [5].

## 3. METHOD

### 3.1. Pseudo-Labeling

The pseudo-labeling approach uses a supervised teacher ASR model to decode a large volume of unlabeled audio data $X$ and generate pseudo-labels $Y$. The model training proceeds by using these pairs in a supervised learning setup. Following a hybrid-NN training approach, we align the labels to the input feature vectors to get a sequence of frame-level pairs: $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right),\left(x_{3}, y_{3}\right), \ldots,\left(x_{T}, y_{T}\right)$, where $y_{i}$ are represented as chenone units [24].

Let the model be composed of two functions $F_{\text {enc }}$ and $G_{p r e d}$. $F_{\text {enc }}\left(X_{1: T}\right)$ encodes the full input sequence and returns a sequence of contextualized representations $Z_{1: T}$. The predictor network $G_{\text {pred }}(Z)$ produces a sequence of frame-level probability distributions over the chenone output units $p\left(\hat{Y}_{1: T} \mid X\right)$. In this paper, $G_{\text {pred }}$ is chosen to be a linear layer followed by a softmax function.

Our training objective is the cross-entropy loss of predicting frame-level chenones:

$$
C E=-\frac{1}{\sum_{i=1}^{B} T_{i}} \sum_{i=1}^{B} \sum_{t=1}^{T_{i}} \log p\left(\hat{y}_{t i}=y_{t i} \mid X_{i}\right)
$$

Where $B$ is the batch size, $T_{i}$ the length of utterance $i$, and $y_{t i}$ is the chenone target for utterance $i$ at frame $t$.

### 3.2. Contrastive Semi-supervised Learning

Contrastive losses, e.g., InfoNCE [2], are at the heart of many selfsupervised representation learning approaches, where two groups of samples, positive and negative, are selected for a specific anchor data point within a pretext task-the training phase proceeds by iteratively improving the model's ability to distinguish samples in each group. Negative samples are selected randomly from a mini-batch or a memory bank of previous experiences, while positive samples could be augmented versions of the anchor, nearby frames, or samples from the same speaker. Data points' designation as positive and negative samples determines the learned representation nature and performance on downstream tasks.

Our proposed Contrastive Semi-supervised Learning (CSL) approach synthesizes pseudo-labeling and self-supervised methods while solving some of their weaknesses. It bypasses the challenge of positive and negative sample selection of self-supervised methods by utilizing a supervised teacher. Using a large volume of unlabeled data and given the teacher's pseudo-labels, we select audio segments within one mini-batch as positive pairs if they share the same label and as negative pairs otherwise. Compared to standard pseudolabeling methods, CSL is resilient to errors in teacher-generated targets since it utilizes relative distance between label classes as a learning signal.

More precisely, during the CSL pre-training, our model consists of two functions: The encoder $F_{\text {enc }}\left(X_{1: T}\right)=Z_{1: T}$ which encodes input audio into latent representations, and a projection network $M_{p r o j}\left(Z_{1: T}\right)=H_{1: T}$ that maps encoder representations into a new space suitable for applying the contrastive loss. We normalize the projected features $H_{1: T}$ to unit length. A hybrid-NN supervised teacher generates aligned pseudo-labels $Y_{1: T}$ to guide the selection of positive and negative samples for the contrastive loss.

Given the speech signal's local smoothness, each positive or negative sample is an audio segment with a specific label. Concretely, let's define a span $[i: j]$ as a segment if all its pseudo-labels $\left[y_{i}: y_{j}\right]$ are equal, and it is not contained in a larger span with the same pseudo-label. We then randomly sample a single time step $k$ to represent this segment with label $y_{k}$ in the loss function. Assuming $S$ segments in all utterances in the mini-batch, a total of $S$ representative latent vectors are selected $\left\{z_{1}, . ., z_{i}, . ., z_{S}\right\}$, then projected using $M_{p r o j}$ into unit length vectors $\left\{h_{1}, \ldots, h_{i}, \ldots, h_{S}\right\}$. Given the corresponding pseudo-labels $\left\{y_{1}, . ., y_{i}, . ., y_{S}\right\}$ for sampled vectors, our training objective is:

$$
Ł=\frac{1}{S} \sum_{i=1}^{S} \frac{1}{|P(i)|} \sum_{h_{p} \in P(i)}-\log \frac{\exp \left(h_{i} \cdot h_{p} / \tau\right)}{\sum_{h \in N(i) \cup\left\{h_{p}\right\}} \exp \left(h_{i} \cdot h / \tau\right)}
$$

$P(i)$ is the set of samples in the mini-batch with the same pseudolabel as $i$. The rest of the mini-batch samples belong to $N(i)$ as negative examples. $\tau$ is a tunable temperature hyperparameter.

Following the InfoNCE loss, the CSL objective function adjusts the encoder $F_{\text {enc }}$ to bring its features of similar pseudo-labels closer and other negative samples further away. After the CSL pre-training, the prediction network $G_{\text {pred }}$ replaces the projection network $M_{p r o j}$ for supervised fine-tuning of the whole network, including the encoder network $F_{\text {enc }}$.

In this paper, we apply frame-level cross-entropy fine-tuning, but other loss functions, e.g., the Connectionist Temporal Classification (CTC) [25] loss, can also be used. Our formulation is an adaptation of the previous work in computer vision [5] which applies a contrastive loss in a supervised setup. The CSL pre-training offer many benefits:

- Utilizing teacher pseudo-labels for selecting positive and negative samples, CSL is more stable than self-supervised pre-training methods, which are sensitive to the diversity and the criterion for choosing positive and negative samples. Moreover, CSL enables reliable sampling of positive examples within and across utterances in the mini-batch.
- Unlike the standard pseudo-labeling pre-training methods, which forces the model to predict the teacher targets, CSL applies a softer constraint on learned representations through the contrastive loss. Such formulation improves robustness to noisy teacher pseudo-labels.
- Applying the contrastive loss over normalized representations emphasizes hard positives and negative examples, as shown analytically in [5], which enables the pre-trained model to generalize better under out-of-domain conditions.


### 3.3. Label-Aware Batching (LAB)

Large mini-batch sizes are critical for contrastive losses to ensure a diverse set of negative samples. For CSL, it also brings several positive realizations of speech sounds. Given the CSL loss formulation, we require at least two positive instances of each label class in the mini-batch. Some rare sounds might not form positive pairs for small randomly sampled mini-batches, leading to poor representations. To account for this problem, we propose Label-Aware Batching (LAB). LAB incrementally builds each mini-batch considering the number of segments representing label $k$ in the current partially-created mini-batch, $C(k)$. First, it samples a rare label $r$ with probability $\left(\frac{1}{C(r)}\right)^{\alpha} / \sum_{i=1}^{n}\left(\frac{1}{C(i)}\right)^{\alpha}$, and then samples without replacement two random utterances containing the class $r$ to add to the current mini-batch. LAB updates the counts $C(k)$ for all labels appearing in these two utterances and repeats until it reaches the maximum number of utterances in a mini-batch. We use $\alpha=2$ in our experiments. For stable learning, we aggregate gradients over multiple mini-batches to perform a single model update and scale the learning rate accordingly [26].

## 4. EXPERIMENTS

### 4.1. Data

We use two in-house de-identified data sources with no personally identifiable information (PII). The first source contains de-identified public five-minute videos on Facebook in British English and Italian. During training, we segment all videos to a maximum of 10 seconds. To simulate a low-resource condition, we use $10 \mathrm{hr}$ labeled data for both teacher training and the final fine-tuning stage. We also test a more extreme setup with merely $1 \mathrm{hr}$ worth of labels. We use $75,000 \mathrm{hr}$ of unlabeled videos for pre-training and evaluate performance on $14 \mathrm{hr}$ of human-labeled videos for each language. As an upper bound, we compare performance against a fully supervised setting with $650 \mathrm{hr}$ and $3,700 \mathrm{hr}$ labeled data for British English and Italian, respectively. We sample 23hr general English videos to test accent generalization. The second source contains recordings of crowd-sourced workers responding to artificial prompts with mobile devices. We collect two evaluation datasets to test out-of-domain generalization: $15 \mathrm{hr}$ of short message dictation and $13 \mathrm{hr}$ of longform conversations of up to 144 second long. The language model (LM) for British English uses the 650hr transcripts plus an additional 13,000hr of general English video transcripts, while the Italian LM uses only the $3,700 \mathrm{hr}$ transcripts.

### 4.2. Model Details

$F_{\text {enc }}$ has a VGG front-end and 12 transformer blocks (dim $=768$, FFN $=3072,8$ heads, stride 4) following [27] ( 90M parameters). For CSL pre-training, $M_{\text {proj }}$ is a single hidden layer network of size 1024 and 128 output dimension. Input and output representations of $M_{\text {proj }}$ are length normalized. Our input speech features are 80 dimensional, speed perturbed [28] Mel-scale log filterbank coefficients computed every $10 \mathrm{~ms}$ over $25 \mathrm{~ms}$ windows. We report the results of a single pass decoding using a 5 -gram LM.

### 4.3. Training Details

For the pseudo-labeling baseline and CSL pre-training, the teacher model is a hybrid-NN ASR system trained on the initial 10hr of labeled data. Given the small number of supervised labels, we use 368 and 424 chenone outputs [24] for the British English and Italian systems, respectively. A supervised frame-level cross-entropy (CE) fine-tuning stage follows pre-training for all baseline and CSL models presented using the same amount of labels used for the respective teacher (either $10 \mathrm{hr}$ or $1 \mathrm{hr}$ depending on the experiment).

We use the Adam optimizer [29] with mixed-precision training [30] and gradient norm clipping at 10 . We tune the learning rate (lr) for best performance on each task: $\mathrm{lr}=1 \mathrm{e}-4$ for all supervised and pseudo-labeling baselines, $\mathrm{lr}=5 \mathrm{e}-5$ for fine-tuning CSL pre-trained models. We apply $320 \mathrm{k}$ model update steps with a tri-stage $\mathrm{lr}$ schedule for pre-training: 13k warm-up steps, and the remaining steps are split evenly between constant $\mathrm{lr}$ and linear decay. We use 16 GPUs for pre-training, with about 320sec of audio per GPU. We apply 60 epochs of updates on a single GPU with a 0.8 multiplicative decay every epoch after the 40th epoch for fine-tuning. Using the CSL loss introduced a $15 \%$ increase in training time compared to baseline pseudo-labeling due to the pairwise similarity score computations, which can be further optimized by reducing the number of positive and negative examples used for frequent units.

We apply two types of input feature masking: Double masking policy (LD) [31 and a Short Time Masking (STM) policy which follows LD in frequency masking and increases the number of time masks to 15 with lengths uniformly sampled between 16 and 32 timesteps.

|  |  | British English | Italian |
| :---: | :---: | :---: | :---: |
| Supervised Baseline |  |  |  |
| $\overline{\mathrm{A} 1}$ | Full supervised data | 23.1 (650hr) | $11.9(3,700 \mathrm{hr})$ |
| A2 | 10hr of labels | 50.7 | 31.8 |
| CE Pseudo-labeling (CE-PL) with 10hr of labels |  |  |  |
| B1 | Initial policy | 37.5 | 19.6 |
| B2 | B1 + STM masking | 34.2 | 18.1 |
| B3 | B1 + C9 settings | 32.0 | 17.2 |
| Contrastive Semi-supervised Learning (CSL) with 10hr of label |  |  |  |
| C1 | Initial policy | 36.4 | 19.1 |
| $\mathrm{C} 2$ | $\mathrm{C} 1+420$ seconds batch | 35.5 | 18.9 |
| C3 | $\mathrm{C} 1+600$ seconds batch | 34.8 | 18.6 |
| $\mathrm{C} 4$ | $\mathrm{C} 2+8$ random negatives | 35.9 | 19.1 |
| C5 | $\mathrm{C} 2+8$ same uttt. negatives | 35.3 | 18.7 |
| C6 | $\mathrm{C} 5+\mathrm{LAB}$ | 34.8 | 18.3 |
| C7 | $\mathrm{C} 5+4 \mathrm{x}$ grad accumulation | 34.1 | 18.1 |
| C8 | C5 + STM masking | 31.9 | 17.6 |
| C9 | C3+C5:C8 | 29.4 | 16.0 |

Table 1. WER of supervised, pseudo-labeling, and CSL systems.

## 5. RESULTS AND DISCUSSION

### 5.1. The main results

Table 1 presents the main results of our experiments. The best $\mathrm{CE}$ pseudo-labeling (CE-PL) pre-training strategy improves the respective supervised baseline by about $36 \%$ and $46 \%$ for both languages using 10hr of labels. CSL provides a relative improvement of $8 \%$ and $7 \%$ over the best CE-PL system bringing the overall WER closer to the fully supervised case, which utilizes orders of magnitude more labeled data. We observe performance degradation without normalizing the projected features. The $M_{\text {proj }}$ output dimension can be reduced to 128 without significant performance loss. Unlike the supervised case in [5], high values for the contrastive loss temperature $(\tau=1)$ consistently provided the best performance since it reduces the harm of noisy teacher pseudo-labels. The proposed Short Time Masking (STM) policy improves both CE and CSL by a 6-9\% relative (B2, C8) compared to the baseline policy. Fine-tuning the whole network after pre-training offered better performance than freezing the $F_{\text {enc }}$ subnetwork.

|  | British English (WER) |
| :---: | :---: |
| 100\% CSL | 35.3 |
| $50 \%$ CSL + 50\% CE-PL | 36.6 |
| 25\% CSL + 75\% CE-PL | 36.8 |
| 10\% CSL + 90\% CE-PL | 37 |

Table 2. The effect of mixing the CSL and CE-PL losses.

|  | CE-PL | CSL | WERR |
| :---: | :---: | :---: | :---: |
| British English Videos | 32.0 | 29.4 | 8.1 |
| General English Videos | 37.2 | 32.8 | 11.8 |
| Messoge Dictation | 21.6 | 17.8 | 17.3 |
| Long-form Conversation | 26.0 | 22.0 | 15.4 |

Table 3. Relative WERR (WER reduction) of using CSL and CE-PL for out of domain conditions

### 5.1.1. Effect of large and label-aware batching

Increasing the per-GPU batch size from 320 seconds to 600 seconds improves WER by 3-4\% relative (C3), supporting our hypothesis that incorporating more positive and negative examples improves learned representations during the CSL pre-training. Gradient accumulation, which comes at no extra GPU memory cost, improves learning stability as each update observes more speech units. It provides a 3-6\% relative WER improvement (C7) and points to a potential future direction of adding a memory bank to hold extra positive and negative samples. Label-Aware Batching (LAB) provides a small but consistent boost in performance (C9) and provides the best performance both for the CSL and CE-PL systems when combined with all other improvements (C9 and B3).

### 5.1.2. Effect of positives and negatives sampling policy

In line with findings in self-supervised speech representation learning [4. 2], using negative samples from the same utterance offers better WER compared to a random sampling of negative examples across utterances. The model learns to marginalize the speaker identity information while focusing on the speech content, which is preferable for an ASR downstream task but not necessarily for other speech applications. Using eight samples for both negative and positive comparisons was enough to reach the best performance. Limiting positives samples to be from the same or different utterances did not make a significant difference.

### 5.1.3. Effect of best policy on CE-PL

The last row of Table 1 presents the best CSL results by combining all the improvements above. Applying the same improvements to cross-entropy pseudo-labeling (CE-PL) offers about a $15 \%$ and a $12 \%$ relative WER reduction (B3) compared to the original policy (B1).

### 5.2. Combining CE-PL and CSL

Table 2shows the experiments to test potential complementarity between CE pseudo-labeling (CE-PL) loss and the contrastive loss. Using the setup of (C3) from Table 1 the two loss functions are applied to mini-batches using shown percentages in Table 2. Applying the CE-PL to some mini-batches does not improve the learned representation for the downstream ASR task than the CSL case.

### 5.3. Out of Domain Generalization of CSL

Table 3 examines the generalization performance to out-of-domain test sets for British English. We compare two models pre-trained using CE-PL and CSL then fine-tuned in a low-resource condition using only 10hr labels. Although CSL offers about an $8 \%$ relative improvement under matching training and testing conditions, it

|  | CE-PL | CSL | WERR |
| :--- | :---: | :---: | :---: |
| Generation 1 | 37.5 | 34.8 | 7.2 |
| Generation 2 | 33.1 | 30.9 | 6.6 |
| Generation 3 | 31.5 | 29.4 | 6.7 |

Table 4. Effect of three iterations of data re-labeling on CSL pretraining.

|  | British English (WER) |
| :---: | :---: |
| Supervised 1hr | 80.5 |
| CE-PL | 53.1 |
| CSL | 42.8 |
| CSL Generation 2 | 32.3 |
| Supervised 650 Hours | 23.1 |

Table 5. Ultra low-resource condition using $1 \mathrm{hr}$ of labels.

brought more significant improvements 11-17\% when tested in outof-domain conditions. The CSL pre-training discovers more stable representations compared to CE-PL, which confirms our initial intuition.

### 5.4. Effect of iterative labeling

Using multiple rounds of pseudo-labeling for model pre-training was found very effective for the classical GMM/HMM systems in the mid 2000s [9] and more recently for neural systems [15, 32, 12]. Table 4 examines if the gain observed using CSL diminishes under an iterative labeling scheme due to the improved baseline CEPL performance. Each generation starts by generating new teacher pseudo-labels for the unlabeled videos using the previous generation's model. A CE fine-tuning step is performed for all experiments reported in Table 4 Both CE-PL and CSL improve using iterative re-labeling. The gains from CSL pre-training are complementary to iterative re-labeling.

### 5.5. Ultra low-resource condition with $1 \mathrm{hr}$ of labels

To confirm CSL pre-training resilience to teacher pseudo-labeling errors, we test CSL and CE-PL under an ultra low-resource condition using only $1 \mathrm{hr}$ of labeled data for teacher training. Table 5 shows the WER on British English with about 19\% relative improvement achieved by CSL compared to CE-PL, which suffers from teacher label noise. Using $1 \mathrm{hr}$ of labels and two iterations of CSL, we can close about $80 \%$ of the performance gap with $650 \mathrm{hr}$ of supervised data in the challenging domain of social media video transcription.

## 6. CONCLUSION

Inspired by the impressive performance of self-supervised representation learning in computer vision and speech, we introduced Contrastive Semi-supervised Learning (CSL), which applies a contrastive loss for semi-supervised ASR pre-training. For the challenging task of social media video transcription, using $75,000 \mathrm{hr}$ of unlabeled videos and only $10 \mathrm{hr}$ of labeled data, CSL offers an $8 \%$ relative WER reduction compared to the strongest Cross-Entropy pseudolabeling (CE-PL) baseline. CSL pre-training proved more resilient under out-of-domain conditions and even using $1 \mathrm{hr}$ of labeled data.

## 7. REFERENCES

[1] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick, "Momentum contrast for unsupervised visual representation learning," in CVPR, 2020.

[2] Aäron van den Oord, Yazhe Li, and Oriol Vinyals, "Representation learning with contrastive predictive coding," in CoRR, 2018, vol. abs/1807.03748.

[3] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli, "wav2vec: Unsupervised pre-training for speech recognition," in INTERSPEECH, 2019.

[4] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," in NeurIPS, 2020.

[5] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan, "Supervised contrastive learning," in NeurIPS, 2020.

[6] Thomas Kemp and Alex Waibel, "Unsupervised training of a speech recognizer: Recent experiments," Eurospeech, 1999.

[7] George Zavaliagkos and Thomas Colthurst, "Utilizing untranscribed training data to improve performance," in DARPA Broadcast News Transcription and Understanding Workshop, 1998.

[8] Jeff Ma, S. Matsoukas, O. Kimball, and Richard Schwartz, "Unsupervised training on large amounts of broadcast news data," in ICASSP, 2006.

[9] F. Wessel and H. Ney, "Unsupervised training of acoustic models for large vocabulary continuous speech recognition," IEEE Transactions on Speech and Audio Processing, vol. 13, 2005.

[10] Bhuvana Ramabhadran, "Exploiting large quantities of spontaneous speech for unsupervised training of acoustic models.," in Interspeech, 2005, pp. 1617-1620.

[11] L. Wang, M. J. F. Gales, and P. C. Woodland, "Unsupervised training for mandarin broadcast news and conversation transcription," in ICASSP, 2007.

[12] Kritika Singh, Vimal Manohar, Alex Xiao, Sergey Edunov, Ross Girshick, Vitaliy Liptchinsky, Christian Fuegen, Yatharth Saraf, Geoffrey Zweig, and Abdelrahman Mohamed, "Large scale weakly and semi-supervised learning for low-resource video asr," in INTERSPEECH, 2020.

[13] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, et al., "End-to-end asr: from supervised to semi-supervised learning with modern architectures," 2019.

[14] Jacob Kahn, Ann Lee, and Awni Hannun, "Selftraining for end-to-end speech recognition," arXiv preprint arXiv:1909.09116, 2019.

[15] Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V. Le, "Improved noisy student training for automatic speech recognition," in INTERSPEECH, 2020.

[16] Bo Li, Ruoming Pang, Tara Sainath, and Zelin Wu, "Semisupervised training for end-to-end models via weak distillation," in ICASSP, 2019.

[17] Jimmy Ba and Rich Caruana, "Do deep nets really need to be deep?," in Advances in Neural Information Processing Systems
27, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds., pp. 2654-2662. Curran Associates, Inc., 2014.

[18] Hank Liao, Erik McDermott, and Andrew Senior, "Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription," in ASRU, 2013.

[19] Alexei Baevski, Michael Auli, and Abdelrahman Mohamed, "Effectiveness of self-supervised pre-training for speech recognition," arXiv:1911.03912, 2019.

[20] Yu-An Chung and James Glass, "Generative pre-training for speech with autoregressive predictive coding," in ICASSP, 2020.

[21] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi Lee, "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders," in ICASSP, 2020.

[22] Shaoshi Ling, Yuzong Liu, Julian Salazar, and Katrin Kirchhoff, "Deep contextualized acoustic representations for semisupervised speech recognition," in ICASSP, 2020.

[23] Wei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Ruslan Salakhutdinov, and Abdelrahman Mohamed, "HuBERT: How much can a bad teacher benefit ASR pre-training?" ICASSP, 2021.

[24] Duc Le, Xiaohui Zhang, Weiyi Zheng, Christian Fügen, Geoffrey Zweig, and Michael L Seltzer, "From senones to chenones: Tied context-dependent graphemes for hybrid speech recognition," in ASRU, 2019.

[25] Alex Graves, Santiago Fernández, and Faustino Gomez, "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks," in ICML, 2006.

[26] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He, "Accurate, large minibatch sgd: Training imagenet in 1 hour," arXiv preprint arXiv:1706.02677, 2017.

[27] Yongqiang Wang, Abdelrahman Mohamed, Duc Le, Chunxi Liu, Alex Xiao, et al., "Transformer-based acoustic modeling for hybrid speech recognition," in ICASSP, 2020.

[28] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, "Audio augmentation for speech recognition.," in $I N$ TERSPEECH, 2015.

[29] Diederik Kingma and Jimmy Ba, "Adam: A method for stochastic optimization," ICLR, 2014.

[30] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu, "Mixed precision training," in ICLR, 2018.

[31] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le, "Specaugment: A simple data augmentation method for automatic speech recognition," arXiv preprint arXiv:1904.08779, 2019.

[32] Qiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel Synnaeve, and Ronan Collobert, "Iterative pseudo-labeling for speech recognition," in INTERSPEECH, 2018.

