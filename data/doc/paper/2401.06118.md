# Extreme Compression of Large Language Models via Additive Quantization 

Vage Egiazarian ${ }^{* 12}$ Andrei Panferov ${ }^{* 12}$ Denis Kuznedelev ${ }^{23}$ Elias Frantar $^{4}$ Artem Babenko ${ }^{2}$ Dan Alistarh ${ }^{45}$


#### Abstract

The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in MultiCodebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across entire layer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.


## 1. Introduction

The rapid advancement of generative large language models (LLMs) has led to massive industrial and popular interest, driven in part by the availability of accurate open LLMs, such as Llama 1 and 2 (Touvron et al., 2023), Falcon (TII UAE, 2023), BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), or NeoX/Pythia (Biderman et al., 2023). A key advantage of open models is that they can be inferenced or fine-tuned locally by end-users, assuming that their computational and memory costs can be reduced to be manageable on commodity hardware. This has led to several methods for[^0]

Preliminary work. To be extended with additional experiments.

![](https://cdn.mathpix.com/cropped/2024_06_04_46c218fad80e5caf3b83g-01.jpg?height=458&width=656&top_left_y=606&top_left_x=1144)

Figure 1: Comparison of AQLM (2-bit) relative to the stateof-the-art QuIP\# (2-bit) and the original 16-bit weights on LLAMA 2 7, 13, and 70B models.

inference and fine-tuning on compressed LLMs (Dettmers et al., 2022; Frantar et al., 2022a; Dettmers \& Zettlemoyer, 2022; Lin et al., 2023; Dettmers et al., 2023a). Currently, the primary approach for accurate post-training compression of LLMs is quantization, which reduces the bit-width at which model weights (and possibly activations) are stored, leading to improvements in model footprint and memory transfer.

By and large, LLM weights are compressed via "direct" quantization, in the sense that a suitable quantization grid and normalization are first chosen for each matrix subcomponent, and then weights are each mapped onto the grid either by direct rounding, e.g. (Dettmers \& Zettlemoyer, 2022), or via more complex allocations, e.g. (Frantar et al., 2022a). Quantization induces a natural compression-vsaccuracy trade-off, usually measured in terms of model size vs model perplexity (PPL). Existing approaches can achieve arguably low accuracy loss at 3-4 bits per element (Dettmers et al., 2023b; Chee et al., 2023; Kim et al., 2023), and can even stably compress models to 2 or even less bits per element, in particular, for extremely large models (Frantar \& Alistarh, 2023). Yet, in most cases, low bit counts come at the cost of significant drops in accuracy, higher implementation complexity and runtime overheads. Specifically, from the practical perspective, "extreme" quantization in the 2-bit range using current techniques is inferior to simply using a smaller base model and quantizing it to higher bitwidths, such as 3-4 bits per parameter, as the latter yields higher accuracy given the same model size in bytes (Dettmers \& Zettlemoyer, 2022; Chee et al., 2023).

Contribution. In this work, we improve the state-of-the-art in LLM compression by showing for the first time that MultiCodebook Quantization (MCQ) techniques can be extended to LLM weight compression. Broadly, MCQ is a family of information retrieval methods (Chen et al., 2010; Jegou et al., 2010; Ge et al., 2013; Zhang et al., 2014; Babenko \& Lempitsky, 2014; Martinez et al., 2016; 2018), consisting of specialized quantization algorithms to compress databases of vectors, allowing for efficient search. Unlike direct quantization, MCQ compresses multiple values jointly, by leveraging the mutual information of quantized values.

More precisely, we extend Additive Quantization (AQ) (Babenko \& Lempitsky, 2014; Martinez et al., 2016), a popular MCQ algorithm, to the task of compressing LLM weights such that the output of each layer and Transformer block are approximately preserved. Our extension reformulates the classic AQ optimization problem to reduce the error in LLM layer outputs under the input token distribution and as well as to jointly optimize codes over layer blocks, rather than only preserving the weights themselves as in standard AQ. We refer to the resulting procedure as Additive Quantization of Language Models (AQLM). Unlike some extreme LLM quantization approaches that require hybrid sparse-quantized formats which separate outlier quantization (Kim et al., 2023; Dettmers et al., 2023b), AQLM quantizes models in a simple homogeneous format, which is easy to support in practice. Our main contributions are as follows:

1. We propose the AQLM algorithm, which extends AQ to post-training compression of LLM weights, via two innovations: (1) adapting the MAP-MRF optimization problem behind AQ to be instance-aware, taking layer calibration input \& output activations into account; (2) complementing the layer-wise optimization with an efficient intra-layer tuning technique, which optimizes quantization parameters jointly over several layers, using only the calibration data.
2. We evaluate the effectiveness of this algorithm on the task of compressing accurate open LLMs from the LlamA 2 (Touvron et al., 2023) family with compression rates of 2-4 bits per parameter. We find that AQLM outperforms the previous state-of-the-art across the standard 2-4 bit compression range, with the most significant improvements for extreme 2-bit quantization (see Figure 1). We provide detailed ablations for the impact of various algorithm parameters, such as code width and number of codebooks, and extend our analysis to the recent Mixtral model (Jiang et al., 2024).
3. We show that $\mathrm{AQLM}$ is practical, by providing efficient GPU and CPU kernels implementations for specific encodings, as well as end-to-end generation ${ }^{1}$. Results[^1]

show show that our approach can match or even outperform the floating point baseline in terms of speed, while reducing the memory footprint by up to $8 x$. Specifically, AQLM can be executed with layer-wise speedups of $\sim 30 \%$ for GPUs, and of up to $4 x$ for CPU inference.

## 2. Background \& Related Work

### 2.1. LLM Quantization

Early efforts towards post-training quantization (PTQ) methods (Nagel et al., 2020; Gholami et al., 2021) that scale to LLMs such as ZeroQuant (Yao et al., 2022), LLM.int8() (Dettmers et al., 2022), and nuQmm (Park et al., 2022) employed direct round-to-nearest (RTN) projections, and adjusted quantization granularity to balance memory efficiency and accuracy. GPTQ (Frantar et al., 2022a) proposed a more accurate data-aware approach via an approximate large-scale solver for minimizing layer-wise $\ell_{2}$ errors.

Dettmers \& Zettlemoyer (2022) examined the accuracycompression trade-offs of these early methods, suggesting that 4-bit quantization may be optimal for RTN quantization, and observing that data-aware methods like GPTQ allow for higher compression, i.e. strictly below 4 bits/weight, maintaining Pareto optimality. Our work brings this Pareto frontier below 3 bits/weight, for the first time. Parallel work quantizing both weights and activations to 8 -bits, by Dettmers et al. (2022), Xiao et al. (2022), and Yao et al. (2022) noted that the "outlier features" in large LLMs cause substantial errors, prompting various mitigation strategies.

Recently, several improved techniques have focused on the difficulty of quantizing weight outliers, which have high impact on the output error. SpQR (Dettmers et al., 2023b) addresses this by saving outliers as a highly-sparse higherprecision matrix. AWQ (Lin et al., 2023) reduces the error of quantizing channels with the highest activation magnitudes by employing per-channel scaling to reduce the error on important weights. SqueezeLLM (Kim et al., 2023) uses the diagonal Fisher as a proxy for the Hessian and implements non-uniform quantization through K-means clustering.

The state-of-the-art method in terms of accuracy-to-size trade-off is QuIP (Chee et al., 2023). Concurrent to our work, an improved variant called QuIP\# (Tseng et al., 2023) was introduced. Roughly, these methods work by first "smoothening" weights by multiplying with a rotation matrix, and then mapping them onto a lattice. QuIP was the first method to obtain stable results (i.e., single-digit PPL increases) in the 2-bit per parameter compression range. At a high level, QuIP and QuIP\# aim to minimize the "worstcase" error for each layer, given initial weights and calibration data. For instance, in QuIP\#, the distribution of the rotated weights approximates a Gaussian, while the encoding lattice (E8P) is chosen to minimize "rounding" error.

By contrast, our approach uses a different weight encoding (codebooks are additive), and learned codebooks instead of a fixed codebook. Thus, our insight is that we should be able to obtain higher accuracy by direct optimization of the codebooks over the calibration set, removing the rotation. Further, we show that codebooks for different layers can co-train via joint fine-tuning over the calibration data.

### 2.2. Quantization for Nearest Neighbor Search

Our work builds on approximate nearest neighbor search (ANN) algorithms. Unlike PTQ, ANN quantization aims to compress a database of vectors to allow a user to efficiently compute similarities and find nearest neighbors relative to a set of query points. For high compression, modern ANN search algorithms employ vector quantization (VQ)-which quantizes multiple vector dimensions jointly (Burton et al., 1983; Gray, 1984). It achieves this by learning "codebooks": i.e. a set of learnable candidate vectors that can be used to encode the data. To encode a given database vector, VQ splits it into sub-groups of entries, then encodes every group by choosing a vector from the learned codebook. The algorithm efficiently computes distances or dot-products for similarity search by leveraging the linearity of dot products.

Quantization methods for ANN search generalize vector quantization and are referred to as multi-codebook quantization (MCQ). MCQ methods typically do not involve information loss on the query side, which makes them the leading approach for memory-efficient ANN (Ozan et al., 2016; Martinez et al., 2018). We briefly review MCQ below.

Product quantization (PQ) (Jegou et al., 2010) is an early version of MCQ, which encodes each vector $x \in \mathbf{R}^{D}$ as a concatenation of $M$ codewords from $M \frac{D}{M}$-dimensional codebooks $C_{1}, \ldots, C_{M}$, each containing $K$ codewords. PQ decomposes a vector into $M$ separate subvectors and applies vector quantization (VQ) to each subvector, while using a separate codebook. Thus, each vector $x$ is encoded by a tuple of codeword indices $\left[i_{1}, \ldots, i_{M}\right]$ and approximated by $x \approx\left[c_{1 i_{1}}, \ldots, c_{M i_{M}}\right]$. Fast Euclidean distance computation becomes possible using lookup tables:

$\|q-x\|^{2} \approx\left\|q-\left[c_{1 i_{1}}, \ldots, c_{M i_{M}}\right]\right\|^{2}=\sum_{m=1}^{M}\left\|q_{m}-c_{m i_{m}}\right\|^{2}$,

where $q_{m}$ is the $m$ th subvector of a query $q$. This sum can be calculated using $M$ additions and lookups if the distances from query subvectors to codewords are precomputed. Since product-based approximations work better if the $\frac{D}{M}$ dimensional components independent distributions, subsequent work has looked into finding better transformations (Ge et al., 2013; Norouzi \& Fleet, 2013). As for the other similarity functions, (Guo et al., 2016) proposes a quantization procedure for maximum inner product search (MIPS). They minimize quantization error in the inner products be- tween database and query vectors by solving a constrained optimization problem. Similarly to the formula above, this procedure allows for efficient inner product search by precomputing dot products between the query $q$ an all codes in the learned codebooks, then adding these partial dot products to recover the full similarity score.

Non-orthogonal quantizations. Follow-up work (Chen et al., 2010; Babenko \& Lempitsky, 2014; Martinez et al., 2016; Zhang et al., 2014; Ozan et al., 2016; Martinez et al., 2018) generalized the idea of Product Quantization by approximating each vector by a sum of $M$ codewords instead of concatenation. The resulting procedure is still efficient while the approximation accuracy is increased.

For this, Residual Vector Quantization (Chen et al., 2010), quantizes original vectors, and then iteratively quantizes the approximation residuals from the previous iteration. Additive Quantization (AQ) (Babenko \& Lempitsky, 2014) is more general, as it does not impose constraints on the codewords from the different codebooks. Usually, AQ provides the smallest compression errors, but is more complex to train for large $M$. We discuss this in detail in Section 3.

Finally, several recent works (Martinez et al., 2016; 2018; Zhang et al., 2014) elaborate the idea of Additive Quantization, proposing the more effective procedure for codebooks learning. Composite Quantization (CQ) (Zhang et al., 2014) learns codebooks with a fixed value of scalar product between the codewords from different codebooks. Currently, the state-of-the-art compression accuracy is achieved by the LSQ method (Martinez et al., 2018).

Vector quantization for model compression. There has been significant work on exploiting vector quantization in the context of machine learning. For instance, Zhou et al. (2017); Li et al. (2017); Chen et al. (2019) use multi-codebook quantization to compress word embeddings within deep learning models. Another line of work (Blalock \& Guttag, 2021; McCarter \& Dronen, 2022; FernándezMarqués et al., 2023) explores vector quantization for linear models, or linear layers within deep models. Similarly to $\mathrm{PQ}$ above, these techniques pre-compute inner products between inputs and all codes, then compute linear layer via look-up, which speeds up inference. However, these algorithms introduce significant prediction error that does not allow them to compress deep models. Thus, we believe we are the first to successfully adapt and scale MCQ to LLMs.

## 3. AQLM: Additive Quantization for LLMs

### 3.1. Overview

We start from the observation that additive quantization (AQ) solves a related problem to post-training quantization (PTQ) (Nagel et al., 2020; Frantar et al., 2022b): both settings assume the existence of a set of "input" vectors, i.e.
input data for AQ, and the weight matrix rows for PTQ. The goal is to compress these inputs while preserving dot product similarity, against query vectors (for AQ), and against layer input embeddings (for PTQ). The difference between the two is that AQ assumes that the distribution of queries is unknown, whereas PTQ methods, e.g. (Frantar et al., 2022b), show that it is sufficient to optimize for sample input embeddings from a set of calibration data.

At a high level, we start by solving the following problem: for a linear layer with $d_{i n}$ input and $d_{o u t}$ output features given its weights $\mathbf{W} \in \mathbb{R}^{d_{\text {out }} \times d_{\text {in }}}$ and a set of calibration inputs $\mathbf{X} \in \mathbb{R}^{d_{i n} \times n}$, one seeks for a configuration of quantized weights $\hat{W}$ that optimizes squared error between the output of the original and compressed layer:

$$
\begin{equation*}
\underset{\widehat{\mathbf{W}}}{\arg \min }\|\mathbf{W} \mathbf{X}-\widehat{\mathbf{W}} \mathbf{X}\|_{2}^{2} \tag{1}
\end{equation*}
$$

In the following, we will assume that $\widehat{\mathbf{W}}$ is quantized using AQ, and adopt standard notation (Martinez et al., 2016). AQ splits weight rows into groups of $g$ consecutive elements, and represents each group of weights as a sum of $M$ vectors chosen from multiple learned codebooks $C_{1}, \ldots C_{M}$, each containing $2^{B}$ vectors (for $\mathrm{B}$-bit codes). A weight is encoded by choosing a single code from each codebook and summing them up. We denote this choice as a one-hot vector $b_{m}$, which results in the following representation for a group: $\sum_{m=1}^{M} C_{m} b_{i j m}$. This is similar to PTQ algorithms (Frantar et al., 2022a), except for using much more complex coding per group. To represent the full weights, we simply concatenate:

$$
\begin{equation*}
\widehat{\mathbf{W}}_{i}=\sum_{m=1}^{M} C_{m} b_{i, 1, m} \oplus \ldots \oplus \sum_{m=1}^{M} C_{m} b_{i, d_{i n} / g, m} \tag{2}
\end{equation*}
$$

where $\oplus$ denotes concatenation and $b_{i j m} \in \mathbb{R}^{2^{B}}$ represents a one-hot code for the $i$-th output unit, $j$-th group of input dimensions and $m$-th codebook.

![](https://cdn.mathpix.com/cropped/2024_06_04_46c218fad80e5caf3b83g-04.jpg?height=236&width=228&top_left_y=1823&top_left_x=211)

Weight Matrix

![](https://cdn.mathpix.com/cropped/2024_06_04_46c218fad80e5caf3b83g-04.jpg?height=341&width=523&top_left_y=1800&top_left_x=451)

Figure 2: Groups of weights are represented by a sum of codes selected from codebooks by corresponding indices.

Our algorithm will learn codebooks $C_{m} \in \mathbb{R}^{g \times 2^{B}}$ and the discrete codes represented by one-hot $b \in$ $\mathbb{R}^{d_{\text {out }} \times d_{\text {in }} / g \times M \times 2^{B}}$. The resulting scheme encodes each group of $g$ weights using $M \cdot B$ bits and further requires $g \cdot 2^{B} \cdot 16$ bits for FP16 codebooks. The error becomes:

$$
\begin{equation*}
\underset{C, b}{\arg \min } \| \mathbf{W X}-\left(\text { Concat }_{i, j} \sum_{m=1}^{M} C_{m} b_{i, j, m}\right) \mathbf{X} \|_{2}^{2} \tag{3}
\end{equation*}
$$

To learn this weight representation, we initialize codebooks $C$ and codes $b$ by running residual K-means as in Chen et al. (2010). Then, we alternate between updating codes $b_{i, j, m}$ and codebooks $C_{m}$ until the loss function (3) stops improving up to the specified tolerance. Since codes are discrete and codebooks are continuous, and we are optimizing over multiple interacting layers, our approach has three phases, described in Algorithm 1 and detailed below.

### 3.2. Phase 1: Beam search for codes

First, AQLM updates the codes $b_{i, j, m}$ to minimize the MSE objective (3). Similarly to Babenko \& Lempitsky (2014); Martinez et al. (2016; 2018), we reformulate the objective in terms of a fully-connected discrete Markov Random Field (MRF) to take advantage of MRF solvers.

To simplify the derivation, let us first consider a special case of a single output unit $\left(d_{\text {out }}=1\right)$ and a single quantization group (i.e. $g=d_{i n}$ ), to get rid of the concatenation operator: $\left\|\mathbf{W X}-\sum_{m=1}^{M} C_{m} b_{m} \mathbf{X}\right\|_{2}^{2}$. We rewrite this objective by expanding the squared difference:

$$
\begin{align*}
& \left\|\mathbf{W X}-\sum_{m=1}^{M} C_{m} b_{m} \mathbf{X}\right\|_{2}^{2}=\|\mathbf{W} \mathbf{X}\|_{2}^{2}- \\
& -2\left\langle\mathbf{W X}, \sum_{m=1}^{M} C_{m} b_{m} \mathbf{X}\right\rangle_{F}+\left\|\sum_{m=1}^{M} C_{m} b_{m} \mathbf{X}\right\|_{2}^{2} \tag{4}
\end{align*}
$$

Above, $\langle\cdot, \cdot\rangle_{F}$ denotes a Frobenius inner product of two matrices. Next, let us consider the three components of Eqn. (4) in isolation. First, note that $\|\mathbf{W} \mathbf{X}\|_{2}^{2}$ is constant in $b$ and can be ignored. The second component can be expanded further into pairwise dot products:

$$
\begin{equation*}
\left\|\sum_{m=1}^{M} C_{m} b_{m} \mathbf{X}\right\|_{2}^{2}=\sum_{i=1}^{M} \sum_{j=1}^{M}\left\langle C_{i} b_{i} \mathbf{X}, C_{j} b_{j} \mathbf{X}\right\rangle_{F} \tag{5}
\end{equation*}
$$

Note that both the second and third components rely on Frobenius products of $C_{m} b_{m} \mathbf{X}$-like matrices. These matrices can be inconvenient in practice: since $\mathbf{X} \in \mathbb{R}^{d_{i n} \times n}$, the size of each matrix will scale with the size of calibration dataset $n$. To circumvent this, we rewrite the products as:

$$
\begin{equation*}
\left\langle C_{i} b_{i} \mathbf{X}, C_{j} b_{j} \mathbf{X}\right\rangle_{F}=\left\langle C_{i} b_{i} \mathbf{X} \mathbf{X}^{T}, C_{j} b_{j}\right\rangle_{F} \tag{6}
\end{equation*}
$$

Thus one can pre-compute $\mathbf{X} \mathbf{X}^{T} \in \mathbb{R}^{d_{i n} \times d_{i n}}$. We will denote this type of product as $\langle\mathbf{A}, \mathbf{B}\rangle_{\mathbf{X X}^{T}} \stackrel{\text { def }}{=}\left\langle\mathbf{A X X}{ }^{T}, \mathbf{B}\right\rangle_{F}$ in future derivations. Then, Eqn. (4) becomes:

$$
\begin{align*}
\left\|\mathbf{W X}-\sum_{m=1}^{M} C_{m} b_{m} \mathbf{X}\right\|_{2}^{2}=\|\mathbf{W} \mathbf{X}\|_{2}^{2}- \\
-2 \sum_{m=1}^{M}\left\langle\mathbf{W}, C_{m} b_{m}\right\rangle_{\mathbf{X} \mathbf{x}^{T}}+\sum_{i=1}^{M} \sum_{j=1}^{M}\left\langle C_{i} b_{i}, C_{j} b_{j}\right\rangle_{\mathbf{X X}^{T}} \tag{7}
\end{align*}
$$

Finally, we generalize this equation to multiple output units $\left(d_{\text {out }}>1\right)$ and quantization groups $\left(g \neq d_{\text {in }}\right)$. For $d_{\text {out }}>1$, note that the original objective (3) is additive with respect to output units: thus, we can apply (7) independently to each output dimension and sum up results. To support multiple input groups $\left(g \neq d_{i n}\right)$, we can treat each group as a separate codebook where only the codes for the active group are nonzero. Thus, we need to repeat each codebook $d_{i n} / g$ times and pad it with zeros according to the active group.

It is now evident that minimizing (4) is equivalent to MAP inference in a Markov Random Field with $\left\langle\mathbf{W}, C_{m} b_{m}\right\rangle \mathbf{X X}^{T}$ as unary potentials and $\left\langle C_{i} b_{i}, C_{j} b_{j}\right\rangle_{\mathbf{X x}^{T}}$ as pairwise potentials. While finding the exact optimum is infeasible, prior work has shown that this type of MRF can be solved approximately via beam search or ICM (Besag, 1986).

To solve this problem, we chose to adapt a beam search algorithm from Babenko \& Lempitsky (2014). This algorithm maintains a beam of $k$ (beam size) best configurations for the codes, starting from the previous solution. On each step, the algorithm attempts to replace one code by trying all $2^{B} k$ alternatives and selecting the $k$ best based on MSE (7).

Since the loss function is additive, changing one code only affects a small subset of loss components. Thus, we can compute the loss function efficiently by starting with a previous loss function (before code replacement), then adding and subtracting the components that changed during this iteration. These few loss components can be computed efficiently by multiplying with $\mathbf{X} \mathbf{X}^{T}$ ahead of beam search. The beam search runs over all $d_{o u t}$ output units in parallel. This is possible because encoding one output unit does not affect the objective (7) of other units. Note that beam search is not necessarily the best solution to this problem. AQ variants for retrieval (Martinez et al., 2016; 2018) use randomized ICM to find solutions faster. In this study, we chose beam search because it was easier to implement in ML frameworks like PyTorch/JAX.

### 3.3. Phase 2: Codebook update

In the second phase, we find the optimal codebook vectors $C_{1}, \ldots, C_{M}$ that minimize the same squared error as the beam search. If we treat the codes $b$ as constants, minimizing (3) becomes a least squares problem for $C_{m}$. The original AQ algorithm solves this problem in closed form, relying on the fact that each vector dimension can be optimized independently. Our problem is complicated due to the presence of $\mathbf{X} \mathbf{X}^{T}$ : the optimal value of one codebook coordinate depends on the values of all others. In principle, we could optimize $C_{m}$ in closed form, but it would require inverting a large matrix, or using iterative least squares solvers (e.g. conjugate gradients) specialized to this problem.

For simplicity, our current implementation defaults to using Adam (Kingma \& Ba, 2015) for approximately solving this minimization problem. In practice, this codebook tuning phase takes up a small fraction of the total compute time. We compute the objective as follows:

$$
\begin{align*}
\|\mathbf{W} \mathbf{X}-\widehat{\mathbf{W}} \mathbf{X}\|_{2}^{2} & =\|(\mathbf{W}-\widehat{\mathbf{W}}) \mathbf{X}\|_{2}^{2}= \\
& =\left\langle(\mathbf{W}-\widehat{\mathbf{W}}) \mathbf{X} \mathbf{X}^{T},(\mathbf{W}-\widehat{\mathbf{W}})\right\rangle_{F} \tag{8}
\end{align*}
$$

where $\widehat{\mathbf{W}}$ is the quantized weight matrix from 2 , and the $\mathbf{X X}^{T}$ matrix is pre-computed. We optimize this objective by iterating (non-stochastic) full-batch gradient descent.

For each update phase, our implementation runs 100 Adam steps with learning rate 1e-4. However, we found that the final result is not sensitive to either of these parameters: training with smaller number of steps or learning rate achieves the same loss, but takes longer to converge. In future work, these hyperparameters could be eliminated by switching to dedicated least squares solver for codebooks. Similarly to other algorithms, we also learn per-unit scales $s \in \mathbb{R}^{h}$ that are initialized as $s_{i}:=\left\|\mathbf{W}_{i}\right\|_{2}$ and updated alongside codebooks via the same optimizer (line 19 in Algorithm 1).

### 3.4. Phase 3: Fine-tuning for intra-layer cohesion

So far, our algorithm compresses each weight matrix independently of the rest of the model. However, in practice, quantization errors interact differently between matrices. This issue is especially relevant in the case of extreme (2bit) compression, where quantization errors are larger.

Prior work addresses this issue via quantization-aware training (QAT), e.g. (Gholami et al., 2021). Instead of compressing the entire model in a single pass, they quantize model parameters gradually and train the remaining parameters to compensate for the quantization error. Unfortunately, running QAT in our setting is infeasible, since most modern LLMs are extremely expensive to train or even fine-tune. Thus, most PTQ algorithms for LLMs only adjust model parameters within the same linear layer (Frantar et al., 2022a; Lin et al., 2023; Dettmers et al., 2023b).

Here, we opt for a middle ground by performing optimization at the level of individual transformer blocks, i.e. groups of 4-8 linear layers ${ }^{2}$ that constitute a single multi-head self-[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_46c218fad80e5caf3b83g-06.jpg?height=284&width=1705&top_left_y=197&top_left_x=188)

Figure 3: AQLM compressed weight format. Horizontal and vertical axes are input features and output units, respectively. Depth represents the codebook index. Reconstruction procedure, from left to right: i) compressed weight codes ii) zoom-in one weight group, each code is an index in its respective codebook iii) select codes from each codebook iv) add up codes as in (2) v) multiply by scales (one scale per output dimension).

```
Algorithm 1 AQLM: Additive Quantization for LLMs
Require: model, data
    $\mathbf{X}_{\text {block }}:=$ model.input_embeddings (data)
    for $i=1, \ldots$,model.num_layers do
        block := model.get_block(i)
        $\mathbf{Y}_{\text {block }}:=\operatorname{block}\left(\mathbf{X}_{\text {block }}\right)$
        for layer $\in$ linear_layers (block) do
            $\mathbf{W}:=$ layer.weight
            $\mathbf{X}:=$ layer_inputs(layer, $\mathbf{X}_{\text {block }}$ )
            $C, b, s:=$ initialize(W) // k-means
            while loss improves by at least $\tau$ do
                    $C, s:=$ train_Cs_adam $\left(\mathbf{X X}^{T}, \mathbf{W}, C, b, s\right)$
                    $b:=$ beam_search $\left(\mathbf{X X}^{T}, \mathbf{W}, C, b, s\right)$
            end while
            /* save for fine-tuning */
            layer.weight:= AQLMFormat $(C, b, s)$
        end for
        $\theta:=$ trainable_parameters(block)
        while loss improves by at least $\tau$ do
            $L:=\left\|b l o c k\left(\mathbf{X}_{\text {block }}\right)-\mathbf{Y}_{\text {block }}\right\|_{2}^{2}$
            $\theta:=\operatorname{adam}\left(\theta, \frac{\partial L}{\partial \theta}\right)$
        end while
        $\mathbf{Y}_{\text {block }}:=\operatorname{block}\left(\mathbf{X}_{\text {block }}\right)$
    end for
```

attention, followed by a single and MLP layer. Having quantized all linear layers within a single transformer block, we fine-tune its remaining parameters to better approximate the original outputs of that transformer block by backpropagating through the weight representation (2).

Concretely, we use the PyTorch autograd engine to differentiate the $\left\|\operatorname{block}\left(\mathbf{X}_{\text {block }}\right)-\mathbf{Y}_{\text {block }}\right\|^{2}$, where $\mathbf{X}_{\text {block }}$ are the inputs activations for that transformer block and $\mathbf{Y}_{\text {block }}$ are output activations of block $\left(\mathbf{X}_{\text {block }}\right)$ recorded prior to quantization. We train the codebooks $C_{m}$, scale vectors $s$ and all non-quantized parameters (RMSNorm scales and biases), while keeping the codes $b_{i, j, m}$ frozen. Similarly to Section 3.3, we train these parameters using Adam to minimize the MSE against the original block outputs (prior to quantization). This phase uses the same calibration data as for the individual layer quantization. The full procedure is summarized in Alg. 1.

While fine-tuning blocks is more expensive than individual linear layers, it is still possible to quantize billion-parameter

![](https://cdn.mathpix.com/cropped/2024_06_04_46c218fad80e5caf3b83g-06.jpg?height=46&width=813&top_left_y=2433&top_left_x=190)

models on a single GPU in reasonable time. Also, since the algorithm only modifies a few trainable parameters, it uses little VRAM for optimizer states. This fine-tuning converges after a few iterations, as it starts from a good initial guess. In practice, fine-tuning transformer layers takes a minority (10-30\% or less) of the total calibration time.

## 4. Experiments

We evaluate the AQLM algorithm in typical scenarios for post-training quantization of modern LLMs. Our evaluation is focused on the LLAMA 2 model family since it is a popular backbone for fine-tuned models or general LLM applications, e.g. (Dettmers et al., 2023a), and we also present results on Mistral-family models (Jiang et al., 2024). In Section 4.1, we evaluate the full AQ procedure for various LLAMA 2 models and quantization bit-widths; Section 4.2 presents an ablation analysis for individual AQ components and implementation details.

### 4.1. Compression quality for modern LLMs

We report perplexity on WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020) validation sets. We also measure zero-shot accuracy on WinoGrande (Sakaguchi et al., 2021), PiQA (Tata \& Patel, 2003), HellaSwag (Zellers et al., 2019), ARC-easy and ARC-challenge (Clark et al., 2018) via the LM Eval Harness (Gao et al., 2021). We broadly follow the evaluation setup of GPTQ (Frantar et al., 2022a).

We consider three main targets in terms of compression ranges: 2-2.8 bits, 3-3.1 bits, and 4-4.1 bits per model parameter. In the results below average bits per parameter takes into account only quantized weights, we do not include parameters kept in floating precision similarly to the related work. The details on the model size estimate are provided in Appendix G. We compare AQ against GPTQ for $3 \& 4$ bits (Frantar et al., 2022a), SpQR for 3\&4 bits (Dettmers et al., 2023b), QuIP in 2,3 \& 4 bits (Chee et al., 2023) and QuIP\# for $2 \& 4$ bits (Tseng et al., 2023). While GPTQ and SpQR technically support 2-bit quantization, they perform poorly in the 2-3 bit range. For QuIP, we omit results for the 7B model, as we could not achieve competitive performance in this one scenario using the available implementations. (Currently, there is no official implementation of the origi-

Extreme LLM Compression via Additive Quantization

Table 1: Evaluation of quantized LLAMA 2 models for 2-2.8 bits per parameter, with an extra section for higher bitwidth. We report perplexity on WikiText2 (Merity et al., 2016) \& C4 (Raffel et al., 2020) and accuracy for zero-shot tasks. The Average accuracy is the mean of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy.

| Size | Method | Avg bits | $\mid$ Wiki $2 \downarrow$ | $\mathrm{C} 4 \downarrow$ | $\mid$ WinoGrande $\uparrow$ | PiQA $\uparrow$ | HellaSwag $\uparrow$ | $\operatorname{ArcE} \uparrow$ | $\operatorname{ArcC} \uparrow$ | Average accuracy $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 7B | - | 16 | 5.12 | 6.63 | 67.25 | 78.45 | 56.69 | 69.32 | 40.02 | 62.35 |
|  | AQLM | 2.02 | 6.64 | 8.56 | 64.17 | 73.56 | 49.49 | 61.87 | 33.28 | 56.47 |
|  | QuIP\# | 2.02 | 8.22 | 11.01 | 62.43 | 71.38 | 42.94 | 55.56 | 28.84 | 52.23 |
|  | AQLM | 2.29 | 6.29 | 8.11 | 65.67 | 74.92 | 50.88 | 66.50 | 34.90 | 58.57 |
| 13B | - | 16 | 4.57 | 6.05 | 69.61 | 78.73 | 59.72 | 73.27 | 45.56 | 65.38 |
|  | $\mathrm{AQLM}$ | 1.97 | 5.65 | 7.51 | 65.43 | 76.22 | 53.74 | 69.78 | 37.8 | 60.59 |
|  | QuIP | 2.00 | 13.48 | 16.16 | 52.80 | 62.02 | 35.80 | 45.24 | 23.46 | 43.86 |
|  | QuIP\# | 2.01 | 6.06 | 8.07 | 63.38 | 74.76 | 51.58 | 64.06 | 33.96 | 57.55 |
|  | AQLM | 2.18 | 5.41 | 7.20 | 68.43 | 76.22 | 54.68 | 69.15 | 39.42 | 61.58 |
|  | AQLM | 2.53 | 5.15 | 6.80 | 68.11 | 76.99 | 56.54 | 71.38 | 40.53 | 62.71 |
|  | AQLM | 2.76 | 4.94 | 6.54 | 68.98 | 77.58 | 57.71 | 72.90 | 43.60 | 64.15 |
| 70B | - | 16 | 3.12 | 4.97 | 76.95 | 81.07 | 63.99 | 77.74 | 51.11 | 70.17 |
|  | AQLM | 2.07 | 3.94 | 5.72 | 75.93 | 80.43 | 61.79 | 77.68 | 47.93 | 68.75 |
|  | QuIP | 2.01 | 5.90 | 8.17 | 67.48 | 74.76 | 50.45 | 62.16 | 33.96 | 57.76 |
|  | QuIP\# | 2.01 | 4.16 | 6.01 | 74.11 | 79.76 | 60.01 | 76.85 | 47.61 | 67.67 |

Table 2: Evaluation of quantized LLAMA 2 models for 3-3.1 bits per parameter, with the same metrics as in Table 1.

| Size | Method | Avg bits | $\mid$ Wiki $2 \downarrow$ | $\mathrm{C} 4 \downarrow$ | WinoGrande $\uparrow$ | PiQA $\uparrow$ | HellaSwag $\uparrow$ | $\operatorname{ArcE} \uparrow$ | $\operatorname{ArcC} \uparrow$ | Average accuracy $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 7B | - | 16 | 5.12 | 6.63 | 67.25 | 78.45 | 56.69 | 69.32 | 40.02 | 62.35 |
|  | AQLM | 3.04 | 5.46 | 7.08 | 66.93 | 76.88 | 54.12 | 68.06 | 38.40 | 60.88 |
|  | GPTQ | 3.00 | 8.06 | 10.61 | 59.19 | 71.49 | 45.21 | 58.46 | 31.06 | 53.08 |
|  | $\mathrm{SpQR}$ | 2.98 | 6.20 | 8.20 | 63.54 | 74.81 | 51.85 | 67.42 | 37.71 | 59.07 |
| 13B | - | 16 | 4.57 | 6.05 | 69.61 | 78.73 | 59.72 | 73.27 | 45.56 | 65.38 |
|  | AQLM | 3.03 | 4.82 | 6.37 | 68.43 | 77.26 | 58.30 | 70.88 | 42.58 | 64.49 |
|  | GPTQ | 3.00 | 5.85 | 7.86 | 63.93 | 76.50 | 53.47 | 65.66 | 38.48 | 59.61 |
|  | SpQR | 2.98 | 5.28 | 7.06 | 67.48 | 77.20 | 56.34 | 69.78 | 39.16 | 61.99 |
|  | QuIP | 3.00 | 5.12 | 6.79 | 69.93 | 76.88 | 57.07 | 70.41 | 41.47 | 63.15 |
| 70B | - | 16 | 3.12 | 4.97 | 76.95 | 81.07 | 63.99 | 77.74 | 51.11 | 70.17 |
|  | AQLM | 3.01 | 3.36 | 5.17 | 77.19 | 81.28 | 63.23 | 77.61 | 50.00 | 69.86 |
|  | GPTQ | 3.00 | 4.40 | 6.26 | 71.82 | 78.40 | 60.00 | 72.73 | 44.11 | 65.41 |
|  | $\mathrm{SpQR}$ | 2.98 | 3.85 | 5.63 | 74.66 | 80.52 | 61.95 | 75.93 | 48.04 | 68.22 |
|  | QuIP | 3.01 | 3.87 | 5.67 | 74.59 | 79.98 | 60.73 | 73.19 | 46.33 | 66.96 |

nal QuIP (non-\#) for the LLAMA 2 model.) For QuIP\#, we focus on 2 and 4 bit because the available implementation does not yet support 3-bit compression. We calibrate each algorithm using the subset of RedPajama dataset (Computer, 2023), with a sequence length of 4096.

The exact bit-widths for each method are dictated by parameters such as the number of codebooks and code width. We report results for the $2-2.8$ and $3-3.1$ bitwidth ranges in Tables 1 and 2, respectively. Additional results for $4-4.1$ bits are deferred to Appendix E.2.

The results show that $\mathrm{AQLM}$ outperforms the previous best PTQ algorithms across all settings, often by wide margins, especially at high compression. This holds both in terms of
PPL across standard validation sets (Wiki-Text2 and C4), and accuracy across zero-shot tasks. Specifically, we observe the highest accuracy gains in the "extreme" 2-2.1 bits per parameter range, where the deviation from the uncompressed model becomes large for all methods.

Mixtral quantization. Table 3 presents results on the Mixtral MoE-type model, comparing against QuIP\# at 2-bits. (See Appendix E. 1 for full results.) AQLM outperforms QuIP\# in this case as well. Although the margins are lower compared to LLAMA 2 models, they are still significant for "harder" tasks, such as Arc Challenge ( +3 points).

Pareto optimality of AQLM. The significant error improve-

Table 3: Evaluation of quantized Mixtral (Jiang et al., 2024) models for 2 bits. The table reports perplexity on WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy column is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy.

| Size | Method | Avg bits | Wiki $2 \downarrow$ | C4 $\downarrow$ | WinoGrande $\uparrow$ | PiQA $\uparrow$ | HellaSwag $\uparrow$ | ArcE $\uparrow$ | ArcC $\uparrow$ | Average accuracy $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 8x7B |  |  |  |  |  |  |  |  |  |  |
|  | - | 16 | 3.46 | 5.02 | 75.45 | 82.37 | 64.65 | 83.38 | 55.80 | 72.33 |
|  | QuLM | 1.98 | $\mathbf{4 . 6 1}$ | $\mathbf{5 . 7 5}$ | $\mathbf{7 3 . 6 4}$ | $\mathbf{7 9 . 2 7}$ | 57.91 | $\mathbf{7 8 . 9 6}$ | $\mathbf{4 8 . 6 3}$ | $\mathbf{6 7 . 6 8}$ |
|  | QuIP\# | 2.01 | 4.75 | 5.89 | 71.11 | 79.05 | $\mathbf{5 8 . 2 3}$ | 77.57 | 45.73 | 66.34 |

ments raise the question of choosing the "optimal" model variant to maximize accuracy within a certain memory budget. For this, we follow Dettmers \& Zettlemoyer (2022): a quantized model is said to be Pareto-optimal if it maximizes accuracy at the same or lower total size (bytes). Despite rapid progress, prior art methods are not Pareto-optimal at 2-bits: for instance, the previous best 2-bit LLAMA 2 13B (QuIP\#, Table 1) achieves Wiki2 PPL of 6.06, but one can get much lower 5.21 PPL by using a 7B model with 4-bit quantization, which is smaller (see Appendix Table 10).

AQLM compression to strictly 2 bits for the same model is also below Pareto-optimality, as it is outperformed by 4-bit AQLM compression for LLAMA 2 7B (5.21 vs 5.65). To find the Pareto-optimal quantization bitwidth, we run experiments between 2-3 bits per parameter and report them in Table 1, below horizontal bars. Thus, the Pareto-optimal bitwidth for AQLM appears to be around 2.5 bits per parameter (Table 1), at which point we are comparable to 5-bit AQLM for LLAMA 2 7B (Appendix Table 10). In turn, the 2.76-bit AQLM on 13B outperforms the uncompressed 7B model. As such, AQLM is the first algorithm to achieve Pareto-optimality at less than 3 bits per parameter.

### 4.2. Ablation analysis

In Appendix D, we examine key design choices regarding initialization, alternating optimization, the impact of the fine-tuning protocol, distribution of codebooks vs groups, as well as other hyper-parameters. In brief, we first find that the residual $K$-means initialization is critical for fast algorithm convergence: when compared with random initialization, it needs significantly fewer training iterations. Second, to validate our calibration fine-tuning procedure, we compare it against 1) no fine-tuning, 2) fine-tuning only of non-linear layers (e.g. RMSNorm) but not of codebook parameters, and 3) fine-tuning only the codebooks (but not other layers). The results, presented in full in Appendix D, show that finetuning the codebook parameters has the highest impact on accuracy, by far, while fine-tuning the RMSNorm only has minor impact. This validates our choice of leveraging the calibration set for learned codebooks.

Further, we observe that increasing the number of sample sequences in the range 128 to 4096 leads to a gradual PPL improvement, but with diminishing returns. In this respect, AQLM benefits more from larger calibration sets (similarly to QuIP\#), as opposed to direct methods like GPTQ which saturate accuracy at around 256 input sequences. Finally, we investigate various options for investing a given bit budget, comparing e.g. longer codes (e.g. $1 \times 15$ ) vs multiple codebooks with shorter codes (e.g. $2 x 8$ ).

### 4.3. Inference Speed

Although our primary objective is to maximize accuracy for a given model size, AQLM can also be practical in terms of inference latency. To demonstrate this, we implemented efficient GPU and CPU kernels for a few hardware-friendly configurations of AQLM. The results can be found in Table 4. For GPU inference, we targeted quantized Llama 2 models with 16 -bit codebooks, corresponding to 2.07 bits for LLAMA 2 70B, 2.18 bits for 13B, and 2.29 bits for 7B models (see Table 1), as well as a $2 \times 8$-bit codebook model with perplexity 7.98 on Wiki2. For each model we benchmark the matrix-vector multiplication subroutine performance on a standard layer. The results show that AQLM can execute at speeds comparable to or better than FP16. End-to-end generative numbers on a preliminary HuggingFace integration can be found in Appendix H: for instance, we can achieve $\sim 6$ tokens/s on LLAMA 2 70B in this setting. We observe that multiple smaller codebooks allow efficient GPU cache utilization, leading to greater speedup, at the price of slightly lower accuracy.

Table 4: Speed of the FP16 gate_proj layer matrix-vector multiplication in PyTorch, and relative AQLM speedups.

| Llama 2 | $7 \mathrm{~B}$ | 13B | $70 B$ |
| :---: | :---: | :---: | :---: |
| 2 bit speedup over FP16 on Nvidia RTX 3090 GPU |  |  |  |
| Original (float16) | $129 \mu \mathrm{s}$ | $190 \mu \mathrm{s}$ | $578 \mu \mathrm{s}$ |
| AQLM (Table 1) | $\mathrm{x} 1.31$ | $\mathrm{x} 1.20$ | $\mathrm{x} 1.20$ |
| AQLM $(2 \times 8$-bit $)$ | $\mathrm{x} 1.57$ | $\mathrm{x} 1.82$ | $\mathrm{x} 3.05$ |
| 2 bit speedup over FP32 on Intel i9 CPU, 8 cores |  |  |  |
| Original (float32) | $1.83 \mathrm{~ms}$ | $3.12 \mathrm{~ms}$ | $11.31 \mathrm{~ms}$ |
| AQLM (2×8-bit) | $x 2.75$ | $\mathrm{x} 3.54$ | $x 3.69$ |
| AQLM (4×8-bit) | $x 2.55$ | $x 3.02$ | $x 4.07$ |
| AQLM (8×8-bit) | $\mathrm{x} 2.29$ | $x 2.68$ | $x 4.03$ |

Next, we explore how to leverage AQLM to accelerate CPU inference. As discussed in Section 2.2, additive quantization can compute dot products efficiently if the codebook size is small. One way to achieve it for AQLM is to replace each 16-bit codebook with a number of smaller 8-bit ones. This leads to higher quantization error, but still outperforms the baselines in terms of accuracy (see Appendix Table 9). The results in Table 4 show that this also allows for up to $4 x$ faster inference relative to $\mathrm{FP} 32$ on CPU.

## 5. Conclusion and Future Work

We presented AQLM, a new form of additive quantization (AQ) targeted to LLM compression, which significantly improved the state-of-the-art results for LLM quantization in the regime of 2 and 3 bits per weight. In terms of limitations, AQLM is more computationally-expensive than direct post-training quantization methods, such as RTN or GPTQ, specifically because of the use of a more complex coding representation. Yet, despite the more sophisticated encoding and decoding, we have shown AQLM lends itself to efficient implementation on both CPU and GPU. Overall, we find it remarkable that, using AQLM, massive LLMs can be executed accurately and efficiently using little memory.

## 6. Acknowledgements

Authors would like to thank Ruslan Svirschevski for his help in solving technical issues with AQLM and baselines. We also thank Tim Dettmers for helpful discussions on the structure of weights in modern LLMs and size-accuracy trade-offs. The authors would also like to thank Daniil Pavlov for his assistance with CPU benchmarking. Finally, authors would like to thank the communities of ML enthusiasts known as LocalLLaMA ${ }^{3}$ and Petals community on discord ${ }^{4}$ for the crowd wisdom about running LLMs on consumer devices.

## References

Babenko, A. and Lempitsky, V. Additive quantization for extreme vector compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 931-938, 2014.

Besag, J. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society Series B: Statistical Methodology, 48(3):259-279, 1986.

Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for ana-[^3]

lyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.

Blalock, D. and Guttag, J. Multiplying matrices without multiplying. In International Conference on Machine Learning, pp. 992-1004. PMLR, 2021.

Burton, D., Shore, J., and Buck, J. A generalization of isolated word recognition using vector quantization. In ICASSP '83. IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 8, pp. 1021-1024, 1983. doi: 10.1109/ICASSP.1983.1171915.

Chee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit quantization of large language models with guarantees, 2023.

Chen, S., Wang, W., and Pan, S. J. Deep neural network quantization via layer-wise optimization using limited training data. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3329-3336, Jul. 2019. doi: 10.1609/aaai.v33i01.33013329. URL https://ojs.aaai.org/index.php/AAAI / article/view/4206.

Chen, Y., Guan, T., and Wang, C. Approximate nearest neighbor search by residual vector quantization. Sensors, $10(12): 11259-11273,2010$.

Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai 2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/ RedPajama-Data.

Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.

Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.

Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023a.

Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr: A sparse-quantized representation for near-lossless $11 \mathrm{~m}$ weight compression. arXiv preprint arXiv:2306.03078, 2023b.

Fernández-Marqués, J., AbouElhamayed, A. F., Lane, N. D., and Abdelfattah, M. S. Are we there yet? product quantization and its hardware acceleration. ArXiv, abs/2305.18334, 2023. URL https: //api.semanticscholar.org/CorpusID: 258967539 .

Frantar, E. and Alistarh, D. Qmoe: Practical sub-1-bit compression of trillion-parameter models. arXiv preprint arXiv:2310.16795, 2023.

Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022a.

Frantar, E., Singh, S. P., and Alistarh, D. Optimal Brain Compression: A framework for accurate posttraining quantization and pruning. arXiv preprint arXiv:2208.11580, 2022b. Accepted to NeurIPS 2022, to appear.

Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for fewshot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.

Ge, T., He, K., Ke, Q., and Sun, J. Optimized product quantization. IEEE transactions on pattern analysis and machine intelligence, 36(4):744-755, 2013.

Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.

Gray, R. Vector quantization. IEEE ASSP Magazine, 1(2): 4-29, 1984. doi: 10.1109/MASSP.1984.1162229.

Guo, R., Kumar, S., Choromanski, K., and Simcha, D. Quantization based fast inner product search. In Artificial intelligence and statistics, pp. 482-490. PMLR, 2016.

Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network, 2015.

Jegou, H., Douze, M., and Schmid, C. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2010.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.

Kurtic, E., Kuznedelev, D., Frantar, E., Goin, M., and Alistarh, D. Sparse fine-tuning for inference acceleration of large language models, 2023.

Li, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Performance guaranteed network acceleration via high-order residual quantization, 2017.

Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.

Martinez, J., Clement, J., Hoos, H. H., and Little, J. J. Revisiting additive quantization. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 137-153. Springer, 2016.

Martinez, J., Zakhmi, S., Hoos, H. H., and Little, J. J. Lsq++: Lower running time and higher recall in multi-codebook quantization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 491-506, 2018.

McCarter, C. and Dronen, N. Look-ups are not (yet) all you need for deep learning inference. ArXiv, abs/2207.05808, 2022. URL https: //api.semanticscholar.org/CorpusID: 250491319 .

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.

Norouzi, M. and Fleet, D. J. Cartesian k-means. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pp. 3017-3024, 2013.

Ozan, E. C., Kiranyaz, S., and Gabbouj, M. Competitive quantization for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 28(11):2884-2894, 2016. doi: 10.1109/ TKDE.2016.2597834.

Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuQmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In Conference on Neural Information Processing Systems (NeurIPS). 2019.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21 (140):1-67, 2020.

Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, 2021. doi: 10.1145/3474381. URL https://doi.org/ $10.1145 / 3474381$.

Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Sun, S., Cheng, Y., Gan, Z., and Liu, J. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.

Tata, S. and Patel, J. M. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.

TII UAE. The Falcon family of large language models. https://huggingface.co/tiiuae/ falcon-40b, May 2023.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and Sa, C. D. Quip\#: Quip with lattice codebooks, 2023.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.

Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In Korhonen, A., Traum, D. R., and Màrquez, L. (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4791-4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https:// doi.org/10.18653/v1/p19-1472.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Zhang, T., Du, C., and Wang, J. Composite quantization for approximate nearest neighbor search. In International Conference on Machine Learning, pp. 838-846. PMLR, 2014.

Zhou, S.-C., Wang, Y.-Z., Wen, H., He, Q.-Y., and Zou, Y.-H. Balanced quantization: An effective and efficient approach to quantized neural networks. Journal of Computer Science and Technology, 32(4):667-682, Jul 2017. ISSN 1860-4749. doi: 10.1007/s11390-017-1750-y. URL https://doi.org/10.1007/s11390-017$1750-\mathrm{y}$.
