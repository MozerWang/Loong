# Hallucination is Inevitable: <br> An Innate Limitation of Large Language Models 

Ziwei Xu Sanjay Jain Mohan Kankanhalli<br>School of Computing, National University of Singapore<br>ziwei.xu@u.nus.edu \{sanjay,mohan\}@comp.nus.edu.sg


#### Abstract

Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.


## 1 Introduction

The emergence of large language models (LLMs) has marked a significant milestone in the field of artificial intelligence, particularly in natural language processing. These models, with their vast knowledge bases and ability to generate coherent and contextually relevant text, have greatly impacted research, industry, and society. However, one of the critical challenges they face is the problem of "hallucination," where the models generate plausible but factually incorrect or nonsensical information. This issue has brought increasing concerns about safety and ethics as LLMs are being applied widely, resulting in a growing body of literature trying to classify, understand, and mitigate it.

Prior works have identified multiple possible sources of hallucination in LLMs from the data collection to the training and inference aspects. For example, in the survey paper [29], the authors attribute hallucination in natural language generation to heuristic data collection, innate divergence, imperfect representation learning, erroneous decoding, exposure bias, and parametric knowledge bias. A plethora of methods have been proposed to mitigate hallucination. For example, factual-centred metrics [19, 20, 40, 57] and benchmarks [34, 35, 65] have been proposed to measure and reduce hallucination on specific datasets. Retrieval-based methods reinforce LLM by knowledge graphs or databases to help correct factual errors in models' outputs [57, 76]. Prompting the models to reason [69] and verify [13] their answers has also been shown to reduce hallucination.

Up to now, research on LLM hallucination remains largely empirical. Useful as they are, empirical studies cannot answer the fundamental question: can hallucination be completely eliminated? The answer to this question is fundamental as it indicates a possible upper limit of LLMs' abilities. How-
ever, since it is impossible to empirically enumerate and test every possible input, formal discussion on this question is impossible without a clear definition and formal analysis of hallucination.

In the real world, formally defining hallucination, a factual or logical error of LLM, turns out to be extremely difficult. This is because a formal definition of semantics in the real world is still an open problem [12, 58]. Hence in this work, we rigorously define a formal world of computable functions, wherein precise discussions on hallucination is feasible. In this world, hallucination occurs whenever an LLM fails to exactly reproduce the output of a computable function. Under this definition, we present a fundamental result that hallucination is inevitable for any computable LLM, regardless of model architecture, learning algorithms, prompting techniques, or training data. Since this formal world is a part of the real world, the result also applies to LLMs in the real world.

Based upon the theoretical results, we identify some formal problems in which real-world LLMs are likely to hallucinate and empirically verify this identification. The theoretical and empirical results lead to our discussions on their practical implications on proper usage of LLMs.

The contribution of this paper is summarized as follows:

- We formally define and discuss hallucination for LLMs and, by employing results in learning theory [2, 3, 18, 28], we show that hallucination is inevitable for LLMs.
- Empirical study shows that state-of-the-art LLMs are hallucination-prone in some real-world problems, which validates the theoretical results.
- We discuss practical implications of our theoretical results on the design of hallucination mitigators and deployment of LLMs in the real world.


## 2 Related Works

Recent years have seen a growing interest in exploring hallucination in LLMs. This section offers a concise overview of pertinent studies in the field. For an extensive review of hallucination in LLMs, readers are directed to recent surveys [24, 29, 37, 46, 66].

### 2.1 Classification of Hallucination

Hallucination in language models is typically classified either phenomenally or mechanically. The former approach classifies hallucination based on outcomes, while the latter focuses on training and deployment methodologies. This section addresses the phenomenal classification and we will discuss the mechanical classification in Section 2.2

A conventional classification of hallucination is the intrinsic-extrinsic dichotomy [14, 25, 29, 75]. Intrinsic hallucination occurs when LLM outputs contradict with the provided input, such as prompts. On the other hand, extrinsic hallucination occurs when LLM outputs cannot be verified by the information in the input. Huang et al. [24] extends this dichotomy by introducing faithfulness hallucination, which takes user instructions into account and includes instructional, contextual, and logical inconsistencies. Rawte et al. [52] divided hallucination into "factual mirage" and "silver lining", denoting erroneous outputs based on factually correct or incorrect inputs. Each class is further divided into intrinsic and extrinsic subclasses, and further decomposed into cases such as acronym ambiguity and geographic erratum.

### 2.2 Causes of Hallucination

Hallucination has been generally attributed to issues in data, training, and inference stages as identified in existing surveys [24, 29]. Issues in the data include poor quality [31], misinformation [35], bias [41, 47], and being outdated [33, 43] knowledge. Moreover, a fair portion of knowledge in the wild is long-tailed, making it challenging to recall during deployment [30, 38]. During training, the model could suffer from architectural and strategic deficiencies which hinder proper learning. For instance, exposure bias [4] is a well-known problem that results from an inconsistency between training and inference caused by teacher-forced training strategy [50]. Attention mechanism, a key component of transformer-based LLMs, can also contribute to hallucination [36], possibly because attention across tokens is diluted as the sequence length grows [10, 22]. During inference, hallucination can also be
caused by sampling randomness [1, 14], insufficient context attention [55], and softmax bottleneck [9. 73.

### 2.3 Mitigating Hallucination

The mitigation of hallucination involves tackling its underlying causes. For data-related issues, solutions include fact-focused datasets [17, 21] and developing automatic data cleaning techniques [42. 51, 54|. Retrieval augmentation, which uses relevant external documents to ground LLMs, can help reduce knowledge gap and reduce hallucination [57, 76]. Prompting techniques like Chainof-Thought [69] and Tree-of-Thought [74] have been applied to improve knowledge recall and reasoning [67]. To mitigate training-related hallucination, architectural improvements and training objectives are proposed. For example, sharpening softmax functions [36] can tackle the issue of diluted attention. Factuality-enhanced training objectives [32, 56] have been shown to improve the model's ability to associate facts and produce logical outputs. To overcome inference-related hallucination, new decoding methods are introduced to improve factuality or faithfulness of LLMs. Factual-nucleus sampling, proposed by Lee et al. [32], aims to balance the diversity and the factuality in model outputs. Chain-of-Verification, introduced by Dhuliawala et al. [13], prompts the LLM to self-correct their mistakes during generation.

## 3 Definitions

In this section, we detail our definitions of LLMs, hallucination, and training and deployment of LLMs under the hallucination context. While we will define hallucination in a small formal world, we will generalize our definition for LLMs as much as possible. Readers are referred to Section A for a Table of Notation.

### 3.1 Large Language Model

We start with the basic definitions for our alphabet and strings:

Definition 1 (Alphabet and Strings). An alphabet $\mathcal{A}$ is a finite set of $N$ tokens $\mathcal{A}=$ $\left\{a_{0}, a_{1}, \ldots, a_{N-1}\right\}$. A string is a sequence $w_{0} w_{1} \ldots w_{n-1}$ obtained by concatenating tokens for $n$ times, where $w_{i} \in \mathcal{A}$.

Tokens are atomic symbols used to construct strings. For real-world LLMs, token are usually unique combinations of alphanumerical characters. An LLM is a probabilistic model of a string that conditions the output at time $t$ based on all the tokens that come before it in the string. Formally, the likelihood of string $w_{0} w_{1} \ldots w_{n-1}$ is represented as

$$
\begin{equation*}
p\left(w_{0: n-1}\right)=p\left(w_{0}\right) \prod_{t=1}^{n-1} p\left(w_{t} \mid w_{0: t-1}\right), w_{i} \in \mathcal{A}, n \in \mathbb{N} \tag{1}
\end{equation*}
$$

The most common usage of LLM is to complete a partial string $w_{0} w_{1} \ldots w_{q-1}$, or a "prompt", into a complete string $w_{1} w_{2} \ldots w_{n}$, by maximising the likelihood of the complete string:

$$
\begin{equation*}
p\left(w_{0: n-1}\right) \propto p\left(w_{q: n} \mid w_{0: q-1}\right)=\prod_{t=q}^{n-1} p\left(w_{t} \mid w_{0: t-1}\right), w_{i} \in \mathcal{A}, q, n \in \mathbb{N}, q<n \tag{2}
\end{equation*}
$$

As the equation suggests, such completion occurs iteratively, one token at a time. In practice, the iteration stops after a fixed number of iterations, or when a special stopping token is generated.

State-of-the-art LLMs [6, 45, 60, 61] are generally implemented using the Transformer architecture, which is a multi-layer feedforward network equipped with attention mechanism [64]. The "large" in LLM describes both the scale of the models, which is usually billions of parameters, and the size of training corpora, which is usually trillions of tokens [45, 61]. An LLM is trained with the objective that the output string $w_{1: n}$ resembles the real-world corpora and is coherent and reasonable. Common training procedures include (1) unsupervised pretraining where LLMs are trained to complete strings from general corpus auto-regressively, (2) supervised finetuning where the pretrained LLMs are finetuned on training samples for specific tasks, and (3) reinforcement learning where human feedback
is put as training signals. We will discuss the formal definition of training samples in Section 3.2 and a general training procedure in Section 3.3 .

To facilitate more general discussions, we now ignore all the implementation details to build an LLM, e.g., the probabilistic model in Equation (1) and (2), and only focus on its input and output. We put a single assumption on LLMs: an LLM is a function $h$ which, given any finite-length input string $s=w_{0: q-1}$, outputs a string of tokens $h(s)$ that completes $s$ within a finite time. This assumption is true for any LLM proposed so far and is more general than any specific LLM. Formally, we define the large language model as:

Definition 2 (Large Language Model). Let $\mathcal{S}$ be a computable se all the finite-length strings of alphabet $\mathcal{A}$ and $\left(s_{0}, s_{1}, \ldots\right)$ be an one-to-one enumeration of all the elements in $\mathcal{S}$. A large language model, denoted $h$, is a function that completes the input string $s \in \mathcal{S}$ using the function's predicted tokens $h(s)$, in a finite time. Function $h$ is attained procedurally using a set of training samples of input-completion pairs.[^0]

Based on this definition, all possible LLMs are total computable functions [5] on $\mathcal{S}$, which means that all of them produce output for any input $s \in \mathcal{S}$ in finite steps. Furthermore, all real-world LLMs have provable properties, such as (a) they are all total computable by design, or (b) they all complete their computation in polynomial time. To formally describe these properties, we define LLMs with properties provable by some algorithm $P$ :

Definition 3 ( $P$-proved LLMs). Let $P$ be a computable algorithm that takes in a function and returns "true" only if the function has a specific property (e.g. total computable or polynomialtime complexity) of a function. Then a $P$-proved LLM is an LLM in Definition 2 which can be proved by $P$ to have the specific property in finite steps.

Following the definition, $P$-proved LLMs form a proper subset of all LLMs. This classification is important for our further discussion about LLMs' limitations, where we will view LLMs as a subset of total computable functions. Different from general total computable functions, LLMs can be categorised by the desirability of its outputs along a spectrum. On the "nonsensical" side is a clueless token predictor which produces meaningless completion of input strings $s$. On the "ideal" side, a hallucination-free function completes any well-formed input strings into a sensible and factual string. In between are the real-world LLMs: their outputs are comprehensible most of the time; however, they occasionally "hallucinate" and generate nonfactual statements. This spectrum, along with the relation between LLMs and total computable functions, is shown in Figure 1 .

![](https://cdn.mathpix.com/cropped/2024_06_04_b37e1b6c0a0c5e4b8ad8g-04.jpg?height=586&width=1244&top_left_y=1729&top_left_x=430)

Figure 1: Illustration of LLMs and their relation with computable functions. The "ideal" side of spectrum is shown as dashed lines because it is not attainable by any LLMs and therefore does not exist in the set of LLMs, as will be shown in this paper.

### 3.2 A Formal World and Hallucination

Hallucination is in essence an erroneous output produced by an LLM. Without getting tangled in the difficult problem of formalising "correctness" in our real world, we define hallucination in a formal world where all we care about is a computable ground truth function $f$ on $\mathcal{S}$. In other words, in our formal world, $f$ is the ideal function that produces correct completion $f(s)$ for any input string (or prompt, question, query, etc.) $s \in \mathcal{S}$.

Definition 4 (Formal World of $f$ ). A formal world of ground truth function $f$ is a set $\mathcal{G}_{f}=$ $\{(s, f(s)) \mid s \in \mathcal{S}\}$, where $f(s)$ is the only correct completion of input string $s$, for all $s \in \mathcal{S}$.

For example, $f$ could be a function that answers "true" for factual statements and "false" for non-factual ones. It could also be a function that completes a prompt to produce a factual statement. We only assume that such computable $f$ always exists (though its exact implementation might be unknown) in our formal world for all tasks. Otherwise, we can immediately say LLM will hallucinate on some tasks in the formal world because LLM is computable while the ground truth function $f$ is not. We can similarly define $\mathcal{G}_{h}=\{(s, h(s)) \mid s \in \mathcal{S}\}$ to be the world of LLM $h$.

The training samples $\mathcal{T}$ can be defined as a set of input-output pairs we get from the formal world.

```
Definition $\mathbf{5}$ (Training Samples $\mathcal{T}$ ). Training samples $\mathcal{T}$ is a set
$\left\{\left(s_{0}, y_{0}\right),\left(s_{1}, y_{1}\right), \ldots,\left(s_{i}, y_{i}\right), \ldots \mid s_{i} \in \mathcal{S}, i \in \mathbb{N}, y_{i}=f\left(s_{i}\right)\right\}$
```

Set $\mathcal{T}$ is a generalized corpus of how ground-truth $f$ answers/completes input strings. For example, if $f$ answers "true" for factual inputs and "false" otherwise, then our training samples could look like \{ ("A shark is a mammal.", "false"), ("Earth orbits around the Sun.", "true"), ... \}. On the other hand, if $f$ is a function that completes or answers input string $s$, then $\mathcal{T}$ could look like \{ ("Is shark a fish or mammal?", "Fish."), ("What is the sum of binary numbers 10001 and 10110?", "100111."), ... \}. Thanks to this general definition, we do not need to worry about the exact task and corpus on which an LLM is trained. Furthermore, we do not assume anything about the size of $\mathcal{T}$, meaning the training procedure to be introduced in Section 3.3 can use as many samples as needed.

With the ground truth $f$ introduced, it is straightforward to define hallucination in the formal world as when a trained LLM fails to fully reproduce the output of ground truth function $f$. Formally:

Definition 6 (Hallucination). An LLM $h$ is hallucinating with respect to a ground truth function $f$, if $\exists s \in \mathcal{S}$ such that $h(s) \neq f(s)$.

With this definition, hallucination is no longer related to correctness or factualness in the real world. It is simply an inconsistency between the formal world of a ground truth function $\mathcal{G}_{f}$ and that of an $\operatorname{LLM} \mathcal{G}_{h}$. There are three possible relations between $\mathcal{G}_{f}$ and $\mathcal{G}_{h}$ :

- Total hallucination: $\mathcal{G}_{h} \cap \mathcal{G}_{f}=\varnothing$, where the LLM hallucinates on all $s \in \mathcal{S}$.
- Some hallucination: $\mathcal{G}_{h} \cap \mathcal{G}_{f} \neq \varnothing$ and $\mathcal{G}_{h} \neq \mathcal{G}_{f}$, where LLMs hallucinates on some $s \in \mathcal{S}$.
- Hallucination-free: $\mathcal{G}_{h}=\mathcal{G}_{f}$, which is an ideal LLM that is hallucination-free w.r.t. $f$.

Venn diagrams showing the relation between $\mathcal{G}_{h}$ and $\mathcal{G}_{f}$ for the three cases above are shown in Figure 2 .

![](https://cdn.mathpix.com/cropped/2024_06_04_b37e1b6c0a0c5e4b8ad8g-05.jpg?height=258&width=380&top_left_y=2099&top_left_x=466)

(a) Total hallucination.

![](https://cdn.mathpix.com/cropped/2024_06_04_b37e1b6c0a0c5e4b8ad8g-05.jpg?height=260&width=349&top_left_y=2101&top_left_x=888)

(b) Some hallucination.

![](https://cdn.mathpix.com/cropped/2024_06_04_b37e1b6c0a0c5e4b8ad8g-05.jpg?height=263&width=225&top_left_y=2102&top_left_x=1357)

(c) Hallucination-free

Figure 2: Venn diagrams showing possible relations between $\mathcal{G}_{h}$ and $\mathcal{G}_{f}$.

### 3.3 Training an LLM

In the formal world, using our definition of hallucination in Definition 6, the core question of whether hallucination can be eliminated can be translated to:

Definition 7 (The Fundamental Question). For any ground-truth function $f$, using training samples $\mathcal{T}$, can an LLM $h$ be trained such that $\forall s \in \mathcal{S}, h(s)=f(s)$ ?

We are not interested in the training and deployment details like initialisation of parameters, selection of optimisers, learning rates, objective functions, stopping criteria, inference hyperparameters (e.g., temperature, and so on, and wish our discussion to be independent of these factors. Therefore, we lump together all the factors and call it a "training and deploying" procedure. In this procedure, an LLM $h$ is iteratively updated by training samples. We only check the output of the LLM during deployment, when the stopping criteria are met and the training procedure stops.

The workflow of training an LLM $h$ is the following iteration: a (batch of) training sampl ${ }^{1}$ is taken from $\mathcal{T}$, then $h$ is updated by the training procedure using the training sample. This repeats until the training procedure stops. The final updated $h$ will produce an output $h(s)$ to complete any test string s. Formally:

Definition 8 (Training and deploying an LLM). An LLM $h$ is trained and deployed by the following procedure, whose implementation must be computable:

- Inputs:
- A stream ${ }^{a}$ of training samples $\mathcal{T}=\left(\left(s_{0}, f\left(s_{0}\right)\right),\left(s_{1}, f\left(s_{1}\right)\right), \ldots\right)$.
- Output: trained LLM $h^{[i]}$ for some $i \in \mathbb{N}$, which is expected to be equal to $f$.
- Procedure:

1. Let $h^{[0]}$ be the LLM with randomly initialised model parameters.
2. Let $i=0$.
3. Training and Validation Iteration:

(a) If stopping criteria are met (LLM is ready): end iteration, goto step 4

(b) Retrieve a training sample $\left(s_{i}, f\left(s_{i}\right)\right)$ from $\mathcal{T}$.

(c) Update LLM $h^{[i]}$ to $h^{[i+1]}$ according to $\left\{\left(s_{j}, f\left(s_{j}\right)\right) \mid j \leq i\right\}$.

(d) Let $i \leftarrow i+1$, go to step $3 \mathrm{a}$

4. Deployment: Let $h=h^{[i]}$ be the final trained model. End the procedure.

${ }^{a}$ We put training samples in a stream because we assume no limitations on their numbers.

There are a few notes about this procedure:
- In step 3a, traditional stopping criteria include reaching a preset number of training iterations and when loss value stops improving. The procedure above is more permissive because we do not assume any criterion, meaning the training can take arbitrarily many samples and use arbitrarily long time. Nevertheless, it must eventually stop and use a final LLM for deployment. Otherwise, it will loop infinitely and break the computability assumption.
- As a result of this procedure, $h^{[n]}$ is a particular state of LLM $h$ in which it is trained on samples $\left\{\left(s_{i}, f\left(s_{i}\right)\right) \mid i<n\right\}$. In particular, $h^{[0]}$ is an LLM state in which $h$ is initialized but never trained. In practice, we always deploy an LLM in a particular state. Therefore, when there is no ambiguity, both $h$ and $h^{[n]}$ will be referred to as "LLM".
- The LLM can be expected to: (i) answer $f(s)=$ ?, or (ii) given " $(s$," complete it with " $f(s)$ )", or (iii) given "is it true that $f(s)=t$ ?" answer "yes" or "no". For ease of presentation and without loss of generality, we assume case (i), where LLM uses $h(s)$ to answer the question.[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_b37e1b6c0a0c5e4b8ad8g-07.jpg?height=805&width=1322&top_left_y=237&top_left_x=388)

Figure 3: Illustration of relations between all concepts defined in Section 3 (a) shows the real-world corpus, which is a superset of (b) the formal world of ground truth function $f$ and training samples $\mathcal{T}$. (c) shows the procedure that trains LLM $h$ as defined in Definition8, which takes training samples and updates $h$ until the stopping criteria are met. Finally in (d), the trained LLM is deployed and produces output given an unseen string $s$. Hallucination is defined by comparing LLM's answer $h(s)$ with ground truth value $f(s)$.

Up to now, we have defined everything needed for further discussion: LLM, hallucination in the formal world, and a general training procedure which allows us to ignore the learning details. An illustration showing the relations between all these definitions is shown in Figure 3. We would like to highlight that our definition entails not only Transformer-based LLMs, but all the computable LLMs and common learning frameworks. Moreover, LLMs trained by the procedure in Definition 8 are far more powerful and flexible than their counterparts in the real world. Therefore, if hallucination is inevitable for our LLMs in the relatively simple formal world, then hallucination is inevitable for LLMs in the more complicated real world.

## 4 Hallucination is Inevitable for LLMs

In this section, we show that hallucination is inevitable in the formal world by suitably applying results in learning theory literature. Specifically, we will use the diagonalization argument in our proof. The diagonalization argument was originally used to prove that some infinite sets (e.g., the set of real numbers) are larger than others (e.g., the set of natural numbers) by Cantor [8, 15]. The high-level idea is to show that any enumeration of an uncountable set is not exhaustive by constructing a counter example that is in the set but not in the enumeration [70]. The argument gets its name because such counter example is constructed by flipping the diagonal elements, i.e., the $i^{\text {th }}$ component of the $i^{\text {th }}$ element, in the table showing the enumeration.

This section is organized in a restricted-to-general manner, where we first discuss less complex LLMs resembling real-world ones and then generalize to any computable LLMs. First, we show that all LLMs in a (enumerable) set of $P$-proved LLMs will hallucinate on some inputs (Theorem 1). Such provability constraint limits the complexity of LLMs, however, it helps us discuss concrete examples of hallucination-inducing problems. Then, we remove the provability constraint and show that all LLMs in a computably enumerable set will hallucinate on infinite many inputs (Theorem2). Finally, we show that hallucination is inevitable for all computable LLMs (Theorem3). At the end, we will answer the fundamental question posed in Definition 7 .

Table 1: Illustration of diagonalization for the proof of Theorem 1. Each row represents a particular LLM. Each column represents a string $s_{i}$ with which the LLM is tested. A value in the $j^{\text {th }}$ column of $i^{\text {th }}$ row represents $\hat{h}_{i}\left(s_{j}\right)$. Blue cells are the outputs that we use to construct $f$, on which the LLM in the corresponding row will hallucinate.

|  | Test Samples |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLMs | $s_{0}$ | $s_{1}$ | $s_{2}$ | $s_{3}$ | $s_{4}$ | $\cdots$ |
| $\hat{h}_{0}$ | $\hat{h}_{0}\left(s_{0}\right)$ | $\hat{h}_{0}\left(s_{1}\right)$ | $\hat{h}_{0}\left(s_{2}\right)$ | $\hat{h}_{0}\left(s_{3}\right)$ | $\hat{h}_{0}\left(s_{4}\right)$ | $\cdots$ |
| $\hat{h}_{1}$ | $\hat{h}_{1}\left(s_{0}\right)$ | $\hat{h}_{1}\left(s_{1}\right)$ | $\hat{h}_{1}\left(s_{2}\right)$ | $\hat{h}_{1}\left(s_{3}\right)$ | $\hat{h}_{1}\left(s_{4}\right)$ | $\cdots$ |
| $\hat{h}_{2}$ | $\hat{h}_{2}\left(s_{0}\right)$ | $\hat{h}_{2}\left(s_{1}\right)$ | $\hat{h}_{2}\left(s_{2}\right)$ | $\hat{h}_{2}\left(s_{3}\right)$ | $\hat{h}_{2}\left(s_{4}\right)$ | $\cdots$ |
| $\hat{h}_{3}$ | $\hat{h}_{3}\left(s_{0}\right)$ | $\hat{h}_{3}\left(s_{1}\right)$ | $\hat{h}_{3}\left(s_{2}\right)$ | $\hat{h}_{3}\left(s_{3}\right)$ | $\hat{h}_{3}\left(s_{4}\right)$ | $\cdots$ |
| $\hat{h}_{4}$ | $\hat{h}_{4}\left(s_{0}\right)$ | $\hat{h}_{4}\left(s_{1}\right)$ | $\hat{h}_{4}\left(s_{2}\right)$ | $\hat{h}_{4}\left(s_{3}\right)$ | $\hat{h}_{4}\left(s_{4}\right)$ | $\cdots$ |
| $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ |
| $f$ | $\Delta\left(\hat{h}_{0}\left(s_{0}\right)\right)$ | $\Delta\left(\hat{h}_{1}\left(s_{1}\right)\right)$ | $\Delta\left(\hat{h}_{2}\left(s_{2}\right)\right)$ | $\Delta\left(\hat{h}_{3}\left(s_{3}\right)\right)$ | $\Delta\left(\hat{h}_{4}\left(s_{4}\right)\right)$ | $\cdots$ |

### 4.1 Computably Enumerable LLMs will Hallucinate

We first use the classic diagonalization argument to show that LLMs will inevitably hallucinate, with an assumption that LLMs are not only total computable, but are $P$-proved to be total computable (see Definition 3). As we will discuss later in this section, $P$ can be used to restrict the complexity of LLMs, leading to useful corollaries about their limitations for real-world problems.

The $P$-provability assumption is important because we cannot enumerate ${ }^{2}$ the set of total computable functions ${ }^{3}$ and thus cannot apply the classic diagonalization argument (which requires a table of enumeration). However, we can enumerate the set of $P$-proved total computable functions using $P$ as the enumeration algorithm. The provability assumption is true for LLMs because, by design, all LLMs (so far) will output an answer in finite time for any finite input string.

With this assumption, we use the diagonalization argument to prove the following theorem:

Theorem 1. For the set of LLMs $\left\{h_{0}, h_{1}, \ldots \mid h_{i}\right.$ proved by $P$ to be total computable. $\}$, there exists a computable ground truth function $f$, such that all $h_{i}^{[j]}, i, j \in \mathbb{N}$, will hallucinate.

Proof. Consider the set of LLMs: $\left\{h_{0}, h_{1}, \ldots \mid h_{i}\right.$ proved by $P$ to be total computable. $\}$. An LLM $h_{i}$ in this set can be trained on no (zero) training sample, one training sample, two training samples, and so on, resulting in its different states $\left\{h_{i}^{[0]}, h_{i}^{[1]}, \ldots, h_{i}^{[j]}, \ldots\right\}, i, j \in \mathbb{N}$. To facilitate the diagonalization argument, we want to use one index variable for all the states of LLMs. Therefore, using the Cantor pairing function [7], we re-enumerate these LLM states as $\left\{\hat{h}_{0}, \hat{h}_{1}, \ldots, \hat{h}_{k}, \ldots\right\}$, where $h_{i}^{[j]}$ is re-enumerated as $\hat{h}_{k}$ with $k=(i+j)(i+j+1) / 2+j, \forall i, j \in \mathbb{N}$.

In order to demonstrate the contradiction, let us assume that at least one of the LLM states is hallucination-free. This assumption is necessary because if there is no hallucination-free LLM, the theorem is trivially proved. Feeding a string $s$ to LLMs in all the enumerated states, we get their answers as $\left\{\hat{h}_{0}(s), \hat{h}_{1}(s), \ldots\right\}$. Repeat this for all $s \in \mathcal{S}$, we get a table as in Table 1 , which contains all the outputs of all the LLMs. In other words, if a ground truth function $f$ is not in enumeration $\left\{\hat{h}_{0}, \hat{h}_{1}, \ldots\right\}$, then no LLM can learn $f$ (and w.r.t. which all LLMs will hallucinate).

Now we define the ground truth function $f$ so that $f\left(s_{i}\right)$ contradicts the elements along the diagonal of the table (see blue cells in Table 1 :

$$
\begin{equation*}
f\left(s_{i}\right)=\Delta\left(\hat{h}_{i}\left(s_{i}\right)\right), \forall i \in \mathbb{N} \tag{3}
\end{equation*}
$$

where $\Delta$ is a computable function that returns a different string from its input. For example, we could define $\Delta\left(s_{k}\right)=s_{k+1}$, which returns the next string of $s_{k}$ in $\mathcal{S}$. Therefore $\Delta\left(\hat{h}_{i}\left(s_{i}\right)\right) \neq \hat{h}_{i}\left(s_{i}\right)$.[^2]

By the above construction and according to Definition 6, $\hat{h}_{0}$ hallucinates w.r.t. $f$ as $f\left(s_{0}\right) \neq \hat{h}_{0}\left(s_{0}\right)$, so for $\hat{h}_{1}$ as $f\left(s_{1}\right) \neq \hat{h}_{1}\left(s_{1}\right)$, and so for $\hat{h}_{2}$ as $f\left(s_{2}\right) \neq \hat{h}_{2}\left(s_{2}\right)$. Repeating this reasoning for all $s_{i} \in \mathcal{S}$, we see that all LLMs hallucinate w.r.t. $f$ because $f\left(s_{i}\right) \neq \hat{h}_{i}\left(s_{i}\right)$ for all $i \in \mathbb{N}$. This contradicts with our assumption of the existence of at least one hallucination-free trained LLM and thus proves the theorem.

The theorem and its proof indicate that if $f$ is not listed in the enumeration table, then it is not learnable by any LLM in the table, and therefore all LLMs in the table will hallucinate on $f$.

LLMs will Hallucinate on What they Cannot Compute Since $P$ is a prover and an enumeration algorithm for LLMs, we can further examine $P$ to see what problems are hallucination-inducing for LLMs. Up till now, all LLMs produce output not only in finite time, but also in polynomial time $O(\Pi(m))$, where $\Pi(m)$ is a polynomial of $m$, the length of input questions. This means that if $f$ cannot be computed within $O(\Pi(m))$ time, then $P$-proved $O(\Pi(m))$ LLMs will inevitably hallucinate w.r.t $f$. Therefore, we can list some hallucination-prone tasks for these LLMs:

Corollary 1. Let $m$ be the length of LLM's input and $\Pi(m)$ be a polynomial of $m$. If LLM $h$ is $P$-proved to finish in at most $O(\Pi(m))$ steps for any input, then $h$ will inevitably hallucinate w.r.t. $f$, if $f$ is the ground truth function for the following problems:

Problem 1 Combinatorial List: $f$ lists all the strings with length $m$ using an alphabet of two characters. Computing $f$ takes $O\left(2^{m}\right)$ time.

Problem 2 Presburger arithmetic [49, 59]: given a statement in this axiom system, $f$ returns "yes" if the statement can be proved within the system and "no" otherwise ${ }^{a}$ Computing $f$ takes $O\left(2^{2^{\Pi(m)}}\right)$ time $\left.\mid 16\right]$.

Assuming $P \neq N P, O(\Pi(m))$ LLMs will hallucinate on the NP-complete problems such as:

Problem 3 Subset Sum: given a set of $m$ integers and a number $q, f$ returns "yes" when there is a subset that sums up to $q$ and "no" otherwise.

Problem 4 Boolean Satisfiability (SAT) [11]: given a formula of $m$ Boolean variables, $f$ returns "yes" if there exists an assignment on these variables which results in the formula to be true and "no" otherwise.

${ }^{a}$ Presburger arithmetic is the first-order theory of natural numbers with addition and order $<$. It is consistent, complete, and decidable. Therefore, there exists a computable $f$ for this problem.

As a result, real-world LLMs' answers about mathematical problems should always be subjected to proper scrutiny. The result is supported by a recent study [23]. In Section 5], we provide an empirical study to illustrate Problem 1 listed above.

Table 2: Illustration of proof for Theorem 2. Each row represents a particular LLM. Each column represents a string $s_{i}$ where the LLM is tested on. A value in the $j^{\text {th }}$ column of $i^{\text {th }}$ row represents $h_{i}\left(s_{j}\right)$. Blue cells are the outputs that we use to construct $f$, on which the LLM in the corresponding row will hallucinate.

| LLMs | Test Samples |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  | $s_{0}$ | $s_{1}$ | $s_{2}$ | $s_{3}$ | $\cdots$ |
| $\hat{h}_{0}$ | $\hat{h}_{0}\left(s_{0}\right)$ | $\hat{h}_{0}\left(s_{1}\right)$ | $\hat{h}_{0}\left(s_{2}\right)$ | $\hat{h}_{0}\left(s_{3}\right)$ | $\cdots$ |
| $\hat{h}_{1}$ | $\hat{h}_{1}\left(s_{0}\right)$ | $\hat{h}_{1}\left(s_{2}\right)$ | $\hat{h}_{1}\left(s_{3}\right)$ | $\cdots$ |  |
| $\hat{h}_{2}$ | $\hat{h}_{2}\left(s_{0}\right)$ | $\hat{h}_{2}\left(s_{2}\right)$ | $\hat{h}_{2}\left(s_{3}\right)$ | $\cdots$ |  |
| $\hat{h}_{3}$ | $\hat{h}_{3}\left(s_{0}\right)$ | $\hat{h}_{3}\left(s_{1}\right)$ | $\hat{h}_{3}\left(s_{2}\right)$ | $\hat{h}_{3}\left(s_{3}\right)$ | $\cdots$ |
| $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ |
| $f$ | $\Delta\left(\left\{\hat{h}_{0}\left(s_{0}\right)\right\}\right)$ | $\Delta\left(\left\{\hat{h}_{j}\left(s_{1}\right) \mid j \leq 1\right\}\right)$ | $\Delta\left(\left\{\hat{h}_{j}\left(s_{2}\right) \mid j \leq 2\right\}\right)$ | $\Delta\left(\left\{\hat{h}_{j}\left(s_{3}\right) \mid j \leq 3\right\}\right)$ | $\cdots$ |

LLMs will Hallucinate on Infinitely Many Questions One might argue that $P$-provability has limited the complexity of LLMs. Moreover, Theorem 1 only shows that LLM $h$ will hallucinate
on one input but not all inputs. Therefore, hallucination in the formal world could be negligible. However, we can remove the provability constraint and further show the following theorem:

Theorem 2. For all computably enumerable sets of LLMs $\left\{h_{0}, h_{1}, \ldots\right\}$, there exists a computable ground truth function $f$, such that all $h_{i}^{[j]}, i, j \in \mathbb{N}$, hallucinate on infinitely many inputs.

Proof. Consider the following computably enumerable set of LLMs: $\left\{h_{0}, h_{1}, \ldots\right\}$. Note that in this theorem we have dropped prover $P$, which means this enumeration of LLMs is more general than what is used in the proof of Theorem 1. Similar to the proof of Theorem 1, we enumerate all the states of these LLMs as $\left\{\hat{h}_{k} \mid k \in \mathbb{N}\right\}$ using the Cantor pairing function. Feeding a string $s$ to these LLMs, we get their answers as $\left\{\hat{h}_{0}(s), \hat{h}_{1}(s), \ldots\right\}$.

Define a computable function $f$ as follows:

$$
f\left(s_{i}\right)=\Delta\left(\left\{\hat{h}_{j}\left(s_{i}\right) \mid j \leq i\right\}\right), \forall i \in \mathbb{N}
$$

where $\Delta\left(\left\{\hat{h}_{j}\left(s_{i}\right) \mid j \leq i\right\}\right)$ returns a string that is different from all the strings in set $\left\{\hat{h}_{j}\left(s_{i}\right) \mid j \leq i\right\}$.

By the above construction, $\forall i \geq j, f\left(s_{i}\right) \neq \hat{h}_{j}\left(s_{i}\right)$ because the value of $f\left(s_{i}\right)$ is explicitly defined as a string different from $\hat{h}_{j}\left(s_{i}\right), i \geq j$ (see Table 2). Therefore, $\hat{h}_{0}$ will hallucinate on $s_{0}, s_{1}, \ldots ; \hat{h}_{1}$ will hallucinate on $s_{1}, s_{2}, \ldots$; and $\hat{h}_{2}$ will hallucinate on $s_{2}, s_{3}, \ldots$, and so on. In general, LLM $\hat{h}_{k}$ will hallucinate on all input strings after $s_{k-1}$ in the one-to-one enumeration $\left(s_{0}, s_{1}, \ldots\right)$ of $\mathcal{S}$.

Note that $P$-proved total computable LLMs form a computably enumerable set. Therefore, Theorem 2 is a generalisation of Theorem 1 .

### 4.2 Any Computable LLM will Hallucinate

We have shown that all LLMs in a computably enumerable set will inevitably hallucinate. In this section, we go one more step towards a direct answer to the fundamental question by considering LLMs as general total computable functions. The ground truth function $f$ will still be assumed to be any total computable function.

This step can be achieved by using Theorem 2 Any individual computable LLM $h$ forms a set $\{h\}$, which only contains $h$ itself and is thus computably enumerable. Therefore, applying Theorem 2 on this set, we know that there exists a ground truth function $f$ such that this particular $h$ will hallucinate on infinitely many inputs. This can be done for any total computable LLM. Therefore, we get the following theorem similar to [18]:

Theorem 3. For all computable LLMs $h$, there exists a computable ground truth function $\sqrt{a} f$ such that each of $h^{[j]}, j \in \mathbb{N}$ hallucinates on infinitely many inputs.

${ }^{a}$ Note that in this theorem $f$ differs for every individual $h$, whereas $f$ in Theorem 1 and Theorem 2 is constructed for all LLMs in the set being considered.

To give a specific example, in Section B in the appendix we present a special case where $f$ represents a computable linear order, which is an abstraction of many real-world relations such as ranking of movies or songs, chronological ordering of events, alphabetical sorting of words, and so on.

As a result, an LLM will inevitably hallucinate in our formal world of computable functions. Note that we have only required $h$ to be computable. Therefore, the conclusion is generally true independent of model architecture, learning algorithms, training data, and other implementation details. Furthermore, such inevitability indicates that it is impossible to use an LLM to eliminate its hallucination:

Corollary 2. All computable LLMs cannot prevent themselves from hallucinating.

If there exists an LLM $h^{\prime}$ which can prevent itself from hallucination, then $h^{\prime}$ is hallucination-free on any computable $f$, which contradicts Theorem 3. This corollary is an obvious result of Theorem 3, However, it explicitly suggests that methods relying on LLMs themselves to mitigate hallucination, such as prompt-based chain of thoughts [69], cannot eliminate hallucination.

Answer to the Fundamental Question This section has presented theoretical results about hallucination in our formal world:

- Hallucination is inevitable for $P$-proved total computable LLMs (Theorem 1). Furthermore, polynomial-time LLMs will hallucinate when solving many real-world problems, including arithmetics (Corollary 1 ).
- Hallucination is inevitable for LLMs in any computably enumerable set (including the set of $P$-proved total computable LLMs). All these LLMs could hallucinate on infinitely many inputs (Theorem 2 .
- Hallucination is inevitable for any total computable LLMs (Theorem3).

Since our formal world is a part of the real world, and total computable LLMs are at least as powerful as real-world LLMs, we now conclude with a negative answer to the fundamental question: hallucination of LLMs cannot be eliminated in either our formal world, or in the real world. This answer is independent of (1) LLMs' architecture and complexity, (2) procedure used to train the LLM, (3) the prompts and questions, (4) the number of training samples, and (5) inference hyperparameters.

## 5 Empirical Study: Can LLMs List Them All?

Consider a task intellectually simple for human beings: list all strings of a fixed length using a fixed alphabet. Corollary 1 predicted that polynomial-time LLMs will hallucinate on this task. In this section, we empirically validate this prediction. In Section B.2 we provide another empirical study about LLMs and linear orders.

The Task An LLM is required to list all the strings with length $m$ using an alphabet $\mathcal{A}$. We denote this task as $L(m, \mathcal{A})$. For example, a solution to $L(2,\{\mathrm{a}, \mathrm{b}\})$ is "bb, ba, ab, bb". For each $L(m, \mathcal{A})$, we run the LLM three times using three random seeds. The LLM is deemed successful in solving $L(m, \mathcal{A})$ if, in any of the three runs, the LLM's output contains all and only strings of length $m$ using alphabet $\mathcal{A}$. Note that duplicated entries are allowed in the output, for example, " aa, ab, $\mathrm{ba}, \mathrm{bb}, \mathrm{ab}$ " is a valid solution to $L(2,\{\mathrm{a}, \mathrm{b}\})$.

Models In the experiment we study two state-of-the-art LLM families, namely Llama 2 [61] and Generative Pretrained Transformers (GPT) [6, 45]. Models in both families have context windows of at least 4096 tokens. For Llama 2, we use its 70-billion-parameter version llama2-70b-chat-hf publicly accessible on HuggingFace [27]. For GPT, we use the 175-billionparameter gpt-3.5-turbo-16k for GPT-3.5, and gpt-4-0613 for the GPT-4 family with more parameters (GPT-4's parameter number is not disclosed in the report [45]).

Prompt We use the following base prompt adapted from [26], which is used to prompt an LLM towards completion a dialogue made of questions and answers. Modifications have been made to prompt the LLMs to answer directly instead of providing high-level descriptions or code snippets:

Base Prompt

```
You are a helpful, respectful and honest assistant. Always answer as helpfully
as possible, while being safe. Your answers should not include any harmful,
unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure
that your responses are socially unbiased and positive in nature.
If you don't know the answer to a question, please don't share false
information. However, if you know the answer, you should always share it in
every detail and as requested. Always answer directly. Do not respond with a
script or any approximation.
```

Then, we describe the task to the LLM, for example, $L(2,\{\mathrm{a}, \mathrm{b}\})$ is described as:

Task Description for $L(2,\{\mathrm{a}, \mathrm{b}\})$

Table 3: Evaluation results of LLMs on $L(m,\{\mathrm{a}, \mathrm{b}\})$ and $L(m,\{\mathrm{a}, \mathrm{b}, \mathrm{c}\})$ tasks. A check mark $\checkmark$ indicates that the LLM successfully solved the problem and a cross mark $X$ indicates the opposite. The exact parameter number for GPT-4 is not disclosed.

| LLM | \# Param | ![](https://cdn.mathpix.com/cropped/2024_06_04_b37e1b6c0a0c5e4b8ad8g-12.jpg?height=121&width=133&top_left_y=394&top_left_x=882) | $L(m,\{\mathrm{a}, \mathrm{b}\})$ |  |  |  |  |  |  | $L(m,\{\mathrm{a}, \mathrm{b}, \mathrm{c}\})$ |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 1 | 2 | 3 | 4 | 5 |
| llama2-70B-chat-hf | $7.00 \times 10^{10}$ | 4,096 | $\checkmark$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $x$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ |
| gpt-3.5-turbo-16k | $1.75 \times 10^{11}$ | 16,385 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ | $x$ |
| gpt-4-0613 | $\geq 1.75 \times 10^{11}$ | 8,192 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ |

List ALL the strings with a length of two characters and only contains characters "a" and "b". Do not miss a single string.

Result We feed the base prompt and the task descriptions for $L(m,\{\mathbf{a}, \mathbf{b}\})$ and $L(m,\{\mathbf{a}, \mathbf{b}, \mathbf{c}\})$ with $m \in \mathbb{N}$ to the LLMs. The evaluation result is shown in Table 3. Consistent with the theoretical result in Corollary 1. all LLMs failed eventually as $m$ grows. Surprisingly, LLMs fail for tasks with short answer, compared to the length of their context window. For example, the answer of $L(7,\{\mathrm{a}, \mathrm{b}\})$ is a list of $2^{7}=128$ strings, each with 7 characters. This results in an 896 -character-long answer, which is much shorter than the 4096-token context window. Furthermore, the number of parameters and context size does not significantly impact LLMs' performance on this task: they are equally poor.

## 6 Discussion

### 6.1 Existing and Possible Hallucination Mitigators

Existing hallucination mitigators are generally developed using two principles: (a) improve the capacity of LLMs, and (b) provide the LLM with more information about the real world ${ }^{4}$ ground truth function $f$, by either using training samples or inductive biases. In this section, we briefly discuss where these methods and some other possible mitigators are located in our analysis framework, and their limitations.

Larger models and more training data LLMs are believed to show "emergent abilities" [68] which are absent in smaller language models because of the significantly increased number of parameters and training samples. Therefore, it is natural to think that hallucination will disappear as LLMs grow larger. In the context of this work, increasing model parameters boosts the complexity of LLMs, enabling it to capture more complex ground truth functions. Expanding the size of $\mathcal{T}$ helps rule out invalid LLM candidates and help the training converge better. As the Probably Approximately Correct (PAC) learning framework [63] indicates, increasing training data will result in diminishing generalisation errors if training time is sufficient, provided that the task is learnable by LLMs. However, increasing parameters and data is futile if the ground truth function $f$ cannot be captured by LLMs at all as suggested by Theorem 1 and Theorem 3, For example, adding more multi-head attention layers to a polynomial-time LLM will only result in a larger polynomial-time LLM and will only reduce and possibly eliminate hallucination w.r.t. polynomial-time ground truth functions. It will not eliminate hallucination w.r.t. an exponential-time ground truth function, no matter how many layers are added or how much additional training data are provided.

Prompting LLMs with Chain of Thoughts/Reflections/Verification This appoach belongs to the larger family of in-context learning, which provides example solutions or relevant knowledge about a target task in the prompt [13, 69, 74]. Essentially, it tries to introduce the inductive bias that $f$ generates answer in a procedural manner. Complex problems often have multiple solutions with varying degrees of complexity. Consider the computation of the Fibonacci sequence: a recursive approach leads to exponential time complexity, whereas applying dynamic programming significantly[^3]reduces this to linear time in the input number. Prompting is effective in mitigating hallucination by guiding them towards solutions more preferred by humans, which are possibly the ones with lower complexities, and within LLMs' capabilities. However, this approach will only work for specific tasks. It is unlikely that all ground-truth functions $f$ can be fully described by prompts. Moreover, as suggested by Corollary 2 , it is impossible to eliminate hallucination for all tasks by simply changing the prompts and hope LLM can automatically prevent itself from hallucinating.

Ensemble of LLMs This approach uses multiple instances of LLMs to solve a single problem. The solution is usually produced through majority votes or consensus after a dialogue among the LLMs with natural language capabilities. It injects inductive bias that a prompt-completion string cross-validated by different hypotheses of the world is more likely a true element of the world. An ensemble of LLMs has a higher complexity than each member LLMs and therefore the ensemble could be more capable and less hallucination-prone than individual LLMs. However, an ensemble is essentially a single LLM viewed under Definition 2, which means it is bounded by Theorem 3 and cannot eliminate hallucination.

Guardrails and Fences The guardrails are principles that align LLMs' output with human values, ethics, and legal requirements. The fences is a list of critical tasks that should never be fully automated using LLMs. Both of them serve as safety constraints to prevent LLMs (and other AI models) from generating undesirable outcomes. Guardrails and fences can be formally programmed [53] to explicitly affect LLMs' behaviours. Therefore, it is potentially a useful hallucination mitigator for the formal world and some real-world problems. However, its scalability in the real world remains an open problem.

LLMs Enhanced by Knowledge This approach uses external knowledge sources (e.g., knowledge graphs and databases) and symbolic reasoning methods (e.g., logics) to aid LLMs both in training and inference. Popular chatbots driven by LLMs, such as ChatGPT, has started utilising tools such as search engine, code intepreter, and calculators to solve complex problems beyond their innate capabilities [44]. Similar to programmable guardrails and fences, it explicitly controls the LLM workflow by changing how information is recalled (through retrieval from knowledge database [39. 48]) or used (through logic inference, which has been done for deep networks as in [71, 72]). In this way, LLMs receive extra information about the ground truth function $f$ other than via training data and is therefore not bounded by Theorem 3. This is potentially an effective mitigator of hallucination in the formal world. However, whether it works scalably in real world tasks is an open problem.

### 6.2 Practical Implications

So far we have extensively described the limitation of LLMs. We believe these discussions have the following implications for the AI practitioners.

All LLMs will hallucinate. In Section 4.1 we have given concrete examples of scenarios where polynomial-time LLMs are hallucination-prone. Furthermore, such eventuality is guaranteed by Theorem 3 for general computable ground truth functions and for any computable LLMs. An important, but not the only, reason for hallucination is that the problem is beyond LLMs' computation capabilities. For those problems, any answer except "I don't know" is unreliable and suggests that LLMs have added premises implicitly during the generation process. It could potentially reinforce stereotypical opinions and prejudices towards under-represented groups and ideas. Furthermore, it can be dangerous if unexpected premises were added and resulted in unethical, disturbing, or destructive contents.

Without guardrail and fences, LLMs cannot be used for critical decision making. This is a direct corollary of the conclusion above. The purpose of guardrail and fences is to ensure that the LLMs operate within expectation and never deviate into harmful realms. They are absolutely necessary since all LLMs will hallucinate.

Without human control, LLMs cannot be used automatically in any safety-critical decision-making. For example, no LLM should make decisions about human life, even if its decision is aided by guardrail and fences. Any errors due to hallucination in these cases are very likely to be unacceptable. Humans' decision making role is paramount because human values are much more complex than validity. Making safety-critical decisions usually requires rational and humane (understanding,
empathy, and ethical) judgements that are otherwise very difficult, if not impossible, to compute by any algorithm.

Research on LLMs' safety boundaries is crucial in ensuring sustainable development of LLMs. Misuse of LLMs by human users is damaging in many ways and raised wide concern among researchers, users, and regulators. For example, LLMs have been used to mine private information and cook up misinformation and hate speech. On the other hand, usage of LLMs for automated sensing and actuation, such as in a robotics setting, could result in dangerous real-world outcomes if hallucination occurs. It is therefore important for theorists and practitioners to reach consensus on the ability boundaries for LLMs. Towards this, we attempt to preliminarily identify an "upper bound" of LLMs' capabilities in Section C in the appendix. There should also be appropriate regulations to control their use and prevent intentional and unintentional misuse that poses real risk to human society.

### 6.3 In Defence of LLMs and Hallucination

Although we have shown that LLMs will inevitably hallucinate, this does not undermine their tremendous value in enhancing productivity. Moreover, hallucination itself should not be viewed entirely negatively. In this section, we would like to defend the usage of LLMs and their hallucinatory inclinations.

First, the decision to use an LLM is fundamentally a trade-off between precision and efficiency, which is largely determined by their application. LLMs excel in processing and structuring information at scales and speeds unachievable by humans, facilitating rapid decision-making and idea generation. In scenarios where speed and volume of information processing are overwhelming, the occasional inaccuracies of LLMs are acceptable compromises. Conversely, in situations where precision is critical, the outputs of LLMs can (and must) be verified and supplemented with human supervision. It is notable that while LLMs cannot learn all computable ground truth functions $f$, it can learn some $f$ (see Section C) and can be useful therein. The key is not to view LLMs as infallible sources of truth but as powerful assistive tools for information retrieval, analysis, summarisation, and presentation.

Moreover, it is crucial to recognize that LLMs are continuously evolving. Such evolution includes advancements in model architecture, data processing techniques, training methodologies, and error correction strategies. Over time, we anticipate that the nature of hallucination will be better understood by human researchers and users. While it is impossible to completely eliminate hallucination, we are optimistic that its severity may be controlled and reduced for many applications.

Finally, hallucination is not completely detrimental. In art, literature, and design, the unintentional and unpredictable outputs from LLMs could inspire human creators. Such deviation from facts can lead to unique perspectives that a strictly factual and precise system might never generate. In this sense, the hallucinatory aspect of LLMs should be regarded positively as a source of inspiration, innovation, and creativity.

### 6.4 Open Problems

Finally, we list some challenging and fundamental open problems stemming from our discussion:

- What real-world problems can be provably addressed by LLMs without hallucination? The problem can be further divided into the following problems:
- What is the theoretical computational capability of an LLM?
- Are real-world problems always computable? If so, what are the complexities of real-world problems?
- If LLMs are theoretically capable of solving a real-world problem, how can LLMs be trained to achieve their theoretical capabilities so that hallucination can be eliminated in that limited realm?
- Corollary 1 highlights the relation between computational complexity and hallucination. What tasks are intrinsically more difficult than others? How to design benchmarks to evaluate LLMs on tasks with different computational complexities?
- For problems which are provably within LLMs' capabilities, is it more preferable to use LLMs or use programs coded specific for the problems? Should we train an LLM that does everything or an LLM that can use proper tools for specific problems?
- How can hallucination be detected and corrected for any real-world problems using external knowledge sources and reasoning tools? Furthermore, can LLMs always find the appropriate external tools for solving real-world problems?
- How can the risk of hallucination be quantified for real-world problems? Quantifying hallucination risk is an important step to identity on which problems can LLMs be safely deployed, even with inevitable hallucination.


## 7 Conclusion

In this paper, we study the fundamental problem of eliminating hallucinations in LLMs. To do so, we define a formal world where hallucination in LLMs can be clearly defined and discussed. Specifically, hallucination is defined as inconsistencies between computable LLMs and a computable ground truth function. By utilizing results in learning theory, we show that hallucination is inevitable for computable LLMs if the ground truth function is any computable function. Since the formal world is a part of the real world, we further conclude that it is impossible to eliminate hallucination in the real world LLMs. Using the formal world framework, we discuss the possible mechanisms and effectiveness of existing hallucination mitigators and discuss practical implications that our theoretical results have on the deployment of LLMs in the real world. We emphasize that since hallucination is inevitable, rigorous study of the safety of LLMs is critical.

## References

[1] Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yunhsuan Sung. Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. 2023. arXiv: 2302.05578 [cs.CL]

[2] J. Bārzdinš and R. Freivald. "On the prediction of General Recursive Functions". In: Soviet Mathematics Doklady, (Dokl. Akad. Nauk SSSR) 13 (1972), pp. 1224-1228.

[3] Nikolay Bazhenov, Ekaterina B. Fokina, and Luca San Mauro. "Learning families of algebraic structures from informant". In: Information and Computation 275 (2020), p. 104590.

[4] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks". In: Advances in Neural Information Processing Systems. Vol. 28. 2015.

[5] Paul Bernays. "Alonzo Church. An unsolvable problem of elementary number theory. American journal of mathematics, vol. 58 (1936), pp. 345-363." In: The Journal of Symbolic Logic 1.2 (1936), pp. 73-74.

[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. 2020. arXiv: 2005.14165 [cs.CL]

[7] Georg Cantor. "Ein beitrag zur mannigfaltigkeitslehre". In: Journal für die reine und angewandte Mathematik (Crelles Journal) 1878.84 (1878), pp. 242-258.

[8] Georg Cantor. "Ueber eine elementare Frage der Mannigfaltigketislehre." In: Jahresbericht der Deutschen Mathematiker-Vereinigung 1 (1890/91), pp. 72-78. URL: http://eudml.org/ doc $/ 144383$

[9] Haw-Shiuan Chang and Andrew McCallum. "Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions". In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). May 2022, pp. 8048-8073.

[10] David Chiang and Peter Cholak. "Overcoming a Theoretical Limitation of Self-Attention". In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). May 2022, pp. 7654-7664.

[11] Stephen A. Cook. "The Complexity of Theorem-Proving Procedures". In: Proceedings of the Third Annual ACM Symposium on Theory of Computing. 1971, pp. 151-158.

[12] Kees van Deemter. The Pitfalls of Defining Hallucination. 2024. arXiv: 2401.07897 [cs.CL]

[13] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-Verification Reduces Hallucination in Large Language Models. 2023. arXiv: 2309.11495 [cs.CL].

[14] Nouha Dziri, Andrea Madotto, Osmar Zaiane, and Avishek Joey Bose. "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding". In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Nov. 2021, pp. 21972214.

[15] William Ewald. From Kant to Hilbert: A Source Book in the Foundations of Mathematics. Oxford University Press, Apr. 2005. ISBN: 9780198505358.

[16] Michael J Fischer and Michael O Rabin. "Super-exponential complexity of Presburger arithmetic". In: Quantifier Elimination and Cylindrical Algebraic Decomposition. Springer, 1998, pp. 122-135.

[17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020. arXiv: 2101.00027 [cs.CL].

[18] E. Mark Gold. "Language Identification in the Limit". In: Information and Control 10.5 (1967), pp. 447-474.

[19] Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. "Assessing The Factual Accuracy of Generated Text". In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining. 2019, pp. 166-175.

[20] Nuno M. Guerreiro, Elena Voita, and André Martins. "Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation". In: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. May 2023, pp. 1059-1075.

[21] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks Are All You Need. 2023. arXiv: 2306 11644 [cs.CL].

[22] Michael Hahn. "Theoretical Limitations of Self-Attention in Neural Sequence Models". In: Transactions of the Association for Computational Linguistics 8 (2020), pp. 156-171.

[23] Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, and Soujanya Poria. Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating LLMs, Mathematical Competency through Ontology-guided Perturbations. 2024. arXiv: 2401.09395 [cs.CL]

[24] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. 2023. arXiv: 2311.05232 [cs.CL]

[25] Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. 2023. arXiv: 2104.14839 [cs. CL]

[26] Face Hugging. Llama 2 is here - get it on Hugging Face. Accessed: 2023-12-07. URL: https: //web . archive . org/web/20231207183448/https : / / huggingface . co/blog/ llama2

[27] Face Hugging. Llama-2-70b Chat Model on HuggingFace. Accessed: 2023-12-15. URL: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf.

[28] Sanjay Jain, Daniel N. Osherson, James S. Royer, and Arun Sharma. Systems That Learn: An Introduction to Learning Theory. The MIT Press, Feb. 1999. ISBN: 9780262276252.

[29] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. "Survey of Hallucination in Natural Language Generation". In: ACM Computing Surveys 55.12 (Mar. 2023).

[30] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. "Large Language Models Struggle to Learn Long-Tail Knowledge". In: Proceedings of the 40th International Conference on Machine Learning. Vol. 202. July 2023, pp. 15696-15707.

[31] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. "Deduplicating Training Data Makes Language Models Better". In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). May 2022, pp. 8424-8445.

[32] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. "Factuality Enhanced Language Models for Open-Ended Text Generation". In: Advances in Neural Information Processing Systems. Vol. 35. 2022, pp. 34586-34599.

[33] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. "Large Language Models with Controllable Working Memory". In: Findings of the Association for Computational Linguistics: ACL 2023. July 2023, pp. 17741793 .

[34] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. 2023. arXiv: 2305.11747 [cs.CL].

[35] Stephanie Lin, Jacob Hilton, and Owain Evans. "TruthfulQA: Measuring How Models Mimic Human Falsehoods". In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). May 2022, pp. 3214-3252.

[36] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing Attention Glitches with Flip-Flop Language Modeling. 2023. arXiv: 2306.00946 [cs.LG]

[37] Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, and Gregory Dudek. Hallucination Detection and Hallucination Mitigation: An Investigation. 2024. arXiv: 2401.08358 [cs.CL].

[38] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories". In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). July 2023, pp. 9802-9822.

[39] Ariana Martino, Michael Iannelli, and Coleen Truong. "Knowledge Injection to Counter Large Language Model (LLM) Hallucination”. In: The Semantic Web: ESWC 2023 Satellite Events. 2023, pp. 182-185.

[40] Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Talamadupula. "Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization". In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. June 2021, pp. 1322-1336.

[41] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. "Nationality Bias in Text Generation". In: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. May 2023, pp. 116122 .

[42] Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. "A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation". In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. July 2019, pp. 2673-2679.

[43] Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. "Entity Cloze By Date: What LMs Know About Unseen Entities". In: NAACL-HLT (Findings). Association for Computational Linguistics, 2022, pp. 693-702.

[44] OpenAI. ChatGPT Release Notes. Accessed: 2023-12-16. 2023. URL: https : / / web archive.org/web/20231214021113/https://help.openai.com/en/articles/ 6825453-chatgpt-release-notes.

[45] OpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL]

[46] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. 2023. arXiv: 2308.03188 [cs.CL].

[47] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. "Data and its (dis)contents: A survey of dataset development and use in machine learning research". In: Patterns 2.11 (2021), p. 100336.

[48] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. 2023. arXiv: 2302.12813 [cs.CL]

[49] Mojzesz Presburger. "Uber die vollstandigkeiteines gewissen systems der arithmetik ganzer zahlen, in welchen die addition als einzige operation hervortritt". In: Comptes-Rendus du ler Congres des Mathematiciens des Pays Slavs. 1929.

[50] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. "Sequence Level Training with Recurrent Neural Networks". In: International Conference on Learning Representations. 2016.

[51] Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. "The Curious Case of Hallucinations in Neural Machine Translation". In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. June 2021, pp. 1172-1183.

[52] Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, and Amitava Das. The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations. 2023. arXiv: 2310.04988 [cs.AI]

[53] Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher Parisien, and Jonathan Cohen. "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails". In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Dec. 2023, pp. 431-445.

[54] Lei Shen, Haolan Zhan, Xin Shen, Hongshen Chen, Xiaofang Zhao, and Xiaodan Zhu. "Identifying Untrustworthy Samples: Data Filtering for Open-Domain Dialogues with Bayesian Optimization". In: Proceedings of the 30th ACM International Conference on Information \& Knowledge Management. 2021, pp. 1598-1608.

[55] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wentau Yih. Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. 2023. arXiv: 2305.14739 [cs.CL].

[56] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Rich James, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-Context Pretraining: Language Modeling Beyond Document Boundaries. 2023. arXiv: 2310.10638 [cs.CL]

[57] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. "Retrieval Augmentation Reduces Hallucination in Conversation". In: Findings of the Association for Computational Linguistics: EMNLP 2021. Nov. 2021, pp. 3784-3803.

[58] Jeff Speaks. "Theories of Meaning". In: The Stanford Encyclopedia of Philosophy. Ed. by Edward N. Zalta. Spring 2021. Metaphysics Research Lab, Stanford University, 2021. URL: https://plato.stanford.edu/archives/spr2021/entries/meaning/

[59] Ryan Stansifer. Presburger's article on integer arithmetic: Remarks and translation. Tech. rep. Cornell University, 1984. URL: https://dl.acm.org/doi/book/10.5555/867696

[60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. 2023. arXiv: 2302.13971 [cs.CL].

[61] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. 2023. arXiv: 2307.09288 [cs.CL]

[62] A. M. Turing. "On Computable Numbers, with an Application to the Entscheidungsproblem". In: Proceedings of the London Mathematical Society s2-42.1 (1937), pp. 230-265.

[63] L. G. Valiant. "A Theory of the Learnable". In: Communications of the ACM 27.11 (Nov. 1984), pp. 1134-1142.

[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. "Attention is All You Need". In: Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017, pp. 6000-6010.

[65] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. 2023. arXiv: 2310.03214 [cs.CL]

[66] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang. Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. 2023. arXiv: 2310.07521 [cs.CL].

[67] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. "SCOTT: Self-Consistent Chain-of-Thought Distillation". In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). July 2023, pp. 55465558 .

[68] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. "Emergent Abilities of Large Language Models". In: Transactions on Machine Learning Research (2022). URL: https: //openreview.net/forum?id=yzkSU5zdwD

[69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter brian, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". In: Advances in Neural Information Processing Systems. Vol. 35. 2022, pp. 24824-24837.

[70] Wikipedia contributors. Cantor's diagonal argument - Wikipedia, The Free Encyclopedia. [Online; accessed 6-December-2023]. 2023. URL: https://en.wikipedia.org/w/index php?title=Cantor\%27s_diagonal_argument\&oldid=1173962712

[71] Yaqi Xie, Ziwei Xu, Mohan S Kankanhalli, Kuldeep S Meel, and Harold Soh. "Embedding Symbolic Knowledge into Deep Networks". In: Advances in Neural Information Processing Systems. Vol. 32. 2019.

[72] Ziwei Xu, Yogesh Rawat, Yongkang Wong, Mohan S Kankanhalli, and Mubarak Shah. "Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation". In: Advances in Neural Information Processing Systems. Vol. 35. 2022, pp. 14890-14903.

[73] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model". In: International Conference on Learning Representations. 2018.

[74] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. 2023. arXiv: 2305.10601 [cs.CL]

[75] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. 2023. arXiv: 2309.01219 [cs.CL].

[76] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. "Verify-andEdit: A Knowledge-Enhanced Chain-of-Thought Framework". In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, July 2023, pp. 5823-5840.
