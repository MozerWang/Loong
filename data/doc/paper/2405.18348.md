# Can Automatic Metrics Assess High-Quality Translations? 

Sweta Agrawal ${ }^{1 *}$, António Farinhas ${ }^{1,2 *}$, Ricardo Rei ${ }^{3}$, André F.T. Martins ${ }^{1,2,3,4}$<br>${ }^{1}$ Instituto de Telecomunicações, ${ }^{2}$ Instituto Superior Técnico, Universidade de Lisboa<br>${ }^{3}$ Unbabel, ${ }^{4}$ ELLIS Unit Lisbon<br>swetaagrawal20@gmail.com, antonio.farinhas@tecnico.ulisboa.pt


#### Abstract

Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments. However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source. In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality. This effect is most pronounced when the quality is high and the variance among alternatives is low. Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality. Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans. Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in automatic evaluation methods.


## 1 Introduction

The automatic evaluation of machine or humangenerated translations has gained widespread attention over the past few years. These evaluation metrics act as proxies for translation quality in the absence of human judgments, offering immediate feedback. They are widely used not only to provide quality indicators to users and translators (Béchara et al., 2021; Castilho and O'Brien, 2017; Mehandru et al., 2023a), but also to improve machine translation (MT) systems themselves (He et al., 2024; Xu et al., 2024a; Fernandes et al., 2022).

Judging whether, and to what extent, these metrics concur with human evaluation is paramount[^0]

| LP | $\mathrm{N}$ | ZERO-M |  |
| :---: | :---: | :---: | :---: |
| WMT 2023 METRICS DATASET |  |  |  |
| EN-DE (P) | 5520 | $25.4 \%$ | $\square \square$ |
| HE-EN | 9840 | $50.8 \%$ | $\square \square$ |
| ZH-EN | 17655 | $19.1 \%$ | $\square \quad \square$ |
| WMT 2022 METRICS DATASET |  |  |  |
| EN-DE | 18410 | $51.5 \%$ | $\square \quad \square$ |
| EN- - | 19725 | $42.7 \%$ | $\square \square$ |
| $\mathrm{ZH}-\mathrm{EN}$ | 26250 | $46.4 \%$ | $\square \square$ |
| WMT 2022 CHAT DATASET |  |  |  |
|  | 4756 | $63.2 \%$ | $\square \square$ |
| En-XX | 5901 | $60.2 \%$ | $\square \square$ |

Table 1: Gold MQM scores distribution in recent WMT datasets. High-quality translations are represented in shades of green (darker for MQM $=0$ and lighter for $\mathrm{MQM} \geq-5$ ); red represents translations with at least one major error (MQM $\leq-5$ ). P: paragraph-level.

to ensuring their effectiveness and applicability in diverse scenarios. A recent human evaluation study conducted at the Conference on Machine Translation (WMT) revealed that translations produced by current MT systems often achieve very high-quality scores (ranging from 80 to 90 ) when judged by humans on a direct assessment (DA) scale of 0 to 100 (Kocmi et al., 2023). Similarly, Deutsch et al. (2023) observe that these systems increasingly generate numerous "perfect" translations (translations with zero errors), especially for high-resource language pairs, as shown in Table 1. As MT quality advances, evaluating whether evaluation metrics accurately reflect this progress is essential. The absence of clear criteria for assessing these highquality translations can introduce bias, leading to inconsistent assessments based on metric preferences rather than objective measures of accuracy.

Most evaluations of automatic metrics primarily assess their ability to distinguish between good and bad source-translation pairs (Freitag et al., 2023, 2022b), often overlooking their capacity to discern subtle differences in translation quality for a given source. In many practical and high-risk applica-
tions (e.g., within the medical or legal domains), the main concern is not merely measuring the accuracy level of a translation but determining whether it is fit for a specific use (Nida, 1964; Church and Hovy, 1993; Bowker, 2019; Vieira et al., 2021; Mehandru et al., 2023b). While correlations provide valuable insights into the performance of automatic metrics, they do not offer a definitive measure of whether existing metrics can reliably confirm translation accuracy.

Hence, in this work, we systematically investigate how existing MT metrics assess high-quality (HQ) correct translations, defined as translations with zero or minor errors only. We find that:

1. Automatic metrics struggle to distinguish between translations for a given source, especially when comparing HQ translations.
2. Reference-free metrics achieve correlation scores close to reference-based ones in distinguishing HQ translations.
3. Current metrics severely overestimate (for non-HQ translations) or underestimate (for HQ translations) translation quality.
4. GEMbA-MQM (Kocmi and Federmann, 2023), a GPT-based reference-free metric, achieves the highest F1 score in detecting the HQ translations with no errors.
5. Gemba-MQM assigns high scores to erroneous GPT-4 translations, suggesting a preferential bias towards the LLM's own outputs.

These findings highlight the necessity for more robust evaluation protocols to accurately assess the quality of automatic evaluation metrics.

## 2 How good are current MT systems?

The most reliable way to assess translation quality has been through human evaluations. Over the years, several models and frameworks have been proposed for evaluating translation quality. While earlier works consider two dimensions-adequacy and fluency-with a 5-point Likert scale (King, 1996), subsequent work on direct assessments (DA) considers a single continuous scale of $0-100$ (Graham et al., 2017). However, several studies have questioned the credibility and subjectivity of DA-based evaluation (Toral et al., 2018; Läubli et al., 2020; Fischer and Läubli, 2020; Mathur et al., 2020b; Freitag et al., 2021)

Unlike DAs, which assign a numeric score to a translation, a recent alternative relies on explicit error judgments (including error types and severity levels within specific spans of the sourcetranslation pair) by human experts (Freitag et al., 2021) through the Multidimensional Quality Metrics (Lommel et al., 2014, MQM) framework. This enables a more accurate, fine-grained, and objective evaluation compared to previous approaches. Translations under the MQM framework receive a score of 0 if they contain no errors. Minor errors incur a penalty of -1 , while major errors that could potentially impact the usability or understandability of the content are penalized with a score of $-5 .{ }^{1}$ Notably, the range and interpretation of scores in MQM differ from those in DA. In DA, scores range from 0 to 100 , with higher scores indicating better quality. In contrast, MQM employs a penalty system where scores become increasingly negative as the number and severity of errors increase.

We present the distribution of gold MQM scores from the WMT23 Metrics task (Freitag et al., 2023), WMT22 Metrics task (Freitag et al., 2022b), and WMT22 Chat Translation task (Farinha et al., 2022) in Table 1. Across settings and language pairs, the percentage of translations achieving a zero MQM score ranges from $19.1 \%$ to $63.2 \%$. This percentage is highest on the chat dataset, which includes short texts that are relatively easy to translate. This trend holds when translating both to and from English (average of $63.2 \%$ and $60.2 \%$, respectively). On the other hand, longer and more complex texts tend to result in translations with more major errors, as expected (e.g., $47.4 \%$ on WMT23 EN-DE). Nevertheless, the current statistics show that a large percentage of the dataset includes translations with no or only minor errors, emphasizing the importance of accurately identifying these high-quality translations in the evaluation process.

## 3 How well do MT metrics assess HQ translations?

We define HQ translations as those that achieve an MQM score $>-5$, i.e., translations without any major errors according to human evaluators. By definition, these translations can only include errors that do not impede comprehension or usability.

Automatic metrics. We consider a subset of the evaluation metrics evaluated by the shared tasks, including reference-free (also known as quality es-[^1]

| METRIC | NO-GROUPING |  | $\Delta$ | NO-GROUPING $\dagger$ |  | $\Delta$ | GROUP-BY-SRC |  | $\Delta$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\mathrm{ALL}}$ | $\mathrm{HQ}$ |  | ALL | $\mathrm{HQ}$ |  | $\mathrm{ALL}^{\dagger}$ | $\mathrm{HQ}$ |  |
| $\mathrm{chrF}$ | 262 | 137 | 0.124 | $227 \pm 0.030$ | $0.132 \pm 0.022$ | -0.094 | $0.267 \pm 0.050$ | 0.136 | -0.131 |
| $\mathrm{BLEU}$ | 193 | 0.094 | .099 | $90 \pm 0.032$ | $0.087=$ | -0.103 | $303 \pm 0.056$ | 0.146 | -0.156 |
| BERTscore | 355 | 0.190 | -0.165 | co | 0.183 | -0.184 | $.325 \pm 0.035$ | 0.134 | -0.191 |
| COMET |  | 0.385 | -0.194 | $584 \pm 0.024$ | $0.390 \pm 0.031$ | -0.194 | $.461 \pm 0.041$ | 0.202 | -0 |
| BLEURT-20 | 618 | 0.357 | -0.262 | $603 \pm 0.020$ | $0.357 \pm 0.033$ | -0.246 | $.449 \pm 0.043$ | 0.220 | -0.229 |
| XCOMET-XL | 0.713 | 0.454 | -0.259 | $0.705 \pm 0.020$ | $0.449 \pm 0.018$ | -0.256 | $.461 \pm 0.030$ | 0.250 | -0.211 |
| XCOMET-XXL | 0.708 | 0.399 | -0.309 | $0.716 \pm 0.020$ | $0.382 \pm 0.032$ | -0.335 | $0.481 \pm 0.041$ | 0.326 | -0.155 |
| Metric | 0.682 | 0.433 | -0.249 | $0.680 \pm 0.018$ | $0.446 \pm 0.027$ | -0.233 | $0.450 \pm 0.043$ | 0.301 | -0.149 |
| MaTESe | 0.591 | 0.353 |  | $0.593 \pm 0.028$ | $0.370 \pm 0.044$ |  | $0.341 \pm 0.042$ | 0.254 | - |
| quality estimation |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  | 0.368 |  |
| CometKiwi | 0.565 | 0.286 |  | $0.561 \pm 0.019$ | 0.268 | -0.293 | 0.411 | 0.182 | -0.229 |
| CometKiwi-XL | 0.542 | 0.240 | -0.302 | $0.550 \pm 0.023$ | $0.254 \pm 0.032$ | -0.296 | $0.427 \pm 0.029$ | 0.223 | -0.204 |
| CometKiwi-XXL | 0.525 | 0.236 | -0.289 | $0.504 \pm 0.031$ | $0.244 \pm 0.032$ | -0.260 | $0.456 \pm 0.029$ | 0.327 | -0.129 |
| MetricX-23-QE | 0.683 | 0.425 | -0.258 | $0.681 \pm 0.012$ | $0.439 \pm 0.027$ | -0.242 | $0.470 \pm 0.028$ | 0.292 | -0.177 |

Table 2: Spearman correlation on WMT23 EN-DE. $\dagger$ : Subsampled to match GROUP-BY-SRC HQ's sample size.

timation) and reference-based ones (metric description in Appendix A).

### 3.1 How do metrics rank HQ translations?

We first investigate the ability of automatic metrics to rank HQ translations, following the WMT metrics evaluation methodology. This is particularly relevant today, as automatic metrics are often used to guide MT training or decoding processes. For instance, recent work employs both referencebased and reference-free metrics to rerank multiple hypotheses generated by dedicated MT models or large language models (LLMs), aiming to improve translation quality (Fernandes et al., 2022; Freitag et al., 2022a; Farinhas et al., 2023). These metrics are also used to provide quality feedback signals during training, either explicitly in loss signals (Ramos et al., 2023; Yan et al., 2023; He et al., 2024) or implicitly via the creation of preference datasets (Xu et al., 2024b; Yang et al., 2023).

Consider $N$ systems and $M$ source segments. Typically, segment-level correlations are computed between the $N \times M$ translations. However, this differs from the practical setting where metrics are used to rerank several translations for the same source. Therefore, we follow Deutsch et al. (2023) and compute the average correlation between the $N$ translation scores grouped by the source sentences. We refer to the former setting as No-GROUPING and the latter as GROUP-BY-SRC. ${ }^{2}$

Furthermore, we study to what extent these automatic metrics distinguish between HQ translations[^2]

as opposed to considering all translation hypotheses (ALL). Since the number of source segments with only minor errors, $K$, is less than or equal to $M$, we report correlations on subsampled datasets (randomly sampled 10 times) that match the sample size, $K \times M$, marked with the symbol $\dagger$ in Table 2 . This approach is motivated by a similar analysis conducted by Mathur et al. (2020a) on the ability of automatic metrics to rank HQ systems, where a limited number of samples (typically 4 or 5 ) was shown to lead to unreliable conclusions. However, our focus is on segment-level evaluation, where the number of subsampled items is much larger.

Table 2 presents Spearman correlation values for automatic metrics on various configurations of the WMT23 English-German dataset as described above (see App. B for other datasets and correlation metrics). We start by noting that the correlation observed on the entire dataset (GROUP-BY-SRC) and the subsampled dataset (GROUP-BY-SRC ${ }^{\dagger}$ ) is close, with a small standard deviation (0.02 $0.05)$ across all metrics. This establishes that the observed differences cannot merely be attributed to changes in sample size across settings.

Automatic metrics do not correlate well with human judgments when evaluating translation quality for the same source. Automatic metrics can distinguish good source-translation pairs from bad ones but are less effective in differentiating between good and bad translations for the same source. This is evidenced by the the drop in correlation from the No-Grouping ALL to the GRoupBY-SRC ALL setting. A possible reason for this disparity lies in how these metrics are typically

![](https://cdn.mathpix.com/cropped/2024_06_04_75129b3ccd6b076f1620g-04.jpg?height=708&width=988&top_left_y=246&top_left_x=248)

Figure 1: Top: Scores distribution for HQ-ZERO translations on WMT23. Bottom: Precision, recall, and F1.

![](https://cdn.mathpix.com/cropped/2024_06_04_75129b3ccd6b076f1620g-04.jpg?height=614&width=540&top_left_y=247&top_left_x=1295)

Figure 2: Absolute difference of the number of times a metric assigns a valid score to HQ-ZERO and non HQ-ZERO translations. trained. Most learned metrics are trained to predict translation quality for a given source-hypothesis pair (e.g., CometKiwi), reference-hypothesis (e.g., BLEURT-20), or source-reference-hypothesis trio (e.g., Comet, $x$ COMET). While this can still be a useful signal for ranking two systems based on averaged scores across different texts, it may provide limited information for gauging translation quality for the same source. Using contrastive objectives or exposing the metric to multiple translations could help mitigate this issue (Briakou and Carpuat, 2020). This highlights the limitations of using automatic metrics as the sole measure of translation quality, particularly in scenarios where fine-grained distinctions between translations of the same source text are required.

QE metrics are on par with reference-based ones for differentiating translations. $\mathrm{QE}$ metrics show promising results in differentiating translations in the GRouP-BY-SRC setting, often achieving comparable or better correlation than referencebased metrics. For EN-DE, MetricX-23-QE and GEMBA-MQM rank second and third, respectively, in their correlation with human judgments in the ALL setting, following $\times$ COMET-XXL. When contrasting HQ translations, GEMBA-MQM outperforms all other metrics. The relatively strong performance of $\mathrm{QE}$ metrics, particularly in this setting, highlights their potential as valuable tools for translation generation and ranking tasks.

Automatic metrics fail to distinguish highquality translations. We observe a consistent drop in correlation scores for all metrics in the $\mathrm{HQ}$ setting, compared to the ALL setting. In the HQ setting, most translations are often tied in translation quality and receive scores in a narrow range of $(-5,0]$. Deutsch et al. (2023) show that most metrics struggle to predict translation ties accurately, i.e., give the same score to two translations with similar quality, except for error-predicting metrics like Gemba-MQM or MaTESe. They propose a tie calibration method to automatically induce metric ties that searches for an $\epsilon$ difference in the metric value that maximizes a rank-based correlation statistic. However, these $\epsilon$ values are metricspecific and dataset-dependent. Moreover, it is unclear how the $\epsilon$ values depend on the quality of translations - a difference of 0.02 might have a different interpretation when considering high-quality versus low-quality translations. Therefore, we do not explicitly induce ties in our analysis to provide a more realistic view of the metrics and leave this exploration to future work.

We note that the observed decrease in correlation from the $\mathrm{HQ}$ to the ALL setting for these metrics can have significant implications, especially when they are used to rerank translations produced by strong MT systems. This may result in an artificial boost or bias for specific systems or outputs, inadvertently prioritizing translations that align well with metric biases but deviate from appropriate or contextually fitting translations. We uncover one such bias towards specific systems in Section 3.3.

### 3.2 How well do metrics detect $\mathrm{HQ}$ translations?

Given that reranking and scoring translations of close quality is a difficult task, we now assess how
automatic metrics score HQ translations that human evaluators assign zero MQM scores, referred to as HQ-ZERO. We use the normalized range of [0.99, 1.01], a narrow band around the ideal score of 1.0 , which is the highest score a metric should assign to HQ-Zero translations. This allows us to evaluate the precision and recall of automatic metrics in detecting these translations.

Figure 1 shows the distribution of metric scores for the HQ-ZERO on the WMT23 datasets (for all other language pairs and datasets, see App. C).

Most metrics exhibit high variance for $H Q$ translations. 8 out of 15 metrics fail to assign valid scores to translations with no errors. As expected, lexical metrics (chrF and BLEU) produce the lowest scores, possibly due to over-reliance on a specific reference translation. Neural metrics trained to regress on DA scores (BLEURT, COMET and variants, BLEURT-20) also struggle to predict valid scores for these translations, likely due to low agreement between DA and MQM scores, as discussed by Freitag et al. (2021).

Metrics over or underestimate translation quality. Metrics that do score these translations within the valid range, i.e., close to a score of 1.0 (xCOMET, MaTESe, and GEMBA-MQM), exhibit different tradeoffs between precision (P) and recall (R). While $\times$ COMET prioritizes precision, MaTESe excels at recognizing many HQZERO translations, leading to increased recall. This difference might stem from the specific task each metric is optimized for: $\times$ COMET predicts sentence-level quality, whereas MaTESe is optimized to predict word-level error spans. As expected, $x$ COMET-XXL significantly outperforms $x C O M E T-X L .^{3}$ Interestingly, the reference-free GEMBA-MQM, based on GPT-4, achieves the highest F1 score across all language pairs, demonstrating the capabilities of large language model-based evaluation in more nuanced MT evaluation.

### 3.3 Which HQ translations are detected?

To study preference bias from metrics towards specific systems, we compute the absolute difference in the number of times a metric assigns a valid score to HQ-ZERO and non-HQ-ZERO translations.

Figure 2 shows that MaTESe equally overestimates translation quality for many systems, as suggested by its high recall and low precision scores[^3]

(Table 1). Additionally, GEMBA-MQM frequently assigns zero MQM scores to GPT-4 5-shot translations, even when humans identify errors in them. These findings align with concurrent studies that reveal a preference bias of LLMs towards their outputs (Panickssery et al., 2024; Xu et al., 2024c). This underscores the need for a more detailed evaluation to understand the outputs these metrics prefer and whether they align with human preferences.

## 4 Conclusions and Future Work

This work systematically investigates how automatic metrics assess high-quality (HQ) translations. We find that current metrics correlate poorly with human judgments when contrasting translations for a given source, with the correlation being even lower for HQ translations. We then study whether metrics can detect HQ translations that attain zero MQM scores from humans, HQ-ZERO, and find that many metrics fail to reliably assign valid scores to these translations. While GEMBA-MQM, a GPT4-based evaluation metric, attains the highest F1 score for detecting HQ-ZERO translations, it may exhibit a preference for outputs generated by GPT4. Therefore, despite its promise, it is essential to complement GEMBA-MQM with other metrics to ensure robustness and mitigate potential biases.

## Acknowledgments

We thank Ben Peters, Goncalo Faria, and Eleftheria Briakou for their constructive feedback on the paper. This work was supported by EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI), and by Fundação para a Ciência e Tecnologia through contract UIDB/50008/2020.

## References

Sweta Agrawal, Amin Farajian, Patrick Fernandes, Ricardo Rei, and André FT Martins. 2024. Is context helpful for chat translation evaluation? arXiv preprint arXiv:2403.08314.

Lynne Bowker. 2019. Fit-for-purpose translation. In The Routledge handbook of translation and technology, pages 453-468. Routledge.

Eleftheria Briakou and Marine Carpuat. 2020. Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank. In

Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1563-1580, Online. Association for Computational Linguistics.

Hannah Béchara, Constantin Orăsan, Carla Parra Escartín, Marcos Zampieri, and William Lowe. 2021. The role of machine translation quality estimation in the post-editing workflow. Informatics, 8(3).

Sheila Castilho and Sharon O'Brien. 2017. Acceptability of machine-translated content: A multi-language evaluation by translators and end-users. Linguistica Antverpiensia, New Series-Themes in Translation Studies, 16 .

Kenneth W Church and Eduard H Hovy. 1993. Good applications for crummy machine translation. Machine Translation, 8:239-258.

Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1291412929, Singapore. Association for Computational Linguistics.

Ana C Farinha, M. Amin Farajian, Marianna Buchicchio, Patrick Fernandes, José G. C. de Souza, Helena Moniz, and André F. T. Martins. 2022. Findings of the WMT 2022 shared task on chat translation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 724-743, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

António Farinhas, José de Souza, and Andre Martins. 2023. An empirical study of translation hypothesis ensembling with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11956-11970, Singapore. Association for Computational Linguistics.

Patrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1396-1412, Seattle, United States. Association for Computational Linguistics.

Lukas Fischer and Samuel Läubli. 2020. What's the difference between professional human and machine translation? a blind multi-language study on domainspecific MT. In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 215-224, Lisboa, Portugal. European Association for Machine Translation.

Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.

Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022a. High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics. Transactions of the Association for Computational Linguistics, 10:811-825.

Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pages 578-628, Singapore. Association for Computational Linguistics.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022b. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2017. Can machine translation systems be evaluated by the crowd alone. Natural Language Engineering, 23(1):3-30.

Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error detection.

Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, and Zhaopeng Tu. 2024. Improving machine translation with human feedback: An exploration of quality estimation as a reward model. arXiv preprint arXiv:2401.12873.

Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023. MetricX-23: The Google submission to the WMT 2023 metrics shared task. In Proceedings of the Eighth Conference on Machine Translation, pages 756-767, Singapore. Association for Computational Linguistics.

Margaret King. 1996. Evaluating natural language processing systems. Communications of the ACM, 39(1):73-79.

Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, and Mariya Shmatova. 2023. Findings of the 2023
conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, pages 1-42, Singapore. Association for Computational Linguistics.

Tom Kocmi and Christian Federmann. 2023. GEMBAMQM: Detecting translation quality error spans with GPT-4. In Proceedings of the Eighth Conference on Machine Translation, pages 768-775, Singapore. Association for Computational Linguistics.

Samuel Läubli, Sheila Castilho, Graham Neubig, Rico Sennrich, Qinlan Shen, and Antonio Toral. 2020. A set of recommendations for assessing humanmachine parity in language translation. Journal of artificial intelligence research, 67:653-672.

Arle Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014. Multidimensional quality metrics (MQM): A framework for declaring and describing translation quality metrics. Tradumàtica: tecnologies de la traducció, 0:455-463.

Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020a. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984-4997, Online. Association for Computational Linguistics.

Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondřej Bojar. 2020b. Results of the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 688-725, Online. Association for Computational Linguistics.

Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine Khoong, Marine Carpuat, and Niloufar Salehi. 2023a. Physician detection of clinical harm in machine translation: Quality estimation aids in reliance and backtranslation identifies critical errors. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1163311647, Singapore. Association for Computational Linguistics.

Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine Khoong, Marine Carpuat, and Niloufar Salehi. 2023b. Physician detection of clinical harm in machine translation: Quality estimation aids in reliance and backtranslation identifies critical errors. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1163311647 .

Eugene Albert Nida. 1964. Toward a science of translating: with special reference to principles and procedures involved in Bible translating. Brill Archive.

Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv:2404.13076.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Niccolò Campolungo, and Roberto Navigli. 2022. MaTESe: Machine translation evaluation as a sequence tagging problem. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 569-577, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.

Miguel Moura Ramos, Patrick Fernandes, António Farinhas, and André FT Martins. 2023. Aligning neural machine translation models: Human feedback in training and inference. arXiv preprint arXiv:2311.09132.

Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Ricardo Rei, Nuno M Guerreiro, Daan van Stigt, Marcos Treviso, Luísa Coheur, José GC de Souza, André FT Martins, et al. 2023. Scaling up cometkiwi: Unbabelist 2023 submission for the quality estimation shared task. In Proceedings of the Eighth Conference on Machine Translation, pages 841-848.

Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634-645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.

Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018. Attaining the unattainable? reassessing claims
of human parity in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 113-123, Brussels, Belgium. Association for Computational Linguistics.

Lucas Nunes Vieira, Minako O'Hagan, and Carol O'Sullivan. 2021. Understanding the societal impacts of machine translation: a critical review of the literature on medical and legal use cases. Information, Communication \& Society, 24(11):1515-1532.

Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024a. A paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations.

Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024b. Contrastive preference optimization: Pushing the boundaries of $11 \mathrm{~m}$ performance in machine translation. arXiv preprint arXiv:2401.08417.

Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang. 2024c. Perils of self-feedback: Self-bias amplifies in large language models. arXiv preprint arXiv:2402.11436.

Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Mingxuan Wang. 2023. BLEURT has universal translations: An analysis of automatic metrics by minimum risk training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5428-5443, Toronto, Canada. Association for Computational Linguistics.

Guangyu Yang, Jinghong Chen, Weizhe Lin, and Bill Byrne. 2023. Direct preference optimization for neural machine translation with minimum bayes risk decoding. arXiv preprint arXiv:2311.08380.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.
