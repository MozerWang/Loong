# Hyperbolic Sliced-Wasserstein via Geodesic and Horospherical Projections 


#### Abstract

Hyperbolic space embeddings have been shown beneficial for many learning tasks where data have an underlying hierarchical structure. Consequently, many machine learning tools were extended to such spaces, but only few discrepancies to compare probability distributions defined over those spaces exist. Among the possible candidates, optimal transport distances are well defined on such Riemannian manifolds and enjoy strong theoretical properties, but suffer from high computational cost. On Euclidean spaces, sliced-Wasserstein distances, which leverage a closed-form solution of the Wasserstein distance in one dimension, are more computationally efficient, but are not readily available on hyperbolic spaces. In this work, we propose to derive novel hyperbolic sliced-Wasserstein discrepancies. These constructions use projections on the underlying geodesics either along horospheres or geodesics. We study and compare them on different tasks where hyperbolic representations are relevant, such as sampling or image classification.


## 1. Introduction

In recent years, hyperbolic spaces have received a lot of attention in machine learning (ML) as they allow efficiently processing data that present a hierarchical structure (Nickel \& Kiela, 2017; 2018). This encompasses data such as graphs (Gupte et al., 2011), words (Tifrea et al., 2018) or images (Khrulkov et al., 2020). Embedding in hyperbolic spaces has been proposed for various applications such as drug embedding (Yu et al., 2020), image clustering (Park et al., 2021; Ghadimi Atigh et al., 2021), zero-shot recognition (Liu et al., 2020), remote sensing (Hamzaoui et al., 2021) or reinforcement learning (Cetin et al., 2022). Hence, many[^0]

works proposed to develop tools to be used on such spaces, such as generalization of Gaussian distributions (Nagano et al., 2019; Galaz-Garcia et al., 2022), neural networks (Ganea et al., 2018b; Liu et al., 2019) or normalizing flows (Lou et al., 2020; Bose et al., 2020).

Optimal Transport (OT) (Villani, 2003; 2009) is a popular tool used in ML to compare probability distributions. Among others, it has been used for domain adaptation (Courty et al., 2016), learning generative models (Arjovsky et al., 2017) or document classification (Kusner et al., 2015). However, the main tool of OT is the Wasserstein distance which exhibits an expensive, super-cubical computational cost w.r.t. the number of samples of each distribution. Hence, many workarounds have been proposed to alleviate the computational burden such as entropic regularization (Cuturi, 2013), minibatch OT (Fatras et al., 2020) or the sliced-Wasserstein (SW) distance (Rabin et al., 2011). In particular, SW is a popular variant of the Wasserstein distance that computes the expected distance between one dimensional projections on some lines of the two distributions. Its computational advantages and theoretical properties make it an efficient and popular alternative to the Wasserstein distance. For example, it has been used for texture synthesis (Heitz et al., 2021) or for generative modeling with SW autoencoders (Kolouri et al., 2018), SW GANs (Deshpande et al., 2018), SW flows (Liutkus et al., 2019) or SW gradient flows (Bonet et al., 2022).

The theoretical study of the Wasserstein distance on Riemannian manifolds is well developed (McCann, 2001; Villani, 2009). When it comes to hyperbolic spaces, some optimal transport attempts aimed at aligning distributions of data which have been embedded in a hyperbolic space (Alvarez-Melis et al., 2020; Hoyos-Idrobo, 2020). Regarding SW, it is originally defined using Euclidean distances and projections, which are not well suited to other manifolds. Recently, Rustamov \& Majumdar (2020) proposed to defined a $\mathrm{SW}$ distance on compact manifolds using the eigendecomposition of the Laplace-Beltrami operator while Bonet et al. (2023) proposed a SW distance to tackle this problem for measures supported on the sphere by using only objects intrinsically defined on this specific manifold. Contrary to the elliptical geometry of the sphere, the negative curvature of hyperbolic spaces calls for drastically different strategies to define geodesics and the associated
projection operators. This work proposes to close this gap by proposing new SW constructions on these spaces.

Contributions. We extend sliced-Wasserstein to data living in hyperbolic spaces. Analogously to Euclidean SW, we project the distributions on geodesics passing through the origin. Interestingly enough, different projections can be considered, leading to several new SW constructions that exhibit different theoretical properties and empirical benefits. We make connections with Radon transforms already defined in the literature and we show that hyperbolic SW are (pseudo-) distances. We provide the algorithmic procedure and discuss its complexity. We illustrate the benefits of these new hyperbolic SW distances on several tasks such as sampling or image classification.

## 2. Background

In this Section, we first provide some background on Optimal Transport with the Wasserstein and the slicedWasserstein distance. We then review two common hyperbolic models, namely the Lorentz and Poincaré ball models, on which we will define new OT discrepancies in the next section.

### 2.1. Optimal Transport

Optimal transport is a popular field which allows comparing distributions of probabilities by determining a transport plan minimizing some ground cost. The main tool of OT is the Wasserstein distance which we introduce now.

Wasserstein Distance on Riemannian Manifolds. Let $M$ be a Riemannian manifold endowed with a Riemannian distance $d$. For $p \geq 1$, the $p$-Wasserstein distance between two probability measures $\mu, \nu \in \mathcal{P}_{p}(M)=\{\mu \in$ $\mathcal{P}(M), \int_{M} d\left(x, x_{0}\right)^{p} \mathrm{~d} \mu(x)<\infty$ for any $\left.x_{0} \in M\right\}$ is defined as

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu)=\inf _{\gamma \in \Pi(\mu, \nu)} \int_{M \times M} d(x, y)^{p} \mathrm{~d} \gamma(x, y) \tag{1}
\end{equation*}
$$

where $\Pi(\mu, \nu)=\left\{\gamma \in \mathcal{P}(M \times M), \pi_{\#}^{1} \gamma=\mu, \pi_{\#}^{2} \gamma=\nu\right\}$ is the set of couplings, $\pi^{1}(x, y)=x, \pi^{2}(x, y)=y$ and $\#$ is the pushforward operator defined as, for all borelian $A \subset M, T_{\#} \mu(A)=\mu\left(T^{-1}(A)\right)$. For more details about OT, we refer to (Villani, 2009).

The main bottleneck of the Wasserstein distance is its computational complexity. Indeed, for two discrete probability measures with $n$ samples, it can be solved using linear programs (Peyré et al., 2019) with a complexity of $O\left(n^{3} \log n\right)$, which prevents its use when large amount of data are at stake. Hence, a whole literature consists at deriving alternative OT metrics with a smaller computational cost.
Sliced-Wasserstein Distance on Euclidean Space. On Euclidean spaces, a popular proxy of the Wasserstein distance is the so-called sliced-Wasserstein distance. On the real line, for $p \geq 1$, the $p$-Wasserstein distance between $\mu, \nu \in \mathcal{P}_{p}(\mathbb{R})$ admits the following closed-form (Peyré et al., 2019, Remark 2.30) :

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu)=\int_{0}^{1}\left|F_{\mu}^{-1}(u)-F_{\nu}^{-1}(u)\right|^{p} \mathrm{~d} u \tag{2}
\end{equation*}
$$

where $F_{\mu}^{-1}$ and $F_{\nu}^{-1}$ denote the quantile functions of $\mu$ and $\nu$. This can be approximated in practice very efficiently as it only requires to sort the samples, which has a complexity of $O(n \log n)$. Therefore, Rabin et al. (2011) defined the sliced-Wasserstein distance by projecting linearly the probabilities on all the possible directions. For a direction $\theta \in S^{d-1}$, denote, for all $x \in \mathbb{R}^{d}, P^{\theta}(x)=\langle x, \theta\rangle$ the projection in direction $\theta$, and $\lambda$ the uniform measure on $S^{d-1}$. Then, the $\mathrm{SW}$ distance between $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ is defined as

$$
\begin{equation*}
S W_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left(P_{\#}^{\theta} \mu, P_{\#}^{\theta} \nu\right) \mathrm{d} \lambda(\theta) \tag{3}
\end{equation*}
$$

Using a Monte-Carlo approximation, this can be approximated in $O(\operatorname{Ln}(d+\log n))$ where $L$ is the number of projections and $n$ the number of samples.

Moreover, the slicing process has many appealing properties, such as having a sample complexity independent of the dimension (Nadjahi et al., 2020), being topologically equivalent to Wasserstein (Bonnotte, 2013) and being an actual distance. For the latter point, it can be shown to be a pseudo-distance using that $W_{p}$ is a distance. The indiscernible property relies on the link between the projection used in SW and the Radon transform (Bonneel et al., 2015; Kolouri et al., 2019) which is injective on the space of measures (Boman \& Lindskog, 2009, Theorem A). More precisely, let $f \in L^{1}\left(\mathbb{R}^{d}\right)$, then its Radon transform $R: L^{1}\left(\mathbb{R}^{d}\right) \rightarrow L^{1}\left(\mathbb{R} \times S^{d-1}\right)$ is defined for $t \in \mathbb{R}$, $\theta \in S^{d-1}$ as,

$$
\begin{equation*}
R f(t, \theta)=\int_{\mathbb{R}^{d}} f(x) \mathbb{1}_{\{\langle x, \theta\rangle=t\}} \mathrm{d} x \tag{4}
\end{equation*}
$$

This transform admits a dual operator $R^{*}: C_{0}(\mathbb{R} \times$ $\left.S^{d-1}\right) \rightarrow C_{0}\left(\mathbb{R}^{d}\right)$, with $C_{0}\left(\mathbb{R} \times S^{d-1}\right)$ the set of continuous functions that vanish at infinity, such that for all $g \in C_{0}\left(\mathbb{R} \times S^{d-1}\right),\langle R f, g\rangle_{\mathbb{R} \times S^{d-1}}=\left\langle f, R^{*} g\right\rangle_{\mathbb{R}^{d}}$ (Bonneel et al., 2015). This allows defining the Radon transform of a measure $\mu \in \mathcal{M}\left(\mathbb{R}^{d}\right)$ as the measure $R \mu \in \mathcal{M}\left(\mathbb{R} \times S^{d-1}\right)$ satisfying for all $g \in C_{0}\left(\mathbb{R} \times S^{d-1}\right),\langle R \mu, g\rangle_{\mathbb{R} \times S^{d-1}}=$ $\left\langle\mu, R^{*} g\right\rangle_{\mathbb{R}^{d}}$ (Boman \& Lindskog, 2009). Then, it was shown in (Bonneel et al., 2015) that, by denoting by $(R \mu)^{\theta}$ the disintegration w.r.t. to the uniform distribution on $S^{d-1}$,

$$
\begin{equation*}
S W_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left((R \mu)^{\theta},(R \nu)^{\theta}\right) \mathrm{d} \lambda(\theta) \tag{5}
\end{equation*}
$$

Therefore, $S W_{p}^{p}(\mu, \nu)=0$ implies that, for $\lambda$-ae $\theta$, $(R \mu)^{\theta}=(R \nu)^{\theta}$, which implies that $\mu=\nu$ by injectivity of the Radon transform on measures.

Many variants of this distance were recently proposed. Most lines of work considered different subspaces for projecting the data: hypersurfaces (Kolouri et al., 2019), Hilbert curves (Li et al., 2022) or subspace of higher dimensions (Lin et al., 2020; 2021). When it comes to data living on Riemannian manifolds, Rustamov \& Majumdar (2020) defined a variant on compact manifolds and Bonet et al. (2023) extended SW for spherical data.

### 2.2. Hyperbolic Spaces

Hyperbolic spaces are Riemannian manifolds of negative constant curvature (Lee, 2006). They have received recently a surge of interest in machine learning as they allow embedding efficiently data with a hierarchical structure (Nickel \& Kiela, 2017; 2018). A thorough review of the recent use of hyperbolic spaces in machine learning can be found in (Peng et al., 2021).

There are five usual parameterizations of a hyperbolic manifold (Peng et al., 2021). They are equivalent (isometric) and one can easily switch from one formulation to the other. Hence, in practice, we use the one which is the most convenient, either given the formulae to derive or the numerical properties. In machine learning, the two most used models are the Poincaré ball and the Lorentz model (also known as the hyperboloid model). Each of these models has its own advantages compared to the other. For example, the Lorentz model has a distance which behaves better w.r.t. numerical issues compared to the distance of the Poincare ball. However, the Lorentz model is unbounded, contrary to the Poincaré ball. We introduce in the following these two models as we will use both of them in our work.

Lorentz model. First, we introduce the Lorentz model $\llbracket^{d} \subset \mathbb{R}^{d+1}$ of a $d$-dimensional hyperbolic space. It can be defined as

$$
\begin{equation*}
\mathbb{R}^{d}=\left\{\left(x_{0}, \ldots, x_{d+1}\right) \in \mathbb{R}^{d},\langle x, x\rangle_{\mathbb{L}}=-1, x_{0}>0\right\} \tag{6}
\end{equation*}
$$

where

$$
\begin{equation*}
\forall x, y \in \mathbb{R}^{d+1},\langle x, y\rangle_{\mathbb{\circledR}}=-x_{0} y_{0}+\sum_{i=1}^{d} x_{i} y_{i} \tag{7}
\end{equation*}
$$

is the Minkowski pseudo inner-product (Boumal, 2022, Chapter 7). The Lorentz model can be seen as the upper sheet of a two-sheet hyperboloid. In the following, we will denote $x^{0}=(1,0, \ldots, 0) \in \mathbb{R}^{d}$ the origin of the hyperboloid. The geodesic distance in this manifold, which denotes the length of the shortest path between two points, can be defined as

$$
\begin{equation*}
\forall x, y \in \mathbb{\Perp}^{d}, d_{\mathbb{L}}(x, y)=\operatorname{arccosh}\left(-\langle x, y\rangle_{\mathbb{L}}\right) \tag{8}
\end{equation*}
$$

At any point $x \in \mathbb{R}^{d}$, we can associate a subspace of $\mathbb{R}^{d+1}$ orthogonal in the sense of the Minkowski inner product. These spaces are called tangent spaces and are described formally as $T_{x} \llbracket^{d}=\left\{v \in \mathbb{R}^{d+1},\langle v, x\rangle_{\unrhd}=0\right\}$. Note that on tangent spaces, the Minkowski inner-product is a real inner product. In particular, on $T_{x^{0}} \llbracket^{d}$, it is the usual Euclidean inner product, i.e. for $u, v \in T_{x^{0}} \mathbb{}^{d},\langle u, v\rangle_{\mathbb{Q}}=$ $\langle u, v\rangle$. Moreover, for all $v \in T_{x^{0}} \llbracket^{d}, v_{0}=0$.

We can draw a connection with the sphere. Indeed, by endowing $\mathbb{R}^{d+1}$ with $\langle\cdot, \cdot\rangle_{\mathbb{L}}$, we obtain $\mathbb{R}^{1, d}$ the so-called Minkowski space. Then, $\mathbb{L}^{d}$ is the analog in the Minkowski space of the sphere $S^{d}$ in the regular Euclidean space (Bridson \& Haefliger, 2013).

Poincaré ball. The second model of hyperbolic space we will be interested in is the Poincaré ball $\mathbb{B}^{d} \subset \mathbb{R}^{d}$. This space can be obtained as the stereographic projection of each point $x \in \mathbb{L}^{d}$ onto the hyperplane $\left\{x \in \mathbb{R}^{d+1}, x_{0}=0\right\}$. More precisely, the Poincaré ball is defined as

$$
\begin{equation*}
\mathbb{B}^{d}=\left\{x \in \mathbb{R}^{d},\|x\|_{2}<1\right\} \tag{9}
\end{equation*}
$$

with geodesic distance, for all $x, y \in \mathbb{B}^{d}$,

$$
\begin{equation*}
d_{\mathbb{B}}(x, y)=\operatorname{arccosh}\left(1+2 \frac{\|x-y\|_{2}^{2}}{\left(1-\|x\|_{2}^{2}\right)\left(1-\|y\|_{2}^{2}\right)}\right) \tag{10}
\end{equation*}
$$

We see on this formulation that the distance can be subject to numerical instabilities when one of the points is too close to the boundary of the ball.

We can switch from Lorentz to Poincaré using the following isometric projection (Nickel \& Kiela, 2018):

$$
\begin{equation*}
\forall x \in \mathbb{L}^{d}, P_{\mathbb{L} \rightarrow \mathbb{B}}(x)=\frac{1}{1+x_{0}}\left(x_{1}, \ldots, x_{d}\right) \tag{11}
\end{equation*}
$$

and from Poincaré to Lorentz by

$\forall x \in \mathbb{B}^{d}, P_{\mathbb{B} \rightarrow \mathbb{E}}(x)=\frac{1}{1-\|x\|_{2}^{2}}\left(1+\|x\|_{2}^{2}, 2 x_{1}, \ldots, 2 x_{d}\right)$.

## 3. Hyperbolic Sliced-Wasserstein Distances

In this work, we aim at introducing sliced-Wasserstein type of distances on hyperbolic spaces. Interestingly enough, several constructions can be performed, depending on the projections that are involved. The first solution we consider is the extension of Euclidean SW between distributions whose support lies on hyperbolic spaces. We also provide variants that involve a geodesic cost. To do so, we first define the subspace on which the Wasserstein distance can be efficiently computed and then provide two different projection operators: geodesic and horospherical. We finally define the related hyperbolic sliced-Wasserstein distances and discuss some of their properties. All the proofs are reported in Appendix A.

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-04.jpg?height=217&width=216&top_left_y=279&top_left_x=239)

(a) Euclidean.

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-04.jpg?height=214&width=217&top_left_y=283&top_left_x=518)

(b) Geodesics.

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-04.jpg?height=214&width=206&top_left_y=283&top_left_x=797)

(c) Horospheres.

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-04.jpg?height=192&width=247&top_left_y=278&top_left_x=1058)

(d) Euclidean.

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-04.jpg?height=192&width=261&top_left_y=278&top_left_x=1298)

(e) Geodesics.

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-04.jpg?height=206&width=252&top_left_y=266&top_left_x=1576)

(f) Horospheres.

Figure 1: Projection of (red) points on a geodesic (black line) in the Poincaré ball and in the Lorentz model along Euclidean lines, geodesics or horospheres (in blue). Projected points on the geodesic are in green.

### 3.1. Euclidean Sliced-Wasserstein on Hyperbolic Spaces

The support of distributions lying on hyperbolic space are included in the ambient spaces $\mathbb{R}^{d}$ (Poincaré ball) or $\mathbb{R}^{d+1}$ (Lorentz model). As such, Euclidean SW can be used for such kind of data. On the Poincaré ball, the projections lie onto the manifold as geodesics passing through the origin are straight lines (see Section 3.2), but the initial geometry of the data might not be fully taken care of as the orthogonal projection does not respect the Poincaré geodesics. On the Lorentz model though, the projections lie out of the manifold. We will denote SWp and SW1 the Poincaré ball and Lorentz model version. These formulations allow inheriting from the properties of SW, such as being a distance.

### 3.2. Projection Set and Wasserstein Distance

To generalize the sliced-Wasserstein distance on other spaces, we first define on which subspace to project. Euclidean spaces can be seen as Riemannian manifolds of null constant curvature whose geodesics are straight lines. Therefore, analogously to the Euclidean space, we project on geodesics passing through the origin. We now describe geodesics in the Lorentz model and in the Poincaré ball.

Geodesics. In the Lorentz model, geodesics passing through the origin $x^{0}$ can be obtained by taking the intersection between $\mathbb{R}^{d}$ and a 2-dimensional plane containing $x^{0}$ (Lee, 2006, Proposition 5.14). Any such plane can be obtained as $\operatorname{span}\left(x^{0}, v\right)$ where $v \in T_{x^{0}} \llbracket^{d} \cap S^{d}=\left\{v \in S^{d}, v_{0}=0\right\}$. The corresponding geodesic can be described by a geodesic line (Bridson \& Haefliger, 2013, Corollary 2.8), i.e. a map $\gamma: \mathbb{R} \rightarrow \mathbb{L}^{d}$ satisfying for all $t, s \in \mathbb{R}, d_{\mathbb{\Perp}}(\gamma(s), \gamma(t))=$ $|t-s|$, of the form

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma(t)=\exp _{x^{0}}(t v)=\cosh (t) x^{0}+\sinh (t) v \tag{13}
\end{equation*}
$$

On the Poincaré ball, geodesics are circular arcs perpendicular to the boundary $S^{d-1}$ (Lee, 2006, Proposition 5.14). In particular, geodesics passing through the origin are straight lines. Hence, they can be characterized by a point $\tilde{v}$ on the border $S^{d-1}$. Such points will be called ideal points.

Wasserstein distance on geodesics. In order to have an efficient way to compute the discrepancy, we need a practical way to compute the Wasserstein distance on geodesics. As the distance between any point on a geodesic line $\gamma$ and the origin can take arbitrary values on $\mathbb{R}_{+}$, we project points from the geodesic to the real line $\mathbb{R}$. Indeed, on $\mathbb{R}$, there exists a well known closed-form (see Section 2.1) that can be efficiently computed in practice. In the Lorentz model, let $v \in T_{x^{0}} \llbracket^{d} \cap S^{d}$ be a direction such that $\gamma(\mathbb{R})=\mathbb{L}^{d} \cap \operatorname{span}\left(x^{0}, v\right)$. Then, we propose to project a point $x \in \gamma(\mathbb{R})$ using

$$
\begin{equation*}
t_{\mathbb{\sharp}}^{v}(x)=\operatorname{sign}(\langle x, v\rangle) d_{\mathbb{\circledR}}\left(x, x^{0}\right) \tag{14}
\end{equation*}
$$

The scalar product with $v$ gives an orientation to the geodesic, and the distance to the origin the coordinate of $x$. We can do the same on the Poincaré ball with $t_{\mathbb{B}}^{\tilde{v}}(x)=\operatorname{sign}(\langle x, \tilde{v}\rangle) d_{\mathbb{B}}(x, 0)$, where $\tilde{v}$ is one of the ideal point to which the geodesic is perpendicular. In the remainder, we will remove the subscripts $\mathbb{L}$ and $\mathbb{B}$ when it is clear from the context. Finally, we need to check that this projection keeps the geodesic Wasserstein distance unchanged. We formulate the following proposition in the Lorentz model.

Proposition 3.1 (Wasserstein distance on geodesics.). Let $v \in T_{x^{0}} \mathbb{L}^{d} \cap S^{d}$ and $\mathcal{G}=\operatorname{span}\left(x^{0}, v\right) \cap \mathbb{■}^{d}$ a geodesic passing through $x^{0}$. Then, for $p \geq 1$ and $\mu, \nu \in \mathcal{P}_{p}(\mathcal{G})$,

$$
\begin{align*}
W_{p}^{p}(\mu, \nu) & =W_{p}^{p}\left(t_{\#}^{v} \mu, t_{\#}^{v} \nu\right) \\
& =\int_{0}^{1}\left|F_{t_{\#}^{v}}^{-1}(u)-F_{t_{\#}^{v}}^{-1}(u)\right|^{p} \mathrm{~d} u \tag{15}
\end{align*}
$$

The last ingredient of hyperbolic SW is the way the points lying in the manifold are projected onto the geodesic. We introduce here two different projections that are illustrated on Figure 1.

### 3.3. Hyperbolic Sliced-Wasserstein

With geodesic projections. We discuss here the results in the Lorentz model, but we can also obtain all the results in the Poincaré ball. Let $v \in T_{x^{0}} \cap S^{d}$ and $\mathcal{G}^{v}=$ $\left\{\exp _{x^{0}}(t v), t \in \mathbb{R}\right\}$ a geodesic passing through $x^{0}$. As a first generalization of the sliced-Wasserstein distance on hyperbolic spaces, we propose to use the geodesic projection
$\tilde{P}^{v}$, which projects points on $\mathcal{G}^{v}$ following the shortest path (geodesics), and which is defined as

$$
\begin{equation*}
\forall x \in \mathbb{L}^{d}, \tilde{P}^{v}(x)=\underset{y \in \mathcal{G}^{v}}{\operatorname{argmin}} d(x, y) \tag{16}
\end{equation*}
$$

We report in Appendix A. 2 the closed-form formulas on both the Lorentz model and the Poincaré ball. Here, we are mostly interested into the coordinate on $\mathbb{R}$, which can be obtained either by computing $t^{v} \circ \tilde{P}^{v}$, or as

$$
\begin{equation*}
\forall x \in \mathbb{L}^{d}, P^{v}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}} d_{\mathbb{\Perp}}\left(\exp _{x^{0}}(t v), x\right) \tag{17}
\end{equation*}
$$

Regarding the implementation, we derive a closed-form in the following proposition.

Proposition 3.2 (Coordinate of the geodesic projection).

1. Let $\mathcal{G}^{v}=\operatorname{span}\left(x^{0}, v\right) \cap \mathbb{■}^{d}$ where $v \in T_{x^{0}} \mathbb{}^{d} \cap S^{d}$. Then, the coordinate $P^{v}$ of the geodesic projection on $\mathcal{G}^{v}$ of $x \in \mathbb{\Perp}^{d}$ is

$$
\begin{equation*}
P^{v}(x)=\operatorname{arctanh}\left(-\frac{\langle x, v\rangle_{\mathbb{L}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{L}}}\right) \tag{18}
\end{equation*}
$$

2. Let $\tilde{v} \in S^{d-1}$ be an ideal point. Then, the coordinate $P^{\tilde{v}}$ of the geodesic projection on the geodesic characterized by $\tilde{v}$ of $x \in \mathbb{B}^{d}$ is

$$
\begin{equation*}
P^{\tilde{v}}(x)=2 \operatorname{arctanh}(s(x)) \tag{19}
\end{equation*}
$$

where

$$
s(x)= \begin{cases}\frac{1+\|x\|_{2}^{2}-\sqrt{\left(1+\|x\|_{2}^{2}\right)^{2}-4\langle x, \tilde{v}\rangle^{2}}}{2\langle x, \tilde{v}\rangle} & \text { if }\langle x, \tilde{v}\rangle \neq 0  \tag{20}\\ 0 & \text { if }\langle x, \tilde{v}\rangle=0 .\end{cases}
$$

Now, we have all the tools to define the geodesic hyperbolic sliced-Wasserstein discrepancy (GHSW) between $\mu, \nu \in$ $\mathcal{P}_{p}\left(\mathbb{L}^{d}\right)$ as, for $p \geq 1$,

$$
\begin{equation*}
G H S W_{p}^{p}(\mu, \nu)=\int_{T_{x^{0}} \mathbb{■}^{d} \cap S^{d}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda(v) \tag{21}
\end{equation*}
$$

Note that $T_{x^{0}} \llbracket^{d} \cap S^{d} \cong S^{d-1}$ and that $v$ can be drawn by first sampling $\tilde{v} \sim \operatorname{Unif}\left(S^{d-1}\right)$ and then adding a 0 in the first coordinate, i.e. $v=(0, \tilde{v})$ with $\tilde{v} \in S^{d-1}$. Note also that $G H S W_{p}(\mu, \nu)<\infty$ for $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$. We also have the Poincaré formulation using $P^{\tilde{v}}$, and defined between $\mu, \nu \in \mathcal{P}\left(\mathbb{B}^{d}\right)$ as

$$
\begin{equation*}
G H S W_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left(P_{\#}^{\tilde{v}} \mu, P_{\#}^{\tilde{v}} \nu\right) \mathrm{d} \lambda(\tilde{v}) \tag{22}
\end{equation*}
$$

With horospherical projections. As we saw in Section 2.1, the projection on geodesics in the Euclidean space is obtained by taking the inner product. A first viewpoint is to see it as the geodesic projection of $x \in \mathbb{R}^{d}$ on the geodesic $\operatorname{span}(\theta)$ :

$$
\begin{equation*}
\langle x, \theta\rangle \theta=\underset{y \in \operatorname{span}(\theta)}{\operatorname{argmin}}\|x-y\|_{2} \tag{23}
\end{equation*}
$$

In this case, using a similar projection as (14), the coordinates on the line are obtained as the inner product:

$$
\begin{equation*}
t^{\theta}(x)=\operatorname{sign}(\langle x, \theta\rangle)\|\langle x, \theta\rangle \theta-0\|_{2}=\langle x, \theta\rangle \tag{24}
\end{equation*}
$$

However, the inner product $\langle x, \theta\rangle$ can actually also be seen directly as a coordinate on the line $\operatorname{span}(\theta)$. This can be translated by the Busemann function on unit-speed geodesics, which can be generalized on certain Riemannian manifolds. More precisely, the Busemann function associated to the geodesic ray $\gamma$, i.e. a geodesic from $\mathbb{R}_{+}$to the manifold satisfying $d(\gamma(t), \gamma(s))=|t-s|$, is defined as (Bridson \& Haefliger, 2013, Definition 8.17)

$$
\begin{equation*}
B^{\gamma}(x)=\lim _{t \rightarrow \infty}(d(x, \gamma(t))-t) \tag{25}
\end{equation*}
$$

where $x$ belongs to the corresponding manifold and $d$ is the geodesic distance. It can be checked that on Euclidean spaces, $B^{\operatorname{span}(\theta)}(x)=-\langle x, \theta\rangle$. While the Busemann function is not well defined on positively curved spaces such as the sphere (as geodesics are periodic), closed-form are available on hyperbolic spaces and provide different projections. We report them in the next proposition. As we only work with geodesics passing through the origin, we put as indices the directions which fully characterize them (either $v \in T_{x^{0}} \mathbb{■}^{d}$ in $\mathbb{■}^{d}$, or $\tilde{v} \in S^{d-1}$ in $\mathbb{B}^{d}$ ).

Proposition 3.3 (Busemann function on hyperbolic space).

1. On $\llbracket^{d}$, for any direction $v \in T_{x^{0}} \llbracket^{d} \cap S^{d}$,

$$
\begin{equation*}
\forall x \in \mathbb{R}^{d}, B^{v}(x)=\log \left(-\left\langle x, x^{0}+v\right\rangle_{\mathbb{\complement}}\right) \tag{26}
\end{equation*}
$$

2. On $\mathbb{B}^{d}$, for any ideal point $\tilde{v} \in S^{d-1}$,

$$
\begin{equation*}
\forall x \in \mathbb{B}^{d}, B^{\tilde{v}}(x)=\log \left(\frac{\|\tilde{v}-x\|_{2}^{2}}{1-\|x\|_{2}^{2}}\right) \tag{27}
\end{equation*}
$$

To conserve Busemann coordinates, it has been proposed by Chami et al. (2021) to project points on a subset following the level sets of the Busemann function. Those level sets are known as horospheres, which can be seen as spheres of infinite radius (Izumiya, 2009). In the Poincaré ball, a horosphere is a Euclidean sphere tangent to an ideal point. Chami et al. (2021) argued that this projection is beneficial against the geodesic projection as it tends to better preserve the distances. This motivates us to project on geodesics following the level sets of the Busemann function in order to conserve the Busemann coordinates, i.e. we want to have $B^{\tilde{v}}(x)=B^{\tilde{v}}\left(P^{\tilde{v}}(x)\right)\left(\right.$ resp. $B^{v}(x)=B^{v}\left(P^{v}(x)\right)$ ) on the Poincaré ball (resp. Lorentz model) where $\tilde{v} \in S^{d-1}$ (resp.
$\left.v \in T_{x^{0}} \llbracket^{d} \cap S^{d}\right)$ is characterizing the geodesic. We report the closed-forms in Appendix A.5. In practice, noting that $B^{\gamma}(x)=B^{\gamma}(\gamma(t))=-t$, we obtain that the coordinate is $t=-B^{\gamma}(x)$.

Using the projections along the horospheres, we can define a new hyperbolic sliced-Wasserstein discrepancy, called horospherical, between $\mu, \nu \in \mathcal{P}_{p}\left(\llbracket^{d}\right)$ as, for $p \geq 1$,

$$
\begin{equation*}
H H S W_{p}^{p}(\mu, \nu)=\int_{T_{x^{0}} \amalg^{d} \cap S^{d}} W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right) \mathrm{d} \lambda(v) \tag{28}
\end{equation*}
$$

Note that $H H S W_{p}(\mu, \nu)<\infty$ for $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{L}^{d}\right)$ (see Appendix B.1). We also provide a formulation on the Poincaré ball between $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{B}^{d}\right)$, using $B^{\tilde{v}}$, as

$$
\begin{equation*}
H H S W_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left(B_{\#}^{\tilde{\tau}} \mu, B_{\#}^{\tilde{v}} \nu\right) \mathrm{d} \lambda(\tilde{v}) \tag{29}
\end{equation*}
$$

Using that the projections formula between $\mathbb{R}^{d}$ and $\mathbb{B}^{d}$ are isometries, we show in the next proposition that the two formulations are equivalent. Hence, we choose in practice the formulation which is the more suitable, either from the nature of data or from a numerical stability viewpoint.

Proposition 3.4. For $p \geq 1$, let $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{B}^{d}\right)$ and denote $\tilde{\mu}=\left(P_{\mathbb{B} \rightarrow \mathbb{L}}\right)_{\#} \mu, \tilde{\nu}=\left(P_{\mathbb{B} \rightarrow \mathbb{L}}\right)_{\#} \nu$. Then,

$$
\begin{align*}
H H S W_{p}^{p}(\mu, \nu) & =H H S W_{p}^{p}(\tilde{\mu}, \tilde{\nu})  \tag{30}\\
G H S W_{p}^{p}(\mu, \nu) & =G H S W_{p}^{p}(\tilde{\mu}, \tilde{\nu}) \tag{31}
\end{align*}
$$

### 3.4. Properties

It can easily be showed that GHSW and HHSW are pseudodistances as it only depends on the distance properties of the Wasserstein distance. Whether or not they satisfy the indiscernible property remains an open question. As described in the introduction for SW, we can derive the corresponding Radon transform. More precisely, we can show that

$$
\begin{equation*}
G H S W_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left((\bar{R} \mu)^{v},(\bar{R} \nu)^{v}\right) \mathrm{d} \lambda(v) \tag{32}
\end{equation*}
$$

where $\bar{R}$ is the hyperbolical Radon transform, first introduced by Helgason (1959) and more recently studied e.g. in (Berenstein \& Rubin, 1999; 2004; Rubin, 2002). We can also show a similar relation between HHSW and the horospherical Radon transform studied e.g. by Bray \& Rubin (2019); Casadio Tarabusi \& Picardello (2021). If these transforms are injective on the space of measures, then we would have that GHSW or HHSW are distances. However, to the best of our knowledge, the injectivity of such transforms on the space of measures has not been studied yet. We detail the derivations in Appendix B.2.

We also provide in Appendix B. 3 the sample complexity and the projection complexity. We note that the results are

```
Algorithm 1 Guideline of GHSW
    Input: $\left(x_{i}\right)_{i=1}^{n} \sim \mu,\left(y_{j}\right)_{j=1}^{n} \sim \nu,\left(\alpha_{i}\right)_{i=1}^{n},\left(\beta_{j}\right)_{j=1}^{n} \in$
    $\Delta_{n}, L$ the number of projections, $p$ the order
    for $\ell=1$ to $L$ do
        Draw $\tilde{v} \sim \operatorname{Unif}\left(S^{d-1}\right)$, let $v=[0, \tilde{v}]$
        $\forall i, j, \hat{x}_{i}^{\ell}=P^{v}\left(x_{i}\right), \hat{y}_{j}^{\ell}=P^{v}\left(y_{j}\right)$
        Compute $W_{p}^{p}\left(\sum_{i=1}^{n} \alpha_{i} \delta_{\hat{x}_{i}^{e}}, \sum_{j=1}^{n} \beta_{j} \delta_{\hat{y}_{j}^{\ell}}\right)$
    end for
    Return $\frac{1}{L} \sum_{\ell=1}^{L} W_{p}^{p}\left(\sum_{i=1}^{n} \alpha_{i} \delta_{\hat{x}_{i}^{\ell}}, \sum_{j=1}^{n} \beta_{j} \delta_{\hat{y}_{j}^{e}}\right)$
```

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-06.jpg?height=458&width=821&top_left_y=706&top_left_x=1064)

Figure 2: Runtime comparison in log-log scale between Wasserstein and Sinkhorn using the geodesic distance, $S W_{2}$, $G H S W_{2}$ and $H H S W_{2}$ with 200 projections, including the computation time of the cost matrices.

similar as in the Euclidean case (Nadjahi et al., 2020), i.e. the sample complexity is independent of the dimension and the projection complexity converges in $O(1 / \sqrt{L})$ with $L$ the number of projections.

## 4. Implementation

In this Section, we discuss the implementation of GHSW and HHSW, as well as their complexity.

Implementation. In practice, we only have access to discrete distributions $\hat{\mu}_{n}=\sum_{i=1}^{n} \alpha_{i} \delta_{x_{i}}$ and $\hat{\nu}_{n}=\sum_{i=1}^{n} \beta_{i} \delta_{y_{i}}$ where $\left(x_{i}\right)_{i}$ and $\left(y_{i}\right)_{i}$ are sample locations in hyperbolic space, and $\left(\alpha_{i}\right)_{i}$ and $\left(\beta_{i}\right)_{i}$ belong to the simplex $\Delta_{n}=$ $\left\{\alpha \in[0,1]^{n}, \sum_{i=1}^{n} \alpha_{i}=1\right\}$. We approximate the integral by a Monte-Carlo approximation by drawing a finite number $L$ of projection directions $\left(v_{\ell}\right)_{\ell=1}^{L}$ in $S^{d-1}$. Then, computing GHSW and HHSW amount at first getting the coordinates on $\mathbb{R}$ by using the corresponding projections, and computing the 1D Wasserstein distance between them. We summarize the procedure in Algorithm 1 for GHSW.

Complexity. For both GHSW and HHSW, the projection procedure has a complexity of $O(n d)$. Hence, for $L$ projections, the complexity is in $O(\operatorname{Ln}(d+\log n))$ which is the same as for SW. In Figure 2, we compare the runtime between GHSW, HHSW, SW, Wasser-

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-07.jpg?height=374&width=1612&top_left_y=263&top_left_x=213)

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-07.jpg?height=279&width=507&top_left_y=278&top_left_x=229)

(a) SW on Poincaré (SWp), GHSW

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-07.jpg?height=284&width=505&top_left_y=281&top_left_x=772)

(b) HHSW and Wasserstein

![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-07.jpg?height=298&width=523&top_left_y=266&top_left_x=1300)

(c) SW on Lorentz (SWl)

Figure 3: Comparison of the Wasserstein distance (with the geodesic distance as cost), GHSW, HHSW and SW between Wrapped Normal distributions. We gather the discrepancies together by scale of the values. SW on the Poincare model has very small values as it operates on the unit ball, while on the Lorentz model, it can take very high values. GHSW returns small values as the geodesic projections tend to project the points close to the origin. HHSW has values which are closer to the geodesic Wasserstein distance as the horospherical projection tends to better keep the distance between points.

stein and Sinkhorn with geodesic distances in $\mathbb{}^{2}$ for $n \in\left\{10^{2}, 10^{3}, 10^{4}, 5 \cdot 10^{4}, 10^{5}\right\}$ samples which are drawn from wrapped normal distributions (Nagano et al., 2019), and $L=200$ projections. We used the POT library (Flamary et al., 2021) to compute SW, Wasserstein and Sinkhorn. We observe the quasi-linearity complexity of GHSW and HHSW. When we only have a few samples, the cost of the projection is higher than computing the 1D Wasserstein distance, and $\mathrm{SW}$ is the fastest.

## 5. Application

In this Section, we perform several experiments which aim at comparing GHSW, HHSW, SWp and SWl. First, we study the evolution of the different distances between wrapped normal distributions which move along geodesics. Then, we illustrate the ability to fit distributions on $\mathbb{■}^{2}$ using gradient flows. Finally, we use HHSW and GHSW for an image classification problem where they are used to fit a prior in the embedding space. We add more informations about distributions and optimization in hyperbolic spaces in Appendix C. Complete details of the experimental settings are reported in Appendix D. We also report in Appendix D. 4 preliminary experiments on autoencoders with hierarchical latent priors.

## Comparisons of the Different Hyperbolical SW Discrep-

 ancies. On Figure 3, we compare the evolutions of GHSW, HHSW, SW and Wasserstein with the geodesic distance between Wrapped Normal Distributions (WNDs), where one is centered and the other moves along a geodesic. More precisely, by denoting $\mathcal{G}(\mu, \Sigma)$ a WND, we plot the evolution of the distances between $\mathcal{G}\left(x^{0}, I_{2}\right)$ and $\mathcal{G}\left(x_{t}, I_{2}\right)$ where $x_{t}=\cosh (t) x^{0}+\sinh (t) v$ for $t \in[-10,10]$ and $v \in T_{x^{0}} \llbracket^{2} \cap S^{2}$. We observe first that SW on the Lorentz model explodes when the two distributions are getting far from each other. Then, we observe that $H H S W_{2}$ has values with a scale similar to $W_{2}$. We argue that it comes from the observation of Chami et al. (2021) which stated that![](https://cdn.mathpix.com/cropped/2024_06_04_c50c8136bf5de83991c7g-07.jpg?height=800&width=786&top_left_y=868&top_left_x=1100)

Figure 4: Log 2-Wasserstein between a target and the gradient flow of GHSW, HHSW and SW (averaged over 5 runs).

the horospherical projection better preserves the distance between points compared to the geodesic projection. As SWp operates on the unit ball using Euclidean distances, the distances are very small, even for distributions close to the border. Interestingly, as geodesic projections tend to project points close to the origin, GHSW tends also to squeeze the distance between distributions far from the origin. This might reduce numerical instabilities when getting far from the origin, especially in the Lorentz model. This experiments also allows to observe that, at least for WNDs, the indiscernible property is observed in practice as we only obtain one minimum when both measures coincide. Hence, it suggests that GHSW and HHSW are proper distances.

Gradient Flows. We now assess the ability to learn distributions by minimizing the hyperbolic SW discrepancies
$(H S W)$. We suppose that we have a target distribution $\nu$ from which we have access to samples $\left(x_{i}\right)_{i=1}^{n}$. Therefore, we aim at learning $\nu$ by solving the following optimization problem: $\min _{\mu} H S W_{2}^{2}\left(\mu, \frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}\right)$. We model $\mu$ as a set of $n=500$ particles and propose to perform a Riemannian gradient descent (Boumal, 2022) to learn the distribution.

To compare the dynamics of the different discrepancies, we plot on Figure 4 the evolution of the exact $\log 2$-Wasserstein distance, with geodesic distance as ground cost, between the learned distribution at each iteration and the target, with the same learning rate. We use as targets wrapped normal distributions and mixtures of WNDs. For each type of target, we consider two settings, one in which the distribution is close to the origin and another in which the distribution lies closer to the border. We observe different behaviors in the two settings. When the target is lying close to the origin, SW1 and HHSW, which present the biggest magnitude, are the fastest to converge. As for distant distributions however, GHSW converges the fastest. Moreover, SW1 suffers from many numerical instabilities, as the projections of the gradients do not necessarily lie on the tangent space when points are too far of the origin. This requires to lower the learning rate, and hence to slow down the convergence. Interestingly, SWp is the slowest to converge in both settings.

Deep Classification with Prototypes. We now turn to a classification use case with real world data. Let $\left\{\left(x_{i}, y_{i}\right)_{i=1}^{n}\right\}$ be a training set where $x_{i} \in \mathbb{R}^{m}$ and $y_{i} \in$ $\{1, \ldots, C\}$ denotes a label. Ghadimi Atigh et al. (2021) perform classification on the Poincaré ball by assigning to each class $c \in\{1, \ldots, C\}$ a prototype $p_{c} \in S^{d-1}$, and then by learning an embedding on the hyperbolic space using a neural network $f_{\theta}$ followed by the exponential map. Then, by denoting by $z=\exp _{0}\left(f_{\theta}(x)\right)$ the output, the loss to be minimized is, for a regularization parameter $s \geq 0$,

$$
\begin{equation*}
\ell(\theta)=\frac{1}{n} \sum_{i=1}^{n}\left(B^{p_{y_{i}}}\left(z_{i}\right)-s d \cdot \log \left(1-\left\|z_{i}\right\|_{2}^{2}\right)\right) \tag{33}
\end{equation*}
$$

The first term is the Busemann function which will draw the representations of $x_{i}$ towards the prototype assigned to the class $y_{i}$, while the second term penalizes the overconfidence and pulls back the representation towards the origin. Ghadimi Atigh et al. (2021) showed that the second term can be decisive to improve the accuracy. Then, the classification of an input is done by solving $y^{*}=\operatorname{argmax}_{c}\left\langle\frac{z}{\|z\|}, p_{c}\right\rangle$.

We propose to replace the second term by a global prior on the distribution of the representations. More precisely, we add a discrepancy $D$ between the distribution $\left(\exp _{0} \circ f_{\theta}\right)_{\#} p_{X}$, where $p_{X}$ denotes the distribution of the training set, and a mixture of $C$ WNDs where the centers are chosen as $\left(\alpha p_{c}\right)_{c=1}^{C}$, with $\left(p_{c}\right)_{c}$ the prototypes and $0<$
Table 1: Test Accuracy on deep classification with prototypes (best performance in bold)

|  | CIFAR10 |  | CIFAR100 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Dimensions | 2 | 4 | 3 | 5 | 10 |
| PeBuse | $90.64_{ \pm 0.06}$ | $90.59_{ \pm 0.11}$ | $49.28_{ \pm 1.95}$ | $53.44_{ \pm 0.76}$ | $59.19_{ \pm 0.39}$ |
| GHSW | $91.39_{ \pm 0.23}$ | $91.66_{ \pm 0.27}$ | $\mathbf{5 3 . 9 7} \mathbf{7}_{ \pm 1.35}$ | $60.64_{ \pm 0.87}$ | $61.45_{ \pm 0.41}$ |
| HHSW | $91.28_{ \pm 0.26}$ | $\mathbf{9 1 . 9 8 _ { \pm 0 . 0 5 }}$ | $53.88_{ \pm 0.06}$ | $\mathbf{6 0 . 6 9} 9_{ \pm 0.25}$ | $\mathbf{6 2 . 8 0}_{ \pm 0.09}$ |
| SWp | $\mathbf{9 1 . 8 4 _ { \pm 0 . 3 1 }}$ | $91.68_{ \pm 0.10}$ | $53.25_{ \pm 3.27}$ | $59.77_{ \pm 0.81}$ | $60.36_{ \pm 1.26}$ |
| SWl | $91.13_{ \pm 0.14}$ | $91.74_{ \pm 0.12}$ | $53.88_{ \pm 0.02}$ | $60.62_{ \pm 0.39}$ | $62.30_{ \pm 0.23}$ |
| W | $91.67_{ \pm 0.18}$ | $91.83_{ \pm 0.21}$ | $50.07_{ \pm 4.58}$ | $57.49_{ \pm 0.94}$ | $58.82_{ \pm 1.66}$ |
| MMD | $91.47_{ \pm 0.10}$ | $91.68_{ \pm 0.09}$ | $50.59_{ \pm 4.44}$ | $58.10_{ \pm 0.73}$ | $58.91_{ \pm 0.91}$ |

$\alpha<1$. In practice, we use $D=G H S W_{2}^{2}, D=H H S W_{2}^{2}$, $D=S W p_{2}^{2}$ and $D=S W l_{2}^{2}$ to assess their usability on a real problem and compared with $W_{2}^{2}$ and MMD with Laplacian kernel (Feragen et al., 2015). Let $\left(w_{i}\right)_{i=1}^{n}$ be a batch of points drawn from this mixture, then the loss we minimize is

$$
\begin{equation*}
\ell(\theta)=\frac{1}{n} \sum_{i=1}^{n} B^{p_{i}}\left(z_{i}\right)+\lambda D\left(\frac{1}{n} \sum_{i=1}^{n} \delta_{z_{i}}, \frac{1}{n} \sum_{i=1}^{n} \delta_{w_{i}}\right) \tag{34}
\end{equation*}
$$

On Table 1, we report the classification accuracy on the test set for CIFAR10 and CIFAR100 (Krizhevsky, 2009), using the exact same setting as (Ghadimi Atigh et al., 2021). We rerun their method, called PeBuse here. We report results averaged over 3 runs. We observe that the proposed penalization outperforms the original method for all the different dimensions.

## 6. Conclusion and Discussion

In this work, we propose different sliced-Wasserstein discrepancies between distributions lying in hyperbolic spaces. In particular, we introduce two new SW discrepancies which are intrinsically defined on hyperbolic spaces. They are built by first identifying a closed-form for the Wasserstein distance on geodesics, and then by using different projections on the geodesics. We compare these metrics on multiple tasks such as sampling and image classification. We observe that, while Euclidean SW in the ambient space still works, it suffers from either slow convergence on the Poincaré ball or numerical instabilities on the Lorentz model when distributions are lying far from the origin. On the other hand, geodesic versions exhibit the same complexity and converge generally better for gradient flows. Further works will look into other tasks where hyperbolic embeddings and distributions have been showed to be beneficial, such as persistent diagrams (Carriere et al., 2017; Kyriakis et al., 2021). Besides further applications, proving that these discrepancies are indeed distances, and deriving statistical results are interesting directions of work. One might also consider different subspaces on which to project, such as horocycles which are circles of infinite radius and which can be seen as another analog object to lines in hyperbolic spaces (Casadio Tarabusi \& Picardello, 2021).

## Acknowledgements

This research was funded by project DynaLearn from Labex CominLabs and Region Bretagne ARED DLearnMe, and by the project OTTOPIA ANR-20-CHIA-0030 of the French National Research Agency (ANR).

## References

Absil, P.-A., Mahony, R., and Sepulchre, R. Optimization algorithms on matrix manifolds. In Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2009 .

Alvarez-Melis, D., Mroueh, Y., and Jaakkola, T. Unsupervised hierarchy matching with optimal transport over hyperbolic spaces. In International Conference on Artificial Intelligence and Statistics, pp. 1606-1617. PMLR, 2020.

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International conference on machine learning, pp. 214-223. PMLR, 2017.

Berenstein, C. A. and Rubin, B. Radon transform of lp-functions on the lobachevsky space and hyperbolic wavelet transforms. 1999.

Berenstein, C. A. and Rubin, B. Totally geodesic radon transform of 1 p-functions on real hyperbolic space. In Fourier analysis and convexity, pp. 37-58. Springer, 2004.

Boman, J. and Lindskog, F. Support theorems for the radon transform and cramér-wold theorems. Journal of theoretical probability, 22(3):683-710, 2009.

Bonet, C., Courty, N., Septier, F., and Drumetz, L. Efficient gradient flows in sliced-wasserstein space. Transactions on Machine Learning Research, 2022.

Bonet, C., Berg, P., Courty, N., Septier, F., Drumetz, L., and Pham, M.-T. Spherical sliced-wasserstein. In International Conference on Learning Representations, 2023.

Bonnabel, S. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58 (9):2217-2229, 2013.

Bonneel, N., Rabin, J., Peyré, G., and Pfister, H. Sliced and radon wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22-45, 2015.

Bonnotte, N. Unidimensional and evolution methods for optimal transportation. PhD thesis, Paris 11, 2013.

Bose, J., Smofsky, A., Liao, R., Panangaden, P., and Hamilton, W. Latent variable modelling with hyperbolic normalizing flows. In International Conference on Machine Learning, pp. 1045-1055. PMLR, 2020.
Boumal, N. An introduction to optimization on smooth manifolds. To appear with Cambridge University Press, Apr 2022. URL http://www.nicolasboumal.net/ book.

Bray, W. and Rubin, B. Radon transforms over lowerdimensional horospheres in real hyperbolic space. Transactions of the American Mathematical Society, 372(2): $1091-1112,2019$.

Bray, W. O. and Rubin, B. Inversion of the horocycle transform on real hyperbolic spaces via a wavelet-like transform. In Analysis of divergence, pp. 87-105. Springer, 1999 .

Bridson, M. R. and Haefliger, A. Metric spaces of nonpositive curvature, volume 319. Springer Science \& Business Media, 2013.

Carriere, M., Cuturi, M., and Oudot, S. Sliced wasserstein kernel for persistence diagrams. In International conference on machine learning, pp. 664-673. PMLR, 2017.

Casadio Tarabusi, E. and Picardello, M. A. Radon transforms in hyperbolic spaces and their discrete counterparts. Complex Analysis and Operator Theory, 15(1): $1-40,2021$.

Cetin, E., Chamberlain, B., Bronstein, M., and Hunt, J. J. Hyperbolic deep reinforcement learning. arXiv preprint arXiv:2210.01542, 2022.

Chami, I., Gu, A., Nguyen, D. P., and Ré, C. Horopca: Hyperbolic dimensionality reduction via horospherical projections. In International Conference on Machine Learning, pp. 1419-1429. PMLR, 2021.

Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9): $1853-1865,2016$.

Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.

Deshpande, I., Zhang, Z., and Schwing, A. G. Generative modeling using the sliced wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3483-3491, 2018.

Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch wasserstein : asymptotic and gradient properties. In Chiappa, S. and Calandra, R. (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 2131-2141. PMLR, 26-28 Aug

2020. URL https://proceedings.mlr.press/ v108/fatras20a.html.

Feragen, A., Lauze, F., and Hauberg, S. Geodesic exponential kernels: When curvature and linearity conflict. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3032-3042, 2015.

Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., et al. Pot: Python optimal transport. J. Mach. Learn. Res., 22(78):1-8, 2021.

Fournier, N. and Guillin, A. On the rate of convergence in wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3):707-738, 2015.

Galaz-Garcia, F., Papamichalis, M., Turnbull, K., Lunagomez, S., and Airoldi, E. Wrapped distributions on homogeneous riemannian manifolds. arXiv preprint arXiv:2204.09790, 2022.

Ganea, O., Bécigneul, G., and Hofmann, T. Hyperbolic entailment cones for learning hierarchical embeddings. In International Conference on Machine Learning, pp. 1646-1655. PMLR, 2018a.

Ganea, O., Bécigneul, G., and Hofmann, T. Hyperbolic neural networks. Advances in neural information processing systems, 31, 2018b.

Gelfand, I. M., Graev, M. I., and Vilenkin, N. I. Generalized Functions-Volume 5. Integral Geometry and Representation Theory. Academic Press, 1966.

Ghadimi Atigh, M., Keller-Ressel, M., and Mettes, P. Hyperbolic busemann learning with ideal prototypes. Advances in Neural Information Processing Systems, 34:103-115, 2021.

Gupte, M., Shankar, P., Li, J., Muthukrishnan, S., and Iftode, L. Finding hierarchy in directed online social networks. In Proceedings of the 20th international conference on World wide web, pp. 557-566, 2011.

Hagberg, A. A., Schult, D. A., and Swart, P. J. Exploring network structure, dynamics, and function using networkx. In Varoquaux, G., Vaught, T., and Millman, J. (eds.), Proceedings of the 7th Python in Science Conference, pp. 11 - 15, Pasadena, CA USA, 2008.

Hamzaoui, M., Chapel, L., Pham, M.-T., and Lefèvre, S. Hyperbolic variational auto-encoder for remote sensing scene classification. In ORASIS 2021, 2021.

Heitz, E., Vanhoey, K., Chambon, T., and Belcour, L. A sliced wasserstein loss for neural texture synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9412-9420, 2021.
Helgason, S. Differential operators on homogeneous spaces. Acta mathematica, 102(3-4):239-299, 1959.

Hoyos-Idrobo, A. Aligning hyperbolic representations: an optimal transport-based approach. arXiv preprint arXiv:2012.01089, 2020.

Izumiya, S. Horospherical geometry in the hyperbolic space. In Noncommutativity and Singularities: Proceedings of French-Japanese symposia held at IHÉS in 2006, volume 55, pp. 31-50. Mathematical Society of Japan, 2009.

Khrulkov, V., Mirvakhabova, L., Ustinova, E., Oseledets, I., and Lempitsky, V. Hyperbolic image embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6418-6428, 2020.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

Kolouri, S., Pope, P. E., Martin, C. E., and Rohde, G. K. Sliced wasserstein auto-encoders. In International Conference on Learning Representations, 2018.

Kolouri, S., Nadjahi, K., Simsekli, U., Badeau, R., and Rohde, G. Generalized sliced wasserstein distances. Advances in neural information processing systems, 32, 2019.

Krizhevsky, A. Learning multiple layers of features from tiny images, 2009.

Kusner, M., Sun, Y., Kolkin, N., and Weinberger, K. From word embeddings to document distances. In International conference on machine learning, pp. 957-966. PMLR, 2015.

Kyriakis, P., Fostiropoulos, I., and Bogdan, P. Learning hyperbolic representations of topological features. arXiv preprint arXiv:2103.09273, 2021.

LeCun, Y. and Cortes, C. MNIST handwritten digit database. 2010. URL http://yann. lecun.com/ exdb/mnist/.

Lee, J. M. Riemannian manifolds: an introduction to curvature, volume 176. Springer Science \& Business Media, 2006.

Li, T., Meng, C., Yu, J., and Xu, H. Hilbert curve projection distance for distribution comparison. arXiv preprint arXiv:2205.15059, 2022.

Lin, T., Fan, C., Ho, N., Cuturi, M., and Jordan, M. Projection robust wasserstein distance and riemannian optimization. Advances in neural information processing systems, 33:9383-9397, 2020.

Lin, T., Zheng, Z., Chen, E., Cuturi, M., and Jordan, M. I. On projection robust optimal transport: Sample complexity and model misspecification. In International Conference on Artificial Intelligence and Statistics, pp. 262-270. PMLR, 2021.

Liu, Q., Nickel, M., and Kiela, D. Hyperbolic graph neural networks. Advances in Neural Information Processing Systems, 32, 2019.

Liu, S., Chen, J., Pan, L., Ngo, C.-W., Chua, T.-S., and Jiang, Y.-G. Hyperbolic visual embedding learning for zero-shot recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9273-9281, 2020.

Liutkus, A., Simsekli, U., Majewski, S., Durmus, A., and Stöter, F.-R. Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pp. 4104-4113. PMLR, 2019.

Lou, A., Lim, D., Katsman, I., Huang, L., Jiang, Q., Lim, S. N., and $\mathrm{De} \mathrm{Sa}, \mathrm{C}$. M. Neural manifold ordinary differential equations. Advances in Neural Information Processing Systems, 33:17548-17558, 2020.

Mathieu, E., Le Lan, C., Maddison, C. J., Tomioka, R., and Teh, Y. W. Continuous hierarchical representations with poincaré variational auto-encoders. Advances in neural information processing systems, 32, 2019.

McCann, R. J. Polar factorization of maps on riemannian manifolds. Geometric \& Functional Analysis GAFA, 11 (3):589-608, 2001.

Mettes, P., van der Pol, E., and Snoek, C. Hyperspherical prototype networks. Advances in neural information processing systems, 32, 2019.

Nadjahi, K., Durmus, A., Chizat, L., Kolouri, S., Shahrampour, S., and Simsekli, U. Statistical and topological properties of sliced probability divergences. Advances in Neural Information Processing Systems, 33:2080220812, 2020.

Nagano, Y., Yamaguchi, S., Fujita, Y., and Koyama, M. A wrapped normal distribution on hyperbolic space for gradient-based learning. In International Conference on Machine Learning, pp. 4693-4702. PMLR, 2019.

Nickel, M. and Kiela, D. Poincaré embeddings for learning hierarchical representations. Advances in neural information processing systems, 30, 2017.

Nickel, M. and Kiela, D. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In International Conference on Machine Learning, pp. 3779-3788. PMLR, 2018.
Ovinnikov, I. Poincaré wasserstein autoencoder. arXiv preprint arXiv:1901.01427, 2019.

Park, J., Cho, J., Chang, H. J., and Choi, J. Y. Unsupervised hyperbolic representation learning via message passing auto-encoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. $5516-5526,2021$.

Paty, F.-P. and Cuturi, M. Subspace robust wasserstein distances. In International conference on machine learning, pp. 5072-5081. PMLR, 2019.

Peng, W., Varanka, T., Mostafa, A., Shi, H., and Zhao, G. Hyperbolic deep neural networks: A survey. arXiv preprint arXiv:2101.04562, 2021.

Pennec, X. Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements. Journal of Mathematical Imaging and Vision, 25(1):127-154, 2006.

Peyré, G., Cuturi, M., et al. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, 2019.

Rabin, J., Peyré, G., Delon, J., and Bernot, M. Wasserstein barycenter and its application to texture mixing. In International Conference on Scale Space and Variational Methods in Computer Vision, pp. 435-446. Springer, 2011.

Rakotomamonjy, A., Alaya, M. Z., Berar, M., and Gasso, G. Statistical and topological properties of gaussian smoothed sliced probability divergences. arXiv preprint arXiv:2110.10524, 2021.

Rubin, B. Radon, cosine and sine transforms on real hyperbolic space. Advances in Mathematics, 170(2):206-223, 2002.

Rustamov, R. M. and Majumdar, S. Intrinsic sliced wasserstein distances for comparing collections of probability distributions on manifolds and graphs. arXiv preprint arXiv:2010.15285, 2020.

Said, S., Bombrun, L., and Berthoumieu, Y. New riemannian priors on the univariate normal model. Entropy, 16 (7):4015-4031, 2014.

Sala, F., De Sa, C., Gu, A., and Ré, C. Representation tradeoffs for hyperbolic embeddings. In International conference on machine learning, pp. 4460-4469. PMLR, 2018.

Sarkar, R. Low distortion delaunay embedding of trees in hyperbolic plane. In International Symposium on Graph Drawing, pp. 355-366. Springer, 2011.

Tifrea, A., Bécigneul, G., and Ganea, O.-E. Poincaré glove: Hyperbolic word embeddings. arXiv preprint arXiv:1810.06546, 2018.

Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017.

Villani, C. Topics in optimal transportation, volume 58. American Mathematical Soc., 2003.

Villani, C. Optimal transport: old and new, volume 338. Springer, 2009.

Wilson, B. and Leimeister, M. Gradient descent in hyperbolic space. arXiv preprint arXiv:1805.08207, 2018.

Yu, K., Visweswaran, S., and Batmanghelich, K. Semisupervised hierarchical drug embedding in hyperbolic space. Journal of chemical information and modeling, 60 (12):5647-5657, 2020.
