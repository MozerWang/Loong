# Nova ${ }^{+}$: Generative Language Models for Binaries 

Nan Jiang ${ }^{1}$ Chengxiao Wang ${ }^{21}$ Kevin Liu $^{3}$ Xiangzhe Xu ${ }^{1}$ Lin Tan ${ }^{1}$ Xiangyu Zhang ${ }^{1}$


#### Abstract

Generative large language models (LLMs) pretrained on code have shown impressive effectiveness in code generation, program repair, and document analysis. However, existing generative LLMs focus on source code and are not specialized for binaries. There are three main challenges for LLMs to model and learn binary code: hexdecimal values, complex global dependencies, and compiler optimization levels. To bring the benefit of LLMs to the binary domain, we develop Nova and Nova ${ }^{+}$, which are LLMs pre-trained on binary corpora. Nova is pre-trained with the standard language modeling task, showing significantly better capability on five benchmarks for three downstream tasks: binary code similarity detection (BCSD), binary code translation (BCT), and binary code recovery (BCR), over GPT-3.5 and other existing techniques. We build Nova ${ }^{+}$ to further boost Nova using two new pre-training tasks, i.e., optimization generation and optimization level prediction, which are designed to learn binary optimization and align equivalent binaries. $\mathrm{Nova}^{+}$shows overall the best performance for all three downstream tasks on five benchmarks, demonstrating the contributions of the new pretraining tasks.


## 1. Introduction

Large language models pre-trained on source code have brought improvement in various software development domains, such as code generation (Chen et al., 2022a; Liu et al., 2023; Chen et al., 2023; Le et al., 2022), automated program repair (Jiang et al., 2023; Xia et al., 2023), and software specification generation (Xie et al., 2023). Thanks to the massive amount of natural language text and source code the LLMs have seen during pre-training, pre-trained[^0]

LLMs can generate high-quality source code and text.

However, the source code corpus on which LLMs are pretrained is highly biased, with the most popular high-level programming languages (such as Python, Java, JavaScript, and $\mathrm{C} / \mathrm{C}++$ ) taking the majority (Li et al., 2023). Such bias of pre-training corpus makes the pre-trained LLMs not as good on low-resourced programming languages, such as binary (i.e., assembly) code.

Compared to source code and text, there are special challenges for LLMs to model and learn binary code: (1) Hexdecimal values, such as the addresses of instructions, e.g., the address 11c3 in Figure 1(b). Such instruction addresses are low-level, less informative, and long, which is very difficult for LLMs to learn; (2) Complex global dependency such as library function calls, global variables, and structured data, which are typically stored in the . data section. These content in the . data section are retrieved via the frame pointer (e.g., value 1.2322 in the source code is retrieved by $0 x e 3 d$ (\%rbp) in the assembly), requiring awareness of the data's address and calculation of the offset by the frame pointer; and (3) Optimization applied by compilers make the binary code of the same function look different when different optimization flags are used (e.g., o0 and O1), which LLMs cannot learn straightforwardly during training.

To fill in the gap, this work develops the first generative LLM for binary code, namely Nova, by pre-training an existing LLM with an additional binary corpus, which contains 2.14M X86-64 and ARM64 binary and their corresponding source code functions collected from existing binary similarity dataset (BinaryCorp (Wang et al., 2022)) and by compiling open-sourced $\mathrm{C}$ and $\mathrm{C}++$ dataset (The-Stack (Li et al., 2023)).

To prevent Nova from struggling with hex-decimal values and global dependencies, we normalize the binary code to ensure every binary function is self-contained, by replacing hex-decimal addresses and function calls with placeholders (e.g. number1 and func1 in figure 1(d), (e) and (f)). Such normalization makes binary functions feasible for LLMs to learn since LLMs do not need to know the out-of-scope global dependencies such as . data section. In addition, the normalization also makes LLMs focus more on the structure of binary functions (i.e., instruction used, control flow,

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-02.jpg?height=585&width=420&top_left_y=236&top_left_x=213)

(d) normalized source code

| $11 \mathrm{c}:$ <br> $11 \mathrm{cb}:$ <br> $11 \mathrm{~d}:$ <br> $11 \mathrm{~d} 8:$ <br> $11 \mathrm{dd}:$ <br> $11 \mathrm{e}:$ <br> $11 \mathrm{e}:$ <br> $11 \mathrm{eb}:$ | movsd <br> movsd <br> movsd <br> movsd <br> mov <br> movq <br> callq <br> movq | 0xe3d(\%rip), \%xmm0 <br> \%xmm0, -0x10(\%rbp) <br> 0xe38(\%rip), \%xmm0 <br> \%xmm0, -0x8(\%rbp) <br> $-0 \times 10$ (\%rbp), \%rax <br> \%rax, \%xmm0 <br> 10b0 <br> \%xmm0, \%rax |
| :---: | :---: | :---: |

(b) X86-64 assembly

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-02.jpg?height=263&width=602&top_left_y=557&top_left_x=642)

(e) normalized X86-64 assembly

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-02.jpg?height=260&width=588&top_left_y=236&top_left_x=1251)

(c) ARM64 assembly

| label7 | adrp | x0, 0 |
| :--- | :--- | :--- |
| label8 | ldr | d0, $[x 0$, number1] |
| label9 | str | d0, $[s p, \# 32]$ |
| label10 | adrp | x0, 0 |
| label11 | ldr | d0, $[x 0$, number2] |

(f) normalized ARM64 assembly

Figure 1. Example of the source code and its corresponding X86-64 and ARM64 assembly code.

and data dependency of registers), while traditional binary analysis techniques (Duck et al., 2020; Bauman et al., 2018; Zhang et al., 2021) can be used to add the exact addresses of instructions and names of library function calls to the result of Nova.

Moreover, to let Nova better learn the optimization of binary, we propose two novel pre-training tasks, training Nova to (1) optimize binary functions and to (2) predict the optimization flag used during compilation, which results in Nova ${ }^{+}$, a more robust binary LLM against different levels of optimization.

Lastly, to show the effectiveness of pre-training LLMs with binary corpus and our proposed pre-training tasks, We further fine-tune and evaluate Nova and $\mathrm{Nova}^{+}$on three downstream tasks: Binary Code Similarity Detection, Binary Code Translation, and Binary Code Recovery.

To sum up, this paper makes the following contributions:

- The first generative LLMs pre-trained on binary corpora, Nova and $\mathrm{Nova}^{+}$, filling the gap that no existing generative LLM is specialized on binary code.
- Two novel pre-training tasks, optimization generation and optimization-level prediction, which we are the first to use to make LLM learn the optimization of compilation. These tasks align binaries of the same functionality to help LLMs learn binary syntax and semantics more effectively.
- A comprehensive evaluation on three foundational downstream tasks, where Nova and $\mathrm{Nova}^{+}$outperform existing techniques, i.e., GPT-3.5, one of the state-ofthe-art general LLMs, and non-generative techniques for a single binary task. Further evaluation shows that the proposed optimization generation and optimization- level prediction pre-training tasks boost $\mathrm{Nova}^{+}$'s performance on downstream tasks.


## 2. Related Works

### 2.1. Large Language Models

LLMs demonstrate promising results on many software engineering tasks, such as code generation (Chen et al., 2022a; Liu et al., 2023; Chen et al., 2023; Le et al., 2022), automated program repair (Jiang et al., 2023; Xia et al., 2023), and software specification generation (Xie et al., 2023). The advances of LLMs may be attributed to the source code in their training datasets (Touvron et al., 2023; OpenAI, 2023), and the rich natural-language-like information in source code programs (e.g., code comments, variable names). In this paper, we are the first to extend the effectiveness of LLMs to binary programs. The distribution of binary programs differs significantly with the distribution of source code and natural language. Therefore, we propose a comprehensive data preprocessing pipeline and two novel pretraining tasks to train an LLM on binary corpora.

### 2.2. Binary Code Models

Machine learning models are widely-used in binary program analysis tasks (Pei et al., 2020; 2021a; Jin et al., 2022; Chen et al., 2022b; Xu et al., 2023b;a; Wang et al., 2022). However, these models are typically designed for specific downstream tasks such as binary code similarity detection (Pei et al., 2020; Xu et al., 2023a; Wang et al., 2022), variable name prediction (Chen et al., 2022b; Xu et al., 2023b), and binary code type inference (Pei et al., 2021a). In contrast, $\mathrm{Nova}^{+}$is a pre-trained binary code model that can be generalized to various downstream tasks, and it is shown outperforming the existing state-of-the-art techniques in three downstream tasks.

### 2.3. Disassembler

$\mathrm{Nova}^{+}$piggybacks on a disassembler to translate a binary program to a sequence of textual instructions. The quality of instructions generated by the disassembler may affect the performance of $\mathrm{Nova}^{+}$, while modern disassemblers achieve more than $95 \%$ accuracy in most cases (Ye et al., 2022; Pei et al., 2021b; Pang et al., 2021; Miller et al., 2019).

## 3. Approach

Figure 2 illustrates the overview of our approach: we collect and compile source code to obtain a binary corpus (Section 3.1 and Section 3.2), then we start from StarCoder to keep pre-training it with the language modeling task on the binary corpus to get the base generative binary LLM, Nova (Section 3.3). Nova is then further pre-trained with two new tasks designed for binary to enhance its understanding of binary optimization and alignment between equivalent binaries, resulting in our final binary LLM, Nova ${ }^{+}$(Section 3.3). Lastly, we fine-tune and evaluate both Nova and $\mathrm{Nova}^{+}$ on various downstream tasks to show their effectiveness (Section 3.4).

### 3.1. Data Collection

We build our binary data sets on top of existing data sets: The-Stack (Li et al., 2023) and the Binary-Corp-3M (Wang et al., 2022) datasets. We compile the source code into executables with different optimization levels (i.e., $00,01,02$ and 03), strip the executables to remove debug information as this is the most realistic setting, and decompile them into binary assembly code.

The-Stack is a source code corpus containing 3TB of source code of 358 programming languages. We compile 4 million C programs and 1 million $\mathrm{C}++$ programs, among which the compilation of $32,774 \mathrm{C}$ programs and $40,087 \mathrm{C}++$ programs complete successfully. Each compiled program is then stripped and disassembled into both X86-64 (using the g++ and objdump commands on Ubuntu 20.04.6 LTS) and ARM64 (using the gcc-aarch64-linux-gnu and aarch64-linux-gnu-objdump commands on Ubuntu 20.04.6 LST) binaries with 00 to 03 optimization flags. The breakdown is in Table 1.

Binary-Corp-3M contains X86-64 binaries compiled with various optimization flags (i.e., 00 to 03 ) for both training set and testing set. We strip (removing the debug information) and disassemble the binaries with the objdump command to obtain around 3 million binary functions. To filter low-quality data, we remove the compiler-generated functions (such as _init, register_tm_clone, etc.) and only keep user-defined functions, resulting in 1,213,381 binary functions for training. The breakdown of the training data is in Table 1. The binaries from the testing set are left

| Dataset | Arch. | \#Files | \#Functions |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | O0 | O1 | O2 | O3 |
| Binary-Corp-3M | X86-64 | 1,443 | 387,859 | 282,034 | 272,544 | 270,944 |
| The-Stack | X86-64 | 72,861 | 115,441 | 115,376 | 115,375 | 115,375 |
|  | ARM64 | 72,861 | 115,436 | 115,260 | 115,241 | 115,238 |

Table 1. Statistics of the pre-training datasets.

out for the downstream task's evaluation.

### 3.2. Data Preprocessing

To make binaries self-contained and feasible for LLMs to learn, we perform several steps to preprocess binaries:

Addresses: Every instruction address in a binary file is replaced with a placeholder such as label1, label2, and ..., which is the "index" of each instruction. A binary code with $\mathrm{N}$ instructions will have label1 to labelN as the instruction indices.

Functions: The callee function's address typically lies out of the caller's function, thus is different from anyone of label1 to labelN. These callee's addresses are replaced by another group of placeholders, such as func1 and func2. The same function call in source code and binaries are replaced with the same placeholder to ensure consistency. For example, function call asin in Figure 1(a) is refered as 10.b0 in X86-64 binary (Figure 1(b)) and 6£0 in ARM64 binary (Figure 1(c)), and they are all replaced by the same placeholder func1.

Numbers: Lastly, we convert the remaining hex-decimal values to decimal values and replace all the values larger than $255(0 \mathrm{xff})$ with number1, number2, etc. The rationale is that smaller values such as 16,32 are typically stack offsets that reflect the variable's size and can be learned, while larger values such as 0 xe38 are typically address offsets that require global dependency and are infeasible for LLMs to learn.

### 3.3. Pre-Training

Modern LLMs typically have been pre-trained on hundreds of millions of files containing both natural language text and source code, making them good at understanding natural language instructions, coding syntax, basic algorithms, etc (Li et al., 2023; Touvron et al., 2023; Brown et al., 2020). Although we focus on binary, such general knowledge about natural language and coding is still useful. Thus, we build our generative LLM on top of StarCoder (Li et al., 2023), one of the state-of-the-art LLMs on source code, instead of pre-training one from scratch. We further pre-train StarCoder with the binary data using the following tasks:

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-04.jpg?height=616&width=1696&top_left_y=207&top_left_x=190)

Figure 2. Overview of pre-training Nova and $\mathrm{Nova}^{+}$with language modeling, optimization generation, and optimization level prediction tasks, and then fine-tuning both LLMs on three downstream tasks.

### 3.3.1. LANGUAGE MODELING

Language modeling, also known as next-token-prediction for generative LLMs, is the standard task to pre-train a generative LLM. Given a binary code $\mathbf{b}=\left\{b_{1}, b_{2}, \ldots, b_{n}\right\}$ consists of $n$ tokens, the learning objective of language modeling is:

$$
\begin{equation*}
\mathrm{L}_{\mathrm{LM}}=-\sum_{i=2}^{n} \log \mathrm{P}\left(b_{i} \mid b_{1}, b_{2}, \ldots, b_{i-1}\right) \tag{1}
\end{equation*}
$$

which is the sum of the log-likelihood of every token conditioning on its prefixes in the binary code.

The LLM pre-trained with such language modeling task is named Nova, which not only keeps the knowledge about natural language and source code from StarCoder but also learns binary code and is able to complete binary code functions based on the prefix.

### 3.3.2. OPTIMIZATION GENERATION

Language modeling enables LLMs to learn the syntax and basic semantics of a language, but not complex reasoning. Thus, we design the optimization generation task to let the LLM be aware of the equivalency between differentlyoptimized binary code of the same source code.

Given a piece of source code, $\mathrm{s}$ and its corresponding binary code $\mathbf{b}^{\mathbf{i}}$ (i denotes that the binary code is compiled with Oi optimization), the input (notated by in) to the LLM is an instruction to request optimized assembly (e.g., the blue boxes in Figure 2): This is the source code: \{s\}. This is the assembly code with Oi optimization: $\left\{\mathbf{b}^{\mathbf{i}}\right\}$. Optimize the assembly code with Oj optimization.

The expected output is the binary (notated by out), $\mathbf{b}^{\mathbf{j}}=$ $\left\{b_{i}^{j}, b_{2}^{j}, \ldots, b_{n}^{j}\right\}$, of the same source code function $\mathbf{s}$, but compiled with the $\circ j$ level of optimization. The learning objective of this task is:

$$
\begin{align*}
\mathrm{L}_{\mathrm{OG}} & =-\log \mathrm{P}(\text { out } \mid \text { in })=-\log \mathrm{P}\left(\mathbf{b}^{\mathbf{j}} \mid \mathbf{i n}\right) \\
& =-\log \mathrm{P}\left(b_{1}^{j} \mid \mathbf{i n}\right)-\sum_{k=2}^{n} \log \mathrm{P}\left(b_{k}^{j} \mid b_{1}^{j}, \ldots, b_{k-1}^{j}, \mathbf{i n}\right) \tag{2}
\end{align*}
$$

Optimization generation trains LLM to learn data dependencies of binary instructions, understand how binary code can be optimized with more efficient instructions, and potentially learn equivalency between differently-optimized binary code (e.g., $\mathbf{b}^{\mathbf{i}}$ and $\mathbf{b}^{\mathbf{j}}$ are different binary code but essentially have the same functionality). Examples are given in the appendix A.1.

### 3.3.3. OPTIMIZATION-LEVEL PREDICTION

Optimization-level prediction is another pre-training task we use to train LLM to understand binary optimization. Different from generating binary code under certain optimization, this task requires the opposite to predict what optimization flag is used during compilation.

Given a piece of source code, $\mathrm{s}$ and its corresponding binary code $\mathrm{b}$, the input to the LLM is organized as a question to ask for the optimization level (e.g., the green boxes in Figure 2): This is the source code: \{s\}. This is the assembly code of the same function: $\{\mathbf{b}\}$. What optimization level does the compiler use?

The expected output is a short answer, Answer: Oi, where Oi is the optimization flag used during compilation to get the input binary $\mathbf{b}$. The learning objective of this task is the log-likelihood of the output answer conditioned on the input question (similar to equation 2). Examples are given in the appendix A.2

We use optimization generation and optimization level prediction to further pre-train Nova. The training data of the two tasks are mixed and train Nova to learn both tasks at the same time, resulting in $\mathbf{N o v a}^{+}$, a generative binary LLM with an enhanced understanding of binary optimization.

### 3.4. Fine-Tuning

To show the effectiveness of pre-training LLMs with binaries, we select three downstream tasks and fine-tune both Nova and $\mathrm{Nova}^{+}$on them.

### 3.4.1. BinARY Code SiMilaRitY DeteCtion

Binary code similarity detection (BCSD) is a foundational domain that aims to measure the similarity between two binary code snippets (Wang et al., 2022). BCSD is useful in various applications such as plagiarism detection (Luo et al., 2014; Sæbjørnsen et al., 2009), known vulnerability detection (David \& Yahav, 2014; David et al., 2018; 2017; 2016), malware detection (Cesare et al., 2014) and so on.

Given a query binary and a pool of candidate binaries, the goal of BCSD is to retrieve the most similar binary to the query from the pool regarding their functionality.

A widely used setting of BCSD is taking a query binary (e.g., the $b^{q}$ in Figure 12), and a pool of $k$ binaries compiled with different optimization from the query, where the pool contains one binary named positive candidate (e.g. $\mathbf{b}^{+}$in Figure 12) that comes from the same source code function as the query binary and $k-1$ negative candidates come from different source code functions from the query binary (e.g., $\mathbf{b}^{-}$in Figure 12). We want to fine-tune Nova ${ }^{+}$to encode these binaries in an embedding space where the positive candidate has the highest similarity with the query binary among the pool. That is, the learning objective of BCSD is:

$$
\begin{equation*}
\mathrm{L}_{\mathrm{BCSD}}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}^{\mathbf{q}}, \mathbf{h}^{+}\right)}}{e^{\operatorname{sim}\left(\mathbf{h}^{\mathbf{q}}, \mathbf{h}^{+}\right)}+\Sigma_{i=1}^{K-1} e^{\operatorname{sim}\left(\mathbf{h}^{\mathbf{q}}, \mathbf{h}_{\mathbf{i}}^{-}\right)}} \tag{3}
\end{equation*}
$$

, where sim() is the cosine similarity metrics, $\mathbf{h}^{\mathbf{q}}, \mathbf{h}^{+}, \mathbf{h}^{-}$ are the embedding of binaries $\mathbf{b}^{\mathbf{q}}, \mathbf{b}^{+}$, and $\mathbf{b}^{-}$.

### 3.4.2. Binary Code TranSlation

Binary code translation (BCT) translates binary code between different assembly languages, such as translating X86-64 binary into ARM64 and vice versa. BCT is useful to help developers automatically transfer binary executables across architectures when recompilation is unavailable.

Figure 4 shows that BCT can be modeled as an autoregressive generative process that given the binary in one architecture (e.g., X86), $\mathbf{b}^{\mathbf{x}}$, we format the input as an instruction (notated by in): Translate the X86-64 assembly to ARM64 assembly: $\left\{\mathbf{b}^{\mathbf{x}}\right\}$. And the model

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-05.jpg?height=490&width=827&top_left_y=221&top_left_x=1061)

Figure 3. Fine-tuning $\mathrm{Nova}^{+}$on the binary code similarity task. is fine-tuned to generate the binary in the target architecture, $\mathbf{b}^{\mathbf{a}}$, using the learning objective:

$$
\begin{align*}
\mathrm{L}_{\mathrm{BCT}} & =-\log \mathrm{P}\left(\mathbf{b}^{\mathbf{a}} \mid \mathbf{i n}\right) \\
& =-\log \mathrm{P}\left(b_{1}^{a} \mid \mathbf{i n}\right)-\sum_{i=2}^{n} \log \mathrm{P}\left(b_{i}^{a} \mid b_{1}^{a}, \ldots, b_{i-1}^{a}, \mathbf{i n}\right) \tag{4}
\end{align*}
$$

, and translating from ARM binary to X86 binary is modeled in the same way.

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-05.jpg?height=344&width=827&top_left_y=1248&top_left_x=1061)

Figure 4. Fine-tuning Nova $^{+}$on the binary code translation task.

### 3.4.3. BINARY CODE RECOVERY

Binary code recovery (BCR) helps developers to understand binary code by recovering binary code into more readable high-level source code (e.g., C and C++ programs) (Fu et al., 2019; Liang et al., 2021).

As shown in Figure 5, the input to the model for BCR is also formatted as an instruction (notated by in): Translate the X86-64 (or ARM64) assembly to C++ source code: $\{\mathbf{b}\}$. $\mathbf{b}$ is the binary code to recover, and the model is fine-tuned to generate the source code $\mathrm{s}$ using the learning objective:

$$
\begin{align*}
\mathrm{L}_{\mathrm{BCR}} & =-\log \mathrm{P}(\mathbf{s} \mid \mathbf{i n}) \\
& =-\log \mathrm{P}\left(s_{1} \mid \mathbf{i n}\right)-\sum_{i=2}^{n} \log \mathrm{P}\left(s_{i} \mid s_{1}, \ldots, s_{i-1}, \mathbf{i n}\right) \tag{5}
\end{align*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-06.jpg?height=358&width=832&top_left_y=222&top_left_x=186)

Figure 5. Fine-tuning $\mathrm{Nova}^{+}$on the binary code recovery task.

## 4. Experimental Setup

### 4.1. Pre-Training

We pre-train Nova starting from StarCoder-1B (Li et al., 2023), which is an auto-regressive decoder-only Transformer with multi-query attention mechanism (Shazeer, 2019). We use the AdamW (Loshchilov \& Hutter, 2019) optimizer to update the model's weights. The learning rate is set to $5 e^{-5}$ with 1000 steps of linear warm-up and followed by a cosine decay. The batch size is set to 64 and the model is pre-trained for one epoch on the pre-training data.

The pre-training of $\mathrm{Nova}^{+}$starts from the Nova model, using the optimization generation and optimization level prediction data. The optimizer, learning rate, batch size, and number of training epochs are set the same as above.

### 4.2. Fine-Tuning for Binary Code Similarity Detection

We fine-tune both Nova and Nova ${ }^{+}$on the BCSD task using the Binary-Corp-3M dataset, which is also used by existing works (Wang et al., 2022). During fine-tuning, the training functions are grouped with the pool size $\mathrm{K}$ set to 8 . We use AdamW optimizer with the learning rate set to $3 e^{-5}$ and use 1000 steps of warm-up and a cosine decay to adjust the learning rate. The batch size is set to 16 and the model is fine-tuned for two epochs.

For testing, we randomly sample 40,000 binary functions (the $00,01,02$, and 03 binaries of 10,000 unique source code functions) from the Binary-Corp-3M testing set since the whole testing set is too large.

### 4.3. Fine-Tuning for Binary Code Translation

We fine-tune models on the BCT task using two datasets:

Generated: following existing works (Fu et al., 2019; Liang et al., 2021), we generate a synthetic dataset that consists of $\mathrm{C}$ programs with function calls in the math. h library, common operations ( $+,-, *, /, \%, \& \&,||,>,<,==$, etc.), arrays and loops. We generate 60,000 programs for training and 600 for testing. These programs are compiled into X86-64 and ARM64 binaries with no optimization (o0) for binary code translation purposes.
Codeflaws: besides a generated benchmark, we use Codeflaws (Tan et al., 2017) as a real-world dataset whose programs are collected from code contests platforms and are more challenging. We split the programs into a training set with 3,294 programs and a test set with 364 programs. These programs are compiled into X86-64 and ARM64 binaries with 00 to 03 optimization levels.

On both datasets, the models are fine-tuned for two epochs, with a batch size of 4 . The optimizer is AdamW with the learning rate set to $5 e^{-5}$, and the learning rate is adjusted with 1000 steps of linear warm-up and a cosine decay.

### 4.4. Fine-Tuning for Binary Code Recovery

We use the same two datasets, Generated and Codeflaws, for the BCR task to fine-tune the models to recover the source code program from the X86-64 and ARM64 binaries. The hyper-parameters of fine-tuning are the same as fine-tuning models for the BCT task.

### 4.5. Baselines

For the BCSD task, we use the existing state-of-the-art technique jTrans (Wang et al., 2022) as the baseline, which is a BERT (Devlin et al., 2018) model trained on binaries with masked token prediction and jump target prediction tasks (Wang et al., 2022). As jTrans is an encoder model and is designed for BCSD, it can only be used for this task.

For the BCT task, to our best effort, we cannot find any existing baseline, so we query GPT-3.5 (OpenAI, 2022) with few-shot learning (Brown et al., 2020) on both the Generated and the Codeflaws benchmarks as the baseline. As GPT-3.5 is one of the most powerful generative LLMs that can be generalized to many tasks, applying it to BCT gives us a strong baseline.

For the BCR task, in addition to GPT-3.5, we use two additional baselines, Coda (Fu et al., 2019) and Neutron (Liang et al., 2021), which are deep-learning-based techniques designed for binary code recovery. Since Coda and Neutron are not open-sourced, we can not reproduce their results but directly take the reported evaluation on generated programs (i.e., a similar benchmark as the Generated benchmark we create) from their papers. We apply GPT-3.5 on both the Generated and the Codeflaws benchmarks.

Note that we apply GPT-3.5 on the BCT and BCR tasks but not the BCSD task, as we cannot obtain the binaries' embeddings from the GPT-3.5, and also few-shot learning cannot easily give us the ranking of similarities.

### 4.6. Infrastructure

Nova and $\mathrm{Nova}^{+}$are implemented using PyTorch, HuggingFace, and DeepSpeed (Aminabadi et al., 2022) (for

| Models | O0 - O1 | $\mathrm{O} 0$ - O2 | $\mathrm{O} 0$ - O3 | $\mathrm{O1}-\mathrm{O2}$ | $\mathrm{O1}-\mathrm{O3}$ | $\mathrm{O2}-\mathrm{O3}$ | O1 - O0 | $\mathrm{O} 2-\mathrm{O} 0$ | $\mathrm{O3}-\mathrm{O0}$ | $\mathrm{O2}-\mathrm{O1}$ | $\mathrm{O3}-\mathrm{O1}$ | $\mathrm{O3}-\mathrm{O2}$ | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| jTrans | 0.909 | 0.893 | 0.878 | 0.922 | 0.922 | 0.969 | 0.909 | 0.867 | 0.852 | 0.922 | 0.922 | 0.969 | 0.911 |
| Nova | 0.891 | 0.889 | 0.883 | 0.932 | 0.932 | 0.969 | 0.932 | 0.945 | 0.932 | 0.932 | 0.932 | 0.969 | 0.928 |
| Nova $^{+}$ | 0.893 | 0.917 | 0.901 | 0.938 | 0.938 | 0.953 | 0.917 | 0.932 | 0.932 | 0.917 | 0.938 | 0.969 | 0.929 |

Table 2. MRR of binary code similarity ranking produced by jTrans, Nova, and Nova ${ }^{+}$, with pool size $\mathrm{K}=32$

| Models | $\mathrm{O} 0-\mathrm{O1}$ | $\mathrm{O} 0$ - O2 | $\mathrm{O} 0$ - $\mathbf{O 3}$ | O1 - O2 | $\mathbf{O 1 - 0 3}$ | $\mathrm{O2}-\mathrm{O3}$ | $\mathbf{O 1}-\mathrm{O} 0$ | $\mathbf{O 2}-\mathbf{O 0}$ | $\mathrm{O3}-\mathrm{O0}$ | $\mathbf{O 2}-\mathbf{O 1}$ | $\mathrm{O3}-\mathrm{O1}$ | $\mathrm{O3}-\mathrm{O2}$ | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| jTrans | 0.614 | 0.546 | 0.521 | 0.745 | 0.703 | 0.792 | 0.631 | 0.563 | 0.537 | 0.751 | 0.709 | 0.793 | 0.658 |
| Nova | 0.678 | 0.631 | 0.603 | 0.750 | 0.711 | 0.785 | 0.671 | 0.628 | 0.599 | 0.753 | 0.712 | 0.782 | 0.692 |
| Nova $^{+}$ | 0.719 | 0.693 | 0.672 | 0.763 | 0.736 | 0.776 | 0.713 | 0.694 | 0.674 | 0.766 | 0.739 | 0.777 | 0.727 |

Table 3. MRR of binary code similarity ranking produced by jTrans, Nova, and Nova ${ }^{+}$, with pool size $\mathrm{K}=10,000$

distributed training). We perform the pre-training and fine-tuning of the BCSD task with a 96-core server with 4 NVIDIA A5000 GPUs. The fine-tuning of BCT and BCR tasks is conducted on 16-core AWS instances with one NVIDIA A10 GPU.

## 5. Evaluation Results

### 5.1. Evaluation on Binary Code Similarity Detection

Tables 2 and 3 show Nova's and Nova ${ }^{+}$'s performance on task BCSD on the Binary-Corp-3M test set. We report the mean reciprocal rank (MRR), which is the average of the reciprocal ranks of ground-truth positive candidates for a given query:

$$
\begin{equation*}
\mathrm{MRR}=\frac{1}{\mathrm{~N}} \sum_{i=1}^{\mathrm{N}} \frac{1}{\operatorname{rank}\left(\mathbf{b}_{\mathrm{i}}^{+}\right)} \tag{6}
\end{equation*}
$$

where $\mathrm{N}$ is the number of test queries, $\operatorname{rank}\left(\mathbf{b}_{\mathbf{i}}^{+}\right)$is the rank of the ground-truth positive candidate in the candidate pool sorted by their similarity to the $i$-th query binary $\mathbf{b}_{\mathbf{i}}$. Examples are given in appendix B.1.

Table 2 reports the MRR of Nova and Nova ${ }^{+}$compared with the existing state-of-the-art technique jTrans (Wang et al., 2022) when the pool size $\mathrm{K}$ is 32 . The column name shows the optimization used in the query and candidate binaries ( $00-01$ means the query binary is compiled with 00 optimization and all the candidates are compiled with 01 optimization). Both Nova and Nova ${ }^{+}$achieve higher overall MRR than jTrans ( 0.928 vs. 0.911 and 0.929 vs. 0.911 ). Among the 12 settings ( $00-01$ to $03-02$ ), jTrans achieves the highest MRR in three settings, while Nova and $\mathrm{Nova}^{+}$ achieve the highest MRR in six and seven settings.

Table 3 reports the MRR of three models when the pool size $K$ is set to 10,000 , which shows that when the pool size becomes larger, Nova and Nova ${ }^{+}$outperform jTrans by a larger margin. Nova outperforms jTrans under 10 of the 12 settings, and achieves a higher overall MRR ( 0.692 vs. 0.658). With the optimization generation and optimization level prediction pre-training tasks, Nova ${ }^{+}$further improves the overall MRR of Nova by $5.1 \%$ ( 0.727 vs. 0.692).

While MRR measures the average ranking of the ground-

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-07.jpg?height=303&width=437&top_left_y=675&top_left_x=1061)

(a) Ranking of the ground-truth

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-07.jpg?height=307&width=395&top_left_y=676&top_left_x=1491)

(b) Ranking of the ground-truth
Figure 6. Overall ranking of the ground-truth positive candidates.

truth positive candidates, we also evaluate the number of ground-truths that are ranked at top-1, 2, 3, 5, and 10 by each model. As shown in Figure 6 (a), Nova sorts the groundtruth in top-1 and top-2 more than jTrans, and $\mathrm{Nova}^{+}$consistently sorts the most ground-truth in top-1 to top-5. The effectiveness of Nova and $\mathrm{Nova}^{+}$is more obvious when the pool size $\mathrm{K}$ is 10,000 in Figure 6 (b), where both models consistently rank the ground truth higher than jTrans.

### 5.2. Evaluation on Binary Code Translation

For the BCT task, We report four evaluation metrics: (1) Exact Match, which measures if the translated binary is the same as the ground-truth binary generated by the compiler, (2) Instruction Longest Common Subsequence, which measures the length of the longest subsequence of instructions that are common to both generated binary and groundtruth binary, divided by the number of instructions of the ground-truth binary (Essentially, it shows the largest partial binary in the ground truth that is correctly generated),

(3) Instruction BLEU, which measures the BLEU (Papineni et al., 2002) score at the instruction level, essentially reflecting how many instructions and sequences of instructions match those in the ground truth, (4) BLEU (Papineni et al., 2002), which measures how many words and n-grams (Broder et al., 1997) are common in the generated binary and the ground truth.

Table 4 shows Nova and $\mathrm{Nova}^{+}$'s result of the BCT task on the Generated dataset. We provide three most similar examples as few-shots when querying GPT-3.5. Nova and Nova ${ }^{+}$ generate better binary translation (either from X86 to ARM or from ARM to X86) with more exact matches, higher instruction LCS, instruction BLEU, and BLEU scores.

| Models | X86 $\rightarrow$ ARM |  |  |  | ARM $\rightarrow$ X86 |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Exact Match | Inst. LCS | Inst. BLEU | BLEU | Exact Match | Inst. LCS | Inst. BLEU | BLEU |
| GPT-3.5 | 0.33 | 14.34 | 22.99 | 68.88 | 0.00 | 9.41 | 14.54 | 55.49 |
| Nova | 19.50 | 57.02 | 83.17 | 94.55 | 15.33 | 54.56 | 79.22 | 91.13 |
| Nova $^{+}$ | 25.17 | 60.30 | 85.78 | 95.98 | 18.00 | 60.68 | 85.64 | 94.86 |

Table 4. Evaluation of BCT on the Generated Benchmark. The Source Code is Compiled with the O0 Optimization.

| Opt. | Models | X86 $\rightarrow$ ARM |  |  |  | ARM $\rightarrow$ X86 |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Exact Match | Inst. LCS | Inst. BLEU | BLEU | Exact Match | Inst. LCS | Inst. BLEU | BLEU |
| $\mathrm{O} 0$ | GPT-3.5 | 4.02 | 19.08 | 25.92 | 58.22 | 1.16 | 17.12 | 20.04 | 46.52 |
|  | Nova | 10.92 | 34.00 | $\mathbf{5 3 . 3 5}$ | 74.76 | 6.40 | 36.51 | 52.86 | 72.62 |
|  | Nova $^{+}$ | 8.62 | 33.36 | 51.70 | 75.45 | 5.81 | 34.99 | 51.84 | 71.24 |
| $\mathrm{O} 1$ | GPT-3.5 | 1.31 | 13.68 | 12.14 | 42.13 | 1.31 | 12.10 | 10.36 | 31.59 |
|  | Nova | 3.49 | 20.76 | 27.40 | 55.93 | 0.87 | 19.86 | 26.64 | 50.06 |
|  | Nova $^{+}$ | 4.37 | 21.35 | 28.14 | 56.30 | 2.18 | 22.40 | 30.18 | 53.37 |
| $\mathrm{O} 2$ | GPT-3.5 | 1.36 | 11.90 | 9.49 | 38.81 | 0.45 | 11.28 | 9.36 | 29.79 |
|  | Nova | 1.81 | 15.97 | 17.50 | 51.95 | 0.45 | 16.40 | 19.06 | 46.01 |
|  | $\mathbf{N o v a}^{+}$ | 2.26 | 16.61 | 19.12 | 53.49 | 0.91 | 17.81 | 21.84 | 48.90 |
| $\mathrm{O} 3$ | GPT-3.5 | 1.00 | 11.32 | 9.03 | 39.10 | 0.50 | 11.28 | 9.69 | 30.77 |
|  | Nova | 1.48 | 15.48 | 17.71 | 51.08 | 0.50 | 16.26 | 19.34 | 46.13 |
|  | Nova $^{+}$ | 1.49 | 15.78 | 18.59 | $\mathbf{5 2 . 7 2}$ | 1.00 | 18.05 | 22.59 | 49.43 |

Table 5. Evaluation of BCT on the Codeflaws Benchmark. The Source Code is Compiled with the O0, $\mathrm{O} 1, \mathrm{O} 2$ or O3 Optimization.

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-08.jpg?height=675&width=1697&top_left_y=1099&top_left_x=187)

Figure 7. Example from the Codeflaws benchmark and models' translations from ARM64 binary to X86-64 binary.

Table 5 lists the result of BCT on the Codeflaws benchmark. While both models outperform GPT-3.5, it is noticeable that Nova generates better translation than Nova ${ }^{+}$for binaries compiled with 00 optimization and $\mathrm{Nova}^{+}$is consistently better for 01,02 and 03 binaries.

Figure 7 illustrates an example from the Codeflaws benchmark, for which $\mathrm{Nova}^{+}$translations the ARM64 binary to X86-64 binary perfectly. The corresponding source code of the binary is a function comparing the value given by two pointers $* x$ and $* y$, we align the source code, ARM64 binary, and X86-64 binary into five steps (steps (1) to (5) in Figure 7) for readers' better understanding. Nova's translation is not exactly correct since it misses instruction mov \$0, \%eax before label21 pop \%rbp (marked with red background), which essentially misses step (5) so the binary will never return 0. Moreover, GPT-3.5's translation is wrong as the instruction label24 add sp, sp, \#16 is a mix of ARM64 and X86-64 syntax, which shows its lack of specific domain knowledge about binaries.

### 5.3. Evaluation on Binary Code Recovery

For the BCR task, we report three metrics: (1) Token Accuracy, which is the ratio of the recovered code tokens that appear in the ground-truth source code, (2) BLEU, the same metrics as used for BCT, and (3) CodeBLEU (Lu et al., 2021), a metric combining token, abstract syntax tree and data flow similarities.

| Models | X86 $\rightarrow$ src |  |  |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Token Acc. | BLEU | CodeBLEU |  | ARM $\rightarrow$ src |  |  |
| GPT-3.5 | 92.73 | 73.71 | 76.89 |  | 89.46 | 70.80 | 71.99 |
| Coda | 95.55 | - | - |  | - | - | - |
| Neutron | 97.00 | - | - |  | - | - | - |
| Nova | 99.95 | 99.87 | 99.87 |  | 99.86 | 99.64 | 99.57 |
| Nova $^{+}$ | $\mathbf{9 9 . 9 9}$ | $\mathbf{9 9 . 9 8}$ | $\mathbf{9 9 . 9 6}$ |  | $\mathbf{9 9 . 8 8}$ | $\mathbf{9 9 . 7 1}$ | $\mathbf{9 9 . 6 6}$ |

Table 6. Evaluation of BCR on Generated Benchmark. The Source Code is Compiled with O0 Optimization.

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-09.jpg?height=1372&width=832&top_left_y=604&top_left_x=186)

Figure 8. Example from the Generated benchmark and models' recoveries from X86-64 binary to source code.

Table 6 shows Nova and $\mathrm{Nova}^{+}$'s result of the BCR task on the Generated benchmark. Since the generated benchmark is relatively simple, both models can recover almost perfect source code. Compared with existing techniques, Coda (Fu et al., 2019) and Neutron (Liang et al., 2021), Nova and $\mathrm{Nova}^{+}$achieve higher token accuracy on recovering X86-64 binaries and are also able to recover ARM64 binaries that existing techniques do not support. Compared with GPT3.5 , both Nova and $\mathrm{Nova}^{+}$are consistently better on both

| Opt. | Models | $\mathrm{X} 86 \rightarrow$ src |  |  | ARM $\rightarrow$ src |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Token Acc. | BLEU | CodeBLEU | Token Acc. | BLEU | CodeBLEU |
| $\mathrm{O} 0$ | GPT-3.5 | 68.84 | 34.26 | 45.09 | 62.63 | 29.68 | 39.60 |
|  | Nova | 74.61 | 46.10 | 54.22 | 74.70 | 46.74 | 54.16 |
|  | $\mathrm{Nova}^{+}$ | 76.38 | 47.24 | $\mathbf{5 5 . 3 5}$ | 75.38 | 46.95 | 54.31 |
| O1 | GPT-3.5 | 65.44 | 30.87 | 41.48 | 62.97 | 30.67 | 40.10 |
|  | Nova | 71.34 | 40.65 | 49.84 | 71.88 | 41.14 | 50.55 |
|  | $\mathrm{Nova}^{+}$ | 72.74 | 40.93 | $\mathbf{5 0 . 5 3}$ | 74.73 | 42.90 | $\mathbf{5 2 . 1 7}$ |
| $\mathrm{O} 2$ | GPT-3.5 | 66.92 | 29.93 | 42.27 | 62.54 | 27.94 | 39.52 |
|  | Nova | 71.19 | 40.44 | 49.39 | 71.23 | 39.62 | 49.39 |
|  | Nova $^{+}$ | 72.69 | 39.85 | $\mathbf{5 0 . 0 9}$ | 72.08 | 40.15 | 49.73 |
| O3 | GPT-3.5 | 65.02 | 29.36 | 39.52 | 61.73 | 28.13 | 39.44 |
|  | Nova | 69.67 | 39.32 | 48.48 | 70.07 | 39.14 | 48.60 |
|  | $\mathbf{N o v a}^{+}$ | 71.08 | 39.17 | 49.16 | 70.61 | 39.34 | 48.80 |

Table 7. Evaluation of BCR on Codeflaws Benchmark. The Source Code is Compiled with $\mathrm{O} 0, \mathrm{O} 1, \mathrm{O} 2$ and $\mathrm{O} 3$ Optimizations.

recovery from X86-64 and ARM64 to source code.

Figure 8 shows an example of recovering source code from $\mathrm{X} 86-64$ binary. The recovery generated by Nova ${ }^{+}$is the same as the ground truth, which involves declaration of five variables, performs function call and arithmetic operations and contains condition branches. To help readers understand the binary, we align the X86-64 binary and $\mathrm{Nova}^{+}$'s recovery into seven steps (step (1) to (7)). Nova on the other hand, makes two small mistakes on two if conditions ( $\mathrm{d}>=$ e and e > d).

![](https://cdn.mathpix.com/cropped/2024_06_04_b3178edea58813686570g-09.jpg?height=884&width=845&top_left_y=1301&top_left_x=1060)

Figure 9. Example from the Codeflaws benchmark and models' recoveries from X86-64 binary to source code.

Table 7 shows the results on the Codeflaws benchmark, where Nova and Nova ${ }^{+}$consistently outperform GPT-3.5. An example from this benchmark is given in Figure 9 where
the ground-truth source code implements a binary search algorithm. The source code recovered by Nova ${ }^{+}$is the closest to the ground truth except that it fails to initialize $l$ and $r$ to $m n$ and $m x$, which are two out-of-context global variables. Dispute minor mistakes, Nova ${ }^{+}$'s recovery is helpful for developers to quickly understand the intention of the binary. In contrast, Nova and GPT-3.5's recoveries fail to implement the correct algorithm and thus get much lower CodeBLEU scores.

## 6. Limitations

Nova $^{+}$are unable to learn the global dependencies in binary such as the address of instructions, address of callee functions, and contents in the . data section, since the full binary including the global dependencies is too long and complex for LLMs to learn. However, Nova ${ }^{+}$shows impressive capability in learning the structure of binaries to encode, translate, and recover binaries. Train LLMs to learn the whole binaries (e.g., the global dependencies among all the functions in a binary, and the . data section) remains a very challenging future work for both traditional analysis approaches (Kim et al., 2023) and LLM-based approaches.

$\mathrm{Nova}^{+}$focuses on X86-64 and ARM64 binaries, given their wide usage in various domains (Wang et al., 2022; Fu et al., 2019; Liang et al., 2021). Yet, our new pre-training tasks, optimization generation, and optimization level prediction are general and can be applied to train LLM learn binaries in other architectures such as MIPS64 and even 32-bit binaries.

## 7. Conclusion

In this work, we develop the first generative LLMs for binary, namely Nova and Nova ${ }^{+}$. Nova is pre-trained with the standard language modeling task using binary corpora and $\mathrm{Nova}^{+}$is further pre-trained with optimization generation and optimization level prediction tasks that we propose for understanding binary optimization and equivalency. We evaluate our models on binary code similarity detection, binary code translation, and binary code recovery tasks using five benchmarks. Nova outperforms existing techniques and GPT-3.5 on all three tasks, and our new pre-training task makes Nova ${ }^{+}$overall the best LLM for binary.

## References

Aminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A., Li, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O., and He, Y. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale, 2022.

Bauman, E., Lin, Z., Hamlen, K. W., et al. Superset disassembly: Statically rewriting x86 binaries without heuris- tics. In NDSS, 2018.

Broder, A. Z., Glassman, S. C., Manasse, M. S., and Zweig, G. Syntactic clustering of the web. Computer Networks and ISDN Systems, 29 (8):1157-1166, 1997. ISSN 0169-7552. doi: https://doi.org/10.1016/S0169-7552(97)00031-7. URL https://www.sciencedirect.com/ science/article/pii/S0169755297000317. Papers from the Sixth International World Wide Web Conference.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https: / arxiv.org/ $\mathrm{abs} / 2005.14165$.

Cesare, S., Xiang, Y., and Zhou, W. Control flow-based malware variantdetection. IEEE Transactions on Dependable and Secure Computing, 11(4):307-317, 2014. doi: 10.1109/TDSC.2013.40.

Chen, A., Scheurer, J., Korbak, T., Campos, J. A., Chan, J. S., Bowman, S. R., Cho, K., and Perez, E. Improving code generation by training with natural language feedback, 2023.

Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation with generated tests, 2022a. URL https://arxiv.org/abs/ 2207.10397 .

Chen, Q., Lacomis, J., Schwartz, E. J., Goues, C. L., Neubig, G., and Vasilescu, B. Augmenting decompiler output with learned variable names and types. In 31st USENIX Security Symposium (USENIX Security 22), pp. 4327-4343, Boston, MA, August 2022b. USENIX Association. ISBN 978-1-939133-31-1. URL https://www.usenix. org/conference/usenixsecurity22/ presentation/chen-qibin.

David, Y. and Yahav, E. Tracelet-based code search in executables. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI '14, pp. 349-360, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450327848. doi: 10.1145/ 2594291.2594343. URL https://doi.org/10. $1145 / 2594291.2594343$.

David, Y., Partush, N., and Yahav, E. Statistical similarity of binaries. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI '16, pp. 266-280, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342612. doi: 10.1145/ 2908080.2908126. URL https://doi.org/10. $1145 / 2908080.2908126$.

David, Y., Partush, N., and Yahav, E. Similarity of binaries through re-optimization. SIGPLAN Not., 52(6): 79-94, jun 2017. ISSN 0362-1340. doi: 10.1145/ 3140587.3062387. URL https://doi.org/10. $1145 / 3140587.3062387$.

David, Y., Partush, N., and Yahav, E. Firmup: Precise static detection of common vulnerabilities in firmware. SIGPLAN Not., 53(2):392-404, mar 2018. ISSN 03621340. doi: 10.1145/3296957.3177157. URL https: //doi.org/10.1145/3296957.3177157.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805.

Duck, G. J., Gao, X., and Roychoudhury, A. Binary rewriting without control flow recovery. In Proceedings of the 41st ACM SIGPLAN conference on programming language design and implementation, pp. 151-163, 2020.

Fu, C., Chen, H., Liu, H., Chen, X., Tian, Y., Koushanfar, F., and Zhao, J. Coda: An end-to-end neural program decompiler. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 093b60fd0557804c8ba0cbf1453da22f-Paper. pdf.

Jiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program repair. In Proceedings of the 45th International Conference on Software Engineering, ICSE '23, pp. 1430-1442. IEEE Press, 2023. ISBN 9781665457019. doi: 10.1109/ICSE48619. 2023.00125. URL https://doi.org/10.1109/ ICSE48619.2023.00125.

Jin, X., Pei, K., Won, J. Y., and Lin, Z. Symlm: Predicting function names in stripped binaries via context-sensitive execution-aware code embeddings. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pp. 1631-1645, 2022.
Kim, H., Kim, S., Lee, J., Jee, K., and Cha, S. K. Reassembly is hard: A reflection on challenges and strategies. 2023.

Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. arXiv preprint, abs/2207.01780, 2022.

Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M.-H., Umapathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may the source be with you!, 2023.

Liang, R., Cao, Y., Hu, P., and Chen, K. Neutron: an attention-based neural decompiler. Cybersecurity, 4(1):5, 2021. ISSN 2523-3246. doi: 10.1186/ s42400-021-00070-0. URL https://doi.org/10. 1186/s42400-021-00070-0.

Liu, J., Zhu, Y., Xiao, K., Fu, Q., Han, X., Yang, W., and Ye, D. Rltf: Reinforcement learning from unit test feedback, 2023.

Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019.

Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C. B., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664, 2021.

Luo, L., Ming, J., Wu, D., Liu, P., and Zhu, S. Semanticsbased obfuscation-resilient binary code similarity comparison with applications to software plagiarism detection. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2014, pp. 389-400, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450330565. doi: 10.1145/2635868.2635900. URL https://doi . org/10.1145/2635868.2635900.

Miller, K., Kwon, Y., Sun, Y., Zhang, Z., Zhang, X., and Lin, Z. Probabilistic disassembly. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pp. 1187-1198, 2019. doi: 10.1109/ICSE.2019. 00121 .

OpenAI. Gpt-3.5, 2022. URL https://platform. openai.com/docs/models/gpt-3-5.

OpenAI. Gpt-4 technical report, 2023.

Pang, C., Yu, R., Chen, Y., Koskinen, E., Portokalidis, G., Mao, B., and Xu, J. Sok: All you ever wanted to know about x86/x64 binary disassembly but were afraid to ask. In $S P$, pp. 833-851. IEEE, 2021.

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, pp. 311-318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https: //doi.org/10.3115/1073083.1073135.

Pei, K., Xuan, Z., Yang, J., Jana, S., and Ray, B. Trex: Learning execution semantics from micro-traces for binary similarity. arXiv preprint arXiv:2012.08680, 2020.

Pei, K., Guan, J., Broughton, M., Chen, Z., Yao, S., Williams-King, D., Ummadisetty, V., Yang, J., Ray, B., and Jana, S. Stateformer: Fine-grained type recovery from binaries using generative state modeling. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 690-702, 2021a.

Pei, K., Guan, J., Williams-King, D., Yang, J., and Jana, S. Xda: Accurate, robust disassembly with transfer learning. In NDSS. The Internet Society, 2021b.

Sæbjørnsen, A., Willcock, J., Panas, T., Quinlan, D., and $\mathrm{Su}, \mathrm{Z}$. Detecting code clones in binary executables. In Proceedings of the Eighteenth International Symposium on Software Testing and Analysis, ISSTA '09, pp. 117-128, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605583389. doi: 10.1145/1572272.1572287. URL https://doi. org/10.1145/1572272.1572287.

Shazeer, N. Fast transformer decoding: One write-head is all you need, 2019.

Tan, S. H., Yi, J., Yulis, Mechtaev, S., and Roychoudhury, A. Codeflaws: A programming competition benchmark for evaluating automated program repair tools. In 2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C), pp. 180-182, 2017.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Wang, H., Qu, W., Katz, G., Zhu, W., Gao, Z., Qiu, H., Zhuge, J., and Zhang, C. Jtrans: Jump-aware transformer for binary code similarity detection. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2022, pp. 1-13, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393799. doi: 10.1145/3533767.3534367. URL https://doi . org/10.1145/3533767.3534367.

Xia, C. S., Wei, Y., and Zhang, L. Automated program repair in the era of large pre-trained language models. In Proceedings of the 45th International Conference on Software Engineering, ICSE '23, pp. 1482-1494. IEEE Press, 2023. ISBN 9781665457019. doi: 10.1109/ICSE48619. 2023.00129. URL https://doi.org/10.1109/ ICSE48619.2023.00129.

Xie, D., Yoo, B., Jiang, N., Kim, M., Tan, L., Zhang, X., and Lee, J. S. Impact of large language models on generating software specifications, 2023.

Xu, X., Feng, S., Ye, Y., Shen, G., Su, Z., Cheng, S., Tao, G., Shi, Q., Zhang, Z., and Zhang, X. Improving binary code similarity transformer models by semantics-driven instruction deemphasis. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2023, pp. 1106-1118, New York, NY, USA, 2023a. Association for Computing Machinery. ISBN 9798400702211. doi: 10.1145/3597926.3598121. URL https://doi . org/10.1145/3597926.3598121.

Xu, X., Zhang, Z., Feng, S., Ye, Y., Su, Z., Jiang, N., Cheng, S., Tan, L., and Zhang, X. Lmpa: Improving decompilation by synergy of large language model and program analysis, 2023b.

Ye, Y., Zhang, Z., Shi, Q., Aafer, Y., and Zhang, X. Darm: Disassembling arm binaries by lightweight superset instruction interpretation and graph modeling. In 2023 IEEE Symposium on Security and Privacy (SP), pp. 728745. IEEE Computer Society, 2022.

Zhang, Z., You, W., Tao, G., Aafer, Y., Liu, X., and Zhang, X. Stochfuzz: Sound and cost-effective fuzzing of stripped binaries by incremental and stochastic rewriting. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 659-676. IEEE, 2021.
