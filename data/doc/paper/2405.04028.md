# Masked Graph Transformer for Large-Scale Recommendation 

Huiyuan Chen<br>hchen@visa.com<br>Visa Research<br>Foster City, USA<br>Vivian Lai<br>viv.lai@visa.com<br>Visa Research<br>Foster City, USA

$\mathrm{Zhe} \mathrm{Xu}$<br>zhexu3@illinois.edu<br>University of Illinois<br>Urbana-Champaign<br>Illinois, USA<br>Yan Zheng<br>Minghua Xu<br>yazheng@visa.com<br>Visa Research<br>Foster City, USA


#### Abstract

Graph Transformers have garnered significant attention for learning graph-structured data, thanks to their superb ability to capture long-range dependencies among nodes. However, the quadratic space and time complexity hinders the scalability of Graph Transformers, particularly for large-scale recommendation. Here we propose an efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity. To achieve this, we treat all user/item nodes as independent tokens, enhance them with positional embeddings, and feed them into a kernelized attention module. Additionally, we incorporate learnable relative degree information to appropriately reweigh the attentions. Experimental results show the superior performance of our MGFormer, even with a single attention layer.


## CCS CONCEPTS

- Information systems $\rightarrow$ Recommender systems.


## KEYWORDS

Graph Transformer, Linear Attention, Masked Mechanism

## ACM Reference Format:

Huiyuan Chen, Zhe Xu, Chin-Chia Michael Yeh, Vivian Lai, Yan Zheng, Minghua Xu, and Hanghang Tong. 2024. Masked Graph Transformer for Large-Scale Recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3626772.3657971

## 1 INTRODUCTION

Graph Neural Networks (GNNs) are widely used in recommender systems, owing to their remarkable performance $[11,49,52]$. Never theless, GNNs often encounter difficulties in capturing long-range[^0]

Chin-Chia Michael Yeh<br>miyeh@visa.com<br>Visa Research<br>Foster City, USA

![](https://cdn.mathpix.com/cropped/2024_06_04_a38e8181331165d8cc1dg-1.jpg?height=304&width=260&top_left_y=886&top_left_x=1171)

Hanghang Tong<br>htong@illinois.edu<br>University of Illinois<br>Urbana-Champaign<br>Illinois, USA

![](https://cdn.mathpix.com/cropped/2024_06_04_a38e8181331165d8cc1dg-1.jpg?height=347&width=349&top_left_y=862&top_left_x=1514)

Figure 1: Overview of the proposed MGFormer.

dependencies because graph convolutions are inherently local operations, limiting their expressive power [5, 12, 41]. While deep GNNs can increase the receptive field to learn more complex patterns, training deep GNNs poses new challenges, such as oversmoothing [22, 43] and high complexity [38, 42, 46, 51].

Recently, Graph Transformers (GTs) have illuminated the landscape of capturing long-range dependencies, outperforming GNNs across various graph benchmarks [8, 24, 28, 32, 47]. The core idea of GTs is the self-attention mechanism so that the message passing operates on a complete attention graph, in contrast to GNNs whose message passing works on the original graph and is hard to capture long-range interactions. In addition, GTs further develop various positional encodings to capture topology information between nodes, including Laplacian positional encodings [19], centrality and spatial encodings [47], and random walk structural encodings [7, 24, 44], which can further improve the performance.

However, GTs often suffer from quadratic space and time complexity with respect to the number of nodes [15, 20, 33, 45, 47], hindering their applicability for large-scale graphs. In NLP, various efforts have been made to enhance the efficiency of Transformers for handling long sequences, including sparse attentions [2, 50], locality sensitive hashing [18], and low-rank approximation [36]. However, it remains unclear how these lightweight Transformers can be effectively applied to large-scale graphs in recommendation, where the graphs may consist of millions of nodes (tokens).

In this paper, we propose an efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity. Illustrated in Figure 1, we treat each user/item node as an independent token and feed all nodes into our masked kernel attention layer to learn the representations of
users and items. In particular, our MGFormer consists of three key ingredients: 1) Structural encoding, leveraging the bipartite graph's topology to discern each node's position, 2) Kernelized attention layer, approximating self attention as a linear dot-product of kernel features with linear complexity, and 3) Reweighing attention mechanism, utilizing a learnable sinusoidal degree mask to adjust attention distribution, which assigns more credits to more crucial tokens. The experimental results demonstrate that our MGFormer achieves competitive performance across multiple benchmarks, while maintaining a comparable complexity to GNNs.

## 2 RELATED WORK

GNNs for Recommendation: Graph Neural Networks (GNNs) utilize a message-passing mechanism that enables nodes to communicate with their neighbors $[1,3,4,8,24,28,32,35,47]$. This strong graph inductive bias is crucial for modeling user-item interactions. For example, LightGCN [11] learns user and item embeddings by linearly propagating signals on the graph. UltraGCN [25] bypasses infinite layers of explicit message passing for more efficient recommendation. SimGCL [49] adopts a contrastive framework by introducing random noises as data augmentation. Recently, DirectAU [34] explicitly optimizes alignment and uniformity of user and item representations on the hypersphere. Despite the remarkable performance of GNNs, their ability to capture long-range dependencies based on local convolution remains insufficient, especially neglecting the impact of low-degree nodes.

Graph Transformer: Graph Transformers (GTs) typically calculate full attentions, regardless of the edge connectivity $[8,24,28$, 32, 47]. This enables GTs to more effectively capture long-range dependencies compared to GNNs. For example, Graphormer [47] combines self-attention mechanism with graph structural encodings. SAT [1] empowers attention layer with extracting subgraph representations. Remarkably, both methods demonstrate impressive performance on molecular datasets. In the field of recommendation, GFormer [21] leverages the Transformer to acquire environmentinvariant user preference. LightGT [39] employs the Transformer to learn modal-specific embeddings for multimedia recommendation. These studies still incorporate GNN components, thereby potentially inheriting issues such as oversmoothing. In contrast, recent efforts highlight that a pure Transformer yields a powerful graph learner both in theory and practice [12, 17, 41]. Here we align with this research direction, aiming to construct a pure and efficient Transformer for large-scale collaborative filtering.

## 3 METHODOLOGY

### 3.1 Preliminaries

3.1.1 Problem Setup. Following $[1,11,34]$, let $\mathbf{R} \in \mathbb{R}^{M \times N}$ be theuser-item interaction matrix, where $M$ and $N$ denote the number of users and items, respectively. $\mathbf{R}_{u i}$ is set to 1 when the user $u$ has interacted with the item $i$ previously, and 0 for unobserved interactions. The goal is to recommend users a ranked list of items that are unobserved before.

3.1.2 Self-Attention Mechanism. The attention layer is one of the key components in the Transformer [33]. Let $\mathrm{X} \in \mathbb{R}^{n \times d}$ denote the input sequence, where $n$ is the length and $d$ is the dimension size. The input $\mathrm{X}$ is projected to queries $\mathbf{Q}$, keys $\mathrm{K}$, and values $\mathrm{V}$ through linear transformations, $\mathbf{Q}=\mathbf{X W}_{Q}, \mathbf{K}=\mathbf{X W}_{K}$, and $\mathbf{V}=\mathbf{X W}_{V}$, respectively. The output at each position $\mathbf{h}_{i}$ is computed as:

$\mathbf{h}_{i}=\frac{\sum_{j=1}^{n} \operatorname{sim}\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right) \cdot \mathbf{v}_{j}}{\sum_{j=1}^{n} \operatorname{sim}\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right)}, \quad$ where $\quad \operatorname{sim}\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right)=\exp \left(\frac{\mathbf{q}_{i}^{\top} \mathbf{k}_{j}}{\sqrt{d}}\right)$,

where $\operatorname{sim}\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right)$ measures the similarity between $i$-th query in $\mathbf{Q}$ and $j$-th key in $K$. Thus, the time and memory cost for the whole sequence is $O\left(n^{2}\right)$, which limits its scalability to long sequences.

### 3.2 The Proposed MGFormer

In this section, we introduce our MGFormer for collaborative filtering. Given a graph, we simply treat all nodes as independent tokens, augment them with positional encodings, and feed them into a masked kernel attention module. Next, we briefly present the process of MGFormer, including embedding lookup, positional encodings, masked kernel attention, and optimization.

3.2.1 Embedding Lookup. The initial representations of a user $u$ and an item $i$ can be obtained via embedding lookup tables:

$$
\begin{equation*}
\mathbf{e}_{u}=\operatorname{lookup}(u), \quad \mathbf{e}_{i}=\operatorname{lookup}(i) \tag{2}
\end{equation*}
$$

where $\mathbf{e}_{u} \in \mathbb{R}^{d}$ and $\mathbf{e}_{i} \in \mathbb{R}^{d}$ are the embeddings of user $u$ and item $i$, respectively, and $d$ is the embedding size. Then we concatenate all node embeddings as: $\mathrm{E} \in \mathbb{R}^{(M+N) \times d}$.

3.2.2 Structural Encodings. It is important to leverage the graph structural information into Transformer models [8, 14, 28, 47]. For recommendation, we simply design the structural encodings of users/items based on SVD of the user-item interaction matrix:

$$
\begin{equation*}
(\hat{\mathbf{U}} \sqrt{\Sigma}) \cdot(\hat{\mathbf{V}} \sqrt{\Sigma})^{\top} \leftarrow \operatorname{SVD}(\mathbf{R}) \tag{3}
\end{equation*}
$$

where we can regard $(\hat{\mathrm{U}} \sqrt{\Sigma}) \in \mathbb{R}^{M \times d}$ as users' structural encodings, and $(\hat{\mathrm{V}} \sqrt{\Sigma}) \in \mathbb{R}^{N \times d}$ as items' structural encodings. These structural encodings establish coordinate bases to preserve the global graph structures [7]. We then concatenate both users' and items' structural embeddings as: $\mathbf{P} \in \mathbb{R}^{(M+N) \times d}$. Finally, we can combine $\mathbf{E}$ and $\mathbf{P}$ as the input for the graph Transformer model:

$$
\begin{equation*}
\mathrm{X}=[\mathrm{E}, \mathrm{P}] \in \mathbb{R}^{(M+N) \times 2 d} \tag{4}
\end{equation*}
$$

3.2.3 Masked Kernel Attention. Given X, we obtain the queries with weight $\mathbf{W}_{Q} \in \mathbb{R}^{2 d \times 2 d}$, keys with $\mathbf{W}_{K} \in \mathbb{R}^{2 d \times 2 d}$, and values:

$$
\begin{equation*}
\mathrm{Q}=\mathrm{XW}_{Q}, \mathrm{~K}=\mathrm{XW}_{K}, \mathrm{~V}=\mathrm{X} \tag{5}
\end{equation*}
$$

here we remove the feature transformation of values for simplicity as suggested by [10]. Then, we treat all nodes as independent tokens and feed them into Transformers. However, the standard self attention in Eq. (1) is not scalable for large graphs.

To simplify the notation, we denote the input length $n=M+N$ and $m=2 d$ in the rest of the paper.

Linearized Attention: Inspired by [6, 16], we can use an arbitrary positive-definite kernel $\kappa(\cdot, \cdot)$ to serve as $\operatorname{sim}(\cdot, \cdot)$, which can be further approximated by Random Features, i.e., $\kappa(\mathbf{a}, \mathbf{b})=$ $\phi(\mathbf{a})^{\top} \phi(\mathbf{b})$, where $\phi(\cdot)$ is a feature map. As such, Eq. (1) becomes:

$$
\begin{equation*}
\mathbf{h}_{i}=\frac{\sum_{j=1}^{n} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right) \cdot \mathbf{v}_{j}}{\sum_{j=1}^{n} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right)}=\frac{\phi\left(\mathbf{q}_{i}\right)^{\top} \cdot \sum_{j=1}^{n} \phi\left(\mathbf{k}_{j}\right) \mathbf{v}_{j}^{\top}}{\phi\left(\mathbf{q}_{i}\right)^{\top} \sum_{j=1}^{n} \phi\left(\mathbf{k}_{j}\right)} \tag{6}
\end{equation*}
$$

The crucial advantage of Eq. (6) is that we can compute $\sum_{j=1}^{n} \phi\left(\mathbf{k}_{j}\right) \mathbf{v}_{j}^{\top}$ and $\sum_{j=1}^{n} \phi\left(\mathbf{k}_{j}\right)$ once and reuse them for every query, leading to a linear time and memory complexity.

The choice of $\phi(\cdot)$ : The feature map $\phi(\cdot)$ can be some non-linear functions, such as elu $(\cdot)+1[16], r e l u(\cdot)$ [26], or focused linear attention [9]. Alternatively, one can approximate the Softmax attention with Random Fourier Features (RFFs) [23], Orthogonal Random Features (ORFs) [48] or Positive Random Features (PRFs) [6]. Here we adopt recent proposed Simplex Random Features (SimRFs) [29] as the feature map. The SimRFs $\phi(\cdot): \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is defined as:

$$
\begin{equation*}
\phi(\mathbf{a}) \stackrel{\text { def }}{=} \sqrt{\frac{1}{m}} \exp \left(\frac{-\|\mathbf{a}\|_{2}^{2}}{2}\right)\left[\exp \left(\mathbf{w}_{1}^{\top} \mathbf{a}\right), \cdots, \exp \left(\mathbf{w}_{m}^{\top} \mathbf{a}\right)\right] \tag{7}
\end{equation*}
$$

where $\mathbf{W}=\left[\mathbf{w}_{1}, \cdots, \mathbf{w}_{m}\right] \in \mathbb{R}^{m \times m}$ is a random matrix, which is:

$$
\mathbf{W}=\mathrm{DSR}_{o}
$$

where $\mathbf{D}$ is a diagonal matrix with $\mathbf{D}_{i i}$ sampled from the $\chi_{m}$-chi distribution. $\mathbf{R}_{o} \in \mathbb{R}^{m \times m}$ is a random orthogonal matrix drawn from the Haar measure on $\mathrm{O}(m)$ [48]. The rows $s_{i}$ of the simplex projection matrix $\mathrm{S} \in \mathbb{R}^{m \times m}$ are given by:

$$
s_{i}= \begin{cases}\sqrt{\frac{m}{m-1}} \mathbf{u}_{i}-\frac{\sqrt{m}+1}{(m-1)^{3 / 2}}(1, \ldots, 1,0)^{\top} & \text { for } 1 \leq i<m \\ \frac{1}{\sqrt{m-1}}(1,1, \ldots, 1,0)^{\top} & \text { for } i=m\end{cases}
$$

where $\mathbf{u}_{i}$ is the standard basis vector. The SimRFs has been demonstrated to yield the smallest mean square error in unbiased estimates of the Softmax kernel (refer [29] for more details).

Masked Attentions: The graph topology is a strong inductive bias of graph data; beyond using it as the node positional encodings, it can be used to enhance the attention matrix, e.g., reweighing/masking the attention map with the adjacency/shortest-path matrix [12, 47]. Our kernelized attention Eq. (6) can be also empowered with a learnable topology-aware mask $\mathrm{M}$ to concentrate the distribution of attention scores. Here we introduce masked mechanism into Eq. (6):

$$
\begin{equation*}
\mathbf{h}_{i}=\frac{\sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right) \cdot \mathbf{v}_{j}}{\sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right)}=\frac{\phi\left(\mathbf{q}_{i}\right)^{\top} \cdot \sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{k}_{j}\right) \mathbf{v}_{j}^{\top}}{\phi\left(\mathbf{q}_{i}\right)^{\top} \sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{k}_{j}\right)} \tag{8}
\end{equation*}
$$

where $\mathrm{M} \in \mathbb{R}^{n \times n}$ is a mask ${ }^{1}$ that would alter the attention distribution, assigning more credits to more crucial tokens. Moreover, Eq. (6) is a special case of Eq. (8) by setting $\mathbf{M}$ as an all-ones matrix.

Unlike Eq. (6), to obtain outputs $\left\{\mathbf{h}_{i}\right\}_{i=1}^{n}$, we need to compute two matrices $\mathbf{P}_{1}=\left\{\sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{k}_{j}\right) \mathbf{v}_{j}^{\top}\right\}_{i=1}^{n}$ and $\mathbf{P}_{2}=\left\{\sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{k}_{j}\right)\right\}_{i=1}^{n}$, where $\mathbf{P}_{1}$ (similar for $\mathbf{P}_{2}$ ) can be decomposed as:

$$
\mathbf{P}_{1}=\left(\begin{array}{cccc}
\mathbf{M}_{11} & \mathbf{M}_{12} & \cdots & \mathbf{M}_{1 n}  \tag{9}\\
\mathbf{M}_{21} & \mathbf{M}_{22} & \cdots & \mathbf{M}_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{M}_{n 1} & \mathbf{M}_{n 2} & \cdots & \mathbf{M}_{n n}
\end{array}\right)\left(\begin{array}{c}
\operatorname{vec}\left(\phi\left(\mathbf{k}_{1}\right) \mathbf{v}_{1}^{\top}\right) \\
\operatorname{vec}\left(\phi\left(\mathbf{k}_{2}\right) \mathbf{v}_{2}^{\top}\right) \\
\vdots \\
\operatorname{vec}\left(\phi\left(\mathbf{k}_{n}\right) \mathbf{v}_{n}^{\top}\right)
\end{array}\right)
$$

where $\operatorname{vec}(\cdot)$ is the vectorization. Clearly, when the mask $\mathrm{M}$ is dense and requires explicit precomputation, calculating $\mathbf{P}_{1}$ and $\mathbf{P}_{2}$ results in a quadratic complexity, which undermines the benefits of employing kernelized attention.[^1]

In NLP domains, Relative Positional Encoding that encodes the distance between any two positions has achieved promising performance [27, 31]. In light of this analogy, we propose a relative sin-based degree centrality matrix as the mask, which relatively captures the node importance:

$$
\begin{equation*}
\mathbf{M}_{i j}=\sin \left(\frac{\pi}{2} \times \frac{z_{\operatorname{deg}(i)}+z_{\operatorname{deg}(j)}}{2}\right) \tag{10}
\end{equation*}
$$

where $\operatorname{deg}(i)$ is the degree of node $i$, and $z_{\operatorname{deg}(i)} \in(0,1)$ is learnable node degree centrality, in which we use embedding lookup of $\operatorname{deg}(i)$, following a projection matrix with the sigmoid function to learn $z_{\operatorname{deg}(i)}$. Within the range of $\left(0, \frac{\pi}{2}\right)$, the monotonically increasing of sine function ensures that nodes with higher degree are considered to be more influential in the graph. Such information regarding node degree has proven to be critical in Graph Transformers [24, 47].

Then, we can compute the module $\mathbf{M}_{i j} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right)$ in Eq. (8) as:

$$
\begin{align*}
& \mathbf{M}_{i j} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right)=\sin \left(\frac{\pi}{2} \times \frac{z_{\operatorname{deg}(i)}+z_{\operatorname{deg}(j)}}{2}\right) \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right) \\
= & \left(\sin \left(\frac{\pi z_{\operatorname{deg}(i)}}{4}\right) \cos \left(\frac{\pi z_{\operatorname{deg}(j)}}{4}\right)+\cos \left(\frac{\pi z_{\operatorname{deg}(i)}}{4}\right) \sin \left(\frac{\pi z_{\operatorname{deg}(j)}}{4}\right)\right) \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right) \\
= & \left(\phi\left(\mathbf{q}_{i}\right) \sin \left(\frac{\pi z_{\operatorname{deg}(i)}}{4}\right)\right)^{\top}\left(\phi\left(\mathbf{k}_{j}\right) \cos \left(\frac{\pi z_{\operatorname{deg}(j)}}{4}\right)\right)+ \\
& \left(\phi\left(\mathbf{q}_{i}\right) \cos \left(\frac{\pi z_{\operatorname{deg}(i)}}{4}\right)\right)^{\top}\left(\phi\left(\mathbf{k}_{j}\right) \sin \left(\frac{\pi z_{\operatorname{deg}(j)}}{4}\right)\right) \\
= & \phi^{\sin }\left(\mathbf{q}_{i}\right)^{\top} \phi^{\cos }\left(\mathbf{k}_{j}\right)+\phi^{\cos }\left(\mathbf{q}_{i}\right)^{\top} \phi^{\sin }\left(\mathbf{k}_{j}\right), \tag{11}
\end{align*}
$$

where $\phi^{\sin }\left(\mathbf{q}_{i}\right)=\phi\left(\mathbf{q}_{i}\right) \sin \left(\frac{\pi z_{\operatorname{deg}(i)}}{4}\right), \phi^{\cos }\left(\mathbf{k}_{j}\right)=\phi\left(\mathbf{k}_{j}\right) \cos \left(\frac{\pi z_{\operatorname{deg}(j)}}{4}\right)$, $\phi^{\cos }\left(\mathbf{q}_{i}\right)=\phi\left(\mathbf{q}_{i}\right) \cos \left(\frac{\pi z_{\operatorname{deg}(i)}}{4}\right)$, and $\phi^{\sin }\left(\mathbf{k}_{j}\right)=\phi\left(\mathbf{k}_{j}\right) \sin \left(\frac{\pi z_{\operatorname{deg}(j)}}{4}\right)$. In this way, Eq. (8) can be expressed as:

$$
\begin{align*}
\mathbf{h}_{i} & =\frac{\sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right) \cdot \mathbf{v}_{j}}{\sum_{j=1}^{n} \mathbf{M}_{i j} \phi\left(\mathbf{q}_{i}\right)^{\top} \phi\left(\mathbf{k}_{j}\right)} \\
& =\frac{\sum_{j=1}^{n} \phi^{\sin }\left(\mathbf{q}_{i}\right)^{\top} \phi^{\cos }\left(\mathbf{k}_{j}\right) \cdot \mathbf{v}_{j}+\sum_{j=1}^{n} \phi^{\cos }\left(\mathbf{q}_{i}\right)^{\top} \phi^{\sin }\left(\mathbf{k}_{j}\right) \cdot \mathbf{v}_{j}}{\sum_{j=1}^{n} \phi^{\sin }\left(\mathbf{q}_{i}\right)^{\top} \phi^{\cos }\left(\mathbf{k}_{j}\right)+\sum_{j=1}^{n} \phi^{\cos }\left(\mathbf{q}_{i}\right)^{\top} \phi^{\sin }\left(\mathbf{k}_{j}\right)}  \tag{12}\\
& =\frac{\phi^{\sin }\left(\mathbf{q}_{i}\right)^{\top} \cdot \sum_{j=1}^{n} \phi^{\cos }\left(\mathbf{k}_{j}\right) \mathbf{v}_{j}^{\top}+\phi^{\cos }\left(\mathbf{q}_{i}\right)^{\top} \cdot \sum_{j=1}^{n} \phi^{\sin }\left(\mathbf{k}_{j}\right) \mathbf{v}_{j}^{\top}}{\phi^{\sin }\left(\mathbf{q}_{i}\right)^{\top} \sum_{j=1}^{n} \phi^{\cos }\left(\mathbf{k}_{j}\right)+\phi^{\cos }\left(\mathbf{q}_{i}\right)^{\top} \sum_{j=1}^{n} \phi^{\sin }\left(\mathbf{k}_{j}\right)}
\end{align*}
$$

Clearly, our Eq. (12) bears a linear complexity with respect to the input sequence length, similar to Eq. (6). More importantly, it can implement an attention reweighing mechanism by leveraging the relative node degree information.

3.2.4 Optimization. Recent studies [34, 37] identify two key properties highly related to the quality of embeddings: alignment and uniformity. To achieve the two properties, we adopt DirectAU loss [34] to train the model parameters. That is:

$$
\begin{gather*}
\mathcal{L}_{\text {align }}=\underset{(u, i) \sim p_{\text {pos }}}{\mathbb{E}}\left\|\mathbf{h}_{u}-\mathbf{h}_{i}\right\|^{2} \\
\mathcal{L}_{\text {uniform }}=\log \underset{\left(u, u^{\prime}\right) \sim p_{\text {user }}}{\mathbb{E}} e^{-\left\|\boldsymbol{h}_{u}-\mathbf{h}_{u^{\prime}}\right\|^{2}}+\log \underset{\left(i, i^{\prime}\right) \sim p_{\text {item }}}{\mathbb{E}} e^{-\left\|\mathbf{h}_{i}-\mathbf{h}_{i^{\prime}}\right\|^{2}}  \tag{13}\\
\mathcal{L}=\mathcal{L}_{\text {align }}+\lambda \cdot \mathcal{L}_{\text {uniform }}
\end{gather*}
$$

where $\lambda$ is a regularized parameter and $\|\cdot\|$ indicates $l_{2}$ norm; $p_{\text {pos }}$ denotes the distribution of positive user-item pairs; $p_{\text {user }}$ and $p_{\text {item }}$ are the distributions of users and items, respectively. Intuitively, linked user-item pair nodes should be close to each other while random user/item nodes should scatter on the hypersphere.

Table 1: Statistics of three benchmark datasets.

| Dataset | \#user | \#item | \#inter. | density |
| :--- | :---: | :---: | :---: | :---: |
| Beauty | $22.4 \mathrm{k}$ | $12.1 \mathrm{k}$ | $198.5 \mathrm{k}$ | $0.07 \%$ |
| Yelp | $31.7 \mathrm{k}$ | $38.0 \mathrm{k}$ | $1561.4 \mathrm{k}$ | $0.13 \%$ |
| Alibaba | $106.0 \mathrm{k}$ | $53.6 \mathrm{k}$ | $907.5 \mathrm{k}$ | $0.016 \%$ |

Table 2: The performance for different models. The best results are in bold face, and the best baselines are underlined.

|  | Beauty |  | Yelp |  | Alibaba |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Method | recall | ndcg | recall | ndcg | recall | ndcg |
| BPRMF [30] | 0.1153 | 0.0534 | 0.0693 | 0.0428 | 0.0439 | 0.0190 |
| LightGCN [11] | 0.1201 | 0.0581 | 0.0833 | 0.0514 | 0.0585 | 0.0275 |
| SGL [40] | 0.1228 | 0.0644 | 0.0896 | 0.0554 | 0.0602 | 0.0295 |
| GOTNet [5] | 0.1309 | 0.0650 | 0.0924 | 0.0567 | 0.0643 | 0.0301 |
| SimGCL [49] | 0.1367 | 0.0682 | 0.0937 | 0.0571 | $\underline{0.0667}$ | $\underline{0.0311}$ |
| GFormer [21] | 0.1362 | 0.0671 | 0.0955 | 0.0597 | 0.0653 | 0.0305 |
| DirectAU [34] | $\underline{0.1465}$ | $\underline{0.0710}$ | $\underline{0.0981}$ | $\underline{0.0615}$ | 0.0664 | 0.0308 |
| MGFormer | $\mathbf{0 . 1 5 3 1}$ | $\mathbf{0 . 0 7 4 8}$ | $\mathbf{0 . 1 0 5 1}$ | $\mathbf{0 . 0 6 6 8}$ | $\mathbf{0 . 0 7 0 2}$ | $\mathbf{0 . 0 3 2 8}$ |
| Improv. | $+4.50 \%$ | $+5.35 \%$ | $+7.14 \%$ | $+8.62 \%$ | $+5.25 \%$ | $+5.47 \%$ |

Lastly, it is worth mentioning that our MGFormer guarantees the expressivity of learning all-pair interactions even using a singlelayer single-head attention, offering an advantage in capturing long-range dependencies compared to the GNN-based methods.

## 4 EXPERIMENTS

### 4.1 Experimental Settings

Dataset. We conduct experiments on three benchmarks [11, 13, 34]: Amazon-Beauty, Yelp-2018, and Alibaba. The datasets are summarized in Table 1. We follow the strategy in [34] to split the datasets into training, validation, and testing sets. To evaluate the performance, we adopt two common Top- $k$ metrics: Recall@ $k$ and NDCG@ $(k=20$ by default) with the all-ranking protocol [11, 34].

Baselines. We choose the following baselines: 1) BPRMF [30]: a matrix factorization model. 2) LightGCN [11]: a GNN model with linear propagation. 3) SGL [40]: a GNN model with contrastive learning. 4) GOTNet [5]: a non-local GNN model. 5) SimGCL [49]: a contrastive model with random augmentation. 6) GFormer [21]: a rationale-aware generative model. 7) DirectAU [34]: a new loss to optimize alignment and uniformity.

The embedding size $d$ is searched among $\{32,64,128\}$. The hyperparameters of all baselines are carefully tuned to achieve the optimal performance. For DirectAU, we opt for LightGCN as the encoder. For MGFormer, a single-layer single-head attention is employed, as experiments indicate no substantial performance improvement with multi-layer multi-head attentions. Also, we adjust the value of $\lambda$ in Eq. (13) over the range from 0.1 to 5.0.

### 4.2 Experimental Results

Overall Performance. The results of different models in terms of Recall@20 and NDCG@20 are summarized in Table 2. We find that our MGFormer generally outperforms the LightGCN and its variants by a large margin, with up to $8.62 \%$ improvement. This indicates that our one-layer kernelized attention model is indeed a powerful learner for link prediction in recommendation.
![](https://cdn.mathpix.com/cropped/2024_06_04_a38e8181331165d8cc1dg-4.jpg?height=282&width=654&top_left_y=298&top_left_x=1190)

Figure 2: Performance for different item groups. Table 3: Ablation studies of MGFormer on Beauty dataset.

| Version | recall@20 | ndcg@20 |
| :--- | :---: | :---: |
| default MGFormer | 0.1531 | 0.0748 |
| w/o structural encodings | 0.1413 | 0.0712 |
| w/o degree centrality | 0.1501 | 0.0739 |
| choose graph adjacency as mask | 0.1318 | 0.0674 |
| choose 1+elu $(\cdot)$ as $\phi(\cdot)$ | 0.1507 | 0.0740 |
| choose focused function as $\phi(\cdot)$ | 0.1514 | 0.0742 |

For time complexity, both MGFormer and DirectAU theoretically achieve a linear complexity with respect to the number of nodes. As an example, DirectAU and MGFormer approximately take $3.6 \mathrm{~s}$ and $8.7 \mathrm{~s}$ per epoch for the Beauty dataset with the same hardware. The extra cost of MGFormer is from computing the weights for queries and keys in Eq. (5), and the random feature transformation in Eq. (7). Overall, the results demonstrate the superiority of our MGFormer. Specifically, it outperforms all baselines and maintains a comparable complexity for large-scale recommendation.

Sparse Recommendation. GNNs are known for their bias towards high-degree items, often neglecting the influence of lowdegree items. Here we explore the ability of capturing long-range dependencies on Beauty and Yelp datasets. Following [49], we divide the test set into three subsets based on the popularity of items: 'Unpopular', 'Normal', and 'Popular'. As shown in Figure 2, MGFormer outperforms GNN-based models for lower degree nodes, indicating the attention mechanism's proficiency in capturing longrange dependencies for sparse recommendation.

Ablation Study. We further explore several variants of MGFormer: 1) Remove structural encodings, 2) Remove sin-based degree centrality, 3) Choose graph adjacency as the mask [12], 4) Choose $1+e l u(\cdot)$ as $\phi(\cdot)[16], 5)$ Choose focused function as $\phi(\cdot)$ [9]. From Table 3, we observe that: 1) The removal of structural encodings or degree centrality generally hampers performance. 2) The use of the adjacency matrix as a mask significantly diminishes performance, as the resulting attention becomes excessively sparse, thereby constraining its effectiveness. 3) Our model exhibits relative stability across various feature transformations.

## 5 CONCLUSION

This study investigates the potential of a pure Transformer architecture in large-scale recommendation, where the scalability often poses a significant challenge. The core component, a masked kernelized attention module, allows us to attain a linear complexity. Interestingly, the experiments show that a single-layer attention model can deliver exceptionally competitive performance.

## REFERENCES

[1] Dexiong Chen, Leslie O'Bray, and Karsten Borgwardt. 2022. Structure-aware transformer for graph representation learning. In ICLR

[2] Huiyuan Chen, Yusan Lin, Menghai Pan, Lan Wang, Chin-Chia Michael Yeh, Xiaoting Li, Yan Zheng, Fei Wang, and Hao Yang. 2022. Denoising self-attentive sequential recommendation. In RecSys.

[3] Huiyuan Chen, Lan Wang, Yusan Lin, Chin-Chia Michael Yeh, Fei Wang, and Hao Yang. 2021. Structured graph convolutional networks with stochastic masks for recommender systems. In SIGIR

[4] Huiyuan Chen, Chin-Chia Michael Yeh, Yujie Fan, Yan Zheng, Junpeng Wang, Vivian Lai, Mahashweta Das, and Hao Yang. 2023. Sharpness-Aware Graph Collaborative Filtering. In SIGIR.

[5] Huiyuan Chen, Chin-Chia Michael Yeh, Fei Wang, and Hao Yang. 2022. Graph neural transport networks with non-local attentions for recommender systems In Proceedings of the ACM Web Conference 2022.

[6] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. 2021. Rethinking Attention with Performers. In ICLR

[7] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. 2021. Graph Neural Networks with Learnable Structural and Positional Representations. In ICLR.

[8] Simon Geisler, Yujia Li, Daniel J Mankowitz, Ali Taylan Cemgil, Stephan Günnemann, and Cosmin Paduraru. 2023. Transformers meet directed graphs. In International Conference on Machine Learning

[9] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. 2023. Flatten transformer: Vision transformer using focused linear attention. In CVPR.

[10] Bobby He and Thomas Hofmann. 2024. Simplifying Transformer Blocks. In ICLR

[11] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgen: Simplifying and powering graph convolution network for recommendation. In SIGIR

[12] Siyuan Huang, Yunchong Song, Jiayue Zhou, and Zhouhan Lin. 2023. Tailoring Self-Attention for Graph via Rooted Subtrees. In NeurIPS.

[13] Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021. Mixgcf: An improved training method for graph neural network-based recommender systems. In $K D D$.

[14] Md Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian. 2022. Global self-attention as a replacement for graph convolution. In $K D D$.

[15] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. 2024. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325 (2024).

[16] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML.

[17] Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. 2022. Pure transformers are powerful graph learners In NeurIPS.

[18] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2019. Reformer: The Efficient Transformer. In ICLR

[19] Devin Kreuzer, Dominique Beaini, William L Hamilton, Vincent Létourneau, and Prudencio Tossou. 2021. Rethinking Graph Transformers with Spectral Attention. In NeurIPS.

[20] Vivian Lai, Huiyuan Chen, Chin-Chia Michael Yeh, Minghua Xu, Yiwei Cai, and Hao Yang. 2023. Enhancing Transformers without Self-supervised Learning: A Loss Landscape Perspective in Sequential Recommendation. In RecSys.

[21] Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, and Chao Huang. 2023. Graph Transformer for Recommendation. In SIGIR.

[22] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph convolutional networks for semi-supervised learning. In $A A A I$.

[23] Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. 2019. Towards a unified analysis of random Fourier features. In ICML.

[24] Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K. Dokania, Mark Coates, Philip H.S. Torr, and Ser-Nam Lim. 2023. Graph inductive biases in transformers without message passing. In ICML.

[25] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: ultra simplification of graph convolutional networks for recommendation. In CIKM.

[26] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosFormer: Rethinking Softmax In Attention. In $I C L R$.

[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research (2020)

[28] Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. 2022. Recipe for a general, powerful, scalable graph transformer. In NeurIPS

[29] Isaac Reid, Krzysztof Marcin Choromanski, Valerii Likhosherstov, and Adrian Weller. 2023. Simplex random features. In ICML.

[30] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the 25th conference on uncertainty in artificial intelligence.

[31] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In NAACL-HLT.

[32] Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland, and Ali Kemal Sinop. 2023. Exphormer: Sparse transformers for graphs. In ICML.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS

[34] Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoping Ma. 2022. Towards Representation Alignment and Uniformity in Collaborative Filtering. In $K D D$.

[35] Song Wang, Xingbo Fu, Kaize Ding, Chen Chen, Huiyuan Chen, and Jundong Li. 2023. Federated Few-Shot Learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.

[36] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020).

[37] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML.

[38] Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and Tyler Derr. 2022. Improving fairness in graph neural networks via mitigating sensitive attribute leakage. In $K D D$

[39] Yinwei Wei, Wenqi Liu, Fan Liu, Xiang Wang, Liqiang Nie, and Tat-Seng Chua. 2023. Lightgt: A light graph transformer for multimedia recommendation. In SIGIR

[40] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In SIGIR.

[41] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Nodeformer: A scalable graph structure learning transformer for node classification. In NeurIPS.

[42] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. 2023. Kernel Ridge Regression-Based Graph Dataset Distillation. In $K D D$.

[43] Yuchen Yan, Yuzhong Chen, Huiyuan Chen, Minghua Xu, Mahashweta Das, Hao Yang, and Hanghang Tong. 2023. From Trainable Negative Depth to Edge Heterophily in Graphs. In NeurIPS

[44] Yuchen Yan, Yongyi Hu, Qinghai Zhou, Lihui Liu, Zhichen Zeng, Yuzhong Chen, Huiyuan Chen, Mahashweta Das, and Hanghang Tong. 2024. PaCEr: Network Embedding From Positional to Structural. In Proceedings of the ACM Web Conference 2024.

[45] Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey Der, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, et al. 2023. Toward a foundation model for time series data. In CIKM.

[46] Chin-Chia Michael Yeh, Mengting Gu, Yan Zheng, Huiyuan Chen, Javid Ebrahimi, Zhongfang Zhuang, Junpeng Wang, Liang Wang, and Wei Zhang. 2022. Embedding Compression with Hashing for Efficient Representation Learning in Large-Scale Graph. In $K D D$

[47] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly for graph representation?. In NeurIPS

[48] Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and Sanjiv Kumar. 2016. Orthogonal random features. In NeurIPS.

[49] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive learning for recommendation. In SIGIR.

[50] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. In NeurIPS.

[51] Yuying Zhao, Yu Wang, Yi Zhang, Pamela Wisniewski, Charu Aggarwal, and Tyler Derr. 2024. Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation. In $A A A I$.

[52] Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, and Tyler Derr. 2024. Can One Embedding Fit All? A MultiInterest Learning Paradigm Towards Improving User Interest Diversity Fairness. In Proceedings of the ACM Web Conference 2024.


[^0]:    Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

    SIGIR '24, 7uly 14-18, 2024, Washington, DC, USA

    ©๑ 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07

    https://doi.org/10.1145/3626772.3657971

[^1]:    ${ }^{1}$ Note that achieving the reweighing mechanism does not necessarily require $\mathrm{M}$ to be a hard binarized matrix.

