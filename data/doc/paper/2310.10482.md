# xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection 

Nuno M. Guerreiro ${ }^{* 1,3,4,5}$, Ricardo Rei ${ }^{* 1,2,5}$, Daan van Stigt ${ }^{1}$,<br>Luisa Coheur ${ }^{2,5}$, Pierre Colombo ${ }^{4}$, André F. T. Martins ${ }^{1,3,5}$<br>${ }^{1}$ Unbabel, Lisbon, Portugal, ${ }^{2}$ INESC-ID, Lisbon, Portugal<br>${ }^{3}$ Instituto de Telecomunicações, Lisbon, Portugal<br>${ }^{4}$ MICS, CentraleSupélec, Université Paris-Saclay, France<br>${ }^{5}$ Instituto Superior Técnico, University of Lisbon, Portugal


#### Abstract

Widely used learned metrics for machine translation evaluation, such as COMET and BLEURT, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce XCOMET, an open-source learned metric designed to bridge the gap between these approaches. XCOMET integrates both sentencelevel evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that XCOMET is largely capable of identifying localized critical errors and hallucinations.


## 1 Introduction

Automatic metrics for machine translation evaluation are widely used by researchers and practitioners to evaluate the quality of translations and the systems generating them. Notably, learned neural metrics, such as COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020), have demonstrated significant improvements in terms of correlation with human judgements when compared to traditional metrics like BLEU (Papineni et al., 2002; Freitag et al., 2021b, 2022).

These metrics are trained to regress on scores obtained through human annotations, by predicting a single sentence-level score representing the quality of the translation hypothesis. However,[^0]

these single scores do not offer a detailed view into translation errors (e.g., it is not immediate which words or spans of words are wrongly translated). Moreover, as they are obtained by making use of highly complex pre-trained models, they can be difficult to interpret (Rei et al., 2023b; Leiter et al., 2023). One appealing strategy to bring a more detailed view into translation errors is to obtain finergrained information on error spans through highlighting them and indicating their severity (Fonseca et al., 2019; Perrella et al., 2022; Bao et al., 2023). In fact, this is the strategy adopted in recent works that have employed generative large language models (LLMs) for machine translation evaluation: (i) identify errors within a given translation, subsequently (ii) categorize these errors according to their severity, and finally (iii) infer a sentencelevel score from the predicted errors (Fernandes et al., 2023; Xu et al., 2023). However, these methods still lag behind dedicated learned metrics when using open LLMs, such as the LLaMA models (Touvron et al., 2023; Xu et al., 2023). As it stands, competitive performance with generative strategies remains contingent on utilizing large proprietary, closed LLMs such as PaLM-2 and GPT-4 (Fernandes et al., 2023).

In this work, we bridge the gap between these two approaches to machine translation evaluation by introducing XCOMET: a learned metric that simultaneously performs sentence-level evaluation and error span detection. Through extensive experiments, we show that our metrics leverage the strengths of both paradigms: they achieve state-ofthe-art performance in all relevant vectors of evaluation (sentence-level, system-level, and error span prediction), while offering, via the predicted error spans, a lens through which we can analyze translation errors and better interpret the sentence-level scores. We achieve this by employing a curriculum during training that is focused on leveraging highquality publicly available data at both the sentence-

![](https://cdn.mathpix.com/cropped/2024_06_04_e177598bc07aa5aff0ecg-02.jpg?height=416&width=1568&top_left_y=240&top_left_x=244)

Figure 1: The хCOMET framework illustrated through a real example: the metric not only provides a sentence-level score, but also predicts translation error spans along with their respective severity. From these spans, we can infer MQM score (following the MQM typology) that informs and highly correlates with the sentence-level score (see Section 6). These spans complement the sentence-level score by providing a detailed view into the translation errors.

and error span level, complemented by the construction of synthetic data to enhance the metric's robustness. Moreover, xCOMET is a unified metric (Wan et al., 2022b), accommodating all modes of evaluation within a single model. This enables the metric to be used even for quality estimation (when no reference is available), or for reference-only evaluation, similarly to BLEURT (when a source is not provided). Crucially, xCOMET also provides high-quality sentence-level scores that are directly inferred from the predicted error spans, in the style of AUToMQM (Fernandes et al., 2023) and InSTRUCTSCORE (Xu et al., 2023).

Our contributions can be summarized as follows:

1. We introduce хCOMET, a novel evaluation metric that leverages the advantages of regressionbased metrics and error span detection to offer a more detailed view of translation errors.
2. We show that XCOMET is a state-of-the-art metric at all relevant vectors of evaluation sentence-level, system-level, and error span prediction - generally outperforming widely-used neural metrics and generative LLM-based machine translation evaluation.
3. We provide a comprehensive robustness analysis of XCOMET, showing that this new suite of metrics identifies the vast majority of localized critical errors and hallucinations.
4. We release two evaluation models: XCOMET$\mathbf{X L}$, with 3.5B parameters, and $\mathbf{X C O M E T - X X L , ~}$ featuring 10.7B parameters. 1[^1]

## 2 Background

Methodologies for human assessment of translation quality. Human evaluation of machine translation is primarily conducted through three distinct approaches: post-edits (PE), direct assessments (DA), and the Multidimensional Quality Metrics (MQM) framework.

In post-edits (PE), professional translators are tasked with "fixing" a given translation, making minimal edits to improve its quality. Using this edited translation - often termed post-edit - we can evaluate the machine translation output by quantifying the number of edits, thus gauging the initial translation's quality (Snover et al., 2006).

Direct assessments (DA) (Graham et al., 2013) are a simple and widely-used evaluation method. Annotators - non-expert bilingual speakers or professional translators - are asked to annotate each translation with a score ranging from 0 to 100 to reflect its adequacy and fluency, where a score of 100 corresponds to a perfect translation, and 0 corresponds to a completely inadequate one.

The Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014), on the other hand, offers a more comprehensive and systematic approach to MT evaluation. Professional translators highlight errors-typically in the form of error spans-within translations, attributing them severity ratings (e.g., minor, major, or critical) and categorical labels (e.g., fluency, accuracy). Figure 1 illustrates one such annotation. MQM annotations have gained prominence in recent years due to their capacity to offer detailed insights into translation errors, facilitating more fine-grained and accurate comparisons between translation systems (Freitag et al., 2021a). As such, the field of Automatic

Evaluation of MT has increasingly favoured comparisons using MQM annotations over traditional DA and PE methodologies (Freitag et al., 2021b, 2022; Zerva et al., 2022).

Automatic metrics for translation evaluation. Conventional automatic metrics for machine translation (MT) evaluation rely on lexical-based approaches, where the evaluation score is computed through statistics related to lexical overlap between a machine translation and a reference translation. Despite evidence indicating that these lexical metrics (e.g., BlEU (Papineni et al., 2002) and CHRF (Popović, 2015)) do not consistently align with human judgments, particularly when these are obtained through the MQM framework (Freitag et al., 2021b, 2022), they remain very popular. In fact, BLEU remains the most widely employed evaluation metric in machine translation to this day (Marie et al., 2021). On the other hand, neural metrics (e.g., CoMET (Rei et al., 2020) and BlEURT (Sellam et al., 2020)) that rely on complex neural networks to estimate the quality of MT outputs are consistently among the best metrics for MT evaluation according to correlations with human judgments (Freitag et al., 2021b, 2022).

However, contrary to lexical metrics which offer a straightforward interpretation, it can often prove challenging to explain the score predicted by a neural metric to a given translation output. As such, there have been a series of efforts to bring interpretability to neural metrics by focusing on understanding the inner workings of neural metrics (Rei et al., 2023b; Leiter et al., 2023), or on constructing inherently interpretable neural metrics (e.g., MATESE (Perrella et al., 2022) and FG-TED (Bao et al., 2023)) by assigning a central role to the task of predicting word-level errors in a given translation, instead of just a sentence-level score.

More recently, with the rise of generative LLMs, some works have tried to frame the MT evaluation problem as a generative problem. This offers great flexibility, as the LLM can be prompted to either score the translation directly (Kocmi and Federmann, 2023), or to identify errors in the translation (e.g., in line with the MQM framework) (Fernandes et al., 2023; Xu et al., 2023).

## 3 Problem Statement

An automatic metric for translation evaluation aims at predicting the quality of a translated sentence, $\boldsymbol{t}$, in light of a reference translation, $\boldsymbol{r}$, for a given source sentence, $s$. Here, we focus specifically on neural metrics that make use of a neural model, and typically operate under one of the following evaluation scenarios:

- reference-only (REF): the model evaluates the translation by processing it alongside a groundtruth reference sentence (BLEURT is an example of such a metric);
- source-reference combined input (SRC+REF): the model evaluates the translation by jointly processing it with both the source and the reference (COMET is an example of such a metric);
- source-only (SRC): the model evaluates the translation using only its corresponding source sequence (COMETKIWI (Rei et al., 2022b) is an example of such a model). This mode is commonly termed as quality estimation (QE) or referencefree evaluation (Specia et al., 2010).

In essence, the model's input sequence consists of the translation $t$ paired with some additional input-either $\boldsymbol{r},[\boldsymbol{r}, \boldsymbol{s}]$ or $\boldsymbol{s}$-derived from the scenarios above. Given this input, the model may predict the quality of the translation at different granularities, e.g., sentence-level or word(span)-level.

Sentence-level prediction. The model is tasked to predict a single global score-typically between 0 and 1 -for the translation, that represents how well it aligns with its context (i.e., source and/or reference sentence). These scores can be used for a broad range of tasks, such as gauging the quality of different translation systems (Freitag et al., 2022), identifying pathological translations (Guerreiro et al., 2023b), assisting the generation of translations by MT systems (Fernandes et al., 2022), or even acting as reward models for human alignment of language models (Gulcehre et al., 2023).

Word(span)-level prediction. In contrast, wordlevel (or span-level) predictions are more finegrained, identifying individual words or phrases in the translation that may have errors or discrepancies-typically identifying them as OK/BAD or according to their severity, e.g., MINOR/MAJOR. These granular evaluations are more interpretable and assist in pinpointing specific issues, which can be particularly valuable for feedback and iterative translation improvements.

Our metric, XCOMET, emerges in a unique position in the landscape of MT evaluation metrics.

![](https://cdn.mathpix.com/cropped/2024_06_04_e177598bc07aa5aff0ecg-04.jpg?height=568&width=665&top_left_y=236&top_left_x=290)

Figure 2: Architecture of xCOMET. The input to the model starts with a [cls] token followed by a translation and an additional input that will have the source, reference or both. After the pooling layer the $[\mathrm{cls}$ ] token is passed to a feed-forward to produce a quality score while all subword pieces corresponding to the translation are passed to a linear layer that will classify them according to their severity levels, $\mathcal{Y}_{\mathrm{WL}}=\{\mathrm{OK}$, MIN, MAJ, CRIT $\}$.

It can simultaneously perform evaluation under all of the three scenarios (SRC, REF, SRC+REF) presented, and provide sentence-level scores and error span annotations that are in line with the MQM framework, thus bringing further transparency to the evaluation (see Figure 1 for an illustration). In the next section, we detail the design choices and methodology of xCOMET.

## 4 Design and Methodology of xCOMET

In this section, we describe the methodology behind xCOMET, outlining its model architecture, training settings and corpora, and learning curriculum. We detail how the model is designed to perform both regression and error span detection while adopting a unified input approach for enhanced flexibility and performance.

### 4.1 Model Architecture

xCOMET is built upon insights garnered from Unbabel-IST's contributions to the WMT22 Metrics and QE shared tasks (Rei et al., 2022a,b). It is designed to concurrently handle two tasks: sentence-level regression and error span detection. Figure 2 illustrates its architecture. We follow the same architecture of the scaled-up version of COMETKIWI detailed in Rei et al. (2023a), which uses a large pre-trained encoder model as its backbone encoder model. Importantly, following naturally from our multi-task setup, the model has two prediction heads: (i) a sentence-level regression head, which employs a feed-forward network to generate a sentence score, and (ii) a word-level sequence tagger, which applies a linear layer to assign labels to each token in a given translation.

We train two XCOMET versions - $\mathbf{\text { XCOMET-XL }}$ and XCOMET-XXL - using the XL (3.5B parameters) and XXL (10.7B parameters) versions of XLM-R (Goyal et al., 2021). ${ }^{2}$

### 4.2 Fully Unified Evaluation

xCOMET adopts a unified input approach (Wan et al., 2022b), allowing for all the evaluation scenarios described in Section 3-REF, SRC+REF, and SRC evaluation-under a single model. Thus, the input sequence consists of two parts: (i) the translated sentence $\boldsymbol{t}=\left[t_{1}, \ldots, t_{n}\right]$ of length $n$, and (ii) an additional input containing information from the source, reference, or both. ${ }^{3}$ To do so, when a reference is available, we run three distinct forward passes (one for each evaluation scenario), each yielding sentence-level and word-level predictions.

### 4.2.1 Training time

For each forward-pass, we collect the sentencelevel predictions $\left\{\hat{y}_{\mathrm{SL}}^{\mathrm{SRC}}, \hat{y}_{\mathrm{SL}}^{\mathrm{REF}}, \hat{y}_{\mathrm{SL}}^{\mathrm{SRC}+\mathrm{REF}}\right\}$ and the word-level logits $\left\{\hat{\boldsymbol{y}}_{\mathrm{WL}}^{\mathrm{SRC}}, \hat{\boldsymbol{y}}_{\mathrm{WL}}^{\mathrm{REF}}, \hat{\boldsymbol{y}}_{\mathrm{WL}}^{\mathrm{SRC}+\mathrm{REF}}\right\} .4$

As we have mentioned before, xCOMET models are trained with supervision from both sentencelevel quality assessments, $y_{\mathrm{SL}}$, and word-level severity tags, $\boldsymbol{y}_{\mathrm{WL}}=\left[y_{1}, \ldots, y_{n}\right]$, with $y_{i} \in$ $\mathcal{Y}_{\mathrm{WL}}=\{\mathrm{OK}$, MIN, MAJ, CRIT $\}$. In the multi-task setting, we use the following loss $\mathcal{L}$ for each input type (INPUT $\in\{$ SRC, REF, SRC+REF $\}$ ):

$$
\begin{align*}
\mathcal{L}_{\mathrm{SL}}^{\mathrm{INPUT}} & =\left(y_{\mathrm{SL}}-\hat{y}_{\mathrm{SL}}^{\mathrm{INPUT}}\right)^{2}  \tag{1}\\
\mathcal{L}_{\mathrm{WL}}^{\mathrm{INPUT}} & =-\frac{1}{n} \sum_{i=1}^{n} \alpha_{y_{i}} \log p\left(\hat{y}_{i}^{\mathrm{INPUT}}\right)  \tag{2}\\
\mathcal{L}^{\mathrm{INPUT}} & =(1-\lambda) \mathcal{L}_{\mathrm{SL}}^{\mathrm{INPUT}}+\lambda \mathcal{L}_{\mathrm{WL}}^{\mathrm{INPUT}} \tag{3}
\end{align*}
$$

$\alpha \in \mathbb{R}^{\left|\mathcal{Y}_{\mathrm{wL}}\right|}$ represents the class weights given for each severity label and $\lambda$ is used to weigh the combination of the sentence and word-level losses.[^2]

The final learning objective is the summation of the losses for each input type:

$$
\begin{equation*}
\mathcal{L}=\mathcal{L}^{\mathrm{SRC}}+\mathcal{L}^{\mathrm{REF}}+\mathcal{L}^{\mathrm{SRC}+\mathrm{REF}} \tag{4}
\end{equation*}
$$

Furthermore, in line with preceding metrics constructed upon the COMET framework, our models use features such as gradual unfreezing, and discriminative learning rates. See Appendix B for full details and hyperparameters.

### 4.2.2 Inference time

Error span prediction. For each subword in the translation, we average the output distribution of the word-level linear layer obtained for each forward pass. Using this distribution, we predict a set of word-level tags $\hat{\boldsymbol{y}}_{\mathrm{wL}}=\left[\hat{y}_{1}, \ldots, \hat{y}_{n}\right]$. From these tags, we construct a list of error spans, $S$, by grouping adjacent subwords identified as errors. The severity of each span in $S$ is defined according to the most severe error tag found within the span.

Sentence-level prediction. For each forward pass, we obtain the corresponding sentence-level scores: $\hat{y}_{\mathrm{SRC}}, \hat{y}_{\mathrm{REF}}$, and $\hat{y}_{\mathrm{SRC}+\mathrm{REF}}$. Additionally, we leverage the information coming from the predicted list of error spans, $S$, to infer an automated MQM score. To do so, we follow the MQM framework: we obtain the error counts for each severity level- $c_{\mathrm{MIN}}, c_{\mathrm{MAJ}}, c_{\mathrm{CRIT}}$-and apply the predetermined severity penalty multipliers to define the error type penalty total, $e(S)$. Formally:

$$
\begin{equation*}
e(S)=c_{\mathrm{MIN}}+5 \times c_{\mathrm{MAJ}}+10 \times c_{\mathrm{CRIT}} \tag{5}
\end{equation*}
$$

Finally, we obtain $\hat{y}_{\mathrm{MQM}}$ by capping and flipping the sign of $e(S)$ :

$$
\hat{y}_{\mathrm{MQM}}= \begin{cases}\frac{25-e(S)}{25}, & \text { if } e(S)<25  \tag{6}\\ 0, & \text { otherwise }\end{cases}
$$

Note that the predicted score $\hat{y}_{\mathrm{MQM}}$ is bounded between 0 and 1 , with a score of 1 corresponding to a perfect translation.

We aggregate the scores to compute the final sentence-level score, $\hat{y}_{\mathrm{SL}}$, through a weighted sum of the different sentence-level scores. Importantly, we also include the inferred MQM score $\hat{y}_{\mathrm{MQM}}$ to directly inform the final sentence-level prediction. Formally, given $\hat{\boldsymbol{y}}=\left[\hat{y}_{\mathrm{SRC}}, \hat{y}_{\mathrm{REF}}, \hat{y}_{\mathrm{SRC}+\mathrm{REF}}, \hat{y}_{\mathrm{MQM}}\right]$ :

$$
\begin{equation*}
\hat{y}_{\mathrm{SL}}=\boldsymbol{w}^{\top} \hat{\boldsymbol{y}} \tag{7}
\end{equation*}
$$

where $\boldsymbol{w}$ is set to $[1 / 9,1 / 3,1 / 3,2 / 9] .{ }^{5}$

### 4.3 Corpora

Our models are exclusively trained on publicly available DA and MQM annotations, most of which have been collected by WMT over the recent years.

DA data. We use DA annotations collected by WMT from 2017 to 2020 , and the MLQE-PE dataset (Fomicheva et al., 2022). As the MLQE-PE dataset does not contain reference translations, we used the post-edit translations as reference translations. Overall, the corpus consists of around 1 million samples, spanning 36 language pairs.

MQM data. We collected the MQM annotations sourced from WMT from 2020 to $2022 .{ }^{6}$ We also used annotations sourced from other MQMannotated datasets: (i) IndicMT (Sai B et al., 2023), which contains MQM annotations spanning 5 Indian languages, and (ii) DEMETR (Karpinska et al., 2022), a diagnostic dataset with perturbations spanning semantic, syntactic, and morphological error categories.

Corpora with MQM annotations are usually extremely unbalanced with critical errors being underrepresented. In term, this may lead to metrics dealing less well with pathological translations, such as critical errors and hallucinations (Amrhein and Sennrich, 2022; Raunak et al., 2022; Guerreiro et al., 2023b). As such, we augment the MQM corpus with synthetic critical errors. We create different types of detached and oscillatory hallucinations (Raunak et al., 2021; Guerreiro et al., 2023b): (i) detached hallucinations, where we replace the translation with a random sentence; (ii) other detached hallucinations, where we replace the true translation with an unrelated translation that is semantically similar to the source sentence ${ }^{7}$; and (iii) oscillatory hallucinations, where we randomly sample a $n$-gram from the translation (with $n$ in $\{2,3,4\}$ ) and repeat it between 1 and 10 times. We provide examples of these synthetic hallucinations in Appendix A. Overall, our MQM corpus consists of $176 \mathrm{~K}$ samples, spanning 14 language pairs.[^3]

Scaling of sentence-level scores. While the sentence-level scores inferred from MQM annotations (through the procedure in Equation 6) are bounded between 0 and 1, DA annotations usually require $z$-normalization in order to mitigate variations in scoring strategies by different annotators (Bojar et al., 2017). ${ }^{8}$ Thus, as $z$-scores are inherently centered at 0 and unbounded, there is a scaling mismatch between the data samples.

Consequently, to circumvent this limitation, we employ min-max scaling on our DA corpus to set its range of scores to $[0,1]$. To do so, we set a practical minimum and maximum $z$-score value. We obtain the minimum score by averaging the $z$-scores for translations with over 1 annotation, wherein all annotators unanimously scored them with an unnormalized 0 DA score, i.e., they deemed the translation as "random". For determining a maximum value, we applied the same process for perfect translations, i.e., unnormalized 100 DA score. ${ }^{9}$

### 4.4 Training Curriculum

XCOMET models undergo a 3-phase curriculum training. Throughout these phases, the training emphasis alternates between sentence-level prediction and error span prediction by tweaking the parameter $\lambda$ in Equation 3. The curriculum phases can be described as follows:

Phase I: The model is trained exclusively using the DA data. In this phase, the focus is exclusively set on sentence-level regression.

Phase II: In this stage, we introduce word-level supervision. To achieve this, the model is finetuned on our diverse MQM corpus, with most emphasis placed on the word-level task.

Phase III: The last training phase is aimed at unifying both tasks. The model is further fine-tuned using high-quality MQM data from (Freitag et al., 2021a), with a bigger emphasis set to sentencelevel prediction.

We describe how we obtain the values of $\lambda$ for Phases II and III in Appendix B. ${ }^{10}$[^4]

Interpretation of the curriculum. We first start by training a sentence-level metric - similar to UNITE (Wan et al., 2022a) - on the vastly available DA annotations. This first phase acts as a warm-up for subsequent stages. In fact, prior research has shown that models trained on DA annotations leverage token-level information that aligns with MQM error annotations (Rei et al., 2023b). When we move to the second phase, we assume that we have a metric that can perform sentence-level regression. Thus, the aim here shifts to integrating word-level supervision without compromising the previously acquired sentence-level prediction skills. To do so, we use the highly diverse corpora of MQM annotations and set most emphasis on the word-level task. Finally, we exclusively leverage a small corpus (around $25 \mathrm{k}$ samples) of very high-quality MQM annotations from (Freitag et al., 2021a) - each sample has three annotations from separate annotators - with additional synthetic hallucinations. Our focus here is to mitigate any potential decline in sentence-level regression capabilities during Phase II.

## 5 Experimental Setting

### 5.1 Evaluation

We test our metrics on the MQM annotations from the News domain from the WMT 2022 Metrics shared task. These annotations cover three language pairs: Chinese $\rightarrow$ English (zh-en), English $\rightarrow$ German (en-de), and English $\rightarrow$ Russian (en-ru). ${ }^{11}$ We evaluate the metrics in terms of sentence-level, system-level, and error span prediction performance.

At the sentence-level, we report both the Pearson correlation coefficient ( $\rho$ ) and Kendall's Tau $(\tau)$ using the Perm-Both hypothesis test (Deutsch et al., 2021). We also evaluate the metrics on Systemlevel Pairwise Accuracy (Kocmi et al., 2021). We base these evaluations on 200 re-sampling runs, with a significance level $(p)$ set to 0.05 . For error span prediction, we adopt the WMT23 Quality Estimation shared task evaluation methodology and compute F1 scores calculated at the character level, taking into account partial matches for both minor and major errors. ${ }^{12}$[^5]

| METRIC | zh-en |  | en-de |  | en-ru |  | Avg. |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\rho$ | $\tau$ | $\rho$ | $\tau$ | $\rho$ | $\tau$ | $\rho$ | $\tau$ |
| BLEURT-20 | 0.462 | 0.336 | 0.568 | 0.380 | 0.498 | 0.379 | 0.509 | 0.365 |
| COMET-22 | 0.423 | 0.335 | 0.581 | 0.369 | 0.516 | 0.391 | 0.507 | 0.361 |
| METRICX | 0.573 | 0.415 | 0.640 | 0.405 | 0.581 | 0.444 | 0.598 | 0.421 |
| GEMBA-GPT4-DA* | 0.318 | 0.292 | 0.508 | 0.387 | 0.454 | 0.383 | 0.427 | 0.354 |
| xCOMET-XL | 0.556 | 0.399 | 0.653 | 0.414 | 0.611 | 0.448 | 0.607 | 0.420 |
| XCOMET-XXL | 0.554 | 0.390 | 0.644 | 0.435 | 0.628 | 0.470 | 0.609 | 0.432 |
| Predicted MQM scores from the error spans $\left(\hat{y}=\hat{y}_{\mathrm{MQM}}\right)$ |  |  |  |  |  |  |  |  |
| XCOMET-XL (MQM) | 0.447 | 0.374 | 0.561 | 0.389 | 0.534 | 0.445 | 0.514 | 0.402 |
| XCOMET-XXL (MQM) | 0.446 | 0.332 | 0.597 | 0.415 | 0.533 | 0.439 | 0.525 | 0.395 |

Table 1: Segment-level Pearson $(\rho)$ and Kendall-Tau $(\tau)(\uparrow)$ using the Perm-Both hypothesis test (Deutsch et al., 2021). Numbers in bold belong to the top-performing cluster according to statistical significance ( $p<0.05$ ).

### 5.2 Baselines

Sentence and system-level. We benchmark our metrics widely used open neural metrics: CoMET22 (Rei et al., 2022a) ${ }^{13}$ and BleURT-20 (Pu et al., 2021). Additionally, we include METRICX, the best performing metric from WMT22 Metrics shared task (Freitag et al., 2022). ${ }^{14}$ Finally, we also include Gemba (Kocmi and Federmann, 2023), which employs GPT4 (OpenAI, 2023) to evaluate translations following DA guidelines.

Error span prediction. We report results using GPT3.5 and GPT4 models, by prompting it in the style of AutoMQM (Fernandes et al., 2023). ${ }^{15}$ We carefully select 5 shots that are held constant for all samples. This way, we can directly compare our results with state-of-the-art LLMs, which have been shown to be able to perform the task of error detection (Fernandes et al., 2023; Xu et al., 2023).

## 6 Correlations with Human Judgements

In this section, we present a standard performance analysis of our metrics in terms of correlations with human judgments. Overall, we find xCOMET to be a state-of-the-art in sentence-level and error span prediction, being competitive with generative LLMs in terms of system-level evaluation.

Sentence-level evaluation. Table 1 shows that both XCOMET metrics outperform other strong performing neural metrics, including the generative[^6]

| METRIC | zh-en | en-de | en-ru | Avg. |
| :--- | :---: | :---: | :---: | :---: |
| BLEURT-20 | 0.762 | 0.771 | 0.743 | 0.759 |
| COMET-22 | 0.705 | 0.800 | 0.733 | 0.746 |
| METRICX | 0.762 | 0.781 | 0.724 | 0.756 |
| GEMBA-GPT4-DA | 0.752 | $\mathbf{0 . 8 4 8}$ | $\mathbf{0 . 8 7 6}$ | $\mathbf{0 . 8 2 5}$ |
| xCOMET-XL | $\mathbf{0 . 8 0 0}$ | 0.743 | 0.790 | 0.778 |
| xCOMET-XXL | $\mathbf{0 . 8 0 0}$ | $\mathbf{0 . 8 2 9}$ | $\mathbf{0 . 8 2 9}$ | $\mathbf{0 . 8 1 9}$ |
| MCOMET-XL (MQM) | 0.781 | 0.762 | 0.762 | 0.768 |
| XQM scores from the error | spans | $\left(\hat{y}=\hat{y}_{\text {MQM }}\right)$ |  |  |
| xCOMET-XXL (MQM) | 0.781 | $\mathbf{0 . 8 3 8}$ | 0.810 | 0.810 |

Table 2: System-level Pairwise Accuracy ( $\uparrow$ ) (Kocmi et al., 2021) using the Perm-Both hypothesis test (Deutsch et al., 2021). Numbers in bold belong to the top-performing cluster according to statistical significance $(p<0.05)$.

approach leveraging GPT4 of GEmba. In particular, xCOMET-XXL sets a new state-of-the-art for en-de and en-ru. Interestingly, we can see that, while scaling up the encoder model of the xCOMET metrics (from XL to XXL) holds better results, xCOMET-XL is very competitive. ${ }^{16}$ In fact, it outperforms METRICX, which runs at even a larger size than xCOMET-XXL. Finally, we can also observe that the MQM scores inferred exclusively from the predicted error spans also exhibit strong performance, outperforming widely used metrics BleUrt-20 and Comet-22. This is particularly relevant: the predicted error spans bring not only a more detailed view into translation errors but also provide high-quality sentence-level scores.

System-level evaluation. Table 2 shows results for system-level. Similarly to what we observed at the sentence-level, our metrics show consistently superior performance when compared to other dedicated neural metrics. Notably, although genera-[^7]

| METRIC | zh-en | en-de | en-ru | Avg. |
| :--- | :---: | :---: | :---: | :---: |
| - AutoMQM (GPT3.5) | 0.143 | 0.160 | 0.166 | 0.156 |
| - AutoMQM (GPT4) | 0.248 | 0.257 | $\mathbf{0 . 2 8 1}$ | 0.262 |
| - xCOMET-XL | 0.237 | 0.290 | $\mathbf{0 . 2 8 1}$ | 0.269 |
| - xCOMET-XXL | $\mathbf{0 . 2 5 7}$ | $\mathbf{0 . 3 2 0}$ | 0.262 | $\mathbf{0 . 2 8 0}$ |
| Error spans detected with source-only |  |  |  |  |
| - xCOMET-XL (SRC) | 0.208 | 0.264 | 0.252 | 0.242 |
| - xCOMET-XXL (SRC) | 0.229 | 0.298 | 0.238 | 0.255 |

Table 3: F1 scores ( $\uparrow$ ) on error span detection for reference-free ( $\odot$ ) and reference-based ( $\odot$ ) evaluation.

| SCORE | zh-en | en-de | en-ru | All |
| :---: | :---: | :---: | :---: | :---: |
| $\hat{y}_{\mathrm{SRC}}$ | 0.73 | 0.75 | 0.79 | 0.78 |
| $\hat{y}_{\mathrm{REF}}$ | 0.75 | 0.74 | 0.75 | 0.77 |
| $\hat{y}_{\text {SRC+REF }}$ | 0.78 | 0.79 | 0.82 | 0.82 |
| $\hat{y}_{\mathrm{SL}}{ }^{\dagger}$ | 0.90 | 0.92 | 0.92 | 0.92 |

Table 4: Pearson correlations between the regression scores produced by хCOMET-XXL ( $\hat{y}_{\mathrm{SRC}}, \hat{y}_{\mathrm{REF}}, \hat{y}_{\mathrm{SRC}+\mathrm{REF}}$, $\hat{y}_{\mathrm{SL}}$ ) and the MQM inferred score, $\hat{y}_{\text {MQM }}$, computed from the identified error spans. ${ }^{\dagger}$ The computation of $\hat{y}_{\mathrm{SL}}$, contrary to the computation of the other regression scores, makes direct use of $\hat{y}_{\mathrm{MQM}}$ (see Equation 7).

tive approaches typically do much better at systemlevel evaluation when compared to dedicated models (Kocmi and Federmann, 2023; Fernandes et al., 2023), xCOMET-XXL remains competitive in all language pairs with GEMBA using GPT4. Finally, building on the findings at the sentence-level, the MQM scores inferred directly and exclusively from the predicted error spans also exhibit very competitive performance in terms of system-level accuracy.

Error span prediction. While we have highlighted the utility of the predicted error spans through the inferred sentence-level MQM scores, here we turn to evaluating them directly. Table 3 shows that the error spans predicted via XCOMET metrics outperform those obtained with both GPT3.5 and GPT4 despite being smaller in capacity relative to these models. In fact, our metrics achieve close performance to that of GPT4, even when a reference is not provided.

Interplay of error spans and sentence-level scores. Table 4 shows a strong correlation between the different score types predicted by XCOMET and the MQM inferred score derived exclusively from error spans. This interplay is highly important: the predicted error spans may be valuable, not just for the sake of accuracy but also for interpretability. Interestingly, these high correla- tions with the predicted scores from each forward pass ( $\hat{y}_{\mathrm{SRC}}, \hat{y}_{\mathrm{REF}}, \hat{y}_{\mathrm{SRC}+\mathrm{REF}}$ ) are obtained despite no explicit alignment mechanism governing the relationship between the predictions of the sentencelevel and word-level heads. We hypothesize that it is thus the shared encoder that, during the multitask training, aligns the representations between the two tasks. As such, xCOMET provides, through its predicted error spans, a potential lens through which we can better understand, contextualize, and even debug its own sentence-level predictions.

## 7 Robustness of XCOMET to pathological translations

In the previous section, we have shown that xCOMET metrics exhibit state-of-the-art correlations with human judgements when evaluating on high-quality MQM annotations. However, more often than not, these MQM annotations are highly unbalanced and contain little to no major or critical errors. As such, they may not offer a full picture of the metrics' performance. In this section, we shift our focus to studying how xCOMET metrics behave when evaluating translations with localized major or critical errors, such as named-entity errors or mismatches in numbers, and highly pathological translations, such as hallucinations.

### 7.1 Localized errors

We employ SMAUG (Alves et al., 2022) ${ }^{17}$, a tool designed to generate synthetic data for stresstesting metrics, to create corrupted translations that contain major or critical errors. Concretely, we generate translations with the following pathologies: addition of text, negation errors, mask in-filling, named entity errors, and errors in numbers. For this evaluation, we use data from the WMT 2023 Metrics shared task. ${ }^{18}$ Specifically, we corrupt the released synthetic references for which the metrics found no errors. ${ }^{19}$ Moreover, as the full suite of SMAUG transformations can only be applied to English data, we focus solely on Chinese $\rightarrow$ English (zh-en) and Hebrew $\rightarrow$ English (he-en) translations. Full details about the corrupted data and examples are shown in Appendix C.[^8]

![](https://cdn.mathpix.com/cropped/2024_06_04_e177598bc07aa5aff0ecg-09.jpg?height=483&width=723&top_left_y=244&top_left_x=244)

(a) Percent of error types on data with critical errors (for both zh-en and he-en data), as predicted by XCOMET-XXL.

![](https://cdn.mathpix.com/cropped/2024_06_04_e177598bc07aa5aff0ecg-09.jpg?height=437&width=780&top_left_y=290&top_left_x=1072)

(b) Impact of the perturbations, as measured by the difference in XCOMET-XXL $\left(\hat{y}=\hat{y}_{\mathrm{SL}}\right)$ between the original and the perturbed translation, on the zh-en data.

Figure 3: Analysis of xCOMET-XXL for data with localized critical errors in terms of (a) distribution of error severities for the predicted error spans, and (b) sensitivity of the sentence-level scores.

## xCOMET predicts most localized errors as major

 or critical errors. Table 5 shows that xCOMET metrics identify the vast majority of localized errors, with trends varying across scale and language pair. Generally, negation errors and mismatches in numbers are the most easily identified by the metrics. This is interesting: localized errors, such as mismatches in numbers and named-entity errors, had been pinpointed as weaknesses of previous COMET metrics (Amrhein and Sennrich, 2022; Raunak et al., 2022). This earlier limitation seems to now have been addressed successfully. In fact, the results in Figure 3a show that most of these errors are indeed predicted as critical errors. One plausible hypothesis for these improvements is the incorporation of datasets that contain several negative translations, such as DEMETR, MLQE-PE, and synthetic hallucinations into our training set.хCOMET sentence-level scores are sensitive to localized perturbations. Figure $3 b^{20}$ shows that localized errors can lead to significant decreases in the predicted sentence-level scores, with perturbation-wise trends mirroring those of the error span predictions: the most pronounced decreases are found for negation errors and mismatches in numbers and named-entities (median decreases of around 20 points). The distribution of the decreases in quality also reveals two relevant trends: (i) localized perturbations can cause xCOMET-XXL to shift from a score of a perfect translation to that of an unrelated translation, and (ii) the behavior of XCOMET-XXL is not perfect and[^9]

|  | zh-en |  |  | he-en |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
| ERROR | XL | XXL |  | XL | XXL |
| Add. of text | 3.66 | 10.7 |  | 6.15 | 7.35 |
| Negation | 0.20 | 0.20 |  | 3.89 | 4.90 |
| Mask in-fill | 5.01 | 17.0 |  | 4.78 | 3.92 |
| Swap NUM | 3.19 | 2.88 |  | 0.16 | 0.00 |
| Swap NE | 3.66 | 6.94 |  | 9.81 | 7.01 |
| Al1 | $\mathbf{2 . 2 4}$ | 10.7 |  | 9.81 | $\mathbf{7 . 0 0}$ |

Table 5: Percentage (\%) of translations, segmented by perturbation type, that are predicted to have no errors $(\downarrow)$. We show results for both zh-en and he-en language pairs across XCOMET (XL and XXL) sizes.

can be further improved: in some rare cases, perturbations may actually lead increase in the score. Nevertheless, upon closer inspection, we found that, for over $90 \%$ of these cases, the decrease is smaller than 1 point.

### 7.2 Hallucinations

Hallucinations lie at the extreme-end of machine translation pathologies (Raunak et al., 2021), and can have devastating impact when models are deployed in the wild. Yet, these translations are often overlooked when assessing the performance of different translation systems. Their rarity means that performance, usually judged according to an aggregated corpus-level score, may remain largely unperturbed by a very small number of hallucinations. ${ }^{21}$[^10]

| METRIC | All | Full Det. | Osc. |
| :--- | :---: | :---: | :---: |
| - BLEURT-20 | 0.824 | 0.892 | 0.799 |
| - COMET-22 | 0.829 | 0.878 | 0.883 |
| - COMETKIWI-XXL | 0.839 | 0.834 | 0.902 |
| XCOMET-XL | 0.865 | 0.907 | 0.922 |
| XCOMET-XXL | 0.890 | $\mathbf{0 . 9 6 4}$ | 0.844 |
| QE scores from the |  | error spans |  |
| - | $\left.\hat{y}=\hat{y}_{\text {SRc }}\right)$ |  |  |
| XCOMET-XL (SRC) | 0.885 | 0.924 | $\mathbf{0 . 9 4 4}$ |
| XCOMET-XXL (SRC) | $\mathbf{0 . 9 0 2}$ | 0.959 | 0.866 |

Table 6: Hallucination detection performance on the de-en hallucination benchmark from Guerreiro et al. (2023b) as measured by AUROC ( $\uparrow$ ) for reference-free ( $\odot$ ) and reference-based ( $\odot$ ) quality metrics. We report results for all the dataset, for fully detached, and oscillatory hallucinations separately.

In this section, we want to assess how the xCOMET metrics rank hallucinations among other translations. To do so, we will use the German $\rightarrow$ English hallucination benchmark introduced in Guerreiro et al. (2023b). This benchmark involves over $3.4 \mathrm{k}$ translations of different error types, including omissions, named-entity errors, and hallucinations (oscillatory, fully, and strongly detached). For a metric that has not been trained explicitly to rank translations, the benchmark is quite challenging: hallucinations should be ranked below other severe errors and incorrect translations. We provide examples of the hallucinations in the dataset in Appendix D.

xCOMET metrics can distinguish hallucinations from other translations. The results in Table 6 show that both xCOMET metrics largely rank hallucinations lower than other errors. This is especially true for the most severe type of hallucination (fully detached), for which the AUROC exceeds 95 for the XXL metric. In fact, Figure 4 reveals that xCOMET-XXL assigns over $90 \%$ of these fully detached hallucinations a score under 10. Relative to previous metrics, xCOMET achieves overall improvements. Interestingly, we also find that SRC-based evaluation (i.e., without the use of a reference translation) can reap benefits in this scenario. We hypothesize that this is due to the metric over-relying on the reference when it is made available (Rei et al., 2023b). While hallucinations contain content that is detached from the source, some of their text may still overlap (even if just lexically) with the reference text (e.g., in strongly[^11]

![](https://cdn.mathpix.com/cropped/2024_06_04_e177598bc07aa5aff0ecg-10.jpg?height=611&width=782&top_left_y=231&top_left_x=1048)

Figure 4: Category-wise distribution of XCOMET-XXL scores on the hallucination benchmark.

detached or oscillatory hallucinations), leading to higher scores. In future work, it would be interesting to explore whether these trends hold for other language pairs, including low-resource ones, through the use of multilingual hallucination benchmarks like HalOmi (Dale et al., 2023). ${ }^{22}$

## 8 Conclusions

We introduced xCOMET: a novel suite of metrics for machine translation evaluation that effectively combines sentence-level prediction with finegrained error span prediction. Through extensive experiments, we have shown that XCOMET is a state-of-the-art metric at all relevant vectors of evaluation: sentence-level, system-level, and error span prediction. Notably, through xCOMET's capabilities to predict error spans, we can not only obtain useful signals for downstream prediction (either directly through error span prediction or by informing sentence-level scores) but also gain access to a lens through which we can better understand and interpret its predictions. Finally, we also stress-tested the suite of metrics by analyzing their behavior on scoring localized critical errors and hallucinations: xCOMET metrics identify the vast majority of localized errors and can appropriately penalize the severity of hallucinations.

We hope xCOMET can serve as a further step towards more detailed and informed machine translation evaluation. The full suite of metrics (xCOMET-XL and xCOMET-XXL) will be made available through the HuggingFace Hub: https://huggingface.co/Unbabel.[^12]

## Acknowledgements

We are grateful to José Pombal, José G. C. de Souza and Sweta Agrawal for their valuable feedback and discussions.

This work was supported by the Portuguese Recovery and Resilience Plan (PRR) through project C645008882-00000055, Center for Responsible AI, by the European Research Council (DECOLLAGE, ERC-2022-CoG 101088763), by EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), and by the Fundação para a Ciência e Tecnologia (contracts UIDB/50021/2020 and UIDB/50008/2020). We also thank the HPC resources from GENCI-IDRIS (Grants 2023AD011014714, 2023-AD0110146A68R1 and AD011012377R2).

## References

Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A nextgeneration hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.

Duarte Alves, Ricardo Rei, Ana C Farinha, José G. C. de Souza, and André F. T. Martins. 2022. Robust MT evaluation with sentence-level multilingual augmentation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 469-478, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Chantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125-1141, Online only. Association for Computational Linguistics.

Keqin Bao, Yu Wan, Dayiheng Liu, Baosong Yang, Wenqiang Lei, Xiangnan He, Derek F. Wong, and Jun Xie. 2023. Towards fine-grained information: Identifying the type and location of translation errors.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169-214, Copenhagen, Denmark. Association for Computational Linguistics.
David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loïc Barrault, and Marta R. Costa-jussà. 2023. Halomi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation.

Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. Transactions of the Association for Computational Linguistics, 9:11321146.

Daniel Deutsch, Juraj Juraska, Mara Finkelstein, and Markus Freitag. 2023. Training and meta-evaluating machine translation evaluation metrics at the paragraph level.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. $J$. Mach. Learn. Res., 22(1).

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878-891, Dublin, Ireland. Association for Computational Linguistics.

Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation.

Patrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1396-1412, Seattle, United States. Association for Computational Linguistics.

Marina Fomicheva, Shuo Sun, Erick Fonseca, Chrysoula Zerva, Frédéric Blain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia, and André F. T. Martins. 2022. MLQE-PE: A multilingual quality estimation and post-editing dataset. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4963-4974, Marseille, France. European Language Resources Association.

Erick Fonseca, Lisa Yankovskaya, André F. T. Martins, Mark Fishel, and Christian Federmann. 2019. Findings of the WMT 2019 shared tasks on quality
estimation. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), pages 1-10, Florence, Italy. Association for Computational Linguistics.

Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.

Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 29-33, Online. Association for Computational Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33-41, Sofia, Bulgaria. Association for Computational Linguistics.

Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023a. Hallucinations in large multilingual translation models.

Nuno M. Guerreiro, Elena Voita, and André Martins. 2023b. Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1059-1075, Dubrovnik, Croatia. Association for Computational Linguistics.

Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. 2023
Reinforced self-training (rest) for language modeling.

Marzena Karpinska, Nishant Raj, Katherine Thai, Yixiao Song, Ankita Gupta, and Mohit Iyyer. 2022. DEMETR: Diagnosing evaluation metrics for translation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9540-9561, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193-203, Tampere, Finland. European Association for Machine Translation.

Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478-494, Online. Association for Computational Linguistics.

Christoph Leiter, Piyawat Lertvittayakumjorn, M. Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2023. Towards explainable evaluation metrics for machine translation. ArXiv, abs/2306.13041.

Arle Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014. Multidimensional quality metrics (MQM): A framework for declaring and describing translation quality metrics. Tradumàtica: tecnologies de la traducció, 0:455-463.

Benjamin Marie, Atsushi Fujita, and Raphael Rubino. 2021. Scientific credibility of machine translation research: A meta-evaluation of 769 papers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7297-7306, Online. Association for Computational Linguistics.

OpenAI. 2023. Gpt-4 technical report.

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Niccolò Campolungo, and Roberto Navigli. 2022. MaTESe: Machine translation evaluation as a sequence tagging problem. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 569-577, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.

Amy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 751-762, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Vikas Raunak, Arul Menezes, and Marcin JunczysDowmunt. 2021. The curious case of hallucinations in neural machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172-1183, Online. Association for Computational Linguistics.

Vikas Raunak, Matt Post, and Arul Menezes. 2022. SALTED: A framework for SAlient long-tail translation error detection. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5163-5179, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Ricardo Rei, Nuno M. Guerreiro, José Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, José G. C. de Souza, and André F. T. Martins. 2023a. Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task.

Ricardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon Lavie, and André Martins. 2023b. The inside story: Towards better understanding of machine translation neural evaluation metrics. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1089-1105, Toronto, Canada. Association for Computational Linguistics.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.

Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In
Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634-645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Ananya Sai B, Tanay Dixit, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra, and Raj Dabre. 2023. IndicMT eval: A dataset to meta-evaluate machine translation metrics for Indian languages. In Proceedings of the 61 st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14210-14228, Toronto, Canada. Association for Computational Linguistics.

Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223-231.

Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine translation evaluation versus quality estimation. Machine translation, 24:39-50.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Dazhen Wan, Zheng Zhang, Qi Zhu, Lizi Liao, and Minlie Huang. 2022a. A unified dialogue user simulator for few-shot data augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3788-3799, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022b. UniTE: Unified translation evaluation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8117-8127, Dublin, Ireland. Association for Computational Linguistics.

Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023. Instructscore: Towards explainable text generation evaluation with automatic feedback.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:

Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.

Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 69-99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

# Supplemental Material 
