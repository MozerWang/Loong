# OpenAgents: AN OPEN PlATFORM FOR LANGUAGE AGENTS IN THE WILD 

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-01.jpg?height=49&width=1298&top_left_y=480&top_left_x=411) <br> Toh Jing Hua ${ }^{\diamond}$ Junning Zhao ${ }^{\diamond}$ Qian Liu ${ }^{\ominus \diamond}$ Che Liu ${ }^{\diamond}$ Leo Z. Liu

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-01.jpg?height=49&width=623&top_left_y=561&top_left_x=748)

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-01.jpg?height=47&width=361&top_left_y=603&top_left_x=877) <br> $\cdot$ The University of Hong Kong ${ }^{\diamond}$ XLang Lab ${ }^{\ominus}$ Sea AI Lab ${ }^{\star}$ Salesforce Research <br> Code: https://github.com/xlang-ai/OpenAgents <br> Demos: https://chat.xlang.ai Docs: https://docs.xlang.ai/ <br> ABSTRACT
}

Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-01.jpg?height=670&width=1222&top_left_y=1519&top_left_x=446)

Figure 1: The OpenAgents platform for general users, developers, and researchers. (a) General users can interact with the agents via our online web interface, instead of programmer-oriented console or packages. (b) Developers can effortlessly deploy the frontend and backend for further developments given our codes. (c) Researchers can build new language agents or agent-related methods given the examples and shared components, and see how they perform with the web UI. Our OpenAgents serves to be a simple and versatile platform for using, developing, and evaluating language agents.[^0]

## 1 INTRODUCTION

Intelligent agents are broadly conceptualized as autonomous problem solvers with the ability to sense their environment, decide, and act upon that environment (Wooldridge \& Jennings, 1995, Sutton \& Barto, 2005, Russell, 2010). With the advent of large language models (LLMs) (Brown et al., 2020: Chen et al., 2021; Chowdhery et al., 2022; OpenAI, 2023b, Touvron et al., 2023), recent implementations from the academic, industry, and open-source communities have leveraged this concept to create language agents. These agents are capable of utilizing natural language to perform a variety of intricate tasks in diverse environments, showcasing notable potentials (Yao et al. 2022b, Chase, 2022; Gravitas, 2023; OpenAI, 2023a; Wang et al., 2023a).

Meanwhile, current agent frameworks such as Chase (2022), Gravitas (2023), Xu et al. (2023a), Nakajima (2023), Chen et al. (2023), Zhou et al. (2023c) provide proof-of-concept implementations and console interfaces largely tailored for developers. This often restricts access to a wider audience, particularly those not versed in programming or consoles. Current agent benchmarks are constructed within specific environments for deterministic evaluation, especially in scenarios involving coding (Yang et al., 2023a), tool utilization (Li et al., 2023c, Patil et al., 2023; Qin et al., 2023b), web browsing (Shi et al.|, 2017; Yao et al., 2022a; Deng et al., 2023; Zhou et al., 2023b) or a combination of above (Liu et al.| 2023). These offer initiatives toward agent evaluations while general agent usage is bound to be connected to more open or unlimited environments and real users with their special needs and materials.

Aiming to develop LLM-powered offerings for a broader user base, OpenAI has crafted and deployed well-designed products ${ }^{1}$. specifically Advanced Data Analysis (previously known as Code Interpreter), Plugins, and Browse with $b$ Bing, leveraging their further trained models (which has been reverse engineered and accepted by many Zheng et al. (2023b) and Gou et al. (2023)), business logic code, and a nurtured software community (e.g., OpenAI plugins store). Despite the significant success attained, the models and business logic code have not been open-sourced due to business considerations, hindering not only users from free access but also developers and researchers from further exploring, evaluating, and improving upon them.

Recognizing this, OpenAgents stems from the motivation to democratize agent access as an opensource platform for using and hosting agents, currently encompassing three integral agents: Data Agent for data analysis with Python and SQL, Plugins Agent for 200+ tool usages, and Web Agent for autonomous web browsing. We believe that for LLMs to reach their full potential, they must transition from purely theoretical or developer-centric tools to dynamic, interactive systems that cater to a diverse user base. As depicted in Figure 1 general users can readily explore agent capabilities via the online Web UI without any coding expertise required. In addition, OpenAgents provides full business logic and research codes for developers to easily deploy it locally and researchers to further introspect and build language agents. Lastly, given all provided above, OpenAgents is meant to be a realistic and holistic human-in-the-loop agent evaluation platform: out of real needs, real users interact with agents to fulfill their tasks, and the whole human-agent interaction traces and user feedback are recorded for further evaluation. Compared to existing benchmarks and platforms, OpenAgents provides an in-the-wild environment where agents tackle a variety of genuine user needs.

During building OpenAgents, we first underscore the significance of effectively specifying application requirements via LLM prompting, a process that often requires crafting instructions that cater to backend logic, enhance output aesthetics, and safeguard against adversarial inputs. Our findings indicate that the build-up of such instructions can, at times, be substantial, posing challenges in terms of token limitations and context handling for the LLMs. Additionally, for effective realworld deployment, agent models must not only exhibit high performance but also be able to handle real-time, interactive scenarios, such as streaming, to provide an optimal user experience. Furthermore, our exploration reveals that current research often gravitates towards idealized performance metrics, sometimes sidelining critical real-world considerations, such as the trade-offs between system responsiveness and accuracy, and the nuanced complexities introduced when application-based failures arise, potentially obfuscating the true capabilities of the LLMs.

For future directions and extensions based on OpenAgents, we envision the expansion of new agents, agent-related methods, models, and tools for researchers. Plus, the built-in web UI paves the way[^1]towards human-in-the-loop agent and LLM evaluation and interaction in the wild under realistic needs, which could benefit the NLP and HCI communities. Finally, we hope that our codebase, which includes off-the-shelf frontend and backend codes and shared components of agents, will inspire the development of more innovative applications.

## 2 RELATED WORKS AND PRELIMINARIES

Building agents that operate intelligently in specific environments has a long history across the fields of traditional artificial intelligence (Kaelbling et al., 1987, Maes, 1990, Brooks, 1991, Russell, 2010), reinforcement learning (Sutton \& Barto, 2005|| Mnih et al.| 2013; Finn et al., 2017), and recent language agents (Yao et al., 2022b; Yang et al. 2023a; Wang et al., 2023a) which are built on LLMs.

Generally, language agents can be formalized as a partially observable Markov decision process (POMDP) ( $\mathcal{H}, \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{R})$ with natural language space $\mathcal{H}$, state space $\mathcal{S}$, action space $\mathcal{A}$, observation space $\mathcal{O}$, transition function $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, and reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$. Given the dialogue history (user queries and agent responses) $h \in \mathcal{H}$, an agent generates executable action $a_{t} \in \mathcal{A}$ and interacts with the environment. An action is executable if it is within the action space $\mathcal{A}$ and incurs a change in the state $s_{t+1} \in \mathcal{S}$, and an execution feedback as observation $o_{t+1} \in \mathcal{O}$. The interaction loop repeats until certain stop tokens as action is generated. It is worth noting that currently most LLM-based language agents typically do not require training, and as a result, rewards are often not regarded.

Table 1: Comparison between OpenAgents and current existing works on building prototypes and benchmarks on agent concept. Online stands for whether it is deployable and can be online-hosted. UI stands for providing a user interface. \#Tools is the number of tools contained. Feedback stands for supporting feedback from users. Web. is short for web browsing. "Controlled" means the framework/benchmark sets up deterministic environments ahead for the agent to operate actions, while "Wild" ones let the agent take actions in the open-ended real-world environment. The "**" marker indicates their agent only implements basic web browsing methods (e.g. web crawl) or uses external APIs (e.g., google search). The " " marker indicates OpenAgents offers additional features beyond just statistical numbers, for example automatic selection on tools. All statistics in the table are collected by September 2023.

| Name | Interface |  |  | Environment |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Online | Human Feedback | UI | Coding Env. | \#Tools | Web |
| AutoGPT (Gravitas, 2023) | $x$ | $\checkmark$ | CLI | Wild | 15 | $\checkmark$ |
| BMTools Qin et al. 2023a) | $x$ | $x$ | - | Controlled | 32 | $x^{*}$ |
| BabyAGI (Nakajima 2023) | $x$ | $x$ | - | Controlled | ![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-03.jpg?height=43&width=113&top_left_y=1726&top_left_x=1539) | $x^{*}$ |
| Gentopia (Xu et al. 2023a) | $x$ | $\checkmark$ | CLI | Controlled | 15 | $x^{*}$ |
| Open Interpreter (Lucas 2023) | $x$ | $\checkmark$ | CLI | Wild | 1 | $x$ |
| GAs (Park et al., 2023) | $x$ | $x$ | Web | - | - | $x$ |
| Agent Verse (Chen et al. 2023) | $x$ | $x$ | Web | - | - | $x$ |
| Camel (Li et al. 2023b) | $\checkmark$ | $x$ | Web | - | - | $x$ |
| Agents (Zhou et al. 2023c) | $\checkmark$ | $\checkmark$ | Web | Wild | 11 | $x^{*}$ |
| OpenAgents (ours) | $\bar{\checkmark}$ | $\bar{\Omega}$ | Web | Controlled \& Wild | $\geq 200^{+}$ | $\overline{\Omega^{+}}$ |
| ChatGPT Plus (closed-source) | $\checkmark$ | $\checkmark$ | Web | Controlled \& Wild | $\geq 500$ | $\checkmark$ |

Numerous frameworks and codebases surrounding language agents have been established. One category comprises the construction of some proof-of-concept, catchy prototype implementations (Chase, 2022, Gravitas, 2023, Nakajima, 2023). The core technique is prompting LLMs and demonstrating astonishing results in certain cases. On this basis, some improved and updated conceptual frameworks (Xu et al. 2023a; Zhou et al. 2023c) have been proposed. Another category emphasizes the evaluation of language agents. These evaluations are often conducted in fully simulated environments (self-hosted web pages (Yao et al., 2022a; Zhou et al., 2023b), etc.), semi-real environments (for instance, utilizing real tool API calls (Qin et al. 2023b)) or combination (Liu et al. 2023, Wang et al. 2023b). Additionally, there are frameworks focusing on the study of multiagents and their behaviors (Li et al., 2023b; Park et al., 2023, Chen et al. 2023, Hong et al. 2023. Wu et al. 2023). Table 1 shows an overall comparison of these frameworks and codebases.

## 3 PlATFORM DESIGN AND IMPLEMENTATION

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-04.jpg?height=410&width=1350&top_left_y=348&top_left_x=385)

Figure 2: System overview of OpenAgents' architecture comprising (1) User Interface, a bridge that facilitates communication between the user and the agent, and manages backend operations; and (2) Language Agent, encompassing the language model, tools, and environment, driving the agent's decision-making processes. The flow of interaction typically follows from the user needs, through the user interface, and culminates in the language agent taking specific actions using its various components.

### 3.1 SYStEMATIC DESIGN

In this section, we introduce our systematic efforts in OpenAgents, aiming towards addressing this gap between existing agent frameworks and real-world functional agent applications.

The OpenAgents architecture can be generally split into two parts corresponding to Figure 2. (1) User Interface, including both the frontend and backend; (2) Language Agent, including language models, tools, and environments. OpenAgents provides a well-built interface for user-agent communication. Upon receiving instructions from the users, the agent will then plan, and take action in the environment via using tools.

User Interface We have devoted significant effort to crafting a user-friendly and highly functional User Interface. We have tackled a multitude of highly reusable business logic to use and host agents. This effort has resulted in robust support for various technical aspects within the demo, such as backend server operations, error handling, data streaming, etc. Our goal has been to make the OpenAgents not only user-friendly but also to provide enhanced usability. Although these aspects were not the central focus of the current agent frameworks (Chase, 2022; Xu et al., 2023a, Zhou et al. 2023c), we firmly believe that these behind-the-scenes efforts should not be. They hold significant meaning for future research, contributing to the overall robustness and usability of the system.

Language Agent In our design, the language agent comprises three essential components: the language model, the tool interface, and the environment. Currently, we adopt our prompting method based on the approach introduced in ReAct (Yao et al. 2022b). The agent generally follows a sequential process of Observation $\rightarrow$ Deliberation $\rightarrow$ Action in each turn of interaction. We also prompt the language model to produce easily parsable text. The tool interface includes parsers capable of translating this text into executable actions, such as generating code or making API calls. Subsequently, these actions are executed within the corresponding environment. We've also invested considerable effort into behind-the-scenes challenges, like stable API calling, tools scaling, and building a sandbox environment.

### 3.2 PRACTICAL IMPLEMENTATION CHALLENGES

The implementation journey unveiled several challenges essential for in-the-wild agent readiness. Below are the simplified insights and highlights into the core facets of our implementation. For more details, please kindly refer to Appendix A.

For User Interface implementation, we tackle the following challenges:

1. Adaptive Data Mapping $\$$ A.1.1 Drawing from database terminology, we employ the concept of DataModel. This model effectively converts various raw data types-such as text, codes, images, and tables-into formats optimized for both human-side pretty formatting, LLM contexts, and persistent data storage, streamlining communication between system components and external entities.
2. Strategic Data Storage $\$$ A.1.2 We adopt a strategic data storage paradigm catering to the multiuser nature of OpenAgents. Utilizing in-memory storage for temporary variables, Redis (Carlson, 2013) for global variables, and MongoDB (Banker et al. 2016) for user-centric data orchestrated an efficient data management and retrieval ecosystem.
3. User-Centric Interface \$A.1.3 An Adaptive User Interface was crafted, bridging the interaction chasm between users and the system. This UI, catering to various operational environments, renders rich media and interactive content (images, code snippets, console outputs, and interactive visualizations, etc.), augmenting user engagement and task efficiency.
4. Real-time Response Streaming $\$$ A.1.4 A streaming approach was adopted to mitigate the latency inherent in receiving long text completions. Based on streaming API and push-down automata, this innovation allowed for real-time parsing and rendering of generated tokens, significantly ameliorating the immediacy of user feedback.
5. System Robustness $\$$ A.1.5 Enhancing agent robustness was identified as a quintessential requirement for delivering a realistic user experience. Key areas including effective failure handling (e.g., API calling failure), prompt response generation (e.g., LLM calling APIs pool set and streaming handling), and token overflow management (chat history over length for LLM to handle) were delineated and addressed, ensuring reliable functionality under diverse real-world scenarios.
6. Browser Control via Chrome Extension $\$$ A.1.6 A Chrome extension was engineered, endowing our web agent with direct browser control capabilities. This setup facilitates real-time user monitoring and intervention during web interactions, and enables web browsing commands on the user side, enhancing user control and trust in the system.

For Language Agents, we provide solutions to the following challenges:

1. Data Grounding $\$$ A.2.1. Our system lets users upload files which agents can process by writing codes. We've thus established a grounding source pool to store user-uploaded data. With DataModel, each file type is linearized and indexed by its name, allowing agents to retrieve and use content as instructed by humans.
2. Automatic Tool Selection $\$$ A.2.2 Users previously had to select a plugin manually, like OpenAI Plugins (OpenAI 2023a), for command execution. Recognizing the challenge of manual plugin selection from numerous options, we've integrated a feature namely "Auto Selection", which auto-detects the most relevant tool based on user instructions, streamlining the process.
3. Automatic Tool Scaling $\$$ A.2.3. While tool creation poses unique challenges, especially for agent use within the LLM infrastructure, we've sourced API provider information from platforms like RapidAPI and OpenAI plugins store. This approach has led to over 200 high-quality plugins, albeit with challenges that occasionally require human oversight. Further exploration is needed for efficient scaling.
4. Executable Environments $\$$ A.2.4 . By "executable", we imply the transformation of language model outputs into actionable tasks within a specific context. Constructing an application-level, multi-user executable environment poses challenges due to the need for safety, robustness, and functionality. We managed to implement sandbox environments catering to code execution, API interactions, and web navigation. These environments serve as a comprehensive testbed for agents, facilitating tasks like code generation, plugin interactions, and web manipulations.

Collectively, these advancements contribute towards architecting a more coherent, robust, and userfriendly platform. Moreover, We believe they also provide a rich repository of learnings, promising to guide future implementations in similar domains.

## 4 OpenAgents

In OpenAgents, we develop three distinct agents: namely the Data Agent for data analysis, the Plugins Agent for plugin integration, and the Web Agent for autonomous web browsing. The three agents are experts in different domains, just like OpenAI's ChatGPT Plugins (OpenAI, 2023a). Moreover, our implementation is purely based on the top of open language APIs (and theoretically any other open-sourced LLMs), such as GPT-4 (OpenAI, 2023b), and Claude (Anthropic, 2023). We hope OpenAgents can serve as a pioneer in the move towards democratizing potent, real-world language agents. Further, we contend that OpenAgents can serve as an experimental testbed for researchers across various disciplines who share an interest in the future trajectory of language agents.

### 4.1 DATA AGENT

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-06.jpg?height=408&width=1348&top_left_y=836&top_left_x=388)

Figure 3: Pipeline (left) and demonstrations (mid and right) of Data Agent.

The data agent has been designed and implemented to deal with a wide spectrum of data-related tasks that real users encounter daily. We support code generation and execution in two programming languages: Python and SQL. We also integrate several data tools for the agent to use: (1) Kaggle Data Search (datasets search on Kaggle via calling API); (2) Data Profiling (heuristic data profiling providing basic data information); (3) ECharts Tool (interactive ECharts plotting). We prompt the agent to proactively use these data tools to respond to user requests. Recognizing the intensive coding requirements for data agent, we have opted to embed language models in the tool, and let the tools generate code rather than the agent. Specifically, tools such as Python, SQL, and ECharts will generate code, thereby harnessing the language models' full programming prowess and alleviating the strain on the agent itself.

Equipped with these data tools, the agent adeptly manages various data-centric requests. It transcends the boundaries of mere text and code generation to proficiently perform data queries, visualization, manipulation tasks, etc. An illustrative example is depicted in Figure 3, where users can upload custom files, such as tables and images, and make successive inquiries about the uploaded data. While we've highlighted the data agent's key capabilities, the full potential of our data agent extends beyond this brief overview. Much like OpenAI's Advanced Data Analysis(previously known as Code Interpreter) (OpenAI 2023a), we hope the data agent can serve as a versatile tool for myriad user workflows; for more use cases, please see Appendix B.1.

### 4.2 PlUGINS AGENT

The plugin agent has been meticulously designed to cater to the multifaceted requirements of users' daily tasks which necessitate additional plugins, such as shopping, searching, news reading, weather forecasting, and website creation. We have integrated over 200 plugins from various sources, some of the prominent plugins include (1) Google Search; (2) Wolfram Alpha; (3) Zapier; (4) Klarna; (5) Coursera; (6) Show Me; (7) Speak; (8) AskYourPDF; (9) BizToc; and (10) Klook. Special attention has been paid to ping the APIs, function calling interface, and API response length, enabling LLMbased agents to optimally leverage these plugins.

Users can choose one or multiple plugins they would like to let their agent leverage based on the given instructions of their needs. Examples can be observed in Figure 4 In instances where users
![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-07.jpg?height=414&width=1348&top_left_y=281&top_left_x=386)

Figure 4: Pipeline (left) and demonstrations (mid and right) of Plugins Agent.

are uncertain about the appropriate plugins for their needs, we have incorporated a feature that automatically selects the most relevant plugins based on their instructions. We posit that the plugin agent can be instrumental in myriad scenarios, enriching various aspects of users' daily lives. For a more comprehensive list of use cases, refer to Appendix B. 2

### 4.3 WEB AGENT

![](https://cdn.mathpix.com/cropped/2024_06_04_962b0325ed43ede6d1f0g-07.jpg?height=418&width=1369&top_left_y=1103&top_left_x=389)

Figure 5: Pipeline (left) and demonstrations (middle and right) of Web Agent.

We present the web agent as a specialized tool designed to enhance the capabilities of the chat agent. The main interaction interface still lies with the chat agent, but when necessary, it seamlessly involves the web agent. The web agent then delivers the final response to the user, as shown in Figure 5. This design strategy is not just theoretical; it has been implemented and demonstrates several notable advantages.

First, the chat agent systematically processes important parameters like user inquiries or initiating URLs before transferring them to the web agent. This ensures that there is a strong alignment with user intentions and facilitates clearer communication. To clarify further, when dealing with complex tasks that may seem ambiguous or have multiple aspects, the chat agent uses a decompositional approach. Herein, overarching directives from users are segmented into more digestible sub-tasks. The chat agent then interfaces with the web agent using these sub-instructions, sequentially, ensuring more granular and efficient problem resolution. Moreover, our configuration empowers a dynamic interplay of multi-turn web navigation interspersed with chat dialogues, thereby accommodating more layered and adaptable user queries. By distinctly demarcating the roles and responsibilities of the web-browsing agent and the chat agent, we pave the way for the independent evolution and continual refinement of each module. For a deeper dive into practical applications and the potential of the web agent, we invite readers to peruse Appendix B. 3

## 5 FROM RESEARCH TO REAL-WORLD DEPLOYMENT

As we navigated the trajectory from conceptualization to real-world deployment, the journey was replete with challenges and learnings. The section reflects on the crucial challenges encountered,
the lessons imbibed from real-world scenarios, and the evaluation complexities that surfaced through this transition.

Challenges of Transforming LLMs into Real-world Apps through Prompting When building applications for real users based on LLM prompting, we specify certain requirements through the instructions in the prompt. Some of these instructions are designed to ensure that the output of the LLMs conforms to a specific format for our backend logic to process (e.g., output in the form of a dictionary with specific keys). Others aim to enhance the aesthetic appeal of the output (e.g., listing items separately whenever possible). Additionally, some instructions serve to defend against potential attacks (e.g., refuse maliciously crafted infinite loops in the program by users). As illustrated in the $\S$ C, the accumulation of these instructions often results in several hundred tokens, thereby demanding certain requirements on the instruction tracking ability and supported context length of the LLMs. With inferior LLMs, sometimes the improper output format leads to an unsatisfactory frontend appearance or even an inability to complete the response. A positive sign is that there have been significant improvements in these aspects in current open-source models (i.a. Peng et al., 2023a; Tworkowski et al., 2023, Zhou et al., 2023a, Peng et al., 2023b; Xiong et al., 2023, Xu et al., $2023 \mathrm{c}$. Additionally, a greater emphasis is required on the foundational development and research of agent models, and on training dedicated agent models (maybe not LLMs) that are tailored for specific domains and requirements. This approach may turn out to be more effective and controllable than relying on purely prompting a general powerful but fixed model.

Uncontrollable Real-world Factors Upon deployment in the real world, we encountered numerous uncontrollable factors triggered by users, internet infrastructure, business logic, and so forth, that haven't been well-modeled, necessitating a reevaluation and often overturning of many assumptions and forms adopted in past research. We had to assume the possibility of the server hosting the APIs we call crashing, monitoring and robustly completing user commands during such crashes than those assumed in tool-use studies (i.a. Cheng et al., 2023, Schick et al. 2023, Qin et al. 2023a). There could be instances where users may become dissatisfied during the response generation, intervening and causing the language model to halt generation midway. Unpredictable occurrences like CAPTCHAs popping up or advertisements altering the web page, could introduce a degree of randomness even in relatively stable web structures, which is unconsidered in previous autonomous web browsing works (i.a. Shi et al., 2017, Zhou et al., 2023b). These uncontrollable scenarios beckon a deeper exploration or the proposition of more realistic modeling approaches.

Extra Metrics from Real-world Scenarios Research primarily emphasizes performance metrics, frequently overlooking essential requirements derived from real-world scenarios. An instance we learn during implementation is streaming. It allows users to quickly perceive system responses instead of waiting for lengthy text to generate. Specially designed prompting makes the format of agent response look prettier. These significantly impact user experience. Existing methods (i.a. Yao et al., 2022b; Xu et al., 2023b, Lin et al. 2023) haven't considered much of its effects, resulting in slower response times and bad user experience in practical applications, regardless of their superior performance metrics in accuracy. These findings should be contemplated in subsequent applicationdriven studies. And we also need to mention the trade-offs between the performance and user experience.

Evaluation Complexity Arising from System Issues While constructing agents directly aimed at applications might cater to a broader user base and unearth more challenges for better evaluation, building applications based on LLMs introduces extra complexity. This complexity makes it challenging to discern whether certain failure cases stem from the limitations of the LLM-based application or from inadequacies in our logic code. For instance, when a user attempts to download the file generated from plugins and our system fails to support it, this doesn't necessarily reflect the capabilities of the agent. Another example is that after building for a specific application, due to the extended system chain and the instability of LLMs, it becomes challenging to adjust components like prompt. Refinement of the system on design and operational logic of intelligent agents to simplify the procedures is promising and essential in this way.

## 6 DISCUSSIONS AND FUTURE WORK

In this section, we aim to share some insights on future directions.

Agent Applications OpenAgents sets up a whole pipeline of building application-level language agents, thus paving the way for other innovative applications such as customizable dialogue systems, multimodal interaction, and automated workflow integrations for end-users. Each of these applications not only offers unique advantages but also collectively contributes to a richer and more user-centric agent application environment.

Tool and Component Integration OpenAgents explores and addresses the fundamental requisites for constructing a practical-level agent application, laying down a robust foundation that allows the community to effortlessly expand horizontally by integrating additional components such as tools (e.g., integrate from more diverse API sources like PublicAPI ${ }^{2}$ ), extending more foundation models (e.g., recent large multimodal models (i.a. Li et al., 2023a; Yang et al., 2023b)), adapting to new UIs designs, etc.

Human-LM Interaction Benefiting from the easiness of building new LLM-based agent applications from our platform, we believe OpenAgents can consequently be helpful in building application demos for the studies of Human-Computer Interaction (HCI) researchers to delve into the design of more intuitive and user-friendly interfaces (i.a. Suh et al., 2023, Kim et al., 2023, Angert et al. 2023), thereby enhancing user engagement and satisfaction.

Adaptive UI Generation Automatic creation of user interfaces that adjust or tailor themselves based on specific criteria, such as the user's device, preferences, or context (i.a. Ontanon \& Zhu, 2021; Todi et al., 2021; Jiang et al., 2023) is another interesting but challenging direction. Researchers can further explore the various ways LLMs can be employed in adaptive UI based on OpenAgents, as well as their impact on user experience, in different formats.

In-the-wild Evaluation of LLMs Establishing robust, impartial evaluation methodologies for LLMs is essential for an unbiased assessment of their capabilities and performance. Current agentaround benchmarks (i.a. Yang et al., 2023a, Liu et al., 2023; Wang et al., 2023b) evaluate agents through benchmarks with pre-collected data and under controlled environments. While these evaluations are crucial, they often do not fully represent the dynamic challenges encountered in real-world scenarios. Platforms like Vicuna Arena (Zheng et al., 2023a) have taken the lead in evaluating chatbots in the wild, marking a significant step towards more realistic evaluation settings. However, there's room for further development in metrics that cater to the nuances of language agents, especially those that tie language understanding with grounding in real-world contexts. Community contributions in expanding or refining these evaluation metrics and platforms are highly encouraged as they would significantly advance the field, providing more accurate insights into the practical performance and capabilities of LLMs.

## 7 CONCLUSION

In this work, we introduced OpenAgents, an open-source platform, tailored to meld the advancements of LLMs with practical, user-focused applications. OpenAgents has built agents for three typical applications: data analysis, tool utilization, and web browsing, demonstrating its practical utility. During the development of OpenAgents, we identified numerous challenges that were unique to constructing in-the-wild agents. These challenges highlighted the complexities of transitioning from theoretical designs to fully operational agents that cater to actual user needs and interactions. By providing a holistic, transparent, and deployable platform, we aim not only to make the immense potential of LLMs accessible to a wider audience beyond developers and researchers but also to facilitate grounded research through exposure to real-world scenarios, empowering the community to harness and refine the capabilities of state-of-the-art language agents. We sincerely hope OpenAgents can inspire and pave the way for a plethora of applications and platforms, streamlining the symbiotic relationship between humans and intelligent language agents.[^2]

## ACKNOWLEDGMENTS

We extend our profound gratitude to Google Research, Amazon AWS, and Salesforce Research. The munificent gift funds and computational resources they bestowed upon us have been instrumental in the realization of this project. Their unwavering support has been pivotal in the success of our endeavors. We also wish to express our heartfelt appreciation to our contributors for their relentless commitment and insightful perspectives: Roxy Rong, Yixian Zhang, Chen Henry Wu, Toh Jing Qiang, Jansen Wong, and Jixuan Chen. Equally, our discourse was enriched by the sagacious discussions and invaluable advice from Yiru Chen, Bohan Zhang, Rui Zhang, Ruiqi Zhong, Michihiro Yasunaga, and Ansong Ni. Their collective wisdom has been an invaluable cornerstone in shaping this research.

## REFERENCES

Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, and Hariharan Subramonyam. Spellburst: A node-based interface for exploratory creative coding with natural language prompts. arXiv preprint arXiv:2308.03921, 2023.

Anthropic. Introducing claude, 2023. URL https://www.anthropic.com/index/ introducing-claude. Accessed: October 11, 2023.

Jean-Michel Autebert, Jean Berstel, and Luc Boasson. Context-free languages and pushdown automata. Handbook of Formal Languages: Volume 1 Word, Language, Grammar, pp. 111-174, 1997

Kyle Banker, Douglas Garrett, Peter Bakkum, and Shaun Verch. MongoDB in action: covers MongoDB version 3.0. Simon and Schuster, 2016.

Rodney A Brooks. Intelligence without representation. Artificial intelligence, 47(1-3):139-159, 1991 .

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.

Josiah Carlson. Redis in action. Simon and Schuster, 2013.

Harrison Chase. Langchain: Building applications with llms through composability, 2022. URL https://github.com/langchain-ai/langchain.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023.

Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Binding language models in symbolic languages. ICLR, abs/2210.02875, 2023.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web, 2023.

Roy Thomas Fielding. Architectural styles and the design of network-based software architectures. University of California, Irvine, 2000.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126-1135. PMLR, 2017.

Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2023.

Significant Gravitas. Auto-gpt, 2023. URL https://github.com/ Significant-Gravitas/Auto-GPT

Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.

Yue Jiang, Yuwen Lu, Christof Lutteroth, Toby Jia-Jun Li, Jeffrey Nichols, and Wolfgang Stuerzlinger. The future of computational approaches for understanding and adapting user interfaces. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, CHI EA '23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394222. doi: 10.1145/3544549.3573805. URL https://doi.org/10.1145/ 3544549.3573805 .

Leslie Pack Kaelbling et al. An architecture for intelligent reactive systems. Reasoning about actions and plans, pp. 395-410, 1987.

Tae Soo Kim, Yoonjoo Lee, Minsuk Chang, and Juho Kim. Cells, generators, and lenses: Design framework for object-oriented interaction with large language models. In The 36th Annual ACM Symposium on User Interface Software and Technology (UIST'23), 2023.

Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023a.

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023b.

Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Apibank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023c.

Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. arXiv preprint arXiv:2305.17390, 2023.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. AgentBench: Evaluating LLMs as Agents, August 2023.

Killian Lucas. open-interpreter: Openai's code interpreter in your terminal, running locally. https://github.com/KillianLucas/open-interpreter, 2023. URL https: //github.com/KillianLucas/open-interpreter. GitHub repository.

Pattie Maes. Designing autonomous agents: Theory and practice from biology to engineering and back. MIT press, 1990.

Andrew M McNutt, Chenglong Wang, Robert A Deline, and Steven M Drucker. On the design of ai-powered code assistants for notebooks. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pp. 1-16, 2023.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

Yohei Nakajima. Babyagi, 2023. URLhttps://github.com/yoheinakajima/babyagi

Santiago Ontanon and Jichen Zhu. The personalization paradox: The conflict between accurate user models and personalized adaptive systems. In 26th International Conference on Intelligent User Interfaces - Companion, IUI '21 Companion, pp. 64-66, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380188. doi: 10.1145/3397482.3450734. URL https://doi.org/10.1145/3397482.3450734.

OpenAI. Chatgpt plugins. https://openai.com/blog/chatgpt-plugins, 2023a.

OpenAI. GPT-4 Technical Report, March 2023b.

Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior, August 2023.

Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023a.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023b.

Cheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Disentangling abstract and concrete reasonings of large language models through tool creation. arXiv preprint arXiv:2305.14318, 2023.

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023a.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023b.

Stuart J Russell. Artificial intelligence a modern approach. Pearson Education, Inc., 2010.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pp. 3135-3144. PMLR, 2017.

Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. Restgpt: Connecting large language models with real-world applications via restful apis. arXiv preprint arXiv:2306.06624, 2023.

Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings, 2023.

Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. Sensecape: Enabling multilevel exploration and sensemaking with large language models. arXiv preprint arXiv:2305.11483, 2023.

Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 16:285-286, 2005. URL https://api.semanticscholar. org/CorpusID: 9166388

Kashyap Todi, Gilles Bailly, Luis Leiva, and Antti Oulasvirta. Adapting user interfaces with model-based reinforcement learning. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445497. URL https: //doi.org/10.1145/3411764.3445497.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.

Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.

Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115-152, 1995.

Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multiagent conversation framework. arXiv preprint arXiv:2308.08155, 2023.

Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.

Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023.

Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. Gentopia: A collaborative platform for tool-augmented llms. arXiv preprint arXiv:2308.04030, 2023a.

Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. Rewoo: Decoupling reasoning from observations for efficient augmented language models. arXiv preprint arXiv:2305.18323, 2023b.

Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing natural language and code for language agents. arXiv preprint arXiv:2310.06830, 2023c.

John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. arXiv preprint arXiv:2306.14898, 2023a.

Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023b.

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757, 2022a.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022b.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023a.

Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, and Kevin ChenChuan Chang. Gpt-fathom: Benchmarking large language models to decipher the evolutionary path towards gpt-4 and beyond, 2023b.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023a.

Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023b. URL https://webarena. $\mathrm{dev}$.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents, 2023c.
