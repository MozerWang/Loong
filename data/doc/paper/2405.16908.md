# Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words? 

Gal Yona<br>Google Research<br>galyona@google.com roeeaharoni@google.com

Mor Geva<br>Tel Aviv University,<br>Google Research<br>pipek@google.com


#### Abstract

We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., "I'm not sure, but I think..."). We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully communicating uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.


## 1 Introduction

Despite their unprecedented capabilities, large language models (LLMs) often output erroneous information (Ji et al., 2023; Lin et al., 2021; Mallen et al., 2022; Kandpal et al., 2023). Moreover, LLMs typically communicate such inaccurate information in a fluent, decisive, and persuasive manner, which may lead users to overly rely on their false output (Buçinca et al., 2021; Passi and Vorvoreanu, 2022).

We argue that a possible pathway for improving LLM trustworthiness is to have the model communicate its uncertainty in words, as part of its generated response (Baan et al., 2023; Vasconcelos et al., 2023). Expressing uncertainty in natural language has several benefits over using numerical estimates. First, language provides a rich space, which can be useful to convey the source of the model's uncertainty. Second, it is generally perceived as more intuitive to humans (Zimmer, 1983;

![](https://cdn.mathpix.com/cropped/2024_05_29_9dc2d3e643f0c9812366g-01.jpg?height=454&width=759&top_left_y=732&top_left_x=1051)

Figure 1: We define faithful response uncertainty based on the gap between the decisiveness (blue) of the response and the model's intrinsic confidence in it (hatched orange). We empirically show: (1) with standard decoding, models answer decisively even in the presence of uncertainty (top left); (2) when prompted to express uncertainty, generated hedges are not faithful to the model's intrinsic uncertainty (bottom left).

Wallsten et al., 1993; Windschitl and Wells, 1996). Indeed, a recent user study by Kim et al. (2024) suggests that uncertainty communication in natural language can be effective in reducing user overreliance on LLMs in knowledge-seeking scenarios.

However, uncertainty expressions are only useful when they faithfully reflect the model's intrinsic uncertainty. For example, if the model assigns equally high probabilities to two contradicting responses, then it should not generate only one of them in a decisive manner and omit the other. We formalize this through the notion of faithful response uncertainty (\$2), an example-level score that quantifies the gap between the (linguistic) decisiveness in which the model conveys its assertions and its intrinsic confidence in them ( Fig. 1). Our approach differs significantly from prior work in that we aim to align the decisiveness of the generated response with the model's intrinsic confidence, rather than with external factuality verdicts (Kadavath et al., 2022; Kuhn et al., 2023; Lin et al., 2022; Mielke et al., 2022); see discussion in §C and additional related work in $\S \mathrm{D}$.

Next (§3), we propose a concrete implementation for decisiveness and confidence scoring, using Gemini Ultra (Gemini-Team, 2023) as a judge, which shows high correlation with human judgement. Then ( $\$ 4$ and $\S 5$ ), we use our implementation to evaluate the faithfulness of leading LLMs (Gemini family (Gemini-Team, 2023), GPT-3.5 and GPT-4 (Achiam et al., 2023)) on two question answering (QA) datasets (Natural Questions (Kwiatkowski et al., 2019) and PopQA (Mallen et al., 2022)), using greedy decoding with a standard QA prompt as well as a series of prompting methods that encourage the expression of uncertainty. We find that:

- With standard decoding, virtually all the models answer decisively, even in the presence of significant intrinsic uncertainty.
- While prompting the model to express uncertainty sometimes induces expressions of uncertainty, these hedges are not well-aligned with the model's intrinsic uncertainty.

Taken together, our results suggest LLMs are incapable of faithfully conveying their uncertainty in natural language, hindering their trustworthiness.

## 2 Faithful Response Uncertainty

Our goal is to evaluate whether models can express uncertainty in words to faithfully reflect their intrinsic uncertainty. To this end, we first propose to consider the decisiveness with which assertions in a response are expressed. Given a query $\mathbf{Q}$ and a response $\mathbf{R}$ generated by a model $M$, we view $\mathbf{R}$ as a sequence of assertions $\mathcal{A}(\mathbf{R})=\left\{A_{1}, \ldots, A_{n}\right\}$, each expressed with some level of decisiveness that is derived from possible hedging expressions associated with it. ${ }^{1}$ For example, given the query "Tell me about Barack Obama", the response "Barack Obama is an American politician. I think he was born in 1961, but I'm not sure." contains two assertions: $A_{1}=$ Barack Obama is an American politician, and $A_{2}=$ Barack Obama was born in 1961. While $A_{1}$ is conveyed decisively in the response, $A_{2}$ is less decisive due to the hedging expressions "I think" and "I'm not sure".

We consider a response $\mathbf{R}$ as faithful to $M$ if for every assertion $A \in \mathcal{A}(\mathbf{R})$, the decisiveness in[^0]

which $A$ is conveyed matches $M$ 's intrinsic confidence in $A$ :

## Definition 1 (Faithful Response Uncertainty)

For a query $\mathbf{Q}$ and a response $\mathbf{R}$ generated by a model $M$, the faithfulness of $\mathbf{R}$ with respect to $M$ 's intrinsic confidence is given by:

$$
\begin{aligned}
& \text { faithfulness }_{M}(\mathbf{R} ; \mathbf{Q}) \equiv 1- \\
& \frac{1}{|\mathcal{A}(\mathbf{R})|} \sum_{A \in \mathcal{A}(\mathbf{R})}\left|\operatorname{dec}(A ; \mathbf{R}, \mathbf{Q})-\operatorname{conf}_{M}(A)\right|
\end{aligned}
$$

where $\operatorname{dec}(A ; \mathbf{R}, \mathbf{Q}) \in[0,1]$ quantifies the decisiveness of the assertion $A$ in $\mathbf{R}$ and $\operatorname{conf}_{M}(A) \in$ $[0,1]$ quantifies the intrinsic uncertainty of $M$ regarding $A$.

Note that faithfulness (shorthand f) is in $[0,1]$, where a maximal value of 1 is obtained when every assertion's decisiveness matches the model's intrinsic confidence. Lower faithfulness values are obtained in cases of unnecessary hedging, that is, expressing uncertainty in assertions that the model is certain about, or lack of hedging, i.e., not expressing uncertainty in assertions the model is not confident about. See examples in Fig. 1.

## 3 Measuring Decisiveness \& Uncertainty

We now propose an implementation to the faithfulness score, focusing on the setting of short-form question answering (QA), where $\mathbf{Q}$ is a factual question (e.g., "When was Barack Obama born?") and $\mathbf{R}$ is typically a short answer with a single (possibly hedged) assertion (e.g., "August 196l").

Quantifying Decisiveness Prior work quantified the decisiveness of an assertion as a binary notion, based on whether the assertion is accompanied by a hedging expression or not (Mielke et al., 2022). However, this captures only little of the expressivity through which hedging expressions can convey uncertainty. Recognizing that decisiveness is subjective in nature, we draw inspiration from definitions of veridicality (Giannakidou, 1999; De Marneffe et al., 2012) and propose the notion of perceived decisiveness, which aims to be relativized to particular agents or perspectives. Formally, we define the perceived decisiveness of an assertion $A \in \mathcal{A}(\mathbf{R})$ as the probability an agent would assign to $A$ being true judging purely based on $\mathbf{R}$ :

$$
\begin{equation*}
\operatorname{dec}(A ; \mathbf{R}, \mathbf{Q})=\operatorname{Pr}[\mathrm{A} \text { is True } \mid \mathbf{R}, \mathbf{Q}] \tag{1}
\end{equation*}
$$

Quantifying Uncertainty Following previous work (Kuhn et al., 2023; Manakul et al., 2023; Tian et al., 2023a), we quantify certainty via consistency. Concretely, for a query Q (e.g., "When was Barack Obama born?") we quantify the uncertainty of a generated assertion A (e.g., "Barack Obama was born in 1961") by examining the consistency between this assertion and re-sampled answers to $\mathbf{Q}$ : If the generated answers agree with $A$ (e.g., "1961", "I think he was born in 1961", or "August 4, 1961."), then we say $M$ is confident in $A$. Conversely, assertions that contradict $A$ (e.g., " 1962 " or "Probably 1955 ”) indicate that $M$ 's confidence in $A$ is lower. ${ }^{2}$ Formally, given a question $\mathbf{Q}$ and a generated response $\mathbf{R}$ consisting of a single assertion $A$, let $\left\{\mathbf{R}_{1}, \ldots, \mathbf{R}_{k}\right\}$ be the set of sampled responses and $\left\{A_{1}, \ldots, A_{k}\right\}$ the set of corresponding assertions (i.e., $\mathcal{A}\left(\mathbf{R}_{i}\right)=\left\{A_{i}\right\}$ ). We quantify the confidence of $M$ in $A$ as the fraction of sampled assertions that contradict $A$ :

$$
\begin{equation*}
\operatorname{conf}_{M}(A) \equiv 1-\frac{1}{k} \sum_{i} \mathbf{1}\left[A \text { contradicts } A_{i}\right] \tag{2}
\end{equation*}
$$

Implementation Details We implement the above scores (Eq. 1, Eq. 2) by prompting a "judge" LLM. For a given query $\mathbf{Q}$ and a generated response $\mathbf{R}$, we first extract the assertion $A$ in $\mathbf{R}$ and its decisiveness score using a few-shot prompt $\mathcal{P}_{d}$ (see Tab. 5 in $\S E$ ). Next, to quantify the model's intrinsic confidence, we sample $k=20$ additional answers for $\mathbf{Q}$ and extract their corresponding assertions with $\mathcal{P}_{d}$. Then, we use another few-shot prompt $\mathcal{P}_{c}$ (see Tab. 6 in $\S \mathrm{E}$ ) to check for every extracted assertion whether it contradicts $A$. In our experiments, we use Gemini Ultra as the judge.

Correlation with Human Judgement We evaluate the quality of our LLM-based scores, showing that they correlate well with human judgment.

For decisiveness (Eq. 1), we randomly sample 100 model answers generated in our experiments (§4) and rewrite each answer to include a hedging expression (e.g., "Highly likely"). Then, we score answers with our decisiveness prompt $\mathcal{P}_{d}$. Fig. 2 shows for each hedging expression the mean decisiveness score versus the distribution of perceived probabilities humans assigned to it (using survey data from Fagen-Ulmschneider (2023)). Overall, the LLM scores agree with the human evaluations.[^1]

![](https://cdn.mathpix.com/cropped/2024_05_29_9dc2d3e643f0c9812366g-03.jpg?height=388&width=777&top_left_y=246&top_left_x=1051)

Figure 2: Our mean decisiveness score ( $\star$ ) vs. IQR of human perceptions of probability (blue bars), obtained by Fagen-Ulmschneider (2023). The LLM-based outputs generally agree with the human judgements.

For confidence (Eq. 2), we compare the confidence scores for 100 randomly selected examples, when calculated with our prompt $\mathcal{P}_{c}$ versus when using labels written by the authors. We observe a high correlation of 0.97 between the two scores.

## 4 Experimental Setting

We evaluate whether LLMs faithfully reflect their uncertainty when answering questions.

Data We use knowledge-intensive QA datasets:

- PopQA (Mallen et al., 2022): Entity-centric questions constructed based on WikiData (Vrandečić and Krötzsch, 2014). PopQA covers many tail entities, which LLMs struggle to capture (Mallen et al., 2022; Kandpal et al., 2023; Yona et al., 2024). Thus, faithful responses are expected to require expressing uncertainty.
- Natural Questions (NQ) (Kwiatkowski et al., 2019): Unlike PopQA, NQ is comprised of user queries - hence it is more natural and better reflects the behavior of LLMs on real tasks.

As we focus on model uncertainty, ${ }^{3}$ we exclude ambiguous questions for which uncertainty can rise due to data uncertainty (see details in $\S \mathrm{A}$ ).

Models We evaluate leading instruction-tuned LLMs: OpenAI's GPT (gpt-3.5-turbo and gpt-4turbo) (Achiam et al., 2023) and Google's Gemini (Nano, Pro and Ultra) (Gemini-Team, 2023).

Methods We obtain answers using multiple prompts (see $\S \mathrm{E}$ ) with greedy decoding:

- Vanilla: The LLM is instructed to answer the question using a standard format Question: \{question\} \nAnswer: .[^2]

| Method | PopQA |  |  |  |  | Natural Questions |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | GemNano | GemPro | GemUltra | GPT-T-3.5 | GPT-T-4 | GemNano | GemPro | GemUltra | GPT-T-3.5 | GPT-T-4 |
| Vanilla | 0.52 | 0.53 | 0.54 | 0.52 | 0.53 | 0.54 | 0.54 | 0.54 | 0.54 | 0.57 |
| Granularity | 0.51 | 0.52 | 0.53 | 0.52 | 0.53 | 0.54 | 0.53 | 0.54 | 0.54 | 0.54 |
| Uncertainty | 0.51 | 0.57 | 0.70 | 0.53 | 0.58 | 0.53 | 0.56 | 0.59 | 0.54 | 0.57 |
| Uncertainty+ | 0.52 | 0.56 | 0.53 | 0.57 | 0.63 | 0.54 | 0.53 | 0.54 | 0.55 | 0.57 |

Table 1: State of the art models struggle at faithfully communicating uncertainty: $\mathrm{CMFG}$ results for each of the methods we test (higher is better). All models perform poorly, with cMFG close to the baseline value of 0.5 .

- Granularity: We prepend Vanilla an additional instruction to answer at an appropriate level of granularity (Yona et al., 2024), which may induce coarser and more-confident answers.
- Uncertainty: We prepend Vanilla an additional instruction to express uncertainty (via hedging) in cases of uncertainty.
- Uncertainty+: A variant of Uncertainty with few-shot demonstrations, which we manually craft per model using questions from TriviaQA (Joshi et al., 2017). We take $m(\mathbf{Q}, \mathbf{R})$ pairs where $M$ is certain in $\mathbf{R}$ and $\mathbf{R}$ is decisive, and $m$ pairs where $M$ is uncertain in $\mathbf{R}$ and $\mathbf{R}$ is not decisive. To account for model sensitivity to the particular choice of demonstrations (Perez et al., 2021; Lu et al., 2021; Min et al., 2022), we average the results over $r$ random choices of $2 m$ demonstrations. We use $m=2$ and $r=3$, which were sufficient to get consistent results.

Evaluation Given a model $M$ and a set of QA pairs $\left\{\left(\mathbf{Q}_{i}, \mathbf{R}_{i}\right)\right\}_{i=1}^{n}$, the mean faithful generation metric (MFG) quantifies the expected faithfulness of a single answer: $\mathrm{MFG}=\frac{1}{n} \sum_{i=1}^{n}\left[\mathrm{f}_{M}\left(\mathbf{R}_{i} ; \mathbf{Q}_{i}\right)\right]$. While MFG is a useful indicator, it heavily depends on the distribution of confidence values the model admits $^{4}$, making it less useful for comparing different models. Therefore, we utilize a second metric, conditional mean faithful generation (cMFG), that additionally conditions on the confidence level: $\underset{\substack{i \sim n \\ v \sim U[0,1]}}{ }\left[\mathbf{f}_{M}\left(\mathbf{R}_{i} ; \mathbf{Q}_{i}\right) \mid \operatorname{conf}_{M}\left(\mathbf{R}_{i} ; \mathbf{Q}_{i}\right)=v\right]$. In practice, we bin the conf. scores to 10 equallysized bins and condition on each bin. Note that cMFG essentially simulates MFG with uniformly random confidence scores, making it more appropriate for comparing different models. Particularly, 0.5 is a baseline value for $\mathrm{CMFG}$ as it is obtained for two simple decisiveness strategies that are independent of the model's confidence (always answering decisively / at a random level of decisiveness). We additionally track mean accuracy, decisiveness, confidence, and punting rate.[^3]

![](https://cdn.mathpix.com/cropped/2024_05_29_9dc2d3e643f0c9812366g-04.jpg?height=343&width=691&top_left_y=614&top_left_x=1091)

Figure 3: Mean accuracy, confidence, and decisiveness scores for Vanilla on PopQA (results on NQ show similar trends, see $\S \mathrm{B}$ ). Even the most accurate models answer decisively, despite non-trivial uncertainty.

## 5 Results

In Tab. 1 we report our faithfulness metric (cMFG) for all model-method-dataset combinations.

## Without special instructions, models generate decisive answers, even for uncertain answers

 (Fig. 3). Considering the Vanilla baseline, all models perform poorly in terms of faithfulness with cMFG close to 0.5 (Tab. 1 , top row). Specifically, models do not generate any expressions of uncertainty (i.e., dec $=1$ ), irrespective of the model's intrinsic confidence in the answer.State-of-the-art models cannot be easily steered towards faithfully expressing uncertainty via prompting. For the non-Vanilla baselines, there is a small increase in CMFG, with maximal scores reaching 0.63 for GPT-4 and 0.7 and Gemini-Ultra. While prompting models to express uncertainty sometimes yields hedging expressions (see examples in Tab. 3 in §B), the correlation between decisiveness and confidence is weak, indicating LLMs often hedge confident answers and answer decisively despite uncertainty (see Fig. 4 in §B).

## 6 Conclusion

We formalize the desiderata that a model's generated response should reflect its intrinsic uncertainty in natural language. Our evaluations reveal that modern LLMs perform poorly at this task, stressing the need for better alignment techniques towards ensuring trustworthiness in LLMs.

## Acknowledgements

We thank Jonathan Berant, Amir Globerson, Arslan Chaudhry, Ori Ram, Shuali Ravfogel, Or Honovich and Zorik Ghekman for providing helpful feedback on this manuscript.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.

Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734.

Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. 2023. Uncertainty in natural language generation: From theory to applications. arXiv preprint arXiv:2307.15703.

Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on ai in aiassisted decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1):1-21.

Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827.

A Philip Dawid. 1982. The well-calibrated bayesian. Journal of the American Statistical Association, 77(379):605-610.

Marie-Catherine De Marneffe, Christopher D Manning, and Christopher Potts. 2012. Did it happen? the pragmatic complexity of veridicality assessment. Computational linguistics, 38(2):301-333.

Ran El-Yaniv et al. 2010. On the foundations of noisefree selective classification. Journal of Machine Learning Research, 11(5).

Wade Fagen-Ulmschneider. 2023. Perception of probability words. Ms., UIUC, 05-24-2023.

Bruce Fraser. 2010. Pragmatic competence: The case of hedging. In New approaches to hedging, pages $15-34$. Brill.

Yonatan Geifman and Ran El-Yaniv. 2017. Selective classification for deep neural networks. Advances in neural information processing systems, 30.

Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904.
Gemini-Team. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.

Anastasia Giannakidou. 1999. Affective dependencies. Linguistics and Philosophy, 22:367-421.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.

Marie Juanchich, Amélie Gourdon-Kanhukamwe, and Miroslav Sirota. 2017. "i am uncertain" vs "it is uncertain". how linguistic markers of the uncertainty source affect uncertainty communication. Judgment and Decision Making, 12(5):445-465.

Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.

Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. arXiv preprint arXiv:2006.09462.

Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696-15707. PMLR.

Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. 2024. Unfamiliar finetuning examples control how language models hallucinate. arXiv preprint arXiv:2403.05612.

Sunnie SY Kim, Q Vera Liao, Mihaela Vorvoreanu, Stephanie Ballard, and Jennifer Wortman Vaughan. 2024. " i'm not sure, but...": Examining the impact of large language models' uncertainty expression on user reliance and trust. arXiv preprint arXiv:2405.00623.

Svenja Kranich. 2011. To hedge or not to hedge: the use of epistemic modal expressions in popular science in english texts, english-german translations, and german original texts.

Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023 Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466.

George Lakoff. 1973. Hedges: A study in meaning criteria and the logic of fuzzy concepts. Journal of philosophical logic, 2(4):458-508.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334.

Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.

Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.

Sabrina J Mielke, Arthur Szlam, Emily Dinan, and YLan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.

Samir Passi and Mihaela Vorvoreanu. 2022. Overreliance on ai literature review. Microsoft Research.

Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in neural information processing systems, 34:11054-11070.

Alexandre Piché, Aristides Milios, Dzmitry Bahdanau, and Chris Pal. 2024. Llms can learn self-restraint through iterative self-reflection. arXiv preprint arXiv:2405.13022.

Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. 2023a. Finetuning language models for factuality. arXiv preprint arXiv:2311.08401.
Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023b. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975.

Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q Vera Liao, and Jennifer Wortman Vaughan. 2023. Generation probabilities are not enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code completions. arXiv preprint arXiv:2302.07248.

Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10):78-85.

Thomas S Wallsten, David V Budescu, Rami Zwick, and Steven M Kemp. 1993. Preferences and reasons for communicating probabilistic information in verbal or numerical terms. Bulletin of the Psychonomic Society, 31(2):135-138.

Paul D Windschitl and Gary L Wells. 1996. Measuring psychological uncertainty: Verbal versus numeric methods. Journal of Experimental Psychology: Applied, 2(4):343.

Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, and Kai Yu. 2024. Rejection improves reliability: Training llms to refuse unknown questions using rl from knowledge feedback. arXiv preprint arXiv:2403.18349.

Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for honesty. arXiv preprint arXiv:2312.07000.

Gal Yona, Roee Aharoni, and Mor Geva. 2024. Narrowing the knowledge evaluation gap: Open-domain question answering with multi-granularity answers. arXiv preprint arXiv:2401.04695.

Hiyori Yoshikawa and Naoaki Okazaki. 2023. SelectiveLAMA: Selective prediction for confidence-aware evaluation of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2017-2028, Dubrovnik, Croatia. Association for Computational Linguistics.

Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024. Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation. arXiv preprint arXiv:2402.09267.

Alf C Zimmer. 1983. Verbal vs. numerical processing of subjective probabilities. In Advances in psychology, volume 16, pages 159-182. Elsevier.

| Method | Instruction |
| :--- | :--- |
| Vanilla | Answer the following question us- <br> ing a succinct (at most one sen- <br> tence) and full answer. |
| Granularity | Answer at a level of granular- <br> ity that matches your knowledge. <br> For example, if you are uncertain <br> about the specific details, output a <br> coarser (less specific) answer. |
| Uncertainty | If you are uncertain about your an- <br> swer to the question, convey this <br> uncertainty linguistically by pre- <br> cisely hedging this answer. |

Table 2: The specific instructions we use in the baselines we evaluate (see $\S 4$ ).
