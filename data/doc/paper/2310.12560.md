# Fast Model Debias with Machine Unlearning 

Ruizhe Chen ${ }^{1}$, Jianfei Yang ${ }^{2}$, Huimin Xiong ${ }^{1}$, Jianhong Bai ${ }^{1}$, Tianxiang $\mathbf{H u}^{1}$, Jin Hao ${ }^{3}$,<br>Yang Feng ${ }^{4}$, Joey Tianyi Zhou ${ }^{5}$, Jian Wu ${ }^{1}$, Zuozhu Liu ${ }^{1 *}$<br>${ }^{1}$ Zhejiang University ${ }^{2}$ Nanyang Technological University<br>${ }^{3}$ Stanford University ${ }^{4}$ Angelalign Technology Inc. ${ }^{5}$ Centre for Frontier AI Research<br>ruizhec.21@intl.zju.edu.cn


#### Abstract

Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets along with experiments with large language models demonstrate that our method achieves superior or competing accuracies compared with state-of-the-art methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.


## 1 Introduction

Biased predictions are not uncommon in well-trained deep neural networks [1-3]. Recent findings indicate that many deep neural networks exhibit biased behaviors and fail to generalize to unseen data [4, 5], e.g., convolutional neural networks (CNNs) might favor texture over shape for object classification [6]. For instance, well-trained networks on a large-scale dataset (e.g. CelebA) tend to predict a female person to be with blonde hair, and a male to be with black hair [7, 8]. This is because the number of <blonder hair, female> and <black hair, male> image pairs is significantly higher than others, although there is no causal relationship between hair color and gender [9]. In this case, the model does not learn the correct classification strategy based on human appearance, but rather shows a preference for specific individuals or groups based on irrelevant attributes (error correlations) [2]. Such error correlations not only affect the model's ability to make robust predictions but also perpetuate and exacerbate social bias, resulting in potential risks in many real-world scenarios, such as racism, underestimating minorities, or social disparities among groups in crime prediction [10], loan assessment [11], and recruitment [12] etc.[^0]

Efforts have been made to remove bias in models based on innate or acquired characteristics of individuals or groups. Existing debiasing mechanisms could be categorized into three types depending on when debiasing is conducted: pre-processing, in-processing, and post-processing [2, 13, 14]. Pre-processing debiasing methods usually modify the dataset for fair learning, which often involve reweighing samples [15, 16], modifying feature representations [17, 18], changing labels [19] etc. Another line of research accounts for fairness during training, i.e., in-processing [20--24], including feature-level data augmentation or adversarial training [25, 26] etc. However, the aforementioned methods require expensive costs for human labeling of misleading biases or computationally-intensive debiased model retraining, resulting in unsatisfactory scalability over modern large-scale datasets or models. Few research explore post-processing strategies to achieve fairness with minimal cost [27.29]. They ensure group fairness by alternating predictions of some selected samples, causing degraded accuracy or unfairness on individuals. Moreover, most methods assume that the biased attributes were known, while a generalized debiasing framework should be able to verify whether an attribute (e.g. shape, texture, and color in an image classification task) is biased or not as well [30].

![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-02.jpg?height=448&width=1198&top_left_y=844&top_left_x=472)

Figure 1: Pipeline of our proposed FMD.

In this paper, we propose FMD, an all-inclusive framework for fast model debiasing. As illustrated in Fig. 1, the FMD comprises three distinct steps: bias identification, biased-effect evaluation, and bias removal. In contrast to pre- or in-processing debiasing methods, our approach eliminates the need for supervised retraining of the entire model or additional labeling of bias attributes. Notably, FMD leverages only a small external dataset, thereby obviating the requirement for access to extensive or unavailable training data in practical scenarios. Furthermore, achieving fair outputs through FMD necessitates updating only a minimal number of parameters, such as the top MLP layers of pre-trained deep networks. Compared to post-processing debiasing methods, FMD yields superior debiasing performance and consistently enhances fairness across diverse bias metrics with little costs.

The FMD operates through the following procedure. Given an attribute and a well-trained model, our first step is to ascertain whether and to what extent the model exhibits bias towards the attribute. To achieve this, we construct a dataset comprising factual samples along with their corresponding counterfactual samples [31], wherein the attribute in question can be varied. By observing how the model's predictions change with the attribute variations, we can effectively identify any bias present. In the biased-effect evaluation phase, we quantitatively assess the extent to which a biased training sample contributes to the model's biased predictions. This evaluation entails measuring how the biased training sample misleads the model and influences its predictions. To this end, we extend the theory of influence functions [32], employing it to estimate the impact of perturbing a biased attribute within the training data on the model's prediction bias measurement. Finally, we introduce an unlearning mechanism that involves performing a Newton step [33] on the learned model parameters to remove the learned biased correlation. We further design an alternative strategy to unlearn biases with the counterfactual external dataset, avoiding hard requirements on access to the training data which might be unavailable in practice. Our unlearning strategy effectively eliminates the estimated influence of the biased attribute, leading to a more fair and unbiased model. Experiments on multiple datasets show that our method can achieve accuracies on par with bias-tailored training methods with a much smaller counterfactually constructed dataset. The corresponding biases and computational costs are significantly reduced as well. Our main contributions are summarized as:

- We propose a counterfactual inference-based framework that can quantitatively measure the biased degree of a trained (black-box) deep network with respect to different data attributes with a novel influence function.
- We propose an unlearning-based debiasing method that effectively and efficiently removes model biases with a small counterfactual dataset, getting rid of expensive network re-training or bias labeling. Our approach inherently applies to in-processing debiasing.
- Extensive experiments and detailed analysis on multiple datasets demonstrate that our framework can obtain competing accuracies with significantly smaller biases and much fewer data and computational costs.


## 2 Related Works

### 2.1 Group, Individual and Counterfactual Fairness

The pursuit of fairness in machine learning has led to the proposal of fairness-specific metrics. These metrics have been mainly categorized into two types: metrics for group fairness that require similar average outputs of different demographic groups [34-38]; and metrics for individual fairness that necessitate similarity in the probability distributions of individuals that are similar in respect to a specific task, regardless of their demographic group [39-42]. Generally, statistical parity among protected groups in each class (group fairness) could be intuitively unfair at the individual level [43]. Moreover, existing fairness metrics put a heavy emphasis on model predictions, while underestimating the significance of sensitive attributes for decision-making and are insufficient to explain the cause of unfairness in the task [31, 44]. Recently, [31] introduces counterfactual fairness, a causal approach to address individual fairness, which enforces that the distribution of potential predictions for an individual should remain consistent when the individual's protected attributes had been different in a causal sense. In contrast to existing individual bias metrics, counterfactual fairness can explicitly model the causality between biased attributes and unfair predictions, which provides explainability for different biases that may arise towards individuals based on sensitive attributes [45--47].

### 2.2 Bias Mitigation

Proposed debiasing mechanisms are typically categorized into three types 2, 13, 14]: pre-processing, in-processing, and post-processing. Pre- and in-processing algorithms account for fairness before and during the training process, where typical techniques entail dataset modification [15-19] and feature manipulation [20-24, 26, 25]. Post-processing algorithms are performed after training, intending to achieve fairness without the need of modifying data or re-training the model. Current post-processing algorithms make more fair decisions by tweaking the output scores [48-50]. For instance, Hardt [27] achieves equal odds or equal opportunity by flipping certain decisions of the classifier according to their sub-groups. [29, 28] select different thresholds for each group, in a manner that maximizes accuracy and minimizes demographic parity. However, achieving group fairness by simply changing the predictions of several individuals is questionable, e.g., the process might be unfair to the selected individuals, leading to an unsatisfactory trade-off between accuracy and fairness.

### 2.3 Machine Unlearning

Machine unlearning [51-53] is a new paradigm to forget a specific data sample and remove its corresponding influence from a trained model, without the requirement to re-train the model from scratch. It fulfills a user's right to unlearn her private information, i.e., the right to be forgotten, in accordance with requests from the General Data Protection Regulation (GDPR) [54]. Existing unlearning approaches can be roughly categorized into two types: exact unlearning [55, 56] and approximate unlearning [57,-60]. Data influence-based unlearning is a representative branch of approximate unlearning that utilizes influence functions [32] to approximate and remove the effect of a training sample on the model's parameters [61-63]. In this paper, we are inspired by the paradigm of machine unlearning and extend it to remove the model's bias from a deep network without retraining it from scratch.

## 3 Method

### 3.1 Overview and Preliminaries

Problem Formulation. Consider a supervised prediction task with fairness considerations that maps input attributes $\mathcal{A}$ (biased attribute) and $\mathcal{X}$ (other attributes except $\mathcal{A}$ ) to certain outputs $\mathcal{Y}$ (labels). The training dataset $D_{t r}$ can be represented as $\left\{z_{1}, z_{2}, \ldots, z_{n}\right\}$ where each training point $z_{i}=\left\{\left(a_{i}, x_{i}\right), y_{i}\right\} \in \mathcal{A} \times \mathcal{X} \times \mathcal{Y}$. Let $f_{\hat{\theta}}$ denote the trained model (predictor) with parameter $\hat{\theta}$. Let $L\left(z_{i}, \theta\right)$ denote the loss on the training sample $z_{i}$ w.r.t. parameter $\theta$. It is deemed biased if a biased attribute $a$ is highly correlated but wrongly correlated to the prediction $\hat{y}=f_{\hat{\theta}}(x, a)$, e.g., a CNN is biased if it predicts hair color (black/blonde) with the biased attribute genders (male/female).

Motivation. In large part, existing works focused on measuring fairness with implicit quantitative values (e.g. accuracy). However, they do not provide explicit illustrations on whether the decisionmaking is based on sensitive/protected attributes. Furthermore, based on the bias identified, research on how such bias is learned from training samples is limited. Our proposed method bridges this gap with two components: identifying bias from different predictions with counterfactual samples and evaluating the biased-effect from training samples with a modified influence function. Furthermore, we propose a novel machine unlearning-based method to efficiently and effectively remove the biases.

Counterfactual Fairness. We identify the biases of trained models with the concept of counterfactual fairness [31, 46, 45] which better models the causality between biased attributes and unfair predictions. We detail the definition following [31]:

Definition 1 (Counterfactual fairness). A trained model $f_{\hat{\theta}}$ is counterfactual fair on $\mathcal{A}$ if for any $a, \bar{a} \in \mathcal{A}$,

$$
\begin{equation*}
P\left(\hat{Y}_{A \leftarrow a}=y \mid X=x, A=a\right)=P\left(\hat{Y}_{A \leftarrow \bar{a}}=y \mid X=x, A=a\right) \tag{1}
\end{equation*}
$$

for all $x \in \mathcal{X}$ attainable by $X$.

Note that $y=f_{\bar{\theta}}(X, A)$, which implies the process of attribute changing. The definition suggests that, for any individual, changing $a$, i.e., from $a$ to $\bar{a}$, while holding other attributes $x$ unchanged should not change the distribution of $\hat{Y}$ if $a$ is a biased attribute.

Influence function. Influence functions, a standard technique from robust statistics, are recently extended to characterize the contribution of a given training sample to predictions in deep networks [32, 64, 65], e.g., identify whether a sample is helpful or harmful for model predictions. A popular implementation of influence functions is to approximate the effects by applying the perturbation $z=(x, y) \mapsto z_{\delta}=(x+\delta, y)$ [32] that define the parameters resulting from moving $\epsilon$ mass from $z$ onto $z_{\delta}: \hat{\theta}_{\epsilon, z_{\delta},-z}=\operatorname{argmin}_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^{n} L\left(z_{i}, \theta\right)+\epsilon L\left(z_{\delta}, \theta\right)-\epsilon L(z, \theta)$. An approximated computation of the influence as in [32] can be defined as:

$$
\begin{equation*}
\left.\frac{d \hat{\theta}_{\epsilon, z_{\delta},-z}}{d \epsilon}\right|_{\epsilon=0}=-H_{\hat{\theta}}^{-1}\left(\nabla_{\theta} L\left(z_{\delta}, \hat{\theta}\right)-\nabla_{\theta} L(z, \hat{\theta})\right) \tag{2}
\end{equation*}
$$

### 3.2 Bias Identification and Biased-Effect Evaluation

Counterfactual bias identification. We first identify the biases in a trained model with counterfactual concepts. Given a trained model $f_{\hat{\theta}}$ and an attribute of interest $\mathcal{A}$, a primary question is whether $f_{\hat{\theta}}$ is fair on $\mathcal{A}$. We employ an external dataset $D_{e x}$ (can be constructed from the test set) to identify biases. To measure how prediction changes in accordance with the attribute, for each sample $c_{i}=\left(x_{i}, a_{i}\right) \in D_{e x}$, where $a_{i} \in \mathcal{A}$, we alter $a_{i}$ while keeping $x_{i}$ unchanged based on the requirements of counterfactual fairness. The generated counterfactual sample is denoted as $\bar{c}_{i}=\left(x_{i}, \bar{a}_{i}\right), \bar{a}_{i} \in \mathcal{A}$. We further define the counterfactual bias of the model $f_{\hat{\theta}}$ on sample $c_{i}$ as the difference in predictions:

$$
\begin{equation*}
\left.\left.B\left(c_{i}, \mathcal{A}, \hat{\theta}\right)=\left|P\left(\hat{Y}=f_{\hat{\theta}}(X, A)\right)\right| X=x_{i}, A=a_{i}\right)\right)-P\left(\hat{Y}=f_{\hat{\theta}}(X, A) \mid X=x_{i}, A=\overline{a_{i}}\right) \mid \tag{3}
\end{equation*}
$$

The counterfactual bias on the whole dataset $D_{e x}$ can be represented as the average of individual counterfactual biases:

$$
\begin{equation*}
B\left(D_{e x}, \mathcal{A}, \hat{\theta}\right)=\frac{1}{\left|D_{e x}\right|} \sum_{i} B\left(c_{i}, \mathcal{A}, \hat{\theta}\right) \tag{4}
\end{equation*}
$$

The measured bias is a scalar normalized from 0 to 1 . We set a bias threshold $\delta$ that if the measured $B\left(D_{e x}, \mathcal{A}, f_{\hat{\theta}}\right)$ is larger than $\delta$, we regard $f_{\hat{\theta}}$ to be biased on $\mathcal{A}$. Note that our method could also generalize to other individual bias metrics besides Eq. 3 .

Biased-Effect Evaluation. Based on the identified counterfactual bias, we then investigate how the bias on $\mathcal{A}$ is learned by the model from training samples. Considering $B(\hat{\theta})$ measured on any $\mathcal{A}$ with any $D_{e x}$, our goal is to quantify how each training point $z_{k}$ in the training set $D_{t r}$ contributes to $B(\hat{\theta})$. Let's denote the empirical risk minimizer as $\hat{\theta}=\arg \min _{\theta} \frac{1}{n} \sum_{i=1}^{n} L\left(z_{i}, \theta\right)$, and assume that the empirical risk is twice-differentiable and strictly convex in $\theta$. The influence function [64] provides an approximation on the updates to parameters if $z_{k}$ were removed from $D_{t r}$ with a small coefficient $\epsilon$. The new parameters can be obtained as $\hat{\theta}_{\epsilon, z_{k}}=\arg \min _{\theta} \frac{1}{n} \sum_{i=1, i \neq k}^{n} L\left(z_{i}, \theta\right)+\epsilon L\left(z_{k}, \theta\right)$. By doing so, the influence of removing $z_{k}$ on the bias $B(\hat{\theta})$ can be defined as:

$$
\begin{equation*}
I_{u p, \text { bias }}\left(z_{k}, B(\hat{\theta})\right)=\left.\frac{d B\left(\hat{\theta}_{\epsilon, z_{k}}\right)}{d \epsilon}\right|_{\epsilon=0}=\left.\frac{d B\left(\hat{\theta}_{\epsilon, z_{k}}\right)}{d \hat{\theta}_{\epsilon, z_{k}}} \frac{d \hat{\theta}_{\epsilon, z_{k}}}{d \epsilon}\right|_{\epsilon=0}=-\nabla_{\hat{\theta}} B(\hat{\theta}) H_{\hat{\theta}}^{-1} \nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right) \tag{5}
\end{equation*}
$$

where $H_{\hat{\theta}} \stackrel{\text { def }}{=} \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta}^{2} L\left(z_{k}, \hat{\theta}\right)$ is the positive definite (PD) Hessian, and the closed form expression of $\left.\frac{d \hat{\theta}_{\epsilon, z_{k}}}{d \epsilon}\right|_{\epsilon=0}$, explaining the influence of $z_{k}$ to model parameters, is provided by the influence function [32]. Note that "up" denotes "upweight". Refer to Appendix A for the derivation. Intuitively, this equation can be understood in two parts: the latter part calculates the impact of removing on the parameters. The former part corresponds to the derivative of bias with respect to parameters, assessing how changes in parameters affect the bias. Hence, this equation quantifies the influence of removing on the bias. Note that $B(\hat{\theta})$ can be any bias measurement of interest. Taking $B\left(D_{\text {ex }}, \mathcal{A}, \hat{\theta}\right)$ defined in Eq. 4 as an example, the influence on counterfactual bias can be boiled down as:

$$
\begin{equation*}
I_{u p, b i a s}\left(z_{k}, B\left(D_{e x}, \mathcal{A}, \hat{\theta}\right)\right)=\frac{1}{\left|D_{e x}\right|} \sum_{c_{i} \in D_{e x}}\left(\nabla_{\hat{\theta}} f_{\hat{\theta}}\left(c_{i}\right)-\nabla_{\hat{\theta}} f_{\hat{\theta}}\left(\bar{c}_{i}\right)\right) H_{\hat{\theta}}^{-1} \nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right) \tag{6}
\end{equation*}
$$

where $I_{\text {up,bias }}\left(z_{k}, B\right)$ is a scalar that measures how each training sample contributes to $B$. If removing the point $z_{k}$ increases the bias, we regard $z_{k}$ as a helpful sample, or harmful otherwise. We provide an illustration of the helpful and harmful samples with a toy example in Section 4.2

### 3.3 Bias Removal via Machine Unlearning

After quantifying how biases are learned by the model from harmful samples, the next question is how to remove such biases. Here we propose a machine unlearning-based strategy to remove the biases caused by harmful samples. In particular, we exploit the powerful capability of machine unlearning paradigms for forgetting certain training samples [66, 62, 63, 61]. Specifically, for a bias measurement $B(\hat{\theta})$,we first rank the influence $I_{u p, \text { bias }}\left(z_{k}, B(\hat{\theta})\right)$ of every training sample $z_{k}$ in $D_{t r}$, and then select the top- $K$ harmful samples. Afterward, we unlearn, i.e., let the model forget, these samples by updating the model parameters $\theta$ with a Newton update step as in [63]:

$$
\begin{equation*}
\theta_{\text {new }}=\hat{\theta}+\sum_{k=1}^{K} H_{\hat{\theta}}^{-1} \nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right) \tag{7}
\end{equation*}
$$

where $H_{\hat{\theta}}^{-1} \nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right)=I_{u p, \text { params }}\left(z_{k}\right)$ is explained as the influence of $z_{k}$ on model parameter [32]. Note that $I_{\text {up,params }}\left(z_{k}\right)$ share similar computation as in Eq. 6 , while $I_{u p, \text { params }}\left(z_{k}\right)$ estimates the influence on model parameter and $I_{u p, \text { bias }}\left(z_{k}, B\right)$ focuses on influence on biases.

Our unlearning strategy is further refined following the observations from experiments in Section 4.2 In particular, by ranking and visualizing the harmful and helpful samples on the biases (as shown in Fig. 33, we have observed that the harmful samples heavily lead to biased/error correlations (i.e., bias-aligned) while the helpful samples behave oppositely (i.e., bias-conflicting). Hence, we propose a straightforward solution that further mitigates the influence of a harmful sample with a
bias-conflicting sample. Consequently, we update the parameters to unlearn the harmful samples by:

$$
\begin{equation*}
\theta_{n e w}=\hat{\theta}+\sum_{k=1}^{K} H_{\hat{\theta}}^{-1}\left(\nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right)-\nabla_{\hat{\theta}} L\left(\bar{z}_{k}, \hat{\theta}\right)\right) \tag{8}
\end{equation*}
$$

where $\bar{z}_{k}$ denotes the bias-conflicting sample of $z_{k}$. Following the explanation in influence theory [32], our unlearn mechanism removes the effect of perturbing a training point $(\bar{a}, x, y)$ to $(a, x, y)$. In other words, we not only remove the influence caused by harmful sample $z_{k}$, but further ensure fairness with the corresponding counterfactual sample $\bar{z}_{k}$, see more details in Section 4.2, 4.4 and Appendix.

Alternative Efficient Unlearn with Cheap External Datasets. In the above sections, the unlearning process is based on the assumption that we could access the original training sample $z_{k}$ to identify and evaluate biases and then forget them. However, in practice, the training set might be too large or even unavailable in the unlearning phase. In response, we further propose to approximate the unlearning mechanism with a small external dataset. As the influence to be removed can be obtained from the change of the protected attribute, we can construct the same modification to the protected attribute on external samples. In particular, we employ the $D_{e x}$ as in Section 3.2 to construct counterfactual pairs for unlearning, which redefines Eq. 8 as:

$$
\begin{equation*}
\theta_{\text {new }}=\hat{\theta}+\sum_{i} H_{\hat{\theta}}^{-1}\left(\nabla_{\hat{\theta}} L\left(c_{i}, \hat{\theta}\right)-\nabla_{\hat{\theta}} L\left(\bar{c}_{i}, \hat{\theta}\right)\right) \tag{9}
\end{equation*}
$$

As $D_{\text {ex }}$ can be easily obtained from an external dataset rather than the training set, the practical applicability of our method could be greatly enhanced, as demonstrated in the experiments.

### 3.4 Model Generalization

Extension to Different Biases. To fulfill different fairness demands, we further discuss the generalization of the bias function $B(\hat{\theta})$ in Eq. 6 to other bias measurements. We provide the extension to the most frequently used group fairness measurement demographic parity [34] which requires equal positive prediction assignment across subgroups (e.g. male and female). Eq. 6 can be rewritten as:

$$
\begin{equation*}
I_{u p, \text { bias }}\left(z_{k}\right)=-\left(\nabla_{\hat{\theta}} \frac{1}{\left|\mathcal{G}_{A=1}\right|} \sum_{c_{i} \in \mathcal{G}_{A=1}} f_{\hat{\theta}}\left(c_{i}\right)-\nabla_{\hat{\theta}} \frac{1}{\left|\mathcal{G}_{A=0}\right|} \sum_{c_{j} \in \mathcal{G}_{A=0}} f_{\hat{\theta}}\left(c_{j}\right)\right) H_{\hat{\theta}}^{-1} \nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right) \tag{10}
\end{equation*}
$$

where $\mathcal{G}_{A=1}$ and $\mathcal{G}_{A=0}$ represents the subgroup with protected attribute $A=1$ and 0 . The extension to equal opportunity [35], which requires the positive predictions to be equally assigned across positive classes, can be rewritten as:

$$
\begin{equation*}
I_{u p, \text { bias }}\left(z_{k}\right)=-\left(\nabla_{\hat{\theta}} \frac{1}{\left|\mathcal{G}_{1,1}\right|} \sum_{c_{i} \in \mathcal{G}_{1,1}} f_{\hat{\theta}}\left(c_{i}\right)-\nabla_{\hat{\theta}} \frac{1}{\left|\mathcal{G}_{0,1}\right|} \sum_{c_{j} \in \mathcal{G}_{0,1}} f_{\hat{\theta}}\left(c_{j}\right)\right) H_{\hat{\theta}}^{-1} \nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right) \tag{11}
\end{equation*}
$$

where $\mathcal{G}_{1,1}$ represents the sub-group where $A=1$ and $Y=1$.

Extension to Deep Models. In the previous sections, it's assumed that $\hat{\theta}$ could be the global minimum. However, if $\hat{\theta}$ is obtained in deep networks trained with SGD in a non-convex setting, it might be a local optimum and the exact influence can hardly be computed. We follow the strategy in [32] to approximate the influence in deep networks, and empirically demonstrate the effectiveness of FMD in deep models. Moreover, for deep networks where a linear classifier is stacked on a backbone feature extractor, we apply our unlearning mechanism to the linear classifier or several top MLP layers.

Efficient Influence Computation. A critical challenge to compute the influence in Eq. 6 is to explicitly calculate the inverse Hessian. Here we employ the implicit Hessianvector products (HVPs) [32, 67] to efficiently approximate $\nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right)$. Meanwhile, $\nabla_{\hat{\theta}} L\left(z_{k}, \hat{\theta}\right)$ in Eq. 6 can be precalculated and applied to different $\nabla_{\hat{\theta}} B(\hat{\theta})$. To avoid the $O\left(d^{3}\right)$ computational cost to calculate the inverse Hessian in every step, we pre-calculate it before the removal and keep it constant during unlearning phase [63]. The alternative strategy which continuously updates the inversion Hessian is also analyzed in the Appendix.

```
Algorithm 1: The FMD framework.
    Input: dataset $D_{e x}$, loss $\mathcal{L}$, attribute of
            interest $\mathcal{A}$, Hessian matrix $H$,
            bias threshold $\delta$, parameter $\theta$,
            $n=\left\|D_{e x}\right\|$.
    $B \leftarrow B\left(D_{e x}, \mathcal{A}, \hat{\theta}\right)$
    $H^{-1} \leftarrow$ Inverse $(H)$
    if $B>\delta$ then
        for $i=1,2,3, \ldots, n$ do
            $\triangle \leftarrow \nabla_{\hat{\theta}} L\left(c_{i}, \hat{\theta}\right)-\nabla_{\hat{\theta}} L\left(\bar{c}_{i}, \hat{\theta}\right)$
            $\theta \leftarrow \theta+H^{-1} \triangle$
        end
    end
    Output: $\theta$
```


## 4 Experiment

### 4.1 Experiment details

Dataset. Our experiments are conducted on three datasets. Colored MNIST is constructed by adding color bias to the MNIST dataset [68]. Bias-aligned samples are constructed by adding a particular color to a particular digit like \{Digit1_Color1\} while other colors are for bias-conflicting samples. Following [3, 69, 70], we build 3 different training sets by setting different biased ratios $\{0.995,0.99$, $0.95\}$ for biased-aligned training samples, where a high ratio indicates a high degree of bias. CelebA [71] is a face recognition dataset with 40 types of attributes like gender, age (young or not), and lots of facial characteristics (such as hair color, smile, beard). We choose Gender as the bias attribute, and Blonde hair and Attractive as the outputs following [7, 8]. Adult Income Dataset is a publicly available dataset in the UCI repository [72] based on the 1994 U.S. census data. The dataset records an individual's income (more or less than $\$ 50,000$ per year) along with features such as occupation, marital status, and education. In our experiment, we choose gender and race as biased attributes following [73, 74]. We follow the pre-processing procedures in [75]. As for the experiment on the language model, we use StereoSet [76] as our test set. StereoSet is a large-scale natural dataset to measure stereotypical biases in gender, profession, race, and religion.

Baselines. For the sanity check experiment on a toy Colored MNIST dataset, we use a vanilla logistic regression model as the baseline. For experiments with deep networks, we compare our method with one pre-processing baseline Reweigh [77], 6 in-processing debiasing baselines (LDR [25], LfF [78], Rebias [79], DRO [7], SenSEI [80], and SenSR [81]) and 4 post-processing baselines (EqOdd [35], CEqOdd [35], Reject [82] and PP-IF [83]). We compare our method on language model with five debiasing baselines: Counterfactual Data Augmentation (CDA) [84], Dropout [85], Iterative Null-space Projection (INLP) [86], Self-debias [87], and SentenceDebias [88]. Details can be referred to Appendix C.

Construction of Counterfactual Dataset $D_{e x}$. We separately construct counterfactual sets for the three datasets, while bias-aligned samples in the small dataset $D_{e x}$ are all split from the test set. In the Colored MNIST dataset, we randomly add another color on the same digit image as the counterfactual sample. As for the Adult dataset, we flip the protected attribute to the opposite while keeping other attributes and target labels exactly the same. For the CelebA dataset, we select images with the same target labels but the opposite protected attribute. To fulfill the request for counterfactual, we rank the similarity between images by comparing the overlap of other attributes and choose the most similar pair to form the factual and counterfactual samples. Part of the generated sample pairs is visualized in Fig. 2. Note that for the CelebA dataset, the counterfactual data are not that strict as the gender attribute is not independent of other features in the natural human facial images. We use Crows-Pairs [89] as our external dataset for the language model. Each sample in Crows-Pairs consists of two sentences: one that is more stereotyping and another that is less stereotyping, which can be utilized as counterfactual pairs.

![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-07.jpg?height=314&width=1247&top_left_y=1800&top_left_x=431)

Figure 2: Visualization of factual and counterfactual pairs for three datasets.

Implementation details. We use multi-layer perceptron (MLP) with three hidden layers for Colored MNIST and Adult, and ResNet-18 [90] for CelebA following the setting in [8]. During training, we set the batch size of 256 for Colored MNIST and Adult, respectively, and 64 for CelebA following [25, 78, 7]. We use pre-trained BERT [91] and GPT-2 [92], provided by Huggingface. During unlearning, we freeze the parameters of all other layers except the last classifier layer. The running time of all baselines is evaluated on a single RTX3090 GPU for a fair comparison. In our experiment, we select the number of samples $\mathrm{k}=5000$ for Colored MNIST, and $\mathrm{k}=200$ for both Adult and CelebA. The bias threshold is set to 0 .

### 4.2 Sanity Check on Logistic Regression with a Toy Dataset

We conduct an experiment on a logistic regression task to illustrate our method. We simplify the Colored MNIST classification task to a binary classification problem of distinguishing between only digits 3 and 8 , on a training set with a bias ratio of 0.95 . and a balanced test set. We trained a regularized logistic regressor: $\operatorname{argmin}_{w \in R^{d}} \sum_{i=1}^{n} l\left(w^{T} x_{i}, y_{i}\right)+\lambda\|w\|_{2}^{2}$. Fig. 3 (a) illustrates the classification results of the vanilla regressor on part of test samples. We denote Digit by shape (triangle and rectangle) and Color by color (red and blue). The solid line represents the learned classification boundary and the dotted line represents the expected classification boundary. The test accuracy is 0.6517 and it can be observed that most bias-conflict samples tend to be misclassified according to their colors. Moreover, we select and visualize the most helpful and harmful samples in Fig. 3.c) based on Eq. 6. We found that the most helpful samples are in the $5 \%$ bias-conflict samples while harmful samples are bias-aligned samples. The unlearning curve is provided in Fig. 3.b). With only 50 samples, the accuracy is improved amazingly by $25.71 \%$ and the counterfactual bias decreases by 0.2755 , demonstrating the effectiveness of our method.

![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-08.jpg?height=244&width=350&top_left_y=886&top_left_x=454)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-08.jpg?height=247&width=361&top_left_y=890&top_left_x=817)

(b)
![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-08.jpg?height=224&width=208&top_left_y=912&top_left_x=1168)

![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-08.jpg?height=225&width=344&top_left_y=907&top_left_x=1324)

(c)

Figure 3: (a) Illustration of the learned pattern on our toy dataset.(b) Accuracy and bias curves during unlearning. (b) Visualization of helpful samples (top row) and harmful samples (bottom row).

### 4.3 Experiment on Deep Models

Results on Colored MNIST. Tab. 1 shows the comparisons on the Colored MNIST dataset. We reported test accuracy, counterfactual bias, debiasing time, and the number of samples used for all methods. Our approach demonstrates competing performance on accuracy and superior performance on bias compared with retraining baselines. Meanwhile, we only make use of one-tenth of unlearning samples and reduce the debiasing time by 1-2 magnitudes.

| Bias Ratio | Method | Acc.(\%) $\uparrow$ | Bias $\downarrow$ | Time(s) | \# Samp. |
| :---: | :---: | :---: | :---: | ---: | ---: |
| 0.995 | Vanilla | 38.59 | 0.5863 | - | - |
|  | LDR | 66.76 | 0.4144 | 1,261 | $50 \mathrm{k}$ |
|  | LfF | 56.45 | 0.3675 | 661 | $50 \mathrm{k}$ |
|  | Rebias | $\underline{71.24}$ | $\underline{0.3428}$ | 1,799 | $50 \mathrm{k}$ |
|  | Ours | $\mathbf{7 1 . 7 0}$ | $\mathbf{0 . 3 0 2 7}$ | $\mathbf{5 9}$ | $5 \mathrm{k}$ |
| 0.99 | Vanilla | 51.34 | 0.4931 | - | - |
|  | LDR | 76.48 | 0.2511 | 1,330 | $50 \mathrm{k}$ |
|  | LfF | 64.71 | 0.2366 | 726 | $50 \mathrm{k}$ |
|  | Rebias | $\mathbf{8 0 . 4 1}$ | $\underline{0.2302}$ | 1,658 | $50 \mathrm{k}$ |
|  | Ours | $\underline{80.04}$ | $\mathbf{0 . 2 0 4 2}$ | $\mathbf{4 8}$ | $5 \mathrm{k}$ |
| 0.95 | Vanilla | 77.63 | 0.2589 | - | - |
|  | LDR | $\mathbf{9 0 . 4 2}$ | 0.2334 | 1,180 | $50 \mathrm{k}$ |
|  | LfF | 85.55 | 0.1264 | 724 | $50 \mathrm{k}$ |
|  | Rebias | $\underline{89.63}$ | $\underline{0.1205}$ | 1,714 | $50 \mathrm{k}$ |
|  | Ours | $\underline{89.26}$ | $\mathbf{0 . 1 1 8 9}$ | $\mathbf{5 6}$ | $5 \mathrm{k}$ |

Table 1: Results on Colored MNIST. (bold: best performance, underline: second best performance.)

| Attr. | Method | Acc.(\%) $\uparrow$ | Bias $\downarrow$ | Time(s) | \# Samp. |
| :---: | :---: | :---: | :---: | ---: | ---: |
| Gender | Vanilla | $\mathbf{8 5 . 4 0}$ | 0.0195 | - | - |
|  | LDR | 77.69 | 0.0055 | 927 | 26,904 |
|  | LfF | 73.08 | 0.0036 | 525 | 26,904 |
|  | Rebias | 76.57 | 0.0041 | 1292 | 26,904 |
|  | Reweigh | 82.60 | 0.0051 | 36 | 26,904 |
|  | SenSR | 84.09 | 0.0049 | 571 | 26,904 |
|  | SenSeI | 83.91 | 0.0016 | 692 | 26,904 |
|  | PP-IF | 81.96 | 0.0027 | 13 | 26,904 |
|  | Ours | 81.89 | $\mathbf{0 . 0 0 0 5}$ | $\mathbf{2 . 4 9}$ | 500 |
| Race | Vanilla | $\mathbf{8 4 . 5 7}$ | 0.0089 | - |  |
|  | LDR | 78.32 | 0.0046 | 961 | 26,904 |
|  | LfF | 75.16 | 0.0024 | 501 | 26,904 |
|  | Rebias | 77.89 | 0.0038 | 1304 | 26,904 |
|  | Reweigh | 82.97 | 0.0015 | 36 | 26,904 |
|  | SenSR | 84.09 | 0.0036 | 571 | 26,904 |
|  | SenSeI | 83.91 | 0.0015 | 692 | 26,904 |
|  | PP-IF | 82.37 | 0.0015 | 13 | 26,904 |
|  | Ours | 83.80 | $\mathbf{0 . 0 0 1 3}$ | $\mathbf{2 . 5 4}$ | 500 |

Table 2: Results on Adult.

Results on Adult. The results are in Table 2 It can be observed that the vanilla method performed the best in accuracy on both tasks, since in the real-world dataset, race and gender are biased w.r.t income in both training and test set and the well-trained model fits this correlation. However, to achieve fair prediction, we would not expect biased attributes to dominate predictions. Compared with other debiasing methods, our method achieved the best results in both accuracy and bias, with much less debiasing time on a smaller dataset.

Results on CelebA. We compare on average accuracy (Avg.), Unbiased accuracy [8] (Unb.) tested on the balanced test set, and Worst-group accuracy [7] (Wor.) tested on the unprivileged group to illustrate the performance, as reported in Tab. 3 It can be observed that the vanilla model performs well on the whole dataset (Avg.) but scores a really low accuracy (Wor.) on the worst group, which means the learned model heavily relies on the bias attribute to achieve high accuracy. Our method obviously bridges this gap and outperforms all other debiasing baselines on Wor. and Unb. in the two experiments. The experiments also demonstrate that our method is consistently feasible even in the absence of perfect standardized counterfactual samples in real-world datasets, by selecting a certain amount of approximate counterfactual data.

Results on Large Language Models (LLM). We further extended our method to the LLM debiasing scenario. Results are presented in Tab. 4. We report two metrics: Language Modeling Score (LMS) measures the percentage of instances in which a language model prefers the meaningful over meaningless association. The LMS of an ideal language model will be

| Attr. | Method | Unb.(\%) $\uparrow$ | Wor.(\%) $\uparrow$ | Avg.(\%) $\uparrow$ | Bias $\downarrow$ | Time(s) |
| :---: | :--- | :---: | :---: | :---: | :---: | ---: |
| Blonde | Vanilla | 66.27 | 47.36 | $\mathbf{9 4 . 9 0}$ | 0.4211 | - |
|  | LfF | 84.33 | 81.24 | 93.52 | 0.2557 | 67,620 |
|  | LDR | 85.01 | 82.32 | 86.67 | 0.3126 | 24,180 |
|  | DRO | $\underline{85.66}$ | $\underline{84.36}$ | 92.90 | 0.3206 | 28,860 |
|  | Ours | $\mathbf{8 9 . 7 3}$ | $\mathbf{8 7 . 1 5}$ | $\underline{93.41}$ | $\mathbf{0 . 0 7 1 7}$ | $\mathbf{1 9 1}$ |
| Attractive | Vanilla | 63.17 | 40.59 | 77.42 | 0.3695 | - |
|  | LfF | 67.44 | 52.25 | 77.24 | 0.2815 | 67,560 |
|  | LDR | $\underline{68.14}$ | 54.47 | $\mathbf{8 1 . 7 0}$ | 0.2986 | 24,420 |
|  | DRO | $\mathbf{6 6 . 1 4}$ | $\underline{62.33}$ | 78.35 | 0.3004 | 30,540 |
|  | Ours | $\mathbf{7 2 . 1 8}$ | $\mathbf{6 8 . 1 6}$ | $\underline{80.99}$ | $\mathbf{0 . 1 2 7 3}$ | $\mathbf{1 8 7}$ |

100 (the higher the better). Stereotype

Table 3: Results on CelebA. Score (SS) measures the percentage of examples in which a model prefers a stereotypical association over an anti-stereotypical association. The SS of an ideal language model will be 50 (the closer to 50 the better). It shows that our method can outperform or achieve comparable performance with baseline methods. As for BERT, our method reaches the best (denoted by bold) or second best (denoted by underline) performance in 5 of 6 metrics.

| Backbone | Attribute | Method | $\mathrm{SS}$ | LMS | Attribute | Method | $\mathrm{SS}$ | LMS | Attribute | Method | $\mathrm{SS}$ | LMS |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| BERT | gender | Vanilla | 60.28 | 84.17 | race | Vanilla | 57.03 | 84.17 | religion | Vanilla | 59.7 | 84.17 |
|  |  | CDA | 59.61 | 83.08 |  | CDA | 56.73 | 83.41 |  | CDA | 58.37 | 83.24 |
|  |  | Dropout | 60.66 | 83.04 |  | Dropout | 57.07 | 83.04 |  | Dropout | 59.13 | 83.04 |
|  |  | INLP | $\mathbf{5 7 . 2 5}$ | 80.63 |  | INLP | 57.29 | 83.12 |  | INLP | 60.31 | 83.36 |
|  |  | Self-debias | 59.34 | 84.09 |  | Self-debias | 54.30 | 84.24 |  | Self-debias | 57.26 | 84.23 |
|  |  | SentDebias | 59.37 | 84.20 |  | SentDebias | 57.78 | 83.95 |  | SentDebias | 58.73 | 84.26 |
|  |  | Ours | 57.77 | $\overline{85.45}$ |  | Ours | 57.24 | $\overline{84.19}$ |  | Ours | 57.85 | 84.90 |
| GPT-2 | gender | Vanilla | 62.65 | 91.01 | race | Vanilla | 58.9 | 91.01 | religion | Vanilla | 63.26 | $\underline{91.01}$ |
|  |  | CDA | 64.02 | 90.36 |  | CDA | 57.31 | 90.36 |  | CDA | 63.55 | $\overline{90.36}$ |
|  |  | Dropout | 63.35 | 90.40 |  | Dropout | $\overline{57.50}$ | 90.40 |  | Dropout | 64.17 | 90.40 |
|  |  | INLP | 60.17 | 91.62 |  | INLP | 58.96 | 91.06 |  | INLP | 63.95 | 91.17 |
|  |  | Self-debias | $\overline{60.84}$ | 89.07 |  | Self-debias | 57.33 | $\overline{89.53}$ |  | Self-debias | 60.45 | 89.36 |
|  |  | SentDebias | 56.05 | 87.43 |  | SentDebias | 56.43 | 91.38 |  | SentDebias | 59.62 | 90.53 |
|  |  | Ours | 60.42 | $\underline{91.01}$ |  | Ours | 60.42 | 91.01 |  | Ours | $\overline{58.43}$ | 86.13 |

Table 4: Results with Large Language Models (BERT and GPT-2).

### 4.4 Analysis

Effectiveness on Different Bias Metrics. We validate the generalization ability of our unlearning method based on different fairness metrics on the Colored MNIST with bias severity 0.99 . In Tab. 5. we compare the performance of unlearning harmful samples based on three different biases: Counterfactual bias (Co.), Demographic parity bias (De.) [34], and Equal opportunity bias (Eo.) [35]. For each experiment, we report the changes in three biases. We can note that our method is consistently effective on all three bias metrics. Meanwhile, our counterfactual-based unlearning can significantly outperform the other two in terms of accuracy, Co., and De., and is comparable with them on Eo..

|  | Acc. $(\%) \uparrow$ | Co. $\downarrow$ | De. $\downarrow$ | Eo. $\downarrow$ |
| :--- | :---: | :---: | :---: | :---: |
| Vanilla | 65.17 | 0.3735 | 0.5895 | 0.2235 |
| Unlearn by De. | 71.52 | 0.1796 | 0.4026 | 0.0116 |
| Unlearn by Eo. | 71.12 | 0.1826 | 0.4217 | $\mathbf{0 . 0 1 0 3}$ |
| Unlearn by Co. (Ours) | $\mathbf{8 7 . 9 0}$ | $\mathbf{0 . 1 0 5 1}$ | $\mathbf{0 . 1 4 9 8}$ | 0.0108 |

Table 5: Ablation on Different Biases.

|  | Acc. $\uparrow$ | Bias $\downarrow$ | Time(s) $\downarrow$ |
| :--- | ---: | ---: | ---: |
| Vanilla | 65.17 | 0.3735 | - |
| Unlearn by Eq. 7 | 90.68 | 0.1182 | 36.87 |
| Unlearn by Eq. 8 | $\mathbf{9 1 . 1 8}$ | $\mathbf{0 . 1 0 2 3}$ | 39.63 |
| Unlearn by Eq. 9 (Ours) | 90.42 | 0.1051 | $\mathbf{0 . 0 5 9}$ |

Table 6: Ablation on Unlearning Strategies.

Effectiveness of Unlearn Strategies. We empirically investigate the feasibility of the unlearning mechanism on training and external samples on the Colored MNIST with bias severity 0.99. In Tab.6, we report the results of unlearning harmful training samples (Eq. 7), unlearning by replacing harmful
samples with their bias-conflict helpful samples (Eq. 87) and unlearning with external counterfactual sample pairs (Eq. 9). It can be observed that unlearning in the training dataset can achieve higher accuracy and less bias, and Eq. 8 excels on both metrics. But unlearning with training samples requires much more time and training samples might not be available in practice, while unlearning with external samples provides a satisfactory alternative.

| Attr. | Method | Acc.(\%) $\uparrow$ | Co. $\downarrow$ | De. $\downarrow$ | Eq. $\downarrow$ | Time(s) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Gender | EqOdd | 82.71 | 0.0247 | 0.5991 | $\mathbf{0 . 0 0 2 1}$ | $\mathbf{0 . 0 1 1 3}$ |
|  | CEqOdd | 83.22 | 0.0047 | 0.4469 | 0.0125 | 0.5583 |
|  | Reject | 74.63 | 0.0876 | 0.2744 | 0.3140 | 14.420 |
|  | Ours | $\mathbf{8 3 . 4 9}$ | $\mathbf{0 . 0 0 1 9}$ | $\mathbf{0 . 1 4 3 8}$ | 0.0460 | 0.0389 |
| Race | EqOdd | 83.22 | 0.0139 | 0.7288 | $\mathbf{0 . 0 0 2 1}$ | $\mathbf{0 . 0 1 0 5}$ |
|  | CEqOdd | 82.88 | 0.0012 | 0.6803 | 0.0054 | 3.6850 |
|  | Reject | 74.63 | 0.1156 | 0.4349 | 0.1825 | 14.290 |
|  | Ours | $\mathbf{8 3 . 1 2}$ | $\mathbf{0 . 0 0 0 6}$ | $\mathbf{0 . 4 2 1 9}$ | 0.0367 | 0.0360 |


| Method | \# Lay. | \# Para. | Acc(\%) $\uparrow$ | Bias $\downarrow$ | Time(s) |
| :--- | :---: | ---: | ---: | ---: | ---: |
| Vanilla | - | - | 51.34 | 0.4931 | - |
| Ours $^{1}$ | 1 | $1 \mathrm{~K}$ | 71.19 | $\mathbf{0 . 2 7 5 7}$ | $\mathbf{3 . 7 5}$ |
| Ours $^{2}$ | 2 | $11 \mathrm{~K}$ | $\mathbf{7 4 . 1 8}$ | 0.3134 | 432.86 |
| Ours $^{3}$ | 3 | $21 \mathrm{~K}$ | 61.45 | 0.2949 | 496.44 |

Table 8: Ablation on \# MLP Layers.

Table 7: Discussion on Post-processing Methods.

Discussion on Post-processing Methods. We compare our method to post-processing methods, i.e., Equalized Odds Post-processing (EqOdd) [35], Calibrated Equalized Odds Post-processing (CEqOdd) [35] and Reject Option Classification (Reject) [82], as shown in Tab. 7, Note these methods only apply to logistic regression. Our method outperforms them in most cases on Adult. It is also worth noting that these post-processing methods aimed at a specific fairness measure tend to exacerbate unfairness under other measures while our method consistently improves the fairness under different measures.

Ablation on the Number of Samples. Fig. 4 demonstrates the sensitivity of our unlearning performance w.r.t. number of samples on Colored MNIST with a bias ratio of 0.99 . The accuracy increases and bias decreases incrementally with more samples, and becomes steady after the number is beyond 5,000 . On the other hand, the unlearning time increases linearly with the number of samples. Additionally, constructing a large number of counterfactual samples in practice might be time-consuming as well. Practical usage of the FMD would require a trade-off based on utility requirements.

![](https://cdn.mathpix.com/cropped/2024_06_04_3c39cd930b6e5467e029g-10.jpg?height=347&width=483&top_left_y=1214&top_left_x=1233)

Figure 4: Ablation on \# Samples.

Ablation on Number of Fine-tuning Layers. We explore the impact of unlearning different numbers of layers (i.e., the last (one), two, three MLP) on the Color MNIST, with results in Tab. 8. Interestingly, the accuracy excels with two layers but decreases with three layers. Additionally, fine-tuning multiple layers takes much longer time on computation on much more parameters. It is also worth noting that our method could achieve such superior or competing performance even only by updating the last layer in deep models, which calls for more in-depth analysis in the future

## 5 Conclusion and Limitation

Biased behaviors in contemporary well-trained deep neural networks can perpetuate social biases, and also pose challenges to the models' robustness. In response, we present FDM, an all-inclusive framework for fast and effective model debiasing. We explicitly measure the influence of training samples on bias measurement and propose a removal mechanism for model debiasing. Comprehensive experiments on multiple datasets demonstrate that our method can achieve superior/competing accuracies with a significantly lower bias as well as computational cost.

Our work preliminarily explored the application of our method to large language models, as well as more analysis on model fairness from different perspectives, which will be in our future plan. In addition, our method is not applicable to black-box models, which are of high interest in real-world scenarios. Our proposed method requires generating counterfactual pairs with labeled sensitive attributes, while many datasets do not have enough labels. Research on fairness with few/no attribute labels is still in the infant stage [93], and we will further explore it.

## Acknowledgements

This work is supported by the National Natural Science Foundation of China (Grant No. 62106222), the Natural Science Foundation of Zhejiang Province, China(Grant No. LZ23F020008) and the Zhejiang University-Angelalign Inc. R\&D Center for Intelligent Healthcare.

## References

[1] M. Du, F. Yang, N. Zou, and X. Hu, "Fairness in deep learning: A computational perspective," IEEE Intelligent Systems, vol. 36, no. 4, pp. 25-34, 2020.

[2] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, "A survey on bias and fairness in machine learning," ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1-35, 2021.

[3] B. Kim, H. Kim, K. Kim, S. Kim, and J. Kim, "Learning not to learn: Training deep neural networks with biased data," in IEEE Conference on Computer Vision and Pattern Recognition, 2019 .

[4] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann, "Shortcut learning in deep neural networks," Nature Machine Intelligence, vol. 2, no. 11, pp. 665673,2020 .

[5] S. Sagawa, A. Raghunathan, P. W. Koh, and P. Liang, "An investigation of why overparameterization exacerbates spurious correlations," in International Conference on Machine Learning, pp. 8346-8356, PMLR, 2020.

[6] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel, "Imagenettrained cnns are biased towards texture; increasing shape bias improves accuracy and robustness," arXiv preprint arXiv:1811.12231, 2018.

[7] S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang, "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization," in International Conference on Learning Representations, 2020.

[8] E. Tartaglione, C. A. Barbano, and M. Grangetto, "End: Entangling and disentangling deep representations for bias correction," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13508-13517, June 2021.

[9] S. Sagawa, A. Raghunathan, P. W. Koh, and P. Liang, "An investigation of why overparameterization exacerbates spurious correlations," in International Conference on Machine Learning, pp. 8346-8356, PMLR, 2020.

[10] T. Brennan, W. Dieterich, and B. Ehret, "Evaluating the predictive validity of the compas risk and needs assessment system," Criminal Justice and behavior, vol. 36, no. 1, pp. 21-40, 2009.

[11] J. F. Mahoney and J. M. Mohen, "Method and system for loan origination and underwriting," Oct. 23 2007. US Patent 7,287,008.

[12] M. Bogen and A. Rieke, "Help wanted: An examination of hiring algorithms, equity, and bias," 2018.

[13] D. Pessach and E. Shmueli, "A review on fairness in machine learning," ACM Computing Surveys (CSUR), vol. 55, no. 3, pp. 1-44, 2022.

[14] O. Parraga, M. D. More, C. M. Oliveira, N. S. Gavenski, L. S. Kupssinsk, A. Medronha, L. V. Moura, G. S. Simes, and R. C. Barros, "Debiasing methods for fairer neural models in vision and language research: A survey," arXiv preprint arXiv:2211.05617, 2022.

[15] Y. Li and N. Vasconcelos, "Repair: Removing representation bias by dataset resampling," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, $\mathrm{pp}$. 9572$9581,2019$.

[16] M. Tanjim, R. Sinha, K. K. Singh, S. Mahadevan, D. Arbour, M. Sinha, G. W. Cottrell, et al., "Generating and controlling diversity in image search," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 411-419, 2022.

[17] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney, "Optimized preprocessing for discrimination prevention," Advances in neural information processing systems, vol. 30, 2017.

[18] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, "Certifying and removing disparate impact," in proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 259-268, 2015.

[19] F. Kamiran and T. Calders, "Data preprocessing techniques for classification without discrimination," Knowledge and information systems, vol. 33, no. 1, pp. 1-33, 2012.

[20] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, "Fairness beyond disparate treatment \& disparate impact: Learning classification without disparate mistreatment," in Proceedings of the 26th international conference on world wide web, pp. 1171-1180, 2017.

[21] E. Adeli, Q. Zhao, A. Pfefferbaum, E. V. Sullivan, L. Fei-Fei, J. C. Niebles, and K. M. Pohl, "Representation learning with statistical independence to mitigate bias," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2513-2523, 2021.

[22] P. Dhar, J. Gleason, A. Roy, C. D. Castillo, and R. Chellappa, "Pass: protected attribute suppression system for mitigating bias in face recognition," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15087-15096, 2021.

[23] M. Alvi, A. Zisserman, and C. Nellker, "Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings," in Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pp. 0-0, 2018.

[24] H. Bahng, S. Chun, S. Yun, J. Choo, and S. J. Oh, "Learning de-biased representations with biased representations," in International Conference on Machine Learning, pp. 528-539, PMLR, 2020 .

[25] J. Lee, E. Kim, J. Lee, J. Lee, and J. Choo, "Learning debiased representation via disentangled feature augmentation," Advances in Neural Information Processing Systems, vol. 34, pp. 25123$25133,2021$.

[26] T. Wang, J. Zhao, M. Yatskar, K.-W. Chang, and V. Ordonez, "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5310-5319, 2019.

[27] M. Hardt, E. Price, and N. Srebro, "Equality of opportunity in supervised learning," Advances in neural information processing systems, vol. 29, 2016.

[28] A. K. Menon and R. C. Williamson, "The cost of fairness in binary classification," in Conference on Fairness, accountability and transparency, pp. 107-118, PMLR, 2018.

[29] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, "Algorithmic decision making and the cost of fairness," in Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining, pp. 797-806, 2017.

[30] P. Saleiro, B. Kuester, L. Hinkson, J. London, A. Stevens, A. Anisfeld, K. T. Rodolfa, and R. Ghani, "Aequitas: A bias and fairness audit toolkit," arXiv preprint arXiv:1811.05577, 2018.

[31] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, "Counterfactual fairness," Advances in neural information processing systems, vol. 30, 2017.

[32] P. W. Koh and P. Liang, "Understanding black-box predictions via influence functions," in International conference on machine learning, pp. 1885-1894, PMLR, 2017.

[33] Z. Cao, J. Wang, S. Si, Z. Huang, and J. Xiao, "Machine unlearning method based on projection residual," arXiv preprint arXiv:2209.15276, 2022.

[34] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, "Fairness through awareness," in Proceedings of the 3rd innovations in theoretical computer science conference, pp. 214-226, 2012.

[35] M. Hardt, E. Price, and N. Srebro, "Equality of opportunity in supervised learning," Advances in neural information processing systems, vol. 29, 2016.

[36] S. Jung, S. Chun, and T. Moon, "Learning fair classifiers with partially annotated group labels," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10348-10357, 2022.

[37] S. Verma and J. Rubin, "Fairness definitions explained," in Proceedings of the international workshop on software fairness, pp. 1-7, 2018.

[38] M. Kearns, S. Neel, A. Roth, and Z. S. Wu, "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness," in International conference on machine learning, pp. 25642572, PMLR, 2018.

[39] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, "Fairness through awareness," in Proceedings of the 3rd innovations in theoretical computer science conference, pp. 214-226, 2012.

[40] M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth, "Rawlsian fairness for machine learning," arXiv preprint arXiv:1610.09559, vol. 1, no. 2, p. 19, 2016.

[41] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel, "The variational fair autoencoder," arXiv preprint arXiv:1511.00830, 2015.

[42] W. Fleisher, "What's fair about individual fairness?," in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 480-490, 2021.

[43] R. Binns, "On the apparent conflict between individual and group fairness," in Proceedings of the 2020 conference on fairness, accountability, and transparency, pp. 514-524, 2020.

[44] S. Chiappa, "Path-specific counterfactual fairness," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 7801-7808, 2019.

[45] S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel, "Counterfactual fairness in text classification through robustness," in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 219-226, 2019.

[46] Y. Wu, L. Zhang, and X. Wu, "Counterfactual fairness: Unidentification, bound and algorithm," in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, 2019 .

[47] N. Kilbertus, P. J. Ball, M. J. Kusner, A. Weller, and R. Silva, "The sensitivity of counterfactual fairness to unmeasured confounding," in Uncertainty in artificial intelligence, pp. 616-626, PMLR, 2020.

[48] F. Kamiran, A. Karim, and X. Zhang, "Decision theory for discrimination-aware classification," in 2012 IEEE 12th international conference on data mining, pp. 924-929, IEEE, 2012.

[49] C. Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson, "Decoupled classifiers for group-fair and efficient machine learning," in Conference on fairness, accountability and transparency, pp. 119-133, PMLR, 2018.

[50] N. Kallus, X. Mao, and A. Zhou, "Assessing algorithmic fairness with unobserved protected class using data combination," Management Science, vol. 68, no. 3, pp. 1959-1981, 2022.

[51] T. Baumhauer, P. Schttle, and M. Zeppelzauer, "Machine unlearning: Linear filtration for logit-based classifiers," Machine Learning, vol. 111, no. 9, pp. 3203-3226, 2022.

[52] Q. P. Nguyen, R. Oikawa, D. M. Divakaran, M. C. Chan, and B. K. H. Low, "Markov chain monte carlo-based machine unlearning: Unlearning what needs to be forgotten," arXiv preprint arXiv:2202.13585, 2022.

[53] A. Tahiliani, V. Hassija, V. Chamola, and M. Guizani, "Machine unlearning: Its need and implementation strategies," in 2021 Thirteenth International Conference on Contemporary Computing (IC3-2021), pp. 241-246, 2021.

[54] M. Magdziarczyk, "Right to be forgotten in light of regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec," in 6th International Multidisciplinary Scientific Conference on Social Sciences and Art Sgem 2019, pp. 177-184, 2019.

[55] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, "Machine unlearning," in 2021 IEEE Symposium on Security and Privacy $(S P)$, pp. 141-159, IEEE, 2021.

[56] J. Brophy and D. Lowd, "Machine unlearning for random forests," in International Conference on Machine Learning, pp. 1092-1104, PMLR, 2021.

[57] A. Golatkar, A. Achille, and S. Soatto, "Eternal sunshine of the spotless net: Selective forgetting in deep networks," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9304-9312, 2020.

[58] A. Mahadevan and M. Mathioudakis, "Certifiable machine unlearning for linear models," arXiv preprint arXiv:2106.15093, 2021.

[59] Y. Wu, E. Dobriban, and S. Davidson, "Deltagrad: Rapid retraining of machine learning models," in International Conference on Machine Learning, pp. 10355-10366, PMLR, 2020.

[60] S. Neel, A. Roth, and S. Sharifi-Malvajerdi, "Descent-to-delete: Gradient-based methods for machine unlearning," in Algorithmic Learning Theory, pp. 931-962, PMLR, 2021.

[61] Z. Cao, J. Wang, S. Si, Z. Huang, and J. Xiao, "Machine unlearning method based on projection residual," arXiv preprint arXiv:2209.15276, 2022.

[62] Z. Izzo, M. A. Smart, K. Chaudhuri, and J. Zou, "Approximate data deletion from machine learning models," in International Conference on Artificial Intelligence and Statistics, pp. 20082016, PMLR, 2021.

[63] C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, "Certified data removal from machine learning models," arXiv preprint arXiv:1911.03030, 2019.

[64] R. D. Cook and S. Weisberg, Residuals and influence in regression. New York: Chapman and Hall, 1982 .

[65] X. Han, B. C. Wallace, and Y. Tsvetkov, "Explaining black box predictions and unveiling data artifacts through influence functions," arXiv preprint arXiv:2005.06676, 2020.

[66] A. Peste, D. Alistarh, and C. H. Lampert, "Ssse: Efficiently erasing samples from trained machine learning models," arXiv preprint arXiv:2107.03860, 2021.

[67] B. A. Pearlmutter, "Fast exact multiplication by the hessian," Neural computation, vol. 6, no. 1, pp. 147-160, 1994.

[68] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.

[69] Y. Li and N. Vasconcelos, "Repair: Removing representation bias by dataset resampling," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9572$9581,2019$.

[70] H. Bahng, S. Chun, S. Yun, J. Choo, and S. J. Oh, "Learning de-biased representations with biased representations," in International Conference on Machine Learning, 2020.

[71] Z. Liu, P. Luo, X. Wang, and X. Tang, "Deep learning face attributes in the wild," in IEEE International Conference on Computer Vision, 2015.

[72] A. Frank, A. Asuncion, et al., "Uci machine learning repository, 2010," URL http://archive. ics. uci. edu/ml, vol. 15, p. 22, 2011.

[73] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, "Learning fair representations," in International conference on machine learning, pp. 325-333, PMLR, 2013.

[74] T. Kamishima, S. Akaho, and J. Sakuma, "Fairness-aware learning through regularization approach," in 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 643650, IEEE, 2011.

[75] R. K. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovi, et al., "Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias," IBM Journal of Research and Development, vol. 63, no. 4/5, pp. 4-1, 2019 .

[76] M. Nadeem, A. Bethke, and S. Reddy, "Stereoset: Measuring stereotypical bias in pretrained language models," arXiv preprint arXiv:2004.09456, 2020.

[77] P. Li and H. Liu, "Achieving fairness at no utility cost via data reweighing with influence," in International Conference on Machine Learning, pp. 12917-12930, PMLR, 2022.

[78] J. Nam, H. Cha, S. Ahn, J. Lee, and J. Shin, "Learning from failure: Training debiased classifier from biased classifier," in Advances in Neural Information Processing Systems, 2020.

[79] H. Bahng, S. Chun, S. Yun, J. Choo, and S. J. Oh, "Learning de-biased representations with biased representations," in International Conference on Machine Learning (ICML), 2020.

[80] M. Yurochkin and Y. Sun, "Sensei: Sensitive set invariance for enforcing individual fairness," arXiv preprint arXiv:2006.14168, 2020.

[81] M. Yurochkin, A. Bower, and Y. Sun, "Training individually fair ml models with sensitive subspace robustness," arXiv preprint arXiv:1907.00020, 2019.

[82] F. Kamiran, A. Karim, and X. Zhang, "Decision theory for discrimination-aware classification," in 2012 IEEE 12th International Conference on Data Mining, pp. 924-929, 2012.

[83] F. Petersen, D. Mukherjee, Y. Sun, and M. Yurochkin, "Post-processing for individual fairness," Advances in Neural Information Processing Systems, vol. 34, pp. 25944-25955, 2021.

[84] R. Zmigrod, S. J. Mielke, H. Wallach, and R. Cotterell, "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology," arXiv preprint arXiv:1906.04571, 2019.

[85] K. Webster, X. Wang, I. Tenney, A. Beutel, E. Pitler, E. Pavlick, J. Chen, E. Chi, and S. Petrov, "Measuring and reducing gendered correlations in pre-trained models," arXiv preprint arXiv:2010.06032, 2020.

[86] S. Ravfogel, Y. Elazar, H. Gonen, M. Twiton, and Y. Goldberg, "Null it out: Guarding protected attributes by iterative nullspace projection," arXiv preprint arXiv:2004.07667, 2020.

[87] T. Schick, S. Udupa, and H. Schtze, "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp," Transactions of the Association for Computational Linguistics, vol. 9, pp. 1408-1424, 2021.

[88] P. P. Liang, I. M. Li, E. Zheng, Y. C. Lim, R. Salakhutdinov, and L.-P. Morency, "Towards debiasing sentence representations," arXiv preprint arXiv:2007.08100, 2020.

[89] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman, "Crows-pairs: A challenge dataset for measuring social biases in masked language models," arXiv preprint arXiv:2010.00133, 2020.

[90] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," arXiv preprint arXiv:1512.03385, 2015.

[91] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[92] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.

[93] S. Seo, J.-Y. Lee, and B. Han, "Unsupervised learning of debiased representations with pseudoattributes," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16742-16751, 2022.


[^0]:    Corresponding author.

