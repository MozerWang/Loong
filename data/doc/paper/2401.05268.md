# AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning 

Shuofei Qiao ${ }^{\star} \times$, Ningyu Zhang ${ }^{\star} \odot$, Runnan Fang ${ }^{\wedge} \bigcirc$, Yujie Luo ${ }^{\star}$,<br>Wangchunshu Zhou ${ }^{\star}$, Yuchen Eleanor Jiang ${ }^{\star}$, Chengfei Lv ${ }^{\diamond}$, Huajun Chen ${ }^{\star}{ }^{*}$<br>"Zhejiang University<br>${ }^{9}$ Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph<br>*AIWaves Inc. ${ }^{\diamond}$ Alibaba Group<br>\{shuofei,zhangningyu\}@zju.edu.cn


#### Abstract

Language agents have achieved considerable performance on various complex questionanswering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on largescale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AUTOACT yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutOAct generally outperforming that of others ${ }^{1}$.


## 1 Introduction

Language agents (Wang et al., 2023a; Xi et al., 2023; Guo et al., 2024), which leverage the powerful reasoning capabilities (Qiao et al., 2023b; Zhang et al., 2023) of Large Language Models (LLMs) to interact with executable tools, have emerged as essential components of AI systems designed to address complex question-answering tasks (Torantulino, 2023; Osika, 2023; Nakajima, 2023; Tang et al., 2023; Xie et al., 2023). The process of endowing LLMs with such interactive capabilities is referred to as Agent Learning wherein[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-01.jpg?height=326&width=780&top_left_y=725&top_left_x=1049)

Figure 1: The basic framework of AutoAct. Armed with just one tool library, the Meta-Agent can automatically differentiate based on the target task information and produce a sub-agent group that can collaborate to complete the task.

planning (Huang et al., 2024b) plays a pivotal role, which is responsible for decomposing complex questions into simpler ones (Wei et al., 2022; Yao et al., 2023; Team, 2023; Qian et al., 2023), invoking external tools (Shen et al., 2023; Lu et al., 2023; Qin et al., 2023), reflecting on past mistakes (Shinn et al., 2023; Madaan et al., 2023), and aggregating information from various sources to reach the final answer. There have been a lot of works (Li et al., 2023; Shen et al., 2023; Hong et al., 2023; Talebirad and Nadiri, 2023; Chen et al., 2023d,b) that directly prompt closed-source off-the-shelf LLMs to plan on particular tasks. Despite their convenience and flexibility, closed-source LLMs inevitably suffer from unresolved issues, as their accessibility often comes at a steep price and their black-box nature makes the result reproduction difficult. In light of this, some recent endeavors have shifted their focus towards imbuing open-source models with planning capabilities through fine-tuning (Chen et al., 2023a; Zeng et al., 2023; Yin et al., 2023).

However, despite the achievements of the existing fine-tuning-based methods, they are not without limitations. On the one hand, training opensource models necessitates a substantial amount of annotated QA data pairs and still relies on closedsource models to synthesize planning trajectories. However, fulfilling these requirements in many
real-world scenarios, such as private personal bots or sensitive company business, often proves to be rocky. On the other hand, from the perspective of agent framework, fine-tuning-based methods compel one single language agent to learn all planning abilities, placing even greater pressure on them. These contradict Simon's principle of bounded rationality (Mintrom, 2015), which states that "precise social division-of-labor and clear individual tasks can compensate for the limited ability of individuals to process and utilize information".

To this end, we introduce AUToAct, an automatic agent learning framework for QA, which does not rely on large-scale annotated data and synthetic trajectories from closed-source models while incorporating explicit individual tasks with precise division-of-labor (see Fig. 1). Given a limited set of user-provided data examples, AutoAct starts with a MetA-AGENT to obtain an augmented database through self-instruct (Wang et al., 2023b). Then, armed with a prepared tool library, the MeTAAGENT can automatically synthesize planning trajectories without any assistance from humans or strong closed-source models. Finally, we propose the division-of-labor strategy which resembles cell differentiation based on the self-synthesized trajectories (genes), where the META-AGENT acts as a stem cell (Colman, 2008) and differentiates into three sub-agents with distinct functions: task decomposition, tool invocation, and self-reflection, respectively. Our differentiation process is essentially a parameter-efficient training process on the self-synthesized trajectories with low-consumption resources. We list the differences between AUTOACT and prior works in Tab. 3.

Experiments on complex question-answering tasks with different LLMs demonstrate that AUTOACT yields better or parallel performance compared to various strong baselines. Extensive empirical analysis demonstrates the effectiveness of our appropriate division-of-labor strategy.

## 2 AUTOACT

### 2.1 Critical Components of AutoAct

Meta-Agent. The Meta-Agent is responsible for all the preparatory work before selfdifferentiation and serves as the backbone model for all sub-agents. Given limited target task information and a pre-prepared tool library, the METAAGENT can differentiate into an agent group capable of collaborating to accomplish the target task.
In AutoAct, the MetA-AGENT can be initialized with any kind of open-source model.

Target Task Information. In this paper, we mainly focus on agent learning from scratch, which means the task information at hand is quite limited, primarily encompassing three aspects: task name $\mathcal{M}$, task description $\mathcal{P}$, task data examples $\mathcal{C}$. Concretely, $\mathcal{P}$ represents a detailed description of the task's characteristics. $\mathcal{C}=\left\{q_{i}, a_{i}\right\}_{i=1}^{|\mathcal{C}|}$ indicates $|\mathcal{C}|$ question-answer example pairs of the task, where $|\mathcal{C}|$ is very small which users can effortlessly provide (e.g., a few demonstrations). For a more in-depth view of task information, please refer to Appx. E. Note that the task information serves as the only user-provided knowledge of the task for AutoAcT to conduct automatic agent learning.

Tool Library. To facilitate our agents in automatic task planning, we provide a comprehensive tool library at their disposal. The tool library can be denoted as $\mathcal{T}=\left\{m_{i}, d_{i}, u_{i}\right\}_{i=1}^{|\mathcal{T}|}$, where $m$ represents the tool name, $d$ defines the tool functionality, $u$ details the tool usage instruction, and $|\mathcal{T}|$ stands for the tool amount of the library. In our automatic procedure, the META-AGENT has the autonomy to select appropriate tools from the tool library based on the task information. Users also have the option to expand the tool library according to their specific needs, allowing for more flexible utilization. We list the details of our tool library in Appx. F.

### 2.2 Starting from Scratch via Self-Instruct

To acquire a sufficient amount of task data and provide an ample training resource, it is necessary to augment the data based on the examples at hand. We accomplish this process through selfinstruct. Initially, the database $\mathcal{D}$ is set to be equal to the task data examples $\mathcal{C}$, with $\mathcal{C}$ as the seed for data generation. In each round, the MeTA-AgENT generates new question-answer pairs by few-shot prompting, and the few-shot prompt examples are randomly sampled from $\mathcal{D}$. The generated data will be added to $\mathcal{D}$ followed by filtering, with the exclusion of format erroneous and duplicate data before its inclusion. Eventually, we obtain a database $\mathcal{D}=\left\{q_{i}, a_{i}\right\}_{i=1}^{|\mathcal{D}|}$, where the number of data $|\mathcal{D}|$ satisfies $|\mathcal{D}| \gg|\mathcal{C}|$. The prompt we use for selfinstruct can be seen in Appx. G. 1 and we list some cases generated through self-instruct in Appx. H.

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-03.jpg?height=937&width=1424&top_left_y=237&top_left_x=316)

Figure 2: The overview of our proposed framework AutoAct. We initiate with self-instruct to extend the task database from scratch. Then self-planning is applied to conduct automatic agent learning, including automatic tool selection, trajectories synthesis, self-differentiation and group planning. Our self-differentiation is a parameterefficient fine-tuning process to achieve resource-efficient learning.

### 2.3 Automatic Agent Learning via Self-Planning

Automatic Tool Selection. With the tool library at hand, we ask the META-AGENT to select applicable tools for each task automatically. Specifically, we put $\mathcal{T}=\left\{m_{i}, d_{i}, u_{i}\right\}_{i=1}^{|\mathcal{T}|}$ in the form of a tool list as part of the prompt. Along with $\mathcal{T}$, the prompt also includes the task's description $\mathcal{C}$. Finally, we instruct the META-AGENT to select an appropriate set of tools $\mathcal{T}_{s}\left(\mathcal{T}_{s} \subset \mathcal{T}\right)$ to wait for synthesizing trajectories. The prompt we use for automatic tool selection can be seen in Appx. G.2.

Trajectories Synthesis. Without depending on closed-source models, we enable the METAAGENT to synthesize planning trajectories on its own. Equipped with $\mathcal{T}_{s}$, we instruct the METAAGENT to synthesize trajectories in a zero-shot manner on the database $\mathcal{D}$ adhering to the format of Thought-Action-Observation as defined in Yao et al. (2023). In order to obtain high-quality synthesized trajectories, we filter out all the trajectories with reward $<1$ and collect trajectories with exactly correct answers (reward $=1$ ) as the training source for self-differentiation. The prompt for trajectories synthesis can be seen in Appx. G.3.
Self-Differentiation. In order to establish a clear division-of-labor, we leverage synthesized planning trajectories to differentiate the META-AGENT into three sub-agents with distinct functionalities:

- $\vdots$ Plan-AGENT $\pi_{\text {plan }}$ undertakes question decomposition and determines which tool to invoke in each planning loop (Eq. 2).
- ※ Tool-AgENT $\pi_{\text {tool }}$ is responsible for how to invoke the tool (Eq. 3) by deciding the parameters for the tool invocation.
- Q REFLECT-AGENT $\pi_{\text {reflect }}$ engages in reflection by considering all the historical trajectories and providing a reflection result (Eq. 4).

We assume that the planning loop at time $t$ can be denoted as $\left(\tau_{t}, \alpha_{t}, o_{t}\right)$, where $\tau$ denotes Thought, $\alpha$ signifies Action, and $o$ represents Observation. $\alpha$ can be further expressed as $\left(\alpha^{m}, \alpha^{p}\right)$, where $\alpha^{m}$ is the name of the action, and $\alpha^{p}$ is the parameters required to perform the action. Then the historical trajectory at time $t$ can be signaled as:

$$
\begin{equation*}
\mathcal{H}_{t}=\left(\tau_{0}, \alpha_{0}, o_{0}, \tau_{1}, \ldots, \tau_{t-1}, \alpha_{t-1}, o_{t-1}\right) \tag{1}
\end{equation*}
$$

Eventually, supposing that the prompts of target task information, planning format requirements,
and the question are all combined as $\mathcal{S}$, the responsibilities of each sub-agent can be defined as:

$$
\begin{align*}
\tau_{t}, \alpha_{t}^{m} & =\pi_{\text {plan }}\left(\mathcal{S}, \mathcal{T}_{s}, \mathcal{H}_{t}\right)  \tag{2}\\
\alpha_{t}^{p} & =\pi_{\text {tool }}\left(\mathcal{S}, \mathcal{T}_{s}, \mathcal{H}_{t}, \tau_{t}, \alpha_{t}^{m}\right)  \tag{3}\\
\tau^{r}, \alpha^{r} & =\pi_{\text {reflect }}\left(\mathcal{S}, \mathcal{T}_{s}, \mathcal{H}\right) \tag{4}
\end{align*}
$$

where $\tau^{r}$ and $\alpha^{r}$ represent the thought and action of the reflection process, and $\mathcal{H}$ is the planning history after finishing the answer. The trajectories can be reorganized based on the responsibilities above and fed to the META-AGENT for self-differentiation. Our differentiation is a parameter-efficient finetuning process to achieve resource-efficient learning. We give examples of the training data for each sub-agent in Appx. I. Particularly, for each subagent, we train a specific LoRA (Hu et al., 2022).

Group Planning. At inference time, once the tool name $\alpha_{t}^{m}$ generated by the PLAN-AGENT is triggered at time $t$, the ToOL-AGENT is roused to decide the parameters $\alpha_{t}^{p}$ transferred to the specific tool. The return result of the tool is treated as the observation $o_{t}$ and handed to the PlANAgEnT. After the collaboration between the PlanAGEnt and Tool-AGEnT reaches a prediction, the REFLECT-AGENT comes to reflect on the history and provide a reflection result contained in the reflection action $\alpha^{r}$. If the reflection result indicates that the prediction is correct, the whole planning process ends. Otherwise, the PlanAgEnt and Tool-Agent will continue the planning based on the reflection information. The specific sequence of the group planning process can be found in the example on the right of Fig. 2.

## 3 Experimental Setup

Tasks and Metrics. We evaluate AutoAct on HotpotQA (Yang et al., 2018) and ScienceQA (Lu et al., 2022). HotpotQA is a multi-hop QA task challenging for rich background knowledge, the answer of which is usually a short entity or yes/no. Following Liu et al. (2023), we randomly select 300 dev questions divided into three levels for evaluation, with 100 questions in each level. For HotpotQA, the reward $\in[0,1]$ is defined as the $\mathrm{F} 1$ score grading between the prediction and groundtruth answer. ScienceQA is a multi-modal QA task spanning various scientific topics. We also divide the test set into three levels based on the grade, with 120 randomly sampled data in each level. Since ScienceQA is a multi-choice task, the reward $\in\{0,1\}$ is exactly the accuracy. Note that due to the limitations of LMs in generating images, for ScienceQA, during the self-instruct stage, we directly generate captions for the images instead.

Baselines. We choose the open-source Llama2 models (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023) as the backbones of our METAAGENT and sub-agents. The compared baselines include CoT (Wei et al., 2022), ReAct, Chameleon (Lu et al., 2023), Reflexion (Shinn et al., 2023), BOLAA (Liu et al., 2023), ReWOO (Xu et al., 2023), FireAct (Chen et al., 2023a). We detail each baseline in Appx. B. To ensure fairness, we maintain an equal training trajectory volume of 200 for FIREACT and AUTOACT (200 synthesized data). As Reflexion provides answer correctness labels during reflection but other methods including AutoAct do not, we test all the other methods twice and choose the correct one for evaluation. For all the prompt-based baselines, we uniformly provide two examples in the prompt.

Training Setups. We fine-tune all our models with LoRA (Hu et al., 2022) in the format proposed in Alpaca (Taori et al., 2023). All the training and inference experiments are conducted on 8 V100 GPUs within 16 hours. We detail the hyperparameters for training in Appx. B.

## 4 Results

Compare to Prompt-based Agent Learning Baselines. As shown in Tab. 1, the Mistral-7B and Llama-\{13,70\}B models consistently outperform various prompt-based baselines. The Llama70B model even surpasses the agent performance of GPT-3.5-Turbo, achieving a rise of $\uparrow 3.77 \%$ on HotpotQA and $\uparrow 6.39 \%$ on ScienceQA. Therefore, whether in a single-agent or multi-agent architecture, prompt-based methods relying on few-shot demonstrations fail to precisely customize the behavior of the agent, which is also supported by the fact that FIREACT widely outperforms REACT and BOLAA in the context of iterative planning.

Compare to Fine-tuning-based Agent Learning Baselines. Further focusing on FIREAct in Tab. 1, despite the aid of GPT-4, FIREACT's approach of assigning the entire planning task to a single model proves to be burdensome. As a result, its performance on ScienceQA even falls short compared to the prompt-based global planning method, Chameleon. AutoAct decouples

| Backbone | Method | HotpotQA |  |  |  | ScienceQA |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Easy | Medium | Hard | All | G1-4 | G5-8 | G9-12 | All |
| GPT-3.5 | $\infty \bullet \mathrm{CoT}$ | 48.21 | 44.52 | 34.22 | 42.32 | 60.83 | 55.83 | $\overline{65.00}$ | 60.56 |
| Turbo | $\infty$ \& Zero-Shot Plan* | 50.71 | 45.17 | 38.23 | 44.70 | 76.67 | 61.67 | 78.33 | 72.22 |
| Mistral-7B <br> Instruct-v0.2 | $\sigma: \mathrm{CoT}$ | 33.70 | 22.38 | 22.14 | 26.07 | 54.17 | 50.00 | 60.00 | 54.72 |
|  | $\infty \because$ ReAct | 38.09 | 27.57 | 22.05 | 29.24 | 63.33 | 58.33 | 62.50 | 61.39 |
|  | $\infty$ Chameleon | 37.07 | 26.67 | 19.20 | 27.65 | 65.83 | 62.50 | 66.67 | 65.00 |
|  | $\boldsymbol{\infty} \triangle$ Reflexion | 40.78 | 35.02 | 28.36 | 34.72 | $\underline{67.50}$ | $\underline{65.83}$ | $\underline{69.17}$ | $\underline{67.50}$ |
|  | $\Phi$ : BOLAA | 40.86 | 32.11 | 22.36 | 31.78 | 64.17 | 61.67 | 65.83 | 63.89 |
|  | $\sigma: \operatorname{ReWOO}$ | 38.42 | 31.89 | 25.98 | 32.10 | 60.83 | 58.33 | 64.17 | 61.11 |
|  | $0 \because$ FireAct | 45.52 | 32.02 | 30.17 | 35.90 | 65.00 | 62.50 | 64.17 | 63.89 |
|  | O : AUTOACT | $\overline{48.69}$ | 36.65 | 31.37 | 38.89 | 69.17 | 68.33 | 72.50 | 70.00 |
| Llama-2 <br> 13B-chat | $0 \because \mathrm{CoT}$ | 37.90 | 25.28 | 21.64 | 28.27 | 61.67 | 52.50 | 69.17 | 61.11 |
|  | $\infty$ (2) ReAct | 28.68 | 22.15 | 21.69 | 24.17 | 57.50 | 51.67 | 65.00 | 58.06 |
|  | $\infty$ Chameleon | 40.01 | 25.39 | 22.82 | 29.41 | 69.17 | 60.83 | 73.33 | 67.78 |
|  | $\infty$ (D) Reflexion | 44.43 | 37.50 | $\underline{28.17}$ | 36.70 | $\overline{67.50}$ | $\underline{64.17}$ | $\overline{73.33}$ | 68.33 |
|  | $\sigma: \because:$ BOLAA | 33.23 | 25.46 | $\overline{25.23}$ | 27.97 | 60.00 | $\overline{54.17}$ | 65.83 | 60.00 |
|  | $\sigma: \operatorname{ReWOO}$ | 30.09 | 24.01 | 21.13 | 25.08 | 57.50 | 54.17 | 65.83 | 59.17 |
|  | O : FireAct | $\underline{45.83}$ | 38.94 | 26.06 | 36.94 | 60.83 | 57.50 | 67.50 | 61.94 |
|  | 0 : AUTOACT | 47.29 | 41.27 | 32.92 | 40.49 | 70.83 | 66.67 | 76.67 | 71.39 |
| Llama-2 <br> 70B-chat | ब $\triangle \mathrm{CoT}$ | 45.37 | 36.33 | 32.27 | 37.99 | 74.17 | 64.17 | 75.83 | 71.39 |
|  | $\infty$ (2 ReAct | 39.70 | 37.19 | 33.62 | 36.83 | 64.17 | 60.00 | 72.50 | 65.56 |
|  | $\infty \therefore$ Chameleon | 46.86 | 38.79 | 34.43 | 40.03 | 77.83 | $\underline{69.17}$ | 76.67 | $\underline{74.56}$ |
|  | (D) Reflexion | 48.01 | 46.35 | 35.64 | 43.33 | $\overline{75.83}$ | $\overline{67.50}$ | 78.33 | $\overline{73.89}$ |
|  | $\boldsymbol{D}: \boldsymbol{B}$ BOLAA | 46.44 | 37.29 | 33.49 | 39.07 | 70.00 | 67.50 | 75.00 | 70.83 |
|  | $\infty: 0$ : ReWOO | 42.00 | 39.58 | 35.32 | 38.96 | 65.00 | 61.67 | 76.67 | 67.78 |
|  | o : FireAct | 50.82 | 41.43 | 35.86 | 42.70 | 72.50 | 68.33 | 75.00 | 71.94 |
|  | o : : AUTOA | $\overline{56.94}$ | $\mathbf{5 0 . 1 2}$ | 38.35 | 48.47 | 82.50 | 72.50 | $\mathbf{8 0 . 8 3}$ | 78.61 |

Table 1: Main results of AutoAct compared to various baselines on HotpotQA and ScienceQA. The icon $\boldsymbol{0}$ indicates prompt-based agent learning without fine-tuning, while $\boldsymbol{O}$ means fine-tuning-based agent learning.

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-05.jpg?height=40&width=1582&top_left_y=1545&top_left_x=243)
in bold and the second-best results are marked with underline. *We compare the zero-shot plan performance of GPT-3.5-Turbo to ensure fairness in our evaluation since our setup does not include annotated trajectory examples.

the planning process and reaches a clear divisionof-labor among sub-agents for group planning, resulting in an improvement than FIREACT, with $\uparrow 5.77 \%$ on HotpotQA and $\uparrow 6.67 \%$ on ScienceQA with Llama-70B model. Additionally, AuToAcT achieves self-planning without relying on closedsource models and large-scale labeled datasets, which paves the way for automatic agent learning with open-source models from scratch. In ablation study (§4) and human evaluation (§5), we will further validate that the quality of trajectories synthesized by AUTOACT is not inferior to FIREACT trained on trajectories synthesized using GPT-4.

## Single-agent Learning vs. Multi-agent Learn-

ing. Under identical settings, multi-agent architectures generally exhibit better performance than single-agent (REACT vs. BOLAA, FIREACT vs. AUTOACT), which aligns with Simon's theory of bounded rationality. Seemingly contrary to expec-

|  | HotpotQA | ScienceQA |
| ---: | :---: | :---: |
| AUTOACT | 48.47 | 78.61 |
| - reflection | $45.66_{\downarrow 2.81}$ | $75.28_{\downarrow 3.33}$ |
| - multi | $42.81_{\downarrow 5.66}$ | $69.72_{\downarrow 8.89}$ |
| - fine-tuning | $32.84_{\downarrow 15.63}$ | $61.94_{\downarrow 16.67}$ |
| - filtering | $32.51_{\downarrow 15.96}$ | $59.17_{\downarrow 19.44}$ |

Table 2: Approach ablations of AutoAct. - $\boldsymbol{r e}$ flection symbolizes removing the reflect-agent in AUтоAст. - multi denotes feeding all the differentiated data into one model for fine-tuning. - fine-tuning indicates zero-shot prompt planning with the three agents defined in AUToAcT. - filtering represents selfdifferentiation on all the trajectories generated in zeroshot planning without filtering wrong cases.

tations, despite being a single-agent architecture, Chameleon outperforms BOLAA (even FIREACT on ScienceQA). However, we analyze that this can be attributed to the way it leverages tools. In Chameleon, the process of deciding tool parame-

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=763&width=1533&top_left_y=224&top_left_x=273)

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=311&width=486&top_left_y=236&top_left_x=291)

(a) 7B-model 7B-data

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=320&width=491&top_left_y=617&top_left_x=286)

(d) 7B-model 13B-data

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=312&width=488&top_left_y=238&top_left_x=790)

(b) 13B-model 13B-data

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=319&width=488&top_left_y=620&top_left_x=790)

(e) 7B-model 70B-data

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=314&width=488&top_left_y=234&top_left_x=1298)

(c) 70B-model 70B-data

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=325&width=491&top_left_y=617&top_left_x=1294)

(f) 13B-model 70B-data

Figure 3: Performance of AutoAct on HotpotQA with different training data scales. The $\{7,13,70\} \mathrm{B}$ represents Llama-2-\{7,13,70\}B-chat models respectively. (a-c) shows the results of the model trained on selfsynthesized trajectories. (d-f) represents the results of the model trained on trajectories synthesized by a stronger model, where the dashed line is the baseline trained on self-synthesized trajectories.
![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-06.jpg?height=286&width=1534&top_left_y=1205&top_left_x=272)

Figure 4: Performance of AutoAct on HotpotQA based on different degrees of labor division. One is training a single model with all the differentiated data. Three represents the differentiation into three agents: plan, tool, and reflect. Tool Specified indicates further differentiating the tool-agent with one tool, one agent.

ters is considered a form of tool invocation, and specialized few-shot prompts are designed to guide the model through this process. From this aspect, Chameleon, despite nominally a single-agent architecture, exhibits features resembling a multi-agent one, which does not contradict our initial conclusion. Indeed, we can also explain from the perspective of optimizing objectives. Another well-known principle, Goodhart's Law (Goodhart, 1984), states that "When a measure becomes a target, it ceases to be a good measure". This implies that optimizing one objective on the same agent will inevitably harm other optimization objectives to some extent. Therefore, it is not optimal to optimize all objectives on a single agent, and a multi-agent architecture happens to address this issue. However, we analyze in $\S 5$ that excessive fine-grained divisionof-labor is not the best approach.

Approach Ablations. Tab. 2 presents the performance of AUToAct on the Llama-70B model after removing certain key processes. It can be observed that the least impactful removal is the - reflect. We investigate that in the zero-shot scenario, the model tends to be over-confident in its answers (as also confirmed in Huang et al. (2024a)). It typically only recognizes its errors when there are obvious formatting mistakes or significant repetitions in the planning process. Consistent with previous findings, the removal of the - multi agents leads to a noticeable decrease in performance. A more exciting discovery is that the results of - multi are comparable to those of FIREACT. This indirectly suggests that the trajectory quality generated by the 70B model may be no worse than that of GPT-4. As expected, the performance deteriorates after fine-tuning, which once again confirms the previous conclusion. To demonstrate the necessity of filtering out planning error data, we specifically remove the filtering process (- filtering) to examine the performance of AutoAct. The results indicate

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-07.jpg?height=808&width=1422&top_left_y=233&top_left_x=317)

Figure 5: Case study on HotpotQA. AutoAct (b) successfully addresses the failure in REAct (a) by employing a more scientific combination of tools and making more accurate tool invocations. With more planning rounds, AutoAct (c) can validate its inner answers by continuing more rounds of self-verification. While this can also lead to a longer context, gradually deviating AUTOACT (d) from the original question.

![](https://cdn.mathpix.com/cropped/2024_06_04_6f018f6db5d26e5c3375g-07.jpg?height=371&width=665&top_left_y=1299&top_left_x=296)

Figure 6: Human evaluation of trajectories generated by Llama-2-70B-chat on HotpotQA. We compare the number of planning rounds, the logical correctness of thoughts, action types, action parameters, and the overall coherence of each trajectory. The figure above displays the Win Rate of each method in each aspect.

that the damage caused by training on unfiltered data is even greater than that of - fine-tuning.

## 5 Analysis

Larger training data scale does not necessarily mean better results. We evaluate the influence of different training data scales on the performance of self-planning with Llama-\{7,13,70\}B models on HotpotQA in Fig. 3 (a-c). It can be observed that the overall performance of different models goes to stability with minimal waves once the data scale exceeds 200 . We speculate that this may be due to the limited ability of naive self-instruct to boost internal knowledge of the language model. As the training data increases, the knowledge which can be extracted through self-instruct decreases. Despite our efforts to filter out duplicate data, the mindless increase can inevitably lead to a significant surge in similar data, which undermines the benefits of increasing the data scale and makes it challenging to improve model performance or even leads to over-fitting. To further confirm the role of training data, we decouple the models from the training data and evaluate their training results on trajectories synthesized by stronger models. From Fig. 3 (d-f), we can see consistent conclusions with previous findings. Therefore, maximizing the diversity of the synthesized data in the database may be a key improvement direction for AUToAct and we leave this for our future work. We can also observe from Fig. 3 (d-e) that the larger the model, the higher the quality of the synthesized data, as the performance of the $7 \mathrm{~B}$ model shows a gradual increase on self, 13B, and 70B synthesized data.

Moderate division-of-labor benefits group planning performance. To explore the impact of different granularity of self-differentiation, we further subdivide the tool agent, assigning dedicated agents to manipulate each specific tool. We compare the performance of One agent, Three agents (AUToAcT), and the Tool-Specified setting on Hot-

potQA in Fig. 4. It can be observed that excessive differentiation (Tool-Specified) not only fails to achieve better results but can sometimes even be less effective than not differentiating (One) at all. This is consistent with the findings in Qiao et al. (2023a) which indicate that multi-tool joint learning often outperforms single-tool individual learning. Moreover, it appears that the performance loss of tool-specific agents compared to AUTOACT is more significant on harder problems. This is because challenging problems typically require more planning steps and higher levels of collaboration among tools. By unifying tool invocations under one agent, it becomes possible to effectively learn the interconnectedness between tools, thereby compensating for potential information gaps arising from using tool-specific agents. Note the difference from Li et al. (2024), here we are discussing the granularity of division-of-labor among agents with different responsibilities, rather than the voting quantity among mutually equal agents.

Human Evaluation. To get a deeper understanding of the quality of trajectories generated by different methods, we manually compare them from the number of planning rounds, the logical correctness of thoughts, action types, action parameters, and overall coherence. The detailed human evaluation process can be found in Appx. C. The evaluation results are depicted in Fig. 5\&6. We can observe a clear advantage for AUTOAcT over other methods in the action type and action parameters. This indicates that decoupling the missions of planning and tool invocation can lead to better performance for both, alleviating the overwhelming pressure on a single agent. A more intuitive comparison can be observed in Fig. 5 (a-b). AutoAct successfully addresses the failure in REACT by employing a more scientific combination of tools and making more accurate tool invocations. Furthermore, AuTOACT tends to consume more planning rounds than other methods (the specific average planning rounds is in Appx. D). This allows AutoAct to perform better on harder problems. However, this characteristic can be a double-edged sword when it comes to simple problems. A surprising aspect is that AutoAct can validate its inner answers by continuing more rounds of verification (Fig. 5 (c)). But this can also lead to a longer context, gradually deviating AutoAct from the original question (Fig. 5 (d)).

## 6 Related Work

LLM-Powered Agents. The rise of LLMs has positioned them as the most promising key to unlocking the door to Artificial General Intelligence (AGI), providing robust support for the development of LLM-centered AI agents (Wang et al., 2023a; Xi et al., 2023; Wang et al., 2023c,d). Related works focus primarily on agent planning (Yao et al., 2023; Song et al., 2022; Chen et al., 2023a), external tools harnessing (Patil et al., 2023; Qiao et al., 2023a; Qin et al., 2023), collective intelligence among multi-agents (Liang et al., 2023; Liu et al., 2023; Chen et al., 2023c), etc. However, despite their success, existing methods still face two major troubles. Firstly, most agents heavily rely on prompts for customization, which makes it difficult to precisely tailor the behavior of the agent, resulting in unexpected performance at times. Secondly, each agent is compelled to master all skills, making it challenging for the agent to achieve expertise in every domain. In response, our approach leverages a proper division-of-labor strategy and fine-tuning each sub-agent to equip different agents with distinct duties. These agents collaborate to accomplish tasks orderly and effectively.

Agent Fine-Tuning. Despite the vast interest in LLM-powered agents, the construction of agents through fine-tuning has received limited attention. Most early works concentrate on fine-tuning to optimize the model's reasoning capabilities (Liu et al., 2022; Fu et al., 2023) or tool proficiency (Patil et al., 2023; Qiao et al., 2023a; Qin et al., 2023). Recently, more works have emphasized endowing open-source LLMs with agent capabilities through fine-tuning (Chen et al., 2023a; Zeng et al., 2023; Yin et al., 2023; Shen et al., 2024). However, these works suffer from at least one of the following issues: $\boldsymbol{i}$ ) the requirement of one single model to be a generalist, $\boldsymbol{i i}$ ) the need for a large amount of annotated data, iii) the need for trajectory annotation of closed-source models. Our approach enables the MetA-Agent to synthesize trajectories and achieve a division-of-labor strategy in a zero-shot manner, without relying on closed-source models.

## 7 Conclusion and Future Work

In this paper, we propose AUTOAcT, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic trajectories from closed-source models, while alleviating
the pressure on individual agents by explicitly dividing the workload. Interesting future directions include: $\boldsymbol{i}$ ) expanding AutoAcT to more realistic task scenarios (Puig et al., 2018; Zhou et al., 2023a; Xie et al., 2024), ii) boosting more knowledge via self-instruct (as analyzed in $\S 5$ ), iii) iteratively enhancing synthetic trajectories via self-improvement (Huang et al., 2023; Aksitov et al., 2023).

## Limitations

In this paper, we focus on constructing an automatic agent learning framework dubbed AUTOAcT. Despite our best efforts, this paper may still have some remaining limitations.

Tasks. In this paper, we mainly focus on complex question-answering tasks. However, there are many other more complex interactive scenarios, including web (Yao et al., 2022; Zhou et al., 2023a), household (Puig et al., 2018; Shridhar et al., 2021), traveling (Xie et al., 2024), robotics (Ichter et al., 2022), etc. For example, we have investigated the use of META-AGENT performing random explorations (Xiang et al., 2023; Murty et al., 2024) in virtual environments to replace the process of task and trajectory synthesis through self-instruct and zero-shot planning. We plan to conduct further research on applying AutoAct to a wider range of tasks based on this in the future.

Boosting Knowledge via Self-Instruct. As analyzed in $\S 5$, the planning performance of AUTOACT can be limited by the model's ability to access internal knowledge through self-instruct. While the current phenomenon allows us to achieve lightweight self-differentiation in terms of parameters and data, it is still necessary to research how to enrich knowledge as much as possible within the constraints of limited data.

Self-Improvement. Recent research has shed light on self-improvement techniques that enhance LLMs by iteratively training them on selfsynthesized data (Zelikman et al., 2022; Huang et al., 2023; Gülçehre et al., 2023; Aksitov et al., 2023). This approach allows the model to continually learn and refine its performance on its own. Our approach also involves training on selfsynthesized data and we believe that further using the iterative thinking of self-improvement will significantly enhance the performance of our method.

## Ethics Statement

This research was conducted with the highest ethical standards and best practices in research. All our experiments use publicly available datasets (as detailed in §3), avoiding ethical concerns related to privacy, confidentiality, or misuse of personal biological information. The human evaluation process (as detailed in Appx. C) was carried out strictly with fairness and transparency. Consequently, this research is free from any ethical concerns.

## Acknowledgements

We would like to express our sincere gratitude to the anonymous reviewers for their thoughtful and constructive feedback. This work was supported by the National Natural Science Foundation of China (No. 62206246), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent Introduction Programme (2021A-156-G), Tencent AI Lab Rhino-Bird Focused Research Program (RBFR2024003), and Information Technology Center and State Key Lab of CAD\&CG, Zhejiang University.

## References

Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. 2023. Rest meets react: Self-improvement for multistep reasoning llm agent.

Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023a. Fireact: Toward language agent fine-tuning. CoRR, abs/2310.05915.

Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F. Karlsson, Jie Fu, and Yemin Shi. 2023b. Autoagents: A framework for automatic agent generation. CoRR, abs/2309.17288.

Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2023c. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. CoRR, abs/2309.13007.

Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023d. Agentverse: Facilitating multiagent collaboration and exploring emergent behaviors in agents. CoRR, abs/2308.10848.

Alan Colman. 2008. Human embryonic stem cells and clinical applications. Cell Research, 18(1):S171S171.

Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 10421-10430. PMLR.

C. A. E. Goodhart. 1984. Problems of Monetary Management: The UK Experience, pages 91-121. Macmillan Education UK, London.

Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. CoRR, abs/2308.08998.

Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based multi-agents: A survey of progress and challenges. CoRR, abs/2402.01680.

Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. 2023. Metagpt: Meta programming for multi-agent collaborative framework. CoRR, abs/2308.00352.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.

Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1051-1068. Association for Computational Linguistics.

Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024a. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations.

Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b. Understanding the planning of llm agents: A survey.
Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, KuangHuei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. 2022. Do as I can, not as I say: Grounding language in robotic affordances. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287-318. PMLR.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. CoRR, abs/2310.06825.

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. CAMEL: communicative agents for "mind" exploration of large scale language model society. CoRR, $\mathrm{abs} / 2303.17760$.

Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. 2024. More agents is all you need.

Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. CoRR, abs/2305.19118.

Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 31543169. Association for Computational Linguistics.

Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. 2023. BOLAA: benchmarking and orchestrating llm-augmented autonomous agents. CoRR, abs/2308.05960.

Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. CoRR, $\mathrm{abs} / 2304.09842$.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651.

Michael Mintrom. 2015. 12Herbert A. Simon, Administrative Behavior: A Study of Decision-Making Processes in Administrative Organization. In The Oxford Handbook of Classics in Public Policy and Administration. Oxford University Press.

Shikhar Murty, Christopher D. Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. 2024. BAGEL: bootstrapping agents by guiding exploration with language. CoRR, abs/2403.08140.

Yohei Nakajima. 2023. Babyagi. https://github. com/yoheinakajima/babyagi.

OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/ chatgpt/.

OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.

Anton Osika. 2023. Gpt-engineer. https://github. com/AntonOsika/gpt-engineer.

Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334.

Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018. Virtualhome: Simulating household activities via programs. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8494-8502. Computer Vision Foundation / IEEE Computer Society.

Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. CoRR, abs/2307.07924.

Shuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. 2023a. Making language models better tool learners with execution feedback. CoRR, $\mathrm{abs} / 2305.13068$.

Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023b. Reasoning with language model prompting: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 53685393. Association for Computational Linguistics.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. CoRR, abs/2307.16789.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In $K D D$ '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 3505-3506. ACM.

Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024. Small llms are weak tool learners: A multi-llm agent. CoRR, abs/2401.07324.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580.

Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: language agents with verbal reinforcement learning. CoRR, abs/2303.11366.

Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. 2021. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. 2022 Llm-planner: Few-shot grounded planning for embodied agents with large language models. CoRR, $\mathrm{abs} / 2212.04088$

Yashar Talebirad and Amirhossein Nadiri. 2023. Multiagent collaboration: Harnessing the power of intelligent LLM agents. CoRR, abs/2306.03314.

Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2023. Medagents: Large language models as collaborators for zero-shot medical reasoning. CoRR, $\mathrm{abs} / 2311.10537$.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.

XAgent Team. 2023. Xagent: An autonomous agent for complex task solving.

Torantulino. 2023. Autogpt: build \& use ai agents. https://github.com/Significant-Gravitas.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, and et. al. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023a. A survey on large language model based autonomous agents. CoRR, $\mathrm{abs} / 2308.11432$.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 13484-13508. Association for Computational Linguistics.

Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023c. Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. In Thirtyseventh Conference on Neural Information Processing Systems.

Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. 2023d. Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models. arXiv preprint arXiv:2311.05997.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The rise and potential of large language model based agents: A survey. CoRR, abs/2309.07864.

Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. 2023. Language models meet world models: Embodied experiences enhance language models. In Advances in Neural
Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - $16,2023$.

Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world planning with language agents. CoRR, abs/2402.01622.

Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. 2023. Openagents: An open platform for language agents in the wild. CoRR, abs/2310.10634.

Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. 2023. Rewoo: Decoupling reasoning from observations for efficient augmented language models. CoRR, abs/2305.18323.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2369-2380. Association for Computational Linguistics.

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. In NeurIPS.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.

Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2023. Lumos: Learning agents with unified data, modular design, and open-source llms. CoRR, abs/2311.05657.

Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In NeurIPS.

Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for llms. CoRR, $\mathrm{abs} / 2310.12823$.

Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao. 2023. Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents. CoRR, abs/2311.11797.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.

Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2023a. Webarena: A realistic web environment for building autonomous agents. CoRR, abs/2307.13854.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. 2023b. Agents: An open-source framework for autonomous language agents. CoRR, abs/2309.07870.
