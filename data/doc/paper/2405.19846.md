# Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model 

Chaochen Gao<br>Institute of Information Engineering<br>Chinese Academy of Sciences<br>gaochaochen@iie.ac.cn<br>Qi Fu<br>Xiaohongshu Inc<br>fuqi@xiaohongshu.com

Xing Wu ${ }^{*}$<br>Institute of Information Engineering<br>Chinese Academy of Sciences<br>Xiaohongshu Inc<br>wuxing@iie.ac.cn<br>Songlin Hu ${ }^{\dagger}$<br>Institute of Information Engineering<br>Chinese Academy of Sciences<br>husonglin@iie.ac.cn


#### Abstract

Large language models, initially pre-trained with a limited context length, can better handle longer texts by continuing training on a corpus with extended contexts. However, obtaining effective long-context data is challenging due to the scarcity and uneven distribution of long documents across different domains. To address this issue, we propose a Query-centric data synthesis method, abbreviated as Quest. Quest is an interpretable method based on the observation that documents retrieved by similar queries are relevant but low-redundant, thus well-suited for synthesizing long-context data. The method is also scalable and capable of constructing large amounts of long-context data. Using Quest, we synthesize a long-context dataset up to $128 \mathrm{k}$ context length, significantly outperforming other data synthesis methods on multiple long-context benchmark datasets. In addition, we further verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models.


## 1 Introduction

Large Language Models (LLMs) are typically pre-trained using predefined context lengths, and recent advancements have emphasized the increasing importance of extending the context lengths. For example, the LLaMA series of models [45, 46 has progressively extended the context length from $2 \mathrm{k}$ (LLaMA) to 4k (LLaMA2) and then to 8k (LLaMA3). LLMs equipped with extended context windows have shown superior capabilities in handling complex tasks [9, 5, 30, 48]. When facing demands for very long contexts, such as $128 \mathrm{k}$ tokens, a widely adopted, effective and efficient method is to continue training LLMs with long-context data after pre-training [38, 52, 15].

To obtain long-context data, [52, 15] have focused on filtering out long documents that meet the target context length demand. However, these documents are often limited to a few specific domains. As shown in Figure 1](left), we analyze a widely used pre-training corpus (the Pile [16]), long documents primarily comes from the Books3 dataset. This results in an skewed distribution of long-context data, a problem that intensifies as the target context length increases. Previous studies [17, 25, 41] have demonstrated the potential of synthesizing long-context data by aggregating semantically similar[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-02.jpg?height=508&width=990&top_left_y=243&top_left_x=556)

Figure 1: (Left) Distribution of long documents up to 128k in Pile. (Right) Distribution of $128 \mathrm{k}$ long-context data synthesized by Quest. For more details see Appendix A

![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-02.jpg?height=635&width=1309&top_left_y=886&top_left_x=403)

Figure 2: (1) Overview of Query-centric Data Synthesis (Quest) method. (2) Unlike the standard pre-training strategy that randomly shuffled documents in the input context, Quest places relevant documents in the same context.

documents, such as concatenating a document with its top $\mathrm{k}$ retrieved documents. However, documentsimilarity-based methods often prefer similar sentences, resulting in redundancy of retrieved top documents, especially in large-scale corpora. Redundancy will reduce token prediction difficulty and context diversity, weakening the effectiveness of long-context modeling (we provide analysis in Section 6.2). Therefore, there is an urgent need for a method that effectively aggregates relevant but low-redundant documents for synthesizing long-context data. Moreover, the method is expected to be highly scalable, i.e., capable of constructing large amounts of data to satisfy continued-training demands.

This paper proposes a Query-centric data synthesis approach, abbreviated as Quest, for constructing highly scalable long-context data. We draw our inspiration from the fact that similar queries can aggregate relevant but non-redundant documents via search engines [29, 3, 23]. However, although large amounts of queries can be crawled on the Internet, ensuring the diversity and quality of these queries remains challenging. Thus, we predict potential queries for each document through a trained generative model. We can balance diversity and quality by controlling how the generative sampling process. Specifically, Quest begins by employing a lightweight query-prediction model [36, 31, 49], to predict varied potential queries for each document. Documents that share the same query are grouped as relevant, similar to an inverse search process. Then, Quest clusters the large number of queries into coarse-grained keywords, similar to topics. Thus documents associated with similar queries are further indexed by the identical keywords. Finally, Quest randomly samples from documents indexed by the same keywords and concatenates the selected documents to build long-context data.

The scaling law has been widely studied in the field of pre-training [22, 19, 20, 2, 32, 6, 42] . However, scaling law for long-context synthesized data have not yet been explored, while it is crucial to evaluate the robustness and predictability of methods. Therefore, we further explore the scaling laws of long-context synthesized data under various model scales based on our proposed Quest method. Through accurately modeling and curve fitting many training processes in small settings, it is expected that the validation loss for training on larger data settings can be predicted.

Extensive experiments demonstrate that Quest significantly outperforms other data synthesis methods on multiple long-context benchmarks with context lengths ranging from $32 \mathrm{k}$ to $128 \mathrm{k}$. In addition, we verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models.

Our contributions are summarized below:

1. We propose a query-centric data synthesis method to alleviate the problems of long-context data scarcity and uneven distribution across different domains.
2. Extensive experiments and evaluations on $32 \mathrm{k}$ and $128 \mathrm{k}$ context lengths demonstrate that the proposed method outperforms existing methods.
3. We explore the scaling law of long-context synthesized data and verify the predictability of the proposed method.

## 2 Related work

Long-Context Language Models With the great success of LLMs, enabling LLMs to process longer texts attracts a lot of attention. Some works adapt models to handle longer texts without training by directly modifying position encoding. For instance, Han [18] and Xiao [50] modify the attention matrix to allow the model to generate long contexts. Jin [21] compresses the model's position encoding into the pre-trained position encoding range. To achieve better performance, other works involve continued training of the model [38]. Xiong [52] validates that long-context capabilities can be acquired by continually pre-training from short-context models. Chen [11] uses position interpolation to change the distribution of position encoding. Yen [55] proposes context expansion with parallel encoding. Some works [35, 43] make progress on top of RoPE [44], which enables the LLM to handle longer positions. PoSE [59] uses skip-wise position indices to train the model, allowing the position encoding to adapt to different lengths. However, most previous works focus on improving position encoding and overlook the scarcity and uneven distribution of long text data used during continued training. Previous approaches often rely on filtering long documents from pre-existing corpora [52, 15], or randomly splicing short documents to achieve a fixed length [38, 12, 47, 10, 26].

Data Synthesis and Augmentation For Long-Context Acquiring effective long-context data for training remains a significant challenge. As far as we know, some previous retrieval-augmented pre-training works [17, 25] have the potential to synthesize long-context data. Guu [17] proposes retrieval-augmented pre-training, clustering semantically similar texts within the same context window. Levine [25] demonstrates that incorporating semantically related but non-adjacent sentences within the same pre-training example enhances sentence representations. Shi [41] uses a traveling salesman algorithm to address the document redundancy problem in the kNN method. However, previous data synthesis efforts have been limited to a context length of $8 \mathrm{k}$ or less, and the benefits of synthesizing longer data in these efforts are unclear. In this work, we use various methods to synthesize texts up to $128 \mathrm{k}$ and demonstrate the effectiveness of Quest in synthesizing long-context data.

Scaling Laws For a broad spectrum of factors $x$, scaling laws [22, 19, 20] indicate that their impact on the loss $L$ of a pre-trained model follows a power law relationship. Here, $x$ may represent model sizes, quantities of training data, or training steps, with parameters to be determined. Previous research [2, 32, 6, 42, 53] highlights the impressive predictive power of scaling laws. Notably, fitting this relationship to a set of smaller models, training datasets, or computational resources enables precise extrapolation to predict the test loss for much larger cases across several orders of magnitude. This capability allows practitioners to estimate the performance of a pre-trained large language model without incurring the substantial cost of completing extensive training runs.

## 3 Method

This section details our proposed Query-centric Data Synthesis (Quest) method. Given a dataset with diverse documents $D=\left\{d_{i}\right\}$, our goal is to effectively aggregate relevant but low-redundant documents for synthesizing training texts with a context length of $L$. An overview of our approach can refer to Figure 2. Quest mainly includes five steps. First, a query $\left\{q_{i}\right\}$ is predicted for each document $\left\{d_{i}\right\}$ in the corpus. Next, a topic keyword $\left\{k_{i}\right\}$ is extracted from each query. Thirdly, documents with the same keyword are grouped or indexed together. Then, we split the keyword-based inverted indexes according to the number of documents. During training, at each step, we perform sampling without replacement for the documents $\left\{k_{i}\right\}$ within a sampled index. The details of each stage are provided below.

1. Query Prediction: We utilize the open-source doc2query model[31] to predict queries $\left\{q_{i}\right\}$ for each document $\left\{d_{i}\right\}$. For texts that exceed the context length limit of the doc2query model, we segment them into parts and generate a query for each segment. Consequently, for a document $\left\{d_{i}\right\}$, a list of queries $Q_{i}=\left\{q_{i}^{1}, \ldots, q_{i}^{n}\right\}$ is predicted.
2. Keyword Extraction: We extract keywords from each query $\left\{q_{i}\right\}$ with an efficient tool, Rake ${ }^{3}$ For texts with multiple queries, Rake generates several lists of keywords $K_{i}=$ $\left\{k_{i}^{1}, \ldots, k_{i}^{n}\right\}$. To ensure the quality of extracted keywords, we adopt two filtering strategies. First, we filter out keywords with a Rake score below 3.0. Second, we remove frequent but non-informative keywords such as"following sentence" or "best way" (see Appendix B. 2 for details). Then, we randomly select one of the remaining keywords to serve as the representative keyword for the document.
3. Building a Keyword-based Inverted Index: We can build a keyword-based inverted index $I$ after we map each document to its representative keyword. Documents with an identical representative keyword are indexed together.
4. Indexes Split: We rank the keyword-based inverted indexes in ascending order based on the number of documents within each index and divide the sorted indexes into two sets. The top-ranked split_ratio\% of the keyword-based inverted indexes are assigned to the short-index set $I_{s}$, while the remainder is assigned to the long-index set $I_{l}$.
5. Training Process: We perform sampling without replacement from the documents within a sampled index and concatenate the selected documents up to the model's context length $L$ for training. We oversample the short indexes to ensure that the number of tokens participating in training is evenly distributed between the short and long indexes.
```
Algorithm 1 Query-centric Data Synthesis (Quest) Method
Require: Dataset $D=\left\{d_{i}\right\}$, Context length $L$, Split ratio $r$
Ensure: Training texts with context length $L$
    Initialize lists $Q$ and $K$
    for each $d_{i} \in D$ do
        $Q \leftarrow Q \cup$ doc2query $\left(d_{i}\right)$
    end for
    for each $q_{i} \in Q$ do
        $K_{i} \leftarrow\left\{k \in \operatorname{Rake}\left(q_{i}\right) \wedge \operatorname{score}(k) \geq 3.0\right\}$
        $K \leftarrow K \cup\left\{\operatorname{random}\left(K_{i}\right)\right\}$
    end for
    $I \leftarrow\left\{\left(k_{i}, d_{i}\right) \mid d_{i} \in D\right\}$
    Sort $I$ by size and split: $I_{s}=\{i \in I|\operatorname{rank}(i) \leq r \times| I \mid\}, I_{l}=I \backslash I_{s}$
    for each training step do
        Sample $I_{k} \in I_{s} \cup I_{l}$ (oversample $I_{s}$ )
        $T \leftarrow \operatorname{concat}\left(\operatorname{sample}\left(I_{k}\right)\right),|T| \geq L$
        Train with $T$
    end for
```

${ }^{3}$ https://pypi.org/project/rake-nltk

## 4 Experiments

In this section, we first introduce the experimental settings (Section 4.1). Then we provide a detailed description of our baseline methods (Section 4.2) and the experimental results comparison (Section 4.3).

### 4.1 Experimental Setup

To verify the effectiveness and scalability of the proposed Quest, we conduct continued training on Pythia[7] models of different sizes, specifically 1.4B, 6.9B, and 12B. Pythia is a series of models trained on the Pile [16] dataset, explicitly designed for research with several versions available in different sizes. Experiments conducted with Pythia at different scales offer good reproducibility.

We apply the Quest method on Pythia's pre-training data, i.e., the Pile dataset, which does not lead to domain transfer issues. Specially, we extract 30B tokens of keyword-indexed documents from the 300B tokens of the original Pile dataset. Documents indexed by an identical keyword are randomly concatenated to form long-context data that reach the training context length.

During the training process, we use the open-source training framework GPT-NeoX 4 , and the batch size is $4 \mathrm{M}$ tokens for all settings. We employe the AdamW[28] optimizer with parameters $\beta_{1}=0.9$ and $\beta_{2}=0.95$ and a cosine learning rate schedule. Additionally, we utilize Flash Attention2[14] and ZeRO[37] technology to optimize memory usage and accelerate performance. The learning rates are set to $5 e^{-5}$ for the 1.4B model, $4 e^{-5}$ for the $6.9 \mathrm{~B}$ model, and $2 e^{-5}$ for the $12 \mathrm{~B}$ model. For more training details, see Appendix B. 1

### 4.2 Baselines Methods

We compare the proposed Quest method with the previous data synthesis methods:

1. Standard Method shuffles and concatenates documents randomly in the input context and has been the standard practice in pre-training [33, 24, 45].
2. kNN (Retrieval-augmented Language Model Pre-training) [17, 25] places each document along with the top $\mathrm{k}$ retrieved documents in the same input context. Due to potential document duplication or redundancy, the variety of documents in $\mathrm{kNN}$ is relatively limited.
3. ICML[41] Method: is a recently proposed method that utilizes a traveling salesman algorithm to alleviate the document redundancy problem in the kNN method by ranking similarities and determining the optimal training path.

For implementing kNN, we utilized a product quantized inverted file (IVFPQ) FAISS index with a code size of 32 and 32,768 corresponding inverted lists. For ICLM, we followed the GitHub repository ${ }^{5}$ to synthesize long-context data from the Pile dataset.

### 4.3 Evaluation

We evaluate four methods, including Quest and three baseline methods, with evaluation lengths ranging from $32 \mathrm{k}$ to $128 \mathrm{k}$. To comprehensively compare Quest with baseline methods, the datasets from different evaluation tasks are divided into two categories: long text and short text.

1. Long-context Benchmark: For $32 \mathrm{k}$ context length, we conduct evaluations on the widelyused Longbench[4] Benchmark. We extensively tested six types of tasks within this dataset, namely single-document QA, multi-document QA, summarization, few-shot learning, Synthetic, and code completion, which together comprise 17 sub-datasets. For $128 \mathrm{k}$ context length, we report the results on Longbook QA task [58].
2. Short Text Benchmark: To assess the performance of long-text models on standard short-text tasks, we selected seven standard short-text datasets, including WinoGrande [40], PIQA[8], Logiqa[27], Lambada (OpenAI)[34], HellaSwag[57], ARC-Easy and ARCChallenge [13] for evaluation.

Table 1: Experimental results of models with $32 \mathrm{k}$ context length on the Longbench. For detailed results, please refer to Appendix $\mathrm{C}$

| Train\&Test | Model size | Method | Avg. | Sgl. | Multi. | Sum. | Few. | Syn. | Code. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $32 \mathrm{k}$ | 1.4B | Standard | 20.94 | 19.24 | 17.46 | 20.65 | 26.75 | 2.04 | 36.41 |
|  |  | KNN | 19.97 | 17.26 | 13.01 | 22.97 | 24.16 | 2.33 | 39.22 |
|  |  | ICLM | 19.82 | 20.01 | 14.71 | 21.95 | 23.09 | 1.94 | 35.31 |
|  |  | Quest | 22.06 | 17.97 | 17.98 | 21.91 | 28.06 | 2.33 | 42.25 |
| $32 \mathrm{k}$ | $6.9 \mathrm{~B}$ | Standard | 22.48 | 18.07 | 16.83 | 22.33 | $\mathbf{3 0 . 2 3}$ | 3.86 | 40.91 |
|  |  | KNN | 21.65 | 18.5 | 13.64 | 22.56 | 28.18 | 3.76 | 41.88 |
|  |  | ICLM | 20.86 | 17.82 | 15.34 | 22.35 | 25.86 | 1.21 | 41.15 |
|  |  | Quest | 23.23 | 19.21 | 14.13 | 22.45 | 30.14 | 2.96 | 50.55 |
| $32 \mathrm{k}$ | 12B | Standard | 24.85 | 22.18 | 21.94 | 22.30 | 32.05 | 3.78 | 43.73 |
|  |  | KNN | 22.95 | 20.55 | 20.48 | 23.51 | 29.19 | 2.47 | 37.44 |
|  |  | ICLM | 24.07 | 22.67 | 23.29 | 23.41 | 30.99 | 1.5 | 37.09 |
|  |  | Quest | 25.24 | 22.34 | 21.08 | 23.74 | 31.91 | 3.22 | 46.8 |

Table 2: Experimental results of models with 128k context length on the Longbook QA.

| Train\&Test | Model size | Method | Longbook QA |
| :---: | :---: | :---: | :---: |
| $128 \mathrm{k}$ | 1.4B | Standard | 9.94 |
|  |  | $\mathrm{KNN}$ | 10.36 |
|  |  | ICLM | 10.70 |
|  |  | Quest | 11.30 |
| $128 \mathrm{k}$ | $6.9 \mathrm{~B}$ | Standard | 14.47 |
|  |  | KNN | 13.38 |
|  |  | ICLM | 14.92 |
|  |  | Quest | 17.95 |
| $128 \mathrm{k}$ | $12 \mathrm{~B}$ | Standard | 17.81 |
|  |  | KNN | 16.42 |
|  |  | ICLM | 18.44 |
|  |  | Quest | 18.92 |

Quest achieves the best performance on 32k. To evaluate the effectiveness of the Quest method, we increased the context length of models from $2 \mathrm{k}$ to $32 \mathrm{k}$ tokens. The comparison results on the Longbench are presented in Table 1, where Quest outperforms other methods across different model sizes. According to the results, KNN and ICLM underperform the Standard method, likely because they group textually similar documents, leading to redundancy in the context. To analyze the behavior of these methods further with a $32 \mathrm{k}$ context length, we employ TSNE for visualization. Figure 3](left) illustrates that the Standard method results in the most dispersed document clusters due to random aggregation. Conversely, KNN and ICLM methods show tightly clustered documents, whereas Quest exhibits moderate aggregation. This indicates that Quest minimizes document redundancy within the extended context, aligning with its superior performance in the Longbench results.

Quest achieves the best performance on 128k. To further evaluate the efficacy of the Quest method in extended long-context settings, we increase the context length to $128 \mathrm{k}$ and evaluate the trained models on the Longbook QA task. Table 2 shows that Quest consistently outperforms other methods across various model sizes. As shown in Figure 3 (right), the distribution characteristics of documents aggregated by different methods are similar under the 128k and 32k context lengths. Quest continues to aggregate low-redundant documents within the $128 \mathrm{k}$ context. Besides, ICLM identifies a document-similarity-based pathway that, while not as effective as Quest, also decreases document redundancy and improves model performance compared to KNN. The standard method, which uses randomly sampled documents with no semantic relevance, performs poorly, with this weakness becoming more pronounced at $128 \mathrm{k}$. Overall, the superior performance of the Quest method at $32 \mathrm{k}$ and 128k on the long-context evaluation, along with the visualization results, demonstrates Quest's scalability in synthesizing better long-context data.

4 https://github.com/EleutherAI/gpt-neox

5 https://github.com/swj0419/in-context-pre-training
![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-07.jpg?height=426&width=1134&top_left_y=258&top_left_x=476)

Figure 3: TSNE visualization of aggregated documents derived from different methods. For more examples see Appendix D

Table 3: Short text performance comparison of different models on various tasks.

| Model | Avg | Win | PIQA | LogiQA | LAMBADA | Hella | ARC-E | ARC-C |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Pythia | 0.4830 | 0.5746 | 0.7095 | 0.2120 | 0.6163 | 0.4042 | 0.6048 | 0.2594 |
| + Standard | 0.4802 | 0.5675 | 0.6975 | 0.2227 | 0.6507 | 0.3943 | 0.5821 | 0.2466 |
| + KNN | 0.4769 | 0.5651 | 0.7089 | 0.2028 | 0.6480 | 0.3946 | 0.5737 | 0.2449 |
| + ICLM | 0.4816 | 0.5785 | 0.7024 | 0.2120 | 0.6546 | 0.3941 | 0.5753 | 0.2543 |
| + Quest | 0.4831 | 0.5691 | 0.7024 | 0.2304 | 0.6472 | 0.3961 | 0.5770 | 0.2594 |

Quest retains good performance on short text. To verify how well Quest maintains model performance on short text tasks, we evaluate it on seven commonly reported tasks, as shown in Table 3. Compared with the base model, the performance on short text evaluation remains after continued training with long context data derived from the Quest method. In contrast, other long text synthesis methods result in varying degrees of degradation in short text evaluation.

## 5 Scaling Law of Synthesized Long-context Data

To explore the scaling law of synthesized long-context data, we vary the amount of training data for different model sizes (1.4B, 6.9B, and 12B) under the $32 \mathrm{k}$ context length setting. Formally, we formulate the scaling law of the validation loss by studying different model sizes $N$ and dataset sizes $D$ :

$$
L(D)=\alpha \exp (-\beta D)+\gamma
$$

This formula applies to each model size, where $\{\alpha, \beta, \gamma\}$ are variables to be learned. In our experiments, each model is trained separately on datasets of different sizes: 250 million, 500 million, 1 billion, 2 billion, and 4 billion tokens. Then, we fit a curve for each model size, showing the relationship between the data scaling and the validation loss at the end of each training, as shown in Figure 4 .

Furthermore, we verify the correctness of the learned scaling law formula under the data scale of 8B. For each model size, we compare the relative error between the validation loss at the end of training with $8 \mathrm{~B}$ data and its predicted counterpart via the learned scaling law. The relative error of the 1.4B model is $0.5 \%$, the relative error of the $6.9 \mathrm{~B}$ model is $-0.5 \%$, and the relative error of the $12 \mathrm{~B}$ model is $0.4 \%$. These negligible errors are a strong demonstration of the scalability and predictability of Quest's data synthesis approach.

## 6 Analysis

This section provides an in-depth analysis of the Quest method. Considering the high computational cost of LLM experiments, our ablation experiments are performed in a $32 \mathrm{k}$ context length, 1.4B model setting unless otherwise stated.

![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-08.jpg?height=528&width=805&top_left_y=240&top_left_x=649)

Figure 4: Scaling law of synthesized long-context data under different model sizes.

![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-08.jpg?height=528&width=792&top_left_y=880&top_left_x=664)

Figure 5: Performance trends during the training process using data synthesis methods.

### 6.1 Quest's Advantage Gradually Expands with Training Progress

This section studies the performance trends of the training process using data synthesized by the Quest method. As shown in Figure 5, on the Longbench benchmark, the Quest method consistently outperforms other data synthesis methods from start to finish and exhibits superior evaluation performance. Additionally, the training process using the Quest method saturates significantly later. In contrast, other methods generally reach performance saturation within the first $40 \%$ of the training process. These two distinct advantages further demonstrate that the Quest method is a superior long-context data synthesis approach compared to other methods.

### 6.2 Quest Balances Document Similarity for Superior Performance

To investigate the impact of document similarity within the same context on performance, we randomly sample contexts derived from different methods and calculate the similarity between documents aggregated into the same context. As shown in Figure 6, we find that under different model sizes and context length settings, the performance of models shows a trend of first improving and then declining as similarity increases. This indicates that both irrelevant and highly similar document aggregation can lead to performance degradation in long-context modeling.

### 6.3 Quest-Synthesized Long-context Outperforms Long Documents

Some long documents have already reached the target context length in the pre-training corpus. We compare the performance of using Quest-synthesized long-context data with using only long documents for training. Table 4 shows that using Quest-synthesized long-context data achieves better results on Longbench than using only long documents. Long documents perform worse because
![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-09.jpg?height=498&width=1002&top_left_y=239&top_left_x=556)

Figure 6: The trend of long-context performance as the similarity of aggregated documents increases. All results are normalized within the similarity range.

Table 4: Performance comparison of using long document and Quest synthesized long-context data.

| Method | Avg. | Sgl. | Multi. | Sum. | Few. | Syn. | Code. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Long document | 21.11 | 19.77 | 15.15 | 22.11 | 24.81 | 2.46 | 41.84 |
| Quest | $\mathbf{2 2 . 0 6}$ | 17.97 | 17.98 | 21.91 | 28.06 | 2.33 | 42.25 |

they only exist in a few domains, resulting in a skewed data distribution. The Quest method, on the other hand, can cover every domain, resulting in more diverse synthesized long-context data and better performance in evaluation tasks. We also attempt further comparisons with a context length of $128 \mathrm{k}$. However, long documents exceeding 128k in the Pile are rare and inadequate to support a fair comparison experiment. As the target context length increases, the scarcity problem becomes more pronounced, highlighting the importance of effective long-context synthesis methods.

### 6.4 Impact of Split Ratio

This section studies the impact of split_ratio, which controls the proportion of oversampled keywordbased inverted indexes. Figure 7 shows the performance changing trend with split_ratio increase. Experimental results indicate that overall performance follows a trend of initially growing and then declining with the increase in split_ratio. The proportion of oversampled indexes between $10 \%$ and $30 \%$ yields the best results in average. This shows that appropriate oversampling of the indexes with fewer documents is beneficial to long-context modeling, again illustrating the importance of balanced data distribution for long-context capabilities.
![](https://cdn.mathpix.com/cropped/2024_06_04_bac201226b3cd839d2e7g-09.jpg?height=374&width=990&top_left_y=2030&top_left_x=556)

Figure 7: The performance changing trend with split_ratio increase. For detailed results, please refer to Appendix $\mathrm{C}$

## 7 Limitations

While employing GPT-4 1] for keyword generation could potentially enhance performance due to its proficiency in handling complex tasks, the computational requirements to process large datasets make this option impractical for our purposes. Consequently, we have opted for a more resource-efficient doc2query[31] model for query prediction and the Rake algorithm for keyword extraction. However, these methods introduce biases that may affect the diversity and quality of the outputs.

## 8 Conclusion

In this paper, we introduce the Query-centric Data Synthesis (Quest) approach, a novel method for creating topic-related and manageable long-context training corpora. Our technique leverages doc2query models to generate specific queries for each document. These queries are then used to group documents with similar queries, thereby enhancing the ability of large language models (LLMs) to handle extended contexts. Quest effectively addresses the challenges of data scarcity and uneven distribution in long-context data. Extensive experiments confirm that this method significantly improves the performance of language models across various long-context benchmarks.

To the best of our knowledge, Quest is the first systematic approach that organizes data from a query-centric perspective. This paper presents preliminary evidence showing Quest's effectiveness in extending the context window for large language models. Moreover, Quest has the potential to enhance pre-training or fine-tuning, as previous work[51] on optimizing data mixtures can be applied with finer granularity in the query-centric dataset.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:2230022312, 2022.

[3] Artem Babenko and Victor Lempitsky. The inverted multi-index. IEEE transactions on pattern analysis and machine intelligence, 37(6):1247-1260, 2014.

[4] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

[5] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, VageeshD C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Sep 2023.

[6] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek 1lm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.

[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR, 2023.

[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439, 2020.

[9] Avi Caciularu, MatthewE. Peters, Jacob Goldberger, Ido Dagan, and Arman Cohan. Peek across: Improving multi-document modeling via cross-document question-answering. May 2023.

[10] Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. Clex: Continuous length extrapolation for large language models. arXiv preprint arXiv:2310.16450, 2023.

[11] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

[12] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023.

[13] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[14] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.

[15] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to $128 \mathrm{k}$ context. arXiv preprint arXiv:2402.10171, 2024.

[16] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

[17] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR, 2020.

[18] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023 .

[19] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.

[20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[21] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning, 2024.

[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

[23] Raghav Kaushik, Rajasekar Krishnamurthy, Jeffrey F Naughton, and Raghu Ramakrishnan. On the integration of structure indexes and inverted lists. In Proceedings of the 2004 ACM SIGMOD international conference on Management of data, pages 779-790, 2004.

[24] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. 2023.

[25] Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. arXiv preprint arXiv:2110.04541, 2021.

[26] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023 .

[27] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020.

[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

[29] Antonio Mallia, Omar Khattab, Torsten Suel, and Nicola Tonellotto. Learning passage impacts for inverted indexes. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1723-1727, 2021.

[30] Sahisnu Mazumder and Bing Liu. Lifelong and continual learning dialogue systems. Nov 2022.

[31] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019.

[32] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[34] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.

[35] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.

[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.

[37] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16. IEEE, 2020.

[38] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[39] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.

[40] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

[41] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. arXiv preprint arXiv:2310.10638, 2023.

[42] Hui Su, Zhi Tian, Xiaoyu Shen, and Xunliang Cai. Unraveling the mystery of scaling laws: Part i. arXiv preprint arXiv:2403.06563, 2024.

[43] Jianlin Su. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.

[44] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.

[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[47] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training for context scaling. Advances in Neural Information Processing Systems, 36, 2024.

[48] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1-26, 2024.

[49] Xing Wu, Guangyuan Ma, Wanhui Qian, Zijia Lin, and Songlin Hu. Query-as-context pretraining for dense passage retrieval. arXiv preprint arXiv:2212.09598, 2022.

[50] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.

[51] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024.

[52] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.

[53] Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei Niu, and Guiguang Ding. Temporal scaling law for large language models. arXiv preprint arXiv:2404.17785, 2024.

[54] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022.

[55] Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. arXiv preprint arXiv:2402.16617, 2024.

[56] Haofei Yu, Yue Zhang, Wei Bi, et al. Trams: Training-free memory selection for long-range language modeling. arXiv preprint arXiv:2310.15494, 2023.

[57] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

[58] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. obench: Extending long context evaluation beyond 100k tokens. arXiv preprint arXiv:2402.13718, 2024.

[59] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023.
