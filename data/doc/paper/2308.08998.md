# Reinforced Self-Training (ReST) for Language Modeling 

Caglar Gulcehre ${ }^{* \dagger, 1}$, Tom Le Paine ${ }^{* \dagger, 1}$, Srivatsan Srinivasan ${ }^{* \dagger, 1}$, Ksenia Konyushkova ${ }^{\dagger, 1}$, Lotte Weerts ${ }^{\dagger, 1}$<br>Abhishek Sharma ${ }^{\dagger, 1}$, Aditya Siddhant ${ }^{\dagger, 1}$, Alex Ahern ${ }^{1}$, Miaosen Wang ${ }^{1}$, Chenjie $\mathbf{G u}^{1}$,<br>Wolfgang Macherey ${ }^{2}$, Arnaud Doucet ${ }^{1}$, Orhan Firat ${ }^{\dagger}, 1$, Nando de Freitas ${ }^{1}$<br>"Contributed equally, ${ }^{\dagger}$ Core contributors<br>${ }^{1}$ Google DeepMind, ${ }^{2}$ Google Research


#### Abstract

Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.


Keywords: Offline RL, reinforcement learning, RL from human feedback, language, natural language processing, machine translation

## 1. Introduction

Large language models (LLMs) have demonstrated impressive abilities in generating high-quality text and in solving numerous language tasks (Brown et al., 2020; Bubeck et al., 2023; Rae et al., 2021). These models are trained to maximize the likelihood of the next token autoregressively using massive amounts of text and compute (Hoffmann et al., 2022; Srivastava et al., 2022). However, Perez et al. (2022) showed that producing text with high likelihood does not necessarily align well with human preferences on various tasks. Without proper alignment, the language models can also output unsafe contents with harmful consequences. Moreover, aligning LLMs helps to improve on other downstream tasks (Ouyang et al., 2022b). Reinforcement learning from human feedback (RLHF) aims to address the alignment problem by using human preferences (Glaese et al., 2022; Stiennon et al., 2020; Wu et al., 2021). Typically, human feedback is used to learn a reward model, which is then used to fine-tune LLM with a reinforcement learning (RL) objective.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0902b20900d4deb5aabg-01.jpg?height=474&width=534&top_left_y=1690&top_left_x=1252)

Figure $1 \mid$ ReST method. During Grow step, a policy generates a dataset. At Improve step, the filtered dataset is used to fine-tune the policy. Both steps are repeated, Improve step is repeated more frequently to amortise the dataset creation cost.

RLHF methods often rely on online RL methods such as PPO (Schulman et al., 2017) and A2C (Mnih et al., 2016). Online training requires sampling from the updated policy and scoring the samples with the reward model many times during training. The computational cost of dealing with a continual flow of new samples becomes a limitation of online methods, especially when the sizes of the policy and reward networks grow. Moreover, these methods are prone to reward "hacking" (Skalse et al., 2022), and prior works (Glaese et al., 2022) explored model regularization to mitigate this issue. Alternatively, offline RL methods learn from a fixed dataset of examples and, thus they are more computationally efficient and less prone to reward hacking. However, the quality of the policy learnt offline inevitably depends on the properties of the offline dataset (Fu et al., 2020; Gulcehre et al., 2021). As a result, carefully curated datasets become very important for the success of offline RL. Otherwise, the performance gains over supervised learning may be limited (Kumar et al., 2021). Concurrent to our work, (Rafailov et al., 2023) proposed a method called DPO (Direct Preference Optimization) that can make use of offline data to align an LM with human preferences.

We frame the alignment problem of a language model as a growing batch RL problem (Lange et al., 2012). Specifically, our Reinforced Self-Training (ReST) method includes two loops: in the inner loop (Improve), we improve the policy on a fixed dataset and in the outer loop (Grow), we grow the dataset by sampling from the latest policy (see Figure 1). In this work we consider conditional language modelling, then the steps of ReST are as follows:

1. Grow (G): The language model policy (initially, a supervised policy) is used to generate multiple output predictions for each context to augment the training dataset.
2. Improve (I): We rank and filter the augmented dataset with a scoring function. We use a learned reward model trained on human preferences as the scoring function in our experiments. Then, the language model is fine-tuned on the filtered dataset with an offline RL objective. This step can be repeated with an increasing filtering threshold. The final policy is then used in the next Grow step.

ReST is a general approach that allows different offline RL losses to be used in the inner loop when executing the Improve steps. To put it in practice, one only needs the ability to: i) sample from a model efficiently, ii) score the model's samples. ReST provides several advantages over typical RLHF methods with online or offline RL:

- The computational burden is significantly reduced compared to online RL thanks to the output of Grow step being exploited across several Improve steps.
- The quality of the policy is not restricted by the quality of the original dataset (as in offline RL) since new training data is sampled from an improved policy during the Grow step.
- It is easy to inspect the data quality and potentially diagnose alignment issues, e.g., reward hacking, as the Grow and Improve steps are decoupled.
- The approach is simple, stable and has only a small number of hyperparameters to tune.

We explain the details of our proposed ReST approach in Section 3. Then, we present our experimental results on machine translation benchmarks in Section 4. Machine translation is a sequence-to-sequence learning problem (Sutskever et al., 2014), which is usually formulated as conditional language modelling where the context for conditioning is a sentence in a foreign language (source). We chose machine translation because i) it is an impactful application with strong baselines and a well-defined evaluation procedure, ii) several existing reliable scoring and evaluation methods are available for the use as a reward model (Freitag et al., 2022). In our experiments, we compare several offline RL algorithms on the IWSLT 2014 (Cettolo et al., 2014) and WMT 2020 benchmarks (Koehn et al., 2020) as well as more competitive, high-fidelity internal benchmarks on Web Domain. In our experiments ReST significantly improves reward model scores on test and validation sets. Furthermore, according to human raters, ReST generates higher quality translations compared to a supervised learning baseline.

## 2. Preliminaries

A conditional language model produces an output sequence $\boldsymbol{y}=\left(y_{1}, y_{2}, \ldots . y_{T}\right)$ given a context (or source input) $x=\left(x_{1}, x_{2}, \ldots x_{L}\right)$, where the tokens $x_{l}, y_{t}$ belong to a chosen vocabulary. A language generation policy $\pi$ in an auto-regressive model characterized by a conditional probability distribution parameterized by $\theta$ as

$$
\pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})=\prod_{t=1}^{T} \pi_{\theta}\left(y_{t} \mid \boldsymbol{y}_{1: t-1}, \boldsymbol{x}\right)
$$

with the convention $\boldsymbol{y}_{1: 0}=\emptyset$ and $\boldsymbol{y}_{1: t-1}=\left(y_{1}, y_{2}, \ldots . y_{t-1}\right)$.

Let $p(\boldsymbol{x}, \boldsymbol{y})=p(\boldsymbol{x}) p(\boldsymbol{y} \mid \boldsymbol{x})$ denote the data distribution. A given dataset $\mathcal{D}$ consists of samples from this distribution:

$$
\mathcal{D}=\left\{\left.\left(x^{i}, y^{i}\right)\right|_{i=1} ^{N} \text { such that } x^{i} \sim p(x), y^{i} \sim p\left(y \mid x=x^{i}\right)\right\}
$$

Given this dataset, the supervised policy is trained by minimizing the negative log likelihood (NLL) loss:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{NLL}}(\theta)=-\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\sum_{t=1}^{T} \log \pi_{\theta}\left(y_{t} \mid y_{1: t-1}, x\right)\right] \tag{1}
\end{equation*}
$$

We refer to the model that is trained with the NLL loss as behavioral cloning (BC) (Pomerleau, 1989) following the RL literature nomenclature.

## 3. Reinforced Self-Training (ReST)

We present ReST, an RLHF algorithm that aligns the language model's outputs with human preferences. Human preferences over sequences are modelled using a learned reward function (see Appendix A.4). In the underlying Markov decision process for conditional language modelling the states are the partial sequences, and the actions are the generated tokens (see Appendix A.1).

The ReST algorithm decouples the dataset growth and policy improvement of a typical RL pipeline into separate offline stages (Figure 1 and 2). We start by training an initial model $\pi_{\theta}(y \mid x)$ to map input sequences $x$ to output sequences $y$ on a given dataset of sequence pairs $\mathcal{D}$ using the NLL loss from Equation (1). Next, the Grow step creates a new dataset $\mathcal{D}_{g}$, which augments the initial training dataset with samples from the model:

$$
\mathcal{D}_{g}=\left\{\left.\left(x^{i}, y^{i}\right)\right|_{i=1} ^{N_{g}} \text { such that } x^{i} \sim \mathcal{D}, y^{i} \sim \pi_{\theta}\left(y \mid x^{i}\right)\right\} \cup \mathcal{D} \text {. }
$$

Here, the conditioning inputs are resampled from the original dataset $x^{i} \sim \mathcal{D}$, as in self-training, but in situations where one has access to $p(\boldsymbol{x})$ they could sample directly from it, i.e., $\boldsymbol{x}^{i} \sim p(\boldsymbol{x})$. For example, consider a model that generates image from a textual description, in this case, the distribution of text inputs can be sampled from a language model $p(x)$.

Subsequently, the Improve steps use $\mathcal{D}_{g}$ to fine-tune the policy $\pi_{\theta}$. Note that we keep the original dataset in the training mixture to ensure that the policies do not diverge. Below, we describe Grow and Improve steps in more details.

Grow

The Grow step corresponds to the acting or data-generation step in RL. We create an augmented dataset of trajectories $\mathcal{D}_{g}$ by sampling many output sequences from the current policy $\pi_{\theta}$, i.e., $y \sim \pi_{\theta}(y \mid x)$ for $x \sim \mathcal{D}$. The new dataset of sequences is then scored with a reward function $R(\boldsymbol{x}, \boldsymbol{y})$. The datapoints with the reward above a threshold score are used to update the policy (see next). Once the policy is improved, a new dataset of better quality samples can be created once again (Figure 2, bottom).

## Improve

At the Improve step (exploitation or policy improvement in RL terminology), the goal is to use the new dataset $\mathcal{D}_{g}$ to fine-tune the policy $\pi_{\theta}$. We start by defining a filtering function that includes only samples with rewards higher than a certain threshold $\tau$ :

$$
F(\boldsymbol{x}, \boldsymbol{y} ; \tau)=\mathbb{1}_{R(x, y)>\tau}
$$

Let us note that the threshold based filtering function may result into learning suboptimal behaviors that favors outcomes with high variance in the environments with stochastic dynamics (Brandfonbrener et al., 2022). However, in this work we formulate the language modeling and translation tasks as deterministic RL problems (Appendix A.1.)

Next, we finetune the current best policy typically trained with either the supervised learning loss $\mathcal{L}_{\mathrm{NLL}}$ from equation 1 or an offline RL loss $\mathcal{L}(x, y ; \theta)$ on the filtered data such as V-MPO (Song et al., 2020) or offline actor-critic (Mathieu et al., 2021). To sum up, we use the following reward weighted loss $J$ :

$$
\begin{equation*}
J(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}_{g}}[F(\boldsymbol{x}, \boldsymbol{y} ; \tau) \mathcal{L}(\boldsymbol{x}, \boldsymbol{y} ; \theta)] \tag{2}
\end{equation*}
$$

Standard imitation learning approaches, such as BC (Pomerleau (1989), equation 1) and onestep RL methods like Behavior Value Estimation (BVE) (Gulcehre et al., 2021) perform one-step of Improve on the fixed dataset $\mathcal{D}$. In contrast, the basic version of ReST additionally includes a Grow step that allows the model to gather multiple new output sequences (potential translations) for contexts $x$ from the original dataset (source sentences to translate).

When iterating over Improve steps, we increase the filtering thresholds: $\tau_{1}<\cdots<\tau_{N-1}<\tau_{N}$ (Figure 2). This filtering with the growing threshold results in data subsets of increasing quality but of decreasing size. As LLMs overfit to small datasets quickly, we fine-tune every new policy from the previous policy with a lower learning rate. Consecutive fine-tuning of policies $\left\{\pi_{\theta_{k}}\right\}_{k \geq 1}$ on higher quality data subsets ensures policy improvement with a fixed dataset $\mathcal{D}_{g}$. If we were to sample from policies $\left\{\pi_{\theta_{k}}\right\}_{k \geq 1}$, the average reward of the generated samples would be increasing (shown in grey in Figure 2). As sampling from a policy in the Grow step is computationally expensive, after each such step we perform several Improve steps. Thus, the cost of a single dataset generation is amortised over multiple Improve steps. Algorithm 1 outlines the full ReST algorithm with multiple dataset growth and policy improvement steps.

```
Algorithm 1: ReST algorithm. ReST is a growing-batch RL algorithm. Given an initial
policy of reasonable quality (for example, pre-trained using BC) iteratively applies Grow and
Improve steps to update the policy. Here $F$ is a filtering function, and $\mathcal{L}$ is an loss function.
    Input: $\mathcal{D}$ : Dataset, $\mathcal{D}_{\text {eval }}$ : Evaluation dataset, $\mathcal{L}(\boldsymbol{x}, \boldsymbol{y} ; \theta)$ : loss, $R(\boldsymbol{x}, \boldsymbol{y})$ : reward model, $G$ :
            number of grow steps, $I$ : number of improve steps, $N$ : number of samples per context
    Train $\pi_{\theta}$ on $\mathcal{D}$ using loss $\mathcal{L}$.
    for $g=1$ to $G$ do
        // Grow
        Generate dataset $\mathcal{D}_{g}$ by sampling: $\mathcal{D}_{g}=\left\{\left.\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right)\right|_{i=1} ^{N_{g}}\right.$ s.t. $\left.\boldsymbol{x}^{i} \sim \mathcal{D}, \boldsymbol{y}^{i} \sim \pi_{\theta}\left(\boldsymbol{y} \mid \boldsymbol{x}^{i}\right)\right\} \cup \mathcal{D}$.
        Annotate $\mathcal{D}_{g}$ with the reward model $R(\boldsymbol{x}, \boldsymbol{y})$.
        for $i=1$ to $I$ do
            // Improve
            Choose threshold s.t. $\tau_{1}>V_{\pi_{\theta}}$ for $V_{\pi_{\theta}}=\mathbb{E}_{\mathcal{D}_{g}}[R(\boldsymbol{x}, \boldsymbol{y})]$ and $\tau_{i+1}>\tau_{i}$.
            while reward improves on $\mathcal{D}_{\text {eval }}$ do
                Optimise $\theta$ on objective: $J(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}_{g}}\left[F\left(\boldsymbol{x}, \boldsymbol{y} ; \tau_{i}\right) \mathcal{L}(\boldsymbol{x}, \boldsymbol{y} ; \theta)\right]$
            end
        end
    end
    Output: Policy $\pi_{\theta}$
```

Probabilistic interpretation of the Improve step Let us consider the particular choice $\mathcal{L}=\mathcal{L}_{\text {NLL }}$, with $\theta^{\prime}$ being the parameters of the model from the last Grow step, $\lambda$ the proportion of data sampled from this model in $\mathcal{D}_{g}$ and a single step of growth. The expression for the gradient in this case takes the following form:

$\nabla J(\theta)=-\mathbb{E}_{x \sim \mathcal{D}}\left[\lambda \mathbb{E}_{\boldsymbol{y} \sim \pi_{\theta^{\prime}}(y \mid x)}\left[F(x, y ; \tau) \nabla \log \pi_{\theta}(y \mid x)\right]+(1-\lambda) \mathbb{E}_{y \sim p(y \mid x)}\left[F(x, y ; \tau) \nabla \log \pi_{\theta}(y \mid x)\right]\right]$.

The first term on the RHS of (3) is similar to an online policy gradient term at the beginning of training when $\theta \approx \theta^{\prime}$ with $F(x, y ; \tau)$ replacing the state-action value function $Q^{\pi}(x, y)$, when starting in state $x$ and taking sequential actions $y$, that is generating synthetic data $y$ using policy $\pi_{\theta}$ in our context. For the second term on the RHS of (3), we consider the original data $\mathcal{D}$, but we still ensure that it passes the threshold $F(x, y ; \tau)$. Intuitively, people choose $\mathcal{D}$ for training according to some possibly unknown criteria. In this work, we make the criterion $F(\boldsymbol{x}, \boldsymbol{y} ; \tau)$ explicit. The last term is therefore a form of offline policy gradients which prevents $\pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})$ to move too far from $p(\boldsymbol{y} \mid \boldsymbol{x})$ which could lead to model collapse (Shumailov et al., 2023). Finally, note the similarity of this approach with self-training (Clark et al., 2003; Scudder, 1965; Xie et al., 2020) techniques. We provide a population interpretation (i.e., as $N, N_{g} \rightarrow \infty$ ) of ReST in Appendix A.9.

In the following section, we explore how the choice of loss, filtering function and threshold, and synthetic data generated by language policy via sampling (exploration data) empirically affect the performance of the resulting policies $\pi_{\theta}$.

## 4. Experiments and analysis

We chose machine translation as a testbed for ReST as it is an impactful application of conditional language modeling where established reward models are available, for example, Metric X (Freitag et al., 2022), BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020). We ran experiments on two common benchmarks: IWSLT 2014 (Cettolo et al., 2014), and WMT 2020 (Koehn et al., 2020),

![](https://cdn.mathpix.com/cropped/2024_06_04_d0902b20900d4deb5aabg-06.jpg?height=454&width=1650&top_left_y=276&top_left_x=203)

Figure 3 | ReST with multiple Improve steps. Average reward model scores on IWSLT 2014 De-En, WMT 2020 Zh-En, and Web Domain En-Zh validation sets. On each dataset, we report results with BC ( $G=0, I=0$ ) and ReST with a single Grow step and several Improve steps with an increasing reward threshold. Each Improve step increases the reward model score in all three validation datasets. We found the suitable number of Improve steps to be a dataset-dependent hyperparameter.

as well as an internal benchmark dataset which we call Web Domain (a version of this dataset was previously used by Ghorbani et al. (2021)). These datasets contain a set of sentences in the source language and the corresponding human "reference" translation. We selected a different language pair for each dataset to test the generality of the results. We kept a separate validation and test sets with unseen source sentences for the evaluation purposes.

We used Metric X in our experiments, a state-of-art reference-free reward model (Freitag et al., 2022) which, for a given source text and a proposed translation, outputs a numerical score. We report results in terms of average rewards on samples generated by a policy on the validation set ${ }^{1}$. For the details of the datasets and models, we refer to Appendix A.3. Also, Table 2 indicates the size of the datasets by reporting the number of samples per source sentence generated at each Grow step.

Nomenclature We named variants of ReST by the loss type, number of Grow steps, and number of Improve steps, for example GOLD $\mathrm{G}=1 \quad \mathrm{I}=2$. With this convention, $\mathrm{BC} \quad \mathrm{G}=0 \quad \mathrm{I}=0$ refers to standard supervised learning, which is trained only on the original dataset $\mathcal{D}$ and performs neither Grow nor Improve steps. When the loss type is not specified, the BC loss is used, i.e., the model is trained with auto-regressive supervised learning with the NLL loss as typical in training language models. In all plots, we colored supervised learning in grey and ReST variants in shades of purple.

Baselines We reported the results with several different offline RL method, including Offline Actor Critic (OAC) (Mathieu et al., 2021), Behavior VMPO (BVMPO), Generation by Off-policy Learning from Demonstrations (GOLD) (Pang and He, 2021), and BC (Pomerleau, 1989) ${ }^{2}$.

Do multiple Improve steps in ReST increase the reward model scores? We evaluated ReST on three different datasets by fixing the loss function to BC and increasing the number of Improve steps. The range of rewards for training was normalized between 0 and $1^{3}$. For our experiments, we[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_d0902b20900d4deb5aabg-07.jpg?height=466&width=1100&top_left_y=278&top_left_x=475)

Figure $4 \mid$ ReST with two Grow steps. The second Grow step with subsequent Improve steps improves the performance by 5.3 points on IWSLT 2014 De-En and 0.8 points on Web Domain En-Zh task over the first Grow step.

picked the filtering thresholds $\tau_{i}$ from a sequence of increasing values $[0.0,0.7,0.8,0.9,0.95,0.99$ ] 4. The $\tau_{0}=0.0$ case corresponds to using the full dataset. We did five Improve steps on IWSLT 2014, four on WMT-2020, and two on Web Domain. In Figure 3 we plotted the average reward of different variants of ReST. We see that each subsequent Improve step improves the performance of the translation model significantly across all three datasets.

## Do additional Grow steps improve reward model

 scores? We performed a second Grow step with successive Improve steps to measure the effect of the extra Grow step on the performance. In Figure 4, a method with an additional Grow step achieves further improvement on the IWSLT 2014 and Web Domain datasets. We noticed a 5.3 point improvement between the end of the first and the second Grow step.Does ReST improve over supervised training? To answer this question, in Figure 5 we plotted the average reward achieved by the supervised learning model as well as several variants of ReST with different losses and the number of Grow and Improve steps. Different variants of ReST (purple) significantly outperform supervised learning (gray) even after just the first grow step. This observation was consistent across different datasets and language pairs that we tested.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0902b20900d4deb5aabg-07.jpg?height=434&width=691&top_left_y=1296&top_left_x=1139)

Figure 5| WMT 2020 zh-en (test): BC (in grey, $G=0 I=0$ ) and ReST trained with different offline RL losses. ReST is trained with one Grow and Improve step except $G=$ $1 I=0$, which is trained on the entire dataset generated after the first Grow step without any Improve (all in purple). All variants of ReST outperform the initial BC baseline, with $\mathrm{BC}$ loss resulting in the best performance.

Which loss is the best for a single step of ReST? Figure 5 depicts variants of ReST with different offline RL losses $\mathcal{L}(x, y ; \theta)$. We find that BC loss outperforms other loss functions. Note that normally $\mathrm{BC}$ algorithm does not depend on the reward, but in ReST, the reward is taken into account through the reward filtering stage for $I \geq 1$ (with $\tau_{1}=0.8$ for WMT 2020.) Results with multiple Grow and Improve steps are displayed in Figure 4 (see also Appendix A.6).[^1]

| Algorithm | Average Reward | Distinct samples |
| :---: | :---: | :---: |
| $\mathrm{BC}(\mathrm{G}=0, \mathrm{I}=0)$ | 70.9 | 16000000 |
| $\operatorname{ReST}(\mathrm{G}=1, \mathrm{I}=0)$ | 71.9 | 16000000 |
| $\operatorname{ReST}(\mathrm{G}=1, \mathrm{I}=4)$ | 77.8 | 16000000 |
| $\operatorname{ReST}(\mathrm{G}=\mathbf{2}, \mathrm{I}=\mathbf{3})$ | $\mathbf{8 3 . 1}$ | 32000000 |
| Online $\mathrm{RL}$ | 71.6 | 24000000 |

Table 1 | Online RL for IWSLT 2014: Online RL performs as well as $\operatorname{ReST}(\mathrm{G}=1, \mathrm{I}=0)$ and $\operatorname{ReST}$ $(\mathrm{G}=1, \mathrm{I}=4)$ is significantly better.

Can ReST be improved further with Best-of-N sampling at inference time? Best-of-N sampling technique at inference time generates $N$ samples which are then ranked by the reward model. Then, the top ranked candidate is selected (Gao et al., 2022). We show results with Best-of-N sampling on top of $\mathrm{BC}(\mathrm{G}=0 \quad \mathrm{I}=0)$ and $\operatorname{ReST}$ variants in Figure 6. The performance of ReST improves both with $N$ and with the number of Improve steps. The best ReST variant with $N<10$ matches the performance of the BC model with $N=200$. Even though RL is known to limit the diversity of samples, this experiment shows that ReST can still benefit from Best-of-N sampling. After three Improve steps with $N=200$, ReST achieves the highest possible reward of 1 , outperforming the "reference" translations in $\mathcal{D}$.

How does ReST compare with Online RL? We compared ReST with PPO (Schulman et al., 2017), an online RL algorithm widely used for RLHF (Glaese et al., 2022; Ouyang et al., 2022a). For our online RL experiments, we used the setup of Donato et al. (2022) where PPO had access to a similar amount of training data as ReST with 1 Grow step. The results are summarized in Table 1. Online RL performs as well as ReST with one Grow and no Improve steps which is equivalent to $\mathrm{BC}$ on the $\mathcal{D}_{g}$ dataset. With the same amount of training data, ReST with multiple Improve steps achieves significantly higher rewards. Furthermore, we noticed that the BLEU score for the online RL policy on the validation set dropped by nearly 8 points (BLEU score of ReST did not change) which indicates a potential reward hacking behaviour. ReST's ability to improve the reward model score without deteriorating the performance on other metrics suggests that the "alignment tax" it pays is lower than for online RL

![](https://cdn.mathpix.com/cropped/2024_06_04_d0902b20900d4deb5aabg-08.jpg?height=477&width=580&top_left_y=1326&top_left_x=1229)

Figure 6 | Best-of-N sampling at inference time. All variants of ReST benefit as much from Best-of-N sampling as supervised models. approaches.

Does ReST improve human preferences? We evaluated the ReST models by human raters to investigate if ReST can outperform BC in human evaluations as well. We displayed a sentence in the source language and two generated translations: one by $\mathrm{BC}$ model ( $\mathrm{G}=0 \quad \mathrm{I}=0$ ) and one by a ReST variant. Human raters scored each translation on a scale from 0 to 6 , and we measured the difference between the average score of ReST method and of BC which we refer as "Human eval diff". In Figure 7 (right), we see that all variants of ReST outperform the BC baseline significantly. However, if we compare the human score gains with the gains in the learned reward (Figure 7, left), the rankings do not match. We hypothesise that the difference is due to the fact that the reward
models cannot generalize well on OOD data since the learned reward models are an imperfect proxy of human preferences. In particular, we found that the reward models generalise worse as our policy moves away from the behaviour model which can happen as the number of Grow and Improve steps increases at which point ReST can start overfitting to the reward model. Thus, in our analysis, we focused on evaluating models based on how well they align with a reward signal and we treat reward model generalisation as an independent issue that could be mitigated by, for example, finetuning the reward model between the consecutive Grow steps on the human-annotated data from the most recent policy. ReST with the B-VMPO loss utilises a learned value function and $\mathrm{KL}$ regularisation term to prevent over-fitting and thus attains high human preference scores.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0902b20900d4deb5aabg-09.jpg?height=554&width=1196&top_left_y=748&top_left_x=430)

Figure 7 | Comparison of performance based on learned reward and on human evaluation. All ReST variants outperform BC in terms of human ratings, but the rankings of the methods based on reward model scores and human scores are different.

## 5. Related works

There has been large number of works recently on self-improving alignment algorithms for language modelling. In Figure 8, we also compare ReST against different approaches: supervised learning, self-training, online and offline RL. We conclude from this comparison that ReST is the only approach that is compute efficient, but also can leverage exploration data and rewards. Next, we describe some of the particular works related to ReST .

|  | Exploration | Rewards | Compute <br> Efficient |
| :---: | :---: | :---: | :---: |
| Supervised <br> Learning | $x$ | $x$ | $v$ |
| Self-training | $\checkmark$ | $x$ | $\checkmark$ |
| Online RL | $\checkmark$ | $\checkmark$ | $x$ |
| Offline $R L$ | $x$ | $\checkmark$ | $v$ |
| $\operatorname{ReST}$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |

Figure $8 \mid \operatorname{ReST}$ vs alternatives: $\operatorname{ReST}$ is the only approach that can leverage the exploration data and rewards, but is also computationally efficient.

Self-training Self-training is an established semi-supervised learning approach which utilizes unlabeled data to improve a model (Scudder, 1965). Since its introduction, self-training has been successfully applied to many tasks, including image classification and recognition (Xie et al., 2020), protein folding (Jumper et al., 2021), as well as several language tasks (He et al., 2019; Sun et al., 2021; Yarowsky, 1995; Zhang and Zong, 2016). He et al. (2019) empirically demonstrated that noisy self-training improves the performance of translation models. The Improve step of ReST resembles self-training. ReST's main difference from self-training is that the Grow step of ReST generates synthetic exploration data for training with RL.

Expert Iteration (EI) Anthony (2021) proposed an RL framework which is a form of policy iteration approach that makes use of a planning mechanism. EI explicitly decomposes the RL problem into two parts: planning and generalisation. Similar to ReST, EI uses the policy to generate data and exploit it to learn a policy with RL. Unlike EI, ReST does not require any planning mechanism and it makes use of iterative Improve steps that enables it to leverage the gathered data more effectively.

Reasoning with language models The EI method inspired several related approaches (Uesato et al., 2022; Zelikman et al., 2022). Zelikman et al. (2022) proposed a technique called STAR that iteratively leverages a small number of rationales to fine-tune the model. Then, they sample rationales with the answers from the model and filter the generated answers by their correctness. Uesato et al. (2022) proposed a similar method which learns to solve math problems using learned reward models. Recently, Jung et al. (2023) proposed a method called "Impossible Distillation" similar to ours, which generates datasets for large language models by sampling from a suboptimal model and filters the low-quality examples with a filtering mechanism. This approach corresponds to ReST with a single Grow and Improve steps. In contrast to these methods, ReST can be trained with any offline RL losses, with or without planning and various filtering mechanisms. It can also deal with continuous-valued reward scores. Furthermore, ReST can perform iterative policy improvement on a fixed dataset with the Improve steps.

Iterated Learning (IL) IL is the process where an agent learns its behavior by being exposed to another agent's behavior, which itself learned it in the same way (Kirby et al., 2014). Recently, this approach was adopted for interactive language learning using deep learning (Lu et al., 2020a,b). IL differs from ReST as it operates in a multi-agent setting and does not use RL.

Self Imitation Learning (SIL) SIL learns a policy for an off-policy actor-critic algorithm where the policy tries to reproduce the good behaviour demonstrated by the agent (Oh et al., 2018). SIL achieves it by filtering out the unsuccessful trajectories from the replay buffer and training the agent only on the high-reward trajectories. In that sense, ReST can be considered to be closely related to the SIL-based approaches. The main difference is that ReST is agnostic to the underlying RL algorithm used to train the policy and, unlike SIL, does not necessitate a value function to filter out the unsuccessful trajectories. Also, ReST is applied to generative AI settings, which do not require interactions with an environment in an online fashion.

Reward ranked Fine-Tuning (RAFT) Concurrently to our work, Dong et al. (2023) proposed RAFT. RAFT can be interpreted as a particular case of ReST which uses only one Improve step for each Grow step, and relies on a filtering threshold which is a fixed quantile of the empirical distribution of the rewards of the current samples. The authors reported improvements over BC and PPO on a variety of language modeling and image generation tasks. Our experiments with ReST showed that multiple Improve steps with an increasing filtering threshold for one Grow step lead to further performance improvements.

## 6. Discussion

In this paper, we proposed an algorithm called ReST which is simple, has minimal hyper-parameters to tune, and is flexible to work with many designs of Grow and Improve steps. We studied the performance of ReST in machine translation as robust and established reward models are available for
this task. We experimented with different offline RL losses in the ReST loop, but found BC to perform the best for improving the reward model scores. Multiple steps of NLL training with progressively increasing filtering thresholds in the Improve step lead to continuous improvements in the model's reward on the holdout set. However, improvements in reward model scores do not necessarily reflect human preferences since the reward model is only a proxy for human preferences. The results indicate that one Grow step is the best option when considering human evaluation scores, even though rewards continue to grow with more Grow steps. To overcome this limitation, the reward models could be fine-tuned on the subset of $\mathrm{D}_{g}$ annotated with human preferences similar to Bai et al. (2022) and Glaese et al. (2022), which we leave as future work. Let us note that the risk of overfitting to the reward model increases with the repeated iterations of the Grow steps; thus we believe it is essential to address this issue, especially in cases where multiple Grow steps are needed to train the model.

As we have seen, simple BC loss still outperforms many offline RL losses in terms of aligning the model with the reward model scores. However, we found that BC can overfit to the reward model so we explain it by the fact that learning value functions in RL is challenging due to sparse rewards, credit assignment problems, sensitivity to hyperparameters, and limited exploration in the Grow step. ReST could benefit from better RL exploration strategies at Grow step, such as MCTS (Leblond et al., 2021). The ability to exploit the generated data during the Grow step could result in a broader exploration of the state-action space and better generalization. Additionally, the determinism of the environment does not allow for large gains over BC for offline RL losses.

To conclude, ReST is a general and efficient approach. It can be applied when 1) a robust reward model of human preferences is available and 2) we are able to generate samples from the model at scale. Thus, it can be applied to many tasks within the language domain, such as summarization, turn-based dialogue, and other generative audio and video models. With several avenues for future exploration and applications, we believe that ReST is a useful growing batch RL methodology for RLHF.

Acknowledgements We would like to thank the members from the machine translation teams at Google and Google DeepMind for their inputs to the project during the brainstorming phases and for setting up the codebase that this project was developed upon (Yu et al., 2020). We would like to thank Matt Hoffman, Bobak Shahriari, Taylan Cemgil and Chris Dyer for the discussions about this project. We are grateful for the feedback provided by Bilal Piot for an early draft of this paper. We would also like to thank those responsible for various different frameworks that we used during the project such as the DeepMind JAX ecosystem (Babuschkin et al., 2020) and Launchpad (Yang et al., 2021).

## References

A. Abdolmaleki, S. Huang, G. Vezzani, B. Shahriari, J. T. Springenberg, S. Mishra, D. Tirumala, A. Byravan, K. Bousmalis, A. György, et al. On multi-objective policy optimization as a tool for reinforcement learning. arXiv preprint arXiv:2106.08199, 2021.

R. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. G. Bellemare. Beyond tabula rasa: Reincarnating reinforcement learning. arXiv preprint arXiv:2206.01626, 2022.

T. W. Anthony. Expert iteration. PhD thesis, UCL (University College London), 2021.

I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark, I. Danihelka, C. Fantacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou,

S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik, T. Norman, J. Quan, G. Papamakarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener, S. Spencer, S. Srinivasan, L. Wang, W. Stokowiec, and F. Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind.

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

R. Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, 6(5):679-684, 1957.

D. Brandfonbrener, A. Bietti, J. Buckman, R. Laroche, and J. Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? Advances in Neural Information Processing Systems, 35:1542-1553, 2022.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020.

S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.

M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and M. Federico. Report on the 11th iwslt evaluation campaign. In Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, 2014.

L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, 2021.

S. Clark, J. R. Curran, and M. Osborne. Bootstrapping pos-taggers using unlabelled data. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003, pages 49-55, 2003.

D. Donato, L. Yu, W. Ling, and C. Dyer. Mad for robust reinforcement learning in machine translation. arXiv preprint arXiv:2207.08583, 2022.

H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.

L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, 2018.

M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, G. Foster, A. Lavie, and O. Bojar. Results of the wmt21 metrics shared task: Evaluating metrics with expert-based human evaluations on ted and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, 2021.

M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, E. Avramidis, T. Kocmi, G. Foster, A. Lavie, and A. F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.2.

J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022.

B. Ghorbani, O. Firat, M. Freitag, A. Bapna, M. Krikun, X. Garcia, C. Chelba, and C. Cherry. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740, 2021.

A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.

C. Gulcehre, S. G. Colmenarejo, Z. Wang, J. Sygnowski, T. Paine, K. Zolna, Y. Chen, M. Hoffman, R. Pascanu, and N. de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021.

J. He, J. Gu, J. Shen, and M. Ranzato. Revisiting self-training for neural sequence generation. arXiv preprint arXiv:1909.13788, 2019.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems, 2022.

J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.

J. Jung, P. West, L. Jiang, F. Brahman, X. Lu, J. Fisher, T. Sorensen, and Y. Choi. Impossible distillation: from low-quality model to high-quality dataset \& model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635, 2023.

S. Kirby, T. Griffiths, and K. Smith. Iterated learning and the evolution of language. Current Opinion in Neurobiology, 28:108-114, 2014.

P. Koehn, V. Chaudhary, A. El-Kishky, N. Goyal, P.-J. Chen, and F. Guzmán. Findings of the wmt 2020 shared task on parallel corpus filtering and alignment. In Proceedings of the Fifth Conference on Machine Translation, pages 726-742, 2020.

T. Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 66-75, 2018.

A. Kumar, J. Hong, A. Singh, and S. Levine. Should i run offline reinforcement learning or behavioral cloning? In International Conference on Learning Representations, 2021.

S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In M. Wiering and M. van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 45-73. Springer Berlin Heidelberg, 2012.

R. Leblond, J.-B. Alayrac, L. Sifre, M. Pislar, L. Jean-Baptiste, I. Antonoglou, K. Simonyan, and O. Vinyals. Machine translation decoding beyond beam search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2021.

Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022.

Y. Lu, S. Singhal, F. Strub, A. Courville, and O. Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning, 2020a.

Y. Lu, S. Singhal, F. Strub, O. Pietquin, and A. Courville. Supervised seeded iterated learning for interactive language learning. arXiv preprint arXiv:2010.02975, 2020b.

M. Mathieu, S. Ozair, S. Srinivasan, C. Gulcehre, S. Zhang, R. Jiang, T. Le Paine, K. Zolna, R. Powell, J. Schrittwieser, et al. Starcraft ii unplugged: Large scale offline reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.

V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Learning Representations, 2016.

J. Oh, Y. Guo, S. Singh, and H. Lee. Self-imitation learning. In International Conference on Machine Learning, pages 3878-3887. PMLR, 2018.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022a.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022b.

R. Y. Pang and H. He. Text generation by learning from demonstrations. In International Conference on Learning Representations, 2021.

E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.

D. A. Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems, pages 305-313, 1989.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis \& insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

R. Rei, C. Stewart, A. C. Farinha, and A. Lavie. COMET: A neural framework for MT evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2020.

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11(3):363-371, 1965.

T. Sellam, D. Das, and A. Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.

I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arxiv:2305.17493, 2023.

J. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing reward hacking. In Advances in Neural Information Processing Systems, 2022.

H. F. Song, A. Abdolmaleki, J. T. Springenberg, A. Clark, H. Soyer, J. W. Rae, S. Noury, A. Ahuja, S. Liu, D. Tirumala, N. Heess, D. Belov, M. Riedmiller, and M. M. Botvinick. V-mpo: On-policy maximum a posteriori policy optimization for discrete and continuous control. In International Conference of Learning Representations, 2020.

A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.

N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 2020.

H. Sun, R. Wang, K. Chen, M. Utiyama, E. Sumita, and T. Zhao. Self-training for unsupervised neural machine translation in unbalanced training data scenarios. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.

I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, 2014.

J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.

J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.

Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10687-10698, 2020.

F. Yang, G. Barth-Maron, P. Stańczyk, M. Hoffman, S. Liu, M. Kroiss, A. Pope, and A. Rrustemi. Launchpad: A programming model for distributed machine learning research. arXiv preprint arXiv:2106.04516, 2021.

D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, 1995.

L. Yu, L. Sartran, P.-S. Huang, W. Stokoweic, D. Donato, S. Srinivasan, A. Andreev, W. Ling, S. Mokra, A. D. Lago, Y. Doron, S. Young, P. Blunsom, and C. Dyer. The DeepMind chinese-english document translation system at wmt2020. In Proceedings of the Fifth Conference on Machine Translation, 2020.

E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, 2022.

J. Zhang and C. Zong. Exploiting source-side monolingual data in neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2016.
