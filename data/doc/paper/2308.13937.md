# A Comparative Study on Reward Models for UI Adaptation with Reinforcement Learning 

Daniel Gaspar-Figueiredo<br>ITI \& Universitat Politècnica de València<br>Valencia, Spain<br>Email: dagasfi@epsa.upv.es<br>Marta Fernández-Diego<br>Universitat Politècnica de València<br>Valencia, Spain<br>Email: marferdi@omp.upv.es

Silvia Abrahão<br>Universitat Politècnica de València<br>Valencia, Spain<br>Email: sabrahao@dsic.upv.es<br>Emilio Insfran<br>Universitat Politècnica de València<br>Valencia, Spain<br>Email: einsfran@ dsic.upv.es


#### Abstract

Background: Adapting the User Interface (UI) of software systems to user requirements and the context of use is challenging. The main difficulty consists of suggesting the right adaptation at the right time in the right place in order to make it valuable for end-users. We believe that recent progress in Machine Learning techniques provides useful ways in which to support adaptation more effectively. In particular, Reinforcement learning (RL) can be used to personalise interfaces for each context of use in order to improve the user experience (UX). However, determining the reward of each adaptation alternative is a challenge in $R L$ for UI adaptation. Recent research has explored the use of reward models to address this challenge, but there is currently no empirical evidence on this type of model. Objective: In this paper, we propose a confirmatory study design that aims to investigate the effectiveness of two different approaches for the generation of reward models in the context of UI adaptation using RL: (1) by employing a reward model derived exclusively from predictive Human-Computer Interaction (HCI) models (HCI), and (2) by employing predictive HCI models augmented by Human Feedback (HCI\&HF). Method: The controlled experiment will use an AB/BA crossover design with two treatments: HCI and HCI\&HF. We shall determine how the manipulation of these two treatments will affect the UX when interacting with adaptive user interfaces (AUI). The UX will be measured in terms of user engagement and user satisfaction, which will be operationalized by means of predictive HCI models and the Questionnaire for User Interaction Satisfaction (QUIS), respectively. By comparing the performance of two reward models in terms of their ability to adapt to user preferences with the purpose of improving the $U X$ (i.e. increasing user engagement, improving user satisfaction), our study contributes to the understanding of how reward modelling can facilitate UI adaptation using RL.


Index Terms-User Interface Adaptation, Reinforcement Learning, Reward Modelling, Human Feedback, Experiment

## I. INTRODUCTION

Adaptive systems and adaptive user interfaces (AUI) have become increasingly important in modern software applications. They have been introduced in order to address some of the usability problems in many software applications [2]. These systems can change aspects of their structure, functionality, or interface to accommodate the differing needs of individuals or groups of users and the changing needs of users over time [3]. However, suggesting the right adaptation at the right time in the right place in order to make it valuable for end-users is challenging. In previous work, we proposed a framework for intelligent UI adaptation [4]. This framework proposes the use of Machine Learning algorithms in order to provide valuable UI adaptations. Several approaches have been proven successful in simple adaptation problems, such as recommendations and the calibration of interface parameters [5], [6]. However, Reinforcement Learning (RL) is more appropriate as regards learning policies for sequences of adaptations in which rewards are not immediately achievable. Indeed, the UI adaptation problem can be defined as a stochastic sequential decision problem, where the adaptive system should plan a sequence of adaptations over a long horizon [7]. RL involves training an agent to make decisions based on rewards received from the environment for certain actions [8], [9]. In UI adaptation, the agent will make decisions regarding which adaptations to apply by considering the user's interaction behaviour. The goal of UI adaptation is to personalize the UI for each context of use so as to improve the user experience (UX).

In this context, one challenge of RL for UI adaptation is determining the reward of each adaptation. Traditional RL uses numerical rewards, which are difficult to define in scenarios in which the outcome of the action or the agent's goal is complex.

Recent research has, therefore, explored ordinal rewards as an alternative [10], which induce scale-invariance and reduce manual reward engineering [11]. Another approach is to make the reward function part of the learning process. In complex environments, the use of reward models can be very profitable. Reward models are a key component of RL that are used to specify the user's goals and the system [12]. During the process of learning the goal, putting the human into the loop may be particularly powerful. Human Feedback (HF) could be used as regards specifying the goal more intuitively and quickly when compared to manual objective hand-crafted methods [13], [14].

## Model-Based RL with MCTS

![](https://cdn.mathpix.com/cropped/2024_06_04_47a1a533c140f3a4a1e0g-2.jpg?height=580&width=1789&top_left_y=203&top_left_x=165)

Fig. 1. An interface is adapted by simulating several possible sequences of adaptations and evaluating them using predictive models in HCI. This approach avoids greedy, disadvantageous adaptations, and may anticipate possible user responses even with limited observation data. Figure adapted from [1]

On the one hand, reward models in UI adaptation can be defined using predictive HCI models. An HCI model is a computational model that can explain how users interact with interfaces at the level of individual human cognition [15]. It simulates consequences - benefits and costs - of possible adaptation sequences without actually executing them. These models are based on various factors, such as the user's behaviour, task difficulty, and interface design features. By simulating human behaviour, HCI models can provide insights into how users will interact with a new interface (or an adapted version) before it is even built, thus allowing software developers to make informed decisions about interface design. However, finding the best adaptation is computationally costly, especially when considering sequences of changes over a long horizon. To solve this computational problem in an online setting, Monte Carlo Tree Search (MCTS) could be used for planning adaptations [7] (see Section 2). On the other hand, putting the user into the loop, or using user feedback, involves collecting user ratings or preferences on various aspects of the UI, which can be used to enhance a reward model by reflecting the users' goals and needs (preferences). This falls in the promising field of Reinforcement Learning from Human Feedback (RLHF), or RL from human preferences, in which the HF helps to train a reward model [14], [16]. Although these techniques for generating reward models appear to be useful, there is, to the best of our knowledge, no empirical evidence concerning which is most effective as regards supporting UI adaptation in a given context of use.

In this paper, we present a confirmatory study design to compare two different approaches to generate reward models in the context of UI adaptation using RL. Our study aims to investigate the effectiveness of using a reward model derived exclusively from predictive HCI models and predictive HCI models augmented with HF. The user goal may differ depending on the application domain. For example, user engagement is a key factor for the success of e-commerce systems and online catalogues as it can significantly impact on the users' purchase decisions and improve overall satisfaction. By comparing the performance of different types of reward modelling in terms of their ability to adapt to user preferences and improve UX, our study contributes to the understanding of how reward modelling can facilitate UI adaptation using RL.

## II. BACKGROUND AND RELATED WORK

Adaptive User Interfaces (AUI) have attracted increasing attention in recent years as a means to improve UX and task performance. Several studies have explored the benefits of using AUI in various application domains, such as e-commerce, education, and healthcare systems. The major challenge in this context is to determine the sequence of adaptations that have to be carried out in order to improve the system's quality, the user performance or UX. To this end, several studies have applied different methods, such as defining rules and heuristics or the use of Machine Learning and RL methods. For example, in the context of menu searching in UI, MCTS has been proposed as a promising technique for the development of adaptive menu search interfaces [7]. In this recent work, Todi et al. [7] used predictive HCI models to predict rewards for each state during simulations. Since online simulations can be computationally expensive, a pretrained value network was used to directly obtain value estimates for unexplored states. Training data for this neural network was generated using the predictive HCI models. The authors showed that while the computation time increases drastically with simulations as search depth increases, it remains constant with the neural network approach without interfering much in the overall success rate $(92.7 \%$ with model-based simulation vs. $89.6 \%$ ).

Fig. 1 shows how MCTS planning could be applied to our domain. In the current state of the tree at $t 0$ (i.e., the actual design of the interface and user observations from his/her previous interaction session with the interface), the algorithm i) selects a child node based on a selection policy that balances exploration and exploitation until reaching the most promising
leaf node; ii) expands the node only if it has already been explored by adding child nodes for all the possible adaptations and pick one child node at random; iii) roll-outs and simulates rewards using predictive HCI models while adaptations are chosen at random since the tree has no value estimates with which to inform the selection of consequential states. A sample of these roll-outs will be augmented by HF, and iv) backpropagates the updated statistics of all nodes visited during selection and expansion up the tree. These four steps are repeated until a stopping criterion is met. After the $X$ iterations allowed by the MCTS algorithm and a shallow roll-out horizon of $H$ steps, an adaptation decision is made on the basis of the highest expected reward from among the nodes that are located at $t 1$, resulting in an adapted UI. To move from $t 1$ to $t 2$, the tree is re-unfolded from the current UI at $t 1$, and so on.

Note that we have defined an environment in which to train and compare RL agents for the context of AUI in a previous study [17]. In this work, we have defined the state representation, and the actions that the system can perform to adapt the UI: i) change the UI layout; ii) change the colour scheme; iii) change the font size; iv) show/hide content, and v) do nothing.

In [18], the authors formulated UI adaptation as a multiagent RL problem, with both a user agent who learns to interact with a UI so as to complete a task and an interface agent who learns UI adaptations to maximise the user agent's performance. The joint exploration of both agents makes the adaptative UIs goal-agnostic. However, we believe that HF could be used to optimise the reward model. For example, Christiano et al. [14], propose a method with which to train RL agents using human preferences. The authors present an off-policy RL algorithm that learns from HF in the form of pairwise comparisons of trajectories. The algorithm uses a deep neural network to represent the value function and a ranking loss to optimise the network parameters. The authors evaluate their method with a variety of tasks, including Atari games and robotic manipulation, and show that it outperforms existing methods that use hand-crafted reward functions. The paper concludes that their method is a promising approach with which to train RL agents in complex environments where designing reward functions is difficult. In [19], the authors propose an algorithm that does not require any special adaptations for the off-policy or offline RL settings.

## III. EXPERIMENTAL DESIGN

According to the GQM template for goal definition [20], the goal of this study is to analyze reward models derived from predictive HCI models and predictive HCI models augmented with HF in RL algorithms with the purpose to assess their impact to adapt $U I$ with respect to their ability to improve the UX of software applications from the point-of-view of both developers and researchers interested in reward modelling in RL-based methods for UI adaptation. The former might be interested in how the use of RL for UI adaptation can improve UX in order to inform the design of more effective interfaces. This study will also provide insights into obtaining user feedback for the development of adaptive systems that can effectively adapt the UI. The latter might be interested in our study results to delineate future research (e.g., means of attaining feedback with which to evaluate the UI adaptation). Furthermore, the findings could inform the development of new approaches and tools for the improvement of UX in software applications, and could ultimately lead to more efficient and effective software design and development processes. The context consists of a group of undergraduate and Master's degree students in Computer Science at the Universitat Politècnica de València interacting with UIs from the e-commerce and e-learning domains.

## A. Research questions and hypotheses

We aim to study the systematic variation of reward models so as to measure their influence on the selection of UI adaptations that maximise the user's experience (i.e., user engagement and user satisfaction) in an experimental setup. For this purpose, we shall implement and monitor two RL-based AUI strategies (using each of the reward models selected) in two different domains. Fig. 2 shows the context used to train the RL agent that employs MCTS, with reward models such as HCI models and with HF. In our case, the RL agent will take actions (UI adaptations) and the environment will return observations. A reward model will then return reward predictions on the basis of 1) predictive HCI models only and 2) predictive models enriched with HF.

The objective of this study is to answer the following research questions and hypotheses:

RQ1: Does the incorporation of $\mathrm{HF}$ in the reward models improve the effectiveness of AUIs in terms of user engagement, when compared to reward models derived exclusively from predictive HCI models? RQ1 sheds light on the cost and benefits of training RL models with or without human feedback in terms of their impact on user engagement (operationalized by means of predictive HCI models). Since deciding about the best adaptation or sequence of adaptations to perform is not obvious for a human, incorporating humans into the loop to train the agent needs to be tested. As far as we are concerned, there is no evidence of the improvement in AUI. According to literature, the use of human feedback could bring benefits such as capturing the way in which humans naturally propose or expect what to adapt, and when to make changes, but it could also have costs related to the ability of the system to scale with the availability of computational resources. Since the analysis of costs vs. benefits for the specific problem of AUIs obtained using RL models with or without human feedback has not yet been studied, we prefer to be conservative and not postulate an effect in favour of AUIs with human feedback. We therefore, propose the following null hypothesis: $H_{n 1}$ There is no significant difference in user engagement between AUIs that incorporate HF in the reward models and those that use reward models derived exclusively from predictive HCI models.

Moreover, to assess the benefits of AUIs, it is desirable to understand the degree to which they improve the UX when
compared to non-adaptive interfaces. This could be done by comparing the user engagement (operationalized using the User Engagement Scale - UES [21]) and user satisfaction (operationalized using the Questionnaire for User Interaction Satisfaction - QUIS [22]) with the adapted and non-adapted UIs in different application domains.

Literature has already demonstrated the positive effect of AUIs in certain contexts, but has also reported the limitations that could impede the expected benefits [23]: risk of misfit (the end-user's needs are incorrectly captured or interpreted), user cognitive disruption (the end-user is disrupted by the adaptation), lack of prediction (the end user does not know when and how the adaptation will take place), lack of explanation (the end-user is not informed of the reasons for adaptation). Moreover, adopting a poor adaptation strategy may have negative effects on the user due to surprise or relearning effort. Such costly changes should be avoided. Thus, to be conservative, we prefer not to postulate that AUIs will be superior when compared to its non-AUIs version. Also, the impact of system adaptivity on user engagement has been studied in certain contexts (e.g., adaptive video streaming systems [24], large display-based UIs [25]), but evidence to understand the benefits of using RL models to support UI adaptation is required. To achieve this, we formulate the following research questions and hypotheses:

RQ2: Does the use of AUIs, obtained with reward models, improve user engagement when compared to nonadaptive interfaces in different application domains?

- $H_{n 2}$ There is no significant difference in user engagement between AUIs obtained with reward models derived exclusively from predictive HCI models and non-adaptive interfaces.
- $H_{n 3}$ There is no significant difference in user engagement between AUIs obtained with reward models derived from predictive HCI models that incorporate HF and nonadaptive interfaces.

RQ3: Does the use of AUIs, obtained with reward models, improve user satisfaction when compared to nonadaptive interfaces in different application domains?

$H_{n 4}$ There is no significant difference in user satisfaction between AUIs obtained with reward models derived exclusively from predictive HCI models and non-adaptive interfaces.

$H_{n 5}$ There is no significant difference in user satisfaction between AUIs obtained with reward models derived from predictive HCI models that incorporate HF and non-adaptive interfaces.

The scope of RQ1 lies in training the RL agent with the two different reward models, while the scope of RQ2 and RQ3 is the actual usage of systems (using UIs that do or do not employ the RL agent) by end users. The goal of the statistical analysis will be to reject these hypotheses and possibly to accept the alternative ones (e.g., $H_{a 1}=\neg H_{n 1}$ ). All the hypotheses are two-sided because we did not postulate that any effect would occur as a result of different reward models usage.

![](https://cdn.mathpix.com/cropped/2024_06_04_47a1a533c140f3a4a1e0g-4.jpg?height=236&width=878&top_left_y=169&top_left_x=1076)

Fig. 2. The RL agent employing two strategies to obtain reward predictions: predictive HCI models only (orange) and predictive HCI models with human feedback (green)

## B. Variables

This section outlines the variables and measurement scales, and how they will be operationalized to ensure reliable data collection. Statistical analyses are detailed in Section III.E.

1) Independent variables: The main independent variable in this study is the type of reward model employed by the two RL-based UI adaptation strategies that will be implemented as part of our experimentation. The scale of this variable is nominal, and can assume two possible values: adaptive UI using predictive $\mathrm{HCI}$ models only ( $\mathrm{HCI})$ and adaptive UI using predictive HCI models augmented with HF (HCI\&HF). However, to understand the gain in the improvement of UX when compared to a baseline (non-adaptive UI), we introduce a third system without any adaptive features (non-adaptive, NA). The secondary independent variable is the application domain (i.e., adaptive system), which is a nominal variable with three possible values: Sports, Courses and Trips. The former is an online sporting goods store (e-commerce domain) while the latter is a Course management system (e-learning domain).
2) Dependent variables: The dependent variable in this study is user experience (UX). It is defined by the ISO 9241210: 2010 as "a person's perceptions and responses that result from the use and/or anticipated use of a product, system or service". In contrast to task-oriented interactions, UX represents a focus on "experience", encompassing also hedonic qualities, emotions and affect (e.g., interested, enthusiastic, irritable) that people experience while interacting with software systems. It can be operationalized in several ways as this is a complex concept with several dimensions. In this study, we measure UX in terms of user engagement and user satisfaction.

User engagement is a quality of user experience with software systems that is characterised among others by aesthetic, sensory appeal, perceived control and time, awareness, motivation, and affect [26] [27] [28]. It will be measured in two different stages: 1) when training the RL model (for calculating reward predictions using predictive HCI models that employs behaviour logging approaches), and 2) during the user interaction with the system (i.e., experimental task). In the fist stage, user engagement will be operationalized using a set of metrics collected from the literature [28], [29], [30] (i.e., individual event counts, relative frequencies of events, total number of user actions, and task completion time); while, in the second stage, it will be operationalized using the UES questionnaire in a post-experimental task.

TABLE I

THE TREATMENT SEQUENCE WITH THREE TECHNIQUES (NA, HCI AND HCI\&HF) AND THREE APPLICATION DOMAINS (TRIPS, SPORTS AND COURSES)

| Domain | Trips |  |  | Sports |  |  | Courses |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Period | Period 1 |  |  | Period 2 |  |  | Period 3 |  |  |
| Technique | NA | HCI | HCI\&HF | NA | HCI | HCI\&HF | NA | HCI | HCI\&HF |
| Group 1 | $\mathrm{x}$ | - | - | - | $\mathrm{x}$ | - | - | - | $\mathrm{x}$ |
| Group 2 | $\mathrm{x}$ | - | - | - | - | $\mathrm{x}$ | - | $\mathrm{x}$ | - |
| Group 3 | - | $\mathrm{x}$ | - | $\mathrm{x}$ | - | - | - | - | $\mathrm{x}$ |
| Group 4 | - | $\mathrm{x}$ | - | - | - | $\mathrm{x}$ | $\mathrm{x}$ | - | - |
| Group 5 | - | - | $\mathrm{x}$ | $\mathrm{x}$ | - | - | - | $\mathrm{x}$ | - |
| Group 6 | - | - | $\mathrm{x}$ | - | $\mathrm{x}$ | - | $\mathrm{x}$ | - | - |

On the other hand, user satisfaction is a measure of the quality of the UX and refers to the extent to which a user's expectations are met [31]. It will be measured after the user interaction with each of the adaptive systems (i.e., postexperimental task) using the Questionnaire for User Interaction Satisfaction (QUIS) [22]. It contains a measure of overall system satisfaction along six sub-scales and measures of four specific UI factors (screen factors, terminology and system feedback, learning factors, and system capabilities). Each area measures the overall satisfaction on a 10-point scale.

## C. Subjects

Participants for this study will be recruited using a convenience sampling technique, a non-probability sampling method that selects individuals based on their availability and willingness to participate [32]. The study will target undergraduate and Master's students in computer science at the Universitat Politècnica de València. We will will recruit participants during class time, providing a brief overview of the study and inviting interested students to participate. To ensure the voluntary nature of participation, we will emphasise that all participants have the opportunity to decline without any negative consequences. Once the participants have been selected, we will give them the informed consent document. The experiment will be conducted online and will follow the university IRB protocol. In particular, the consent form will state that no personal data is collected, or if it is collected, that it will not be published, and will be destroyed.

## D. Design

The experiment is designed as a balanced within-subject three-treatment factorial crossover design. We will carry out a power analysis to allow us to determine the appropriate sample size to detect an effect of medium size. We chose this design because it addresses the issue of small sample sizes and increases the sensitivity of experiments. In a balanced crossover design the measures are taken several times from a participant (i.e., a participant is assigned to a sequence of treatments). We follow the guidelines proposed by Vegas et al. [33] to define the crossover design. In particular, we will analyse not only the effects of the treatments (i.e., NA, $\mathrm{HCI}$ and HCI\&HF) on the dependent variables, but also the effects of critical crossover variables (i.e., period, sequence and carryover). In this scenario, it is necessary to differentiate between the concepts of period and session. A period is defined by the application of one treatment by one participant to one application domain (adaptive system), whereas a session is a portion of time taken by a subject to complete (one or more) experimental tasks [33]. We consequently will have three periods, since each participant had to perform two treatments and for reasons of the students' class timetable. We, therefore, will carry out one period in a session. Carryover is the persistence of the effect of one treatment when another treatment is applied later. The objective is to find out whether these additional factors are influencing the dependent variables. Table $\Pi$ shows that we have a special type of design called a factorial crossover design, which has the same number of periods as treatments.

Each period will be assigned to a domain and each group will follow a different sequence of adaptation techniques. Producing 6 different sequences (see Table $\square$ ). In the first period, every group will interact with a Trip Planner system. Groups 1 and 2 will use the system without adaptation capabilities (NA), while Groups 3 and 4 will interact with AUI that employs a RL agent with predictive HCI models (HCI) and Groups 5 and 6 will interact with AUI that employs a RL agent with predictive HCI models and Human Feedback (HCI\&HF). In the second period, everyone will interact with an e-commerce system (Sporting Goods Store). Groups 3 and 5 will use the system without adaptation capabilities (NA), while Groups 1 and 6 will interact with AUI-HCI and, Groups 2 and 4 will interact with AUI-HCI\&HF. In the third period, everyone will interact with an e-learning system (Courses Management system). Groups 4 and 6 will use the system without adaptation capabilities (NA), while Groups 2 and 5 will interact with AUIHCI and, Groups 1 and 3 will interact with AUI-HCI\&HF. We carefully selected these sequences to ensure that any potential order effects are minimised. In this scenario, we do not believe that there is the possibility that any of the sequences would have improved the experimental results.

We choose the trip, sporting, and courses systems from the e-commerce and e-learning domains as application domains for this experiment because, these domains are representative of different types of software products and user requirement needs. Moreover, these domains are popular and widely used, making them relevant for a wide range of users. Finally, the selected systems have similar complexity.

## E. Analysis plan

The results of the experiment will be collected using $i$ ) the user engagement computed with the predictive HCI models exclusively and ii) predictive HCI models with HF. Additionally, the participants will carried out a post-experimental task that we will allow us to measure their user satisfaction by using the QUIS questionnaire.

We will use descriptive statistics, violin plots, and statistical tests to analyse the data collected from the experiment. As is usual, we accepted a probability of $5 \%$ of committing a Type-I Error in all the statistical tests.

We will first carry out a descriptive study of the measures for the dependent variables. Then, as a crossover design will be used in the study, it is necessary to analyse the experiment factors including periods, sequences, and carryover. To test the formulated hypotheses, the Linear Mixed Model (LMM) will be employed. Following the guidelines proposed by Vegas et al. [33], the LMM includes the following fixed factors: Reward model technique (treatment), period (confounded with the application domain), and sequence (confounded with carryover and period-technique interaction), and subject as a random factor nested within the sequence. The LMM will be used to assess whether these factors have influenced the results. All the assumptions of LMMs will be tested and reported. In order to apply the LMM, the residuals had to meet the condition of normality [33]. To ensure that the model was valid, we therefore will use the Shapiro-Wilk test if the sample size ends up being smaller than 50 and Kolmogorov-Smirnov if the sample size is higher than 50 to confirm the normality of the residuals. We will then apply the LMM to each dependent variable in order to assess whether the period (confounded with the application domain), sequences (confounded with period-technique and carryover) or technique (treatment) had statistical significance.

We will report the results of the QUIS scores for the experiment. We will also assess the questionnaire quality in terms of its reliability, validity and sensitivity. Then, we will measure the effect size to assess the magnitude of differences caused by the treatments. The effect size of the treatments should be measured only if the period, sequence or any blocking variables have no bearing, and there is no carryover [34]. To this end, we will use Cohen's d.

## F. Threats to validity

In this section, we discuss the issues that will may threat the validity of our experiment and how we will mitigate them [35]. One potential threat to internal validity is the Hawthorne effect [36], where participants may alter their behaviour due to being observed. To mitigate this threat, participants will be instructed to behave as they would in a natural setting. Another potential threat is the maturation effect, where participants' responses may change over time due to natural changes such as fatigue or boredom. To mitigate this, we will have a long break in the middle of each session. Also, to mitigate potential threats related to participant's interaction, participants will be recruited from different academic degrees (i.e., Bachelor's and Master's in Computer Science) which happen in different semesters. Participants from these groups are not expected to interact with each other during the study periods/sessions.

To address the potential threat to conclusion validity, a sufficient number of participants will be recruited to ensure adequate statistical power and the proper statistical tests will be performed for the variables and their assumptions [37].

The potential threats to the construct validity, are the validity and reliability of the HCI and HCI\&HF models and the questionnaires used to measure user satisfaction and engagement. To mitigate these threats, the models will be based on existing research [28]-[30] and validated by domain experts. To validate the measures of user engagement obtained with the predictive HCI models, during the interaction, we will gather the participants' engagement through User Engagement Scale (UES) [21] and the participants' satisfaction through QUIS questionnaire [22], which are empirically validated questionnaires, as post-experimental tasks.

To increase external validity, we will address the potential threat of participants' behaviour not generalising to real-world situations by using a realistic scenario for each domain to make the tasks more meaningful. Additionally, we will design the tasks used to measure the effectiveness of the adaptive UIs to reflect realistic browsing and purchasing behaviours. Nevertheless, the representativeness of the results may be affected by the profile of the participants. Replications with industry participants are necessary.

## IV. EXECUTION PLAN

The experiment will be conducted over the course of two weeks, with each participant attending three sessions. Before the first session, all participants will complete a brief demographic survey and will be assigned to one of the six groups. In the first session, they will interact with a Trip Planner system using the three approaches (NA, HCI, HCI\&HF). Groups 1 and 2 will use the system without adaptation capabilities (NA). Groups 3 and 4 will interact with AUI-HCI and, Groups 5 and 6 will interact with AUI-HCI\&HF. In the second session, participants will switch to another approach in an e-commerce system (Sporting Goods Store). Specifically, Groups 3 and 5 will use the NA, while Groups 1 and 6 will use the HCI and, Groups 2 and 4 will use the HCI\&HF. Finally, in the third session, participants will switch to the last approach that they haven't used in an an e-learning system (Courses Management system). Groups 4 and 6 will use NA, while Groups 2 and 5 will use HCI and, Groups 1 and 3 will use HCI\&HF.

Each session will consist of the following steps:

1) Introduction and task explanation.
2) Task execution: Participants will perform the tasks using the assigned AUI. The tasks will be defined once the adaptations for the selected sytems are defined.
3) Post-task questionnaires: The participant will be asked to fill the QUIS and the UES to evaluate their satisfaction and engagement with the UI, respectively. The questionnaires will be administered through an Excel file and participants will have to complete after each session.

All sessions will be conducted in a controlled lab environment with identical equipment and software configurations. Participants will be seated at a desk with a computer screen, mouse, and keyboard. The order of the sessions will be counterbalanced to control for any potential order effects. Each session will last, at most, 120 minutes. However, participants will not be required to complete the tasks within this time frame, and there will be no time pressure on them to do so.

## ACKNOWLEDGMENT

This work is supported by the AKILA project (CIAICO/2021/303) funded by the Generalitat Valenciana (GVA). Daniel Gaspar-Figueiredo is funded by the GVA under the grant ACIF/2021/172, which is cofunded by the European Union through the ESF.

## REFERENCES

[1] G. Chaslot, S. Bakkes, I. Szita, and P. Spronck, "Monte-carlo tree search: A new framework for game ai," Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, vol. 4, no. 1, pp. 216-217, 2021

[2] P. A. Akiki, A. K. Bandara, and Y. Yu, "Adaptive model-driven user interface development systems," ACM Comput. Surv., vol. 47, no. 1, may 2014. [Online]. Available: https://doi.org/10.1145/2597999

[3] G. Viano, A. Parodi, J. Alty, C. Khalil, I. Angulo, D. Biglino, M. Crampes, C. Vaudry, V. Daurensan, and P. Lachaud, "Adaptive user interface for process control based on multi-agent approach," in Proc. of the Working Conference on Advanced Visual Interfaces, ser. AVI '00. New York, NY, USA: Association for Computing Machinery, 2000, p. 201-204. [Online]. Available: https://doi.org/10.1145/345513.345316

[4] S. Abrahão, E. Insfrán, A. Sluÿters, and J. Vanderdonckt, "Model-based intelligent user interface adaptation: challenges and future directions," Software and Systems Modeling, vol. 20, no. 5, pp. 1335-1349, 2021.

[5] J. D. Lomas, J. Forlizzi, N. Poonwala, N. Patel, S. Shodhan, K. Patel, K. Koedinger, and E. Brunskill, "Interface design optimization as a multi-armed bandit problem," in Proc. of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 2016, p. 4142-4153.

[6] J. J. Dudley, J. T. Jacques, and P. O. Kristensson, "Crowdsourcing interface feature design with bayesian optimization," in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI '19. New York, NY, USA: ACM, 2019, p. 1-12.

[7] K. Todi, G. Bailly, L. Leiva, and A. Oulasvirta, "Adapting user interfaces with model-based reinforcement learning," in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, ser. CHI '21. New York, NY, USA: Association for Computing Machinery, 2021. [Online]. Available: https://doi.org/10.1145/3411764.3445497

[8] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.

[9] L. P. Kaelbling, M. L. Littman, and A. W. Moore, "Reinforcement learning: A survey," Journal of artificial intelligence research, vol. 4, pp. 237-285, 1996

[10] A. Zap, T. Joppen, and J. Fürnkranz, "Deep ordinal reinforcement learning," in Machine Learning and Knowledge Discovery in Databases, U. Brefeld, E. Fromont, A. Hotho, A. Knobbe, M. Maathuis, and C. Robardet, Eds. Springer International Publishing, 2020, pp. 3-18.

[11] D. Schmidt, N. Moran, J. Rosenfeld, J. Rosenthal, and J. Yedidia, "Selfplay learning without a reward metric," 122019.

[12] X. Chen, L. Yao, X. Wang, A. Sun, and Q. Z. Sheng, "Generative adversarial reward learning for generalized behavior tendency inference," IEEE Trans on Knowledge and Data Engineering, pp. 1-12, 2022.

[13] S. Armstrong, J. Leike, L. Orseau, and S. Legg, "Pitfalls of learning a reward function online," in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, ser. IJCAI'20, 2021.

[14] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, "Deep reinforcement learning from human preferences," in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.

[15] C. Paton, A. W. Kushniruk, E. M. Borycki, M. English, and J. Warren, "Improving the usability and safety of digital health systems: The role of predictive Human-Computer interaction modeling," J Med Internet Res, vol. 23, no. 5, p. e25281, May 2021.

[16] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano, "Learning to summarize with human feedback," Advances in Neural Information Processing Systems, vol. 33, pp. 3008-3021, 2020.

[17] D. Gaspar-Figueiredo, S. Abrahao, E. Insfran, and M. FernándezDiego, "Reinforcement learning agent environment for adaptation of user interfaces (in Spanish)," in JISBD2023. SISTEDES, 2023 (submitted on April).
[18] T. Langerak, S. Christen, M. Albaba, C. Gebhardt, and O. Hilliges, "Marlui: Multi-agent reinforcement learning for goal-agnostic adaptive uis," arXiv preprint arXiv:2209.12660, 2022.

[19] J. Schrittwieser, T. K. Hubert, A. Mandhane, M. Barekatain, I. Antonoglou, and D. Silver, "Online and offline reinforcement learning by planning with a learned model," in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=HKtsGW-lNbw

[20] V. R. Basili, G. Caldiera, and H. D. Rombach, "The goal question metric approach," 1994.

[21] H. L. O'Brien, P. Cairns, and M. Hall, "A practical approach to measuring user engagement with the refined user engagement scale (ues) and new ues short form," International Journal of Human-Computer Studies, vol. 112, pp. 28-39, 2018.

[22] K. L. Norman, B. Shneiderman, B. Harper, and L. Slaughter, "Questionnaire for user interaction satisfaction," Available at: https://site.unibo.it/ hfrs/en/questionnaire-and-scales-2/quis. 1998.

[23] T. Lavie and J. Meyer, "Benefits and costs of adaptive user interfaces," International Journal of Human-Computer Studies, vol. 68, no. 8, pp. 508 - 524, 2010. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S1071581910000145

[24] C. Qiao, J. Wang, Y. Wang, Y. Liu, and H. Tuo, "Understanding and improving user engagement in adaptive video streaming," in 2021 IEEE/ACM 29th International Symposium on Quality of Service (IWQOS), 2021, pp. 1-10.

[25] A. X. Li, X. Lou, P. Hansen, and R. Peng, "Improving the User Engagement in Large Display Using Distance-Driven Adaptive Interface," Interacting with Computers, vol. 28, no. 4, pp. 462-478, 06 2016. [Online]. Available: https://doi.org/10.1093/iwc/iwv021

[26] H. L. O'Brien and E. G. Toms, "What is user engagement? a conceptual framework for defining user engagement with technology," Journal of the American society for Information Science and Technology, vol. 59, no. 6, pp. 938-955, 2008.

[27] K. Doherty and G. Doherty, "Engagement in hci: conception, theory and measurement," ACM Computing Surveys (CSUR), vol. 51, no. 5, pp. $1-39,2018$.

[28] J. Lehmann, M. Lalmas, E. Yom-Tov, and G. Dupret, "Models of user engagement," in User Modeling, Adaptation, and Personalization, J. Masthoff, B. Mobasher, M. C. Desmarais, and R. Nkambou, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 164-175.

[29] E. Barbaro, E. M. Grua, I. Malavolta, M. Stercevic, E. Weusthof, and J. van den Hoven, "Modelling and predicting user engagement in mobile applications," Data Science, vol. 3, no. 2, pp. 61-77, 2020.

[30] J. Carlton, A. Brown, C. Jay, and J. Keane, "Using interaction data to predict engagement with interactive media," in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 1258-1266.

[31] "Ergonomics of human-system interaction-Part 11: Usability: Definitions and concepts." International Organization for Standardization, Tech. Rep., 2018

[32] S. Baltes and P. Ralph, "Sampling in software engineering research: a critical review and guidelines," Empirical Software Engineering, vol. 27, no. 4, p. 94, Apr 2022. [Online]. Available: https: //doi.org/10.1007/s10664-021-10072-8

[33] S. Vegas, C. Apa, and N. Juristo, "Cross-over designs in software engineering experiments: Benefits and perils," IEEE Transactions on Software Engineering, vol. 42, pp. 1-1, 012015.

[34] J. C. Carver, "Towards reporting guidelines for experimental replications: A proposal," in 1st international workshop on replication in empirical software engineering, vol. 1, 2010, pp. 1-4.

[35] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, and A. Wesslén, Empirical Strategies. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 9-36.

[36] J. G. Adair, "The hawthorne effect: a reconsideration of the methodological artifact." Journal of applied psychology, vol. 69, no. 2, p. 334, 1984.

[37] M. K.D., "Applied statistics for software managers," Applied Statistics for Software Managers, 2002. [Online]. Available: https://cir.nii.ac.jp/ crid/1573668924056253312

