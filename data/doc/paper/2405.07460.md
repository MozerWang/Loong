# HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models 

Aakash Tripathi ${ }^{\dagger *}$<br>Department of Machine Learning<br>Moffit Cancer Center<br>Tampa, FL, 33620<br>aakash.tripathi@moffitt.org<br>Yasin Yilmaz<br>Department of Electrical Engineering<br>University of South Florida<br>Tampa, FL, 33620<br>yasiny@usf.edu

Asim Waqas ${ }^{\dagger}$<br>Department of Machine Learning<br>Moffit Cancer Center<br>Tampa, FL, 33620<br>asim.waqas@moffitt.org<br>Ghulam Rasool ${ }^{\dagger}$<br>Department of Machine Learning<br>Moffit Cancer Center<br>Tampa, FL, 33620<br>ghulam.rasool@moffitt.org


#### Abstract

Developing accurate machine learning models for oncology requires large-scale, high-quality multimodal datasets. However, creating such datasets remains challenging due to the complexity and heterogeneity of medical data. To address this challenge, we introduce HoneyBee, a scalable modular framework for building multimodal oncology datasets that leverages foundational models to generate representative embeddings. HoneyBee integrates various data modalities, including clinical records, imaging data, and patient outcomes. It employs data preprocessing techniques and transformer-based architectures to generate embeddings that capture the essential features and relationships within the raw medical data. The generated embeddings are stored in a structured format using Hugging Face datasets and PyTorch dataloaders for accessibility. Vector databases enable efficient querying and retrieval for machine learning applications. We demonstrate the effectiveness of HoneyBee through experiments assessing the quality and representativeness of the embeddings. The framework is designed to be extensible to other medical domains and aims to accelerate oncology research by providing high-quality, machine learning-ready datasets. HoneyBee is an ongoing open-source effort, and the code, datasets, and models are available at the project repository.


## 1 Introduction

Multimodal medical datasets are crucial in developing machine-learning models for oncology applications, including diagnosis, prognosis, and treatment planning. These datasets combine information from various data modalities, such as clinical records, imaging data, genomic information, and patient outcomes, to comprehensively represent patient characteristics and disease progression. Integrating multimodal data enables the discovery of complex patterns and relationships that may not be apparent when considering individual data modalities in isolation. Machine learning models trained on[^0]multimodal datasets can potentially improve the accuracy and robustness of predictions, leading to better clinical decision-making and personalized treatment strategies [2].

However, creating large-scale, high-quality multimodal datasets for oncology remains a significant challenge. Medical data is inherently complex and heterogeneous, with each data modality presenting unique characteristics and challenges. Clinical records often contain unstructured and inconsistent information, requiring extensive preprocessing and standardization [16. Imaging data, such as whole slide images and radiology scans, are high-dimensional and require specialized techniques for feature extraction and representation learning [13]. Genomic data, including DNA sequencing and gene expression profiles, is high-dimensional and sparse, necessitating appropriate normalization and feature selection methods [20]. Moreover, integrating these diverse data modalities into a coherent dataset poses additional challenges related to data synchronization, missing data handling, and data harmonization [2].

Existing oncology datasets often have limitations that reduce their utility for machine learning research and development. Many datasets lack the necessary scale and diversity to capture the full spectrum of patient characteristics and disease subtypes [17]. Small sample sizes and limited representation of specific patient populations can lead to biased and underpowered models [20]. Additionally, datasets may be fragmented across different institutions and data repositories, making it difficult to access and integrate them effectively [19]. Furthermore, raw medical data often requires extensive preprocessing and feature engineering before it can be used for machine learning tasks, which is time-consuming and requires domain expertise [13].

To address these limitations, there is a pressing need for machine learning-ready datasets that provide large-scale, diverse, and high-quality multimodal data in a structured and accessible format. Using state-of-the-art foundational models, such datasets should include representative embeddings generated from raw medical data. Foundational models, such as transformer-based architectures, have demonstrated remarkable performance in learning rich representations from large-scale datasets across various domains [20, 24, 1, 3, 10]. By leveraging foundational models to generate embeddings, machine learning-ready datasets can capture the essential features and relationships within the raw medical data, reducing the need for manual feature engineering and enabling the development of more accurate and generalizable models.

This paper proposes HoneyBee, a scalable and modular framework for building multimodal oncology datasets with foundational model embeddings. HoneyBee extends the capabilities of the Multimodal Integration of Oncology Data System (MINDS) [19] by incorporating state-of-the-art transformerbased architectures to generate embeddings from raw medical data, such as whole slide images and radiology scans. The generated embeddings and tabular data are stored in a structured format using Hugging Face datasets [12] and PyTorch dataloaders, making them readily accessible for machine learning research and development. The primary objectives of HoneyBee are as follows:

1. To build large-scale, diverse, and high-quality multimodal oncology datasets by integrating various modalities, including clinical records, imaging data, genomic information, and patient outcomes.
2. To generate representative embeddings from raw medical data using foundational models, capturing the essential features and relationships within the data.
3. To provide machine learning-ready datasets with embeddings stored in a structured and accessible format, facilitating the development and evaluation of advanced models for oncology applications.
4. To establish a scalable and modular framework that can be extended to other medical domains, fostering the creation of comprehensive datasets and driving the adoption of foundational models in healthcare.

The remainder of this paper is organized as follows. Section 2 reviews related work on multimodal oncology datasets, foundational models, and machine learning-ready datasets. Section 3 describes the HoneyBee framework in detail, including the data acquisition and integration process, embedding generation using foundational models, and data storage and accessibility components. Section 4 presents the datasets generated using the HoneyBee framework, focusing on The Cancer Genome Atlas (TCGA) dataset as a case study. Section 5 demonstrates the utility of the HoneyBee datasets through various use cases and experimental evaluations, such as similarity retrieval, cancer type
classification, and survival analysis. We discuss the limitations, challenges, and future directions of the HoneyBee framework in Section6. Finally, Section 7 concludes the paper, highlighting the contributions and potential impact of the HoneyBee framework in advancing oncology research and clinical applications.

## 2 Related Work

### 2.1 Multimodal Integration of Oncology Data System (MINDS)

MINDS [19] is a framework developed to integrate multimodal oncology data from publicly available datasets. MINDS consolidates metadata from various datasets and provides efficient query capabilities for interactive data exploration. The framework has demonstrated the feasibility and value of incorporating diverse oncology datasets by enabling sub-5-second query response times. MINDS has made significant contributions to the field of oncology data integration by providing a centralized platform for accessing and analyzing multimodal data. However, MINDS does not directly address the generation of embeddings from raw medical data or the creation of machine learning-ready datasets. While MINDS facilitates the integration and querying of metadata, it does not focus on the preprocessing and representation learning aspects required for developing machine learning models. The framework does not incorporate state-of-the-art foundational models for generating embeddings from raw medical data, such as whole slide images or radiology scans. Consequently, there remains a need for a framework that extends the capabilities of MINDS by incorporating embedding generation techniques and providing structured, machine learning-ready datasets.

### 2.2 Hugging Face Datasets Library

The Hugging Face datasets library [12] is a popular open-source library that provides a largescale, structured repository for various datasets across different domains. The library offers a standardized interface for accessing, processing, and sharing datasets, making it easier for researchers and practitioners to utilize and contribute to the dataset ecosystem. The Hugging Face datasets library includes several datasets for training models in the medical domain [4, 11, 15].

While the Hugging Face datasets library provides a valuable resource for accessing and processing datasets, it does not explicitly focus on creating multimodal oncology datasets with embeddings generated from raw medical data. The library primarily serves as a repository for existing datasets. It does not incorporate specific techniques for generating embeddings from complex medical data modalities, such as whole slide images or radiology scans. Moreover, the datasets available in the library may not always be tailored to the specific requirements of machine learning-ready oncology datasets, such as the inclusion of representative embeddings and structured metadata.

### 2.3 Foundational Models in Medical Imaging

Foundational models, particularly transformer-based architectures, have shown promising results in various medical imaging tasks, including classification, segmentation, and detection [23, 10, 24]. These models can learn rich representations from large-scale datasets and capture complex patterns and relationships within medical images. Transformer-based models, such as the Vision Transformer (ViT) [7] and the Swin Transformer [14], have achieved state-of-the-art performance on several medical imaging benchmarks [18].

However, the application of foundational models to generating embeddings for multimodal oncology datasets has not been extensively explored. While these models have demonstrated success in learning representations from individual medical imaging modalities, their effectiveness in integrating and generating embeddings from multiple data modalities, such as clinical records, genomic data, and patient outcomes, remains an open research question. Additionally, the scalability and computational requirements of training foundational models on large-scale multimodal datasets pose challenges that must be addressed.

### 2.4 Machine Learning-Ready Oncology Datasets

Despite the advancements in multimodal data integration and foundational models, machine learningready oncology datasets, including raw multimodal medical data and their corresponding embeddings,

![](https://cdn.mathpix.com/cropped/2024_06_04_169ed6dfbaccd00d7f72g-04.jpg?height=428&width=894&top_left_y=241&top_left_x=369)

Multimodal Integration of Oncology Data System

![](https://cdn.mathpix.com/cropped/2024_06_04_169ed6dfbaccd00d7f72g-04.jpg?height=425&width=483&top_left_y=243&top_left_x=1271)

Harmonized Oncology Network Enhancing Yield through Big-Data Exploration and Evaluation

Figure 1: Schematic of the HoneyBee framework showing the process of data acquisition, embedding generation, and data storage. It outlines the flow from raw data sources to the integration of various data modalities, embedding creation using foundational models, and structured storage for machine learning applications.

are scarce. Existing oncology datasets often suffer from limitations in terms of scale, diversity, and accessibility, slowing the development and evaluation of advanced machine-learning models.

Publicly available datasets, such as TCGA [21] and the International Cancer Genome Consortium (ICGC) [5], provide valuable resources for cancer research but may not continuously be optimized for machine learning tasks. These datasets often require extensive preprocessing, data harmonization, and feature engineering before they can be effectively utilized for model development. Moreover, the lack of representative embeddings generated from raw medical data limits the ability to leverage the full potential of deep learning techniques.

To address these limitations, there is a need for a framework that can create large-scale, high-quality, and machine learning-ready multimodal oncology datasets with embeddings generated from raw medical data. Such a framework should leverage the strengths of existing tools and libraries, such as MINDS for data integration, Hugging Face datasets for structured storage and accessibility, and foundational models for embedding generation. By providing comprehensive and well-curated datasets with representative embeddings, researchers and practitioners can focus on developing and evaluating advanced machine learning models for oncology applications. HoneyBee aims to bridge this gap by integrating the capabilities of the MINDS framework, Hugging Face datasets library, and foundational models to create large-scale, high-quality, and machine learning-ready multimodal oncology datasets with embeddings generated from raw medical data. By addressing the limitations of existing datasets and leveraging state-of-the-art techniques, HoneyBee seeks to accelerate the adoption of machine learning in oncology research and clinical practice.

## 3 The HoneyBee Framework

The HoneyBee framework consists of three main components: data acquisition and integration, embedding generation, and data storage and accessibility. Figure 1 provides an overview of the framework, illustrating the data flow, the interactions between the components, and how the data is acquired from the MINDS database.

### 3.1 Data Acquisition and Integration

HoneyBee extends the MINDS framework [19] for acquiring and integrating large-scale multimodal oncology datasets. Data is collected from various sources, including public repositories (e.g., TCGA, ICGC), clinical institutions, and research collaborations. The following are the key data modalities incorporated in HoneyBee datasets:

1. Clinical Records: Structured and unstructured medical reports, including pathology reports, radiology reports, and clinical notes.
2. Imaging: Whole slide images (WSIs) of histopathology specimens, radiology scans such as computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and other pathology images.
3. Genomics: DNA sequencing data, gene expression profiles, mutation data, and other genomic information.
4. Other Omics: Proteomics and metabolomics data, providing additional molecular-level information.
5. Electronic Health Records (EHRs): Structured data including patient demographics, medical history, diagnosis, treatment, and outcomes.

The acquired data undergoes a series of integration and preprocessing steps to ensure data quality, harmonize heterogeneous datasets, and align data elements across modalities. Data cleaning techniques are applied to handle missing values, outliers, and inconsistencies. Normalization methods such as min-max scaling and z-score normalization are employed to standardize numerical features. Standardization protocols like Digital Imaging and Communications in Medicine (DICOM) for imaging data and Fast Healthcare Interoperability Resources (FHIR) for EHRs are utilized to ensure interoperability and facilitate data exchange.

### 3.2 Embedding Generation

HoneyBee utilizes state-of-the-art foundational models to generate representative embeddings from raw medical data. The embedding generation process involves three key steps: selecting the appropriate foundational model for each data modality, preprocessing the raw medical data to ensure compatibility with the chosen model, and generating the embedding vectors.

### 3.2.1 Selection of Foundational Models

Transformer-based architectures have demonstrated superior performance in various domains, including computer vision and natural language processing. Vision Transformers (ViT) [7] is employed for imaging data, leveraging self-attention mechanisms to capture complex patterns and relationships in high-dimensional visual data. For textual data, Bidirectional Encoder Representations from Transformers (BERT) [6] and its variants are utilized, enabling contextual understanding and effective representation learning.

### 3.2.2 Preprocessing of Raw Medical Data

Before generating embeddings, the raw medical data undergoes preprocessing to ensure compatibility with the selected foundational models. The preprocessing steps vary depending on the data modality. For imaging data, techniques such as resizing, normalization, and data augmentation (e.g., rotation, flipping, cropping) are applied to standardize the input images and improve model robustness. Textual data preprocessing includes tokenization, stopword removal, and normalization to convert the raw text into a suitable format for embedding generation. Genomic data may require feature selection, normalization, and encoding techniques to effectively represent the high-dimensional genomic information. As shown in Figure 2, HoneyBee processes each data modality separately and generates embeddings using transformer-based models. The embeddings are then combined and stored in a structured format, making them easily accessible for downstream machine-learning tasks. This modular approach enables the framework to handle a variety of data modalities and allows for the incorporation of new modalities as they become available.

### 3.2.3 Embedding Generation Process

The preprocessed medical data is fed into the selected foundational models to generate embeddings. Algorithm 1 outlines the embedding generation process. For each data modality and each sample, the preprocessed data is passed through the corresponding foundational model, which generates a fixedlength embedding vector. The generated embeddings are high-dimensional vectors, typically ranging from 768 to 1024 dimensions, depending on the specific model architecture. These embeddings capture the essential features and relationships within the raw data, enabling efficient storage, retrieval, and analysis.

![](https://cdn.mathpix.com/cropped/2024_06_04_169ed6dfbaccd00d7f72g-06.jpg?height=878&width=1391&top_left_y=241&top_left_x=367)

Figure 2: Illustration of a multimodal medical data processing workflow divided into three main sections. Section A focuses on pathology workflows, showing the processing of whole slide images into $512 \times 512$ pixel tiles, followed by tissue detection and embedding generation for each tile. Section $\mathrm{B}$ outlines the processing of electronic health records and textual data, converting tabulated and report data into text, which is then chunked and processed through the GatorTron model to produce embeddings. Section C depicts the workflow for radiology imaging, processing 3D volumetric CT scans into axial slices and generating embeddings for each slice using a specialized radiology embedding model. This figure highlights the integration of different medical data modalities-pathology, text, and radiology-into a unified embedding generation framework.

```
Algorithm 1 Embedding Generation Process
    for each data modality do
        for each data sample do
            Preprocess the raw medical data
            Feed the preprocessed data into the selected foundational model
            Generate the embedding vector
            Store the embedding vector along with the associated metadata
        end for
    end for
```

The generated embeddings, along with the associated metadata, are stored in a structured format to facilitate downstream tasks such as similarity search, clustering, and machine learning model training.

### 3.3 Data Storage and Accessibility

HoneyBee prioritizes the structured storage and accessibility of the generated embeddings and associated tabular data. The framework utilizes the following components for efficient data storage and retrieval:

### 3.3.1 Hugging Face Datasets and PyTorch Dataloaders

The generated embeddings and tabular data are stored using the Hugging Face datasets library [12], which provides a standardized interface for data access and processing. The datasets are organized
into a structured format, containing the embeddings, metadata, and labels (if available). PyTorch DataLoaders are employed to efficiently load and iterate over the datasets during model training and evaluation, handling tasks such as batching, shuffling, and parallel processing.

### 3.3.2 Vector Databases for Efficient Querying and Retrieval

To enable fast similarity search, nearest neighbor retrieval, and clustering on the high-dimensional embedding vectors, HoneyBee datasets can be integrated into vector databases such as Faiss [8] or Annoy [22]. These databases are optimized for efficient querying and retrieval of embeddings based on similarity metrics such as Euclidean distance or cosine similarity. By leveraging vector databases, researchers can quickly identify similar samples, perform data exploration, and retrieve relevant subsets based on embedding similarity, facilitating various downstream analysis tasks.

The structured storage and accessibility components of HoneyBee ensure that the generated embeddings and associated data are readily available for researchers and practitioners to use in their machine-learning pipelines and downstream applications. The combination of Hugging Face datasets, PyTorch DataLoaders, and vector databases provides a robust and efficient framework for managing and utilizing multimodal oncology data.

## 4 Datasets

To demonstrate the utility of the HoneyBee framework, we have created a comprehensive multimodal oncology dataset by extracting and processing data from TCGA. This landmark cancer genomics program has molecularly characterized over 11,000 primary cancer patients and matched normal samples spanning 33 cancer types. We have extracted all available data modalities from the TCGA project, including clinical data (electronic health records and pathology reports), pathology images (whole slide images of tumor and diagnostic samples), and radiology images (CT, MR, PT). Table 1 provides an overview of the TCGA data used in our dataset.

We used pre-trained models on large-scale datasets to generate embeddings for each data modality. For textual data like clinical notes and pathology reports, we used GatorTron-medium [23], a 3.9 billion parameter model trained on biomedical corpora. For pathology WSIs, we used UNI [3], a vision transformer-based encoder pre-trained on the Mass-100K dataset with 100 million tissue patches from over 100,000 WSIs. UNI uses the DINOv2 self-supervised training algorithm with mask image modeling and self-distillation objectives. UNI performs well across tasks in computational pathology, including classification, segmentation, image retrieval, and slide-level weakly supervised learning. For radiology images, we used REMEDIS [1], trained on over 4.5 million medical images from various modalities like CT, MRI, and PET scans. REMEDIS employs a two-step pre-training strategy: supervised representation learning on labeled natural images using the Big Transfer (BiT) method and self-supervised learning to adapt to medical data. REMEDIS provides a transferable representation fine-tuned for tasks such as tumor detection, segmentation, and classification with minimal annotated data.

The embedding generation process is detailed in Algorithm 1. We preprocess the raw data for each modality and sample, feed it into the model, and generate a fixed-length embedding vector. The embeddings are stored with associated metadata using the Hugging Face datasets library. The embedding generation process follows the steps outlined in Algorithm 1. For each data modality and each sample, we preprocess the raw data, feed it into the corresponding foundational model, and generate a fixed-length embedding vector. The resulting embeddings are stored along with the associated metadata in a structured format using the Hugging Face datasets library.

To facilitate easy access and utilization of the generated embeddings, we have publicly made the processed TCGA dataset available on the Hugging Face platform ${ }^{3}$. The dataset is organized by cancer type and data modality, with each subset containing the generated embeddings, metadata, and relevant labels (e.g., survival outcomes, tumor stage). Researchers and practitioners can integrate the TCGA dataset into their machine-learning pipelines using the Hugging Face datasets library and PyTorch DataLoaders. In addition to the embedding datasets, we also provide the original raw data files, accessed through the MINDS platform, and the associated code used for data extraction, preprocessing, and embedding generation.[^1]

Table 1: Number of patients in each project within The Cancer Genome Atlas (TCGA) dataset used for generating multimodal oncology embeddings.

| Projects | \# of Patients |
| :--- | :---: |
| Adrenocortical Carcinoma (ACC) | 92 |
| Bladder Urothelial Carcinoma (BLCA) | 412 |
| Breast Invasive Carcinoma (BRCA) | 1,098 |
| Cervical Squamous Cell Carcinoma \& Endocervical Adenocarcinoma (CESC) | 307 |
| Cholangiocarcinoma (CHOL) | 51 |
| Colon Adenocarcinoma (COAD) | 461 |
| Lymphoid Neoplasm Diffuse Large B-cell Lymphoma (DLBC) | 58 |
| Esophageal Carcinoma (ESCA) | 185 |
| Glioblastoma Multiforme (GBM) | 617 |
| Head and Neck Squamous Cell Carcinoma (HNSC) | 528 |
| Kidney Chromophobe (KICH) | 113 |
| Kidney Renal Clear Cell Carcinoma (KIRC) | 537 |
| Kidney Renal Papillary Cell Carcinoma (KIRP) | 291 |
| Acute Myeloid Leukemia (LAML) | 200 |
| Lower Grade Glioma (LGG) | 516 |
| Liver Hepatocellular Carcinoma (LIHC) | 377 |
| Lung Adenocarcinoma (LUAD) | 585 |
| Lung Squamous Cell Carcinoma (LUSC) | 504 |
| Mesothelioma (MESO) | 87 |
| Ovarian Serous Cystadenocarcinoma (OV) | 608 |
| Pancreatic Adenocarcinoma (PAAD) | 185 |
| Pheochromocytoma and Paraganglioma (PCPG) | 179 |
| Prostate Adenocarcinoma (PRAD) | 500 |
| Rectum Adenocarcinoma (READ) | 172 |
| Sarcoma (SARC) | 261 |
| Skin Cutaneous Melanoma (SKCM) | 470 |
| Stomach Adenocarcinoma (STAD) | 443 |
| Testicular Germ Cell Tumors (TGCT) | 263 |
| Thyroid Carcinoma (THCA) | 507 |
| Thymoma (THYM) | 124 |
| Uterine Corpus Endometrial Carcinoma (UCEC) | 560 |
| Uterine Carcinosarcoma (UCS) | 57 |
| Uveal Melanoma (UVM) | 80 |
| Total | $\mathbf{1 1 , 4 2 8}$ |

## 5 Use Cases

To validate the effectiveness of the HoneyBee framework and the generated TCGA dataset, we conducted a series of experiments assessing the quality and utility of the embeddings for various downstream machine-learning tasks. For this, we use all the clinical text data generated from the 33 cancer sites in the TCGA dataset and extracted embeddings using the GatorTron-medium [23] and BERT (bert-base-uncased) models [6]. Our primary task is to train a random forest classifier to classify the cancer type using the embeddings generated by the models. In addition to showcasing the capability of generating embeddings directly from the models, we demonstrate how parameterefficient fine-tuning can make the embedding models more suited for the task. This technique has gained popularity recently with the explosion of large language model research.

### 5.1 Extracted Embedding Analysis

We began by analyzing the quality of the extracted embeddings and visualizing them using t-SNE. This dimensionality reduction technique maps high-dimensional data to a lower-dimensional space while preserving local structure. Figures 3 and 4 show the t-SNE plots of the embeddings generated by the pre-trained and fine-tuned versions of the GatorTron and BERT models, respectively. Each point represents a patient's clinical note; the colors correspond to the different TCGA project IDs.
![](https://cdn.mathpix.com/cropped/2024_06_04_169ed6dfbaccd00d7f72g-09.jpg?height=606&width=1394&top_left_y=236&top_left_x=365)

Figure 3: t-SNE visualization of pre-trained and fine-tuned embeddings generated by the GatorTron model. Each point represents a clinical note, and colors correspond to different TCGA project IDs.
![](https://cdn.mathpix.com/cropped/2024_06_04_169ed6dfbaccd00d7f72g-09.jpg?height=608&width=1404&top_left_y=1010&top_left_x=360)

Figure 4: t-SNE visualization of pre-trained and fine-tuned embeddings generated by the BERT model. Each point represents a clinical note, and colors correspond to different TCGA project IDs.

In both cases, the fine-tuned embeddings exhibit more apparent separation between the different project IDs compared to the pre-trained embeddings, suggesting that the fine-tuning process has successfully adapted the language models to capture the nuances and distinguishing features of the various cancer types in the TCGA dataset.

We employed the adapter-based fine-tuning approach to fine-tune the models to showcase the HoneyBee framework's capability for parameter-efficient fine-tuning. In this approach, a small set of low-rank adapters are inserted into the transformer (query, key, and value) linear layers of the pre-trained model, and only these adapter layers are updated during fine-tuning. In contrast, the pre-trained model parameters remain frozen.

### 5.2 Downstream Model Training

To evaluate the utility of the generated embeddings for cancer-type classification, we trained a random forest classifier using the embeddings as input features. We split the data into training and test sets, trained the classifier on the training set, and evaluated its performance on the held-out test set. Table 2 shows the classification accuracies achieved by the random forest classifier using the pretrained and fine-tuned embeddings from both the GatorTron and BERT models. The adapter-based

Table 2: Cancer type classification accuracies using pre-trained and fine-tuned embeddings.

| Model | Pre-trained Accuracy | Fine-tuned Accuracy |
| :--- | :---: | :---: |
| GatorTron-medium | $\mathbf{0 . 9 1}$ | $\mathbf{0 . 9 8}$ |
| BERT-base-uncased | 0.78 | 0.90 |

approach performs better than pre-trained models while significantly reducing the number of trainable parameters, making it more computationally efficient for adapting large language models to specific domains.

These results demonstrate that HoneyBee is a highly effective platform for quickly testing models on their ability to extract information and that better embedding models yield better results. Moreover, thanks to its comprehensive and well-structured dataset, HoneyBee makes it easy to fine-tune these models using medical data. The use cases presented in this section highlight the potential of the HoneyBee framework in enabling the development of accurate and efficient models for analyzing cancer clinical notes and advancing precision medicine.

## 6 Discussion

The HoneyBee framework and the generated TCGA dataset demonstrate the potential of integrating multimodal data and leveraging advanced representation learning techniques for oncology research. By combining clinical data, pathology images, and radiology images, our framework enables the creation of comprehensive, machine learning-ready datasets that can drive the development of innovative models and analyses.

However, we acknowledge several limitations and challenges in the current work. First, the TCGA dataset and others accessible from the MINDS database, despite its scale and diversity, may contain biases due to the selection criteria and data collection processes. These biases could impact the generalizability of the models trained on this dataset. To mitigate this, future work should focus on incorporating additional datasets from diverse populations and conducting external validation to assess the robustness of the models. Second, integrating specific data modalities, such as genomic and proteomic data, poses unique challenges due to their high dimensionality and complexity. While we have demonstrated the effectiveness of embedding generation for clinical data and imaging modalities, further research is needed to develop optimal strategies for integrating and representing these complex data types within the HoneyBee framework. Third, the interpretability of the generated embeddings and the resulting models is a crucial aspect that requires further investigation. Understanding the biological and clinical factors captured by the embeddings is essential for trust and adoption in clinical settings. Future work should explore techniques for interpreting and visualizing the learned representations, such as attribution methods.

We outline several future directions to enhance the HoneyBee framework and address these limitations. One promising avenue is incorporating advanced data augmentation techniques, such as generative adversarial networks (GANs) [9], to generate synthetic samples and improve the robustness of the models. Additionally, exploring transfer learning approaches could enable leveraging knowledge from related tasks or domains, potentially reducing the need for large-scale annotated datasets.

Another important direction is extending the HoneyBee framework to other disease areas beyond oncology. The modular nature of our framework allows for the integration of disease-specific data modalities and the adaptation of foundational models to capture relevant biological and clinical factors. Collaborations with domain experts and the incorporation of domain-specific knowledge will be crucial in expanding the applicability of HoneyBee to various medical fields.

Moreover, integrating the HoneyBee framework with clinical workflows and decision support systems is a critical challenge that needs to be addressed. Developing user-friendly interfaces, ensuring data security and privacy, and validating the models in real-world clinical settings are essential steps toward the practical implementation of our framework.

## 7 Conclusion

This paper introduced HoneyBee, a scalable modular framework for creating multimodal oncology datasets with embeddings generated from raw medical data using foundational models. HoneyBee addresses the need for large-scale, high-quality, machine learning-ready datasets in the oncology domain by integrating various data modalities, employing state-of-the-art embedding generation techniques, and providing structured storage and accessibility using Hugging Face datasets and vector databases. Through extensive experimental evaluation, we demonstrated the effectiveness of the HoneyBee framework in generating representative embeddings that capture meaningful patterns and relationships within the data. The HoneyBee datasets showed promising results on various downstream tasks, such as survival analysis, treatment response prediction, and biomarker discovery, highlighting their value for advancing oncology research and clinical applications. The HoneyBee framework is designed to be scalable, modular, and extensible to other medical domains. It fosters the creation of comprehensive datasets and drives the adoption of foundational models in healthcare. By providing high-quality, machine learning-ready datasets, HoneyBee aims to accelerate the development of innovative solutions for cancer diagnosis, prognosis, and treatment planning. The HoneyBee framework and datasets will be valuable resources for the machine learning and oncology research communities. We encourage researchers and practitioners to explore and utilize the HoneyBee datasets and contribute to the framework's ongoing development and improvement.

## References

[1] Shekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa, Sebastien Baur, Simon Kornblith, Ting Chen, Nenad Tomasev, Jovana Mitrović, Patricia Strachan, et al. Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging. Nature Biomedical Engineering, 7(6):756-779, 2023.

[2] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):423-443, 2018.

[3] Richard J Chen, Tong Ding, Ming Y Lu, Drew FK Williamson, Guillaume Jaume, Andrew H Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, et al. Towards a generalpurpose foundation model for computational pathology. Nature Medicine, 30(3):850-862, 2024.

[4] Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng, Ruisi Zhang, Ruoyu Zhang, Meng Zhou, Penghui Zhu, and Pengtao Xie. Meddialog: a large-scale medical dialogue dataset. arXiv preprint arXiv:2004.03329, 2020.

[5] Data coordination centre Kasprzyk (Leader) Arek 1 Stein (Leader) Lincoln D. 1 Zhang Junjun 1 Haider Syed A. 98 Wang Jianxin 1 Yung Christina K. 1 Cross Anthony 1 Liang Yong 1 Gnaneshan Saravanamuttu 1 Guberman Jonathan 1 Hsu Jack 1 et al. International network of cancer genome projects. Nature, 464(7291):993-998, 2010.

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arxiv. arXiv preprint arXiv:1810.04805, 2019.

[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[8] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. arXiv preprint arXiv:2401.08281, 2024.

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.

[10] Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics, $37(15): 2112-2120,2021$.

[11] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. arXiv preprint arXiv:2009.13081, 2020.

[12] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175-184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

[13] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. A survey on deep learning in medical image analysis. Medical image analysis, 42:60-88, 2017.

[14] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021.

[15] Clara H McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Effective transfer learning for identifying similar questions: matching user questions to covid19 faqs. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining, pages 3458-3465, 2020.

[16] Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep patient: an unsupervised representation to predict the future of patients from the electronic health records. Scientific reports, 6(1):1-10, 2016.

[17] Martha Quinn, Jane Forman, Molly Harrod, Suzanne Winter, Karen E. Fowler, Sarah L. Krein, Ashwin Gupta, Sanjay Saint, Hardeep Singh, and Vineet Chopra. Electronic health records, communication, and data sharing: challenges and opportunities for improving the diagnostic process. Diagnosis, 6(3):241-248, 2019.

[18] Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20730-20740, 2022.

[19] Aakash Tripathi, Asim Waqas, Kavya Venkatesan, Yasin Yilmaz, and Ghulam Rasool. Building flexible, scalable, and machine learning-ready multimodal oncology datasets. Sensors, 24(5), 2024.

[20] Asim Waqas, Aakash Tripathi, Ravi P Ramachandran, Paul Stewart, and Ghulam Rasool. Multimodal data integration for oncology in the era of deep neural networks: a review. arXiv preprint arXiv:2303.06471, 2023.

[21] John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, and Joshua M Stuart. The cancer genome atlas pan-cancer analysis project. Nature genetics, 45(10):1113-1120, 2013.

[22] Joachim Wolff. Approximate nearest neighbor query methods for large scale structured datasets. 2016.

[23] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records. NPJ digital medicine, 5(1):194, 2022.

[24] Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100, 2023.


[^0]:    * Corresponding Author

    ${ }^{\dagger}$ Also part of the Department of Electrical Engineering, University of South Florida, Tampa, FL, 33620

[^1]:    ${ }^{3}$ https://huggingface.co/datasets/Lab-Rasool/TCGA

