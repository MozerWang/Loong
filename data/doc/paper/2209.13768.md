# Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines 

Mino Woo ${ }^{1,2}$, Terry Jordan ${ }^{1}$, Robert Schreiber ${ }^{3}$, Ilya<br>Sharapov ${ }^{3}$, Shaheer Muhammad ${ }^{3}$, Abhishek<br>Koneru $^{3}$, Michael James ${ }^{3^{*}}$ and Dirk Van Essendelft ${ }^{1 *}$<br>${ }^{1}$ National Energy Technology Laboratory, Morgantown, 26505,<br>WV, United States.<br>${ }^{2}$ Oak Ridge Institute for Science and Education, Oak Ridge,<br>37830, TN, United States.<br>${ }^{3}$ Cerebras Systems Inc, Sunnyvale, 94085, CA, United States.<br>*Corresponding author(s). E-mail(s): michael@cerebras.net;<br>dirk.vanessendelft@netl.doe.gov;


#### Abstract

We present a high-level and accessible Application Programming Interface (API) for the solution of field equations on the Cerebras Systems Wafer-Scale Engine (WSE) with over two orders of magnitude performance gain relative to traditional distributed computing approaches. The domain-specific API is called the WSE Field-equation API (WFA). The WFA outperforms OpenFOAM ${ }^{\circledR}$ on NETL's Joule 2.0 supercomputer by over two orders of magnitude in time to solution. While this performance is consistent with hand-optimized assembly codes, the WFA provides an easy-to-use, high-level Python interface that allows users to form and solve field equations effortlessly. We report here the WFA programming methodology and achieved performance on the latest generation of WSE, the CS-2.


Keywords: Cerebras, NETL, Wafer Scale Engine, Stencil, Computational Fluid Dynamics

## 1 Introduction

According to the principle of locality, an object is directly influenced by its immediate surroundings. Outside of limited quantum phenomena, ${ }^{1,2}$ the principal of locality holds true for fundamental forces ${ }^{3}$ and is the foundation of a mathematical construct called a field. A field represents any physical quantity that has a value at every point in spacetime. ${ }^{4}$ Examples include gravitational and electromagnetic fields. The principle of locality and the field concept underpin sets of partial differential equations (known as field equations), which describe most phenomena observed around us.

To solve these equations on computers, practitioners divide space into discrete control volumes (often called cells or voxels) where each field is approximated by a single average value (Fig 1a). The terms grid and mesh refer to the entire set of control volumes. Differences between neighboring cells provide approximations of the field's gradients. Field equations specify sparse relations among field values and field gradients that hold over all spacetime.

Computations that accurately model complex physical phenomena can be very large, requiring a prohibitively long time to solve, unless high-performance computing (HPC) systems comprising thousands of processing nodes are employed. To distribute the computation between multiple nodes, practitioners apply domain decomposition methods ${ }^{5}$ with individual nodes solving the problem over distinct physical regions (Fig. 1b, left).

Applying computational resources in parallel increases the performance, measured as the amount of aggregate computational work divided by the time taken to solve the problem. There are two common strategies to scale workloads on a distributed system. First, strong scaling, which keeps the problem size in the numerator fixed while applying additional processor nodes, decreases the time to solution. Second, in contrast with weak scaling, the problem size increases proportionally with the node count, while the resulting time to solution in the denominator stays roughly constant. ${ }^{a}$

The system can only achieve high levels of utilization when the rate of data exchange between nodes handling neighboring subdomains keeps up with the computations performed on each node. This limitation poses a challenge for strong scaling. As a fixed-sized grid is distributed over a growing number of nodes, each processor handles fewer cells, and the ratio of communication to computation grows with the surface-to-volume ratio of subdomains. The attainable communication to computation ratio is set in silicon during system design. For distributed computing, this ratio is relatively low. Communication limits result in distinct performance degradation in strong scaling plots such as those in Fig. 5, which shows a log-log plot of compute time per iteration as a function of the number of grid cells per core. The linear region with a slope of -1 at high workload per core shows that processors remain efficiently utilized.[^0]

The distinct point where performance falls off this trend as workload per core decreases marks the end of strong scaling and the point where communication begins to dominate computation rate.

In recent decades, the gap between the computation and communication capabilities of HPC systems has been widening. ${ }^{6}$ At the node level, the improvements in processing rates have outpaced the advances in memory and IO bandwidth, increasing the minimum cell count per node for balanced computations. The problem for strong scaling is further exacerbated by cross-node interconnects whose latency and bandwidth characteristics penalize fine-grain parallelization. ${ }^{7}$ As a result, over the past two decades and for most computational fluid dynamics (CFD) applications, strong scaling has failed for workloads that use fewer than $\sim 10,000-15,000$ cells per core. Further, upcoming generations of distributed HPC architectures show few signs of significant progress in strong scaling capability (See Section 3.1.3 for detailed discussion).

The Wafer-Scale Engine (WSE) departs from this trend. The latest generation WSE consists of 850,000 cores manufactured on a single 12-inch wafer with over 2.6 trillion transistors. It is packaged in a system less than a meter high and a rack wide. The WSE requires less than 1 percent of the space and power of NETL's Joule 2.0, ${ }^{b}$ yet has comparable peak performance and much higher algorithmic performance. The "tiles" on the WSE minimize the fundamental components of a traditional HPC node. Each tile has a Turing-complete, multi-gigaflop-capable processor, a local memory, and a router for communication with neighboring tiles. Tiles form a 2D array with local communication paths between all Cartesian neighbors. Every tile can act independently with its own program code or as a group for collective operations. Tiles are a few hundred microns in size. The small size allows each processor to access its own memory within a single clock cycle and a neighbor's within a single fabric hop (also single cycle). These properties minimize latency to local and neighbor data and provide tens of petabytes per second of aggregate bandwidth. Further, the miniaturization means that all necessary data movement is done over nanowire connections which minimizes energy consumption. ${ }^{8}$

There is no memory hierarchy on the WSE. All memory is static random access memory local to each processor and contained within the tile. Each processor can read 128 bits from and write 64 bits to its own memory on each cycle, and each processor can support up to eight operations per cycle depending on precision and configuration. In addition, on each cycle, each tile can simultaneously send a 32 -bit value to the interconnect and receive a 32-bit value from the interconnect. The results in this paper show that this balanced approach to computing makes strong scaling eminently feasible for reduction-free methods and significantly improves methods with reductions.[^1]a
![](https://cdn.mathpix.com/cropped/2024_06_04_34ad043f48d1349b8c6ag-04.jpg?height=470&width=1130&top_left_y=185&top_left_x=211)

Fig. 1 (a) A magnetic field ${ }^{9}$ (left) and its approximation through discretization in a field equation computer model (right). (b) The parallel computation strategy of the current generation of distributed computing and WSE.

### 1.1 The Heat Equation

The heat equation is among the simplest field equations and follows Fourier's law as seen in Eq. 1. Because of its simplicity, the heat equation is often used as a comparative test for field equation modeling software.

$$
\begin{equation*}
\frac{\partial T}{\partial t}=\alpha \nabla^{2} T=\alpha\left(\frac{\partial^{2} T}{\partial x^{2}}+\frac{\partial^{2} T}{\partial y^{2}}+\frac{\partial^{2} T}{\partial z^{2}}\right) \tag{1}
\end{equation*}
$$

The scalar field $T$ evolves in Cartesian spacetime $(x, y, z$, and $t$ ) with thermal diffusivity $\alpha$.

### 1.1.1 The Explicit Formulation

The forward time, centered space method applied to the heat equation on a uniform Cartesian grid results in Eq. 2.

$$
\begin{gather*}
T_{C}^{n+1}=\omega \sum_{D \in \mathcal{N}(C)}\left(T_{D}^{n}\right)+(1-6 \omega) T_{C}^{n} \quad \forall C \notin \mathrm{bc} \\
T_{C}^{n+1}=T_{C}^{n}=\gamma \quad \forall C \in \mathrm{bc}  \tag{2}\\
\omega=\alpha \Delta t \Delta l^{-2}
\end{gather*}
$$

Forward time methods track a fixed spatial grid as it evolves over time. $T_{C}^{n}$ denotes the field $T$ in cell $C$ at time $n$. The next time step $T_{C}^{n+1}$ is a weighted average of a Cartesian neighborhood of the current time step. $\mathcal{N}(C)$ is the neighborhood operator which defines the set of cells included in the neighborhood. For first-order formulations, $\mathcal{N}(C)$ includes the cells to the bottom $(B)$, top $(T)$, east $(E)$, west $(W)$, north $(N)$ and south $(S)$ of the cell under consideration. The weighting depends on diffusivity $\alpha$, time step $\Delta t$, and cell length $\Delta l$. Cells at the boundary, bc, of the simulation are constrained
to a constant value $(\gamma)$ in spacetime. The equation set is stable when the of diagonal constant $(\omega)$ is less than $\frac{1}{6}$. All testing adopts a $\omega$ of $\frac{1}{10}$.

This simple field equation illuminates several key aspects of the hardware ecosystems used to solve it. This equation has low arithmetic intensity. ${ }^{c}$ Processors that are separate from their main memory (e.g., with memory on physically independent RAM chips) are particularly limited in this scenario because data cannot be moved between memory and the processor fast enough to sustain peak arithmetic capabilities. Memory hierarchies such as on-chip caches can only mitigate this problem when data is reused. For field equations, the data is only reused after the entire field is accessed. At useful field resolutions this renders caching strategies ineffective. Therefore, modern Central Processing Units (CPUs) and Graphics Processing Units (GPUs) are both limited by memory bandwidth on these problems. In contrast, the WSE does not use a memory hierarchy. All its memory interleaves with processing cores, allowing it to process field equations without bandwidth limitations.

### 1.1.2 The Implicit Formulation

The backward time, centered space method applied to the heat equation on a uniform Cartesian grid results in Eq. 3.

$$
\begin{gather*}
T_{C}^{n+1}-\omega \psi \sum_{D \in \mathcal{N}(C)}\left(T_{D}^{n+1}\right)=\psi T_{C}^{n} \quad \forall C \notin \mathrm{bc} \\
\psi=(1+6 \omega)^{-1}  \tag{3}\\
T_{C}^{n+1}=T_{C}^{n}=\gamma \quad \forall C \in \mathrm{bc}
\end{gather*}
$$

Eq. 3 represents a set of linear equations of the form $\mathbf{A} x=b$. $\mathbf{A}$ is a sparse matrix with the characteristic sparsity pattern found in Cartesian grid decomposition. ${ }^{5}$ The diagonal is unity and non-zero off-diagonals share the same value $(-\omega)$. A single multiplication of $T_{C}$ by the constant $\psi$ is the only formation operation needed as $\mathbf{A}$ is constant. The constant, $\omega$, has the same definition as in Eq. 2. The Conjugate Gradient (CG) solver is applicable here as $\mathbf{A}$ is a symmetric, positive-definite matrix. ${ }^{10}$

The CG solver is one of a class of related solvers known as Krylov Subspace solvers. In these solvers, all-reduce operations are used to determine search step magnitudes. In the classic CG algorithm, two reductions per iteration are needed. They are a bottleneck at high parallelism. Solution progress must halt while reductions are completed, which makes reduction time critical to performance for this class of solvers. The classical CG solver iteration time was benchmarked in this study to illustrate the performance differences in reduction-dominated algorithms.[^2]

## 2 WFA Programming Approach

The WFA offers a simple and intuitive Python user interface in a NumPylike style. It compiles the Python code into an executable that can run on WSE hardware and in Cerebras' visual debugger, Portrait. Fig. 2a outlines this workflow, and section 2.1 gives examples of program code.
![](https://cdn.mathpix.com/cropped/2024_06_04_34ad043f48d1349b8c6ag-06.jpg?height=910&width=1164&top_left_y=498&top_left_x=190)

C

![](https://cdn.mathpix.com/cropped/2024_06_04_34ad043f48d1349b8c6ag-06.jpg?height=533&width=533&top_left_y=879&top_left_x=811)

Tile Type

Moat $\square$ Worker

Control Transmission

$\longrightarrow$ Control wavelet $\longrightarrow$ Reduce

Control

$\longrightarrow$ Broadcast

Fig. 2 Schematic overview of the computation strategy in the WFA (a), RPC streaming to execute vector operations in the WSE (b), and data streaming for central reductions (c).

As shown in Fig. 2b, the WFA organizes groups of tiles with specific functions. To run a program, the WFA uses Remote Procedure Calls (RPCs) enacted through a bytecode sequence that is sent to an array of tiles consisting of Worker and Moat tiles. The Control Tile dynamically interprets the bytecode sequence generated at compile time, generates RPCs, and broadcasts these through the Control Transmission tiles to the Worker and Moat tiles. Each RPC launches a coordinated set of kernels on Moats and Workers to complete a portion of a program. Workers handle computation in the central domain. Moats surround the Workers and handle the edge requirements of
computation tasks. These tasks could be boundary cell computations, catching data coming out of the Worker group's edge, or sending appropriate data to Workers on the edge to correctly complete a tensor operation without stalls or hangs. The WFA has RPCs for common scalar/tensor operations and a growing library of RPCs for explicit and implicit solvers.

This control strategy separates the main program code from the kernel code associated with RPCs (stored on Workers and Moats), which minimizes code space on the Worker and Moats. Since there is significant memory space on the Control Tile and Control Transmission tiles, large programs can fit on the WSE without need for host-WSE interaction. Further, even larger programs can be streamed through the twelve $100 \mathrm{~Gb}$ ethernet ports. A single $100 \mathrm{~Gb}$ ethernet port has enough bandwidth to support bytecode streaming for large and complex programs. However, this strategy has yet to be developed in the WFA because the Control Tile has more than enough memory to house the bytecode sequence for a simple CFD simulation.

### 2.1 Example Code

The first-generation WFA uses the domain decomposition approach shown in Fig. 1b. Space is discretized into a uniform Cartesian grid, and each tile handles a column of control volumes. This decomposition strategy acts as proof of concept. Fully unstructured decompositions will be supported in future versions of the WFA.

Fig 3 shows two example implementations of the explicit heat equation in the WFA. The left side shows the general-purpose implementation, which specifies a for loop with a number of vector operations that complete an explicit time step. Notice that the definition is exactly as written in Eq. 2.

```
from WSE_FE.WSE_Interface import WSE_Interface
from WSE_FE. WSE_Array import WSE_Array
from WSE_FE.WSE_Loops import WSE_For_Loop
import numpy as np
\# Instont1ate the WSE Interface
wse $=$ WSE Interface(
\# defince constants
$c=0.1$
center $=1.0-6.0 * c$
* Create the initial temperature field and BC's
T_init $=$ np.ones $((102,102,102)) * 500 . \theta$
T_init $[1:-1,1:-1,0]=300.0$
$T_{\text {init }[1:-1,1:-1,-1]}^{-1}=400.0$
\# Instantiate the WSE Array objects needed
\# Loop over time
with WSE For Loop('time Loop', 40000):
    T_n $[1:-1,0,0]=$ center * T_n[1:-1, 0,0$]$
        $+c^{*}\left(T \_n[2:, 0,0]+T \_n[:-2,0, \theta]\right.$
            + T_n[1:-1, 1, 0] + T n[1:-1, 0, -1]
            $\left.+T_{-} n[1:-1,-1, \theta]+\bar{T} \_n[1:-1, \theta, 1]\right)$
wse.make_WSE(answer=T_n)
```

Fig. 3 Sample WFA code for the explicit solution of the heat equation

In the example on the left in Fig. 3, a temperature field is initialized in a traditional NumPy array, Tinit. A WSE_Array object, T_n, is created from
the initialization array. WSE_Array objects are very similar to NumPy arrays, but the indexing is relative to local data and processor position. Local vector slicing is accomplished in the first axis. The second and third axis specify relative tile position in the $\mathrm{X}$ and $\mathrm{Y}$ directions, respectively. Values of $-1,0$, and 1 in these axes specify $W / S, C$, and $E / N$ directions, respectively. While the WFA does not currently understand more than these limited neighborhood specifications, there is nothing limiting the ability to specify data farther away than the immediate local neighborhood other than implementation time. The WFA contains all the necessary functionality to write kernels for common numerical methods in Python's object-oriented programming language.

It is also possible to create complex kernels which do more than a single scalar/tensor operation. The sample on the right does the same explicit time step in a single RPC. In this case, it was possible to reduce overhead. ${ }^{d}$ The ability to customize kernels allows users to optimize performance-critical code, implement custom communication patterns such as the central reduction shown in Fig 2c, and overlap communication and computation. Custom kernels can be used together with general purpose expressions, which is valuable for making a flexible and high-performance field equation solver.

The WFA has a validation capability that can run the field equation code in NumPy and debugging capability that can be used to track data transformations on any tile ( $\mathrm{Z}$ column in NumPy Arrays) at every operation. This can be used to track the same data transformations in the Cerebras visual debugger, Portrait. Finally, the entire program is compiled into WSE compatible binaries that can be shipped to an available WSE. In this way, the ease of programmability in NumPy-like Python programming is coupled with exceptional performance that matches or exceeds the performance of past hand-optimized WSE programming efforts. ${ }^{11,12}$

## 3 Measured Performance

We compared the performance of the WFA on the WSE with that of OpenFOAM ${ }^{\circledR}$ (hereafter referred to as OpenFOAM) on Joule 2.0. OpenFOAM is an open-source, distributed, CFD code that is commonly used in industrial modeling and research. OpenFOAM was used as a basis for comparison because it is open-source, has single-precision support, and is often used to solve industrial problems of similar size to those possible on the WSE. As closely as possible, the same algorithms were implemented in the WFA and in OpenFOAM.

Fig. 4 shows the performance of the WFA on the WSE against a custombuild version of OpenFOAM $8^{13,14}$ on NETL's Joule 2.0 supercomputer. Fig. 4a shows the measured iteration speed of Eq. 2 for each time step. Fig. 4b shows the measured inner iteration speed of the classic CG algorithm.[^3]

The code in OpenFOAM is complex, as it involves traditional halo data ${ }^{e}$ exchange and all-reduce operations, expressed through the Message Passing Interface (MPI) for distributed memory parallelization. The code in the WFA is relatively simple and can achieve parallelization on the WSE with far less code than what was required to implement an efficient distributed OpenFOAM code.

Each result of the sparse matrix-vector product for the heat equation combines the center value and the sum of six neighboring values scaled with the diagonal and off-diagonal coefficients, respectively. The WFA implementation uses two temporaries for this computation, one to store an element-wise product of the diagonal constant with the center value, and the other to store the sum of neighboring values. This sum is initialized by performing an elementwise addition of locally stored top and bottom cell data. Then, neighbor data is added to the off-diagonal sum by launching background threads that simultaneously send data to and receive data from four neighboring tiles in $W \rightarrow C \rightarrow E, N \rightarrow C \rightarrow S, E \rightarrow C \rightarrow W$, and $S \rightarrow C \rightarrow N$ directions. Once the sum of six neighboring values is computed, a single fused multiplyaccumulate (FMAC) instruction scales it with the off-diagonal coefficient and adds it to the previously stored scaled center value. This implementation is very lean and efficient because in Cerebras' architecture the FMAC instruction takes one cycle, and each background thread is a single machine instruction.

In contrast, the sparse matrix-vector product in OpenFOAM cannot be briefly described at the machine instruction level. It is implemented with coarser building blocks including: C++ libraries, MPI interfaces, and host network stack. It does local calculations on owned data, exchanges neighbor data through MPI, and then updates boundary cells with neighbor data. While these steps are similar, the implementation in code is much more complex and lengthy than what is described for the WFA above.

All-reduce operations were handled via standard calls to the MPI library in OpenFOAM. A reduction-to-center strategy was implemented within the WFA to handle reductions as shown in Fig 2c.

The WSE can achieve scaling even under the worst possible communication to compute ratios. In traditional computing, practitioners use strong scaling to find the point where communication rate fails to keep up with computation rate. Remarkably, this never happens on the WSE, even as the ratio of communication to computation is maximized. The sparse matrix vector multiplication (SpMV) is independent of workload per processor. Instead, there are limits related to the overhead of setting addresses, vector lengths, and starting threads, which is expressed as the constants in the denominators of the performance models in Eqs. 6 and 16. Therefore, our design is optimized for simplicity. It uses a $1 \times 1 \times \mathrm{Z}$ domain decomposition that places every cell on four communication boundaries. The single-cycle, balanced computation[^4]![](https://cdn.mathpix.com/cropped/2024_06_04_34ad043f48d1349b8c6ag-10.jpg?height=496&width=1174&top_left_y=192&top_left_x=162)

Fig. 4 Performance analysis of WSE (green) and distributed computing weak scaling (blue) in explicit (a) and implicit (b) formulations at various values of workload per processor (W).

and communication rate allows all vector operations to proceed at half to single cycle rates regardless of data placement. The WFA achieves perfect weak scaling in the explicit case and shows no dependency on the workload per processor as seen in Eq. 6. This allows strong scaling to proceed unimpeded by bandwidth and latency, which is not possible on current HPC systems.

Algorithms with reductions that can't be overlapped with computation are different (such as classic CG). Global reductions proceed in proportion to the sum of the fabric extents (the $X+Y$ term in Eq. 16 as illustrated in Fig. 2c). Each hop costs 1 cycle. This imposes a per-iteration latency that is independent of the workload per processor, and results in weak scaling that is not flat as processor allocation grows.
![](https://cdn.mathpix.com/cropped/2024_06_04_34ad043f48d1349b8c6ag-10.jpg?height=388&width=1150&top_left_y=1348&top_left_x=173)

Fig. 5 Explicit (a) and implicit (b) strong scaling of OpenFOAM on Joule 2.0

### 3.1 Explicit Formulation Performance

Eq. 2 was implemented in both the WFA and OpenFOAM. The explicit loop time was recorded for the implementations on the WSE and on Joule 2.0, then compared.

### 3.1.1 OpenFOAM on Joule 2.0

It is important to determine the correct workload per core, as it impacts processor utilization and achievable speed. Workload per core is determined through strong scaling at the mesh size of interest. The workload per processor is chosen at the point where linear strong scaling stops, as this represents the fastest iteration rate that also has high processor utilization. Strong scaling was done at both ends of the industrially relevant zone ( 5.8 and 46.7 million cells) as shown in Fig. 5a. This workload per processor was then used in a weak scaling study to generate competitive benchmarks (blue data in Fig. 4a). The scaling studies used regular hexahedral cells and the "simple" decomposition method, which creates hexahedral subdomains.

Table 1 OpenFOAM Explicit Performance on Joule 2.0.

|  | $W=4,096$ |  | $W=15,625$ |  |
| ---: | :---: | :---: | :---: | :---: |
|  | Fastest | Slowest | Fastest | Slowest |
| Iteration Speed (1/s) | 13,862 | 3,535 | 4,263 | 2,027 |
| Cell Count | $1.31 \times 10^{6}$ | $4.01 \times 10^{7}$ | $5.00 \times 10^{6}$ | $1.51 \times 10^{8}$ |
| Core Count | 320 | 9800 | 320 | 9680 |
| Nodes | 8 | 245 | 2 | 242 |
| Speed Ratio | 3.9 |  |  | 2.1 |

The measured performance in OpenFOAM on Joule 2.0 decays linearly with the total number of cells as seen in Eqs. 4 and 5 for the $W=4096$ and $W=15625$ cases, respectively. Performance parameters are summarized in Table 1 .

$$
\begin{gather*}
R_{i}(W=4096)=1.36 \times 10^{4}-2.55 \times 10^{-4} N_{c}  \tag{4}\\
R_{i}(W=15625)=4.20 \times 10^{3}-1.37 \times 10^{-5} N_{c} \tag{5}
\end{gather*}
$$

In the $W=4096$ case, performance dropped by 3.9 times when scaling from 320 to 9800 cores. This is expected, given the small workload per processor. Typical workload ranges are 10,000-15,000 cells per processor. The $W=15625$ case scaled almost twice as well with a performance reduction of only 2.1 times from 320 cores to 9680 cores. It is likely that explicit weak scaling could be improved further with substantial optimization efforts, but that is beyond the scope of this investigation.

### 3.1.2 The WFA on the WSE

When scaling with constant workload per processor (weak scaling), the iteration time remains constant as shown in Fig. 4, implying that the compute rate grows linearly with the number of processors. Using timings generated by the Cerebras hardware simulator, we developed a "roofline" model (Eq. 6) that shows the scaling trend. We used iterations per second as our metric. For weak scaling, it is more important to measure the number of iterations
per second (when scaling up processors at constant workload) than the rate of floating point arithmetic. According to the calibrated model, the achievable iterations rate $\left(R_{i}\right)$ depends only on the clock frequency $\left(F_{c}\right)$ and the workload per processor $(W)$, and not on the scale (i.e., the number of processors). This is perfect weak scaling.

The WFA achieves high utilization on the WSE. At these utilization levels, application performance is limited by power delivery. The single-precision WFA workload is strikingly different than the half-precision workloads typical in artificial intelligence (AI) applications, which the WSE's default power settings are tuned for. Therefore, we tuned the power settings (including processor clock, current, and voltage) to maximize performance. With this tuning, the actual hardware ran at speeds close to those predicted by the simulation-informed model. We intend to integrate these power settings into a future release of the WSE platform software. The power tuning results in an approximately 30 percent improvement in performance.

$$
\begin{equation*}
R_{i}=\frac{F_{c}}{6.5 W+78} \tag{6}
\end{equation*}
$$

The WFA has the ability to scale to just 50 cells per processor while almost achieving the ideal performance in Eq. 6. The WSE is capable of doing 8 single precision flops of work in 6.5 cycles due to additions being done with simultaneous instruction, multiple data instructions and the FMAC. 78 cycles of overhead were involved in setting vector lengths and addresses. It may be possible to reduce this overhead through further optimizations. By contrast, the distributed case in OpenFOAM needed over 15,000 cells to scale well. The difference in scaling capability results in a $470 \times$ improvement in iteration speed. The WSE attains these speeds because of the balanced compute to memory and fabric access speeds, which allows it support low arithmetic intensity operations. The stark difference in attainable speed illustrates the severe constraints imposed by the latency and bandwidth limitations in the memory and network subsystems of distributed computers during the solution of field equations.

### 3.1.3 Limits of Possible Distributed Performance

GPUs are the state of the art in modern HPC because the aggregate device memory bandwidth is much higher than CPUs, which can make them faster at a device level. However, this does not necessarily translate to greater speed at the distributed system level. Fisher et. al. presented a first-principles model of distributed performance for field equation solvers. ${ }^{15}$ For explicit steps, Eq. 7 shows Fischer's model, where the minimum time per iteration $\left(t_{\mathrm{i}}^{\mathrm{min}}\right)$ is a function of the time to compute the inner portion of the local subdomain ( $\left.t_{\text {comp }}\right)$, the time to communicate boundary data $\left(t_{\text {comm }}\right)$, and the time to update the boundary cells $\left(t_{\mathrm{b}}\right)$.

$$
\begin{equation*}
t_{\mathrm{i}}^{\min }=\max \left(t_{\mathrm{comp}}, t_{\mathrm{comm}}\right)+t_{\mathrm{b}} \tag{7}
\end{equation*}
$$

Eq. 7 models concurrent boundary data exchange with computation over the subdomains' interiors followed by computation over subdomains' boundaries. Communication time tends to increase with the number of applied processors due to the effects of congestion, decomposition, and topology. Therefore, good weak scaling is only achieved when $t_{\text {comm }}<t_{\text {comp }}$ at the largest number of applied processors of interest. When this condition is met, $t_{\mathrm{i}}^{\mathrm{min}}$ is proportional to subdomain size and inversely proportional to memory bandwidth. In this regime, increasing memory bandwidth must be matched by a larger subdomain size per GPU to maintain the scaling constraint ( $\left.t_{\text {comm }}<t_{\text {comp }}\right)$. Over the past decade, communication times have been marginally decreasing, while memory bandwidth has been significantly increasing. This has led to large increases in workload per GPU but little (if any) progress in time to solution. To illustrate this point, we survey literature for similar problems, report weak scaling domain size, and calculate maximum possible iteration rate in Table 2.

Table 2 Literature survey of weak scaling field equation solvers and estimated maximum computing rate

| Study | Subdomain <br> Width <br> (Cells) | W <br> (Cells) | Processor | $R_{\mathrm{it}}^{\max }$ <br> (iters $/ \mathrm{sec}$ ) |
| :---: | :---: | :---: | :---: | :---: |
| Pfister et. al. ${ }^{16}$ | 300 | $3.28 \times 10^{7}$ | V100 | 4167 |
| Rass et. al. ${ }^{17}$ | 383 | $5.62 \times 10^{7}$ | P100 | 1557 |
| Rass et. al. ${ }^{17}$ | 512 | $1.34 \times 10^{8}$ | V100 | 838 |
| Rass et. al. ${ }^{17}$ | 512 | $1.34 \times 10^{8}$ | A100 | 1863 |
| Xue et. al. ${ }^{18}$ | 256 | $1.68 \times 10^{7}$ | P100 | 5215 |
| Xue et. al. ${ }^{18}$ | 256 | $1.68 \times 10^{7}$ | V100 | 6706 |
| Pearson et. al. ${ }^{19}$ | 750 | $4.22 \times 10^{8}$ | V100 | 267 |

The maximum possible iteration rate, when communication time is properly hidden ${ }^{f}$, can be determined by following the methods proposed by Rass et. al. ${ }^{20}$ Their method for assessing the performance of field equation solvers is shown in Eqs. 8 and 9. The effective memory bandwidth ( $T_{\text {eff }}$ in Eq. 9) is related to memory footprint for unknown $\left(D_{u}\right)$ and known $\left(D_{k}\right)$ variables as well as the iteration time $\left(t_{\mathrm{it}}\right)$. For memory bound algorithms, this is a good metric for performance because it relates effective memory bandwidth to peak memory bandwidth. It is also a good measure for distributed computing because $T_{\text {eff }}$ will remain high when communication is properly hidden.

$$
\begin{equation*}
A_{\text {eff }}=2 D_{u}+D_{k} \tag{8}
\end{equation*}
$$[^5]

$$
\begin{equation*}
T_{\mathrm{eff}}=\frac{A_{\mathrm{e} f f}}{t_{\mathrm{it}}} \tag{9}
\end{equation*}
$$

Further, this method can estimate the maximum possible iteration rate directly from the subdomain size. Substituting $A_{\text {eff }}$ into Eq. 9 and solving for $t_{\text {it }}$, we obtain Eq. 10. We now find a lower bound for $t_{\text {it }}$ with aggressively optimistic assumptions. We take $D_{k}=0, D_{u}=1$, and assume the computation proceeds in single-precision representation while also achieving the full peak memory bandwidth $\left(w_{\mathrm{m}}\right)$. This reduces iteration time to Eq. 11. The maximum possible iteration rate is then the inverse of the iteration time (Eq. 12).

$$
\begin{gather*}
t_{\mathrm{it}}=\frac{2 D_{u}+D_{k}}{T_{\mathrm{eff}}}  \tag{10}\\
t_{\mathrm{it}}^{\min }=\frac{8 W}{w_{\mathrm{m}}}  \tag{11}\\
R_{\mathrm{it}}^{\max }=\left(t_{\mathrm{it}}^{\min }\right)^{-1}=\frac{w_{\mathrm{m}}}{8 W} \tag{12}
\end{gather*}
$$

The inferred maximum iteration rates in Table 2 are all comparable to or less than those measured in OpenFOAM on Joule 2.0's CPU nodes. The workloads per GPU are also consistent with our own experience. There are two reasons why GPU performance is not significantly higher. First, single device scaling on GPUs is relatively poor. Rass et. al. report that $T_{\text {eff }}$ does not peak for single-device scaling until $W$ is greater than $128^{3}$. While GPU bandwidth is high, the latency is also high. Little's law dictates that a large amount of data needs to be in flight to keep utilization high when both latency and bandwidth are high. Sustaining significant amounts of data in flight translates to large subdomain sizes. These single device scaling properties, limit attainable iteration rates on GPUs. On the other hand, the WSE has L1 cache bandwidths and single cycle latency, thus the attainable iteration rates on each processor are much higher.

Second, $t_{\text {comm }}$ is a not-necessarily-linear combination of all the boundary traffic communication times. Boundary communication times are affine functions of the ratio of message size to available bandwidth and latency. Available bandwidths tend to be low because message sizes are relatively small. Further, latencies can be large depending on network topology, traffic, and congestion. Traffic and congestion can have a large, negative impact on latencies and bandwidths. ${ }^{21}$ These factors drive $t_{\text {comm }}$ up as more processors are utilized, which can only make iteration rate drop. We have little reason to believe that the attainable iteration rates with this specific problem will be much different than those reported in Table 2. ${ }^{g}$ By way of comparison, the WSE inter-processor[^6]bandwidth matches the memory bandwidth, and inter-processor latency is single cycle. This is necessary to sustain low intensity operations between local and neighbor data at small workloads per processor. Further, recruiting more processors does not change anything in the communication pathways, as everything is local and no resources are shared as more cores are recruited. These properties are foundational to the ideal weak scaling observed in the WFA in Fig. 4a.

### 3.1.4 Measured WSE Power Efficiency

At the largest fabric sizes tested in this study $(750 \times 950)$, the WSE sustained an average of $24.6 \mathrm{~kW}$. The computation rate per unit power was calculated to be between 32 and 35 gigaflops per watt. This is comparable to the measured LINPACK power efficiencies reported on the Top 500 list. ${ }^{23,24}$ However, the reported values for the WFA are for real application performance, not a benchmark designed to maximally stress node level components with compute to communication ratios that are not relevant to solving field equations. It is well known that the maximum attainable flop rates in the LINPACK benchmark are close to two orders of magnitude higher than those possible in solving field equations. ${ }^{25,26}$ Unfortunately, users are not required to report power consumption for the HPCG benchmark, and Joule 2.0 does not have power monitoring equipment. We suspect that power consumption will be close to the sum of the thermal design power limit for all processors used for explicit problems, as processor utilization is high. Using this as a gauge, the WFA on the WSE may be more than two orders of magnitude more energy efficient than distributed computing. Investigations are in progress to refine these estimations and will be reported when complete.

### 3.2 Implicit Formulation Performance

Eq. 3 was implemented in both the WFA and OpenFOAM. The CG inner loop time was recorded for the implementations on the WSE and on Joule 2.0.

### 3.2.1 OpenFOAM on Joule 2.0

The same strong scaling method described in 3.1.1 was used to find the optimal workload per processor (Fig. 5b) in the weak scaling analysis of CG (blue data in Fig. 4b). The optimal workload per processor for $5.8 \times 10^{6}$ and $4.87 \times 10^{6}$ cell simulations were 13,824 and 21,952 , respectively. In addition, there was time to run an additional strong scaling case optimized at $1.57 \times 10^{8}$ cells with a determined workload per processor of 27,000 cells per processor.

The implicit weak scaling was also found to be near-linear as seen in Eqs. 13 to 15 .

$$
\begin{align*}
& R_{i}(W=13824)=3.98 \times 10^{3}-2.75 \times 10^{-5} N_{c}  \tag{13}\\
& R_{i}(W=21952)=2.45 \times 10^{3}-8.63 \times 10^{-6} N_{c} \tag{14}
\end{align*}
$$

$$
\begin{equation*}
R_{i}(W=27000)=2.05 \times 10^{3}-5.66 \times 10^{-6} N_{c} \tag{15}
\end{equation*}
$$

The smallest implicit case has a similar workload-per-processor as the largest explicit case, but has almost half the scaling efficiency. The differences in weak scaling are also not as significant as in the explicit case. These differences are due to reductions that occur during the two dot products in the standard CG algorithm versus the single synchronization in the explicit implementation (an MPI Waitall in the halo update).

### 3.2.2 The WFA on the WSE

The ideal performance for the CG algorithm is given in Eq. 16. The performance includes overlapping the update of the solution variable, $T_{c}$, with the all-reduce needed to compute the norm of the residual.

$$
\begin{equation*}
R_{i}=\frac{F_{c}}{10.5 W+2(X+Y)+337} \tag{16}
\end{equation*}
$$

The iteration rate was found to be not only a function of the clock frequency and workload per processor, but also the WSE fabric dimensions, $X$ and $Y$. Performance is dominated by the size of $X$ and $Y$ when a significant portion of the WSE is allocated and $W$ is relatively small. Unlike the explicit solver, the dependence on $X$ and $Y$ degrades weak scaling performance as wafer allocation size grows. Each dot product in the WFA had a simulator measured performance shown in Eq. 17.

$$
\begin{equation*}
t_{d o t}=\frac{W+X+Y+66}{F_{c}} \tag{17}
\end{equation*}
$$

At the maximum size tested ( $W=1000, X=750, Y=950)$, the dot product can be done in $3.25 \mu \mathrm{s}$. This is substantially lower than most measured distributed computing reductions. The MVAPICH group reported all-reduce latencies for 1024 CPU nodes between 15 and $35 \mu$ s with and without SHArP. ${ }^{27}$ SHArP is an in-network computing technology available on InfiniBand networks that is designed to reduce collective latency. ${ }^{28,29,30}$ Typical GPU all-reduce latencies are over $100 \mu$ s without SHArP, and it has been reported that SHArP can double reduction bandwidth. ${ }^{31}$ All-reduce latency is critical to total system performance, especially in algorithms where computation and all-reduce cannot be overlapped. The disparity between the WSE and distributed all-reduce contributes to the performance advantage.

The measured iterative performance on the WSE was approximately 7.7 times lower than the explicit solver at full fabric allocation with small $W$, even though the implicit solver has only twice as many flops (8 vs 15). When the WSE is capable of doing all the work outside of the reductions in close to a microsecond, even a small amount of time ( $\approx 6.5$ microseconds) spent on two dot products has a large impact on achievable performance when communication cannot be overlapped with compute. It may worth exploring
reduction-free and/or reduction-limited implicit methods as they will likely yield higher performance gains. ${ }^{17,32,33}$ However, the iterative convergence rate must be assessed against the iteration time to decide which yields better overall performance.

## 4 Impact on Science and Technology

Should this approach be adopted and democratized, the impact on science and technology would be significant. The technology would help facilitate design and engineering, optimize equipment operations, and boost the rate of scientific progress. Field equation models are widely used in industry to design and engineer products before production. The most common use of field equation models is in system simulations for design optimization and uncertainty quantification. Currently, the limits in time-to-solution often force practitioners to examine fewer design points or make model simplifications to complete design space explorations in a reasonable time frame. Practitioners could use this technology to significantly speed preproduction design activities and/or ensure a more thorough exploration of the design space, which could result in more optimized designs, reduced risks, and greater certainty. For postproduction operations, the speed gains are significant enough to allow high-fidelity models to run at or faster than real-time. This allows detailed scientific models to be used as digital twins for equipment operations and could enable transformative technology in the areas of cyber-physical security, equipment degradation monitoring/prediction, real time command and control, flexible operations, and upset recovery. In regard to research productivity, much of humanities modern understanding of science and the universe comes from developing, testing, and refining complex models. Current simulations of physical systems are slow and tedious, which has a significant impact on the rate at which humankind can generate novel ideas. A significant loosening of this bottleneck will proportionately increase the rate at which researchers can generate and apply new scientific knowledge.

## 5 Conclusions

With a Python front end that can digest high-level tensor expressions directly into compiled code to run on the WSE, the WFA provides WSE users with a simple and intuitive programming interface. The simplicity of the approach should not be ignored. In less than 18 months, NETL was able to develop a new programming methodology with a team of three people on never-beforeseen hardware with a unique instruction set. In contrast, efficient distributed computing efforts often take years to decades of work with very large groups of developers. Optimization of distributed codes is a substantial undertaking and has a high cost in time and labor. This is due in large part to the complexities of overlapping computation with communication and managing complex memory hierarchies and complex network topologies. Codes that can do this well and maintain generality tend to be very big and difficult to use or understand.

The WFA is much simpler. The simplicity and low development time are the result of the straightforward WSE architecture, especially the absence of a memory hierarchy, the ideal ratio of computing rate to memory bandwidth, the inherent support for tensor instructions, and use of a single device that can achieve useful scales. These properties give the WSE an inherent parallelism that significantly reduces development time and complexity. While it is true that these properties end at the wafer's edge, and that there are limits to the available memory, the computing concept should not be ignored, but rather developed further. The value of the WFA on the current generation WSE is in solving medium to large scale industrial problems. The attainable grid size on the WSE is comparable to typical large industrial scales run on mid-size clusters, yet the solution speed can be more than two orders of magnitude faster. Because of this, NETL is investing in developing the WFA further to support finite volume CFD methods.

The WFA on the WSE solved the explicitly formulated heat equation as much as 470 times faster than OpenFOAM on Joule 2.0. The performance leap illustrates how much performance is lost due to latency and bandwidth limitations in the memory and network subsystems of distributed computing architectures. The balanced computing rate to memory access ratio on the WSE architecture supports low arithmetic intensity operations well, which extends strong scaling to much lower workloads per processor. In combination with the substantial array of tiles, the current generation WSE is valuable for solving industrially relevant problems. The WFA on the WSE was also able to perfectly weak scale out to as many as $2.85 \times 10^{9}$ cells, a capacity usually only seen on large supercomputers.

The CG implementation in the WFA outperformed the CG implementation in OpenFOAM by a factor of at least 87 at large industrial scales. The iterative performance of the implicit solver was approximately 7.7 times lower than the explicit solver on the WSE at full fabric allocation with small $W$, even though the implicit solver has only twice the work to do. This illustrates just how powerful the WSE is in low arithmetic intensity operations. The reductions times on the WSE are over an order of magnitude faster than what is possible on distributed computing, yet the time to compute everything outside the dot product is over 400 times faster. Thus, reductions have a proportionately larger impact on WSE performance as compared to distributed computing. Nevertheless, the WFA on the WSE outperformed OpenFOAM on Joule 2.0 by nearly two orders of magnitude with the classic CG algorithm. It is likely that pipelined versions of Krylov subspace solvers will see larger gains, and new reduction-free implicit methods will be close to the performance gains in the explicit formulation.

## 6 Obtaining the Code

The WFA is open source and available by request on NETL's private gitlab server. Instructions and documentation for obtaining the code can be
found here. ${ }^{34}$ The source code for the OpenFOAM benchmarks can be found here. ${ }^{35}$ Instructions for compiling and running the benchmarks are given in the README file.

## 7 Disclaimer

This project was funded by the Department of Energy, National Energy Technology Laboratory an agency of the United States Government, through an appointment administered by the Oak Ridge Institute for Science and Education. Neither the United States Government nor any agency thereof, nor any of its employees, nor the support contractor, nor any of their employees, makes any warranty, expressor implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.

## References

[1] Einstein, A., Podolsky, B., Rosen, N.: Can quantum-mechanical description of physical reality be considered complete? Phys. Rev. 47, 777-780 (1935). https://doi.org/10.1103/PhysRev.47.777

[2] Bell, J.S.: On the einstein podolsky rosen paradox. Physics Physique Fizika 1, 195-200 (1964). https://doi.org/10.1103/ PhysicsPhysiqueFizika.1.195

[3] Braibant, S., Giacomelli, G., Spurio, M.: Particles and Fundamental Interactions: An Introduction to Particle Physics. Undergraduate Lecture Notes in Physics. Springer (2011). https://books.google.com/books?id= $0 \mathrm{Pp}-\mathrm{f0G9} 9 \mathrm{sC}$

[4] Feynman, R.P., Leighton, R.B., Sands, M.L.: The Feynman Lectures on Physics, Volume II. Addison-Wesley (1989). https://www. feynmanlectures.caltech.edu/II_12.html

[5] Hoffmann, K.A., Chiang, S.T.: Computational Fluid Dynamics for Engineers vol. V. 1. Engineering Education System (1993)

[6] McCalpin, J.D.: Sc16 invited talk: Memory bandwidth and system balance in hpc systems. SC '16 (2016)

[7] The Ohio State University: MVAPICH: MPI over InfiniBand, OmniPath, Ethernet/iWARP, and RoCE. http://mvapich.cse.ohio-state.edu/ performance/gdr-pt_to_pt/ Accessed 2022-8-25

[8] Lauterbach, G.: The path to successful wafer-scale integration: The cerebras story. IEEE Micro 41(6), 52-57 (2021). https://doi.org/10.1109/ MM.2021.3112025

[9] Black, N.H.: Practical Physics. Macmillan (1913). https://books.google. com/books?id=BT8AAAAAYAAJ

[10] Daniel, J.W.: The conjugate gradient method for linear and nonlinear operator equations. SIAM Journal on Numerical Analysis 4(1), 10-26 (1967)

[11] Rocki, K., Van Essendelft, D., Sharapov, I., Schreiber, R., Morrison, M., Kibardin, V., Portnoy, A., Dietiker, J.F., Syamlal, M., James, M.: Fast stencil-code computation on a wafer-scale processor. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '20. IEEE Press (2020)

[12] Jacquelin, M., Araya-Polo, M., Meng, J.: Massively scalable stencil algorithm. arXiv (2022). https://doi.org/10.48550/ARXIV.2204.03775. https: //arxiv.org/abs/2204.03775

[13] Weller, H., Greenshields, C., Bainbridge, W.: OpenFOAM-8. https:// github.com/OpenFOAM/OpenFOAM-8/ Accessed 2022-8-25

[14] Weller, H., Greenshields, C., Bainbridge, W.: The OpenFoam Foundation. https://openfoam.org/ Accessed 2022-8-25

[15] Fischer, P.F.: Scaling limits for pde-based simulation. In: 22nd AIAA Computational Fluid Dynamics Conference, p. 3049 (2015)

[16] Pfisterer, N., Berghoff, M., Streit, A.: On gpu optimizations of stencil codes for highly parallel simulations. In: 2021 29th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP), pp. 228-235 (2021). IEEE

[17] Rss, L., Utkin, I., Duretz, T., Omlin, S., Podladchikov, Y.Y.: Assessing the robustness and scalability of the accelerated pseudo-transient method. Geoscientific Model Development 15(14), 5757-5786 (2022). https://doi. org/10.5194/gmd-15-5757-2022

[18] Xue, W., Roy, C.J.: Multi-gpu performance optimization of a cfd code using openacc on different platforms. arXiv preprint arXiv:2006.02602 (2020)

[19] Pearson, C., Hidayetolu, M., Almasri, M., Anjum, O., Chung, I.-H., Xiong, J., Hwu, W.-M.W.: Node-aware stencil communication for heterogeneous supercomputers. In: 2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pp. 796-805 (2020). IEEE

[20] Rss, L., Omlin, S., Podladchikov, Y.: Porting a massively parallel multi-gpu application to julia: a 3-d nonlinear multi-physics flow solver. JuliaCon (2019)

[21] Chunduri, S., Groves, T., Mendygral, P., Austin, B., Balma, J., Kandalla, K., Kumaran, K., Lockwood, G., Parker, S., Warren, S., et al.: Gpcnet: Designing a benchmark suite for inducing and measuring contention in hpc networks. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. $1-33$ (2019)

[22] Omlin, S., Rss, L., De Montserrat, A., Pattyn, A.: ParallelStencil.jls. https://github.com/omlins/ParallelStencil.jl Accessed 2022-8-25

[23] Dongarra, J., Luszczek, P., Petitet, A.: The linpack benchmark: past, present and future. concurr. Computer.: Pract. Exper 15, 1-18 (2003)

[24] Meuer, H., Strohmaier, E., Dongarra, J., Simon, H., Meuer, M.: Top 5000, The List. https://www.top500.org/ Accessed 2022-8-25

[25] Dongarra, J., Heroux, M.A., Luszczek, P.: High-performance conjugategradient benchmark: A new metric for ranking high-performance computing systems. The International Journal of High Performance Computing Applications 30(1), 3-10 (2016)

[26] Dongarra, J., Luszczek, P., Ye, K., Heroux, M.: HPCG. https:// hpcg-benchmark.org/ Accessed 2022-8-25

[27] The Ohio State University: MVAPICH: MPI over InfiniBand, OmniPath, Ethernet/iWARP, and RoCE. https://mvapich.cse.ohio-state.edu/ performance/collectives/ Accessed 2022-8-25

[28] Graham, R.L., Levi, L., Burredy, D., Bloch, G., Shainer, G., Cho, D., Elias, G., Klein, D., Ladd, J., Maor, O., et al.: Scalable hierarchical aggregation and reduction protocol (sharp) tm streaming-aggregation hardware design and evaluation. In: International Conference on High Performance Computing, pp. 41-59 (2020). Springer

[29] Infiniband Trade Association. https://www.infinibandta.org/ Accessed $2022-8-25$

[30] NVIDIA: InfiniBand Networking Solutions. https://www.nvidia.com/ en-us/networking/products/infiniband/ Accessed 2022-8-25

[31] Song, Q.: Pushing the limits of ai with nvidia gpus and mellanox interconnect. GPU Technology Conference (2019). https://ondemand.gputechconf.com/gtc-cn/2019/pdf/CN9496/presentation.pdf

[32] Cools, S., Vanroose, W.: The communication-hiding pipelined bicgstab method for the parallel solution of large unsymmetric linear systems. Parallel Computing 65, 1-20 (2017)

[33] Ghysels, P., Vanroose, W.: Hiding global synchronization latency in the preconditioned conjugate gradient algorithm. Parallel Computing 40(7), 224-238 (2014)

[34] Van Essendelft, D., Woo, M., Jordan, T.: WFA Documentation. https: //dirk-netl.github.io/WSE_FE/ Accessed 2022-8-25

[35] Van Essendelft, D., Woo, M., Jordan, T.: OpenFOAM Benchmark Code. https://github.com/dirk-netl/OpenFoamBenchmarks Accessed 2022-825


[^0]:    ${ }^{a}$ Total time to solution will only remain constant if cell resolution is constant as problem size grows. For CFD simulations, increasing cell resolution may exceed the Courant-Friedrichs-Lewy limit, which could require more iterations per unit runtime. In addition, for explicit formulations, the stability may be affected which may require reduced time step size.

[^1]:    ${ }^{b}$ NETL's Joule 2.0 supercomputer is the size of a typical single family house, and consumes approximately one megawatt of power. The WSE is packaged into a system the size of a dorm refrigerator and consumes approximately 24 kilowatts of power

[^2]:    ${ }^{c}$ Arithmetic intensity is the ratio of arithmetic operations to data accesses. The inverse ratio is data intensity.

[^3]:    ${ }^{d}$ Overhead is any computational step that is not directly an arithmetic evaluation required for the numerical method. Examples include setting pointer addresses and loop iteration counters.

[^4]:    ${ }^{e}$ Halo data is cell data on the periphery of the local mesh of subdomains in distributed computing that must be exchanged between neighbor processors to complete some linear algebra operations, such as a sparse, matrix-vector multiply.

[^5]:    ${ }^{f}$ Hiding communication refers to a process where communication and computation are done in parallel and the communication time is less than the computation time. In doing so, the communication is said to be hidden behind computation as the communication time is not time critical.

[^6]:    ${ }^{g}$ Experiments are underway on modern GPU nodes to verify workload per GPU, iteration rate, and energy consumption of the problem in Eq. 2 using the methods from Rass et. al. ${ }^{17,22}$ These measurements are the subject of ongoing work and the findings will be released when complete.

