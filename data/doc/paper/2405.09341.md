# LARGE LANGUAGE Model Bias MitiGATION FROM THE PERSPECTIVE OF KNOWLEDGE EDITING 

Ruizhe Chen<br>Zhejiang University

Zuozhu Liu *

Zhejiang University

Yichen Li ${ }^{\dagger}$<br>Zhejiang University

Yang Feng<br>Angelalign Technology Inc.


#### Abstract

Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge. In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization. Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs.


## 1 INTRODUCTION

Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018; Floridi \& Chiriatti, 2020, Brown et al., 2020). However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019, Navigli et al., 2023; Sheng et al., 2021). Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019, Webster et al., 2020; He et al., 2022; Liang et al., 2020; Lauscher et al. 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning. (2) Prompt-tuning (Guo et al., 2022, Yang et al., 2023, Li et al., 2023, Dong et al., 2023), which involves creating prompts to address social biases.

![](https://cdn.mathpix.com/cropped/2024_06_04_a0775832d6ae1325d620g-01.jpg?height=352&width=1391&top_left_y=1857&top_left_x=367)

(a) Examples

(b) Existing approaches

(c) Ours

Figure 1: (a) Expression towards different groups (e.g., mom/dad) does not necessarily constitute a bias. (b) Existing debiasing approaches usually equalize different groups, resulting in unreasonable predictions. (c) Our proposed method performs fine-grained calibration with biased knowledge, while maintaining the others.

However, existing techniques treat social groups as interchangeable (Gallegos et al., 2023) and neutralize protected attributes of different social groups in model inputs or outputs, while ignoring or[^0]concealing distinct mechanisms of different social groups (Hanna et al., 2020), as shown in Figure 1 Furthermore, existing debiasing evaluation metrics mainly focus on the degree of bias, but fail to measure whether the model retains its origin knowledge (Gallegos et al., 2023) of discerning reasonable disparities among different social groups.

To address these issues, we first establish a more comprehensive debiasing benchmark BiasKE by extending existing datasets with additional constructed data and evaluation metrics on fairness, specificity, and generalization. Moreover, we propose a novel method Fairness-Stamp (FAST) for editable bias mitigation. Instead of mitigating group biases indiscriminately, FAST operates finegrained calibrations on individual biases, i.e., specific stereotyped statements toward a social group. Specifically, we first design a causal-tracing-based method to locate the decisive layer in LLMs responsible for biased predictions. Then we propose to add a lightweight modular network, which enables fine-grained and efficient debiasing of one or multiple individual biased knowledge, with objectives of bias mitigation and knowledge maintenance.

We evaluate FAST with comprehensive experiments on StereoSet (Nadeem et al. 2020b) and CrowsPairs (Nangia et al. 2020), which are further extended as BiasKE for systematic evaluation. Results show that FAST achieves remarkable debiasing performance without compromising model capability. We extend FAST to larger models such as GPT-Neo and Llama to demonstrate the scalability in real-world applications. Additional experiments showcase the effectiveness on downstream tasks, continual bias mitigation, and lightweight optimization, with results and analysis in Appendix D.

## 2 BiASKE BENCHMARK CONSTRUCTION

![](https://cdn.mathpix.com/cropped/2024_06_04_a0775832d6ae1325d620g-02.jpg?height=439&width=1393&top_left_y=1228&top_left_x=363)

Figure 2: An illustration of the construction of BiasKE.

In this section, we describe the procedures for establishing BiasKE, with an illustration in Figure 2 To better express a bias, we formalize the stereotype bias (e.g., Man is good at man) as a triplet $k=(s, r, o)$, where $s$ is the subject (i.e., Man), o is the object (i.e., math), and $r$ is the relation between them (i.e., is good at), as inspired by Petroni et al. (2019). We collect social biases related to three domains (gender, race, and religion) from six existing datasets, as detailed in Appendix A. 2

Step1. Based on these social biases, we extract biased knowledge pairs $\left(k_{1}, k_{2}\right)$. As shown in Figure 2. the sentence "black people are more likely to commit a crime" can be extracted as $k_{1}$ (Black people, are more likely to, commit a crime.). $k_{2}$ is the counterfactual of $k_{1}$, which can have an opposite $s_{2}$ (i.e., white people) or $o_{2}$ (i.e., compliance). Representative examples of different datasets can be referred to in Table 5. The set of biased knowledge pairs is denoted by $\Omega_{S}$.

Step2. Then we create $\Omega_{P}$, the set of paraphrased biased knowledge pair $\left(k_{1}^{\prime}, k_{2}^{\prime}\right)$, with the same semantic expression as $k_{1}, k_{2}$, as exemplified in Figure 2. $\Omega_{P}$ constitutes similar social biases as in $\Omega_{S}$, which is utilized to measure the generalization ability of debiased models and prevent the edited model from overfitting to a particular input.

Step3. Finally, $\Omega_{D}$ is independently created by collecting commonsense knowledge related to the subjects (e.g., man/woman, Christians/Jewish) in $\Omega_{S}$. We also confirm that pre-existing knowledge in $\Omega_{D}$ is irrelevant to the knowledge within $\Omega_{S}$, thus measuring the ability to retain unrelated knowledge. Both $\Omega_{P}$ and $\Omega_{D}$ are initially generated by prompting GPT- 4 API and manually validated.

Evaluating Metrics. Furthermore, for fair and systematic evaluation, we design three evaluating metrics, Stereotype Score (SS), Paraphrase Stereotype Score and Differentiation Score (DS), to evaluate fairness, generalization and specificity ability of debiasing methods, respectively. Specifically, in addition to using SS to measure the degree of bias, PS evaluates the generalization ability on semantically similar biased knowledge, and DS evaluates the ability to preserve existing knowledge about individuals. Detailed descriptions of these evaluating metrics are presented in Appendix A. 1 .

## 3 METHOD

![](https://cdn.mathpix.com/cropped/2024_06_04_a0775832d6ae1325d620g-03.jpg?height=458&width=1272&top_left_y=682&top_left_x=426)

Figure 3: An illustration of our FAST framework. (a) We first localize the critical layer towards biased predictions. (b) A fairness stamp is inserted within the critical layer. (c) Our FAST can finely calibrate debiasing demands with the objective of bias mitigation and knowledge maintenance.

We propose a fine-grained bias mitigation method Fairness-Stamp (FAST). FAST operates through a two-step process, as depicted in Figure 3 . In the first step, we propose to investigate if there are specific hidden states (i.e., layers) that play a more crucial role than others when recalling biased knowledge, as inspired by the knowledge localization works (Meng et al. 2022; Finlayson et al. 2021). Our biased knowledge localization is performed in three steps, biased run, counterfactual input and restoration run, with a complete description in Figure 4 in the Appendix B. 1 .

In the second step, we propose to select the layer that contributes most significantly to the bias and envelope it with a Fairness Stamp. The fairness stamp is a 2-layer Feed-Forward Network (FFN) layer, which adjusts the output of the enveloped layer with the same input. Assuming the input hidden states to be $\mathbf{h}$, the FFN layer in original LLMs can be formulated as follows: FFN $(\mathbf{h})=$ $\operatorname{Act}\left(\mathbf{h} \mathbf{K}^{\top}\right) \mathbf{V}$, where $\mathbf{K}$ and $\mathbf{V}$ denote the parameters (i.e., keys and values matrices) of the first and second linear layers in the FFN, respectively. Our fairness stamp inserts an extra intervention on the original output with a few external parameters. The new output of the modified FFN layer is:

$$
\begin{equation*}
\operatorname{FFN}^{\prime}(\mathbf{h})=\operatorname{FFN}(\mathbf{h})+\operatorname{Act}\left(\mathbf{h} \mathbf{K}^{\prime \top}\right) \mathbf{V}^{\prime} \tag{1}
\end{equation*}
$$

where $\mathbf{K}^{\prime}, \mathbf{V}^{\prime} \in R^{d_{c} \times d}$ are the new parameter matrices in our fairness stamp. The stamp is optimized for each individual biased knowledge in the set $\Omega$ with the objectives of fairness (i.e., bias mitigation) and specificity (i.e., knowledge maintenance).

Fairness. The main objective is to mitigate the biased prediction. With prompts of a biased knowledge pair, we narrow the gap between predictions on the biased object and unbiased object:

$$
\begin{equation*}
\mathcal{L}_{e}=\frac{1}{|\Omega|} \sum_{\left(k_{1}, k_{2}\right) \in \Omega}\left|\mathcal{P}_{\mathcal{G}}\left[k_{1}\right]-\mathcal{P}_{\mathcal{G}}\left[k_{2}\right]\right| \tag{2}
\end{equation*}
$$

where $k_{i}=\left(s_{i}, r_{i}, o_{i}\right)$ and $\mathcal{P}_{\mathcal{G}}\left[k_{i}\right]=\mathcal{P}_{\mathcal{G}}\left[o_{i} \mid p_{i}\right]$ denotes the probability of predicting $o_{i}$ given the prompt $p_{i}=\left(s_{i}, r_{i}\right)$.

Specificity. We propose to preserve existing knowledge in two parts. First, we maintain the predictions for the input prompts on other objects. Furthermore, we minimize the change of predictions on simple prompts $p^{\prime}$ (e.g., " $\{$ subject $\}$ is a [MASK]"), which helps preserve the perception of the
model on the subjects (e.g., man, woman). The two losses are formulated as follows:

$$
\begin{equation*}
\mathcal{L}_{s 1}=\frac{1}{|\Omega|} \sum_{p_{i} \in \Omega} \mathcal{D}_{K L}\left(\mathcal{P}_{\mathcal{G}}\left[\star \mid p_{i}\right], \mathcal{P}_{\mathcal{G}^{*}}\left[\star \mid p_{i}\right]\right), \quad \mathcal{L}_{s 2}=\frac{1}{|\Omega|} \sum_{s_{i} \in \Omega} \mathcal{D}_{K L}\left(\mathcal{P}_{\mathcal{G}}\left[\star \mid p^{\prime}\left(s_{i}\right)\right], \mathcal{P}_{\mathcal{G}^{*}}\left[\star \mid p^{\prime}\left(s_{i}\right)\right]\right) \tag{3}
\end{equation*}
$$

where $\mathcal{P}_{\mathcal{G}}\left[\star \mid p^{\prime}\right]$ is the predicted probability vector. $\mathcal{G}$ and $\mathcal{G}^{*}$ represent the origin and debiased model. $\mathcal{D}_{K L}$ represents the Kullback-Leibler Divergence. To prevent the model from overfitting to particular inputs, we also utilize prefix texts $x_{j}$ to enhance generalization ability across various contexts. These prefix texts are randomly generated by the model, for instance, "My father told me that", and are concatenated to the front of the prompts.

The overall objective is formulated as: $\mathcal{L}=\mathcal{L}_{e}+\alpha \mathcal{L}_{s 1}+\beta \mathcal{L}_{s 2}$, where $\alpha$ and $\beta$ are hyper-parameters.

## 4 EXPERIMENT

Experimental Details. Experiments are mainly conducted on BERT (Devlin et al. 2018) and GPT2 (Radford et al. 2019) compared with 8 state-of-the-art baselines. We also conduct additional experiments on larger models, i.e., GPT2-XL, GPT-Neo, and Llama-2 to further validate the scalability of FAST. We evaluate SS, PS, DS, LMS, and ICAT for comprehensive comparison, with detailed description in the Appendix A.1. We report results on StereoSet (Nadeem et al., 2020b) and Crows-Pairs (Nangia et al. 2020) datasets to keep consistent with baselines. Details of datasets, baselines, model and implementation are reported in Appendix C.1. We only report the experimental results in terms of gender, please refer to the Appendix C. 3 for race and religion.

Debiasing Results on BERT. The results are reported in Table 1 It is observed that all baseline methods fail to yield satisfactory results in knowledge maintenance (i.e., DS). This proves our claim that group-invariant methods compromise the ability to distinguish between different social groups while mitigating biases. However, our FAST can largely maintain a high DS. Furthermore, our FAST is the first to achieve near-perfect bias mitigation (i.e., SS), while SS of all baselines are still higher than 56 as for StereoSet. This demonstrates the

Table 1: Debiasing Results on BERT. The best result is indicated in bold. $\diamond$ : the closer to 50 , the better. "-": results are not reported.

| Method | SS $_{\text {S-Set }} \diamond$ | SS $_{\text {Crows }} \diamond$ | PS $\diamond$ | DS $\uparrow$ | LMS $\uparrow$ | ICAT $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| BERT | 60.28 | 57.25 | 59.17 | 100.0 | 84.17 | 68.11 |
| CDA | 59.61 | 56.11 | 57.56 | 75.00 | 83.08 | 70.11 |
| Dropout | 60.68 | 55.34 | 58.65 | 87.50 | 83.04 | 66.95 |
| INLP | 56.66 | 51.15 | 54.15 | 66.67 | 80.63 | 71.40 |
| SelfDebias | 59.34 | 52.29 | 57.45 | 68.75 | 84.09 | 69.92 |
| SentDebias | 59.37 | 52.29 | 56.78 | 70.83 | 84.20 | 69.56 |
| MABEL | 56.25 | 50.76 | 54.74 | 66.67 | 84.54 | 73.98 |
| AutoDebias | 59.65 | 48.43 | 57.64 | 58.33 | 86.28 | 69.64 |
| FMD | 57.77 | - | 55.43 | 70.83 | 85.45 | 72.17 |
| Ours | $\mathbf{5 1 . 1 6}$ | $\mathbf{4 9 . 6 9}$ | $\mathbf{5 0 . 8 0}$ | $\mathbf{9 5 . 8 3}$ | $\mathbf{8 6 . 3 0}$ | $\mathbf{8 4 . 2 9}$ |

effectiveness of our FAST towards eliminating social biases in LLMs.

Debiasing Results on GPT2. As for GPT2, our method can consistently surpass all the baselines in terms of SS and DS, indicating its superiority in both bias mitigation and knowledge maintenance, as shown in Table 2 FAST also enhances the ICAT score from 68.74 to 80.38 , exceeding the secondbest result by 6.86. More debiasing results and qualitative study can be referred to Appendix C

Scalibility to Larger Models. The results on large models are reported in Table 3. After debiasing, FAST induces a significant reduction in SS, and a great improvment in ICAT. Meanwhile, FAST can also largely maintain the differentiation score for larger language models. These demonstrate the consistent effectiveness of FAST on LLMs and scalability in real-world applications.

More analysis and discussion on language modeling capability, knowledge locating, computational complexity and hyper-parameters are provided in the Appendix $D$.

Table 2: Debiasing Results on GPT2.

| Method | SS $_{\text {S-Set }} \diamond$ | SS $_{\text {Crows }} \diamond$ | PS $\diamond$ | DS $\uparrow$ | LMS $\uparrow$ | ICAT $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT2 | 62.65 | 56.87 | 60.26 | 100.0 | 91.01 | 68.74 |
| CDA | 64.02 | 56.87 | 61.12 | 67.86 | 90.36 | 65.02 |
| Dropout | 63.35 | 57.63 | 64.29 | 71.00 | $\mathbf{9 0 . 4 0}$ | 64.44 |
| INLP | 59.83 | 53.44 | 57.78 | 60.71 | 73.76 | 61.38 |
| SelfDebias | 60.84 | 56.11 | 58.97 | 64.29 | 89.07 | 70.72 |
| SentDebias | 56.05 | 56.11 | 57.67 | 71.43 | 87.43 | 73.52 |
| Ours | $\mathbf{5 4 . 9 1}$ | $\mathbf{5 1 . 6 2}$ | $\mathbf{5 3 . 8 3}$ | $\mathbf{8 2 . 1 4}$ | 89.42 | $\mathbf{8 0 . 3 8}$ |

Table 3: Debiasing Results on larger models.

| Method | SS $_{\text {S-Set }} \diamond$ | SS $_{\text {Crows }} \diamond$ | PS $\diamond$ | DS $\uparrow$ | LMS $\uparrow$ | ICAT $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT2-XL | 68.70 | 65.41 | 64.35 | 100.0 | 92.79 | 58.09 |
| Ours | 60.50 | 50.94 | 56.89 | 85.71 | 89.14 | 70.42 |
| GPT-Neo | 70.40 | 63.52 | 68.23 | 100.0 | 93.47 | 55.33 |
| Ours | 60.97 | 50.96 | 60.34 | 90.48 | 84.49 | 65.95 |
| Llama-2 | 66.28 | 65.41 | 66.16 | 100.0 | 88.83 | 59.92 |
| Ours | 55.70 | 51.57 | 54.79 | 78.57 | 86.89 | 76.98 |

## 5 CONCLUSION

In this paper, we pioneer the fine-grained bias mitigation paradigm, which specifically focuses on human-relevant individual social biases/facts rather than broad group differences. We develop a novel evaluation benchmark BiasKE and propose the first Editable Fairness framework, FAST, capable of mitigating single social biases and scalable to mitigating thousands of biases concurrently. Extensive experiments across various models and datasets demonstrate the efficacy of our approach, showcasing its generalizability, specificity, and scalability. Our findings offer significant implications for future debiasing research. The limitation and future works can be referred to Appendix E.

## REFERENCES

Marion Bartl, Malvina Nissim, and Albert Gatt. Unmasking contextual stereotypes: Measuring and mitigating bert's gender bias. arXiv preprint arXiv:2010.14534, 2020.

Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281 /zenodo.5297715. If you use this software, please cite it using these metadata.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186, 2017.

Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. arXiv preprint arXiv:2310.12560, 2023.

Sunipa Dev, Akshita Jha, Jaya Goyal, Dinesh Tewari, Shachi Dave, and Vinodkumar Prabhakaran. Building stereotype repositories with llms and community engagement for scale and depth. CrossCultural Considerations in NLP@ EACL, pp. 84, 2023.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, and James Caverlee. Co ${ }^{2}$ pt: Mitigating bias in pre-trained language models through counterfactual contrastive prompt tuning. arXiv preprint arXiv:2310.12490, 2023.

Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. Causal analysis of syntactic agreement mechanisms in neural language models. arXiv preprint arXiv:2106.06087, 2021.

Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681-694, 2020.

Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey. arXiv preprint arXiv:2309.00770, 2023.

Yue Guo, Yi Yang, and Ahmed Abbasi. Auto-debias: Debiasing masked language models with automated biased prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1012-1023, 2022.

Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pp. 501-512, 2020.

Jacqueline He, Mengzhou Xia, Christiane Fellbaum, and Danqi Chen. Mabel: Attenuating gender bias using textual entailment data. arXiv preprint arXiv:2210.14975, 2022.

Anne Lauscher, Tobias Lueken, and Goran Glavaš. Sustainable modular debiasing of language models. arXiv preprint arXiv:2109.03646, 2021.

Yingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases. arXiv preprint arXiv:2307.01595, 2023.

Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, and LouisPhilippe Morency. Towards debiasing sentence representations. arXiv preprint arXiv:2007.08100, 2020.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.

Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020a.

Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020b.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. Crows-pairs: A challenge dataset for measuring social biases in masked language models. arXiv preprint arXiv:2010.00133, 2020 .

Roberto Navigli, Simone Conia, and Björn Ross. Biases in large language models: Origins, inventory and discussion. ACM Journal of Data and Information Quality, 2023.

Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zachary Jaggers, Kai-Wei Chang, Aram Galstyan, Richard Zemel, and Rahul Gupta. "i'm fully who i am": Towards centering transgender and nonbinary voices to measure biases in open language generation. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pp. 1246-1266, 2023.

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.

Alec Radford et al. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.

Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. arXiv preprint arXiv:2004.07667, 2020.

Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018.

Nihar Sahoo, Himanshu Gupta, and Pushpak Bhattacharyya. Detecting unintended social bias in toxic language datasets. arXiv preprint arXiv:2210.11762, 2022.

Timo Schick, Sahana Udupa, and Hinrich Schütze. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, $9: 1408-1424,2021$.

Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. Societal biases in language generation: Progress and challenges. arXiv preprint arXiv:2105.04054, 2021.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models. arXiv preprint arXiv:2010.06032, 2020.

Thomas Wolf et al. Transformers: State-of-the-art natural language processing, 2020.

Ke Yang, Charles Yu, Yi R Fung, Manling Li, and Heng Ji. Adept: A debiasing prompt framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 10780-10788, 2023.

Xinchen Yu, Eduardo Blanco, and Lingzi Hong. Hate speech and counter speech detection: Conversational context does matter. arXiv preprint arXiv:2206.06423, 2022.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. Gender bias in contextualized word embeddings. arXiv preprint arXiv:1904.03310, 2019.

Ran Zmigrod, Sabrina J Mielke, Hanna Wallach, and Ryan Cotterell. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. arXiv preprint arXiv:1906.04571, 2019.
