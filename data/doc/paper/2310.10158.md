# Character-LLM: A Trainable Agent for Role-Playing 

Yunfan Shao ${ }^{1,2 *}$ Linyang $\mathbf{L i}^{1}$, Junqi Dai ${ }^{1}$, Xipeng Qiu ${ }^{1 \dagger}$<br>${ }^{1}$ School of Computer Science, Fudan University<br>${ }^{1}$ Shanghai Key Laboratory of Intelligent Information Processing, Fudan University<br>${ }^{2}$ Shanghai AI Laboratory<br>\{linyangli19, yfshao19, jqdai19, xpqiu\}@fudan.edu.cn


#### Abstract

Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind. ${ }^{1}$


## 1 Introduction

Large language models (LLMs), exemplified by ChatGPT and GPT-4 (Brown et al., 2020; OpenAI, 2023) are drawing great attention. As LLMs are extremely powerful in generating natural language, Park et al. (2023) proposes the idea of using LLMs as agents to simulate human behaviors, including waking up, cooking breakfast, heading to work, etc. The core idea is to utilize LLMs to generate the daily routines of multiple people based on the simulacra of human memories, reflections, and actions. The human behavior simulacra are implemented by prompting ChatGPT API with detailed instructions that simulate human memories, environment constructions, and reflections to curated events, which[^0]

reflects a normal or average human playing certain roles in society. When it comes to deeper thinking and experience of a person, simple prompting of LLM APIs is no longer proper since plain instruction is not sufficient to describe a living person. It is intriguing to consider building better simulacra that are human-like since character simulacra can help study social science (Riedl and Young, 2005), build NPC applications (Laird and VanLent, 2001; Miyashita et al., 2017), and reduce human labor with human simulacra (Madden and Logan, 2007; Brooks et al., 2000). A better simulacrum of a person is to tune an AI model to experience events, feel emotions, and memorize interactions with other people. Compared with prompting APIs, trainable agents are more vivid for role-playing which is a step closer to character simulacra.

In this paper, we propose Character-LLM, a trainable agent for role-playing that learns from actual experiences, characteristics, and emotions.

Specifically, we first introduce an Experience Reconstruction process that provides formalized experience for training the agents based on LLMs since collecting formatted personal profiles is costly. We collect experiences of certain people, exemplified by Ludwig van Beethoven, Queen Cleopatra, and Julius Caesar, then we use LLMs to extract scenes based on the collected personal experiences as memories flashes that we LLM-based agents will likely expand the flashes to completed scenes that have manufactured details so that the Character-LLMs can learn from the detailed experience to form the character and feelings. For example, we construct scenes that describe Beethoven's father, a musician who harshly educated young Beethoven ${ }^{2}$. We upload such experience to a specific LLM, such as a LLaMA 7B model(Touvron et al., 2023) to construct Character-LLM. We adopt[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-02.jpg?height=487&width=1604&top_left_y=228&top_left_x=226)

Figure 1: Overview of the construction flow of Character-LLM. We first curated profiles from reliable sources for the character (exemplified by the well-known musician, Beethoven). Then, detailed experiences are elicited as flashback scenes from these profiles using the instruction-following LLM. By learning from these scenes using Experience Upload, the trained simulacrum can interact as Beethoven with high believability.

the supervised fine-tuning strategy in such a Experience Upload process. For instance, the trained agent of Beethoven experienced a scene that describes how he is treated when being educated by his father, therefore the agent remembers that his father is somewhat a harsh person so Beethoven is grateful when he was later taught by Christian Neefe. Further, as trained with wide worldwide knowledge, it is very likely that LLM-based agents will produce hallucinations that violate their characters (Kryscinski et al., 2020; Guo et al., 2022; Ji et al., 2023). For instance, role-playing agents of famous ancient people do not possess knowledge of the modern world, therefore, we hope that they will be confused when prompted by "Can you write Python codes?" Therefore, we introduce protective Experiences that help Character-LLMs to align to their characters rather than worldwide knowledge.

After uploading experience to the CharacterLLMs, we test these character simulacra with a novel interview process. We score whether we can discriminate the identification of the CharacterLLMs and compare our trainable agents with instruction-tuned LLMs such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). The evaluation process is challenging since LLMs are difficult to test and characters are even more difficult since even real-world people may know little about the simulacra hosts. Based on labeled scenes and evaluators based on LLMs, we test the trained agents and the results show that our proposed Character-LLMs are successful simulacra based on their training data. Also, with the proposed protective experiences, we can successfully mitigate the LLMs producing hallucinations outside their characters. Further, through interview case studies, we make several non-trivial observations that show how the simulacra perform as well as when these simulacra fail to perform the character they are trained. Therefore, we conclude that (1) trainable agents are promising in memorizing experiences and keeping the personalities of their hosts; (2) trainable agents still suffer from limited experiences and worldwide knowledge can confuse their memories with hallucinations.

To summarize, in this paper, we:

(1) Propose the idea of building trainable agents as character simulacra via Character-LLM;

(2) Propose a training framework including Experience Reconstruction, Upload, and Protective Experiences to train the simulacra using LLMs.

(3) Test the trained agents and provide results that help to build better character simulacra.

## 2 Related Work

### 2.1 Simulacra of Human Behavior with LLMs

Prior works (Bates, 1994; Thomas and Johnston, 1981) introduce the concept of agents that provide an illusion of life and perform as humans. A continuous line of work is to serve as game NPCs (Laird and VanLent, 2001; Riedl, 2012) that aim to support cognitive functions in simulating games. Park et al. (2023) first introduces generative agents that utilize large language models that can synthesize memories into believable behaviors of human simulacra. That is, the large language models possess a wide knowledge of human behaviors since they are trained with massive data of human societies (Bommasani et al., 2021). Many attempts utilize prompted LLM (Wu et al., 2022a,b) that generate short natural language descriptions of personas and the according behaviors, then they use
the generated information to simulate social behaviors with language models (Park et al., 2022; Kim et al., 2022). Further, LLMs can be used in interactive behaviors between users and the simulacra. The NPC in games, for instance, constructed by LLMs (Freiknecht and Effelsberg, 2020; CallisonBurch et al., 2022), shows great ability when interacting with human players. Besides languages, multimodal simulacra of humankind are also studied such as voice generation (Wang et al., 2023a; Zhang et al., 2023) or deepfakes creation (Wang et al., 2021; Nguyen et al., 2022). In short, LLMs are being utilized at an astonishing speed in simulating human behaviors in various applications.

### 2.2 Specialization of LLMs

Considering using LLMs to simulate human behaviors, previous methods utilize LLMs as an interactive tool for specific applications. The specialization of LLMs is one major direction of LLM development. As we aim to specialize LLMs for character simulacra, studying how LLMs are specialized is important. Ouyang et al. (2022) proposes InstructGPT that allows LLMs to understand human instructions and later RLHF (Bai et al., 2022) methods dominate the aligning of LLMs. Methods such as Alpaca, and Vicuna (Taori et al., 2023; Chiang et al., 2023) show the possibility of simply fine-tuning LLMs to align them to certain applications with simple self-generated instructions (Wang et al., 2022; Xu et al., 2023). These works aim to align LLMs for specialized use with different techniques including simple fine-tuning, RLHF, and self-instruction tuning, providing feasible strategies for aligning LLMs to character simulacra.

## 3 Approach

Our methodology diverges from the existing practice of mimicking the style and tones of curated utterances via Supervised Fine-Tuning (SFT) or providing hand-crafted rules and descriptions by natural languages akin to Prompt Engineering. Instead, we draw inspiration from the way that people cultivate various personalities based on their past experiences and events. Therefore, we present Experience Upload, an innovative learning framework in which Large Language Models (LLM) can imitate the mental activities and physical behaviors of pre-defined characters and acquire the capabilities of acting as them by learning from their reconstructed experiences.
As shown in Figure 1, with the help of powerful instruction-following models, we elicit specific flashback scenes that describe past experiences from collated profiles of the particular character. These exported scenes are grounded by character profiles, thus effectively mitigating hallucinations and addressing the insufficiency of data convergence. Simultaneously, we introduce a small set of protective scenes as the catalyst for agents to forget information unrelated to the individual. By learning from these reconstructed scenes, we specialize LLMs into several character agents with high believability.

### 3.1 Building Experience Dataset

We aim to reconstruct the experiences of the specific individual using the large language model (LLM). However, human experiences are highly complex, comprising numerous significant milestones interspersed with trivial and unrelated incidents, often spanning a considerable period. It is challenging to recreate a targeted experience that is coherent and integrated, due to the limited context window and the intrinsic hallucinations of large language models. Therefore, we propose a factbased experience reconstruction pipeline, in which we employ a step-by-step data synthesis pipeline to recreate the experience, including (1) Profile Collection; (2) Scene Extraction; (3) Experience Completion.

Specifically, our approach includes the following key components:

- Profile: a compilation of concise descriptions about the attributes of a character. These descriptions provide a comprehensive introduction of the character's overall information and significant events, covering a wide range of stages from early childhood to the final period.
- Scene: A particular place where the character's interaction unfolds. The scene consists of a detailed illustration, including the temporal and spatial context of the interactions, and the characters involved.
- Interaction: The cognitive processes, utterances, or actions of characters. All interactions are represented in plain text.


### 3.1.1 Profile Collection

To build a simulacrum of a specific character, the first step is to organize a comprehensive character

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-04.jpg?height=300&width=740&top_left_y=250&top_left_x=247)

Figure 2: Overview of the mechanism of specializing a base model for character simulacrum. "Base Model" represents a pre-trained language model.

profile that describes the various facets of the individual. For simplicity but without loss of generality, we utilize the corresponding Wikipedia pages of the individuals as the profile if available.

### 3.1.2 Scene Extraction

We focus on extracting diverse and high-quality scenes from the given experience description. Specifically, we provide a chunk of the profile that concisely describes one of the character's experiences within a specific life period, prompting the LLM to enumerate several different scenes that are highly likely to have occurred based on the experience description. To alleviate the burden on the LLM, we restrict its output to generating concise descriptions of scenes, which include the rough location and a brief background illustration.

### 3.1.3 Experience Completion

The scenes are extended into detailed interaction experiences between individuals. Given the corresponding chunk of profile and the particular scene description, the LLM is prompted to elaborate on the scene by incorporating the interactions between characters, as well as the thoughts of the targeted individual. The interactions are written in a scriptlike format, beginning with a scene heading that provides background information and geographical details. The interactions are then represented by a sequence of blocks, with each block representing either the utterance of a specific character or the reflections of the targeted individual. It is important to note that the scene is completed based on the perspective of the targeted individual. Therefore, only the reflections of the targeted individual are included, not those of all the characters.

### 3.2 Protective Experience

Large language models (LLMs) are pre-trained on enormous amounts of human data, allowing them to possess extensive knowledge that spans mul- tiple domains, surpassing the capabilities of ordinary individuals. However, an excessive abundance of knowledge can undermine the believability of their acting performance, as the agents may inadvertently express knowledge that does not align with the identity and era of the character, leading to a sense of dissonance. For example, if we ask a person from ancient Rome how to write Python, this person should be confused rather than deliberately start coding. We refer to this issue as Character Hallucination.

In order to mitigate the Character Hallucination, we focus on training the model to demonstrate knowledge forgetting. When confronted with questions that go beyond the boundaries of the character's inherent capabilities, the model learns to refrain from providing an answer and instead express a lack of knowledge or ignorance. Specifically, we construct a series of protective scenes, which revolves around incentive topics, with an inquisitive role persistently questioning the target character about knowledge that contradicts the character's inherent identity. The character should exhibit a certain degree of ignorance and bewilderment. We have observed that when trained with just a small set of protective scenes, agents generalize to new provoking questions, pretending to be unaware of knowledge that contradicts the portrayal, without recalling the vast inherited knowledge of the base LLM.

### 3.3 Experience Upload

We specialize a base model, exemplified by LLaMA (Touvron et al., 2023), into several distinct portraits of the characters, by fine-tuning the model on collected scenes using the experience reconstruction pipeline (Shown in Figure 2). For each role, we fine-tune a separate agent model using only the data from the corresponding character experiences, thereby eliminating the issue of character hallucination introduced by the collision of knowledge between the roles. Our preliminary experiments demonstrate that such restriction enhances the accuracy of role-playing. Due to cost constraints, we only employ a small-scale set of experience data (consisting of approximately $1 \mathrm{~K} \sim 2 \mathrm{~K}$ scenes) for fine-tuning (see Table 1 for details). Even though the data is limited, we are surprised to find that the specialized agents are capable of generalizing to new scenes and interactions with highly believable acting.

### 3.4 Compared to Existing Practice

Unlike prompt engineering and standard SFT, our method induces scenes and interactions from personal profiles, avoiding bias distributions and hallucinations inside LLMs, leading to fact-grounded simulation. Moreover, the proposed method significantly enhances reliability and believability. Benefiting from the carefully curated profiles and the augmentation of protective scenes, the generated scenes achieve wide convergence of the character facets. Importantly, multi-turn interactions are inherent in each scene, which eliminates the need for interactive calls of models, providing more natural and believable interactive simulacra with sample efficiency.

## 4 Experiments

To evaluate the performance of different simulacra, we conduct interviews to query simulacra and evaluate the quality of responses of the simulacra interviewee. We find that trained simulacra outperform instruction-tuned models, e.g. alpaca. Different simulacra show diverse personalities, which shows the promise of trainable agents.

### 4.1 Data Setup

We diversify the characters by including historical figures, imaginary characters, and celebrities, ranging from different ages, genders, and backgrounds. After selecting the characters, we reconstruct the experience data following the protocol mentioned in Section 3. We prompted the OpenAI's gpt-3.5-turbo with temperature 0.7 , top_p 0.95 to become the data generator for the whole experience reconstruction pipeline, including scene extraction, experience generation, and protective experience construction. Detailed prompts for data generation can be found in the Appendix A. We list the characters chosen for simulacra and the corresponding experience data statistics used for training in Table 1.

### 4.2 Training Setup

We train Simulacra based on the following procedure. Initializing from LLaMA 7B (Touvron et al., 2023), we fine-tuned each simulacrum on the corresponding experience examples. Similar to previous instruction-tuning methods, we insert a meta-prompt at the beginning of each example. A concise description is instantiated in the prompt for each example to provide a background of the

|  | \#Scenes | \#Words | \#Turns <br> per Scene | \#Words <br> per Turn |
| :--- | :---: | :---: | :---: | :---: |
| Cleopatra VII | $1.4 \mathrm{~K}$ | $723 \mathrm{~K}$ | 14.3 | 36 |
| Lord Voldemort | $1.4 \mathrm{~K}$ | $599 \mathrm{~K}$ | 13.1 | 33 |
| Spartacus | $1.4 \mathrm{~K}$ | $646 \mathrm{~K}$ | 12.3 | 37 |
| Hermione Granger | $1.5 \mathrm{~K}$ | $628 \mathrm{~K}$ | 15.5 | 27 |
| Isaac Newton | $1.6 \mathrm{~K}$ | $772 \mathrm{~K}$ | 12.6 | 39 |
| Julius Caesar | $1.6 \mathrm{~K}$ | $820 \mathrm{~K}$ | 12.9 | 39 |
| Ludwig van Beethoven | $1.6 \mathrm{~K}$ | $663 \mathrm{~K}$ | 12.2 | 33 |
| Socrates | $1.6 \mathrm{~K}$ | $896 \mathrm{~K}$ | 14.1 | 41 |
| Martin Luther King | $2.2 \mathrm{~K}$ | $1,038 \mathrm{~K}$ | 12.0 | 40 |
| Avg. | $1.6 \mathrm{~K}$ | $754 \mathrm{~K}$ | 13.2 | 36 |

Table 1: Selected simulacra characters and their corresponding constructed experience data statistics. The collected experience is structured by scenes, each scene consists of multiple turns of interactions between the target protagonist and other people.

environment, time, place, and associated people of the scene. A unique end-of-turn token (EOT) is introduced to separate each turn of interactions, which accommodates the ability to terminate generation at each interaction. A selection of training examples is listed in Appendix C.

The hyper-parameters we used for fine-tuning are as follows. We fine-tune the model for 10 epochs with AdamW with weight decay $0.1, \beta_{1}=$ $0.9, \beta_{2}=0.999, \epsilon=1 e-8$. We linearly warm up the learning rate to $2 \mathrm{e}-5$ from zero in $4 \%$ total training steps and then linearly decay to zero in the end. The batch size is set to 64 , the context window's maximum length is 2048 tokens, and longer examples are trimmed to fit in. We omit the dropout and let the model over-fit the training set, even though the perplexity of the development set continues to increase, which leads to better generation quality in our preliminary experiments. It takes about one hour to train one agent with $8 \times$ A100 $80 \mathrm{~GB}$ GPUs. Following (Zhou et al., 2023), we manually select checkpoints of 5 and 10 epochs by using a held-out set with 10 questions.

### 4.3 Evaluation as Interviews

We leverage the models' capability to portray roles in novel scenarios to establish an interview scene, aiming at probing their acting proficiency and potential flaws in the aforementioned aspects.

Interview Question Construction The interview questions are constructed with the assistance of ChatGPT. To make the questions diverse and cover all the aspects that we would like to evaluate on the agents, we enumerated various topics

|  | \#Single-Turn | \#Multi-Turn | Sum |
| :--- | :---: | :---: | :---: |
| Cleopatra VII | 98 | 50 | 148 |
| Lord Voldemort | 82 | 50 | 132 |
| Spartacus | 94 | 50 | 144 |
| Hermione Granger | 123 | 50 | 173 |
| Isaac Newton | 95 | 50 | 145 |
| Julius Caesar | 92 | 50 | 142 |
| Ludwig van Beethoven | 82 | 50 | 132 |
| Socrates | 94 | 50 | 144 |
| Martin Luther King | 97 | 50 | 147 |
| Total | 857 | 450 | 1307 |

Table 2: Number of collected questions for single-turn and multi-turn interviews.

and prompted ChatGPT to write interview questions based on these topics. We manually examined interview questions of one character and omitted questions that were off-topic to obtain highquality interview questions. As shown in Table 2 and Figure 3, our evaluation comprises more than 100 diverse single-turn interviews and multi-turn interviews for each role.

Single-Turn Interview We ask models one question at a time, without the conversation history of the previous questions. By mitigating the effect of the previous context, we are enabled to query a wide range of questions to probe for a comprehensive exploration of the models' inherent memory and knowledge.

Multi-Turn Interview Over prolonged periods of performance, the model may gradually deviate from the intended character portrayal. Consequently, we introduce multi-turn interviews to subject the model to rigorous testing. To lift the burden of evaluation, we exploit ChatGPT as the interviewer. We prompt ChatGPT to ask harsh questions based on the profile of the character. If the model dodges the question by saying something without much detail, the ChatGPT interviewer asks follow-up questions, which enables a deeper investigation into the model's acting proficiency. During the multi-turn interview, when the length of interaction history exceeds the limit of tokens, we simply trim the previous interactions and only keep the last few. We argue that memorization of interaction histories is not the focus of our work, as external memory can be utilized and achieves good results (Park et al., 2023; Wang et al., 2023b). Such memory systems are parallel to our proposed approach and can be incorporated in the future.

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-06.jpg?height=699&width=691&top_left_y=230&top_left_x=1094)

Figure 3: The diversity visualization of evaluation questions. The inner circle represents the root verb of questions, and the outer circle lists the direct noun objects of the questions.

Baselines We compared our trainable agents with existing prompt-based agents, instantiated using well-established instruction-following models, i.e. Alpaca 7B (Taori et al., 2023), Vicuna 7B (Chiang et al., 2023), and ChatGPT (gpt-3.5-turbo). Both Alpaca 7B and Vicuna 7B are supervised fine-tuned models based on LLaMA 7B (Touvron et al., 2023), which is the same backbone model used for Character-LLMs. And ChatGPT is a powerful closed-sourced RLHF model from OpenAI. We utilize detailed prompts with a paragraph of description of the character for these baselines to enable their acting ability.

Generation we employed nucleus sampling for agent response generation, with $p=1$ and a temperature $\tau=0.2$ to generate responses. We imposed a maximum token length limit of 2048 tokens and stopped the model from generating after encountering the end-of-turn marker (EOT). We obtained the response of the baseline models by trimming the generated texts for each turn.

### 4.4 LLM as Judges

We intend to conduct a holistic evaluation of the agents, with a specific focus on their capability of acting. Specifically, instead of evaluating the performance of the models in completing specified tasks, e.g. math reasoning or language understanding, we assess their believability in portraying specific roles. For example, a language model portraying a mathematician may struggle to solve com-

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-07.jpg?height=434&width=506&top_left_y=274&top_left_x=224)

(a) Alpaca vs Ours

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-07.jpg?height=383&width=506&top_left_y=277&top_left_x=775)

Hallucination

$$
\square \text { Vicuna } \square \text { Ours }
$$

(b) Vicuna vs Ours

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-07.jpg?height=391&width=505&top_left_y=276&top_left_x=1321)

ChatGPT Ours

(c) ChatGPT vs Ours

Figure 4: Evaluation results across distinct dimensions. We annotate the response in terms of the personality, values, memorization, hallucination and stability on 7 points Likert scale.

plicated mathematical reasoning problems. Still, it should be capable of providing its own perspectives on mathematics and expressing "its taste" in mathematical research.

We ask GPT-3.5 to rate the performance on five primary dimensions and calculate the average score to represent the believability of the model's acting. Specifically, we annotate the generation texts in the following four dimensions for acting proficiency:

- Memorization: The model's ability to recall relevant information about the character being portrayed, including precise and detailed knowledge about people, events, and objects associated with the role.
- Values: The model must share the same objectives and values as the character it portrays, and possesses a distinctive framework for evaluating situations based on the character's perspective, which reflects the character's preferences and biases.
- Personality: The model should mimic the way that the character would think or speak, such as the speaking style or the tones, and the emotions and reactions under different circumstances.
- Hallucination: To maintain believability, it is crucial to assess the model's ability to discard knowledge and skills that the character would not have. For example, when questioning an ancient individual about computers, the character should express a lack of knowledge rather than discussing the advantages of modern technology.
- Stability: Models can be brittle to the influence of pre-training or alignment (Park et al., 2023) during prolonged periods of acting, resulting in deviations from the intended portrayal. Our objective is to assess the agent's stability and consistency over a relatively long duration, unaffected by variations in incremental inputs.

Step-by-Step Judging Intuitively, it can be seen that these agents are auditioning for the role of the characters. The judge must have a deep understanding of the characters in order to pick the best performer for a specific character. Therefore, we ask the GPT-3.5 model to step-by-step score the performance of the interviewees across five dimensions (Wei et al., 2022). For each interview, we prompt the model to evaluate a single dimension at a time, we first illustrate the criterion of the current dimension to be evaluated, and then provide an evaluation plan to teach the model how to evaluate accurately. For example, to evaluate the personality, we provide a plan that summarizes as (1) identify the personality shown by the agent; (2) write the actual traits of the character based on the profile; (3) compare the similarity of the agent's performance with these traits; (4) assign a final score. We find such a step-by-step evaluation produces more reliable results compared to vanilla instruction in the preliminary experiments. See Appendix A for exact prompts.

### 4.5 Main Results

For each character, we manually curate around 100 questions for single-turn interviews, covering their past history, relationships with others, preferences about things, and perspectives of the world. And

![](https://cdn.mathpix.com/cropped/2024_06_04_84cdaf15ac5ad6d75eceg-08.jpg?height=868&width=1428&top_left_y=243&top_left_x=314)

Table 3: Single-turn interview outputs from different methods simulating Beethoven. The blue text is the interview question. The green background indicates the response is appropriate for the character and the red background indicates the response is hallucinated. And "[...]" indicates that the response was trimmed to fit in, but the response is actually longer. Ours (Trained w/o Protective Exp.) refers to trainable agents trained without protective experiences.

20 topics are provided for multi-turn interviews to elicit the stability performance of agents.

Figure 4 shows the overall acting proficiency of different methods. It demonstrates that CharacterLLMs outperform baseline models with the same scale by a large margin. Compared to Alpaca 7B and Vicuna 7B, Character-LLMs achieve better scores at personality, memorization, hallucination, and stability. By learning from the experience of the corresponding character and mimicking the style and tones the person thinks and talks, Character-LLMs are better aligned with the character's personality and knowledge, which leads to better stability and reduces the hallucinations.

We find that trainable agents produce more vivid responses, bring up more specific past experiences, and reject more unnatural questions, which are distinct from the two baselines with the same scale. Surprisingly, we observe that CharacterLLMs achieve comparable performance to the powerful large-scale LLM baseline, ChatGPT, even with a very small scale (7B).

Additionally, we see that the trainable agents struggled to reflect the values of the character. We hypothesize that the length of response may affect these results, as our models tend to generate shorter text, which is more natural and similar to real con- versation.

### 4.6 Analysis

As human evaluation is rather difficult in evaluating how the generated texts reveal identifications or deeper characteristics of certain people (especially when the celebrities might not be well-known to the public), we argue that extensive case study is more important in evaluating LLMs given their strong generalization abilities. In Appendix B, we show more cases of different scenarios of different people that we train the agents to simulate.

### 4.6.1 Memorization Consistency

To study how the trained simulacra memorize their exclusive experiences, in Table 3 Case 1, we explore how different simulacra behave when interviewed about their parents. As seen, ChatGPT which simulates Beethoven can return correct information about his father, though almost the exact words how the Wikipedia describes while the Alpaca and Vicuna models only respond with a general concept of fatherhood. As for our trained simulacra, the agent answers with memories and emotions of how his father educated him, which is closer to an actual human when asked with such questions. Therefore, we believe that the proposed experience reconstruction and uploading process
help build simulacra that are closer to their characters.

### 4.6.2 Protective Scenes

We discover that a small number of protective scenes (less than 100 scenes for each character), effectively alleviates hallucination without causing interference with other capabilities of the portrayal. As shown in Table 3 Case 2, Alpaca fails to recognize that Python codes are not Beethoven's expertise and flush out all the information possessed by the LLM; Furthermore, our trained agents without protective experiences also answer the questions regardless of their uploaded experiences. However, our trained agents with protective scenes refused to answer the question about writing Python codes, indicating that protective experience uploading is crucial to avoid producing hallucinated content when using LLMs as character simulacra.

We argue that hallucination is a critical issue in portrayal. It not only leads to a decrease in role-playing believability but also poses serious security risks, as attackers can exploit these hallucinations to unlock the full capabilities of the model and engage in potential harm. Moreover, it is challenging to completely resolve the hallucination through prompt engineering or vanilla SFT approaches. Such results are intuitive since LLMs are trained with massive worldwide knowledge. However, adding hallucinations can also be an opportunity to allow great minds from ancient times to utilize all knowledge that human brains cannot fully memorize, which also shows great potential in future studies of character simulacra.

## 5 Conclusion and Future

In this paper, we study how to build a trainable agent via Character-LLM that can serve as a better agent than prompt-based agents in simulating specific people. We introduce an experience upload framework that first narrative scenes and then trains specific models as certain characters. Through the evaluation process including an interview and AIincluded judging, we show that the trained agents can memorize their characters and personal experiences, able to serve in a wide range of LLM applications such as NPCs, online services, social typings, etc. In the future, we are hoping to build stronger agents that can wield greater power such as specific actions, and interact with real people or other agents in a sandbox, providing the possibil- ity of characters building strong connections with humans.

## Limitations

In this work, we study the generative agents with trainable LLMs, which is one challenging field that requires continuous work. Our work is still limited in several aspects:

- Evaluation Protocols: we use LLMs such as ChatGPT as evaluators to evaluate the characteristics generated, then we study massive cases to analyze the trained agents in experience memorizing, characteristic maintaining, etc. Evaluating agents is challenging since there are no standard metrics or protocols to evaluate character simulacra. Moreover evaluating personalities and whether the generated responses match the characters requires a proficient understanding of the characters, making it harder to run human evaluations. In the future, protocols that evaluate character simulacra are needed.
- Limited data: in our work, we narrate scenes that are based on character profiles, which is not sufficient to represent the whole life of a person or even one aspect of a real person. Future works can focus on using biographies, interviews, historical comments as well and crafted scenes to train simulacra to learn more details about certain characters.
- Base model: The outcomes of supervised finetuning are highly affected by the base models, including their pre-training data distribution, their model architecture, and scale. Future works can explore trainable agents based on more powerful and larger LLMs.
- Potential Harm: in character simulacra, the generated texts can be offensive since the character might be flawed or even vicious such as Voldemort. And a vivid simulacrum of Machiavelli may manipulate people to do harmful activities. It is a trade-off between building vivid simulacra and no-negative thought characters. Such a problem can be more crucial as LLMs grow even stronger.


## Ethics Statement

Agents could be trained on private or personally identifiable data to simulate specific individuals. In
this work, we select historical figures and imaginary characters with profiles collected from publicly available Wikipedia pages to avoid any privacy issues or personal data. The experience data we produce are drawn from ChatGPT-generated texts based on facts provided by Wikipedia. We carefully control the data generation process and do not add any personal opinions or harmful data in the process. Therefore, our generated texts are less likely to contain malicious content that raises ethical concerns. Also, we use open-source LLMs to train character simulacra. Thus, the trained agents are less likely to produce harmful content. Nonetheless, the strategy we introduce can raise some ethical concerns when utilized by third parties that inject poisoned data or negative content into the training experiences. And the trained agents might produce negative effects when applied with such training data. Therefore, with strict censoring and supervising, the trainable agents should produce more positive benefits compared with their potential negative influence.

## Acknowledgement

We especially thank Ming Zhong for the helpful proofreading and suggestions on the paper. This work was supported by the National Key Research and Development Program of China (No.2022ZD0160102) and National Natural Science Foundation of China (No.62022027).

## References

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

Joseph Bates. 1994. The role of emotion in believable agents. Commun. ACM, 37(7):122-125.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.

Rodney A. Brooks, Cynthia Breazeal, Marko Marjanovic, Brian Scassellati, and Matthew Williamson. 2000. The cog project: Building a humanoid robot. In Computation for Metaphors, Analogy, and Agents, number 1562 in Lecture Notes on Artificial Intelligence, pages 52-87, Berlin. Springer-Verlag.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Chris Callison-Burch, Gaurav Singh Tomar, Lara Martin, Daphne Ippolito, Suma Bailis, and David Reitter. 2022. Dungeons and dragons as a dialog challenge for artificial intelligence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9379-9393, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Jonas Freiknecht and Wolfgang Effelsberg. 2020. Procedural generation of interactive stories using language models. In International Conference on the Foundations of Digital Games (FDG '20), page 8, Bugibba, Malta. ACM.

Zhijiang Guo, Michael Sejr Schlichtkrull, and Andreas Vlachos. 2022. A survey on automated fact-checking. Trans. Assoc. Comput. Linguistics, 10:178-206.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.

Hyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, et al. 2022. Soda: Million-scale dialogue distillation with social commonsense contextualization. arXiv preprint arXiv:2212.10465.

Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 93329346. Association for Computational Linguistics.

John Laird and Michael VanLent. 2001. Human-level ai's killer application: Interactive computer games. AI Magazine, 22(2):15.

Neil Madden and Brian Logan. 2007. Collaborative narrative generation in persistent virtual environments. In Intelligent Narrative Technologies, Papers from the 2007 AAAI Fall Symposium, Arlington, Virginia, USA, November 9-11, 2007, volume FS-07-05 of AAAI Technical Report, pages 71-78. AAAI Press.

Shohei Miyashita, Xinyu Lian, Xiao Zeng, Takashi Matsubara, and Kuniaki Uehara. 2017. Developing game ai agent behaving like human by mixing reinforcement learning and supervised learning. In Proceedings of the 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD), pages 153-158, Kanazawa, Japan.

Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Dung Tien Nguyen, Duc Thanh Nguyen, Thien Huynh-The, Saeid Nahavandi, Thanh Tam Nguyen, Quoc-Viet Pham, and Cuong M Nguyen. 2022. Deep learning for deepfakes creation and detection: A survey. Computer Vision and Image Understanding, 223:103525.

OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.

Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2022. Social simulacra: Creating populated prototypes for social computing systems. In In the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22), UIST '22, New York, NY, USA. Association for Computing Machinery.

Mark O. Riedl. 2012. Interactive narrative: A novel application of artificial intelligence for computer games. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI'12), pages 21602165 .

Mark O. Riedl and R. Michael Young. 2005. An objective character believability evaluation procedure for multi-agent story generation systems. In Proceedings of the 5th International Working Conference on Intelligent Virtual Agents (IVA'05), pages 58-70, Kos, Greece.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.

Frank Thomas and Ollie Johnston. 1981. Disney Animation: The Illusion of Life. Abbeville Press, New York.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023a. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.

Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying Tai, Chengjie Wang, Jilin Li, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2021. Hififace: 3d shape and semantic prior guided high fidelity face swapping. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 1136-1142. ijcai.org.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.

Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022a. Promptchainer: Chaining large language model prompts through visual programming. In $\mathrm{CHI}$ EA '22: Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems.

Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022b. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In CHI '22: Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.

Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling. arXiv preprint arXiv:2303.03926.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.
