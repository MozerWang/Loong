# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes 

Ziang Guo ${ }^{1 *}$, Artem Lykov ${ }^{1 *}$, Zakhar Yagudin ${ }^{1 *}$, Mikhail Konenkov ${ }^{1}$ and Dzmitry Tsetserukou ${ }^{1}$


#### Abstract

Recent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields. However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals. To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model. Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system. In the real-world driving dataset, our system achieved $96.16 \%$ success rate in night scenes and $89.7 \%$ in gloomy scenes regarding reasonable predictions. Our Codriver dataset will be released at https://github.com/ZionGo6/Codriver.


## I. INTRODUCTION

## A. Motivation

In autonomous driving system development, two main solutions have been presented both in the academic and industrial fields till now. The first type is modular system design with independent modules such as perception, prediction, control, planning, etc. This design can empower distributed development and flexible extension, while the errors of such a system could accumulate because of asynchronization and delay of communication among modules [1]. The second type is end-to-end system design connecting sensor input and planning policy directly bypassing intermediate tasks and enabling the simple design of the network. But end-toend models demand their interpretability and logicality [2]. Generally, due to the complex traffic environment, long-tail data and dynamic scenes are still remaining limitations of these existing solutions [3].

Promisingly, Large Language Models (LLMs) have been actively developed in recent years, bridging human interaction and reasoning. Based on the advancements in LLMs, in the driving scene, LLMs can provide a more holistic understanding of the driving environment, allowing vehicles to respond more effectively to various driving scenarios with human-like logic which helps alleviate public concerns about the safety and reliability of autonomous vehicles [4],

* denotes the equal contribution. 1 The authors are with the Intelligent Space Robotics Laboratory, Center for Digital Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia \{ziang.guo, artem.lykov, Zakhar.Yagudin, mikhail.konenkov, d.tsetserukou@skoltech.ru\}

![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-1.jpg?height=732&width=810&top_left_y=566&top_left_x=1102)

Fig. 1: System overview. Our Visual Language Model module receives the image input and system prompt, publishing the analysis of environment and instruction results in a behavior tree format. Then the behavior tree of instruction results is mapped to agent behaviors according to the analysis of the environment.

[5]. However, to contribute to the precision and robustness requirements of autonomous driving, LLMs need more longterm verification and real-world experiments [6].

In this work, we introduce Co-driver, an autonomous driving assistant to provide the instructions for driving behavior at an agent level based on the analysis of visual perception input.

## B. Related Work

1) End-to-End Autonomous Driving: Recently, end-toend autonomous driving is developed vigorously. Hao Shao et al. present ReasonNet [7], an end-to-end driving framework that utilizes both temporal and global information of the driving scene to effectively predict the future evolution of the scene and behaviors of objects, especially in dense traffic scenarios. Jia et al. [8] propose a cascaded decoder paradigm for predicting the future action of the ego vehicle in a coarse-tofine fashion, inspired by the behavior of human drivers who check their intended target for safety and legitimacy. Hu et al. [2] propose a planning-oriented framework that incorporates full-stack driving tasks in one network, prioritizing planning and facilitating task coordination. Jiaxun Cui et al. [9] intro-
duce an end-to-end learning model that utilizes cross-vehicle perception for vision-based cooperative driving, aiming to enhance autonomous driving in dangerous or emergencies. Among them, ReasonNet approaches perception via global reasoning, but these methods still do not utilize and integrate a human-driver understanding of complex traffic scenes to fulfill the decision-making tasks.
2) Large Language Models: However, a recent study released the potential combination of large language models and autonomous driving. Boyi Li et al. [10] present LLaDA, a tool that enables human drivers and autonomous vehicles to adapt their driving behavior to new locations by interpreting traffic rules using large language models. Sharan et al. [11] propose a hybrid planner, which combines a rule-based planner with an LLM-based planner. Hao Shao et al. [3] introduce a language-guided, end-to-end autonomous driving framework that integrates multimodal sensor data with natural language instructions, enabling interaction with humans and navigation software. Can Cui et al. [12] introduce a framework that uses large language models to process verbal commands from humans and make autonomous driving decisions, taking into account contextual information and personalized preferences for safety, efficiency, and comfort. Wang et al. [13] explore the integration of Large Language Models (LLMs) into autonomous driving systems to enhance their performance and safety by leveraging the LLMs' common-sense knowledge, reasoning abilities, and human interaction capabilities. The above work mainly shows the exploitation of language modal and its extensions. However, in the autonomous driving field, a combination of multimodal sensors, especially including visual modal is critical for necessary scene understanding.
3) Visual Language Models in Autonomous Driving Scenarios: In this section, we explore various approaches to integrating Visual Language Models (VLMs) into autonomous driving scenarios, highlighting their roles in environmental analysis and decision-making. DriveLM [14] focuses on the integration of VLMs into end-to-end driving systems via Graph Visual Question Answering (VQA). By utilizing this approach, DriveLM enables comprehensive reasoning about driving scenes, encompassing perception, prediction, and planning stages. The introduced DriveLM-Data dataset and baseline approach provide a framework for end-to-end autonomous driving, showcasing competitive performance even when faced with unseen objects or sensor configurations. RAG-Driver [15] addresses the crucial need for human-understandable explanations in autonomous driving systems. Employing retrieval-augmented multimodal large language models, RAG-Driver excels in producing detailed explanations for driving actions and accurate predictions of control signals. Its remarkable zero-shot generalization capabilities to previously unseen environments underscore its potential for real-world deployment. DriveVLM [16] introduces an autonomous driving system that effectively leverages VLMs for enhanced scene understanding and planning. Through the integration of chain-of-thought (CoT) modules, DriveVLM achieves a robust spatial understanding and real-time inference speed. Particularly noteworthy is DriveVLM-Dual, a hybrid system that combines VLMs with traditional autonomous driving pipelines, resulting in superior performance in navigating complex and unpredictable driving conditions.

All of the above research regarding visual language models needs heavy computation resources for both training and inference, which is a critical factor in the robustness and safety of autonomous driving. Besides, a hallucination of Large Language Models is still not explainable [17], resulting in challenges and risks in practical deployment. For Large Language Models, to output the coordinates of waypoints with high precision and stable response for autonomous driving in complex traffic and extreme conditions needs more real-world experiments and long-term verification.

## C. Contribution

Our main contributions to this work are as follows. An image dataset created in CARLA simulator [18] with defined weather, light, road surface, locality, and traffic conditions associated with a prompt dataset with control and behavior parameters based on the scenes defined in the image dataset. Besides, a pipeline of our Co-driver system is presented, where the CARLA simulator is used to run the simulation scenes, publishing the status information of the ego vehicle via Robot Operating System 2 (ROS2) [19] and our Visual Language Model module is wrapped within ROS2, reading the published ROS2 topics of front images of the camera on the ego vehicle. While analyzing the front images, our Visual Language Model module instructs and alters the driving behaviors of the ego vehicle in CARLA. Fig. 1 shows our pipeline in detail, where a single Nvidia 4090 24G GPU is able to handle our whole pipeline.

## II. SYSTEM OVERVIEW

Our system is driven by Qwen-VL by Bai et al. [20]. Qwen-VL is a leading open-source model in the field of Visual Language Models (VLM), showcasing exceptional capabilities in tasks such as image captioning, question answering, visual localization, and interactive tasks. This model processes both textual and visual data and excels in recognizing individual objects and their positions, as well as grounding tasks, which are crucial for our study.

Qwen-VL's high performance is attributed to its positionaware vision-language adapter and its efficient 3-stage training process. With a total of 9.6 billion parameters, the model comprises a visual encoder (1.9 billion parameters), a visionlanguage adapter ( 0.08 billion parameters), and the Qwen large language model ( 7.7 billion parameters).

The advanced environmental analysis capabilities of Qwen-VL, combined with the reasoning power of Qwen [21], make it particularly suitable for our task. The model's compact size allows for seamless integration into a selfdriving car's onboard computer, enabling efficient local deployment without sacrificing performance. This positions Qwen-VL as an ideal choice for enhancing autonomous driving systems.

System architecture is depicted in Fig. 1. The primary task of our system is to analyze the visual input from the front camera of the ego vehicle and draw conclusions about environmental information such as weather, light, road surface, locality, etc., and parameters of control such as maximum speed, maximum brake, maximum throttle, etc. Determining the driving behaviors of a self-driving car based on visual data is a complex task for Visual Language Models. However, by breaking down the task into a two-step process, it becomes manageable.

The task is decomposed to identify environmental information from an image by feeding specifically defined scenes from our image dataset to the model and to predict the levels of control and behavior parameters based on the described environmental data. Both of these tasks pose no significant challenges for a fine-tuned Visual Language Model, which ensures the practical pipeline of implementation in our proposed system.

In the first step of the mentioned task, our Visual Language Model module receives system prompts containing the mission description and destination, along with the images from the ego vehicle's front camera. In this stage, the module identifies locality, lighting, and weather conditions, as well as potential hazards in front. Then our module continues to generate the levels of control and driving behavior parameters, guided by the environmental parameters identified in the first step. Lastly, all the obtained parameters are mapped as a set of agent behaviors altering and influencing the driving style of the ego vehicle in the CARLA simulator based on the image input of our Visual Language Model module.

## III. MethodOlogY

## A. Dataset Collection

Our image dataset is collected in the CARLA simulator from the view of the front camera of ego vehicle under defined weather (clear, rainy, foggy), light (bright, gloomy, dark), locality (city, town, highway) conditions with a classification of safe and unsafe distance concerning the potential obstacle in front [18].

In our prompt dataset, system prompts are given as the request of accomplishment of the driving missions and notice of the environmental information from the perspective of a driver's view. Then we include the defined environmental information and the suggestions for vehicle control and driving behavior regarding control type, maximum speed, maximum brake, maximum throttle, maximum acceleration, and maximum steering speed as the output prompt in a behavior tree format. Here is an example of our dataset in Fig. 2 .

## B. Training Recipe

The Visual Language Model (VLM) of our system was trained on the foundation of the Qwen-VL architecture utilizing the Quantized Low-Rank Adaptation (QLoRA) method [22], which is a form of Parameter Efficient Fine-tuning (PEFT) [23]. During the training process, the weights of the visual encoder were kept frozen to focus on optimizing the language aspect of the model.

Training was carried out on a single Nvidia RTX 4090 GPU, which provided $24 \mathrm{~GB}$ of video memory for processing. The dataset, containing a total of 221,228 samples, was divided into batches of 6 samples each to maintain efficient training throughput. Additionally, gradient accumulation steps were set to 8 , resulting in an epoch comprising approximately 4,600 steps.

With a learning rate of $1 \mathrm{e}-4$, the model quickly adapted to the target emergent capabilities and responded to the desired format. This process only required one epoch of training, which took around 25 hours to complete. Despite the relatively short training time, the approach proved effective, yielding satisfactory results in terms of model performance and output quality.

The progression of the training process is depicted in the training curve presented in Fig. 3 , showcasing the changes in loss over time and offering insights into the model's learning dynamics.

## IV. EXPERIMENTAL RESULTS

## A. Experiment Setup

To verify our system's effectiveness, we conducted two types of experiments. First, in CARLA, test scenes were created with adjustable weather, maps, and traffic settings. During the running of the test simulation, our Visual Language Model module was on, reading the front images from the ego vehicle and performing the scene understanding and behavior instructions. We recorded the driving scenes with vehicle trajectories and vehicle status information such as speed, acceleration, etc. Second, we verified the generalization ability of our system's Visual Language Model module on HawkDrive dataset [24] with real-world driving scenes in gloomy night conditions.

## B. CARLA Simulation

In the CARLA simulator, we compared the driving behaviors between the default built-in agent and our Co-driver

![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-3.jpg?height=608&width=832&top_left_y=1840&top_left_x=1096)

Fig. 2: Example of our dataset with image set and prompt set.

![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-4.jpg?height=605&width=762&top_left_y=169&top_left_x=237)

Fig. 3: The training loss decreases while our fine-tuning progress.

agent in Town 10, Town 04, and Town 02. CARLA map Town 04 in Fig. 5 is selected as one of our experimental maps to demonstrate our results. With the default agent, the ego vehicle was not able to switch driving behaviors along with the weather, light, and traffic conditions. In Fig. 6, under both rainy and foggy weather with gloomy light conditions, the driving behaviors of the ego vehicle along the same trajectory remained nearly identical since the planning and control modules were driven by defined rules only. When passing by the $90 \mathrm{~km} / \mathrm{h}$ speed limit sign, the default agent guided the ego vehicle to reach $90 \mathrm{~km} / \mathrm{h}$, ignoring the environmental information. In Fig. 7, with our Co-driver system running, under both rainy and foggy weather with gloomy light conditions, the driving behaviors of the ego vehicle were instructed according to the front image input. Based on the output of our Visual Language Model module, even if the ego vehicle passed by speed limit signs, our Co-driver system guided the ego vehicle to drive under the instructions.

Fig. 4 shows the acceleration recording of our experiments. To interpret the results, we identified the relative maxima and minima throughout the acceleration recording. Then the frequency of fluctuations is calculated by counting the number of peaks and valleys in the data. Finally, we use the ratio of frequency of fluctuations and running time to denote the smoothness of driving behaviors as follows,

$$
\begin{equation*}
\dot{\mathcal{F}}_{T}=\frac{\text { Concatenate }(\text { relmin }(\mathcal{X}), \operatorname{relmax}(\mathcal{X})) \times \frac{1}{2}}{T} \tag{1}
\end{equation*}
$$

where the arrays of indices of relative minima and relative maxima are concatenated as a combined array that contains the indices where the values in the input data $\mathcal{X}$ reach relative minima and maxima. Smaller $\dot{\mathcal{F}}_{T}$ means smoother driving with less intensive fluctuation of acceleration. $T$ is the running time of our experiments. The comparison of acceleration recording is presented in Table I.
![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-4.jpg?height=640&width=832&top_left_y=173&top_left_x=1080)

(a) Acceleration recording of (b) Acceleration recording of ego ego vehicle with default agent vehicle with Co-driver agent ununder foggy and gloomy con- der foggy and gloomy conditions ditions (above) and rainy and (above) and rainy and gloomy gloomy conditions (below). conditions (below).

Fig. 4: Ego vehicle test with Co-driver agent in CARLA map Town 04 .

TABLE I: Comparison of the smoothness of driving behaviors between default agent and Co-driver agent in Town 04.

|  | Foggy + Gloomy | Rainy + Gloomy |
| :---: | :---: | :---: |
| $\mathcal{F}_{T}$ of Default Agent | 0.117 | 0.153 |
| $\mathcal{F}_{T}$ of Co-driver Agent | $\mathbf{0 . 0 2 1}$ | $\mathbf{0 . 1 0 4}$ |

## C. Real-World Driving Dataset

To present the generalization ability of our system, HawkDrive dataset [24] which provides continuous driving scenes with different light conditions in a closed loop, is used to test the Visual Language Model module of our Co-driver system. In the night scene of the dataset, we labeled the night images corresponding to the ground truth regarding safety distance, weather, light condition, road surface and locality. Among 1,067 images, 41 images gave critical failure of instructions and understanding, showing a $96.16 \%$ precision of successful prediction. Meanwhile, the safety distance was detected correspondingly regarding potential obstacles in the frames in night scenes. The results are displayed in Fig. 8 In the gloomy scene of the dataset, throughout 952 images, 98 images gave critical failure of prediction, showing a $89.7 \%$ precision of successful prediction. The results are presented in Fig. 9

## V. CONCLUSION

In this work, we propose Co-driver, an autonomous driving assistant system to empower autonomous driving vehicles with adjustable driving behaviors based on the understanding of complex road scenes including safety distance, weather, light conditions, road surface and locality. To be practical, we present our system in a pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2), while verifying the effectiveness of our system by comparing the

![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-5.jpg?height=536&width=542&top_left_y=171&top_left_x=344)

Fig. 5: CARLA map Town 04 is a small town embedded in the mountains with a special "figure of 8 " infinite highway.
![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-5.jpg?height=614&width=808&top_left_y=866&top_left_x=191)

(a) The front images were (b) The speed recording of the recorded during the experi- ego vehicle running with dement in foggy and gloomy fault built-in agent in foggy and conditions (above) and rainy gloomy conditions (above) and and gloomy conditions (be- rainy and gloomy conditions (below).

low).

Fig. 6: Ego vehicle test with default built-in agent in CARLA map Town 04.

driving behaviors of the rule-based default agent with our Co-driver agent. In the real-world driving dataset, our system achieved a $96.16 \%$ success rate in night scenes and $89.7 \%$ in gloomy scenes of reasonable prediction. Besides, we also contributed a Co-driver dataset containing 221,228 image samples and a corresponding prompt set to spark further related research.

From our results, the promising capacity of our Co-driver system is displayed. With the vigorous development of Large Multimodal Models, our work is able to enlighten further advancement in the autonomous driving field.

## ACKNOWLEDGMENT

This project is supported by Skolkovo Institute of Science and Technology, Moscow, Russia.
![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-5.jpg?height=608&width=792&top_left_y=190&top_left_x=1096)

(a) Co-driver agent running (b) The speed of ego vehicle under foggy and gloomy con- was adjusted according to the ditions (above) and rainy and image input even if passing by gloomy conditions (below). the speed limit signs.

Fig. 7: Ego vehicle test with Co-driver agent in CARLA map Town 04
![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-5.jpg?height=272&width=788&top_left_y=1091&top_left_x=1117)

(a) Failure case of the test on (b) Successful case of the test HawkDrive dataset in a night on HawkDrive dataset in a scene. night scene.

Fig. 8: Visual Language Model module of our Co-driver system test on HawkDrive dataset in a night scene.
![](https://cdn.mathpix.com/cropped/2024_06_04_a3403ab05a5c54cff7c9g-5.jpg?height=274&width=802&top_left_y=1614&top_left_x=1103)

(a) Failure case of the test (b) Successful case of the test on HawkDrive dataset in a on HawkDrive dataset in a gloomy scene. gloomy scene.

Fig. 9: Visual Language Model module of our Co-driver system test on HawkDrive dataset in a gloomy scene.

## REFERENCES

[1] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H. Li, "Planning-oriented autonomous driving," 2023.

[2] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., "Goal-oriented autonomous driving," arXiv preprint arXiv:2212.10156, 2022.

[3] H. Shao, Y. Hu, L. Wang, S. L. Waslander, Y. Liu, and H. Li, "Lmdrive: Closed-loop end-to-end driving with large language models," arXiv preprint arXiv:2312.07488, 2023.

[4] W. Han, D. Guo, C.-Z. Xu, and J. Shen, "Dme-driver: Integrating human decision logic and 3d scene perception in autonomous driving," arXiv preprint arXiv:2401.03641, 2024.

[5] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin et al., "A survey on large language model based autonomous agents," Frontiers of Computer Science, vol. 18, no. 6 , pp. 1-26, 2024 .

[6] S. Kambhampati, K. Valmeekam, L. Guan, K. Stechly, M. Verma, S. Bhambri, L. Saldyt, and A. Murthy, "Llms can't plan, but can help planning in llm-modulo frameworks," arXiv preprint arXiv:2402.01817, 2024.

[7] H. Shao, L. Wang, R. Chen, S. L. Waslander, H. Li, and Y. Liu, "Reasonnet: End-to-end driving with temporal and global reasoning," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13723-13 733.

[8] X. Jia, P. Wu, L. Chen, J. Xie, C. He, J. Yan, and H. Li, "Think twice before driving: Towards scalable decoders for end-to-end autonomous driving," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 21 983-21 994.

[9] J. Cui, H. Qiu, D. Chen, P. Stone, and Y. Zhu, "Coopernaut: End-toend driving with cooperative perception for networked vehicles," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 17252-17 262.

[10] B. Li, Y. Wang, J. Mao, B. Ivanovic, S. Veer, K. Leung, and M. Pavone, "Driving everywhere with large language model policy adaptation," arXiv preprint arXiv:2402.05932, 2024.

[11] S. Sharan, F. Pittaluga, M. Chandraker et al., "Llm-assist: Enhancing closed-loop planning with language-based reasoning," arXiv preprint arXiv:2401.00125, 2023.

[12] C. Cui, Z. Yang, Y. Zhou, Y. Ma, J. Lu, and Z. Wang, "Large language models for autonomous driving: Real-world experiments," arXiv preprint arXiv:2312.09397, 2023.

[13] Y. Wang, R. Jiao, C. Lang, S. S. Zhan, C. Huang, Z. Wang, Z. Yang, and Q. Zhu, "Empowering autonomous driving with large language models: A safety perspective," arXiv preprint arXiv:2312.00812, 2023.

[14] C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, P. Luo, A. Geiger, and H. Li, "Drivelm: Driving with graph visual question answering," arXiv preprint arXiv:2312.14150, 2023.

[15] J. Yuan, S. Sun, D. Omeiza, B. Zhao, P. Newman, L. Kunze, and M. Gadd, "Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model," arXiv preprint arXiv:2402.10828, 2024.

[16] X. Tian, J. Gu, B. Li, Y. Liu, C. Hu, Y. Wang, K. Zhan, P. Jia, X. Lang, and H. Zhao, "Drivevlm: The convergence of autonomous driving and large vision-language models," arXiv preprint arXiv:2402.12289, 2024.

[17] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin et al., "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions," arXiv preprint arXiv:2311.05232, 2023.

[18] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "CARLA: An open urban driving simulator," in Proceedings of the 1st Annual Conference on Robot Learning, 2017, pp. 1-16.

[19] S. Macenski, T. Foote, B. Gerkey, C. Lalancette, and W. Woodall, "Robot operating system 2: Design, architecture, and uses in the wild," Science Robotics, vol. 7, no. 66, p. eabm6074, 2022. [Online]. Available: https://www.science.org/doi/abs/10.1126/ scirobotics.abm6074

[20] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, "Qwen-vl: A frontier large vision-language model with versatile abilities," arXiv preprint arXiv:2308.12966, 2023.

[21] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., "Qwen technical report," arXiv preprint arXiv:2309.16609, 2023.

[22] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Qlora: Efficient finetuning of quantized llms," Advances in Neural Information Processing Systems, vol. 36, 2024

[23] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, "On the effectiveness of parameter-efficient fine-tuning," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023, pp. 12799-12807.

[24] Z. Guo, S. Perminov, M. Konenkov, and D. Tsetserukou, "Hawkdrive: A transformer-driven visual perception system for autonomous driving in night scene," arXiv preprint arXiv:2404.04653, 2024.

