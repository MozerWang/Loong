# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models 

Chengxing Xie *<br>Department of Computer Science<br>The University of Hong Kong<br>xiechengxing34@gmail.com

Difan Zou<br>Department of Computer Science<br>Institute of Data Science<br>The University of Hong Kong<br>dzou@cs.hku.hk


#### Abstract

Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task [54]. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10 \times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.


## 1 Introduction

Recently, large language models (LLMs), such as GPTs [1, 34] and LLaMAs [44], have demonstrated remarkable potential in handling diverse tasks across various domains, showcasing impressive generalization capabilities. [46] and [33] show that LLM agents could play card games with humanlike proficiency and, in some instances, even outperform most human players in some games [10]. Moreover, [61], [9], and [11] revealed that LLM agents could solve daily tasks such as cooking and door-opening. Additionally, LLM agents can also establish complex software systems through collaboration with different agent roles [18, 38].

Nevertheless, LLM agents still struggle with certain tasks [23, 45, 51], which are relatively solvable to humans. We categorize these challenges as Multi-Phases Planning Tasks. These tasks are difficult because their solutions can be divided into multiple interconnected phases, each requiring

Preprint. Under review.[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-02.jpg?height=760&width=1354&top_left_y=254&top_left_x=384)

Figure 1: Our human-like planning framework consists of three key parts. In the Outline Generation phase, LLM agents produce rough plans and identify key information related to the query, establishing the foundation for detailed future planning. In the Information Collection phase, LLM agents gather the essential data required for comprehensive planning. Finally, in the Plan Making phase, LLM agents explore the potential plan space and return a well-structured, reasonable plan.

shared information and having dependencies with each other. For example, creating a travel plan [54] requires humans to plan for multiple days, encompassing various aspects. This involves addressing different elements in different phases of the planning process. Similarly, solving GitHub issues involves checking various functions and systematically locating and resolving code problems one by one [23]. The procedures for solving these tasks are often not readily available online and cannot be easily converted into instruction fine-tuning data formats to train LLMs. Consequently, tackling these types of tasks remains a significant challenge for today's LLM agents [23, 45, 51, 54].

We categorize the stages of solving such tasks into three main phases, detailed as follows: Outline Generation Phase: When confronted with a Multi-Phase Problem, individuals typically begin by sketching a preliminary outline outlining the path toward a solution. Information Collection Phase: Generally, to solve this type of task, the information provided in the task description alone is completely insufficient. Therefore, individuals must proactively identify the required information and ascertain how to obtain it effectively. Plan Making Phase: Armed with the necessary information, individuals proceed to formulate a plan. Multi-phase problems inherently entail numerous constraints, encompassing factors such as human resources, physical limitations, and more. Consequently, devising effective plans necessitates meticulous consideration of diverse constraints. Given the challenges posed by Multi-Phase Problems, we have chosen travel planning as our focal point for several reasons. Firstly, it is a time-consuming and often challenging problem for many people, demonstrating its difficulty. Secondly, travel planning inherently involves a multitude of constraints, requires long-term strategizing, and demands significant travel-related information gathering. These complexities make travel planning a fitting subject for exploring and improving planning strategies.

Previous reasoning strategies [42, 49, 59] have shown limited effectiveness in addressing the complexities of travel planning [54]. However, by understanding and emulating the processes humans use to tackle Multi-Phases Problems, we can develop more robust planning frameworks to enable LLM agents to tackle such challenges effectively. Our proposed framework aims to capture the essence of human reasoning processes, particularly in the context of Multi-Phase Problems, to enhance the performance of LLM agents.

Our framework is composed of three main components. The first is Outline Generation. When tackling a complex problem, people often create a rough plan to guide them through the process. Similarly, we generate a travel planning outline using multi-agent collaboration, where each agent contributes to a specific part of our travel outline, establishing a foundational guide for planning.

The second component is Information Collection. Just as people gather relevant travel information, LLM agents require sufficient data to create reliable plans. This step ensures that agents have all the necessary details for making informed decisions. The third component is Plan Making. After collecting adequate information, the LLM agent creates detailed daily plans. Given the inherent complexity of travel planning, we employ a plan search and evaluation method. Multiple plans are generated in each iteration, and an evaluation agent identifies the best plan and flags errors in each option. Our framework demonstrates that this structured approach leads to impressive performance on this complex task, improving the complex task planning ability of LLM agents. The major findings of our work are summarized as follows:

- Our work underscores the challenges LLM agents face with Multi-Phase Planning tasks, highlighting the necessity of employing a novel reasoning framework for effective problem-solving.
- We found that utilizing a human reasoning framework for complex tasks is a key factor in enhancing LLM agents' performance on these tasks.
- By identifying key factors contributing to effective problem-solving and integrating them into a human-like planning framework, we enable LLM agents to exhibit human-like reasoning, resulting in impressive performance improvements, significantly enhancing LLM agents' performance in travel planning tasks.
- Through extensive experimentation, we validate the effectiveness of our approach, demonstrating its capability to tackle complex planning problems with remarkable success.


## 2 Related Works

### 2.1 Reasoning Strategy for LLM Agents

Planning without Additional Components: In recent times, the development of large language models (LLMs) agents has enabled numerous tasks to be addressed by directly inputting questions into these models. However, LLM agents often struggle with some problem-solving, particularly in areas like mathematics and other intricate planning tasks. To address this limitation, recent research has developed various reasoning strategies aimed at enhancing the problem-solving abilities of LLM agents [20]. One common strategy, employed by works like [41, 49, 59], leverages the divide-andconquer approach. This involves breaking down a complex task into simpler subtasks and allowing the LLM agents to tackle each subtask sequentially. This method has shown improved performance across a range of tasks. Other works [3, 16, 47, 52, 58, 63] employ similar strategies using tree searches, such as Monte Carlo Tree Search (MCTS), A*, Breadth-First Search (BFS), and Depth-First Search (DFS). They generate multiple alternative plans via various sampling methods and select the optimal plan through different selection techniques. Reflecting on and refining plans based on prior experiences also significantly improves the planning process of LLM agents [13, 19, 31, 42]. By reevaluating their plans, LLM agents can avoid recurring errors, thereby enhancing their problem-solving capabilities.

Planning with Addition Components: The planning capabilities of LLM agents can also be improved through interactions with other components. For instance, [28], [14], [8], and [7] combine symbolic planners with LLM agents, utilizing the natural language generation capabilities of LLMs to create formalized task descriptions. Other works [24, 29, 32, 57] enhance the problem-solving ability of LLMs by integrating additional memory modules. A particularly effective approach involves enabling LLM agents to access various APIs and tools [30, 35, 37, 39, 40, 48, 56, 60]. Some tools can be directly included in the context [41, 43, 50] as prompt. However, improving an LLM agent's toolusing abilities often requires generating appropriate datasets and fine-tuning the models [37, 40, 56]. By providing access to external tools, LLM agents can tackle tasks that are otherwise challenging, such as computational problems and formula verification. Our planning framework integrates these diverse reasoning strategies to improve overall planning capabilities, allowing for more sophisticated problem-solving and strengthening the effectiveness of LLM agents in handling complex tasks.

### 2.2 Multi-Agents Framework

Multi-agent frameworks [15] have garnered significant interest from researchers due to their flexibility across a broad range of tasks. These frameworks can be utilized in simulations of various kinds, including game simulations [33, 46, 55], economic simulations [26, 27], and societal simulations

[2, 12, 36, 53, 62]. Moreover, multi-agent frameworks demonstrate superior performance compared to single-agent systems in solving diverse tasks. For instance, [25], [17], and [6] have developed general multi-agent frameworks that enhance task performance. For specific tasks, works like [17, 18, 38] propose specialized frameworks that enable agents to autonomously develop software. In addition, multi-agent systems have been effectively employed for reasoning in embodied environments [9, 61] and have demonstrated their potential in scientific research [4, 5]. Our framework also incorporates a multi-agent system to enhance overall planning capabilities, leveraging the collaborative and complementary strengths of multiple agents for more effective problem-solving.

## 3 Method

### 3.1 Task Description:

We introduce the task setting of TravelPlanner [54], where the objective is for LLM agents to provide a reasonable travel plan based on a given query, while adhering to the specified constraints and leveraging commonsense knowledge. Each day's plan includes details about the current city, attractions, accommodation, transportation, and dining options (See Appendix A.6 for travel plan examples). A reasonable plan must comply with commonsense principles and meet all requirements specified in the travel query, such as avoiding repeated visits to the same attractions and ensuring the plan stays within the specified budget. An initial analysis of GPT-4-Turbo's performance in [54] revealed that the most advanced model still struggles with efficient information collection and valid plan making. The details are as follows:

- Generating unreasonable travel routes, such as a number of cities visited that is inconsistent with the query or not returning to the original city on the last day.
- Utilizing unreasonable transportation methods, like combining self-driving and flight in one trip.
- The plan made may include unreal information (hallucination) in the plans.
- LLM agents may omit essential information when making the plan and disregard the constraints specified in the query, leading to problematic and incomplete plans.

To address the issues of unreasonable travel routes and transportation methods, we employ the Outline Generation phase (Section 3.3) to create logical outlines and appropriate transportation options. To alleviate the hallucination problem, we enhance the Information Collection phase (Section 3.4) to ensure more accurate data gathering. Finally, to deal with the missing information during plan creation, we utilize the Plan Making phase (Section 3.5) to support the development of correct and comprehensive plans.

### 3.2 Framework Overview

Our framework, as shown in Fig 1, employs a human-like reasoning approach to address travel planning problems. Based on the specific query, we first generate an outline for the query (Section 3.3). Following the Outline Generation, LLM agents equipped with Strategy block and Knowledge Block proceed with Information Collection (Section 3.4) During this stage, once sufficient information is gathered for a single day's plan, the agent creates the daily plan in Plan Making phase (Section 3.5). The final travel plan is composed of each daily plan.

### 3.3 Outline Generation

Each travel query (details in Appendix A.5) involves numerous elements and spans several days, making it challenging to generate a comprehensive plan all at once. When humans tackle such tasks, they typically begin by drafting a rough outline. This outline should encompass the route, transportation options, and key points in the query, facilitating the planning process (details in Appendix A.7). To replicate this approach, we introduce the Route Generation phase, which incorporates transportation evaluation to determine the route and transportation components. Additionally, we integrate two additional agents to generate the key points of the travel, enhancing the usefulness of the outlines.

Route Generation: Drawing inspiration from human reasoning patterns, we've developed the Route Generation component of Outline Generation. Firstly, the query is first passed to the PathFinder

![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-05.jpg?height=637&width=1396&top_left_y=234&top_left_x=362)

Figure 2: This is our transportation evaluation process for each route generated by PathFinder Agent. We will evaluate whether this route is reasonable under the limitation from the specific travel planning query, e.g., cannot take flight. If not reasonable, we will redo the route by giving feedback. Otherwise, if the route is not reasonable for some kind of transportation, we will add the constraint.

Agent. This agent generates a rough route for the entire trip, including city transfers and exploratory travel information. This preliminary route serves as a guide for the subsequent Information Collection and Plan Making phases, providing a clear roadmap for the journey and making the planning process more structured and transparent.

Transportation Evaluation: When constructing a travel route, humans typically do not initially verify transportation availability between target cities, assessing options later based on preferences and availability. Similarly, we've observed that LLM agents may generate invalid routes due to a lack of detailed transportation information between cities. To address this issue, we propose adding an evaluation stage after route generation. As shown in Fig 2, during this stage, each route is evaluated to determine its rationality from a transportation perspective. If the route is perfect, we don't need to do anything. If the route cannot be traveled by driving, we inform the agent accordingly. Similarly, if the route cannot be reached by any means of transportation, we provide the feedback to the PathFinder Agent and prompt the agent to revise the route accordingly.

Keypoints Generation: When humans receive a query, they focus on the key points and ignore irrelevant details. Inspired by this, we introduced the Keypoints Agent (Fig 1 left) to identify the critical points in the query that need to be considered by the Plan Agent. Moreover, as [54] demonstrated, LLM agents tend to overlook common sense when planning, such as navigating the same restaurant or attraction. To address this issue, the Commonsense Agent (Fig 1 left) generates basic guides related to travel planning. This agent doesn't have access to the specific query, as we believe these guides are common sense and universally applicable to every query.

These three parts combine with each other to generate a useful and detailed travel outline, building a strong foundation for the planning task.

### 3.4 Information Collection

After the Outline Generation phase, humans proceed to collect necessary travel-related information, such as specific attractions and restaurants. Collecting sufficient information is crucial as it can significantly decrease the probability of hallucination. In our framework, the Information Collection process begins with the Thought Agent generating the next steps based on the Strategy Block. Subsequently, the Tool Agent utilizes the output of the Thought Agent to generate a suitable function expression. The result of this function, primarily comprising travel information, is then recorded in the Knowledge Block with a description from the Description Agent. Finally, the selected information is forwarded to the Plan Agent for generating the daily travel plan.

Strategy Block: In the information collection process, humans typically remember the types of information they have gathered, which guides their subsequent steps. To emulate this procedure, we introduce a component called the Strategy Block as shown in Fig 1, primarily used by the Thought Agent. First, the outline is stored in the Strategy Block to guide the information collection
process. The Strategy Block also informs the Thought Agent which day it is in the travel plan and short descriptions of the collected data. Besides, we streamline the prompts for better workflow management. Compared to the scratchpad approach in [54], our design provides more task-specific details and better supports the Thought Agent in collecting information.

Tool Agent: We've observed that as the planning process progresses, the context length can become too long, leading to potential oversight of tool documents placed at the beginning of input messages. Thus, we introduce the Tool Agent tasked with generating the correct function expression format messages. The Tool Agent's input includes only the tool document, the query, and some previous content from the Strategy Block to ensure it can generate the correct information based on the context.

Knowledge Block: Humans will make their plans by examining the detailed information they have previously collected. To mimic this process, we introduce the Knowledge Block, as shown in Fig 1, primarily used by the Plan Agent. In Fig 3 left part, this block will automatically record all information and the description of each information, which is generated by Description Agent. However, when travel plans span long periods, the extensive information can make it difficult to extract pertinent details and may even exceed the LLM's context window length. To address this, we implement a stack-like structure within the Knowledge Block. When the Plan Agent needs detailed information, the block only "pops out" the information collected over the past two days, restricted by a minimal threshold. As shown in Fig 3 If the number of knowledge items collected exceeds the threshold, the block pops out the same number of items. If it exceeds the threshold, all

![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-06.jpg?height=414&width=681&top_left_y=644&top_left_x=1058)

Figure 3: This image illustrates the Knowledge Block workflow. The left part is the Knowledge Write process, where a function's result is written to the top of the Knowledge Block. About Knowledge Read, when the knowledge needs to be popped out but is below the threshold, the Knowledge Block pops out enough items to meet the threshold. If the number exceeds the threshold, the items in past two days are popped out. knowledge items from the past two days are popped out.

Lastly, we found that presenting the data in a DataFrame format makes it difficult to read and interpret. Therefore, we have reorganized the information into a more reader-friendly structure. We directly specify each data's name just before presenting the data, like: Flight Number: F3502691 (see Appendix A.7.2 for more details).

### 3.5 Plan Making

When humans make plans, they typically do not plan everything at once; instead, they approach it step by step. Therefore, unlike the approach in [54] that plans the entire trip at once, we adopt a daily planning strategy. This method requires less information at each step, making the planning process easier. Each time the Thought Agent determines that sufficient information has been collected for a specific day, the Tool Agent calls the DailyPlanner tool, and the Plan Agent creates the daily plan.

Plan Search: When examining the results of Plan Agent, we observe that creating a plan often introduces various errors. These errors can stem from insufficient information collected during the information collection stage or from oversights during the planning stage. To address this issue, we propose a plan search method.

Each time the Plan Agent generates a daily plan, it creates several plans based on the same information. An Evaluate Agent reviews each plan, converting them into JSON format and using code to identify and rank errors. Based on these evaluations, we select the best plan. If significant errors are found, these errors are recorded in the Strategy Block, prompting further information collection, and the plans with errors are discarded. This discard process occurs only once per day's plan to prevent unlimited planning iterations. Through daily planning and plan search strategies, the agent can generate more reasonable and reliable travel plans.

Table 1: Our experimental results on the validation dataset of the TravelPlanner benchmark demonstrate that our framework enables GPT-3.5 to surpass previous GPT-4's performance. Additionally, our framework increases the Final Pass Rate of GPT-4-Turbo $10 \times$ compared to previous algorithms. For each model, using our framework results in significantly improved performance.

| Model | Delivery Rate | Commonsense Pass Rate Hard Constraint Pass Rate |  |  |  | Final Pass Rate |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Micro | Macro | Micro | Macro |  |
| TravelPlanner Result |  |  |  |  |  |  |
| Mistral-7B-32K [21] | 8.9 | 5.9 | $\overline{0} \quad$ | $\overline{0} \quad$ | 0 | 0 |
| Mixtral-8×7B-MoE [22] | 49.4 | 30.0 | 0 | 1.2 | 0.6 | 0 |
| GPT-3.5-Turbo [34] | 86.7 | 54.0 | 0 | 0 | 0 | 0 |
| GPT-4-Turbo [1] | 89.4 | 61.1 | 2.8 | 15.2 | 10.6 | 0.6 |
| Our Results |  |  |  |  |  |  |
| Mistral-7B-32K $\sqrt{21}$ | 39.4 | 24.0 | 1.1 | 0.5 | 0.6 | 0.6 |
| Mixtral-8×7B-MoE 22 | 67.8 | 40.2 | 0 | 0 | 0 | 0 |
| GPT-3.5-Turbo [34] | 100.0 | 75.1 | 15.6 | 15.5 | 4.4 | 2.2 |
| GPT-4-Turbo [1] | 91.7 | 74.6 | 24.4 | ![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-07.jpg?height=43&width=98&top_left_y=856&top_left_x=1199) | 16.7 | 6.7 |

## 4 Experiment

### 4.1 Experiment Setup

We utilize the TravelPlanner benchmark proposed by [54], which includes a variety of travel queries with different travel lengths and difficulty levels. This benchmark has a train, valid, and test set. The valid set has 180 queries and the test set has 1000 queries. We use the valid set to examine our framework as running the experiment on the test set is too expensive. We use gpt-3.5-turbo-1106, gpt-4-1106-preview, mixtral, mistral-7B-32K in our experiments [1, 21, 22, 34]. We ran open-source models on 4 NVIDIA RTX A6000.

Hyperparameters Settings: To ensure the reproducibility of our experimental results, we set the temperature to 0 for all processes except for Plan Agent, where it is set to 0.7 to allow the Plan Agent to generate diverse plans enabling a larger search space.

For the travel route generation, we limit the maximum number of retries to 3 to prevent infinite remaking of the travel route. The Plan Agent generates 3 different plans each time. If all generated plans contain errors, we revert to the information collection stage only once. If errors persist after this second attempt, we proceed with the process regardless. About the Knowledge Block, we set the minimal number of pop-outs is 5 .

Metrics: We use the same metrics proposed in TravelPlanner [54].

- Delivery Rate: Measures if agents can successfully deliver a final plan within a set number of steps (max 45). Failure includes dead loops, numerous failed attempts, or exceeding the step limit.
- Commonsense Constraint Pass Rate: Assesses if agents can incorporate commonsense into their plans without explicit instructions, across eight dimensions.
- Hard Constraint Pass Rate: Evaluates if a plan meets all explicitly given hard constraints, testing agents' adaptability to diverse user queries.
- Final Pass Rate: Indicates the proportion of plans that meet all constraints (delivery, commonsense, and hard constraints), reflecting agents' overall proficiency in producing practical plans.
- Micro Pass Rate: The Micro Pass Rate evaluates the proportion of constraints that are successfully passed by an agent's plans, as shown in Formula 1. It is calculated by taking the total number of successfully met constraints across all plans and dividing it by the total number of constraints applied to all plans.
- Macro Pass Rate: The Macro Pass Rate assesses the proportion of plans that satisfy all of their constraints. It calculates the ratio of the number of plans that meet all applicable commonsense or hard constraints to the total number of plans evaluated.

$$
\begin{equation*}
\text { Micro Pass Rate }=\frac{\sum_{p \in P} \sum_{c \in C_{p}} 1_{\text {passed }(c, p)}}{\sum_{p \in P}\left|C_{p}\right|}, \text { Macro Pass Rate }=\frac{\sum_{p \in P} 1_{\operatorname{passed}\left(C_{p}, p\right)}}{|P|} \tag{1}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-08.jpg?height=470&width=1395&top_left_y=234&top_left_x=362)

Commonsense Errors Distribution

![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-08.jpg?height=414&width=724&top_left_y=281&top_left_x=381)

Hard Errors Distribution

![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-08.jpg?height=390&width=642&top_left_y=301&top_left_x=1099)

Figure 5: The error distribution of GPT-3.5-Turbo and GPT-4-Turbo.

$P$ is the set of all plans. $C_{p}$ is the set of constraints applicable to a specific plan $p .1_{\text {passed }(c, p)}$ is an indicator function that returns 1 if constraint $c$ is passed in plan $p$, and 0 otherwise. $1_{\operatorname{passed}\left(C_{p}, p\right)}$ is an indicator function that returns 1 if all constraints in $C_{p}$ are passed in plan $p$, and 0 otherwise.

### 4.2 Experiment Result

As shown in Table 1, we tested four LLMs on the dataset to verify our framework's effectiveness. All models exhibited increased Delivery Rate, with Mistral-7B showing an improvement of $8.9 \%$ to $39.4 \%$ and GPT-3.5-Turbo achieving $100 \%$ Delivery Rate. This indicates that GPT3.5-Turbo can generate a travel plan for every query, strongly demonstrating that our framework enables better planning for the task.

However, we observed limited improvement for GPT-4-Turbo on Delivery Rate. To understand the reason, we visualized the error distribution from GPT-4-Turbo in Fig 4 . We found that the primary issue was the repeated use of the same function three times, with the most frequent being AccommodationSearch. The agent repeatedly attempted to find the 'correct' room type, often overlooking some information, which led to redundancy. Additionally, GPT-4-Turbo aimed to find accommodations that could fit all travelers in one room, ignoring the possibility of booking multiple rooms. The lower Delivery Rate will cause a lower Micro Commonsense pass rate. We conjecture that GPT-4-Turbo's stronger reasoning capabilities hindered its progress by overcomplicating the task.

Commonsense Pass Rate: Our framework demonstrates impressive improvements in the Commonsense Pass Rate across all models. For GPT-3.5-Turbo, the Micro Pass Rate increased from $54.0 \%$ to $75.1 \%$, and the Macro Pass Rate increased from $0 \%$ to $15.6 \%$. For GPT-4-Turbo, the Micro Pass Rate increased from $61.1 \%$ to $74.6 \%$, while the Macro Pass Rate saw a significant boost from $2.8 \%$ to $24.4 \%$. Although Mistral and Mixtral showed improvements in their Micro Pass Rates, their Macro Pass Rates did not see significant increases.

Analyzing the commonsense error distribution in Fig 5, we find that GPT-3.5-Turbo and GPT4-Turbo exhibit very similar error patterns. The top three errors are Hallucinated Information, Necessary Information Absent, and Invalid Accommodation, accounting for nearly $60 \%$ of all errors. Hallucinated Information indicates that the LLM agents still generate unreal information. Necessary Information Absent suggests that the Plan Agent might not strictly follow instructions or the information collection phase misses key details. Invalid Accommodation implies that some points in the accommodation data are overlooked.

Hard Constraints Pass Rate: Additionally, our framework enhances the Pass Rate for Hard Constraints. For GPT-4-Turbo, the Micro Pass Rate increased from $15.2 \%$ to $35.7 \%$, and the Macro Pass Rate increased from $10.6 \%$ to $16.7 \%$. While this improvement is not as substantial as that of the

Table 2: Ablation study results on GPT-3.5-Turbo.

|  | Delivery Rate | Commonsense Pass Rate Hard Constraint Pass Rate |  |  |  | Final Pass Rate |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\overline{\text { Micro }}$ | Macro | Micro | Macro |  |
| Original Result $[54]$ | 86.7  | 54.0 | 0 | 0 | 0 | 0 |
| No Outlines | 93.3 | 59.2 | 1.1 | 3.8 | 1.1 | 0 |
| No Strategy | 100 | 74.9 | 13.9 | 12.1 | 2.2 | 0.6 |
| No Knowledge | 99.4 | 66.0 | 8.3 | 10.2 | 1.1 | 0 |
| No Plan Search | 99.4 | 67.4  | 0.6 | 1.9 | ![](https://cdn.mathpix.com/cropped/2024_06_04_838646902855c054d143g-09.jpg?height=42&width=182&top_left_y=530&top_left_x=1354) | 0 |
| Our Framework | 100.0 | 75.1 | 15.6 | 15.5 | 4.4 | 2.2 |

Commonsense Pass Rate, it is still a notable enhancement. For GPT-3.5-Turbo, the improvements were slightly lower, with increases from $0 \%$ to $15.5 \%$ for the Micro Pass Rate and from $0 \%$ to $4.4 \%$ for the Macro Pass Rate. The last two models, Mistral and Mixtral, showed only slight improvements. Examining the error distribution in Fig 5 , we observe that the main errors mainly appear in accommodation planning. This suggests that the concentrated information about each accommodation is challenging for LLM agents to correctly extract in making accurate plans.

Regarding the Final Pass Rate, GPT-3.5-Turbo's result using our framework surpasses the GPT-4Turbo's result in the previous algorithm, showcasing the effectiveness of our framework. Notably, GPT-4-Turbo's Final Pass Rate increased $10 \times$ compared to the previous algorithm. This significant improvement underscores our framework's capability to enhance the performance and reliability of LLM agents in complex travel planning tasks.

### 4.3 Ablation Study

We analyzed the impact of each component of our framework by removing them one at a time, as shown in Table 2 When the outline generation component was removed, we observed a significant drop in all metrics, although they remained better than the TravelPlanner results. This indicates that Outlines Generation is a crucial element of our framework.

About Strategy Block, we removed some information from the Strategy Block, such as specific daytime guidance, Knowledge Block Information, Budget Requirements, and key points from the query. However, the rest of this block remains necessary because, without it, the information collection process cannot proceed. From the results, we observed that all metrics decreased. However, the degree of decrease in Delivery Rate and Micro Commonsense Pass Rate was not very substantial. This outcome is expected, as this component does not significantly impact the feasibility of creating a plan but rather the quality of the plans produced.

For the Knowledge Block, removing it entirely is impractical. Therefore, we adopted the method from [54], using an extra tool to record information. To prevent exceeding the maximum token limit, we implemented a maximum length limitation. Removing this block caused a notable decline in all metrics except Delivery Rate, echoing findings from the Strategy Block ablation study. These outcomes underscore the critical role of the Knowledge Block.

Additionally, when we removed the Plan Search component, we observed a dramatic drop in all metrics except Delivery Rate. The decrease was particularly severe in the Macro Commonsense Pass Rate compared to other ablation studies, highlighting the critical role of the Plan Search component.

Our ablation study results confirm that each block of our framework is essential. Removing any part will significantly decrease performance across various metrics. These parts are vital for managing information and enhancing plan quality, ensuring a robust and efficient planning process.

## 5 Conclusion and Future Work

In this paper, we presented a novel human-like reasoning framework for travel planning using LLM Agents. Our approach integrates several key components: Outline Generation, Information Collection with blocks, and Plan Search. These components synergistically mimic human problem-solving strategies, enabling LLM agents to handle multi-phase tasks more effectively. Extensive experiments on the TravelPlanner benchmark demonstrated significant improvements across multiple models with our framework.

Future work can focus on applying our planning framework to real-world planning problems in practical environments. Additionally, the current tools within our framework are limited. Enhancing the framework by incorporating more advanced tools, such as those with additional parameters and capabilities, could further improve its planning and problem-solving effectiveness. This expansion could enable more sophisticated and adaptable solutions, making our framework even more robust in addressing complex, real-world challenges.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pages 337-371. PMLR, 2023.

[3] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682-17690, 2024.

[4] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.

[5] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint $\underline{\text { arXiv:2304.05376, } 2023 .}$

[6] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint $\underline{\text { arXiv:2309.17288, } 2023 .}$

[7] Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.

[8] Gautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391, 2023.

[9] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. arXiv preprint arXiv:2302.00763, 2023.

[10] Meta Fundamental AI Research Diplomacy Team (FAIR) †, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan $\mathrm{Hu}$, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074, 2022.

[11] Zipeng Fu, Tony Z Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. arXiv preprint arXiv:2401.02117, 2024.

[12] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. S3: Social-network simulation system with large language model-empowered agents. arXiv preprint arXiv:2307.14984, 2023.

[13] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.

[14] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36:79081-79094, 2023.

[15] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024.

[16] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.

[17] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.

[18] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023.

[19] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender ai agent: Integrating large language models for interactive recommendations. arXiv preprint arXiv:2308.16505, 2023.

[20] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716, 2024.

[21] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[22] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

[23] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.

[24] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474, 2020.

[25] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. 2023.

[26] Nian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for simulating macroeconomic activities. arXiv preprint arXiv:2310.10436, 2023.

[27] Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. arXiv preprint arXiv:2309.03736, 2023.

[28] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023

[29] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023.

[30] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023.

[31] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.

[32] Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553, 2020.

[33] Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton. Welfare diplomacy: Benchmarking language model cooperation. arXiv preprint arXiv:2310.08901, 2023.

[34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, $35: 27730-27744,2022$.

[35] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.

[36] Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22, 2023.

[37] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.

[38] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.

[39] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023.

[40] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.

[41] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024.

[42] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.

[43] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11888-11898, 2023.

[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[45] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.

[46] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon's game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023.

[47] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

[48] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. What are tools anyway? a survey from the language model perspective. arXiv preprint arXiv:2403.15452, 2024 .

[49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

[50] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[51] Yue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023.

[52] Hengjia Xiao and Peng Wang. Llm a*: Human in the loop large language models enabled a* search for robotics. arXiv preprint arXiv:2312.01797, 2023.

[53] Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, and Guohao Li. Can large language model agents simulate human trust behaviors? arXiv preprint arXiv:2402.04559, 2024.

[54] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and $\mathrm{Yu} \mathrm{Su}$. Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024.

[55] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023.

[56] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36, 2024.

[57] Zhun Yang, Adam Ishay, and Joohyung Lee. Coupling large language models with logic programming for robust and general reasoning from text. arXiv preprint arXiv:2307.07696, 2023.

[58] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.

[59] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022 .

[60] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201, 2024.

[61] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023.

[62] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition behaviors in large language model-based agents. arXiv preprint arXiv:2310.17512, 2023.

[63] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. Advances in Neural Information Processing Systems, 36, 2024.
