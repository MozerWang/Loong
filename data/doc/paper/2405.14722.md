# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation 

Chuanyang Zheng ${ }^{1 * \dagger}$ Yihang Gao ${ }^{2 *} \quad$ Han Shi $^{3}$ Minbin Huang ${ }^{1}$ Jingyao $\mathbf{L i}^{1}$<br>Jing Xiong ${ }^{3} \quad$ Xiaozhe Ren $^{3} \quad$ Michael $\mathbf{N g}^{2} \quad$ Xin Jiang ${ }^{3} \quad$ Zhenguo $\mathbf{L i}^{3} \quad \mathbf{Y u ~ L i}^{1}$<br>${ }^{1}$ CUHK $\quad{ }^{2} \mathrm{HKU} \quad{ }^{3}$ Huawei Noah's Ark Lab<br>https://github.com/chuanyang-Zheng/CAPE


#### Abstract

Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be context-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that CAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192 , compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.


## 1 Introduction

Transformer-based models have shown state-of-the-art performances in many language processing tasks, including translation [6], question-and-answer [81, 28, 3], and commonsense reasoning [64]. The transformer mainly consists of attention block, feed-forward block, and positional encoding. Recent works [8] have proved that quadratic-cost attention from the softmax is necessary for better performance, especially in long-context processing. The attention block was originally designed by applying softmax to the key-query multiplication, which requires quadratic computational cost. To address such challenges, some efficient transformers were proposed, including sliding window transformers (e.g., Streaming LLMs [76]), linear transformers (e.g., Performer [17]), and sparse transformers (e.g., Reformer and sparse Sinkhorn transformer [65]), etc. However, some negative results exist regarding efficient transformers' performances [79].

It has been noticed recently that well-designed positional encoding significantly improves the model performances, especially in the long-context tasks [32]. While transformer-based models exhibit satisfying performances in tasks of consistent length and distribution, their effectiveness tends to diminish sharply when the input length exceeds the training length, e.g., long document summarization, "needle in a haystack" search, and long text generation. To avoid the expensive computation in[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_abcb7a1161bf3bf59a3cg-02.jpg?height=346&width=1348&top_left_y=230&top_left_x=384)

Figure 1: Visualization of CAPE learned positional biases for the 8192 th query position with key positions between 1 and 8192 , while the training length is 512 . We notice that CAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is $\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}$; (2) The Kerple bias is $\boldsymbol{B}$; (3) The CAPE (with Kerple) bias is $f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}, \boldsymbol{B}\right)$. More examples are shown in Appendix $\mathrm{F}$

training, the training length is usually preferred to be relatively small due to the quadratic cost of softmax-based transformers. However, real-world applications often require processing longer input sequences, posing a significant challenge. Therefore, there is a growing interest in evaluating model performance by training on shorter sequences while testing on longer inputs. Standard transformers may not distinguish the ordering of tokens without external assistance. In practice, they depend on positional encoding to incorporate positional information, enabling the model to make meaningful token predictions. Without these encodings, token generation would lack the necessary contextual order, rendering the outputs nonsensical. The RoPE [61] positional encoding method demonstrated a notable performance degradation, failing entirely when the input length is double that of the training length $[50,10,24]$. A common characteristic among these positional encodings is their pre-defined and static nature. Specifically, they are fixed across various tasks and models, which may lead to their inability to adapt to varying input lengths and contexts effectively. To address this issue, recent works have introduced Functional Interpolation for Relative Positional Encoding (FIRE) [40], which utilizes a neural network to learn an implicit mapping from input positions to positional bias. A functional approach to positional encoding that dynamically adjusts positional biases based on semantic information (input context) allows the model to empower adaptability beyond the fixed inductive bias as adopted in previous studies (such as RoPE [61] and Alibi [51]). Although FIRE utilizes MLPs to learn positional embeddings, these embeddings remain fixed across different tasks once the training is completed. Intuitively, the learned static positional encoding (such as Kerple and FIRE) is an average optimal solution across all training samples. Consequently, while they might be generally effective, they are inherently suboptimal for any specific instance. This static nature limits their flexibility and applicability in various real-world scenarios that deviate from the training context. In this paper, we introduce a context-adaptive positional encoding (CAPE) method, inspired by the limitations of static PEs. CAPE dynamically adjusts the PE based on the semantic information (e.g., the current attention value) $a$ and the positional indicator $b$. The proposed PE is represented by MLPs due to their universal approximatability, i.e., $\operatorname{MLPs}(a, b)$. We note that CAPE is compatible with all additive relative PEs and offers advantages in terms of interpretability and ease of implementation. The proposed CAPE incorporates both the semantic and the positional information, making the PE adaptive with the input context. The adaptivity allows CAPE to overcome the inflexibility and achieve relatively optimal performance for each individual instance by dynamically adjusting on each specific input context. To the best of our knowledge, this is the first semantically dependent and adaptive positional encoding method introduced in transformer architectures.

The paper is organized as follows. In Section 2, we review some related works on positional encoding methods, including absolute and relative positional encodings as well as the potentially no positional encoding in some transformer models. In Section 3, we introduce the proposed CAPE method with implementation on multi-head attention and analysis on computational costs. We conduct comprehensive experiments on CAPE, validating its effectiveness and performances on various language tasks and datasets, as reported in Section 4. In Section 5, some concluding remarks and potential future works are presented.

## 2 Related Works

No positional encoding. Haviv et al. [29] show that decoder-only Transformers with causal attention masks can learn positional information even without any explicit positional encoding. Recently,

Kazemnejad et al. [32] proved the effectiveness of no positional encoding (NoPE) [70]. Although the NoPE can implicitly catch the positional information, it performs poorly compared with some explicit positional encoding methods [40].


#### Abstract

Absolute positional encoding. Vaswani et al. [68] proposed Absolute positional encoding (APE) to endow transformers with positional information. In particular, in the first layer, a (learnable or fixed sinusoidal) real-valued encoding $[68,34,41,69,46] \boldsymbol{e}_{i} \in \mathbb{R}^{d}$ is assigned to each position $i$, leading to an APE matrix $\boldsymbol{E}=\left[\boldsymbol{e}_{1}, \cdots, \boldsymbol{e}_{n}\right]^{\top}$, which will be added to the input sequence. Though simple and straightforward, APE-based Transformers usually generalize poorly to longer sequences [51].


Relative positional encoding. Relative Positional Encoding (RPE) is another popular way to encode positional information $[57,55,51]$, One popular RPE method in large language models is rotary positional encoding (RoPE) [61, 18, 66]. RoPE rotates the query and key vectors with an angle proportional to their absolute positions before the dot product attention, which results in attention being a function of the relative distance between the tokens, capturing the relative positional information. Press et al. [51] and Kazemnejad et al. [32] found that RoPE-based language models have poor length generalization. To address this, positional interpolation (PI) [11] is proposed to extend the context window. Following the direction, there are LongLora [12], LongRope [24], YaRN [50] and CLEX [10]. Another popular direction is additive positional encoding. For most of these additive RPE methods, the computation of the (pre-softmax) attention logits can be unified using the following formula:

$$
\boldsymbol{A}_{\mathrm{RPE}}(\boldsymbol{X})=\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B}
$$

where the bias matrix $\boldsymbol{B} \in \mathbb{R}^{n \times n}$ is induced by the position encoding function $b: \mathbb{N}^{2} \rightarrow \mathbb{R}$ and the $(i, j)$-th entry of $\boldsymbol{B}$ is defined as $b(i, j)$. Different formulations and parameterizations of $b$ lead to various RPE variants. Several methodologies that facilitate arbitrary sequence lengths include T5's RPE [55], Alibi [51], Kerple [13], Sandwich [14], and FIRE [40]. Currently, additive RPE delivers relatively robust performance in length extrapolation without necessitating additional operations. Alibi constructs the bias matrix $\boldsymbol{B}$ utilizing prior knowledge, resulting in a basis matrix devoid of learnable parameters [51]. Conversely, both Kerple [13] and Sandwich [14] incorporate two learnable parameters to facilitate the learning of a bias matrix while retaining some elements of priors. FIRE suggests adopting a learnable continuous function, such as MLPs, to convert input positions to biases [40]. Observing these developments, it becomes evident that the next generation of bias matrices will likely incorporate adaptivity and flexibility. Based on this insight, we propose our method CAPE, a semantically adaptive method.

# 3 Method 

In this section, we formally introduce CAPE (context-adaptive positional encoding), a new relative positional encoding approach that further enhances transformer performance. Compared with previous works on static positional encoding methods, CAPE adopts semantically adaptive positional bias matrices depending on input context. We first demonstrate that most of the popular positional bias matrices are fixed once the training is finished, independent of the input sequences. To address this limitation, we then accordingly develop CAPE that captures the implicit relationships by MLPs and adjusts the bias matrices based on input context. Furthermore, we discuss a variant of CAPE with residual connections and its extensions to multi-head transformers.

### 3.1 Additive Relative Positional Encoding

For most additive RPE methods, the computation of pre-softmax attention logits can be unified under the following formula:

$$
\begin{equation*}
\boldsymbol{A}_{\mathrm{RPE}}(\boldsymbol{X})=\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B} \tag{1}
\end{equation*}
$$

where the bias matrix $\boldsymbol{B} \in \mathbb{R}^{n \times n}$ is induced by the position encoding function $b: \mathbb{N}^{2} \rightarrow \mathbb{R}$ and the $(i, j)$-th entry of $\boldsymbol{B}$ is defined as $b(i, j)$. Various formulations and parameterizations of $b$ give rise to different variants of RPE. Examples of additive RPE include: (1) Alibi: $b(i, j)=-r|i-j|$, with the scaler $r>0$ as a hyper-parameter; (2) Kerple: $b(i, j)=-r_{1} \log \left(1+r_{2}|i-j|\right)$ with $r_{1}$ and $r_{2}$ are two learnable parameters; (3) FIRE: $b(i, j)=f_{\theta}\left(\frac{\psi(i-j)}{\psi(\max \{L, i\})}\right)$, where the positional encoding function $f_{\theta}$ parameterized by $\theta$ is learned from data and $\psi$ is a transformation function aimed at assigning more model capacity to local positions.

We observe from the formulation of those additive RPEs that they remain static once the training process is completed and depend solely on the positions, regardless of the input context. This inflexibility and lack of adaptivity can lead to performance degradation, especially in tasks involving long-context generation and reasoning. Intuitively, the learned static RPEs are optimal on average across all training samples. However, this means they are suboptimal when considering each individual instance, as they cannot adapt to specific tasks. To address these challenges and enhance model performance, it is essential to adopt an alternative approach using a semantically adaptive RPE that depends on input context.

### 3.2 Context-Adaptive Positional Encoding

For simplicity, we first consider the single-head case and the extension to the multi-head transformer will be discussed subsequently. The design of context-adaptive positional encodings in natural language tasks is motivated by the need to capture the intricate relationships between tokens. Arora et al. [5] reveals that associate recall accounts for most of the perplexity difference between transformer, RNN-based, and convolution models. For example, we consider a consistent pairing that "Hakuna" is always followed by "Matata" in a long paragraph. This pattern suggests a reduced reliance on positional information in favor of enhancing token embedding similarity, thus allowing for 'Hakuna' to be effectively linked with a preceding 'Matata'. Similarly, in tasks involving long-context understanding and search, semantic similarity should be prioritized in the attention mechanism rather than being overshadowed by positional encodings, which can be less relevant over long distances. Consequently, the transformer should preserve information without being influenced overly by positional distance. Instead, a satisfactory PE should integrate both semantic and positional information. Therefore, a semantically dependent positional encoding approach is preferable and expected to enhance model performances. Here, we use the attention $\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}$ to represent the semantic information and positional bias matrices $\boldsymbol{B}$ (e.g., Alibi and FIRE) to capture positional details. Then the context-adaptive PE is described by $f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}, \boldsymbol{B}\right)$, where $f(\cdot)$ is an implicit function that integrates both semantic and positional data into the desired positional encodings. Thus, the pre-softmax attention logit incorporated with CAPE is formulated as

$$
\begin{equation*}
\boldsymbol{A}_{\mathrm{CAPE}}(\boldsymbol{X})=\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}, \boldsymbol{B}\right) \tag{2}
\end{equation*}
$$

Here, $f: \mathbb{R}^{T \times T} \times \mathbb{R}^{T \times T} \rightarrow \mathbb{R}^{T \times T}$ is an element-wise function. In practice, we utilize a two-layer LeakyReLU neural network to parameterize $f(\cdot)$ due to its universal approximability [35]. All parameters are learned directly from the data during the training process. This architecture allows $f(\cdot)$ to dynamically adjust positional embeddings based on the input context, ensuring that the encoding method is both adaptive and dependent on the input data.

Different from FIRE, which also models the implicit positional encoding by MLPs, our approach additionally integrates semantic information. This integration enables the adaptivity, flexibility, and context-dependency of the positional encodings. Significantly, our method is compatible with most additive RPE techniques, as these commonly involve positional bias matrices $\boldsymbol{B}$ that inherently contain positional relations. Unlike previous RPEs, which rely solely on absolute positional differences, our CAPE method, can be seen as utilizing multi-level positional bias matrices. Here, the bias matrices dynamically adjust based on the input context, offering a more reasonable and responsive encoding mechanism.

Expressiveness. Due to the universal approximability of (LeakyReLU) neural networks [35], $f(\cdot)$ is capable of capturing complex relationships between the desired positional encoding and both semantic and positional information. Regardless of the semantic component, when the relative position $i-j$ is used as input, CAPE can realize classical additive RPEs (e.g., Alibi and Kerple), according to [40]. This demonstrates the versatility of CAPE in accommodating traditional encoding schemes while also offering enhanced capabilities. There exists a fundamental trade-off between the expressiveness and computational costs. Wider hidden layers lead to higher expressiveness but also contribute to more computational costs. In practice, we find that two-layer neural networks with 32 hidden units per layer provide sufficient expressiveness to deliver satisfactory performance, balancing complexity and efficiency effectively.

Discussion. We can also interpret the proposed method from an alternative perspective. In the standard transformer architecture, the pre-softmax attention typically involves the key-query similarity and the positional encoding by either addition (in the form of $a+b$, e.g., Alibi and Kerple) or multiplication (in the form of $a * b$, e.g., RoPE). Here, we propose a unified approach by replacing them with
learnable MLPs, i.e., $\operatorname{MLP}(a, b)$. This configuration allows the model to learn the desired relationship between the pre-softmax attention, the key-query similarity and the positional encoding. It can also be regarded as a new transformer architecture that empower the transformer with additional MLPs on pre-softmax attentions.

A variant of CAPE with residual connections. It is well-known that deep neural networks may suffer from gradient vanishing. To further enhance the practical performances, we introduce the residual connection for positional information. Consequently, Equation 2 is modified as follows:

$$
\begin{equation*}
\boldsymbol{A}_{\mathrm{CAPE}}(\boldsymbol{X})=\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B}+f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}, \boldsymbol{B}\right) \tag{3}
\end{equation*}
$$

In this reformulation, $f(\cdot)$ acts as an adaptive correction term to the traditionally fixed RPE, dynamically adjusting the positional bias matrices $\boldsymbol{B}$ based on both semantic and positional inputs. In Section 4 , we empirically explore the impact of residual connections in CAPE. Our observations reveal that for well-behaved bias matrices $\boldsymbol{B}$, the CAPE model with residual connections, as specified in Equation 3, is preferable. Conversely, if the bias matrix is underperforming but still conveys positional information, the original implementation in Equation 2 is more effective.

Multi-head CAPE. In its simplest form, CAPE is considered for a single-head case as described in Equation 2 and Equation 3. However, adopting a multi-head mechanism significantly enhances model capabilities. To effectively combine both semantic and positional information, the CAPE in a multi-head setup processes the key-query similarities and bias matrices from all heads. Specifically, for an $h$-head layer, the function $f(\cdot)$ inputs a $2 h$-dimensional concatenation of key-query similarities and positional biases. It then outputs $h$-dimensional vectors, where each element corresponds to the CAPE for the respective head. Importantly, semantic and positional information across different heads are processed simultaneously within the same MLPs, rather than sequentially. This approach not only improves computational efficiencies through parallel processing but also capitalizes on the richer semantic information available across all heads. Compared to the key-query similarity derived from a single head, the comprehensive attention from all heads yields more substantial semantic information

Computational costs analysis. Here, we evaluate the additional computational costs introduced by the CAPE method, compared with the classical positional encoding methods (e.g., Alibi and Kerple). We consider a transformer model with $h$ heads and assume a sequence length of $N$ and all hidden dimensions in the attention layer being $d$. Then the total computational cost for a standard transformer equipped with classical PEs is $\mathcal{O}\left(h N^{2} d+h N d^{2}\right)$. When incorporating the proposed CAPE, which employs two-layer MLPs with hidden dimension $D_{\text {CAPE }}$, the additional computational costs are $\mathcal{O}\left(h N^{2} D_{\text {CAPE }}\right)$. If the hidden dimensions $D_{\text {CAPE }} \ll d$, the incremental computational cost introduced by CAPE is not significant.

## 4 Experiment

Baselines. We evaluate the proposed CAPE against a range of established baselines, including NoPE [32], RoPE [61], YaRN [50], Randomized RoPE [56, 30], T5's Bias [55], Alibi [51], Kerple [13], and FIRE [40]. For RoPE, the randomized positional encoding [56, 30] is applied to enhance the model performance, extending the randomized length to four times that of the training length.

Datasets. Our analysis involves training language models on the Arxiv and Books3 datasets, which are frequently used benchmarks for evaluating model performance [51, 13, 40, 24]. We start our evaluation by comparing the last 256 tokens' zero-shot perplexity across different input lengths. Besides perplexity as evaluation metrics, we also employ the downstream datasets in randomized positional encoding [56] to evaluate CAPE, where details are included in Appendix D.

Experiment settings. Initially, we compare CAPE with other baselines at training lengths of 128, 512, and 1024, with model size 125M decoder-only Transformers [9], whose configuration is shown in Appendix B. Subsequently, we evaluate the performance of larger model size 350M, CAPE variants and explore the impact of hidden dimension of MLPs $D_{\text {CAPE }}$. We also examine the computational efficiency of CAPE, focusing on processing times. Additionally, we provide visualizations of the CAPE bias in the Appendix F. Finally, we also evaluate CAPE on algorithmic reasoning datasets via accuracy metrics.
![](https://cdn.mathpix.com/cropped/2024_06_04_abcb7a1161bf3bf59a3cg-06.jpg?height=738&width=1304&top_left_y=230&top_left_x=408)

Figure 2: Comparisons with baselines: performance with training lengths 128 and 512 on Arxiv and Books3 datasets.

### 4.1 Comparisons with Baselines

CAPE's superior performance within training length and beyond training length, compared to all baselines. As shown in Figure 4 and Table 5, CAPE consistently outperforms established baselines such as RoPE, Alibi, and Kerple across various settings. Notably, CAPE-Kerple (the positional information in CAPE comes from Kerple bias matrices) outstands in both short and long training lengths (128 and 512), compared to previous RoPE, T5's bias, and so on. It demonstrates that the semantic adaptivity of CAPE significantly enhances its state-of-the-art performance against all other static positional encoding methods.

## The performance on longer training length

1024. As shown in Figure 3, the proposed method consistently delivers state-of-the-art performance for the training length of 1024 . When the evaluation extends to 2048, both CAPEKerple and CAPE-FIRE achieve notable results, recording performances of 3.91 and 3.93 perplexity scores, respectively. Remarkably, CAPEFIRE behaves well at the longer evaluation length of 8192, achieving a performance of 3.91 scores and surpassing Alibi's score of 4.28. These findings reveal that CAPE sustains robust

CAPE enhances intra-length performance, indicating that its lower perplexity may come from thorough utilization of entire sentences but not disregarding long-distance information (Also proved in Figure 1). Compared to Alibi, Kerple, and FIRE, the adapted versions CAPEAlibi, CAPE-Kerple, and CAPE-FIRE demonstrate consistently and significantly better intra-length performance. With the growing sequence length, the Alibi tends to transition from full attention to almost local attention, and this is why Alibi is worse than most baselines within training length but better beyond training lengths. The results (as shown in Table 5) indicate that the superior intralength performance of CAPE is statistically significant, with a p-value less than 0.05. Therefore, the consistent intra-length performances across various training lengths indicate that the lower perplexity of CAPE results from effectively utilizing the entire sequence, rather than focusing on local parts and neglecting long-distance information.

CAPE significantly improves length extrapolation performance, compared to ALibi, Kerple, and FIRE. CAPE-Kerple significantly surpasses competitors like vanilla Kerple when training and evaluating at different lengths. On the Arxiv dataset trained at a length of 128, CAPE-Kerple achieves a remarkably low perplexity of 5.00 at an evaluation length of 8192 , in stark contrast to Kerple's 31.93 . Similarly, on the Books3 dataset with a training length of 512, CAPE-Kerple records a perplexity of 17.88 at the same extended evaluation length, far outperforming Kerple's 39.31. These results affirm that CAPE, through its semantic adaptivity and flexibility, consistently enhances performance beyond training lengths, eclipsing static positional encoding methods.

### 4.2 The Effect of Model Size

![](https://cdn.mathpix.com/cropped/2024_06_04_abcb7a1161bf3bf59a3cg-07.jpg?height=426&width=1270&top_left_y=660&top_left_x=424)

Figure 4: The effect of model size: for the $350 \mathrm{M}$ model, the performance with training lengths 128 and 512 on the Arxiv dataset.

CAPE enhances performance with increasing model sizes. As the model size grows (as shown in Figure 4), CAPE consistently demonstrates an improvement in performance metrics. When the model size is augmented from $125 \mathrm{M}$ to $350 \mathrm{M}$, the perplexity at an evaluation sequence length of 8192 (with a training length of 512) for CAPE-Alibi shows a notable decrease from 3.82 to 3.57 . These numbers are appreciably smaller than those recorded for original Alibi, which decreases from 4.54 to 4.21 in perplexity, indicating a robust performance improvement. Additionally, CAPE-Kerple significantly reduces the perplexity for Kerple, bringing it down from an initial 22.76 to an impressive 3.43. These results confirm that CAPE retains its efficacy and continues to perform well even as the model size is increased, mainly due to the adoption of semantically adaptive PEs.

CAPE methods almost are ranked top-3 with large model size. With the incremental model size, CAPE-FIRE begins to match, and nearly approach, the performance levels of Alibi. Initially, at a model size of 125M and a training length of 512, CAPE-FIRE achieves a perplexity of 5.71 at an evaluation sequence length of 8192 , while Alibi stands at a perplexity of 4.54 . However, as the model size is increased to $350 \mathrm{M}$, the performance gap significantly narrows. Specifically, CAPE-FIRE outperforms Alibi regarding the perplexity scores when the evaluation length is smaller than 4096, as the model size grows for evaluation. In conclusion, as shown in Figure 4, we observe that the CAPE methods almost win the top-3 among all positional encoding methods. This trend underlines the scalability and adaptability of CAPE, emphasizing its potential to handle more substantial computation challenges.

### 4.3 Different Variants of CAPE

In this section, we evaluate the performance of CAPE across its various forms. Our analysis focuses on CAPE-Kerple. Notably, as shown in Figure 5, all variants of CAPE surpass the baseline performance of Kerple. The Addition_Residual variant of CAPE, while requiring less computational effort, delivers relatively inferior results. As illustrated in Figure 5, concatenation methods (either Concat or Concat_Residual) outperform the Addition_Residual approach, for both the training length of 128 and 512. Furthermore, both Concat and Concat_Residual exhibit comparable performance metrics. Specifically, at a training length of 128, Concat_Residual records a score of 5.00 and Concat scores 5.03 at an evaluation length of 8192 , whereas Add_Residual posts a 5.17 perplexity score. With a training length of 512, Concat_Residual achieves a score of 3.70, and Concat scores 3.69 at an evaluation length of 8192 , compared to Add_Residual's 3.75. Based on the current observation, the different variants of CAPE show comparable performances, compared to baselines.
![](https://cdn.mathpix.com/cropped/2024_06_04_abcb7a1161bf3bf59a3cg-08.jpg?height=370&width=1286&top_left_y=232&top_left_x=408)

Figure 5: Different variants of CAPE: the CAPE-Alibi performance under different variants. (1) Add_Residual: $\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B}+f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B}\right)$; (2) Concate: $\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+$ $f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}, \boldsymbol{B}\right) ;$ (3) Concate_Residual: $\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B}+f\left(\boldsymbol{X} \boldsymbol{W}_{Q}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}, \boldsymbol{B}\right)$.

### 4.4 The Effect of the Hidden Dimension $D_{\text {CAPE }}$

Even small $D_{\text {CAPE }}$ can improve the performance. The experiments are conducted with Alibi and CAPE-Alibi. As shown in Appendix Figure 6, when considering the training length 128 and $D_{\text {CAPE }}$ is set as 4 , the CAPE-Alibi achieves 8.25 at evaluation length 128 and 5.67 at length 8192 , which is better than Alibi's 8.31 and 5.85 . Whatever $D_{\text {CAPE }}$ is 4,1632 , or 64 , the performance is always better than the original Alibi at all evaluation lengths. This suggests the effectiveness of CAPE, even with smaller $D_{\text {CAPE }}$.

The choice of $D_{\text {CAPE }}$. Based on the experiment, overly small values of $D_{\text {CAPE }}$ can degrade performance, although they still perform better than the baseline. Conversely, larger values of $D_{\text {CAPE }}$ increase computational costs. The function $f(\cdot)$ is implemented as a two-layer MLP, where the input dimension is either the head number or twice the head number, and the output dimension is the head number. Therefore, we recommend setting the hidden dimension to the head number to prevent information loss and ensure the capacity of $f(\cdot)$.

### 4.5 The Time Cost

Table 1: The computation cost under different testing lengths, with $D_{\text {CAPE }}$ as 32 and default batch size 32 .

| Method | $D_{\mathrm{CAPE}}$ | Length 128 |  | Length 512 |  | Length 2048 |  |  | Length 2048 \& Batch 1 |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Forward | Backward | Forward | Backward | Forward | Backward | Memory (GB) | Forward | Backward |
| Rope | - | 19.65 | 50.34 | 50.85 | 168.72 | 371.69 | 1237.55 | 22.1 | 26.79 | 67.62 |
| T5's bias | - | 17.16 | 44.74 | 54.11 | 181.43 | 684.78 | 2010.62 | 21.9 | 36.22 | 424.04 |
| Alibi | - | 16.68 | 41.05 | 53.34 | 139.23 | 463.00 | 1052.32 | 23.2 | 21.40 | 53.58 |
| CAPE-Alibi | 16 | 21.20 | 50.58 | 117.05 | 256.53 | 1685.45 | 3492.63 | 31.4 | 53.67 | 107.16 |
| CAPE-Alibi | 32 | 22.12 | 52.49 | 133.51 | 285.55 | 1950.39 | 3960.20 | 41.4 | 61.41 | 121.32 |
| Kerple | - | 18.17 | 44.34 | 54.17 | 145.95 | 463.74 | 1103.53 | 22.3 | 28.41 | 74.23 |
| CAPE-Kerple | 16 | 22.04 | 54.35 | 117.15 | 292.76 | 1683.95 | 4045.51 | 30.5 | 60.51 | 132.69 |
| CAPE-Kerple | 32 | 23.38 | 54.48 | 133.36 | 320.44 | 1950.80 | 4508.87 | 40.6 | 60.69 | 133.34 |
| FIRE | - | 21.08 | 53.71 | 64.12 | 162.19 | 734.01 | 1406.39 | 22.3 | 52.02 | 113.91 |
| CAPE-FIRE | 16 | 25.78 | 65.90 | 130.85 | 312.98 | 2192.95 | 4603.17 | 30.7 | 87.91 | 166.26 |
| CAPE-FIRE | 32 | 26.80 | 66.03 | 146.54 | 342.65 | 2457.05 | 5091.79 | 41.1 | 100.59 | 195.41 |

Practical additional time cost. The theoretical analysis of the computational cost for CAPE is presented in Section 3.2. For relatively short training lengths (e.g., 128), as detailed in Table 1, the forward step times are $16.68 \mathrm{~ms}, 18.17 \mathrm{~ms}$, and $21.08 \mathrm{~ms}$ for Alibi, Kerple, and FIRE, respectively. Incorporating CAPE increases these times slightly to $22.12 \mathrm{~ms}, 23.38 \mathrm{~ms}$, and $26.80 \mathrm{~ms}$, respectively. As training length extends to 512, the forward and backward times increase to approximately 2.5 to 3 times those of the baselines. With a batch size reduction from 32 to 1 , the forward step time for FIRE escalates to $52.02 \mathrm{~ms}$, while CAPE reaches $100.59 \mathrm{~ms}$. Future optimizations may focus on enhancing the efficiency of MLP and I/O operations to further accelerate CAPE.

### 4.6 The Visualization of CAPE

In this subsection, we present the visualization of learned positional encoding biases from a CAPEKerple model pretrained on Arxiv (training length is 512). We plot the learned positional encoding bias for the query token at the 8192th position, for all the attention heads from selected layers in

Table 2: Train on length 40 with $200 \mathrm{k}$ steps, and test from lengths 41 to 500. The random accuracy is $50 \%$, except for Modular Arithmetic (Simple), Cycle NAVigation, BucKet Sort, SolVE EQUATION and MODUlar ARITHmetiC, where it is $20 \%$. $\dagger \dagger \dagger$ denotes permutation-invariant tasks, which are expected to be solved without positional information.

| Level | Task | Randomized |  |  |  |  |  |  | CAPE (Ours) |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Learned | $\sin / \cos$ | RoPE | Relative [19] | $\mathbf{A L i B i}$ | Kerple | FIRE | Alibi | Kerple | FIRE |
| $\mathrm{R}$ | EVEN PAIRS | 50.04 | 91.27 | 99.98 | 96.60 | 73.52 | 57.50 | 73.86 | 99.99 | 99.58 | 100 |
|  | MODULAR ARITHMETIC (SIMPLE) | 19.95 | 20.39 | 21.35 | 20.84 | 20.02 | 21.79 | 21.09 | 23.58 | 24.47 | 24.46 |
|  | PARITY CHECK $\dagger \dagger \dagger$ | 50.14 | 50.52 | 50.05 | 50.09 | 50.09 | 50.07 | $\mathbf{5 0 . 9 7}$ | 50.30 | 50.07 | 50.04 |
|  | CYCLE NAVIGATION $\dagger \dagger \dagger$ | 24.97 | 25.37 | 27.63 | 26.95 | 24.64 | 29.47 | 28.41 | 22.99 | 34.53 | 27.54 |
| $\mathrm{DCF}$ | STACK MANIPULATION | 59.92 | 65.92 | 61.49 | 64.73 | 66.42 | 66.93 | 69.33 | 68.18 | ![](https://cdn.mathpix.com/cropped/2024_06_04_abcb7a1161bf3bf59a3cg-09.jpg?height=28&width=82&top_left_y=606&top_left_x=1583) | 70.90 |
|  | REVERSE STRING | 52.76 | 67.28 | 65.23 | 65.59 | 71.09 | 71.54 | 65.89 | 73.37 | 70.74 | $76.40 \quad$ |
|  | MODULAR ARITHMETIC | 31.00 | 30.70 | 31.25 | 31.74 | 30.56 | 24.79 | 30.92 | 31.34 | 32.37 | $31.50 \quad$ |
|  | SOLVE EQUATION | 20.00 | 19.97 | 21.85 | 22.93 | 19.92 | 21.15 | 22.06 | 20.03 | 22.49 | 22.42 |
| $\mathrm{CS}$ | DUPLICATE STRING | 52.77 | 65.44 | 64.97 | 67.66 | 65.13 | 66.72 | 69.03 | 70.84 | 72.95 | 72.71 |
|  | MISSING DUPLICATE | 50.38 | 49.78 | 63.37 | 72.34 | 74.21 | 79.06 | 79.27 | 83.41 | 87.57 | $\mathbf{8 9 . 1 7}$ |
|  | ODDS FIRST | 52.77 | 58.61 | 61.00 | 61.57 | 59.88 | 62.59 | 63.28 | 63.78 | 67.08 | $66.34 \quad$ |
|  | BINARY ADDITION | 54.63 | 55.78 | 55.59 | 56.96 | 54.72 | 56.35 | 55.70 | 59.71 | 60.88 | 56.62 |
|  | COMPUTE SQRT | 50.47 | 51.11 | 51.88 | 51.63 | 50.63 | 51.11 | 50.80 | 51.64 | 51.33 | $\mathbf{5 2 . 4 6}$ |
|  | BUCKET SORT $\dagger \dagger \dagger$ | 98.32 | 98.92 | 98.12 | 99.31 | 98.45 | 99.38 | $\mathbf{9 9 . 5 7}$ | 99.38 | 98.81 | 99.37 |

Figure 1. We would like to highlight two features of CAPE. First, in different attention heads, the bias matrix of CAPE learns both local and "anti-local" attention patterns that emphasize more on far-away keys (just like FIRE), compared to a fixed local inductive bias (such as Kerple and Alibi). Secondly, the bias matrix can be dynamically adjusted with different attention values, compared to the static bias fixed for all attentions. We have shown more examples, including different layers and different samples, in Appendix F.

### 4.7 Experiments on CHE Benchmark

Besides employing perplexity as an evaluation metric, we also evaluated CAPE on downstream Chomsky Hierarchy Evaluation Benchmark (CHE) [21] (need to utilize the whole sentence information to generate correct answers) to further discuss its effects. The experimental setup follows randomized positional encodings [56], detailed in Table 4, with the experiment setting shown in Appendix Section D. Overall, FIRE outperforms Kerple in 9 out of 14 tasks, while Kerple outperforms Alibi in 11 out of 14 tasks. This observation aligns with findings in [40], suggesting that the experiments in Table 2 are reliable and reflect the performance of positional encoding in downstream tasks.

CAPE works better on permutation-variant tasks. CAPE (with Kerple and FIRE) presented the best performance in 10 out of 11 permutation-variant tasks (which require positional information), achieving the second-best performance in the SOlVE EQUATION task. This underscores the efficacy of CAPE with semantic adaptivity in handling permutation-variant challenges.

CAPE's performance on permutation-invariant tasks. In tasks that are permutation-invariant, where positional information is non-critical, CAPE demonstrated comparable performance. Notably, CAPE-Alibi achieved scores of 50.30 on PARITY CHECK and 99.38 on BUCKET SORT tasks, compared to the highest scores of 50.97 and 99.57 , respectively, demonstrating competitive performances.

Comparative performance improvements. CAPE consistently enhanced performance across various tasks, especially on permutation-variant tasks. Specifically, CAPE improved upon Alibi and FIRE's results in all 11 tested permutation-invariant tasks. Similarly, it outperformed Kerple in 10 of these tasks. These results highlight the effectiveness of CAPE over static positional encoding methods like Alibi, Kerple, and FIRE, resulting from its dynamic adaptivity.

## 5 Conclusion

In this paper, we propose the context-adaptive positional encoding (CAPE) by incorporating both the semantic and the positional information to improve the model performance. We show that the additional computation introduced by CAPE is not significant under proper choices of hyperparameters. We conduct comprehensive experiments on Arxiv, Books3, and CHE to validate the effectiveness of the proposed method, revealing that the adaptive PE method has advantages over static PEs. We believe that the CAPE could benefit the whole community, especially on length generalization tasks.

## References

[1] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: KV cache reduction through key tokens selection for efficient generative inference. arXiv preprint arXiv:2403.09054, 2024.

[2] Devanshu Agrawal, Shang Gao, and Martin Gajek. Can't remember details in long documents? you need some r\&r. arXiv preprint arXiv:2403.05004, 2024.

[3] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontanon, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range transformers with conditional computation. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.

[4] Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models. arXiv preprint arXiv:2402.17463, 2024.

[5] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927, 2023.

[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations, 2015.

[7] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024.

[8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901, 2020.

[10] Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. CLEX: Continuous length extrapolation for large language models. In International Conference on Learning Representations, 2023.

[11] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

[12] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. LongLoRA: Efficient fine-tuning of long-context large language models. International Conference on Learning Representations, 2023.

[13] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. KERPLE: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:8386-8399, 2022.

[14] Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $13522-13537,2023$.

[15] Ta-Chung Chi, Ting-Han Fan, and Alexander I Rudnicky. Attention alignment and flexible positional embeddings improve transformer length extrapolation. arXiv preprint arXiv:2311.00684, 2023.

[16] Noam Chomsky. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113-124, 1956.

[17] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021.

[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113,2023 .

[19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, 2019 .

[20] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024.

[21] Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, et al. Neural networks and the chomsky hierarchy. In International Conference on Learning Representations, 2022.

[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.

[23] Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830, 2024 .

[24] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. LongRoPE: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024

[25] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to $128 \mathrm{k}$ context. arXiv preprint arXiv:2402.10171, 2024.

[26] Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael K Ng, Zhenguo Li, and Zhaoqiang Liu. On the expressive power of a variant of the looped transformer arXiv preprint arXiv:2402.13572, 2024.

[27] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

[28] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. Findings of the Association for Computational Linguistics: NAACL, 2022.

[29] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382-1390, 2022.

[30] Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Di He, Jingjing Xu, Zhi Zhang, Hongxia Yang, and Liwei Wang. Two stones hit one bird: Bilevel positional encoding for better length extrapolation. arXiv preprint arXiv:2401.16421, 2024.

[31] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. LLM maybe LongLM: Self-extend LLM context window without tuning. arXiv preprint arXiv:2401.01325, 2024.

[32] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.

[33] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations, 2020.

[34] Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. SHAPE: Shifted absolute position embedding for transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309-3321, 2021.

[35] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6(6):861-867, 1993.

[36] Jingyao Li, Pengguang Chen, Zexin He, Shaozuo Yu, Shu Liu, and Jiaya Jia. Rethinking out-of-distribution (OOD) detection: Masked image modeling is all you need. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages $11578-11589$, June 2023.

[37] Jingyao Li, Pengguang Chen, and Jiaya Jia. Motcoder: Elevating large language models with modular of thought for challenging programming tasks, 2024.

[38] Jingyao Li, Pengguang Chen, Shengju Qian, and Jiaya Jia. Tagclip: Improving discrimination ability of open-vocabulary semantic segmentation, 2023.

[39] Jingyao Li, Pengguang Chen, Shaozuo Yu, Shu Liu, and Jiaya Jia. Bal: Balancing diversity and novelty for active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(5):3653-3664, 2024.

[40] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. In International Conference on Learning Representations, 2023.

[41] Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. CAPE: Encoding relative positions with continuous augmented positional embeddings. Advances in Neural Information Processing Systems, 34:16079-16092, 2021.

[42] Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong Xiao, Qi Xu, Xiafei Qiu, Shen Li, et al. Infinite-LLM: Efficient LLM service for long context with distattention and distributed kvcache. arXiv preprint arXiv:2401.02669, 2024.

[43] Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, and Hannaneh Hajishirzi. Infinigram: Scaling unbounded n-gram language models to a trillion tokens. arXiv preprint arXiv:2401.17377, 2024.

[44] Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et al. E^ 2-LLM: Efficient and extreme length extension of large language models. arXiv preprint arXiv:2401.06951, 2024.

[45] Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of RoPE-based extrapolation. In International Conference on Learning Representations, 2023.

[46] Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Learning to encode position for transformer with continuous dynamical model. In International Conference on Machine Learning, pages 6327-6335. PMLR, 2020.

[47] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. Advances in Neural Information Processing Systems, 34:22795-22807, 2021.

[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.

[49] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. RWKV: Reinventing RNNs for the transformer era. Findings of the Association for Computational Linguistics: EMNLP, 2023.

[50] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In International Conference on Learning Representations, 2023.

[51] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2021.

[52] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024.

[53] Zhen Qin, Yiran Zhong, and Hui Deng. Exploring transformer extrapolation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18897-18905, 2024.

[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.

[56] Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1889-1903, 2023.

[57] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468, 2018 .

[58] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. International Conference on Learning Representations, 2023.

[59] Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Henryk Michalewski, Łukasz Kuciński, and Piotr Miłoś. Structured packing in LLM training improves long context utilization. arXiv preprint arXiv:2312.17296, 2023.

[60] Jianlin Su, Murtadha Ahmed, Luo Ao, Mingren Zhu, Yunfeng Liu, et al. Naive bayes-based context extension for large language models. arXiv preprint arXiv:2403.17552, 2024.

[61] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

[62] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023.

[63] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590-14604, July 2023.

[64] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, 2019.

[65] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438-9447. PMLR, 2020.

[66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[67] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training for context scaling. Advances in Neural Information Processing Systems, 36, 2024.

[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.

[69] Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen. On position embeddings in BERT. In International Conference on Learning Representations, 2020.

[70] Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. arXiv preprint arXiv:2404.12224, 2024.

[71] Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, and Bang Liu. Resonance RoPE: Improving context length generalization of large language models. arXiv preprint arXiv:2403.00071, 2024.

[72] Y Wang, D Ma, and D Cai. With greater text comes greater necessity: Inference-time training helps long text generation. arXiv preprint arXiv:2401.11504, 2024.

[73] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[74] Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li. Long context alignment with short instructions and synthesized positions. arXiv preprint arXiv:2405.03939, 2024.

[75] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. InfLLM: Unveiling the intrinsic capacity of LLMs for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617, 2024.

[76] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In International Conference on Learning Representations, 2024 .

[77] Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, et al. Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning. arXiv preprint arXiv:2310.02954, 2023.

[78] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.

[79] Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, and Liwei Wang. Do efficient transformers really save computation? International Conference on Machine Learning, 2024.

[80] Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. arXiv preprint arXiv:2402.16617, 2024.

[81] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR, 2020.

[82] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia $\mathrm{Wu}$, and Zhangyang Wang. Found in the middle: How language models use long contexts better via plug-and-play positional encoding. arXiv preprint arXiv:2403.04797, 2024.

[83] Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.

[84] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. In International Conference on Learning Representations, 2024.

[85] Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024.

[86] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. PoSE: Efficient context window extension of llms via positional skip-wise training. In International Conference on Learning Representations, 2023.

[87] Shiyi Zhu, Jing Ye, Wei Jiang, Qi Zhang, Yifan Wu, and Jianguo Li. CoCA: Fusing position embedding with collinear constrained attention for fine-tuning free context window extending. arXiv e-prints, pages arXiv-2309, 2023.
