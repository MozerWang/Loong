# Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\mathcal{N}=4$ Super Yang-Mills Theory 

Tianji Cai*a ${ }^{*}$ Garrett W. Merz* ${ }^{* b}$, François Charton*c ${ }^{*}$ Niklas Nolte ${ }^{c}$,<br>Matthias Wilhelm ${ }^{d}$, Kyle Cranmer ${ }^{b}$, Lance J. Dixon ${ }^{a}$<br>${ }^{a}$ SLAC National Accelerator Laboratory<br>${ }^{b}$ Data Science Institute, University of Wisconsin-Madison<br>${ }^{c}$ FAIR, Meta<br>${ }^{d}$ Niels Bohr Institute, University of Copenhagen

May 2024


#### Abstract

We pursue the use of deep learning methods to improve state-of-the-art computations in theoretical high-energy physics. Planar $\mathcal{N}=4$ Super Yang-Mills theory is a close cousin to the theory that describes Higgs boson production at the Large Hadron Collider; its scattering amplitudes are large mathematical expressions containing integer coefficients. In this paper, we apply Transformers to predict these coefficients. The problem can be formulated in a language-like representation amenable to standard cross-entropy training objectives. We design two related experiments and show that the model achieves high accuracy ( $>98 \%$ ) on both tasks. Our work shows that Transformers can be applied successfully to problems in theoretical physics that require exact solutions.


## 1. Introduction

Particle physics at the energy frontier is entering an exciting new era of high-precision experiments, ushered in by the high-luminosity upgrade of the Large Hadron Collider (LHC). Exploiting the full physics potential of the experimental data requires substantial improvements in the predictions of Standard Model (SM) 11 processes, both as backgrounds to new physics, and for measuring Higgs boson couplings and other SM parameters.

Many ingredients are necessary for these predictions, see e.g. 2] for a review. At the heart of all such calculations are scattering amplitudes - the fundamental quantummechanical building blocks for transition probabilities between asymptotic states. The conventional way to compute scattering amplitudes uses Feynman diagrams (see Figure 1 for examples), which graphically organize a series of terms in a perturbative expansion. Performing high-precision calculations in the theory of quantum chromodynamics (QCD)

$*$ Denotes equal contribution
![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-02.jpg?height=554&width=1006&top_left_y=288&top_left_x=516)

Figure 1. Sample Feynman diagrams for the process $g g \rightarrow H g$ at two loops (left) and eight loops (right).

requires Feynman diagrams containing at least two loops 31 . Each loop represents intermediate-state virtual particles whose unobserved momenta must be integrated over. Each successive order of precision demands the addition of another loop to the diagram. Unfortunately, the number of possible Feynman diagrams, and thus the number of integrals that must be performed, grows factorially with loop order, quickly making these calculations intractable.

A recently-developed alternative technique, known as the amplitude bootstrap 11-13, attempts to directly construct candidate solutions for multi-loop amplitudes. It has mainly been applied so far to a simpler relative of QCD, called planar $\mathcal{N}=4$ super-YangMills theory (SYM). The amplitude bootstrap leverages the rich, yet highly constrained, analytical structure of amplitudes that arises from the particular recurrent features of the integrals involved. Using this technique, the form of the amplitude can be determined $a$ priori, and the finite-dimensional solution space at a given loop order can be strongly constrained through a large system of linear relations with integer coefficients. Many of the linear relations are found by analyzing the lower-loop results. A small number of additional constraints, derived from behavior in physical limits, can then be applied in order to obtain a unique solution.

The amplitude bootstrap allows for the computation of amplitudes up to eight loops in SYM [14, vs. two loops using traditional Feynman diagram methods for the same quantity in QCD [15], as depicted in Figure 1. However, the number of linear equations to be solved when using the bootstrap technique scales exponentially with loop order, making it intractable beyond eight loops. This situation necessitates the development of new methods that may exploit hitherto unobserved patterns in the data in order to simplify the computation of scattering amplitudes at higher loops.

Notably, amplitudes in SYM can be expressed as sets of tokenizable "words" with integer coefficients; see Section 2. We refer to these words as keys which index into the coefficients, and to a key-coefficient pair as an element. Exploiting the many linear relations between these elements amounts to solving a large system of linear equations,
where the integer solutions are hard to discover, but easy to verify. Since both the keys and the integers can be represented as sequences of tokens, we can train deep learning models such as Transformers to predict the coefficient associated with each key. In principle, one can easily verify any model predictions using the known linear relations.

Transformers 16 are incredibly versatile neural network architectures that employ an attention mechanism [17] to learn complex nonlinear relationships between input features. They have revolutionized fields ranging from natural language processing 18 and computer vision 19 to formal symbolic mathematics 20, 21. Inspired by these many recent successes, we apply Transformers to two sets of experiments; see Section 3 for our model setup.

We first show in Section 4 that Transformers can accurately predict elements of the solution at a given loop order when trained on other elements at the same loop. In Section 5, we show that the model is able to do this even when data is presented in a highly compressed format. To make these predictions successfully, many features of the complex relationships between individual terms must be learned by the model. In the subsequent Section 6, we explore how a number of the known linear relations are learned as a function of training epoch, and use this information to draw conclusions about the learning dynamics of the model. In Section 7, we show that augmenting a small amount of training data at a given loop with data from a lower loop improves performance, and discuss prospects for future multi-loop experiments.

The mathematical structure of the problem hints that some elements of the solution at higher loops may be determined using related elements of the solution at lower loops. Our second goal is therefore to discover this relationship implicitly, assuming it exists. In Section 8, we train Transformers to predict coefficients of terms at loop $L+1$, given a set of coefficients of potentially related terms at loop $L$. We also perform a number of ablations to determine conditions under which this relationship is no longer learnable. This study allows us to uncover certain features of the cross-loop relationship, which may prove crucial in further developing the bootstrap program.

Additionally, this paper contains several Appendices. In Appendix A, we further describe the mathematical formalism by which scattering amplitudes are expressible in a language-like fashion, via their symbols, which are sums of pairs of keys and integer coefficients. In Appendix B, we give a more comprehensive list of linear relations between symbol terms and evaluate them as a function of training epoch. In Appendix C, we perform a number of architecture ablations and evaluate their effects on the coefficient prediction task of Section 4. In Appendix D. we perform additional ablation experiments in the manner of Section 8 in order to further characterize the correspondence between elements at different loop orders.

### 1.1. Related Work

The amplitude bootstrap program has a long tradition, dating back to [11]. A recent review can be found in [22], and the specific data we use is from 14|. Our work
supplements the traditional approach by offering a novel problem-solving framework, where human intelligence is augmented by artificial intelligence to further push the state-of-the-art for amplitude calculations.

In a similar spirit, a number of recent works have also leveraged deep learning to tackle analytical calculations in theoretical physics. In particular, a sequence-to-sequence Transformer has been used to compute the squared amplitude of a particle interaction symbolically $\sqrt[23]{\text {; }}$ deep reinforcement learning has been applied to explore the landscape of string vacua [24; and Transformers have been employed to simplify polylogarithms 25], which are complicated mathematical functions entering multi-loop amplitudes similar to those we study (see Section 2.1). However, no previous work has used Transformers to perform computations in the amplitude bootstrap paradigm.

Methodologically, our work is closely related to recent works using Transformers for symbolic mathematical data. For example, Transformers have been taught to perform mathematical tasks such as solving differential equations 26, learning recurrent sequences 27, and finding the greatest common divisor of number pairs 28. A comparable approach has also been used to solve linear algebra tasks [21], including eigenvector decomposition and matrix inversion, which share many structural similarities with our amplitude bootstrap method. Additionally, our first experiment, in which we use some elements of a scattering amplitude to predict others, can be framed as a tensor completion problem-at a fixed loop order, the model must learn to fill in unseen elements of the solution based on elements it has seen at training time. This task is similar in some respects to low-rank matrix completion [29], where Transformers have previously been employed 30 .

## 2. Three-Gluon Form Factors in Planar $\mathcal{N}=4$ SYM Theory

The amplitude bootstrap program has seen substantial success in planar $\mathcal{N}=4$ superYang-Mills theory 31]. Similar to QCD, SYM contains gluons which self-interact; but instead of including quarks, it contains four gluinos and six scalars. The gluons, gluinos and scalars are all massless, and all transform into each other under the $\mathcal{N}=4$ supersymmetry. They all have the same number of internal "color" degrees of freedom. We take the number of colors $N_{c}$ to infinity, and refer to the Feynman diagrams that contribute to the scattering of these massless particles as planar. As a theoretical laboratory or model system for QCD, SYM allows us to see much further into the perturbative expansion than QCD. For example, a class of SYM amplitudes was recently computed to eight loops 14,32 . These amplitudes, referred to as three-gluon form factors $\mathcal{F}_{3 \mathrm{gFF}}$, involve three massless gluons and a massive color-singlet operator. The operator couples to gluons very similarly to how the Higgs boson does in the limit of a very heavy top quark. Thus $\mathcal{F}_{\text {3gFF }}$ is the SYM analog of the QCD process $g g \rightarrow H g$, which is known only to two loops 15. In fact, part of the QCD form-factor result (the so-called "highest-weight" part) is identical to the SYM result $33,34 \mid$. Figure 1 shows sample Feynman diagrams for this process to the current highest calculable loop order
in QCD and SYM, respectively.

In this work, we focus exclusively on the three-gluon form factors $\mathcal{F}_{3 \mathrm{gFF}}$ in planar $\mathcal{N}=4$ SYM. The known results up to loop $L=7$ are used for model training and evaluation.

### 2.1. Symbols: A Simple Language for Amplitudes

The three-gluon form factors, like many other amplitudes in SYM, can be expressed in terms of functions called generalized polylogarithms. They are multiple, iterated integrations of rational functions. In SYM, the calculation of scattering amplitudes at loop order $L$ requires integrals that are iterated $2 L$ times. Such amplitudes $\mathcal{F}^{(L)}$ can be characterized by another mathematical object known as the symbol \35]:

$$
\begin{equation*}
\mathcal{S}\left[\mathcal{F}^{(L)}\right]=\sum_{l_{i_{1}}, \ldots, l_{i_{2 L}} \in \mathcal{L}_{m}} C^{l_{1_{1}}, \ldots, l_{i_{2 L}}} l_{i_{1}} \otimes \cdots \otimes l_{i_{2 L}} \tag{1}
\end{equation*}
$$

Here $\mathcal{L}_{m}=\left\{l_{1}, \ldots, l_{m}\right\}$ is the symbol alphabet containing $m$ letters $l_{i}$, which are in turn functions of the particles' four-momenta, and $C^{l_{i_{1}}, \ldots, l_{i_{2 L}}}$ is a $2 L$-fold tensor of integer coefficients, most of which are zero. In other words, a solution for the symbol at loop $L$ can be represented by $m^{2 L}$ integers, with each sequence of $2 L$ letters (i.e., $l_{i_{1}} \otimes \cdots \otimes l_{i_{2 L}}$ ) serving as a key indexing into the integer-valued tensor $C^{l_{1}, \ldots, l_{i_{2 L}}}$. More details about the map from generalized polylogarithms to symbols are given in Appendix A.

The alphabet of $\mathcal{F}_{\text {3gFF }}$ is one of the simplest among all amplitudes and contains only six letters, i.e., $m=6$ :

$$
\begin{equation*}
\mathcal{L}_{\mathrm{3gFF}}=\{a, b, c, d, e, f\} \tag{2}
\end{equation*}
$$

cf. Appendix A. These letters are Lorentz-invariant functions of the gluons' four-momenta. Via their definition they transform under a dihedral symmetry with two generators:

cycle: $\{a, b, c, d, e, f\} \rightarrow\{b, c, a, e, f, d\}$, and flip: $\{a, b, c, d, e, f\} \rightarrow\{b, a, c, e, d, f\}$.

The $L$-loop form factor $\mathcal{F}_{3 \mathrm{gFF}}^{(L)}$ is invariant under dihedral transformations for any $L$.

As concrete examples, the symbols for $\mathcal{F}_{3 \mathrm{gFF}}^{(L)}$ at loops $L=1$ and $L=2$ contain only 6 and 12 nonvanishing terms, respectively:

$$
\begin{align*}
& \mathcal{S}\left[\mathcal{F}_{\mathrm{3gFF}}^{(1)}\right]=(-2) {[b \otimes d+c \otimes e+a \otimes f+b \otimes f+c \otimes d+a \otimes e] } \\
& \mathcal{S}\left[\mathcal{F}_{\mathrm{3gFF}}^{(2)}\right]=8[b \otimes d \otimes d \otimes d+c \otimes e \otimes e \otimes e+a \otimes f \otimes f \otimes f \\
&+b \otimes f \otimes f \otimes f+c \otimes d \otimes d \otimes d+a \otimes e \otimes e \otimes e] \\
&+ 16[b \otimes b \otimes b \otimes d+c \otimes c \otimes c \otimes e+a \otimes a \otimes a \otimes f \\
&\quad+b \otimes b \otimes b \otimes f+c \otimes c \otimes c \otimes d+a \otimes a \otimes a \otimes e] \tag{4}
\end{align*}
$$

Usually, we omit the tensor product " $\otimes$ " and use for example "bd" as shorthand for " $b \otimes d$ ", calling it a word or a key. For $L=1$, the key bd then indexes into the tensor
$C^{l_{1}, l_{2}}=C^{b, d}$, mapping onto the integer coefficient -2 . As another example, the key ab never appears in $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(1)}\right]$, and therefore ab maps onto a coefficient of 0 . The invariance of the form factor under dihedral transformations (3) relates all terms with the same coefficients in eq. (4) to one another. Hence at $L=1(L=2)$ there are really only 1 (2) nonzero terms to predict.

In general, $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(L)}\right]$ is a sum of $6^{2 L}$ elements (i.e., the monomials in eq. (4)) containing a key which is a $2 L$-sequence of letters, and an associated integer coefficient. This large number of $6^{2 L}$ elements can be substantially reduced via a set of conditions on the symbol that restrict which letters can appear next to each other. Therefore, most of the $6^{2 L}$ coefficients are zero, and most zeros can be accounted for by the following two simple rules:

- adjacency rule: any key including one of the subsequences ad, da, de (or their dihedral images) has zero coefficient 14 .
- prefix/suffix rule: any key beginning with $\mathrm{d}$, e or $\mathrm{f}$ or ending with $\mathrm{a}, \mathrm{b}$ or $\mathrm{c}$ has zero coefficient.

We call such zero coefficients trivial zeros. Table 1 records the actual numbers of nonzero coefficients in the symbol at different loop orders, as well as the naive $6^{2 L}$ counts and the counts excluding trivial zeros. The table shows that, at loop $L=6$ and higher, about half of the terms allowed by the above two simple rules still have a zero coefficient. We refer to such terms as nontrivial zeros.

| Loop | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Total $\left(6^{2 L}\right)$ | 36 | 1,296 | 46,656 | $1.7 \cdot 10^{6}$ | $6.0 \cdot 10^{7}$ | $2.2 \cdot 10^{9}$ | $7.8 \cdot 10^{10}$ | $2.8 \cdot 10^{12}$ |
| W/O trivial zeros | 6 | 102 | 1,830 | 32,838 | 589,254 | $1.1 \cdot 10^{7}$ | $1.9 \cdot 10^{8}$ | $3.4 \cdot 10^{9}$ |
| Total nonzero | 6 | 12 | 636 | 11,208 | 263,880 | $4.9 \cdot 10^{6}$ | $9.3 \cdot 10^{7}$ | $1.7 \cdot 10^{9}$ |

Table 1. Elements in the symbol $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(L)}\right]$ for loops $L=1$ to 8 .

Figure 2 shows the distribution of magnitudes of nonzero integer coefficients of $\mathcal{S}\left[\mathcal{F}_{\text {3gFF }}\right]$ at loops $L=4,5,6$ on a log scale. Note that the magnitudes of the coefficients grow quickly from lower to higher loops, a domain shift which may pose a challenge when attempting to train a model to generalize across loop orders.

### 2.2. Linear Relationships among Symbol Elements

In addition to the trivial zeros defined above, there exist many linear correlations in the symbol which highly constrain the values of many coefficients. We study three types of linear relations in this work-the integrability relations, the multiple-final-entry relations, and the triple-adjacency relation.

Many of these constraints are inspired by empirical observations and have deep physical roots yet to be understood. However, some are based on rather elementary mathematical considerations. For example, one important constraint is functional
![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-07.jpg?height=378&width=1156&top_left_y=294&top_left_x=449)

Figure 2. Histograms of the symbol coefficients for the three-gluon form factor at 4,5 , and 6 loops. The horizontal axis is the base 10 logarithm of the magnitude of the coefficient. The vertical axis is the (arbitrarily normalized) frequency with which coefficient magnitudes occur in the form factor.

integrability: a random multi-variate symbol is not the symbol of any function, because mixed partial derivatives must commute. This requirement correlates large sets of coefficients with specific adjacent letter pairs. One such integrability relation reads

$$
\begin{equation*}
F^{a, b}+F^{a, c}-F^{b, a}-F^{c, a}=0 \tag{5}
\end{equation*}
$$

which correlates the coefficients of four terms in the symbol at a time. Here $F^{a, b}$ is the abbreviation of $C^{l_{1}, \ldots, l_{i-1}, a, b, l_{i+2}, \ldots, l_{2 L}}$, the coefficient corresponding to the key “...ab ..." where the letter pair ab can appear in any pair of adjacent positions (or slots) in the key. The remaining letters (indicated by "...") may take any values but must be the same for each of the terms in the relation. In other words, eq. (5) can be written equivalently as

$$
\begin{align*}
& C^{l_{1}, \ldots, l_{i-1}, a, b, l_{i+2}, \ldots, l_{2 L}}+C^{l_{1}, \ldots, l_{i-1}, a, c, l_{i+2}, \ldots, l_{2 L}} \\
& -C^{l_{1}, \ldots, l_{i-1}, b, a, l_{i+2}, \ldots, l_{2 L}}-C^{l_{1}, \ldots, l_{i-1}, c, a, l_{i+2}, \ldots, l_{2 L}}=0 \tag{6}
\end{align*}
$$

for any choice of $l_{1}, \ldots, l_{i-1}, l_{i+2}, \ldots, l_{2 L} \in \mathcal{L}_{3 \mathrm{gFF}}$, and it remains valid for any position $i$ in the $2 L$-length keys.

A concrete instance of the relation (6) at five loops is

![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-07.jpg?height=69&width=1237&top_left_y=1928&top_left_x=401)

where the relevant adjacent letter pairs are underlined and the four integer coefficients are $72,-88,-72,56$, respectively. We thus have $72+(-88)-(-72)-56=0$, satisfying eq. (7). Another integrability relation correlates 14 coefficients at a time, the longest linear relation currently known.

While the integrability and triple-adjacency relations occur in all adjacent pairs of slots in the key, the final-entry conditions relate only sets of keys that have the same beginnings, but different suffixes.

In general, these linear relationships can serve as an excellent probe of the learning dynamics of the models, informing us about which properties of the symbol are learned at different training stages. We explore a subset of them in Section 6, and give a more comprehensive discussion in Appendix B.

### 2.3. Compact Symbol Representations

With increasing loop order, the number of elements in the symbol becomes very large, as can be seen in Table 1. at 8 loops, the symbol has around 1.7 billion terms. A compact symbol representation is therefore necessary at high loops. We derive one such compact representation by noticing that the number of independent suffixes for keys of nonzero coefficients is very limited.

Applying the multiple-final-entry relations (see Appendix B) and dihedral symmetry, one notices that all terms in the symbol can be related to terms ending in the following 8 sequences of four letters: dddd, bbbd, bdbd, bbdd, dbdd, fbdd, dbbd, and cddd. We thus create a new quad representation where 8 new tokens are added to represent these 8 suffixes. All keys at a given loop order are then represented by their first $2 L-4$ letters plus one of the eight quad suffix letters.

Represented in the quad format, there are only 391,570 keys for the loop $L=6$ symbol, in contrast to the 5 million keys in the original uncompressed format. Furthermore, compressing the data using the quad representation naturally removes the dihedral symmetry and all multiple-final-entry relations that involve only the last four entries. It is therefore interesting to see how the model will perform when it is presented with the more "efficient" quad format, versus having to learn these relations from the full symbol.

An even more compact octuple representation can be achieved by considering the last 8 letters in each element. There are 93 possible final-entry octuples after factoring out dihedral symmetry. This representation gives 16,971 keys in the $L=6$ symbol, 312,463 in the $L=7$ symbol, and 5.6 million in the $L=8$ symbol. In the current study, we do not use the octuple representation, since the highest loop order under consideration is $L=7$. However, the octuple representation may become necessary in the future to push past $L=8$, the highest loop order currently known.

## 3. Implementation Details

In this section, we briefly discuss the default architecture and tokenization scheme for the later experiments.

Due to the discrete nature of our problem, all tasks are framed as sequence-tosequence translation problems: coefficients and keys are both encoded as sequences of tokens, and the model is trained to minimize the cross-entropy of the probability distribution for the predicted coefficient sequence with the ground-truth solution. At loop $L$, keys are encoded as sequences of $2 L$ letter tokens, e.g., ' $\mathrm{a}, \mathrm{a}, \mathrm{b}, \mathrm{d}, \mathrm{d}, \mathrm{c}, \mathrm{e}$, e'. While several recent works have explored different ways to tokenize integers [36, 37, we simply encode coefficients as sequences of numerical tokens in base 1000; e.g., 12334 as ' $+, 12,334$ ', with the sign first [27,38]. To preserve syntax, zero coefficients are arbitrarily assigned a sign token of ' + '.

In most experiments, we use encoder-decoder Transformers, which contain a bidirectional Transformer encoder and an autoregressive Transformer decoder linked by
a cross-attention mechanism 16. Both encoder and decoder have the same number of layers (up to 8), the same number of attention heads (8 or 16), and the same dimension $(d=256, d=512$ or $d=1024)$. Henceforth, we describe a model with $N$ layers in the encoder and $N$ layers in the decoder as an $N$-layer Transformer.

Overall, our models have between 4.5 and 245 million trainable parameters, and the best performance on many of our experiments is obtained with models with fewer than

![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-09.jpg?height=57&width=1558&top_left_y=633&top_left_x=240)
tens of billions of parameters. Following similar observations on AI-for-mathematics applications [21, we are able to achieve very good results with these small Transformers trained on domain-specific data. In all experiments except where noted, we use the smallest model that can perform the task with large ( $>98 \%$ ) accuracy, where accuracy is defined per element rather than per individual token.

In the default model, we use a learnable positional encoding in both the encoder and decoder $\mid 16$; ; alternative schemes are explored in Appendix C. The optimizer is Adam [41], with a learning rate of $10^{-4}$ and a flat learning rate schedule. All models are implemented in PyTorch [42] and trained on a single NVIDIA V100 GPU with 32 GB of memory, or on larger architectures (A100).

Throughout the paper, we define an epoch as a pass over 300,000 key-coefficient pairs, as opposed to the more common definition of one full pass over all training data. This makes the notion of epoch size more comparable between different experiments that employ different amounts of data at different loops. At the end of each epoch, the model is evaluated on a held-out test set.

## 4. Predicting Symbol Coefficients from Keys

In the first experiment, we train Transformers to predict coefficients from their keys. The models are trained on a fraction of the symbol at a given loop, $L=5$ or 6 , and are tasked to predict the remaining terms. Because most coefficients are zero, we split the problem into two separate tasks: predicting whether the coefficient corresponding to a given key is zero, and predicting a nonzero coefficient from the associated key.

We first train 1-layer Transformers with dimension $d=256$ and 8 attention heads (i.e., 4.5 million parameters) to predict whether coefficients are zero or nonzero. We construct a dataset consisting of all nonzero-coefficient elements in the symbol plus an equal number of zero-coefficient elements. Here, zero-coefficient elements are selected randomly from the pool of all possible zeros, and the majority of zeros are thus trivial (as defined in Section 2.1). We explore an alternative prescription to handle the nontrivial zeros in Section 6.

At $L=5$, the model correctly classifies $99.96 \%$ of elements in a test set of 10,000 examples, after only one epoch (corresponding to observing only $57 \%$ of the symbol). At $L=6$, the model correctly classifies $99.91 \%$ of the test set after one epoch, i.e., after observing only $3 \%$ of the symbol, and $99.97 \%$ after two epochs (i.e., after observing $6 \%$ of the symbol). Distinguishing nonzero coefficients from these mostly trivial zeros thus
![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-10.jpg?height=930&width=1586&top_left_y=294&top_left_x=258)

Figure 3. Accuracy vs. epoch on the nonzero coefficient-from-key task at loop $L=5$ (left) and $L=6$ (right), for four model initializations shown in different colors. The bottom plots show the balance of predicted signs vs. epoch, with $+(-)$ indicating $100 \%$ $(0 \%)$ positive signs. Initially the model fluctuates between strongly favoring one sign or the other before more accurately predicting the mix of signs for individual terms.

appears to be a very simple problem for Transformers.

Predicting nonzero coefficients from their keys proves to be more difficult, necessitating larger models. We train 2-layer Transformers with dimension $d=512$ and 8 attention heads, for loops $L=5$ and $L=6$. At $L=5$, models are trained on 163,880 nonzero-coefficient elements (i.e., $62 \%$ of the symbol), and tested on 100,000 elements. At $L=6$, models are trained on $4,816,466$ nonzero-coefficient elements (i.e., $98 \%$ of the symbol), and are again tested on 100,000 elements.

At $L=5$, the best model (out of four initializations, or seeds) correctly predicts $43 \%$ of the coefficients from the test set after only one epoch. Accuracy is $95 \%$ after 7 epochs, $99 \%$ after 16 epochs and $99.5 \%$ after 47 epochs. At $L=6$, the best model (again out of four initializations) correctly predicts $95 \%$ of the coefficients in the test set after 66 epochs, $98 \%$ after 88 epochs and $99.3 \%$ after 199 epochs. We show accuracy as a function of epoch in Figure 3.

At both $L=5$ and 6, learning proceeds in two qualitative phases: first the magnitudes of the coefficients are learned, then the signs. At $L=5$, after two epochs, $97 \%$ of magnitudes are correctly predicted, but the signs are predicted at near chance level $(50 \%)$. After 5 epochs, $99 \%$ of magnitudes are predicted correctly, but only $78 \%$ of signs are predicted correctly. By epoch $10,98 \%$ of signs are predicted correctly. For $L=6$, training follows the same pattern, but proceeds more slowly: the model
learns the magnitudes of coefficients during the first 20 epochs. Accuracy then saturates around $50 \%$, while the model predicts the magnitudes of coefficients with more than $95 \%$ accuracy, but predicts their signs at near chance level. Finally, from epoch 40 to 70, the model learns to correctly predict the signs of the coefficients.

Until the sign is learned, the model strongly prefers to predict one sign over the other in each epoch: predictions may for example flip from $98 \%$ '-' signs in one epoch to $90 \%$ ' + ' signs in the next. This preference gradually diminishes with epoch, decreasing to within a few percent of the true proportion of positive and negative signs (which is very close to $50 \%$ positive and $50 \%$ negative) roughly at the midpoint of the second step (epoch 7 for $L=5$; epoch 60 for $L=6$ ). We indicate this behavior for one representative run in the lower portion of Figure 3. These fluctuations are not restricted to a particular subset of magnitudes; when the true or predicted magnitude is restricted to a given value, similar gradually-decreasing fluctuations are observed.

Furthermore, when models are trained to predict only the magnitudes of nonzero coefficients at $L=6$ (by setting all signs to ' + '), the best model can do so at $97.7 \%$ accuracy after 50 epochs. However, when models are trained to predict only the signs of the nonzero coefficients (by setting all magnitudes to ' 1 '), they exhibit random guessing behavior even after 100 epochs. These results suggest that learning the magnitude of the coefficient may be a prerequisite for learning the sign.

In summary, our results indicate that Transformers trained on a small fraction of the symbol can predict coefficients from their keys with very high accuracy.

## 5. Quad Representation of Symbols

At higher loops, a more compact representation of the form factor symbol is necessary for efficient training. For example, the loop $L=7$ symbol has 93 million nonzero-coefficient elements, almost 19 times as many as the $L=6$ symbol. Compressing the data using the quad representation can significantly improve the model training speed by reducing the number of nonzero-coefficient elements to a more manageable 7.3 million.

However, learning coefficients from keys in the quad representation is a harder problem than in the full representation. The quad representation eliminates many of the obvious symmetries in the symbol: it removes both the dihedral symmetry and relations involving up to four final entries. Thus, many potential sources of correlation between the training and test sets are no longer present, and the model is forced to learn more subtle correlations between coefficients and keys in order to correctly predict the coefficients in the test set. Larger models are therefore required for this task.

Here we train models to predict nonzero coefficients from keys in the quad representation at $L=6$ and $L=7$. For $L=6$, at which there are 391,570 quad keys, we use 4-layer Transformers with dimension $d=512$ and 8 attention heads; if we train smaller 2-layer Transformers with the same dimensions, we are unable to reach above $50 \%$ accuracy even after 100 epochs of training. We train on 381,570 key-coefficient pairs and test on the 10,000 held-out elements.

The training curves for the quad representation exhibit a two-step shape very similar to those for the uncompressed representation: during the first 10 epochs, only the magnitudes of coefficients are learned and their signs are predicted at chance level. The model achieves an accuracy of $95 \%$ in 43 epochs, which equates to 34 passes over the compressed training set since an epoch is fixed at 300,000 examples. The accuracy peaks at $99 \%$ after 192 epochs, equivalent to 152 passes over the compressed training set. However, the full representation achieves $95 \%$ accuracy in 64 epochs, which equates to slightly less than 4 passes through the uncompressed training set. Thus, although the larger models trained on the quad representation can reach performance benchmarks in fewer epochs than the smaller models trained on the full representations, they in fact take more passes through the training set in order to converge, confirming our intuition that training on the quad representation is a more challenging task.

For $L=7$, even larger models are required. 4-layer Transformers with dimension $d=1024$ and 16 attention heads are trained on the entire quad-compressed $L=7$ symbol (i.e., 7.3 million elements minus the held-out test set of 10,000 elements), achieving $99.1 \%$ accuracy on the test set. Here, training is considerably slower: the signs only begin to be learned after 100 epochs (vs. 10 epochs for the $L=6$ symbol). The model reaches $98.5 \%$ accuracy in about 400 epochs, which equates to 120 million examples, or 16 passes over the training set. The larger training set partially accounts for this difference in learning speed.

In order to explore the relationship between model capacity and training set size, we present in Table 2 the final overall test-set accuracy for the $L=7$ symbol in the quad representation for different model hyperparameters and training set sizes. We indicate models that fail to achieve at least $90 \%$ accuracy in gray. Accuracy decreases as training set size decreases, but remains above $94 \%$ for all but the smallest model, as long as the models are trained on 3 million examples or more. Such a training set translates to only about $41 \%$ of the symbol elements, which suggests that the models still possess significant predictive power even when data is given in the compressed quad representation.

## 6. Model Characterization via Relationship Accuracy

The results of the previous experiments strongly suggest that the model leverages certain correlations that are present in the data, such as dihedral symmetry and the final-entry relations described in Section 2.2, in order to more easily extrapolate from the training set into the test set. Additionally, the unusual two-phase accuracy curves that occur in both the full and quad representations warrant further investigation. In this section, we therefore explore how the linear relations behave as a function of epoch in order to better understand how the model learns.

We define an instance of a relation as a set of keys and their associated coefficients that obey a given relation. For example, for the relation given in eq. (5), a sample

| Train. size | $7.3 \mathrm{M}$ | $7 \mathrm{M}$ | $6 \mathrm{M}$ | $5 \mathrm{M}$ | $4 \mathrm{M}$ | $3 \mathrm{M}$ | $2 \mathrm{M}$ | $1 \mathrm{M}$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Arch. |  |  |  |  |  |  |  |  |
| 8 8 layers, $d=1024$ | $98.8 \%$ | $98.7 \%$ | $98.2 \%$ | $97.5 \%$ | $96.7 \%$ | $94.8 \%$ | $90.8 \%$ | $78.2 \%$ |
| 8 layers, $d=512$ | $96.2 \%$ | $97.4 \%$ | $98.4 \%$ | $96.6 \%$ | $95.3 \%$ | $93.8 \%$ | $88.5 \%$ | $36.7 \%$ |
| 6 layers, $d=1024$ | $98.6 \%$ | $98.9 \%$ | $98.0 \%$ | $97.9 \%$ | $96.7 \%$ | $94.8 \%$ | $90.3 \%$ | $58.5 \%$ |
| 6 layers, $d=512$ | $95.2 \%$ | $96.6 \%$ | $96.9 \%$ | $95.8 \%$ | $94.4 \%$ | $94.5 \%$ | $87.9 \%$ | $34.8 \%$ |
| 4 layers, $d=1024$ | $99.1 \%$ | $98.9 \%$ | $98.3 \%$ | $97.9 \%$ | $96.6 \%$ | $94.9 \%$ | $89.9 \%$ | $39.1 \%$ |
| 4 layers, $d=512$ | $48.5 \%$ | $96.0 \%$ | $94.1 \%$ | $48.3 \%$ | $94.6 \%$ | $81.7 \%$ | $55.3 \%$ | $33.9 \%$ |

Table 2. Maximum test-set accuracy after 250 epochs at loop 7 in the quad representation, for various training set sizes as well as numbers of layers and dimensions. The best of two models is shown. All models have 16 heads. (The smallest model occasionally does not emerge from the first plateau, with its accuracy then staying below $50 \%$.)

instance at 5 loops is, as in eq. (7):

![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-13.jpg?height=64&width=1227&top_left_y=1155&top_left_x=406)

which corresponds to the following set of keys and coefficients: \{cabcabdccd: 72, caccabdccd: -88 , cbacabdccd: -72 , ccacabdccd: 56$\}$.

We generate 500 instances of each homogeneous linear relation at loop $L$ and use them to evaluate the performance of a model trained on the coefficient-from-key prediction task as in Section 4. To do so, we randomly generate a set of keys that obey the given relation and then pair them with the corresponding coefficients. We discard and re-generate all such multi-term instances that do not contain at least one nonzero coefficient. In this way, we avoid instances with all zero terms that are trivially satisfied.

The relation instances generated are used only as an auxiliary test set. The training set consists of the full nonzero symbol plus an equal proportion of zeros as in Section 4 , while the coefficient-from-key test set is still employed as before. We note that more than $99 \%$ of the nonzero terms in the relation instances appear in the training set. However, we stress that this does not constitute data leakage, as the relation evaluations are auxiliary dataset-level metrics that do not influence training.

The linear relations may relate nonzero terms to the nontrivial zeros, which constitute a very small fraction of all possible zeros. Therefore, when choosing the zeros to be added to the training set, we explicitly select a large proportion of nontrivial zeros. The fraction of trivial zeros in the training set is restricted to be $5 \%$ of all zeros. Prioritizing the nontrivial zeros causes accuracy to decrease slightly on the trivial zeros; however, as trivial zeros are easy to learn and to identify (as per Section 4) we can simply manually set the predicted coefficient to zero for any trivial-zero terms in a relation instance.

The model used for this experiment is a 2-layer Transformer with $d=512$ and 8 heads, trained on $9,732,932$ elements (i.e., the full $L=6$ symbol plus an equal proportion of zeros), and tested on 100,000 randomly chosen held out elements. After 200 epochs, the model correctly predicts $98.47 \%$ of the coefficients in the test set. The learning curves
again reveal the familiar two qualitative phases, though training to a given accuracy now takes twice as many epochs (due to the addition of zeros increasing the dataset size by a factor of two) and overall magnitude and sign accuracy both reach $50 \%$ accuracy within the first epoch, due to the fact that zeros are learned quickly.

A complete list of the relations we evaluate is given in Appendix B. Here, we only describe the following short relations, which form a representative subset of the different types of relations studied. In the triple and integrability relations, the specified adjacent slots can appear anywhere in the key, while the specified adjacent slots in the final-entry relations must appear at the end of the key (which we denote by $\mathcal{E}$ instead of $F$ ).

$$
\begin{gather*}
\text { triple 0: } \quad F^{a, a, b}+F^{a, b, b}+F^{a, c, b}=0  \tag{9}\\
\text { integ 0: } \quad F^{a, b}+F^{a, c}-F^{b, a}-F^{c, a}=0  \tag{10}\\
\text { integ 1: } \quad F^{c, a}+F^{c, b}-F^{a, c}-F^{b, c}=0  \tag{11}\\
\text { final 16: } \quad \mathcal{E}^{b, f}-\mathcal{E}^{b, d}=0  \tag{12}\\
\text { final 17: } \quad \mathcal{E}^{c, d, d}+\mathcal{E}^{c, e, e}=0  \tag{13}\\
\text { final 18: } \quad \mathcal{E}^{d, d, b, d}-\mathcal{E}^{d, b, d, d}=0 \tag{14}
\end{gather*}
$$

For each relation, we quote four metrics: 1) whether the coefficients predicted by the model satisfy the given relation, regardless of whether the individual coefficients themselves are correct (red); 2) whether the relation is satisfied and all coefficients in the instance have the correct magnitudes, regardless of their signs (blue); 3) whether the relation is satisfied and all coefficients in the instance have the correct signs, regardless of their magnitudes (yellow); and 4) whether all coefficients in the instance are correct (green). We plot these metrics as a function of epoch in Figure 4. Similar results for additional relations are provided in Appendix B.

The linear relations can be grouped by structure into three categories that also define their behavior. The six relations discussed in this section contain two examples from each category.

- Group 1 relations, such as eq. (12) (final 16) and eq. (14) (final 18), are equivalence relations that require two coefficients to have the same magnitudes and signs. For all Group 1 relations, the relation is often satisfied before all the magnitudes are predicted correctly - the model predicts that both coefficients in the instance must be the same before it is able to successfully identify what that coefficient is. While the model learns the magnitudes of both coefficients in the relation instances fairly quickly, the signs of the coefficients are only predicted correctly $50 \%$ of the time until the sign is learned; however, both coefficients are consistently predicted to have the same sign. Thus, the sign fluctuations described in Section 4 (Figure 3) largely respect the Group 1 relations.
![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-15.jpg?height=854&width=1556&top_left_y=302&top_left_x=230)

Figure 4. Relation accuracy (red), magnitude accuracy (blue), sign accuracy (yellow), and coefficient accuracy (green) for each of the named relations, grouped by behavior. Relations in Group 1 (left column) are two-term equivalence relations that are consistently satisfied after only a few epochs. Relations in Group 2 (center column) are relations that require at least two coefficients to have different signs, and are not satisfied until the second phase. Relations in Group 3 (right column) are mixed relations, for which some instances decompose into pairs of equivalent terms (as in Group 1 relations) while others do not.

- Group 2 relations, such as eq. (9) (triple 0) and eq. 13) (final 17), are those in which at least two coefficients must have opposite signs. While the magnitudes of coefficients in these relation instances are learned within a few epochs, accuracy on all relation metrics remains low, but steadily increasing, until the signs are learned. This behavior is also largely dictated by the sign fluctuations shown in Figure 3.
- Group 3 relations, such as eq. (10) (integ 0) and eq. (11) (integ 1), are multi-term relations that may be satisfied by two or more pairs of identical coefficients or by a set of related but nonidentical coefficients (e.g., $40 \%$ of the generated (integ 0) instances are expressible as pairs of identical coefficients). Under the conditions where the model has a high probability of satisfying the Group 1 relations, the model predictions for the subset of Group 3 relation instances expressible as pairs of identical coefficients will also satisfy the relation.

These properties suggest an explanation for the double plateau behavior: first, the model learns to group elements whose coefficients have the same magnitude; then it learns to correctly predict those magnitudes. The model predictions for the sign fluctuate from epoch to epoch - rather wildly at first-until the second accuracy step is reached. However, these fluctuations consistently respect the Group 1 equivalence relations between elements. During this fluctuation phase, the model also gradually learns
![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-16.jpg?height=500&width=1290&top_left_y=327&top_left_x=366)

Figure 5. (Left) The leading three PCA components of token embeddings for a 2-layer Transformer with $d=512$ trained for 50 epochs on $L=5$ data, with zeros included. The leading three PCA components explain $63.56 \%$ of variance, and dihedral symmetry is not visible. (Right) The leading three PCA components of token embeddings for a 2-layer Transformer with $d=512$ trained for 200 epochs on $L=6$ data, with zeros included. The leading three PCA components explain $81.76 \%$ of variance. The octahedron exhibits dihedral symmetry.

the Group 2 relations; i.e., the model learns which coefficients with a given magnitude have the same sign and which do not. These fluctuations get smaller until the sign is eventually learned.

Both the cycle and flip symmetries can be expressed as Group 1 relations relating pairs of terms with identical coefficients. We plot the relation evaluation metrics for these relations in Appendix B and find that they behave similarly to other Group 1 relations.

We observe a further intriguing manifestation of the dihedral symmetry in the geometry of the embedding layer representation. Specifically, we extract the learned $d$-dimensional embeddings of the input letter tokens from the embedding layer of the Transformer and calculate the angles between them. These embedding vectors obey both the cycle and flip symmetries. We perform such an extraction experiment with a 2-layer Transformer with $d=512$, first at $L=5$ and then at $L=6$.

At $L=5$, the triangles $\triangle a b c$ and $\triangle d e f$ are approximately equilateral: all angles are within $1.5^{\circ}$ of $60.0^{\circ}$ for $\triangle a b c$ and within $2.7^{\circ}$ of $60.0^{\circ}$ for $\triangle d e f$. This result indicates that the embedding vectors obey the cycle symmetry. Similarly, all angles are within $3.7^{\circ}$ of $60.0^{\circ}$ for triangle $\triangle a b f$, within $1.5^{\circ}$ of $60.0^{\circ}$ for $\triangle b c d$, and within $3.3^{\circ}$ of $60.0^{\circ}$ for $\triangle a c e$; the fact that these triangles are approximately similar indicates that the embedding vectors obey the flip symmetry.

At $L=6$, we observe the same phenomenon even more strongly: $\triangle a b c$ and $\triangle d e f$ are approximately equilateral: all angles are within $1.0^{\circ}$ of $60.0^{\circ}$ for $\triangle a b c$ and within $0.6^{\circ}$ of $60.0^{\circ}$ for $\triangle d e f$, indicating cycle symmetry. Likewise, all angles are within $0.5^{\circ}$ of $60.0^{\circ}$ for triangle $\triangle a b f$, within $0.7^{\circ}$ of $60.0^{\circ}$ for $\triangle b c d$, and within $0.8^{\circ}$ of $60.0^{\circ}$ for $\triangle a c e$, indicating flip symmetry.

We perform standard linear principal component analysis (PCA) and plot the embeddings of these letter tokens in the space of the three leading PCA components in Figure 5. Projecting to the leading three components distorts the angles somewhat: at $L=5$, the dihedral symmetry is no longer apparent, while at $L=6$, the cycle and flip symmetries are visually apparent but the octahedron is no longer regular.

## 7. Mixed-loop Training

To successfully extend the bootstrap program to unseen loops, we must build models that can generalize from lower loops, for which we have the complete symbol, to higher loops, where only a small number of symbol terms may be available.

However, in many AI-for-mathematics applications, Transformers trained exclusively at one input length using absolute position encoding fail to generalize to different input lengths (37. Here we face a similar challenge. When using loop $L=6$ data to evaluate a model trained exclusively at $L=5$ (and vice versa) for the task of predicting nonzero coefficients, our baseline models can only attain an accuracy of at most $3 \%$ for a variety of model sizes and depths. Many predicted coefficients at unseen loops are nonsensical, such as the string ' +++ '. Given that our ultimate goal is to predict coefficients of keys at unseen higher loops, this failure of length generalization presents a major limitation that we must overcome.

In many ways, however, this problem goes even beyond simple length generalization. At each subsequent loop, the number of possible values of keys and coefficients both increase appreciably (see Figure 2), and it is not clear whether or how the functional form that relates them changes as well. Therefore, while alternative architecture designs, positional encoding schemes, and numerical encoding schemes (which we explore in Appendix C) may be helpful for this task, a much more comprehensive strategy will likely be needed.

As a first attempt to address this issue, we train Transformers to predict coefficients using an even proportion of $L=5$ and $L=6$ data. The full nonzero $L=5$ symbol (263,880 elements) is first combined with an equal proportion of zero-coefficient elements at $L=5$. Another 263,880 nonzero elements are then drawn from the $L=6$ symbol (representing $5 \%$ of the $L=6$ symbol) and augmented with a roughly equal amount of zero-coefficient elements at $L=6$. The zero-sampling is done naïvely, as in Section 4, rather than in the nontrivial-zero-biased manner of Section 6. Training and test sets are constructed for $L=5$ and $L=6$ separately, and they are then merged to create mixed-loop training and test sets. Each loop-specific training set for both $L=5$ and $L=6$ contains 517,760 examples, while each loop-specific test set contains 10,000 examples; every set is an equal mix of zero- and nonzero-elements. Additionally, we create a larger $L=6$ training set that is the same size as the mixed-loop set $(1,035,520$ examples, roughly $10 \%$ of the $L=6$ symbol), in order to evaluate whether the effects of mixed-loop training can be explained by the difference in training set size.

We train a model with 2 layers, $d=512$, and 8 attention heads in both the encoder
and decoder for 200 epochs, evaluating on both the mixed-loop test set and the individual $L=5$ and $L=6$ test sets. The model again exhibits two-phase learning behavior in all cases, as shown in Figure 6 (which displays results for all but the larger $L=6$ training set). We measure the performance by reporting three epochs,

(i) the epoch at which the test-set magnitude accuracy first exceeds $90 \%$,

(ii) the midpoint epoch of the plateau step, which is when the model's overall accuracy first reaches the average between its final-state accuracy and $75 \%$,

(iii) the epoch at which the overall test-set accuracy first exceeds $90 \%$,

as well as the best overall test-set accuracy after 200 epochs.

In Table 3, we compare the model performance when trained on each loop-specific training set to its performance when trained on the mixed training set. The dedicated $L=5$ model reaches all performance benchmarks for each loop in almost exactly half the number of epochs as the mixed-loop model. Accounting for the fact that the mixed-loop training set is exactly twice the size of the individual $L=5$ and $L=6$ training sets, this result suggests that the mixed-loop model solves the prediction problem for $L=5$ in
![](https://cdn.mathpix.com/cropped/2024_06_04_ec1ade990271478cbc9fg-18.jpg?height=954&width=1260&top_left_y=1279&top_left_x=365)

Figure 6. Accuracy vs. epoch for models trained on the mixed and per-loop training sets, evaluated on the per-loop test sets. In all cases, accuracy starts at $50 \%$ because zeros are learned in the first epoch. Models trained on the mixed training set take almost exactly half the number of epochs to reach performance benchmarks for $L=5$ as models trained on the dedicated $L=5$ training set, corresponding to approximately equal performance of these two models. However, models trained on a small subset of the $L=6$ symbol mixed with the full $L=5$ symbol are able to generalize to the $L=6$ test set much better than models trained on the $L=6$ symbol alone.
almost exactly the same amount of time as it would without the addition of $L=6$ data. On the other hand, the mixed-loop model learns the $L=6$ magnitudes in approximately the same number of epochs as the dedicated $L=6$ model, corresponding to reaching this benchmark (i.e., First Mag. Acc. $>90 \%$ ) in half as many iterations through the $L=6$ training set. The mixed-loop model also exceeds $\sim 90 \%$ overall accuracy, whereas the dedicated $L=6$ model does not. However, much of this effect may be explained by the dataset size: when a model is trained on the larger $L=6$ dataset that is the same size as the mixed set, the results appear highly similar to those we see when using the mixed training set.

These results are nonetheless rather encouraging: while adding a subset of $L=6$ data to $L=5$ does not lead to improvement on $L=5$ tasks, adding the full $L=5$ symbol to a subset of the $L=6$ symbol does appear to improve performance on $L=6$ tasks. This suggests that features learned at lower loops can be employed to enhance the predictive power of a small number of symbol elements at higher loops (for which we may only be able to determine a handful of coefficients a priori).

## 8. Steps Toward Predicting the Next Loop

Although mixed-loop training may allow us to better generalize from only a small number of symbol elements at unseen loops, we still wish to find ways to predict these elements at loops for which we do not have coefficient information.

As a first attempt at solving this task, we now consider a different problem setup. Instead of predicting unknown symbol elements at a given loop order, we would like to obtain the loop $L$ symbol from the coefficients at loop $(L-1)$. In other words, for any element at loop $L$, we want to recover its coefficient from the coefficients of a list of parent elements at one loop lower that are related by having similar strings of letters in their keys.

The keys of elements at loop $L$ are sequences of $2 L$ letters, whereas the keys at loop $L-1$ are only $(2 L-2)$ letters long. We therefore define the strike-two parents of a given

| Train | Eval | First Mag. Acc. <br> $>90 \%$ [Epoch] | Midpoint of <br> Step [Epoch] | First Total Acc. <br> $>90 \%$ [Epoch] | Best Acc. <br> Epoch 200 |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Mix | $L=5$ | 6 | 16 | 18 | $99.82 \%$ |
| $L=5$ | $L=5$ | 3 | 9 | 10 | $99.9 \%$ |
| Mix | $L=6$ | 25 | 88 | 112 | $94.6 \%$ |
| $L=6$ | $L=6$ | 20 | 51 | $\mathrm{~N} / \mathrm{A}$ | $84.5 \%$ |
| $L=6$, Large | $L=6$ | 17 | 92 | 100 | $97.67 \%$ |

Table 3. Model learning dynamics for mixed training: the epoch at which the test-set magnitude accuracy first exceeds $90 \%$, the midpoint of the plateau step, the epoch at which the overall test-set accuracy first exceeds $90 \%$, and the best overall test-set accuracy after 200 epochs. All training hyperparameters (architecture, initialization seed, etc.) are kept the same; only the training and test sets are changed between runs.
key at loop $L$ as the keys from loop $(L-1)$ which are created by simply striking out two letters from the key. For instance, the six strike-two parents of the loop $L=2$ key aacf are

$$
\begin{equation*}
\text { aacf=cf, } \quad \text { aacf=af, } \quad \text { aacff }=\mathrm{ac}, \quad \mathrm{aacf}=\mathrm{af}, \quad \mathrm{aac} f^{\prime}=\mathrm{ac}, \quad \mathrm{aa} f^{\prime}=\mathrm{aa} \tag{15}
\end{equation*}
$$

which are keys at $L=1$. In general, there are $\binom{2 L}{2}=L(2 L-1)$ strike-two parents of any key at loop $L$.

The dataset for this experiment is constructed by first selecting certain elements at loop $L$. For each key, we then construct the list of strike-two parents at loop $L-1$ and list their coefficients in the strikeout order, i.e., the coefficient corresponding to striking the first two letters from the key is ordered first in the coefficient list, the coefficient corresponding to striking the first and third letters from the key is ordered second in the list, etc. For the above example at $L=2$, the coefficient to be predicted is $C^{\text {aacf }}=0$, and the ordered list of its strike-two parent coefficients is $[0,-2,0,-2,0,0]$. Many of the parent coefficients are zero due to the nature of the strike-out. For example, pairs of letters that are not allowed to be adjacent can become adjacent in the parent keys after a letter in between is removed.

In these experiments, 4-layer Transformers with $d=512$ and 8 heads are trained to predict nonzero coefficients at $L=6$ from the coefficients of their $L=5$ parents. Model inputs are sequences of $\binom{12}{2}=66$ parent coefficients at $L=5$ given in strikeout order, while the targets are single coefficients at $L=6$.

From the $L=5$ and $L=6$ symbols, we can create 4.9 million examples ( $L=5$ parents and $L=6$ coefficients), but this dataset includes many duplicates. In fact, each example is duplicated 6.4 times on average, primarily due to the dihedral symmetry. To avoid contamination between the training and test set, we restrict the dataset to the 767,500 unique examples, split into 757,500 training and 10,000 test examples.

We report our results in the first row of Table 4. After 500 epochs, the model predicts $98.1 \%$ of the test examples. Learning is fast: $90 \%$ accuracy is achieved after 20 epochs, and $95 \%$ after 80 epochs. In these experiments, the magnitudes and signs of coefficients are learned simultaneously. In other words, no two-phase learning dynamics are observed, in contrast to previous experiments.

Our results suggest that there may exist learnable formulas for computing coefficients at $L=6$ from their strike-two parents at $L=5$. We are not, so far, capable of explicitly recovering these formulas, but additional ablation experiments may shed light on some of their features. We perform some such experiments in the remainder of this section, and give further results in Appendix D.

First, we investigate whether we can predict the $L=6$ coefficient from a smaller set of $L=5$ parents. One way is to only strike letters that are no more than $k$ positions away from each other in the $L=6$ keys. For example, with $k=1$ we only strike adjacent letters; for $k=2$ we only strike letters that are either adjacent or separated by one additional letter, etc. The number of parents remaining in this reduced set is $2 k L-\frac{k(k+1)}{2}$. At $L=6$, the 66 parents available at the maximum $k=12$ are reduced to
only 11 parents for $k=1$. We construct the reduced dataset for $L=6$ with $k=1,2,3,5$, remove duplicates, and present the parent coefficients to the model in the strikeout order.

Our models predict $98.3 \%, 98.4 \%, 98.1 \%$ and $94.3 \%$ of test examples for $k=5,3$, 2 and 1 , respectively. Predicting from 21 parents $(k=2)$, instead of the full 66 , has little impact on model performance, suggesting that the majority of coefficients at higher loops may be learnable using only a limited set of parent coefficients at lower loops.

|  | Accuracy | Magnitude accuracy | Sign accuracy |
| :--- | :---: | :---: | :---: |
| Strike-two, all parents | $98.1 \%$ | $98.4 \%$ | $99.6 \%$ |
| Strike-two, $k=5$ | $98.3 \%$ | $98.6 \%$ | $99.7 \%$ |
| Strike-two, $k=3$ | $98.4 \%$ | $98.7 \%$ | $99.7 \%$ |
| Strike-two, $k=2$ | $98.1 \%$ | $98.3 \%$ | $99.5 \%$ |
| Strike-two, $k=1$ | $94.3 \%$ | $95.2 \%$ | $98.5 \%$ |
| Randomly shuffled parents, all parents | $95.2 \%$ | $99.1 \%$ | $96.3 \%$ |
| Randomly shuffled parents, $k=2$ | $93.5 \%$ | $98.1 \%$ | $95.0 \%$ |
| Sorted parents, $k=5$ | $93.9 \%$ | $95.4 \%$ | $97.9 \%$ |
| Parent magnitudes only, all parents | $81.8 \%$ | $98.4 \%$ | $83.2 \%$ |
| Parent signs only, all parents | $93.3 \%$ | $93.5 \%$ | $99.0 \%$ |
| Parent signs only, all parents, sorted | $0.8 \%$ | $61.0 \%$ | $1.6 \%$ |

Table 4. Overall, magnitude, and sign accuracy for the cross-loop strike-out experiments described in the text. Best of four seeds, trained for about 500 epochs.

Next we experiment with the order of the strikeout parents. Surprisingly, even if parent coefficients (i.e., model inputs) are randomly shuffled, the model can still achieve $95.2 \%$ accuracy ( $93.5 \%$ with the 21 parents for $k=2$ ). When parents are sorted by increasing order in their numerical values, the model achieves $93.9 \%$ for $k=5$. This result suggests that the ability of Transformers to compute coefficients at $L=6$ from $L=5$ is mostly unaffected by permutations of the $L=5$ coefficients. However, the result may be an artifact of the way the strikeout experiment is constructed. Because certain letter adjacency conditions lead to zero coefficients, an ordered list of parents implicitly encodes some information about the original letter structure of the key. From previous experiments, we know that information about the letter structure of the key can be used to reconstruct the coefficient. In future strikeout experiments, we may wish to take this effect into account in order to better model the relationships between elements across loop orders.

Finally, we modify the values of the parent coefficients themselves in two different ways: (1) we set the signs of all parents to '+' before removing duplicates, thereby retaining only the magnitude information; or (2) we provide only the signs of the parent coefficients as inputs to the model, i.e., each parent coefficient is encoded as $-1,+0$ or +1 . In both cases, all modified parent coefficients are still presented in the strikeout order.

In case (1), models trained on only the magnitudes of the parents are able to recover
the magnitudes of the target coefficients with $98.4 \%$ accuracy, about the same level as models trained on unmodified parent coefficients. However, the sign of the target coefficient proves harder to learn when the model cannot see the signs of the parents (dropping from $99.6 \%$ to $83.2 \%$ accuracy).

In case (2), the models can achieve an overall accuracy of $93.3 \%$, and correctly predict the sign in $99 \%$ of the test cases. However, if we additionally shuffle or sort these parents in ascending order (i.e., all $-1 \mathrm{~s}$, all $0 \mathrm{~s}$, then all $+1 \mathrm{~s}$ ), we find that the model is totally unable to learn. In other words, we can drastically reduce information about either the values of the strikeout parents or their ordering and still recover the full coefficient, but we cannot do both simultaneously.

The fact that coefficients can be reconstructed reliably from their strike-two parents, despite the fact that symmetries such as dihedral symmetry are removed, suggests that a closed-form solution for computing coefficients from their parents may exist. It is likely that some amount of redundant information exists in the set of strikeout parents, as we are able to reconstruct the coefficient in a number of scenarios when the set of parents is severely altered. We plan to investigate such cross-loop relationships further in future work.

## 9. Conclusions and Future Work

In this study, we have shown that a Transformer model is able to successfully predict the coefficients of elements in the symbols for scattering amplitudes in $\mathcal{N}=4$ planar super Yang-Mills theory. Below we summarize the key findings of our work.

Our models learn in a two-phase fashion, first achieving very high accuracy on the coefficient magnitudes and then learning their signs. The distribution of predicted signs exhibits large fluctuations that gradually decrease as the sign is learned. The models cannot learn the signs of coefficients without the magnitude information, while they can learn magnitudes without sign information.

Transformers perform very well even when the data is compressed into the quad representation, where many trivial correlations between terms are removed. Enough information remains for a (larger) Transformer to successfully reconstruct coefficients from keys. This result bodes well for our ability to move to higher loops, as the space of possible coefficients and keys becomes quite large beyond $L=6$.

To study the learning dynamics, we have evaluated the models' performance on the linear relations. The relations can be classified into three groups based on whether they require coefficients to be identical, to have opposite signs, or some combination of the two. We propose a likely explanation for the two-phase behavior by assessing which of the known linear relations are satisfied at each epoch: the model first learns to group many terms with the same coefficient magnitude; then it learns what those magnitudes are; next it learns which coefficients with a given magnitude have the same sign and which have a different sign; finally it learns the true signs. Additionally, one of the simplest symmetries of the symbol, its dihedral symmetry, can be seen geometrically
in the embedding layers of the Transformer.

We have also trained models on mixed-loop data. Augmenting a small percentage of training examples at one loop with a substantial fraction of the symbol at lower loops leads to faster convergence and higher accuracy on a test set at the higher loop. This performance is particularly encouraging, as it suggests that only a relatively small number of coefficients may need to be provided at unseen loops in order for Transformers to successfully predict the rest of the symbol.

In our second set of experiments, namely the cross-loop strike-out approach, we have shown that a model can predict the coefficients at loop $L$ using only a small subset of related coefficients at loop $(L-1)$. Coefficient information from the lower loop can be scrambled or severely degraded (but not both at once), without hindering the ability to reconstruct the target coefficient.

In the future, we plan to train on more complicated objectives, with the goal of developing a model that encodes many types of information from multiple loops. This evokes the concept of a "foundation model": by training a large, multitask model with information about a number of relevant concepts, we hope to better characterize the complex recurrences and relations present in the symbol in order to generalize to unseen loops. This is a challenging domain-generalization task, as the distribution of possible keys and coefficients changes substantially between loop orders.

Ultimately, our goal is to build machine-learning models capable of computing amplitudes analytically $a b$ initio. In addition to the potential spin-offs for collider physics, understanding even simplified scattering amplitudes to all loop orders would give a remarkable new window into quantum field theory.

## Acknowledgments

We thank Yang-Hui He, Romuald Janik, Matt Schwartz, Jesse Thaler, Dmitris Papailiopoulos, Jordan Ellenberg, Gary Shiu, and Yiqiao Zhong for fruitful discussions. We would like to thank Phil Wang ("lucidrains") for use of his implementation of the RoPE and xPos relative position encodings. The work was supported in part by the U.S. Department of Energy (DOE) under Award No. DE-FOA-0002705, KA/OR55/22 (AIHEP). LD and TC are additionally supported by the U.S. Department of Energy Award No. DE-AC02-76SF00515. MW was supported by the research grant 00025445 from Villum Fonden.

## References

[1] A. Huss, J. Huston, S. Jones and M. Pellen, Les Houches 2021-physics at TeV colliders: report on the standard model precision wishlist, J. Phys. G 50 (2023) 043001 2207.02122.

[2] G. Heinrich, Collider Physics at the Precision Frontier, Phys. Rept. 922 (2021) 1 [2009.00516].

[3] C. Anastasiou, C. Duhr, F. Dulat, F. Herzog and B. Mistlberger, Higgs Boson Gluon-Fusion Production in QCD at Three Loops, Phys. Rev. Lett. 114 (2015) 212001 1503.06056.

[4] C. Anastasiou, C. Duhr, F. Dulat, E. Furlan, T. Gehrmann, F. Herzog et al., High precision determination of the gluon fusion Higgs boson cross-section at the LHC, JHEP 05 (2016) 058 1602.00695 .

[5] B. Mistlberger, Higgs boson production at hadron colliders at $N^{3} L O$ in QCD, JHEP 05 (2018) 028 1802.00833 .

[6] C. Duhr, F. Dulat and B. Mistlberger, Higgs Boson Production in Bottom-Quark Fusion to Third Order in the Strong Coupling, Phys. Rev. Lett. 125 (2020) 051804 1904.09990.

[7] F. A. Dreyer and A. Karlberg, Vector-Boson Fusion Higgs Production at Three Loops in QCD, Phys. Rev. Lett. 117 (2016) 072001 1606.00840.

[8] C. Duhr, F. Dulat and B. Mistlberger, Drell-Yan Cross Section to Third Order in the Strong Coupling Constant, Phys. Rev. Lett. 125 (2020) 1720012001.07717.

[9] C. Duhr, F. Dulat and B. Mistlberger, Charged current Drell-Yan production at $N^{3} L O, J H E P 11$ (2020) 143 [2007.13313.

[10] C. Duhr and B. Mistlberger, Lepton-pair production at hadron colliders at $N^{3} L O$ in $Q C D, J H E P$ 03 (2022) 116 2111.10379.

[11] L. J. Dixon, J. M. Drummond and J. M. Henn, Bootstrapping the three-loop hexagon, JHEP 11 (2011) 023 1108.4461.

[12] S. Caron-Huot, L. J. Dixon, A. McLeod and M. von Hippel, Bootstrapping a Five-Loop Amplitude Using Steinmann Relations, Phys. Rev. Lett. 117 (2016) 241601 1609.00669.

[13] S. Caron-Huot, L. J. Dixon, F. Dulat, M. von Hippel, A. J. McLeod and G. Papathanasiou, Six-Gluon amplitudes in planar $\mathcal{N}=4$ super-Yang-Mills theory at six and seven loops, JHEP $\mathbf{0 8}$ (2019) 016 1903.10890.

[14] L. J. Dixon, Ö. Gürdoğan, A. J. McLeod and M. Wilhelm, Bootstrapping a stress-tensor form factor through eight loops, JHEP 07 (2022) 153 2204.11901.

[15] T. Gehrmann, M. Jaquier, E. W. N. Glover and A. Koukoutsakis, Two-Loop QCD Corrections to the Helicity Amplitudes for $H \rightarrow 3$ partons, JHEP 02 (2012) 056 1112.3554.

[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez et al., Attention Is All You Need, in 31st International Conference on Neural Information Processing Systems, 2017, 1706.03762

[17] D. Bahdanau, K. Cho and Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, in 3rd International Conference on Learning Representations, 2015, 1409.0473

[18] J. Devlin, M. Chang, K. Lee and K. Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, in 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019, 1810.04805

[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner et al., An image is worth $16 x 16$ words: Transformers for image recognition at scale, in 9th International Conference on Learning Representations, 2021, 2010.11929.

[20] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont et al., Mathematical discoveries from program search with large language models, Nature 625 (2024) 468

[21] F. Charton, Linear algebra with transformers, Trans. Mach. Learn. Res. 2022 (2022) (2112.01898.

[22] S. Caron-Huot, L. J. Dixon, J. M. Drummond, F. Dulat, J. Foster, O. Gürdoğan et al., The Steinmann Cluster Bootstrap for $N=4$ Super Yang-Mills Amplitudes, PoS CORFU2019 (2020) 0032005.06735 .

[23] A. Alnuqaydan, S. Gleyzer and H. Prosper, SYMBA: symbolic computation of squared amplitudes in high energy physics with machine learning, Mach. Learn. Sci. Tech. 4 (2023) 015007 2206.08901 .

[24] J. Halverson, B. Nelson and F. Ruehle, Branes with Brains: Exploring String Vacua with Deep Reinforcement Learning, JHEP 06 (2019) 003 1903.11616.

[25] A. Dersy, M. D. Schwartz and X. Zhang, Simplifying Polylogarithms with Machine Learning, Int. J.

Data Sci. Math. Sci. 1 (2024) 135 2206.04115.

[26] G. Lample and F. Charton, Deep learning for symbolic mathematics, in 8th International Conference on Learning Representations, 2020, 1912.01412.

[27] S. d'Ascoli, P. Kamienny, G. Lample and F. Charton, Deep symbolic regression for recurrent sequences, in 39th International Conference on Machine Learning, 2022, 2201.04600.

[28] F. Charton, Learning the greatest common divisor: explaining transformer predictions, in 12th International Conference on Learning Representations, 2024, 2308.15594.

[29] F. J. Király, L. Theran, R. Tomioka and T. Uno, The algebraic combinatorial approach for low-rank matrix completion, J. Mach. Learn. Res. 16 (2015) 1391 1211.4116.

[30] N. Lee, K. Sreenivasan, J. D. Lee, K. Lee and D. Papailiopoulos, Teaching arithmetic to small transformers, in 12th International Conference on Learning Representations, 2024, 2307.03381.

[31] L. Brink, J. H. Schwarz and J. Scherk, Supersymmetric Yang-Mills Theories, Nucl. Phys. B 121 (1977) 77 .

[32] L. J. Dixon, A. J. McLeod and M. Wilhelm, A Three-Point Form Factor Through Five Loops, JHEP 04 (2021) 147 2012.12286.

[33] A. Brandhuber, G. Travaglini and G. Yang, Analytic two-loop form factors in $N=4$ SYM, JHEP 05 (2012) 082 1201.4170.

[34] C. Duhr, Hopf algebras, coproducts and symbols: an application to Higgs boson amplitudes, JHEP 08 (2012) 0431203.0454 .

[35] A. B. Goncharov, M. Spradlin, C. Vergu and A. Volovich, Classical Polylogarithms for Amplitudes and Wilson Loops, Phys. Rev. Lett. 105 (2010) 1516051006.5703.

[36] S. Golkar, M. Pettee, M. Eickenberg, A. Bietti, M. Cranmer, G. Krawezik et al., xval: A continuous number encoding for large language models, 2310.02989 .

[37] R. F. Nogueira, Z. Jiang and J. Lin, Investigating the limitations of the transformers with simple arithmetic tasks, in 1st Mathematical Reasoning in General Artificial Intelligence Workshop, 9th International Conference on Learning Representations, 2021,2102.13019.

[38] E. Wenger, M. Chen, F. Charton and K. Lauter, Salsa: Attacking lattice cryptography with transformers, in 36th Annual Conference on Neural Information Processing Systems, 2022, 2207.04785 .

[39] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal et al., Language models are few-shot learners, in 34th Annual Conference on Neural Information Processing Systems, 2020, 2005.14165.

[40] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix et al., Llama: Open and efficient foundation language models, 2302.13971.

[41] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, in 3rd International Conference on Learning Representations, 2015, 1412.6980 .

[42] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan et al., Pytorch: An imperative style, high-performance deep learning library, in 33rd Annual Conference on Neural Information Processing Systems, 2019, 1912.01703.

[43] A. B. Goncharov, Multiple polylogarithms, cyclotomy and modular complexes, Math. Res. Lett. 5 (1998) 4971105.2076 .

[44] A. B. Goncharov, Multiple polylogarithms and mixed Tate motives, math/0103059

[45] F. C. S. Brown and A. Levin, Multiple Elliptic Polylogarithms, 1110.6917.

[46] J. L. Bourjaily et al., Functions Beyond Multiple Polylogarithms for Precision Collider Physics, in Snowmass 2021, 3, 2022, 2203.07088.

[47] P. Shaw, J. Uszkoreit and A. Vaswani, Self-attention with relative position representations, in 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018, 1803.02155.

[48] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le and R. Salakhutdinov, Transformer-xl: Attentive language models beyond a fixed-length context, in 57th Conference of the Association for Computational Linguistics, 2019, 1901.02860.

[49] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu et al., Conformer:

Convolution-augmented transformer for speech recognition, in 21st Annual Conference of the International Speech Communication Association, 2020, 2005.08100.

[50] J. Su, Y. Lu, S. Pan, B. Wen and Y. Liu, Roformer: Enhanced transformer with rotary position embedding, Neurocomputing 568 (2024) 127063 2104.09864.

[51] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim et al., A length-extrapolatable transformer, in 61st Annual Meeting of the Association for Computational Linguistics, 2023, 2212.10554

[52] P. Wang, "rotary-embedding-torch." https://github.com/lucidrains/rotary-embedding-torch, 2023.

[53] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das and S. Reddy, The impact of positional encoding on length generalization in transformers, in 37th Annual Conference on Neural Information Processing Systems, 2023, 2305.19466
