# Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models 

Sangmin Woo* Donguk Kim ${ }^{*}$ Jaehyuk Jang* Yubin Choi Changick Kim<br>KAIST<br>\{smwoo95, kdu3613, jhyuk, choibinbin, changick\}@kaist.ac.kr<br>https://sangminwoo.github.io/AvisC/


#### Abstract

This study addresses the issue observed in Large Vision Language Models (LVLMs), where excessive attention on a few image tokens, referred to as blind tokens, leads to hallucinatory responses in tasks requiring fine-grained understanding of visual objects. We found that tokens receiving lower attention weights often hold essential information for identifying nuanced object details - ranging from merely recognizing object existence to identifying their attributes (color, position, etc.) and understanding their relationships. To counteract the over-emphasis on blind tokens and to accurately respond to user queries, we introduce a technique called Attentional Vision Calibration (AvISC). During the decoding phase, AvISC identifies blind tokens by analyzing the image-related attention distribution. It then dynamically adjusts the logits for the next token prediction by contrasting the logits conditioned on the original visual tokens with those conditioned on the blind tokens. This effectively lowers the dependency on blind tokens and promotes a more balanced consideration of all tokens. We validate AVISC on benchmarks such as POPE, MME, and AMBER, where it consistently outperforms existing decoding techniques in mitigating object hallucinations in LVLMs.


## 1 Introduction

Large Vision Language Models (LVLMs) [1, 9, 27, 28, 50] have demonstrated remarkable capabilities in generating coherent and contextually relevant descriptions from visual inputs. This success largely hinges on the models' ability to interpret and integrate complex visual information with textual data. However, a significant challenge that persists in these models is their tendency towards "hallucinations" - producing inaccurate or fabricated descriptions that do not accurately reflect the visual data. The phenomenon of hallucination in LVLMs can significantly impede their reliability, especially in applications requiring precise and trustworthy visual descriptions.

As shown in Fig. 1 LVLMs [9, 28] exhibit biased attention towards certain image tokens, which we refer to as blind tokens. Even when all the pixels in the image contain identical information and when the image does not contain any information relevant to the query, LVLMs [9, 28] tend to focus their attention on a few specific image tokens. This pattern of focusing on certain tokens, despite their lack of meaningful content, highlights potential flaws in the decoding processes of LVLMs.

Further analysis of the attention patterns in LVLMs is demonstrated in Fig. 2. We examine the attention distribution of LLaVA-1.5 [28] in response to the given image and query. The attention weights appear to align reasonably well with object regions, indicating that the model's attention is appropriately targeted at relevant areas. However, deeper scrutiny into the functional impact of these weights on response predictions reveals intriguing insights. Zeroing out image tokens that receive the bulk of attention does not significantly impact the original prediction logits. This suggests that LVLMs might assign high attention weights to tokens that do not carry substantial object-discriminative[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-02.jpg?height=486&width=1391&top_left_y=207&top_left_x=367)

Figure 1: Attention bias in LVLMs. Even when the image $(\mathcal{V})$ does not contain information relevant to the query $(Q)$, LVLMs $[9,28]$ exhibit a tendency for attention to be biased towards a few image tokens (i.e., blind tokens). This phenomenon is observed by averaging the attention weights across all layers when generating the first response token.

information. On the other hand, zeroing out image tokens with low attention weights drastically changes the prediction logits, leading to near-equal probabilities and indicating a loss of crucial object-discriminative information. Such skewed attention disproportionately favors blind tokens and often overlooks tokens that might contain finer details, potentially resulting in misclassifications or entirely incorrect predictions.

In response to this challenge, we propose a novel method termed Attentional Vision Calibration (AVISC) which recalibrates the model's attention on the fly during the decoding phase. Unlike existing approaches that typically require extensive training [19,34, 43, 49] or auxiliary models [10, 23, 36, 41, 47], AVISC operates without these prerequisites. AVISC dynamically modifies the decoding process in three steps: (i) Based on our finding that different LVLM exhibit different attentional patterns (see Fig. 4), we first select relevant layers that allocate a higher attention proportion to the image tokens. (ii) Next, we identify blind tokens, which disproportionately monopolize attention. These tokens are isolated, and all other image tokens are zeroed out, creating a biased visual input. (iii) Finally, we employ a contrastive decoding [11,21]. This technique contrasts the logits calculated from the original visual input with those derived solely from the blind tokens. By doing so, it amplifies the influence of tokens that exhibit significant differences between the two distributions. The recalibration process aims to decrease the reliance on blind tokens-those that occupy substantial attention but lack detailed content-and increase focus on underemphasized tokens that may hold crucial details. As a result, AVISC promotes a more balanced attention distribution across the image. This recalibration not only reduces hallucinations but also enhances the overall interpretative accuracy of LVLMs.

Through a series of experiments involving benchmarks like POPE [32], CHAIR [24], MME [12], and AMBER [38], we demonstrate that AVISC significantly mitigates hallucination while simultaneously improving the models' ability to capture and describe detailed image attributes more accurately.

## 2 Related Work

LVLMs [4, 9, 22, 27, 28, 50] are prone to generating hallucinations, i.e., misalignment between visual inputs and textual outputs. These hallucinations manifest across various semantic dimensions such as incorrect object presence, attributes, or relations.

To mitigate these, researchers have developed strategies across three levels:

Input-level. Efforts here focus on data quality improvement to reduce hallucinations [13, 26, 30, 37], including the introduction of negative data [26], counterfactual data [42] to challenge the model's assumptions, dataset cleansing to minimize noise and errors [39, 44].

Model-level. This includes increasing the resolution at which models process visual data $[6,27$, 28, 45], or enhancing perception abilities through advanced vision encoders [15, 18, 35]. These are usually training-based [19, 44], and often involve auxiliary supervision from external datasets [7] and reinforcement learning techniques $[2,14,34,43,48]$ to better align model outputs with accurate visual representations.

Output-level. Techniques like contrastive decoding [11, 21] directly contrast incorrect predictions during decoding, helping models distinguish between accurate and inaccurate descriptions. Guided

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-03.jpg?height=790&width=1392&top_left_y=226&top_left_x=366)

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-03.jpg?height=366&width=371&top_left_y=237&top_left_x=384)

Correct Answer: No

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-03.jpg?height=368&width=371&top_left_y=640&top_left_x=389)

Q Is there a grape in the picture?

Correct Answer:No

Figure 2: Impact of blind/non-blind tokens on prediction logits. (Left) Zeroing out image tokens with attention weights higher than the $\mu+\sigma$ (mean + standard deviation), i.e., blind tokens, does not significantly affect the original prediction logits, suggesting that LVLMs may assign high attention weights to tokens that do not carry significant object-discriminative information. Conversely, zeroing out non-blind tokens drastically disrupts the logits, often leading to near 50:50 probabilities, indicating a loss of object-discriminative information. (Right) Similarly, examples demonstrate that zeroing out non-blind tokens results in a loss of discriminative power for previously well-classified instances or produces entirely incorrect predictions, causing a significant drop in performance.

decoding [5, 10, 47] leverages external models like CLIP [31] or DETR [3] to enhance accuracy. Other approaches include training-free methods [16,36, 46] and post-hoc corrections via selffeedback $[20,40]$.

Among these, we focus on contrastive decoding methods: (1) VCD [21] mitigates statistical biases and language priors by contrasting output distributions from original and distorted visual inputs, moderating decoding probabilities. (2) M3ID [11] uses a similar approach where the reference image amplifies its influence over the language prior, thereby enhancing the generation of tokens with higher mutual information with the visual prompt.

Our approach belongs to the output-level category. AvISC analyzes attention patterns to identify blind tokens during decoding steps. It then utilizes a contrastive decoding technique to enhance token prediction. Our method does not require additional training, external data or models, and costly self-feedback mechanisms.

## 3 Approach: AvisC

We propose a straightforward method, called AVISC, to enhance visual object understanding in LVLMs during the decoding phase. AvISC dynamically calibrates the over-emphasis on blind tokens on-the-fly at every token generation step. The calibration is guided by the attention patterns of image tokens in response to the given image and textual query. Importantly, AVISC operates without additional training, external models, or complex self-feedback mechanisms. A visual summary of our method is

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-03.jpg?height=589&width=838&top_left_y=1863&top_left_x=920)

Figure 3: An overview of AVISC.
shown in Fig. 3. AvISC modifies the decoding process in three steps: (1) Layer selection: choose layers significantly influenced by image tokens, (2) Blind token identification: detect non-relevant tokens in selected layers, and (3) Contrastive decoding: adjust the decoding process to mitigate the influence of blind tokens.

### 3.1 LVLM Framework

Uni-modal encoding. LVLM begins by encoding visual inputs and textual queries into compact representations. Visual inputs provide contextual information that helps generate responses relevant to the textual queries. The text data is tokenized, turning it into a sequence of manageable pieces for further processing. For visual data, a commonly used encoder is a pre-trained model like CLIP [31], which is already semantically aligned with textual data through extensive training on image-text pairs.

Cross-modal alignment. As LLM inherently perceives only text, aligning text and vision modalities is essential. Instead of retraining LLM, which would be prohibitively expensive, a more viable approach is to use a learnable cross-modal alignment module. This module, such as Q-Former [22] or a linear projection layer [28], transforms visual features into a format compatible with the LLM's input space. This process results in a set of visual tokens, $\mathcal{V}=\left\{v_{0}, v_{1}, \ldots, v_{N-1}\right\}$, which are then concatenated with the text tokens, $Q=\left\{\sigma_{N}, \sigma_{N+1}, \ldots, \sigma_{N+M-1}\right\}$, to form a unified input sequence of length $N+M$.

Next token prediction via LLM. The concatenated sequence of visual and textual tokens is then processed by LVLM, parametrized by $\theta$, which generates responses in an auto-regressive manner. The model calculates logits for each potential next token:

$$
\begin{equation*}
\ell_{t}=\log p\left(\xi_{t} \mid \mathcal{V}, Q, \xi_{<t} ; \theta\right) \tag{1}
\end{equation*}
$$

where $\ell_{t}$ are the logits for the next token at timestep $t, \xi_{t}$ denotes the next token being predicted, and $\xi_{<t}$ represents the sequence of tokens generated up to timestep $(t-1)$. From these logits, we apply a softmax function to convert logits into a normalized probability distribution:

$$
\begin{equation*}
p\left(\xi_{t}\right)=\operatorname{Softmax}\left(\ell_{t}\right) \tag{2}
\end{equation*}
$$

The next token $\xi_{t}$ is sampled from this probability distribution, with the model continuing this predictive process until the response sequence is complete.

### 3.2 Attentional Vision Calibration for Alleviating Hallucinations

Visual hallucinations in LVLMs can emerge during the decoding phase when the model selects tokens based on erroneous probability distributions that do not align with the visual inputs. These discrepancies, as demonstrated in our observations (refer to Figs. 1 and 2), often originate from an attentional bias toward certain non-relevant tokens, referred to as blink tokens. Our methodology aims to recalibrate these attention patterns to correct such hallucinations.

Layer selection. As illustrated in Fig. 4, the attentional behavior of LVLM layers varies depending on the model's architecture or size. For example, InstructBLIP [9] shows increasing attention levels in the later layers, whereas LLaVA-1.5 [28] exhibits a concentration of attention in the earlier layers To adapt these diverse models, we initially focus on selecting layers that exhibit a high proportion of image-related attention. Formally, we define the attention weight matrix for $i$-th layer as follows:

$$
\begin{equation*}
\mathbf{A}_{i}=\left[\mathbf{a}_{h, q, k}^{i}\right]_{(h, q, k)=(1,1,1)}^{(h, q, k)=(H, N+M, N+M)} \tag{3}
\end{equation*}
$$

where $\mathbf{a}_{h, q, k}^{i}$ represents the attention weight assigned by head $h$, for query $q$, to key $l_{k}$ in layer $i$. The model handles two types of tokens: image tokens ( $\mathcal{V} \in \mathbb{R}^{N \times D}$ ) and query tokens $\left(Q \in \mathbb{R}^{M \times D}\right)$. Next, we calculate the proportion of attention dedicated to image tokens for each layer $i$ as:

$$
\begin{equation*}
A P_{i}^{\text {layer }}=\frac{\sum_{h} \sum_{k=1}^{N} \mathbf{a}_{h,(N+M), k}^{i}}{\sum_{i, h} \sum_{k=1}^{N} \mathbf{a}_{h,(N+M), k}^{i}} \tag{4}
\end{equation*}
$$

where $H$ is the total number of attention heads, $N$ is the number of image tokens, and $M$ is the number of query tokens. We sort the layers by this proportion and employ top-P sampling based on a predefined threshold value $\gamma$. The selected layers are:

$$
\begin{equation*}
\{\text { Selected Layers }\}=\text { top-P }\left(\left\{A P_{i}^{\text {layer }}\right\}_{i=1}^{L}, \gamma\right) \tag{5}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-05.jpg?height=447&width=1225&top_left_y=224&top_left_x=447)

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-05.jpg?height=384&width=504&top_left_y=239&top_left_x=453)

(a) InstructBLIP [9]

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-05.jpg?height=388&width=523&top_left_y=234&top_left_x=1148)

(b) LLaVA-1.5 [28]

Figure 4: Layer-wise image attention proportion in LVLMs [9, 27]. This shows the proportion of attention given to image tokens at each layer compared to the overall attention. Different layers exhibit distinct attention patterns, and these patterns vary between different models. The attention weights are averaged across 60 questions from the LLaVA-bench [28].

Here, top-P selects layers until the cumulative proportion of image attention across these layers meets or exceeds $\gamma$. These selected layers are used to analyze and adjust the attention at the token level and identify specific image tokens that the model may over-rely on, i.e., blind tokens.

Blind token identification. After selecting relevant layers, we calculate the attention weights for each image token within these layers. The attention proportion for image tokens, denoted as $A P^{\text {image }}$, is calculated by averaging the attention weights across the selected layers and attention heads:

$$
\begin{equation*}
A P^{\text {image }}=\frac{\sum_{i \in\{\text { Selected Layers }\}} \sum_{h=1}^{H} \mathbf{a}_{h,(N+M),[1: N]}^{i}}{\mid\{\text { Selected Layers }\} \mid \times H} \tag{6}
\end{equation*}
$$

To identify tokens that disproportionately capture the model's attention, i.e., blind tokens, we calculate the mean $(\mu)$ and standard deviation $(\sigma)$ of the image attention weights. Tokens with an attention proportion exceeding $\mu+\lambda \sigma$ (where $\lambda$ is a hyperparameter) are classified as blind tokens:

$$
\begin{equation*}
\{\text { Blind Token Indices }\}=\left\{j \mid A P_{j}^{\text {image }}>\mu+\lambda \sigma\right\} \tag{7}
\end{equation*}
$$

Contrastive decoding. Our method seeks to reduce the influence of blind tokens, thereby decreasing the incidence of hallucinations in LVLMs. Drawing inspiration from recent successes in contrastive decoding $[11,21]$, which effectively minimizes hallucinations by contrasting the differences between an image and its distorted counterpart, we adopt a similar scheme. We construct a new set of visual tokens $\mathcal{V}^{*}$ by zeroing out non-blind tokens and only leaving blind tokens, which biases the input towards emphasizing blind tokens:

$$
\begin{equation*}
\mathcal{V}^{*}=\bigcup_{j=1}^{N} \mathbb{1}_{\{j \in \text { Blind Token Indices }\}}(j) v_{j} \tag{8}
\end{equation*}
$$

Next, we compute the logits using both the original input $(\mathcal{V})$ and the biased input $\left(\mathcal{V}^{*}\right)$ :

$$
\begin{align*}
& \ell_{t}=\log p\left(\xi_{t} \mid \mathcal{V}, Q, \xi_{<t} ; \theta\right) \\
& \ell_{t}^{*}=\log p\left(\xi_{t} \mid \mathcal{V}^{*}, Q, \xi_{<t} ; \theta\right) \tag{9}
\end{align*}
$$

where $\ell_{t}$ and $\ell_{t}^{*}$ are the logits computed from the original and the biased inputs, respectively. We adjust the logits by contrasting the original and biased outputs, and then sample the next token $\xi_{t}$ from the following softmax distribution:

$$
\begin{equation*}
\xi_{t} \sim \operatorname{Softmax}\left((1+\alpha) \ell_{t}-\alpha \ell_{t}^{*}\right) \tag{10}
\end{equation*}
$$

Here, $\alpha$ is a hyperparameter that moderates the contrastive effect. This balances the distribution of attention across tokens thereby mitigating the likelihood of visual hallucinations in LVLMs.

## 4 Experiments

### 4.1 Evaluation Setup

In our experiments, we did not constrain the LVLMs to provide one-word answers in discriminative tasks, which often require simple 'Yes' or 'No' responses. For instance, we avoid instructions such as

|  | Setup | Method | InstructBLIP [9] |  |  |  | LLaVA 1.5 [28] |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | Acc. | Prec. | Rec. | F1 | Acc. | Prec. | Rec. | F1 |
| $\sqrt{2}$ <br> 0 <br> 0 <br> 0 <br> $U$ <br> $\dot{1}$ <br> $\sum_{i}^{\infty}$ | Random | base | 82.27 | 82.84 | 81.40 | 82.11 | 84.47 | 83.35 | 86.13 | 84.72 |
|  |  | VCD | 83.37 | 83.39 | 82.60 | 83.24 | 84.80 | 83.00 | 87.53 | 85.20 |
|  |  | M3ID | 84.37 | 84.62 | 84.00 | 84.31 | 86.00 | 85.11 | 87.27 | 86.18 |
|  |  | AvisC | 88.73 | 93.88 | 82.87 | 88.03 | 87.93 | 88.24 | 87.53 | 87.88 |
|  | Popular | base | 77.77 | 74.81 | 83.73 | 79.02 | 82.23 | 79.72 | 86.47 | 82.95 |
|  |  | VCD | 78.00 | 75.12 | 83.73 | 79.19 | 82.27 | 79.19 | 87.53 | 83.15 |
|  |  | M3ID | 77.30 | 74.10 | 83.93 | 78.71 | 82.83 | 79.62 | 88.27 | 83.72 |
|  |  | AvisC | 83.90 | 81.33 | 88.00 | 84.53 | 84.33 | 81.71 | 88.47 | 84.96 |
|  | Adversarial | base | 73.13 | 69.41 | 82.60 | 75.46 | 77.10 | 72.57 | 87.13 | 79.19 |
|  |  | VCD | 75.87 | 72.85 | 82.47 | 77.36 | 76.10 | 71.50 | 86.80 | 78.41 |
|  |  | M3ID | 76.03 | 72.47 | 83.93 | 77.79 | 77.70 | 73.23 | 87.33 | 79.66 |
|  |  | AvisC | 81.57 | 80.37 | 83.53 | 81.92 | 77.53 | 72.82 | 87.87 | 79.64 |
| $\sqrt{2}$ <br> 2 <br> 2 <br> 2 <br> $\dot{c}$ <br> $\dot{4}$ | Random | base | 81.00 | 77.71 | 86.93 | 82.06 | 82.73 | 77.43 | 92.40 | 84.26 |
|  |  | VCD | 81.73 | 78.67 | 87.07 | 82.66 | 81.30 | 75.45 | 92.80 | 83.23 |
|  |  | M3ID | 82.33 | 77.81 | 90.47 | 83.66 | 83.57 | 77.86 | 93.80 | 85.09 |
|  |  | AvisC | 88.47 | 87.66 | 89.53 | 88.59 | 84.60 | 79.29 | 93.67 | 85.88 |
|  | Popular | base | 75.00 | 70.14 | 87.07 | 77.69 | 76.10 | 69.86 | 91.80 | 79.34 |
|  |  | VCD | 75.33 | 70.52 | 87.07 | 77.92 | 75.43 | 68.58 | 93.87 | 79.26 |
|  |  | M3ID | 75.60 | 70.40 | 88.33 | 78.36 | 76.80 | 70.20 | 93.13 | 80.06 |
|  |  | AvisC | 81.77 | 77.82 | 88.87 | 82.98 | 78.83 | 72.10 | 94.07 | 81.63 |
|  | Adversarial | base | 68.80 | 63.57 | 88.07 | 73.84 | 67.90 | 62.11 | 91.80 | 74.09 |
|  |  | VCD | 69.70 | 64.54 | 87.47 | 74.27 | 67.43 | 61.50 | 93.20 | 74.11 |
|  |  | M3ID | 69.57 | 64.21 | 88.40 | 74.39 | 68.10 | 61.99 | 93.60 | 74.58 |
|  |  | AvisC | 72.53 | 67.12 | 88.33 | 76.28 | 68.97 | 62.70 | 93.67 | 75.11 |
| E | Random | base | 80.00 | 77.08 | 85.40 | 81.02 | 82.40 | 77.03 | 92.33 | 83.99 |
|  |  | VCD | 81.73 | 79.35 | 85.80 | 82.45 | 82.27 | 75.85 | 94.67 | 84.22 |
|  |  | M3ID | 80.57 | 76.77 | 87.67 | 81.85 | 82.83 | 76.64 | 94.47 | 84.62 |
|  |  | AvisC | 86.47 | 85.89 | 87.27 | 86.57 | 85.00 | 78.81 | 95.73 | 86.45 |
|  | Popular | base | 73.53 | 68.80 | 86.13 | 76.49 | 72.03 | 65.57 | 92.80 | 76.84 |
|  |  | VCD | 74.10 | 69.45 | 86.07 | 76.87 | 71.77 | 64.90 | 94.80 | 77.05 |
|  |  | M3ID | 74.57 | 69.45 | 87.83 | 77.53 | 72.83 | 66.04 | 94.00 | 77.58 |
|  |  | AvisC | 78.00 | 73.68 | 87.13 | 79.84 | 74.80 | 67.46 | 95.80 | 79.17 |
|  | Adversarial | base | 68.00 | 63.49 | 84.73 | 72.59 | 68.73 | 62.54 | 93.40 | 74.92 |
|  |  | VCD | 70.27 | 65.43 | 85.93 | 74.29 | 68.27 | 62.00 | 94.40 | 74.84 |
|  |  | M3ID | 68.90 | 64.06 | 86.13 | 73.47 | 68.13 | 61.88 | 94.47 | 74.78 |
|  |  | AvisC | 73.07 | 67.80 | 87.87 | 76.54 | 69.20 | 62.61 | 95.33 | 75.58 |

Table 1: POPE benchmark results. AVISC consistently outperforms base decoding and other methods: VCD [21] and M3ID [11]. We reimplemented VCD and M3ID in our evaluation setup.

"Please answer in one word." in the query text. We see that imposing a one-word response constraint on LVLMs leads to notable changes in performance (see Appendix D). For the experiments, we set $\mathrm{P}=0.5$ in Eq. (5), $\lambda=1$ Eq. (7), $\alpha=3$ for InsturctBLIP [9] and $\alpha=2.5$ for LLaVA-1.5 [28] in Eq. (10). ${ }^{2}$

LVLMs. We evaluated AVISC on two state-of-the-art LVLMs: LLaVA-1.5 [28] and InstructBLIP [9], both incorporating Vicuna 7B [8] as an LLM backbone. LLaVA-1.5 synchronizes image and text modalities by applying linear projection layers, while InstructBLIP uses the Q-Former [22] to efficiently link visual and textual features using a fixed number of tokens (e.g., 32 tokens). Notably, AVISC is model-agnostic and can integrate with various of LVLM architectures.

Benchmarks. (1) POPE [24] views hallucination evaluation as a binary classification task (yes/no) with questions regarding object presence (e.g., "Is there a cat in the image?"). It includes 500 images from MS-COCO and evaluates them based on visible objects and imaginary ones across different object categories, using three setups (random, popular, and adversarial). (2) MME [12] evaluates 14 subtasks including object hallucination by answering binary questions about object existence, count, position, color, etc. (3) AMBER [38] includes both generative and discriminative tasks, focusing on[^1]

| Model | Method | Object-level |  | Attribute-level |  | Total <br> Score |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Existence | Count | Position | Color |  |
| InstructBLIP [9] | base | $170.19_{( \pm 11.12)}$ | $89.52_{( \pm 11.04)}$ | $67.62_{( \pm 14.04)}$ | $114.76_{( \pm 9.60)}$ | $442.09_{( \pm 31.51)}$ |
|  | $\mathrm{VCD}$ | $172.62_{( \pm 8.92)}$ | $98.33_{( \pm 15.99)}$ | $71.90_{( \pm 13.42)}$ | $117.14_{( \pm 10.70)}$ | $459.99_{( \pm 16.56)}$ |
|  | M3ID | $173.89_{( \pm 10.52)}$ | $89.72_{( \pm 13.44)}$ | $72.72_{( \pm 14.77)}$ | $110.56_{( \pm 7.20)}$ | $446.88_{( \pm 28.54)}$ |
|  | AVISC (Ours) | $184.76_{( \pm 5.56)}$ | $82.85_{( \pm 12.16)}$ | $74.76_{( \pm 6.19)}$ | $131.43_{( \pm 4.76)}$ | $473.80_{( \pm 19.67)}$ |
| LLaVA 1.5 [28] | base | $173.57_{( \pm 8.16)}$ | $110.00_{( \pm 15.82)}$ | $100.47_{( \pm 18.78)}$ | $125.24_{( \pm 15.91)}$ | $509.28_{( \pm 30.57)}$ |
|  | VCD | $172.14_{( \pm 8.09)}$ | $117.14_{( \pm 8.76)}$ | $103.33_{( \pm 20.56)}$ | $119.52_{( \pm 8.58)}$ | $512.14_{( \pm 31.82)}$ |
|  | M3ID | $178.33_{( \pm 6.83)}$ | $107.22_{( \pm 14.78)}$ | $96.39_{( \pm 5.52)}$ | $127.50_{( \pm 8.28)}$ | $509.44_{( \pm 22.52)}$ |
|  | AVISC (Ours) | $189.29_{( \pm 1.82)}$ | $104.76_{( \pm 11.66)}$ | $106.19_{( \pm 13.93)}$ | $127.86_{( \pm 9.13)}$ | $528.09_{( \pm 24.70)}$ |

Table 2: MME-Hallucination [12] benchmark results. Our method effectively reduces hallucinations at both object and attribute levels, surpassing VCD [21] and M3ID [11] in Total Score.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-07.jpg?height=358&width=1374&top_left_y=810&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-07.jpg?height=284&width=648&top_left_y=823&top_left_x=381)

(a) InstructBLIP [9]

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-07.jpg?height=301&width=661&top_left_y=820&top_left_x=1076)

(b) LLaVA-1.5 [28]

Figure 5: Performance comparison on MME-Fullset. AVISC achieves top performance in 7 of 14 categories with InstructBLIP [9] and in 11 categories with LLaVA-1.5 [28]. Beyond minimizing hallucinations, AVISC also boosts the general functionality of LVLMs.

hallucinations related to object existence, attributes, and relationships, with performance evaluated using CHAIR for generative tasks and an F1 score for discriminative tasks. The overall AMBER score is calculated as $((100-$ CHAIR $)+\mathrm{F} 1) / 2$.

Baselines. AVISC aims to minimize hallucinations in LVLMs without the need for external models, costly self-feedback mechanisms, or further training. We select baseline methods that fulfill these conditions. We choose recent contrastive decoding methods as baselines, notably VCD [21] and M3ID [11]. These methods are designed to reduce object hallucinations by enhancing the influence of the reference image over the language model's prior or statistical bias, by contrasting output distributions from both original and altered visual inputs. We reimplemented VCD and M3ID within our evaluation framework.

### 4.2 Results on Benchmarks

POPE. Table 1 showcases the performance of different methods on the POPE benchmark [24] across MS-COCO [25], A-OKVQA [33], and GQA [17] datasets, evaluated under Random, Popular, and Adversarial setups. (AVISC) consistently outperforms the baseline (base) and other decoding methods (VCD [21], M3ID [11]) in most cases, achieving the highest Accuracy and F1 scores. It also demonstrates balanced improvements in Precision and Recall, indicating a reduction in errors and better information capture. For InstructBLIP, AVISC shows a significant performance boost, particularly in mitigating hallucinations related to object existence. However, LLaVA 1.5 exhibits less pronounced improvements in Popular and Adversarial setups, highlighting its limitations in more challenging scenarios. Yet, overall, AvISC proves to be robust and effective across different datasets and query setups.

MME-Hallucination. Table 2 presents performance results for InstructBLIP [9] and LLaVA 1.5 [28] on the MME-Hallucination benchmark [12], focusing on object-level (Existence, Count) and attributelevel (Position, Color) metrics. Both models exhibit significant improvements in the Existence category with Ours, achieving the highest scores. While VCD [21] performs best in the Count metric, AVISC excels in the Position and Color categories, attaining the top scores for both models. AVISC demonstrates superior performance in Total Score compared to other methods, affirming its effectiveness in reducing hallucinations and improving accuracy across multiple metrics.

|  | Metric | InstructBLIP [9] |  |  |  | LLaVA 1.5 [28] |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | base | VCD [21] | M3ID [11] | AvisC | base | $\operatorname{VCD}[21]$ | M3ID [11] | AvisC |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-08.jpg?height=186&width=40&top_left_y=369&top_left_x=391) | CHAIR $\downarrow$ | $8.40_{( \pm 0.57)}$ | $7.60_{( \pm 0.42)}$ | $6.85_{( \pm 0.07)}$ | $6.70_{( \pm 0.28)}$ | $7.95_{( \pm 0.64)}$ | $6.70_{( \pm 0.42)}$ | $6.00_{( \pm 0.14)}$ | $6.25_{( \pm 0.07)}$ |
|  | Cover $\uparrow$ | $46.40_{( \pm 1.27)}$ | $47.65_{( \pm 0.35)}$ | $47.20_{( \pm 0.71)}$ | $46.65_{( \pm 1.48)}$ | $44.45_{( \pm 0.21)}$ | $46.50_{( \pm 0.28)}$ | $48.90_{( \pm 0.28)}$ | $46.55_{( \pm 0.64)}$ |
|  | $\mathrm{Hal} \downarrow$ | $31.10_{( \pm 0.64)}$ | $29.90_{( \pm 0.99)}$ | $27.50_{( \pm 0.71)}$ | $28.00_{( \pm 0.28)}$ | $31.00_{( \pm 2.83)}$ | $27.80_{( \pm 1.70)}$ | $26.00_{( \pm 0.28)}$ | $25.60_{( \pm 1.70)}$ |
|  | $\operatorname{Cog} \downarrow$ | $2.60_{( \pm 0.05)}$ | $2.20_{( \pm 0.14)}$ | $2.20_{( \pm 0.14)}$ | $2.55_{( \pm 0.35)}$ | $2.15_{( \pm 0.35)}$ | $1.95_{( \pm 0.35)}$ | $1.45_{( \pm 0.07)}$ | $2.00_{( \pm 0.04)}$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-08.jpg?height=186&width=49&top_left_y=559&top_left_x=388) | Acc. $\uparrow$ | $68.20_{( \pm 0.14)}$ | $69.65_{( \pm 0.35)}$ | $69.05_{( \pm 0.35)}$ | $72.60_{( \pm 0.42)}$ | $67.00_{( \pm 0.71)}$ | $67.30_{( \pm 1.41)}$ | $67.25_{( \pm 0.21)}$ | $70.70_{( \pm 0.57)}$ |
|  | Prec. $\uparrow$ | $79.00_{( \pm 0.14)}$ | $80.70_{( \pm 0.42)}$ | $79.70_{( \pm 0.28)}$ | $72.60_{( \pm 0.42)}$ | $85.45_{( \pm 0.49)}$ | $86.10_{( \pm 1.70)}$ | $86.50_{( \pm 0.57)}$ | $85.45_{( \pm 0.21)}$ |
|  | Rec. $\uparrow$ | $70.70_{( \pm 0.42)}$ | $71.60_{( \pm 0.42)}$ | $71.25_{( \pm 0.35)}$ | $76.10_{( \pm 0.05)}$ | $60.95_{( \pm 1.20)}$ | $60.55_{( \pm 1.34)}$ | $60.05_{( \pm 0.07)}$ | $67.55_{( \pm 0.92)}$ |
|  | $\mathrm{F} 1 \uparrow$ | $74.60_{( \pm 0.14)}$ | $75.90_{( \pm 0.42)}$ | $75.25_{( \pm 0.07)}$ | $78.60_{( \pm 0.28)}$ | $71.10_{( \pm 0.99)}$ | $71.10_{( \pm 1.56)}$ | $70.90_{( \pm 0.14)}$ | $75.45_{( \pm 0.64)}$ |
|  | MBER $\uparrow$ | $83.10_{( \pm 0.35)}$ | $84.15_{( \pm 0.05)}$ | $84.20_{( \pm 0.07)}$ | $85.95_{( \pm 0.05)}$ | $81.58_{( \pm 0.18)}$ | $82.20_{( \pm 0.99)}$ | $82.45_{( \pm 0.14)}$ | $84.60_{( \pm 0.35)}$ |

Table 3: AMBER [38] benchmark results. AVISC outperforms contrastive decoding baselines [11, 21 ] in both generative and discriminative tasks, achieving the highest AMBER score.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-08.jpg?height=347&width=572&top_left_y=935&top_left_x=424)

(a) InstructBLIP [9]

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-08.jpg?height=344&width=545&top_left_y=934&top_left_x=1126)

(b) LLaVA-1.5 [28]

Figure 6: Performance comparison on AMBER discriminative tasks. Our demonstrates superior performance overall, particularly excelling in the Existence and Action categories in both InstructBLIP [9] and LLaVA-1.5 [28].

MME-Fullset. Figure 5 compares the performance of various decoding methods on the MMEFullset [12] across 14 categories. AVISC generally outperforms other methods, achieving top performance in 7 categories for InstructBLIP and 11 categories for LLaVA 1.5. This demonstrates AVISC's effectiveness in enhancing understanding of visual information through attention calibration. However, both models see a decline in performance for the Count category with AVISC, and InstructBLIP shows poor OCR performance. Conversely, LLaVA 1.5 experiences significant OCR improvement with AVISC, indicating the method's variable impact across different models. Overall, AVISC provides consistent and superior results across most tasks compared to other methods.

AMBER. Table 3 presents the results of InstructBLIP [9] and LLaVA 1.5 [28] on the AMBER benchmark [38], which includes both generative tasks (detailed image descriptions) and discriminative tasks (answering questions about images). Both models show significant improvements in Accuracy and F1 scores in discriminative tasks using AVISC, outperforming the base, VCD [21], and M3ID [11] methods. In generative tasks, AVISC continues to exhibit substantial gains, indicating its effectiveness in generating detailed image descriptions. Notably, there is a marked improvement in the Existence metric, highlighting the method's accuracy in detecting objects. Overall, both models achieve the highest performance across most metrics with AVISC. AVISC stands out with the highest AMBER score, indicating its comprehensive superiority in both generative and discriminative tasks. Fig. 6 visualizes the performance of each decoding method across discriminative tasks in the AMBER benchmark.

### 4.3 Analysis

Qualitative comparison on LLaVA-Bench. Figure 7 shows responses from LLaVa-1.5 [28] given LLaVA-Bench [28] samples, highlighting the differences between sentences generated by standard decoding (Base), VCD [21], and those produced by AVISC. The results demonstrate the effectiveness of AVISC in dealing with a variety of challenging visual contexts. Base and VCD often generate descriptions that include errors or hallucinations where elements not present in the image are described.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-09.jpg?height=371&width=442&top_left_y=237&top_left_x=424)

Base: Initially, this image portrays three green fruits, possibly ripe figs with a white spet, harmbesly covering the inside of the fruit They nestle

![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-09.jpg?height=22&width=580&top_left_y=298&top_left_x=894)
an scene. The overall frame is filled with an inviting display of fresh fruits for those who appreciate or consume these delicious treats.

VCD: The image showcases a table with several fresh, ripe fruits cut in half. These fruits are not apples but rather a unique type of fruit that has a slightly purple tint, and their appearance is similar to grapefruits. The table displays four cut pieces of this fruit.

AvisC: The image features four plump purple fruit, possibly plums, Avisced on a wooden table. Three of the fruits are still whole, while the fourth fruit appears to be split in half, revealing its interior. The table provides an ideal surface for these fruits to be displayed on.
![](https://cdn.mathpix.com/cropped/2024_06_04_d0022623d9e50a39db64g-09.jpg?height=414&width=1200&top_left_y=699&top_left_x=514)

Figure 7: Response comparison on LLaVA-Bench [28]. Hallucinations are colored in red. AvISC demonstrates a robust understanding of images and reduces hallucinations in responses.

In contrast, AVISC helps counteract these hallucinations, generating sentences that reflect a more accurate comprehension of the image.


#### Abstract

Alternatives to zero-out. Table 4 presents the results of ablation experiments on various deactivation schemes for non-blind image tokens using InstructBLIP [9] and LLaVA 1.5 [28] models, evaluated on the POPE-COCO-random benchmark [24]. We compare Zeros, Ones, Noise, and Mask. For InstructBLIP, Mask achieves the highest Accuracy and F1 score, while Zeros excels in Precision. Ones shows the highest Recall, and Noise provides balanced performance with high Precision and Recall. For LLaVA 1.5, Noise achieves the highest Accuracy and Precision, while Zeros shows balanced performance across all metrics. On average, using Zeros was the most effective in improving model performance by calibrating attention to image tokens.


|  | Case | Acc. $\uparrow$ | Prec. $\uparrow$ | Rec. $\uparrow$ | F1 $\uparrow$ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Zeros | 88.50 | 93.00 | 83.27 | 87.86 |  |
| Ones | 82.50 | 75.48 | 96.27 | 84.62 |  |
| Norse | 86.77 | 84.71 | 89.73 | 87.15 |  |
| Mask | 88.53 | 90.14 | 86.53 | 88.30 |  |
| Z | Zeros | 87.87 | 88.12 | 87.53 | 87.83 |
| Ones | 79.97 | 72.22 | 97.40 | 82.94 |  |
| Noise | 88.47 | 93.19 | 83.00 | 87.80 |  |

Table 4: Design choices for non-blind image token deactivation.

# 5 Conclusion 

Our study demonstrates that addressing the issue of blind tokens in LVLMs significantly mitigates the problem of hallucinatory responses. By introducing a novel technique, termed AVISC, which recalibrates attention distribution during the decoding phase, without the need for extensive retraining, external data or models, or costly self-feedback mechanisms. our approach effectively enhances the model's focus on underemphasized yet crucial tokens. This recalibration leads to a more balanced consideration across all image tokens, significantly reducing reliance on misleading information and minimizing object hallucinations. The efficacy of AVISC is validated across several benchmarks, where it consistently outperforms existing decoding techniques. AVISC not only boosts reliability but also ensures more trustworthy applications of LVLMs in real-world scenarios requiring fine-grained visual understanding. We hope our contributions inspire further research into refining and adapting LVLMs for increasingly complex multimodal tasks.

## References

[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023.

[2] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor. Mocha: Multiobjective reinforcement mitigating caption hallucinations. arXiv preprint arXiv:2312.03631, 2023.

[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213-229. Springer, 2020.

[4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.

[5] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425, 2024.

[6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.

[7] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. Mitigating hallucination in visual language models with visual supervision. arXiv preprint arXiv:2311.16479, 2023.

[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality. See https://vicuna. Imsys. org (accessed 14 April 2023), 2(3):6, 2023.

[9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.

[10] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large visionlanguage models via clip-guided decoding. arXiv preprint arXiv:2402.15300, 2024.

[11] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. arXiv preprint arXiv:2403.14003, 2024.

[12] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, $\mathrm{Ke}$ Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2024.

[13] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. arXiv preprint arXiv:2308.06394, 2023.

[14] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18135-18143, 2024.

[15] Xin He, Longhui Wei, Lingxi Xie, and Qi Tian. Incorporating visual experts to resolve the information loss in multimodal large language models. arXiv preprint arXiv:2401.03105, 2024.

[16] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023.

[17] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709, 2019.

[18] Jitesh Jain, Jianwei Yang, and Humphrey Shi. Vcoder: Versatile vision encoders for multimodal large language models. arXiv preprint arXiv:2312.14233, 2023.

[19] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. arXiv preprint arXiv:2312.06968, 2023.

[20] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: mitigating multimodal hallucination through self-feedback guided revision. arXiv preprint arXiv:2311.07362, 2023.

[21] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922, 2023.

[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730-19742. PMLR, 2023.

[23] Wei Li, Zhen Huang, Houqiang Li, Le Lu, Yang Lu, Xinmei Tian, Xu Shen, and Jieping Ye. Visual evidence prompting mitigates hallucinations in multimodal large language models. 2023.

[24] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.

[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.

[26] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023.

[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.

[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.

[29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.

[30] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. Evaluation and enhancement of semantic grounding in large vision-language models. In AAAI-ReLM Workshop, 2024.

[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.

[32] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018.

[33] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. Aokvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146-162. Springer, 2022.

[34] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023.

[35] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.

[36] David Wan, Jaemin Cho, Elias Stengel-Eskin, and Mohit Bansal. Contrastive region guidance: Improving grounding in vision-language models without training. arXiv preprint arXiv:2403.02325, 2024.

[37] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al. Vigc: Visual instruction generation and correction. arXiv preprint arXiv:2308.12714, 2023.

[38] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023.

[39] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large visionlanguage models with instruction contrastive decoding. arXiv preprint arXiv:2403.18715, 2024.

[40] Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, and Tieniu Tan. Logical closed loop: Uncovering object hallucinations in large vision-language models. arXiv preprint arXiv:2402.11622, 2024.

[41] Dingchen Yang, Bowen Cao, Guang Chen, and Changjun Jiang. Pensieve: Retrospect-then-compare mitigates visual hallucination. arXiv preprint arXiv:2403.14401, 2024.

[42] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. arXiv preprint arXiv:2311.13614, 2023.

[43] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849, 2023.

[44] Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an eos decision perspective. arXiv preprint arXiv:2402.14545, 2024.

[45] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li. Halle-switch: Controlling object hallucination in large vision language models. arXiv e-prints, pages arXiv-2310, 2023.

[46] Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Debiasing large visual language models. arXiv preprint arXiv:2403.05262, 2024.

[47] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. Mitigating object hallucination in large vision-language models via classifier-free guidance. arXiv preprint arXiv:2402.08680, 2024.

[48] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023.

[49] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023.

[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.
