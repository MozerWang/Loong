# A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames 

Pinelopi Papalampidi* $\quad$ Skanda Koppula* $\quad$ Shreya Pathak<br>Justin Chiu Joe Heyward Viorica Patraucean Jiajun Shen Antoine Miech<br>Andrew Zisserman Aida Nematzdeh<br>Google DeepMind<br>\{pinelopi,skandak,shreyapa\}@google.com


#### Abstract

Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image-text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video-language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient imageto-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to $75 \%$ ) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to $1 B$ parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segmentbased information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema).


## 1. Introduction

Long-video understanding requires modeling of the temporal dynamics and long-range visual dependencies of realworld scenes $[67,68]$. However, capturing long-range visual content is challenging, even when assisted by large language models (LLMs). In this paper, we overcome demonstrate how to extend video encoders to directly process minutes-long visual content using language grounding and[^0]

Step 2: Video-to-Text Fine-tuning

![](https://cdn.mathpix.com/cropped/2024_05_26_26b35f959434b3cbacfbg-01.jpg?height=705&width=833&top_left_y=932&top_left_x=1058)

Figure 1. As in Flamingo [1], we first (1) pre-train a visual encoder via Noise Contrastive Estimation, and then (2) use this frozen encoder with a pre-trained LM for video-to-text generation (e.g., video summarization and Q/A). For (1), we propose a two-stage process for pre-training a video encoder: (a) image-to-short video adaptation, and (b) short-to-long video, where we adapt the encoder to longer contexts, using video masking and layer freezing.

overcome hardware memory limitations using simple, established techniques without additional architectural complexity $[27,68]$. We focus on long videos through the lens of language, assessing our models on the widely applicable tasks of visual summarization and question-answering.

Recent work on vision-language models have yielded impressive results $[1,33,76]$, predominantly focusing on understanding images or short clips of sixteen frames or less $[1,16,33,76,77]$. These works recycle strong pretrained image encoders, and usually perform late temporal fusion [1, 75, 77], and employ mostly-frozen, powerful LLMs. The lack of video-first encoders, equipped with
early temporal aggregation, may handicap the ability to process complex visual dependencies, and this is usually reflected in prior work's focus on short video benchmarks ( $<30$ seconds) in which sixteen random frames are sufficient for competitive performance $[5,32]$.

In this work, we follow the standard image-language recipe $[1,33]$, where we first contrastively pre-train a visual encoder (Step 1; Figure 1) and next plug the frozen pre-trained encoder into a pre-trained LM for tuning videoto-text adapter layers (Step 2; Figure 1). Given this demonstrably scalable and simple-to-tune baseline, we systematically explore video-first models. Through our analysis, we are able to scale video-first encoders in a memory-efficient manner to longer sequences of frames, up to 4.3 minutes of video at 1 FPS.

We first explore video-first models on short-video benchmarks (MSR-VTT [71], VATEX [64], YouCook2 [86], ActivityNet [31]) and compare against the SoTA VideoCoCa model [75]. We demonstrate that vanilla joint space-time attention (without factorization) significantly improves performance over frame-level encodings on benchmarks with rich temporal dependencies (YouCook2, VATEX), at the cost of decreased spatial capabilities due to noisy video-text alignments (e.g., objects mentioned in the text but shown in previous or next video segments, text not grounded on visual content) in the pre-training datasets. Overall, our models are able to reach VideoCoCa performance, while requiring fewer parameters and lower frame resolution.

This performance gain incurs extra compute and memory costs that grow quadratically with the video length. To address this, we provide one of the first systematic analyses of the memory/accuracy pareto-front of popular memory-efficient methods; this includes factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Through this analysis, we find that among all these options, simple token masking (up to $75 \%$ ) during contrastive pre-training incurs only a $1 \%$ Recall @ 1 drop on zero-shot text-video retrieval, and no drop in zero-shot video captioning. At the same time, such high masking offers 2-3x memory savings and allows us to generalize to longer video contexts. The alternatives we explore (e.g., efficient backbone architectures, more sophisticated TubeViT-style patchification [52]), do not maintain the same robustness against noisy video inputs and present a $25 \%$ relative decrease in performance for text-video retrieval on challenging benchmarks (YouCook2, VATEX). Finally, although parameterefficient methods $[24,25]$ fail to adapt image encoders to video-first models without suffering performance drops, we find that they can adapt video models trained on short contexts (e.g., 16 second videos) to longer temporal horizons.

Based on the above learnings, we extend our best performing short-video encoder to longer contexts of 256 frames (4.3 minutes at 1 FPS). We use the full-length videos of HowTo100M [45] accompanied by LLM-generated summaries based on the ASR to further contrastively train our LONGVIVIT while masking $75 \%$ of the input video tokens and freezing most parameters of the encoder. LoNGVIVIT-to-text ( $\sim 1 \mathrm{~B}$ parameters) is able to outperform modular methods that use LLM assistance and PALI3 [11] for frame captioning on temporally rich benchmarks (YouCook2, EgoSchema). Even modular methods that consider frame selection (SeViLA [78]) or an oracle segmentation of the video for localizing and captioning key events (on YouCook2) cannot reach LONGVIVIT's performance. An interesting byproduct of our work is that we can glean which video-language benchmarks have strong temporal dependencies, and thus are suitable for testing long video models; we find that papers often use benchmarks in which short video or even blind models perform well $[6,44,71]$. In short, we provide the following contributions:

- We explore the memory/accuracy pareto-frontier of video-first vision-language models, and systematically evaluate many architectural, data, and training alternatives. In the end, we identify a simple recipe that enables scaling to 4.3 minutes at 1 FPS, many times longer than comparable video-language models $[1,75]$.
- We identify short and long video benchmarks with substantial temporal dependencies, for which we demonstrate that the traditional image-first, late-temporal fusion recipe is convincingly weaker than a video-first approach.
- Finally, we compare our long video models to a variety of strong baselines and show competitive performance with far fewer parameters; this includes baselines that use LLM-based aggregation over visual captions, and we quantitatively evaluate this common approach for the first time on standard video benchmarks.


## 2. Related Work

We base our recipes on $[1,33]$, which provide a strong two-step video-language recipe that leverages the strength of pre-trained LLMs and works at scale. Similar work at smaller scale has additionally included captioning losses [36, 80], more contrastive losses [12, 41, 46, 70], masked autoencoding $[18,19,22,38,43,58]$, and combinations thereof [16, 21, 26, 57, 62, 65, 76, 77, 79, 87]. This work largely focuses on image-text modeling and extends to $<30$ seconds via image-to-video transfer, selective finetuning, or temporal fusion of frame encodings $[1,75,77]$.

A volume of work focuses on video-first learning. This includes some of the very early work in image-to-video kernel inflation [7, 56, 59], transformer-based video architectures $[2,4,40]$, image-to-video parameter-efficient adaption $[8,39,49]$, and multiple spatiotemporal resolutions along different network paths [17, 42, 72, 74]. These have still only been demonstrated on short videos, so other works
have broached the challenge of temporal scalability: [27, 54, $68]$ propose alternative encoders, and $[30,51,63]$ propose more exotic attention mechanisms. TubeViT [52] proposes multi-granularity patchification. We systematically dissect what works and scales among some of these alternatives, electing options that enable us to re-use strong pre-trained models and use standard, more easily-tuned architectures.

Specifically in video-to-text generation, approaches that handle longer videos are very limited and mostly target images or short videos [18, 34, 65]. A dominant approach is to summarize frames and aggregate information via LLMs [34, 37, 66, 81]. To the best of our knowledge, we are the first to attempt to train large-scale videoto-text models on longer sequences of frames and directly test them against LLM-assisted modular methods on challenging temporal benchmarks $[44,86]$.

## 3. The Video-to-Text Architecture

We base our approach on the successful two-step recipe that combines pre-trained vision and language models [e.g., 1, 33, 76, 77] as shown in Figure 1: (1) we first follow a two-stage regime for contrastively pre-training the vision encoder, and then (2) fuse the frozen vision representations into a pre-trained, frozen LM.

### 3.1. Video-Language Contrastive Pre-training

Following common practice $[1,33]$, we use a dual visionlanguage architecture with a Noise Contrastive Estimation (NCE) loss $[20,48,69]$ to pre-train our vision encoder, similar to CLIP [53], ALIGN [29] and VideoCLIP [70]. Both encoders are transformers [61]: a BERT-medium (77M) or base (117M) language encoder and ViT-Base (86M parameters) or Large (307M parameters) vision encoder. On the language side, caption representations are computed by averaging across the corresponding token representations. On the vision side, video frames are patchified into a sequence of visual tokens, fed into a vision encoder, and then spatiotemporally mean pooled to produce a final video representation.

Most prior larger-scale video-language models use pretrained image encoders and patchify frames individually via 2D convolutions [e.g., 1, 70, 75]. Instead, we create spatiotemporal tubelets via 3D convolutions as done in recent vision-only models $[2,52,58]$. Using 3D tubelets instead of flat patches has the dual advantage of higher input compression and more explicit temporal contextualization; our early experiments yielded improved performance. The tubelet embedding sequence is then flattened, added to learnable positional embeddings, and fed into the vision encoder. The vision encoder uses spatio-temporal attention as in ViViT [2]: Joint space-time attention, which does not add any new parameters to vanilla image ViT [15], facilitating transfer between image and video models. Such seamless transfer enables us to run a first stage of mixed image and short video pre-training, and a second stage of adapting the encoder to longer videos.

Training a large-scale transformer-based video encoder can be challenging because self-attention across thousands of visual tokens is both compute and memory intensive. Memory bottlenecks a model in two ways: (1) limiting the number of frames, and (2) limiting the contrastive batch size during training, negatively impacting performance. To address (2), we start with an image encoder that is pre-trained with large batch sizes, and further tune it to the video domain, instead of jointly training on images and videos from scratch. For initializing the 3D convolution, we repeat the pre-trained weights across the temporal dimension similarly to $[2,7]$ (see Appendix A). During video-language pretraining, we maintain different embedding paths for images vs. videos: images are embedded with the original 2D convolution and videos with a separate 3D convolution, with no weight sharing.

### 3.2. Video-to-Text Tuning

After pre-training the vision encoder, we add a pre-trained LM to generate video descriptions and summaries. We follow previous work [e.g., 1, 76, 77] by keeping the visual encoder frozen and plugging it into a frozen pre-trained LM. We first temporally mean pool the video representations to keep a fixed number of visual tokens independently of the number of frames; interestingly, we find that temporal pooling critically improves training stability for longer videos, while managing to still encode temporal information implicitly in the fixed length tokens (carried over from temporal position embeddings that are added to the video input). We next use a randomly initialized Perceiver resampler [28] to project the representations to the LM embedding space. We add new randomly initialized cross-attention layers at each layer of the LM to ground generation on the visual content. We train these new layers and the Perceiver resampler with a standard auto-regressive video captioning loss: $-\log p\left(w_{t} \mid w<t ; \mathcal{V}\right)$, where $w_{t}$ is its $t^{t h}$ token, and $\mathcal{V}$ is the video representation. We provide more details in Appendix A.

## 4. Memory-Efficient Encoder Design Space

Device memory is a key bottleneck for video training with joint space-time attention. To overcome this, we explore four broad categories of solutions: (1) efficient attention, (2) parameter-efficient image-to-video adaptation, (3) input token masking, and (4) multi-resolution patchification.

1. Attention mechanism. Factorized attention $[2,4]$ separates the temporal and spatial dimensions over which selfattention is applied, reducing both memory and computational costs. However, this modification introduces a new
temporal block within each transformer layer making initialization and model tuning more challenging. In contrast to [2], that initializes the new blocks with zeroes, we find that we achieve best performance when initializing the temporal blocks with the same self-attention weights of ViT. However, we add a gating mechanism which acts as a residual connection between the self-attention blocks: $h=h+\tanh (\alpha) h_{\text {temporal }}$. Here, $\alpha$ is a trainable parameter initialized to zero, that helps maintain the capabilities of the original ViT during training.
2. Parameter-efficient adaptation. We explore using parameter-efficient methods from NLP [9] to adapt image encoders to video, while only tuning a small percentage of model parameters. Most competing approaches adapt image-based models by freezing an image backbone and adding late, trainable temporal-fusion layers [12, 75, 83]. In contrast, we explore ways to use pre-trained image encoders and adapt them to video-first architectures [8, 39, 49]. Inspired by the success of parameter-efficient adaptation in NLP [84], we consider using MLP Adapters [24] (after the feedforward block) and LoRA [25] (at the self-attention and feed-forward blocks) at every transformer layer (details in Appendix A). Additionally, we explore tuning only temporal self-attention blocks [8], effectively as adapter layers, in factorized attention. In all variants, we still tune the 3D patch convolution which is new in the video domain.
3. Token masking. Most existing work samples videos at a fixed frames per second (FPS) rate [e.g., 1, 2, 58, 78]. However, semantics required for many video-language tasks vary slowly in the temporal dimension [85] and videos present high degree of redundancy between consecutive frames $[21,58]$. We explore ways to sparsely sample the video input to reduce the number of input visual tokens. Specifically, we test random masking of input tubelet embeddings. Since consecutive frames are largely redundant, the same semantic signals could potentially be extracted even with high masking rates. For example, [58] and [21] mask up to $95 \%$ of the input video to reach optimal performance on the task of video-masked autoencoding. We demonstrate similar results in a video-language setting.
4. Multi-resolution patchification. Finally, we test a simple approach to reduce redundancy in videos via more coarse-grained patchification in the temporal or spatial dimension, as commonly done in multiple-view video models [17, 42, 74]. However, this decreases frame resolution, and may lose fine-grained information. As a result, we also experiment with TubeViT [52] variant that combines flat patches and tubelets of different granularity to mitigate information loss. Following [52], we use four different con- volution kernels that can encode either coarse-grained temporal or spatial information; details are in Appendix A.

## 5. Datasets and Benchmarks

For contrastive pre-training, we use: (1) a set of $27 \mathrm{M}$ videotext pairs (VTP) as described in [1], (2) HowTo100M [45] (HT100M; 100M instructional YouTube clips automatically aligned with ASR using their timestamps, called HowTo100M Clips), and (3) VideoCC3M [47] (3M videotext pairs based on Conceptual Captions [55]). Unfortunately, we find the text-video alignment in VideoCC3M to be of poor quality; instead, we use a modified variant where we generate pseudo-labeled captions of every video using PALI [11] (see Appendices B and C). To pre-train with longer videos, we use a long version of HowTo100M (referred to as HowTo100M Summary) consisting of (1) the full-length videos with an average duration of $6.5 \mathrm{~min}-$ utes and (2) their corresponding textual summaries that we generate by automatically cleaning and summarizing the ASR transcripts using an LLM [23]. We also include the same mixture of image datasets as in [1]. For video-to-text tuning, we use the same mixture of datasets but exclude HowTo100M Clips, since the noisy video-text alignments hurt the performance.

We report text-video retrieval and captioning results on short video benchmarks, with average video length $\leq 30$ seconds: MSR-VTT [71], YouCook2 [86], ActivityNet Captions [31], and VATEX [64]. To evaluate performance on longer videos, we consider video summarization on fulllength versions of YouCook2 and ActivityNet Captions, with a video duration of up to 5 minutes; we also evaluate for multiple-choice video question answering (QA) on EgoSchema [44], a dataset constructed to test long-context temporal understanding.

## 6. Experimental Results

In Section 6.1, we describe our results evaluating alternatives in memory-efficient video encoder design; options described in Section 4. For this analysis, we use ViT-B/BERTmedium, with training details in Appendix B and ablations on experimental design in Appendix C.

In Section 6.2, we combine our most competitive design choices from 6.1 and test our models on short and long video understanding benchmarks. We scale our best model variants to ViT-L/BERT-base with a 400M (or 1B) language decoder. We test our short video models on text-video retrieval and video captioning, and our long video models on video summarization and QA on 256 -frame videos.

In Section 6.3, we share our experience working across short and long video benchmarks [6, 14, 44, 64, 71], offering insights about which ones yield robust temporal signal.
![](https://cdn.mathpix.com/cropped/2024_05_26_26b35f959434b3cbacfbg-05.jpg?height=452&width=1694&top_left_y=224&top_left_x=172)

Figure 2. Trade-offs between performance ( $\%$ text-to-video Recall @ 1; y axis) and train-time memory consumption (x axis) for different backbones (joint space-time (JST), factorized space-time (FST), and drame-level encodings) with random input masking ( $0 \%$ up to $75 \%$ ) or parameter-efficient methods for training (Adapters, LoRA, factorized temporal (FST) adaptation; lower opacity).

|  | MSR-VTT |  | VATEX |  | YC2 |  | AN |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | T2V | V2T | T2V | V2T | T2V | V2T | $\mathrm{T} 2 \mathrm{~V}$ | v |
|  |  | 38.1 | 23.8 | 26.3 | 12.3 | 13.6 | 6.7 |  |
|  |  | 36.9 | 25.3 |  | 11.6 | 12.7 | 6 |  |
|  | 39.3 | 34.8 | 24.8 | 25.0 | 9.1 | 7.9 | $\mathbf{0}$ |  |
| t-pool 1 | 38.4 | 37.5 | 21.9 | 26.1 | 9.0 | 8.9 | 6.1 |  |

Table 1. Text-video retrieval results ( $\%$ Recall@1) when considering different visual backbones.

### 6.1. Exploration of Memory-Efficient Designs

We explore memory-efficient methods to train video-first encoders as described in Section 4. We first consider short video inputs of 16 frames at 1 FPS and report peak traintime memory consumption vs. performance on text-video retrieval on short video benchmarks [5]. Then, we test whether our main findings hold for longer inputs (128+ frames) on video summarization on full-length YouCook2.

Base architectures. We explore the memory/accuracy trade-off of different visual backbones in Table 1: ViViT with joint space-time attention (i.e., Joint ST-ViViT), ViViT with factorized attention (i.e., Factorized ST-ViViT) [2], and frame-level (ViT-based) image encodings with average or attentional pooling ('att-pool') [1, 75]. Different methods perform similarly, especially on MSR-VTT and ActivityNet (AN). Interestingly, attentional pooling on top of framelevel encodings does not improve performance. ViViT with either joint or factorized attention performs best and presents higher gains for YouCook2 (YC2), the more temporally challenging benchmark [6.3]. In contrast to prior work $[e . g ., 12,75]$ which tests frozen image-to-video transfer and claims joint attention to be inferior, we find it to be competitive in this fully fine-tuned setting.

Architectures and token masking. We now test robustness of backbones when masking part of the input tubelets
(0-75\%). We report Recall @ 1 on text-to-video retrieval for YouCook2 and VATEX ${ }^{1}$ per backbone for different masking ratios in Figure 2. Joint space-time attention (JST) is robust against noise from masking up to $75 \%$ during pre-training. The same does not hold for frame-level encodings and factorized attention (FST), where performance drops consistently as we increase masking. We conclude that JST can better handle noisy inputs and use it in further exploration.

Parameter-efficient adaptation. We next report performance of parameter-efficient image-to-video adaptation in Figure 2. We consider (1) JST with (a) MLP Adapters at every layer of the encoder, (b) LoRA with rank decomposition matrices in the self-attention and feed-forward transformer blocks, and (2) factorized temporal adaptation where we tune the temporal self-attention. No adaptation method can reach the memory savings provided by high input masking, since we tune parameters depthwise and gradient computation still requires backpropagation through the model. At the same time, we see significant performance drop, suggesting that adaptation of spatial-only models to the temporal dimension cannot be sufficiently addressed in semifrozen fashion. Comparing parameter-efficient methods, we find MLP Adapters to be more competitive than LoRA, which is now canonical for LLMs. We hypothesize that LoRA is successful for tuning very small portions of the network and performing "easier" in-modality transfer.

Adaptation at scale. We next scale from ViT-B/86M to ViT-L/307M in Figure 4 and test whether observations hold with different model scales. We present the $\%$ memory increase from base to large (left bar set) and \% performance decrease of each method at each scale ${ }^{2}$. Joint ST exhibits a similar memory increase to frame-level, while leading to[^1]![](https://cdn.mathpix.com/cropped/2024_05_26_26b35f959434b3cbacfbg-06.jpg?height=312&width=1766&top_left_y=267&top_left_x=148)

Figure 3. Trade-offs between performance (text-to-video Recall @ 1; y axis) and memory consumption (x axis) for input sampling methods: (1) high input masking ratios ( $0 \%$ to $75 \%$ ) with joint space-time attention, (2) coarse-grained temporal (Coarse temp) and/or spatial (Coarse space) patchification with a fixed kernel and TubeViT which samples parts of the video with multiple 3D kernels of different granularity.

![](https://cdn.mathpix.com/cropped/2024_05_26_26b35f959434b3cbacfbg-06.jpg?height=548&width=837&top_left_y=778&top_left_x=164)

Figure 4. Difference (\%) in memory consumption for different model scales: (ViT-B vs ViT-L). We also report performance drop of efficient methods presented in Figure 2 in comparison with the vanilla approach (i.e., no input masking and full fine-tuning) at different model scales to test whether behavior is similar.

smaller accuracy drops, whereas factorized ST presents significant memory overhead with model scale due to the extra temporal self-attention blocks. For this reason, we exclude factorized ST from further experimentation. Finally, parameter-efficient methods are unable to achieve performance similar to their fully fine-tuned counterparts in both model scales, although their memory requirements scale better with model size.

Multi-resolution patchification. Given the outsized memory impact of input token count in Figure 3, we additionally analyze: (1) coarse-grained patchification in the temporal (convolution over 4 instead of 2 frames) and/or spatial (convolution over $32 \times 32$ instead of $16 \times 16$ pixel spaces) dimension, and (2) the TubeViT [52] approach of multiple tube kernels of different spatiotemporal size and strides. For all benchmarks, masking the input at high ratios while maintaining a fine granularity of tubelets decreases performance significantly less than other input processing methods. Temporal coarse-grained patchification negatively affects benchmarks with richer temporal dependencies (i.e., YouCook2, VATEX) more than spatial. The opposite trend holds for datasets depending on spatial understanding (i.e., MSR-VTT, ActivityNet Captions ${ }^{3}$ ). TubeViT acts as the middle ground between the two by employing multiple kernels, with some performance degradation across all benchmarks. However, it is not able to alleviate the negative effects caused by considering coarsergrained information and presents higher memory requirements due to the multiple convolutions. Overall, we find that high masking with Joint ST and small tubelets yields the strongest memory/performance curves.

Scaling to longer videos. We now test the best methods from Figure 3 on 128 input frames ( $32.7 \mathrm{k}$ visual tokens). We select methods that are within a memory budget (red vertical lines) and would fit on a 16GB device when expanded to long videos ( $128+$ frames). We contrastively finetune [3.1] our best performing video model (i.e., Joint ST referred to as SHORTVIVIT) on sequences of 128 frames on HowTo100M Summary [5], as detailed in Appendix B. We refer to this model as LONGVIVIT. Finally, we fine-tune LONGVIVIT for text generation (Section 3.2) on the fulllength YouCook2, and report Rouge-L in Figure 5, measuring memory consumption during both long-context contrastive ( $x$-axis) and video-to-text ( $y$-axis) tuning.

Validating our previous results, IMAGEVIT (frame-level encodings) trained on longer videos with $75 \%$ masking ${ }^{4}$ significantly under-performs video-first models ( $10 \mathrm{R}-\mathrm{L}$ drop). ShortViViT without further HT100M Summary training performs better than IMAGEVIT, but cannot match models adapted to longer videos. LONGVIVIT improves performance by 1.8 Rouge-L points over SHORTVIVIT. Comparing input masking with coarser-grained patchification ${ }^{5}$ provides similar insights to the previous paragraph.

Finally, we test MLP Adapters [24] for tuning SHORTVIVIT to longer videos and observe no performance drop[^2]

![](https://cdn.mathpix.com/cropped/2024_05_26_26b35f959434b3cbacfbg-07.jpg?height=374&width=1756&top_left_y=233&top_left_x=152)

Figure 5. Scaling memory-efficient methods to more frames (i.e., 128 frames) for ViViT-B and variants. We measure performance for video-to-text summarization on the full-length YouCook2 videos via Rouge-L (color-coded) while keeping track of memory consumption during short-to-long video contrastive tuning ( $x$-axis) and video-to-text tuning ( $y$-axis).

|  | MSR-VTT <br> Zero-shot |  | $\frac{\mathrm{FT}}{\mathrm{C} 1}$ | VATEX <br> ro-shot |  | $\frac{\mathrm{FT}}{\mathrm{Cl} 1}$ | YouCook2 <br> Zero-shot |  | FT <br> C1 | ActivityNet <br> Zero-shot |  | $\frac{\mathrm{FT}}{\mathrm{C} 1}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{T} 2 \mathrm{~V} / \mathrm{V} 2 \mathrm{~T}$ | $\mathrm{C} 1 / \mathrm{C} 2$ |  | $\mathrm{~T} 2 \mathrm{~V} / \mathrm{V} 2 \mathrm{~T}$ | $\mathrm{C} 1 / \mathrm{C} 2$ |  | $\mathrm{~T} 2 \mathrm{~V} / \mathrm{V} 2 \mathrm{~T}$ | $\mathrm{C} 1 / \mathrm{C} 2$ |  | T2V/V2T | $\mathrm{C} 1 / \mathrm{C} 2$ |  |
| IMAGEVIT-L | $30.9 / 41.6$ | 24.6/25.1 | 63.6 | 36.2/42.9 | 37.9/39.4 | $61.1 \quad-\quad$ | 18.2/16.8 | 14.5/16.5 | 95.9 | 20.6/18.2 | 16.3/17.7 | 41.1 |
| SHORTVIVIT-L | $31.9 / 38.9$ | $32.7 / 32.9$ | 63.1 | 37.8/42.8 | 43.6/43.0 | 67.5 | 20.4/20.5 | 21.0/22.1 | 131.9 | 21.3/18.9 | 25.2/26.1 | 44.8 |
| EffSHORTVIVIT-L | $29.9 / 38.3$ | 33.8/33.9 | 63.8 | 34.4/42.7 | 41.3/42.7 | 64.7 | $20.5 / 20.3$ | 21.1/21.7 | 127.1 | 20.1/17.7 | 27.0/26.5 | 41.1 |
| VideoCoCa-L [75] | 33.3/- | 24.3 | - | - | - | - | 18.9/- | 20.7 | - | $31.5 * /-$ | 17.4 | - |
| VideoCoCa-2.1B | $\underline{34.3 / 64.7}$ | 27.1 | 73.2 | $\underline{53.2 / 73.6}$ | 22.8 | 77.8 | 20.3/- | 34.3 | 128.0 | $34.5 * / 33.0^{*}$ | 19.3 | 39.3 |
| Flamingo-3B [1] | - | - | - | - | 40.1 | - | - | 55.8 | - | - | - | - |

Table 2. We present three model variants: ImaGEVit-L, that uses frame-level encodings with a late temporal fusion trained on images and videos, SHORTVIVIT-L, our best performing video-first model with joint space-time attention, and Efficient SHORTVIVIT-L (EffSHORTVIVIT-L) where we apply $75 \%$ train-time masking for $3 x$ memory savings. We also report performance for SoTA image-first models: VideoCoCa-L and Flamingo-3B, although they are bigger and not directly comparable. We report Recall @ 1 for zero-shot textto-video (T2V) and video-to-text (V2T) retrieval, and CIDEr for zero-shot and fine-tuned (FT) captioning when considering a 400M (C1) or 1B (C2) frozen LM for generation. ActivityNet retrieval results marked with * are not directly comparable, as these models uniformly sample frames, whereas we use the first frames of the long video with a fixed FPS of 1 to match experimental settings across benchmarks.

in comparison with full fine-tuning. This provides further evidence that parameter-efficient methods can be used for "easier transfers" but not temporal adaptation of spatialonly models. One downside of MLP Adapters is that it increases parameter count during video-to-text tuning ( $y$-axis in Figure 5). Thus, we also experiment with contrastively tuning only the last four layers of the model. With this, we observe a further $3 x$ decrease in memory, since we tune the network widthwise and excise early layer gradient computation. At the same time, there is no memory increase for video-to-text and no performance degradation. We conclude that this combination (high input masking and tuning the last layers) is an effective setting for longer video adaptation. Given the observed robustness to masking, to further decrease video-to-text memory, we also mask $30 \%$ of the input video during training and inference without observing any drop in summarization performance (see Appendix C).

### 6.2. Main Results

Short video benchmarks. We present our main results on short video benchmarks in Table 2. We use ViT-L with BERT-base for contrastive pre-training (Section 3.1) and a
400M frozen LM for video-to-text tuning (Section 3.2). Our entire video-to-text model accounts for $\sim 900 \mathrm{M}$ parameters, although we additionally test scaling the frozen LM to 1B parameters ( $\sim 1.5$ B total count). We report Recall@1 for zero-shot text-video retrieval and CIDEr for zero-shot and fine-tuned video captioning. We consider three model variants: frame-level encodings ImageViT, ShORTViViT, and ShortViVit with $75 \%$ masking that uses 2-3x less memory (referred to as Efficient SHORTVIVIT). We also report results for VideoCoCa [75] and Flamingo [1] ${ }^{6}$.

Our results remain consistent with our earlier observations. Contextualizing only intra-frame dependencies coupled with late temporal fusion (IMAGEVIT) leads to inferior performance for retrieval and captioning on benchmarks with richer temporal dependencies (YouCook2, VATEX) but performs better on retrieval on MSR-VTT which relies on spatial understanding. Video-first architectures further tuned on video datasets (substantially noisier than curated image ones) improve temporal capabilities at the expense[^3]of spatial. For Efficient SHORTVIViT, we find that masking $75 \%$ of the input video causes a performance drop: an average of $1 \%$ absolute difference on zero-shot retrieval and no significant difference on zero-shot captioning across all benchmarks. The efficient model still performs similarly or better than IMAGEVIT, especially on captioning and temporally rich benchmarks (e.g., YouCook2, VATEX), while consuming significantly less memory. Finally, when scaling the frozen LM component from $400 \mathrm{M}$ to $1 \mathrm{~B}(\mathrm{C} 1 \rightarrow \mathrm{C} 2)$ for zero-shot video-to-text generation, we observe moderate improvements across benchmarks and variants.

We compare our results against large image-based models with SoTA performance on video benchmarks (second block of Table 2). Although results are not directly comparable due to different experimental settings, we are competitive and achieve even better results for temporally rich benchmarks (i.e., YouCook2) on text-video retrieval for models of similar parameter count ${ }^{7}$. Moreover, our models significantly outperform VideoCoCa on most video captioning benchmarks even when considering their much larger versions in the zero-shot setting. Finally, when fine-tuning our video-to-text models with the 400M LM, we are again able to match and surpass the performance of the larger VideoCoCa-2.1B in two out of four benchmarks.

Long video understanding. We further tune LoNGVIVIT-L on 256 -frame HT100M Summary videos and evaluate zero-shot and fine-tuned summarization (YouCook2, ActivityNet) and QA (EgoSchema released subset); this is shown in Table 3. Although we focus on long videos at 1 FPS, we additionally report results on Perception Test [50] in Appendix D, where videos are short but contain rich temporal dependencies and can benefit from higher FPS.

We consider two families of models. 1. Models that take as input 256 frames (first block of Table 3): IMAGEVIT and SHORTVIVIT pre-trained on 16 -frame clips, and LoNGVIVIT further trained on 256 -frame clips. 2. Modular approaches from prior work (second block of Table 3): (a) SeViLA Localizer [78] for localizing important frames in the long video given a textual query which are then fed into SHORTVIVIT for performing the task ${ }^{8}$, and (b) the popular paradigm of captioning video segments or frames and using an LLM to aggregate information and form coherent summaries or answer questions $[34,37,81]$. We try this latter approach with our short video models (IMAGEVIT, SHORTVIVIT), generating captions over 16 -second video segments and then feeding the captions to the September[^4]

|  | Zero-shot |  |  |  | Fine-tuned |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | AN | YC2 | ES |  | AN | YC2 |
| Inference with 256 frames |  |  |  |  |  |  |
| IMAGEVIT | 14.4 | 4.6 | 40.8 |  | 23.8 | 29.4 |
| SHORTVIVIT | 15.4 | 7.0 | 47.9 | $\mathbf{2 4 . 3}$ | 29.5 |  |
| LONGVIVIT | 15.2 | $\mathbf{2 0 . 3}$ | $\mathbf{5 6 . 8}$ | 24.0 | $\mathbf{3 0 . 6}$ |  |
| Modular approaches with | 16-frame video | models |  |  |  |  |
| SeViLA-to-SHORTVIVIT | 16.2 | 4.2 | 49.6 | $\mathbf{2 4 . 4}$ | 28.3 |  |
| IMAGEVIT-to-Bard | 18.1 | 15.8 | 35.0 | 22.9 | 19.1 |  |
| $\quad$ + oracle segments | 16.3 | 16.2 | - | 22.7 | 22.1 |  |
| SHORTVIVIT-to-Bard | 19.3 | 18.1 | 42.0 | 22.7 | 20.8 |  |
| $\quad$ + oracle segments | 18.3 | 18.2 | - | 22.7 | 24.7 |  |
| PALI [11] 5B-to-Bard | $\mathbf{2 2 . 0}$ | 19.9 | 44.8 | - | - |  |
| Blind Bard | - | - | 27.0 | - | - |  |
| SoTA [73] | - | - | - | 36.9 | 34.6 |  |

Table 3. Results on long video-to-text benchmarks. We report Rouge-L for zero-shot and fine-tuned video summarization on ActivityNet Captions (AN) and YouCook2 (YC2) and zero-shot accuracy (\%) for multiple choice QA on EgoSchema (ES).

2023 release of Bard, a much larger LLM than the ones used in previous results. We caption clips using uniform video segmentation (every 16 seconds) or an oracle segmentation when available (i.e., we consider ground-truth start and end timestamps for different events within ActivityNet and YouCook2 videos). We also test substituting our small video models with PALI-3 (5B parameters) for frame captioning ${ }^{9}$. Finally, we reference the SoTA finetuned performance on ActivityNet and YouCook2, when using specialized models with pre-computed features by multiple networks, object detectors, and domain-specific vocabulary $[73]$.

Looking through Table 3, we find that on ActivityNet, which contains less temporal dependencies [6.3], modular approaches via frame selection or LLM-based aggregation of information (second block) perform well. Frame captioning via PALI combined with the power of LLMs is enough for the task in a zero-shot setting. For fine-tuned models, feeding either the long input or selected frames into SHORTVIVIT perform better than utilizing Bard. On ActivityNet, we see no benefit from training further on longer videos.

In contrast, we find that short video and modular models are insufficient for addressing video tasks with longer-range temporal dependencies (YouCook2, EgoSchema). Adapting SHORTVIVIT to longer contexts (LONGVIVIT) significantly improves performance and achieves the best scores across all comparison approaches. Using Bard as an information aggregator over individual clip captions cannot achieve competitive performance, even when considering an oracle video segmentation for YouCook2 (Lines 3 and 5 in the second block of Table 3). Surprisingly, even using a[^5]

![](https://cdn.mathpix.com/cropped/2024_05_26_26b35f959434b3cbacfbg-09.jpg?height=325&width=788&top_left_y=236&top_left_x=186)

Figure 6. Performance difference (\%) per benchmark when we remove (1) video or (2) image data from the training mixture.

much larger and more powerful image-based model (PALI) cannot reach LoNGVIVIT on YouCook2 and EgoSchema. Interestingly, selecting 16 key frames and feeding them into SHORTVIVIT also outperforms Bard-based methods on EgoSchema and fine-tuned YouCook2. This suggests there can be temporal dependencies in long videos that cannot be resolved even with an optimal event segmentation for the video, or be aggregated by LLMs given inprecise visual information. On such benchmarks, LoNGVIVIT demonstrates strong performance even without LLM assistance.

### 6.3. Brief Notes on Video Evaluations

We briefly describe some of our findings on video evaluations. Firstly, we find that blind Bard is able to achieve SoTA results on the full set of EgoSchema (no visual input; $33.9 \%$ accuracy vs. $32.1 \%$ for the best model in [44]). Adding visual information from PALI into Bard increases performance to just $39.2 \%$. However, on EgoSchema's released subset, performance of blind Bard is $27 \%$, which is much lower than PALI-to-Bard (44.8\%), suggesting that the subset contains questions that rely more on visual grounding than pure language reasoning, so we report numbers on the subset in Table 3 and on the full set in Appendix D.

Figure 6 details a simple ablation across other video benchmarks to quantify temporal richness. We test removing either video or image data from the training mix and measure the effect on performance (video-to-text Recall@1). We see a dramatic performance drop when removing video data for YouCook2 and VATEX (up to 75\%). ActivityNet and MSRVTT suffer more from the absence of image data, whereas non-video training influences performance in lesser degree (as little as $18 \%$ for MSR-VTT). We believe there's room for more fine-grained, temporalfocused video-language benchmarks in the community.

## 7. Conclusions

In short, we systematically analyze memory-efficient methods to scale video-first architectures to longer sequences of frames and demonstrate that just masking high percentages of the video $(\leq 75 \%)$ yields competitive results on long video-language tasks. Such masking shows a very small performance drop on short videos, provides 2-3x memory savings and allows scaling up to 4.3 minutes at 1 FPS (LONGVIVIT) when freezing part of the short video network in our two-stage training. LONGVIVIT outperforms modular approaches with LLM assistance on video summarization and QA on benchmarks with richer temporal dependencies (YouCook2, EgoSchema). We overall demonstrate that encoding longer-range visual dependencies can make a difference in downstream performance and corrects mistakes that LLMs are unable to rectify.

## Acknowledgements

We thank Chuhan Zhang for her thoughtful feedback on early drafts of the work; Chris Dyer, Joao Carreira, and Gabriel Brostow for their support, feedback, and guidance through the project; and Lucia Lopez for her valuable help in generating synthetic video captions.

## References

[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022. 1, 2, 3, 4, 5, 7, 14, 16, 17, 19

[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6836-6846, 2021. 2, 3, 4, 5, 14, 17

[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 17

[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 2, 3

[5] Shyamal Buch, Cristóbal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the" video" in video-language understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2917-2927, 2022. 2

[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961-970, 2015. 2, 4

[7] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299-6308, 2017. 2, 3,14

[8] Dongsheng Chen, Chaofan Tao, Lu Hou, Lifeng Shang, Xin Jiang, and Qun Liu. Litevl: Efficient video-language learning with enhanced spatial-temporal modeling. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7985-7997, 2022. 2, 4

[9] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning design spaces. arXiv preprint arXiv:2301.01821, 2023. 4

[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 17

[11] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al. Pali-3 vision language models: Smaller, faster, stronger. arXiv preprint arXiv:2310.09199, 2023. 2, 4, 8, 15, 19

[12] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas Bertasius. Vindlu: A recipe for effective video-and-language pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10739-10750, 2023. 2, 4, 5

[13] R Christoph and Feichtenhofer Axel Pinz. Spatiotemporal residual networks for video action recognition. Advances in neural information processing systems, 2, 2016. 14
[14] Pradipto Das, Chenliang Xu, Richard F Doell, and Jason J Corso. A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2634-2641, 2013. 4

[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \times 16$ words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 3

[16] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1, 2

[17] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202-6211, 2019. 2, 4

[18] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021. 2, 3, 17, 18

[19] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked visual modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2289822909, 2023. 2

[20] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 297-304. JMLR Workshop and Conference Proceedings, 2010. 3

[21] Tengda Han, Weidi Xie, and Andrew Zisserman. Turbo training with token dropout. arXiv preprint arXiv:2210.04889, 2022. 2, 4

[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll' ar, and Ross B Girshick. Masked autoencoders are scalable vision learners. 2022 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15979$15988,2021.2$

[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 4, 15

[24] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790-2799. PMLR, 2019. 2, 4, 6, 14

[25] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International Conference on Learning Representations, 2021. 2, 4, 14

[26] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023. 2

[27] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In European Conference on Computer Vision, pages 87-104. Springer, 2022. 1,3

[28] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651-4664. PMLR, 2021. 3

[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021. 3, 16, 17

[30] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. 3

[31] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706-715, 2017. 2, 4

[32] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022. 2

[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 2, 3

[34] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 3, 8

[35] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160, 2022. 17, 18

[36] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying videolanguage understanding as masked language modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23119-23129, 2023. 2

[37] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. $\mathrm{Mm}-\mathrm{vid}$ : Advancing video understanding with gpt-4v(ision), 2023. 3, 8

[38] Yuanze Lin, Chen Wei, Huiyu Wang, Alan Yuille, and Cihang Xie. Smaug: Sparse masked autoencoder for efficient video-language pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 2459-2469, 2023. 2

[39] Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong Wu, and Thomas H Li. Revisiting temporal modeling for clip-based image-to-video knowledge transferring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6555-6564, 2023. 2, 4

[40] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3202-3211, 2022. 2

[41] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293-304, 2022. 2, 17

[42] Chuofan Ma, Qiushan Guo, Yi Jiang, Ping Luo, Zehuan Yuan, and Xiaojuan Qi. Rethinking resolution in the context of efficient video recognition. Advances in Neural Information Processing Systems, 35:37865-37877, 2022. 2, 4

[43] Yue Ma, Tianyu Yang, Yin Shan, and Xiu Li. Simvtp: Simple video text pre-training with masked autoencoders. arXiv preprint arXiv:2212.03490, 2022. 2

[44] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. arXiv preprint arXiv:2308.09126, 2023. 2, 3, 4, 9, 16, 19

[45] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630-2640, 2019. 2, 4, 16

[46] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98799889, 2020. 2, 17

[47] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-video modalities from image captions. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIV, pages 407-426. Springer, 2022. 4, 16

[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 3

[49] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning. Advances in Neural Information Processing Systems, 35:26462-26477, 2022. 2, 4

[50] Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, et al. Perception test: A diagnostic benchmark for multimodal video models. arXiv preprint arXiv:2305.13786, 2023. 8,19

[51] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 3

[52] AJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Rethinking video vits: Sparse video tubes for joint image and video learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22142224, 2023. 2, 3, 4, 6, 14

[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 3, 17

[54] Michael S Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, and Anurag Arnab. Token turing machines. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19070-19081, 2023. 3

[55] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018. 4

[56] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. $A d$ vances in neural information processing systems, 27, 2014. 2

[57] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15638-15650, 2022. 2

[58] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems, 2022. 2, 3, 4

[59] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450-6459, 2018. 2

[60] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. arXiv preprint arXiv:2306.07915, 2023. 17

[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3

[62] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. Advances in neural information processing systems, 35:5696-5710, 2022. 2
[63] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 3

[64] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, highquality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581-4591, 2019. 2, 4

[65] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 2, 3, 16, 19

[66] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image descriptors are strong few-shot video-language learners. Advances in Neural Information Processing Systems, 35: 8483-8497, 2022. 3

[67] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1884-1894, 2021. 1

[68] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13587-13597, 2022. 1, 3

[69] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733-3742, 2018. 3

[70] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787-6800, 2021. 2, 3, 17

[71] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288-5296, 2016. 2, 4

[72] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036-5045, 2022. 2

[73] Kashu Yamazaki, Khoa Vo, Quang Sang Truong, Bhiksha Raj, and Ngan Le. Vltint: visual-linguistic transformer-intransformer for coherent video paragraph captioning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3081-3090, 2023. 8

[74] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview
transformers for video recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3333-3343, 2022. 2, 4, 17

[75] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint arXiv:2212.04979, 2022. 1, 2, 3, 4, 5, 7, 8, 15, 17, 18

[76] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 35:124-141, 2022. 1, 2, 3

[77] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 1, 2, 3

[78] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. arXiv preprint arXiv:2305.06988, 2023. 2, 4, 8, 19

[79] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. 2

[80] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34:23634-23651, 2021. $2,17,18$

[81] Andy Zeng, Maria Attarian, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, et al. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations, 2022. 3, 8

[82] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104-12113, 2022. 16

[83] Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Zhao Zhang, Peng Wang, Xiaohui Shen, and Jiashi Feng. Multimodal video adapter for parameter efficient video text retrieval. arXiv preprint arXiv:2301.07868, 2023. 4

[84] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient finetuning. arXiv preprint arXiv:2303.10512, 2023. 4

[85] Zhang Zhang and Dacheng Tao. Slow feature analysis for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 34(3):436-450, 2012. 4

[86] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 2, 3, 4

[87] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2
