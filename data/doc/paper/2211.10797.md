# An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation 

Yixuan Su ${ }^{\dagger}$<br>${ }^{\oplus}$ University of Cambridge $\quad{ }^{\rho}$ Independent Researcher<br>ys484@cam.ac.uk


#### Abstract

In the study, we empirically compare the two recently proposed decoding methods, i.e. Contrastive Search (CS) and Contrastive Decoding (CD), for open-ended text generation. The automatic evaluation results suggest that, while CS performs worse than CD on the MAUVE metric, it substantially surpasses CD on the diversity and coherence metrics. More notably, extensive human evaluations across three different domains demonstrate that human annotators are universally more in favor of CS over CD with substantial margins.

The contradicted results between MAUVE and human evaluations reveal that MAUVE does not accurately reflect human preferences. Therefore, we call upon the research community to develop better evaluation metrics for open-ended text generation. To ensure the reproducibility of our work, we have open-sourced all our code, evaluation results, as well as human annotations at https://github. com/yxuansu/Contrastive_Search_versus_Contrastive_Decoding.


## 1 Introduction

Open-ended text generation aims at generating coherent as well as informative text continuation based on the given prompt, and it is the core component in various NLP applications [9, 11]. In this study, we compare the two recently proposed decoding methods for open-ended text generation, i.e. (i) contrastive decoding (CD) [3] and (ii) contrastive search (CS) $[10,8]$.

For a comprehensive comparison, we follow Li et al. [3] and conduct experiments on three benchmarks across different domains. On the one hand, the automatic evaluations (\$3.1) indicate that CD performs notably better than CS on the MAUVE metric. However, CS achieves substantially better results on the diversity and coherence metrics. On the other hand, extensive human evaluations (\$3.2) on three benchmarks validate that the human annotators are universally more in favor of the texts produced by CS than the ones produced by CD with substantial margins.

Given the contradicted results of MAUVE and human evaluations, we argue that MAUVE does not accurately reflect human preferences. In $\S 4$, we show that the human preferences better correlate with the balance between the diversity and the coherence aspects of the generated texts. Thereby, we suggest future research on better evaluation metrics for open-ended text generation to take into account these two aspects.

In summary, our contributions are:

- We conduct comprehensive experiments to compare the two recently proposed decoding methods, i.e. CD and CS, for open-ended text generation.
- We demonstrate that MAUVE does not accurately reflect the human preferences on different methods for open-ended text generation. Moreover, we suggest a plausible direction for future research on better evaluation metrics of open-ended text generation.


## 2 Preliminaries

### 2.1 Contrastive Decoding

Contrastive decoding (CD) is introduced by Li et al. [3]. Given a prompt text $\boldsymbol{x}_{<t}$, the selection of the output token $x_{t}$ is decided by comparing two separate language models (LM) as

$$
\begin{equation*}
x_{t}=\underset{\left.v \in \mathcal{V}_{\text {head }} \boldsymbol{x}_{<t}\right)}{\arg \max }\left\{\log p_{\mathrm{EXP}}\left(v \mid \boldsymbol{x}_{<t}\right)-\log p_{\mathrm{AMA}}\left(v \mid \boldsymbol{x}_{<t}, \tau\right)\right\} \tag{1}
\end{equation*}
$$

where $p_{\mathrm{EXP}}\left(\cdot \mid \boldsymbol{x}_{<t}\right)$ is the probability distribution produced by an expert LM. The $p_{\mathrm{AMA}}\left(\cdot \mid \boldsymbol{x}_{<t}, \tau\right)$ is the probability distribution produced by an amateur LM scaled with a predefined temperature $\tau$. Typically, the expert LM (e.g. GPT2-XL) is larger than the amateur LM (e.g. GPT2-Small). The candidate set $\mathcal{V}_{\text {head }}\left(\boldsymbol{x}_{<t}\right)$ is defined as

$$
\begin{equation*}
\mathcal{V}_{\text {head }}\left(\boldsymbol{x}_{<t}\right)=\left\{v \in \mathcal{V}: p_{\operatorname{EXP}}\left(v \mid \boldsymbol{x}_{<t}\right) \geq \alpha \times \max _{w} p_{\mathrm{EXP}}\left(w \mid \boldsymbol{x}_{<t}\right)\right\} \tag{2}
\end{equation*}
$$

where $\alpha$ is a hyperparameter.

### 2.2 Contrastive Search

In contrast to CD, contrastive search (CS) $[10,8]$ only requires a single LM to generate the text continuation conditioned on the prompt. Formally, given the prompt text $\boldsymbol{x}_{<t}$, the selection of the output token $x_{t}$ follows

$$
\begin{equation*}
x_{t}=\underset{v \in V^{(k)}}{\arg \max }\{(1-\alpha) \times \underbrace{p_{\theta}\left(v \mid \boldsymbol{x}_{<t}\right)}_{\text {model confidence }}-\alpha \times \underbrace{\left(\max \left\{s\left(h_{v}, h_{x_{j}}\right): 1 \leq j \leq t-1\right\}\right)}_{\text {degeneration penalty }}\} \tag{3}
\end{equation*}
$$

where $V^{(k)}$ is the set of top- $k$ predictions from the LM's probability distribution $p_{\theta}\left(\cdot \mid \boldsymbol{x}_{<t}\right)$. In Eq. (3), the first term, model confidence, is the probability of the candidate $v$ predicted by the LM. The second term, degeneration penalty, measures how discriminative of the candidate $v$ with respect to the previous context $\boldsymbol{x}_{<t}$ and $s(\cdot, \cdot)$ computes the cosine similarity between token representations. More specifically, degeneration penalty is defined as the maximum cosine similarity between the representation of the candidate $v$ and that of all tokens in $\boldsymbol{x}_{<t}$. Here, the candidate representation $h_{v}$ is computed by the LM given the concatenation of $\boldsymbol{x}_{<t}$ and $v$. Intuitively, a larger degeneration penalty of $v$ means it is more similar to the context, therefore more likely leading to the undesirable repetitions in the generated output. The hyperparameter $\alpha \in[0,1]$ regulates the importance of these two components.

## 3 Experiment

Evaluation Benchmarks. Following Li et al. [3], we conduct experiments on three benchmarks from different domains, including (i) articles from Wikinews ${ }^{1}$ in the news domain; (ii) Wikitext-103 dataset [5] from the Wikipedia domain; (iii) and BookCorpus [13] from the story domain.

Same as in Li et al. [3], the generation of LM is conditioned on the test prompts with a fixed length of 32. And the generation of the text ends upon reaching an end-of-document token or a maximum length of 256 tokens. To ensure our experiments are aligned with Li et al. [3], we directly use the data provided in the authors' released repository ${ }^{2}$.

Model and Baselines. We compare different decoding methods using the GPT2-XL model [7]. (i) Following Li et al. [3], in contrastive decoding (CD), the expert and amateur LM are set as GPT2-XL[^0]and GPT2-Small, respectively; And the $\alpha$ (see Eq. (2)) and $\tau$ (see Eq. (1)) for CD are set as 0.1 and 0.5 , respectively. (ii) For contrastive search (CS), we set $\alpha$ (see Eq. (3)) as a constant 0.6 ; And $k$ (see Eq. (3)) for news, Wikipedia, and story benchmarks is set as 5,5 , and 6 , respectively.

In addition to $\mathrm{CD}$ and $\mathrm{CS}$, in the experiments, we also report the results of other baseline methods, including (i) greedy search; (ii) top- $k$ sampling $(k=50)$ [1]; (iii) nucleus sampling $(p=0.95)$ [2]; and (iv) typical sampling $(\tau=0.95)[4]$.

Note that, for a fair comparison with Li et al. [3], we report the performance of the baseline methods (i.e. greedy search, top- $k$ sampling, nucleus sampling, typical sampling, and contrastive decoding (CD)) using the generated texts provided in the authors' released repository ${ }^{3}$. However, for contrastive search (CS), the reported numbers in Li et al. [3] are different from our reproduced numbers. Therefore, we re-implement the results of CS using the same benchmark data provided by Li et al. [3] in their official repository ${ }^{4}$.

### 3.1 Automatic Evaluation

Following previous studies $[10,8,3]$, we use the following metrics for automatic evaluation.

(i) Diversity takes into account the generation repetition at different $n$-gram levels and it is defined as: diversity $=\prod_{n=2}^{4}\left(1.0-\frac{\text { rep-n }}{100}\right)$, where rep-n $=100 \times\left(1.0-\frac{\mid \text { unique n-grams }(\hat{\boldsymbol{x}}) \mid}{\mid \operatorname{total} \text { n-grams }(\hat{\boldsymbol{x}}) \mid}\right)$ and $\hat{\boldsymbol{x}}$ is the text generated by the LM.

(ii) MAUVE [6] is designed for measuring the token distribution closeness between the generated text and the human-written text over the whole test set. Note that, while the maximum length of generation in the experiments is 256 , we follow $\mathrm{Li}$ et al. [3] and measure the MAUVE score by truncating the generated text to its first 128 tokens.

(iii) Coherence is recently introduced by $\mathrm{Su}$ and Collier [8] and it automatically measures the semantic coherence between the prompt and the generated text. Formally, given the prompt $\boldsymbol{x}$ and the generated text $\hat{\boldsymbol{x}}$, coherence is defined as the averaged log-likelihood of $\hat{\boldsymbol{x}}$ conditioned on $\boldsymbol{x}$ as

$$
\begin{equation*}
\operatorname{coherence}(\hat{\boldsymbol{x}}, \boldsymbol{x})=\frac{1}{|\hat{\boldsymbol{x}}|} \sum_{i=1}^{|\hat{\boldsymbol{x}}|} \log p_{\mathcal{M}}\left(\hat{\boldsymbol{x}}_{i} \mid\left[\boldsymbol{x}: \hat{\boldsymbol{x}}_{<i}\right]\right) \tag{4}
\end{equation*}
$$

where [:] is the concatenation operation and $\mathcal{M}$ is a massively pre-trained LM. In our experiments, we follow Su and Collier [8] and set $\mathcal{M}$ as the OPT-2.7B model [12].

| Method | Wikinews |  |  | Wikitext |  |  | Story |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | div.(\%) | MAUVE(\%) | coh. | $\operatorname{div} .(\%)$ | MAUVE(\%) | coh. | div.(\%) | $\operatorname{MAUVE}(\%)$ | coh. |
| Greedy Search ${ }^{*}$ | 3.55 | 13.96 | -0.47 | 1.77 | 4.91 | -0.41 | 0.86 | 2.65 | -0.34 |
| Top- $k$ Sampling ${ }^{*}$ | 91.56 | 89.86 | -2.22 | 87.49 | 81.00 | -2.37 | 91.22 | 87.49 | -2.45 |
| Nucleus Sampling* | 93.54 | 89.45 | -2.61 | 92.16 | 86.54 | -3.03 | 94.50 | 91.47 | -3.02 |
| Typical Sampling* | 95.37 | 90.97 | -3.26 | 94.82 | 86.07 | -3.71 | 96.29 | 88.58 | -3.68 |
| Contrastive Decoding ${ }^{*}$ | 91.57 | 92.20 | -2.16 | 88.02 | 91.46 | -2.19 | 86.41 | 93.17 | -2.09 |
| Contrastive Search | 93.72 | 84.14 | -1.39 | 89.35 | 77.97 | -1.56 | 93.06 | 84.74 | -1.61 |

Table 1: Automatic evaluation results, where div. and coh. denote diversity and coherence. The numbers marked with * are obtained using the generated texts originally released by Li et al. [3].

Evaluation Results. Table 1 presents the automatic evaluation results. On the one hand, we see that CD achieves the best MAUVE score on all evaluated benchmarks. On the other hand, CS yields competitive performances on the diversity metric and achieves substantially better results on the coherence metric than $\mathrm{CD}$ and other sampling methods.[^1]

### 3.2 Human Evaluation

To further compare contrastive decoding (CD) with contrastive search (CS), we conduct a human evaluation with 4 native-speaker graders from a third-party grading platform. We randomly select 150 test prompts from the benchmarks across different domains, and evaluate CD and CS through pairwise comparison. Specifically, for each test prompt, the annotators are given two texts, with random order, that are generated by $\mathrm{CD}$ and $\mathrm{CS}$. The annotators then decide which one is more likely written by humans considering the following aspects of the generated text:

- Coherence: Whether the generated text is semantically coherent.
- Fluency: Whether the generated text is fluent and easy to understand.
- Informativeness: Whether the generated text is diverse and contains interesting content.

| 领 | Method A is better |  | Neutral <br> $4.2 \%$ <br> $15.1 \%$ <br> $20 \%$ | Method B is better |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  | Nucleus Sampling* <br> Typical Sampling* <br> Contrastive Search | $25.0 \%$ <br> $7.8 \%$ <br> $\mathbf{6 8 . 5} \%^{\dagger}$ |  | $\mathbf{7 0 . 8} \%^{\dagger}$ <br> $\mathbf{7 7 . 1} \%^{\dagger}$ <br> $29.5 \%$ | Contrastive Decoding <br> Contrastive Decoding <br> Contrastive Decoding |
| 类 | Method A is better |  | Neutral | Method B is better |  |
|  | Nucleus Sampling* <br> Typical Sampling* <br> Contrastive Search | $20.2 \%$ <br> $6.7 \%$ <br> $\mathbf{6 5 . 0} \%^{\dagger}$ | $8.3 \%$ <br> $4.6 \%$ <br> $2.0 \%$ | $\mathbf{7 1 . 4} \%^{\dagger}$ <br> $\mathbf{8 8 . 7} \%^{\dagger}$ <br> $33.0 \%$ | Contrastive Decoding <br> Contrastive Decoding <br> Contrastive Decoding |
| $\stackrel{d}{d}$ | Method A is better |  | Neutral | Method B is better |  |
|  | Nucleus Sampling* <br> Typical Sampling* <br> Contrastive Search | $31.8 \%$ <br> $23.8 \%$ <br> $\mathbf{6 7 . 0} \%^{\dagger}$ | $4.5 \%$ <br> $25.6 \%$ <br> $1.0 \%$ | $\mathbf{6 3 . 6} \%^{\dagger}$ <br> $\mathbf{5 0 . 6} \%^{\dagger}$ <br> $32.0 \%$ | Contrastive Decoding <br> Contrastive Decoding <br> Contrastive Decoding |

Table 2: Human evaluation results. ${ }^{\dagger}$ means one method performs significantly better than the other as judged by Sign Test with $p$-value $<0.05$. * the pairwise evaluation results between (i) nucleus sampling and contrastive decoding as well as (ii) typical sampling and contrastive decoding are directly cited from Li et al. [3].

Table 2 presents the human evaluation results which validate that contrastive search (CS) significantly outperforms contrastive decoding (CD) and other sampling methods ${ }^{5}$ in all evaluated benchmarks from different domains. These results clearly demonstrate the superiority of contrastive search over other existing decoding strategies.

It is worth emphasizing that, as shown in Table 1, contrastive search yields notably lower MAUVE scores than CD and other sampling methods. Given this clear contradiction between MAUVE and human evaluations, we argue that MAUVE does not accurately reflect human preferences. Therefore, we call upon the research community to develop better evaluation metrics, for open-ended text generation, that more correlates with human judgements.

### 3.3 Case Study

Table 3 presents a qualitative example, from the news domain, comparing contrastive decoding and contrastive search. We see that the text generated by contrastive decoding contains excessive repetitions both on the lexical and phrasal levels, e.g. "The Pentagon", "The drones would likely", and etc. In contrast, the text generated by contrastive search is semantically coherent as well as grammatically fluent. It elaborates on the reasons of the military strike and provides diverse details of the incident. In Appendix A, we provide more qualitative examples for the comparison between these two decoding methods.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_291f869615666516f328g-5.jpg?height=1821&width=1391&top_left_y=236&top_left_x=367)

Figure 1: Wikitext - Coherence versus MAUVE. Figure 2: Wikitext - Coherence versus Diversity.

## 4 Further Analysis

In this section, we provide in-depth comparison between contrastive search and other decoding methods. Specifically, we vary the $k$ (see Eq. (3)), from 2 to 10 , in contrastive search ${ }^{6}$ to generate texts using the benchmark from the Wikipedia domain. The generated texts are evaluated from three aspects, i.e. (i) coherence; (ii) diversity; and (iii) MAUVE, which are described in §3.1.[^3]

The evaluated results are presented in Figure 1 and Figure 2, respectively. ${ }^{7}$ On the one hand, Figure 1 indicates that the MAUVE score of contrastive search lags behind other decoding methods (except for greedy search) with clear margins, which obviously contradicts to the human judgements as presented in §3.2. Even by jointly considering the coherence and MAUVE metrics, it is hard to identify the better decoding method. On the other hand, from Figure 2, we see that contrastive search notably outperforms other methods on the balance between the coherence and diversity metrics, better correlating with human judgements.

Our results demonstrate that MAUVE does not accurately reflect human preferences on different methods. Moreover, we suggest future research on better evaluation metrics, for open-ended text generation, to take into account both the coherence and the diversity aspects of the generated text.

## 5 Conclusion

In this work, we empirically compare the two recently proposed decoding methods, i.e. contrastive decoding (CD) and contrastive search (CS). We conduct extensive experiments on three benchmarks from different domains. The automatic evaluation results suggest that $\mathrm{CD}$ achieves better results on MAUVE while CS performs better on diversity and coherence. Moreover, through extensive human evaluations, we show that the human annotators are universally more in favor of CS over CD with substantial margins. Given the contradicted results between MAUVE and human evaluations, we provide in-depth analysis which reveals that the balance between the diversity and coherence metrics better correlates with human judgements. Our observation provides a plausible path for future research on better evaluation metrics for open-ended text generation.

## References

[1] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018.

[2] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.

[3] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022.

[4] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural language generation. arXiv preprint arXiv:2202.00666, 2022.

[5] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[6] Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:4816-4828, 2021.

[7] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[8] Yixuan Su and Nigel Collier. Contrastive search is what you need for neural text generation. arXiv preprint arXiv:2210.14140, 2022.

[9] Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier. Language models can see: Plugging visual controls in text generation. arXiv preprint arXiv:2205.02655, 2022.

[10] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive framework for neural text generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022 .[^4]

[11] Yixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna Korhonen, and Nigel Collier. Prototypeto-style: Dialogue generation with style-aware editing on retrieval memory. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:2152-2161, 2021.

[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[13] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages $19-27,2015$.
