# Minimax Optimal Submodular Optimization with Bandit Feedback 

Artin Tajdini, Lalit Jain, Kevin Jamieson<br>University of Washington, Seattle, WA<br>\{artin, jamieson\}@cs.washington.edu, lalitj@uw.edu


#### Abstract

We consider maximizing a monotonic, submodular set function $f: 2^{[n]} \rightarrow[0,1]$ under stochastic bandit feedback. Specifically, $f$ is unknown to the learner but at each time $t=1, \ldots, T$ the learner chooses a set $S_{t} \subset[n]$ with $\left|S_{t}\right| \leq k$ and receives reward $f\left(S_{t}\right)+\eta_{t}$ where $\eta_{t}$ is mean-zero sub-Gaussian noise. The objective is to minimize the learner's regret over $T$ times with respect to $\left(\mathbf{1}-e^{-1}\right)$-approximation of maximum $f\left(S_{*}\right)$ with $\left|S_{*}\right|=k$, obtained through greedy maximization of $f$. To date, the best regret bound in the literature scales as $k n^{1 / 3} T^{2 / 3}$. And by trivially treating every set as a unique arm one deduces that $\sqrt{\binom{n}{k} T}$ is also achievable. In this work, we establish the first minimax lower bound for this setting that scales like $\mathcal{O}\left(\min _{i \leq k}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)\right)$. Moreover, we propose an algorithm that is capable of matching the lower bound regret.


## 1 INTRODUCTION

Optimizing over sets of $n$ ground items given noisy feedback is a common problem. For example, when a patient comes into the hospital with sepsis (bacterial infection of the blood), it is common for a cocktail of $1<k \leq n$ antibiotics to be prescribed. This can be attractive for reasons including 1) the set could be as effective (or more) than a single drug alone, but each unit of the cocktail could be administered at a far lower dosage to avoid toxicity, or 2) could be more robust to resistance by blocking a number of different pathways that would have to be overcome simultaneously, or 3) could cover a larger set of pathogens present in the population. In this setting the prescriber wants to balance exploration with exploitation over different subsets to maximize the number of patients that survive. As a second example, we consider factorial optimization of web-layouts: you have $n$ pieces of content and $k$ locations on the webpage to place them-how do you choose subsets to maximize metrics like click-through rate or engagement?

Given there are $\approx n^{k}$ ways to choose $k$ items amongst a set of $n$, this optimization problem is daunting. It is further complicated by the fact that for any set $S_{t}$ that we evaluate at time $t$, we only get to observe a noisy realization of $f$, namely $y_{t}=f\left(S_{t}\right)+\eta_{t}$ where $\eta_{t}$ is mean-zero, sub-Gaussian noise. In the antibiotics case, this could be a Bernoulli indicating whether the patient recovered or not, and in the web-layout case this could be a Bernoulli indicating a click or a (clipped) real number to represent the engagement time on the website. To make this problem more tractable, practitioners make structural assumptions about $f$. A common assumption is to assume that higher-order interaction terms are negligible Hill et al. [2017, Chen et al. [2021. For example, assuming only interactions up to the second degree would mean that there exist parameters $\theta^{(0)} \in \mathbb{R}$, $\theta^{(1)} \in \mathbb{R}^{n}$, and $\theta^{(2)} \in \mathbb{R}^{\binom{n}{2}}$ such that

$$
\begin{equation*}
f(S)=\theta^{(0)}+\sum_{i \in S} \theta_{i}^{(1)}+\sum_{i, j \in S, i \neq j} \theta_{i, j}^{(2)} \tag{1}
\end{equation*}
$$

However, this model can be very restrictive and even if true, the number of unknowns scales like $n^{2}$ which could still be intractably large.

An alternative strategy is to remain within a non-parametric class, but reduce our ambitions to measuring performance relative to a different benchmark which is easier to optimize. We say a set function $f: 2^{[n]} \rightarrow \mathbb{R}$ is increasing and submodular if for all $A \subset B \subset[n]$ we have $f(A) \leq f(B)$ and

$$
\begin{equation*}
f(A \cup B)+f(A \cap B) \leq f(A)+f(B) \tag{2}
\end{equation*}
$$

Such a condition limits how quickly $f$ can grow and captures some notion of diminishing returns. Diminishing returns is reasonable in both the antibiotics and webpage optimization examples. It is instructive to note that a sufficient condition for the parametric form of $\sqrt{1}$ ) to be submodular is for $\max _{i, j} \theta_{i, j}^{(2)} \leq 0$. But in general, $f$ still has $\approx n^{k}$ degrees of freedom even if it is monotonic and submodular. And it is known that for an unknown $f$, identifying $S^{*}:=\arg \max _{S \subset[n]:|S|=k} f(S)$ may require evaluating $f$ as many as $n^{k}$ times.

The power of submodularity is made apparent through the famous result of Nemhauser and Wolsey [1978] which showed that the greedy algorithm which grows a set one item at a time by adding the item with the highest marginal gain returns a solution that is within a $1-e^{-1}$-multiplicative factor of the optimal solution. That is, if we begin with $S_{g r}^{f}=\emptyset$ and set $S_{g r}^{f} \leftarrow \arg \max _{i \in[n] \backslash S_{g r}^{f}} f\left(S_{g r}^{f} \cup\{i\}\right)$ until $\left|S_{g r}^{f}\right|=k$, then $f\left(S_{g r}^{f}\right) \geq(1-1 / e) f\left(S_{*}^{f}\right)$ where $S_{*}^{f}:=\arg \max _{S \in[n]:|S| \leq k} f(S)$ if $f$ is increasing and submodular. This result is complemented by Feige 1998 which showed that achieving any $1-e^{-1}+\epsilon$-approximation is NP-Hard. Under additional assumptions like curvature, this guarantee can be strengthened.

Due to the centrality of the greedily constructed set to the optimization of a submodular function, it is natural to define a performance measure relative to the greedily constructed set. However, as discussed at length in the next section, because we only observe noisy observations of the underlying function, recovering the set constructed greedily from noiseless evaluations is too much to hope for. Consequently, there is a more natural notion of regret, denoted $R_{\mathrm{gr}}$, that actually appears in the proofs of all upper bounds found in the literature for this setting (see the next section for a definition). For this notion of regret, previous works have demonstrated that a regret bound of $R_{\mathrm{gr}}=O\left(\operatorname{poly}(k) n^{1 / 3} T^{2 / 3}\right)$ is achievable. This $T^{2 / 3}$ rate is unusual in multi-armed bandits, where frequently we expect a regret bound to scale as $T^{1 / 2}$. On the other hand, by treating each $k$-subset as a separate arm, one can easily adapt existing algorithms to achieve a regret bound of $\sqrt{\binom{n}{k} T}$. This leads to the following question:

Does there exist an algorithm that obtains $\sqrt{n^{r} T}$ regret for $r=o(k)$ on every instance? And if not, what is the optimal dependence on $k$ and $n$ for a bound scaling like $T^{2 / 3}$ ?

To address these questions, we prove a minimax lower bound and complement the result with an algorithm achieving a matching upper bound. To be precise, the contributions of this paper include: - A minimax lower bound demonstrating that $R_{\mathrm{gr}} \geq \min _{0 \leq i \leq k}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)$. In words, for small $T$ a $T^{2 / 3}$ regret bound is inevitable, for large $T$ the $\sqrt{\binom{n}{k} T}$ bound is optimal, with an interpolating regret bound for in between.

- We propose an algorithm that for any increasing, submodular $f$, we have $R_{\mathrm{gr}} \leq \min _{0 \leq i \leq k}\left(i n^{1 / 3} T^{2 / 3}+\right.$ $\left.\sqrt{n^{k-i} T}\right)$. As this matches our lower bound, we conclude that this is the first provably tight algorithm for optimizing increasing, submodular functions with bandit feedback.

In what remains, we will formally define the problem, discuss the related work, and then move on to the statement of the main theoretical results. Experiments and conclusions follow.

### 1.1 Problem Statement

Let $[n]=\{1, \ldots, n\}$ denote the set of base arms, $T$ be the time horizon, and $k$ be a given cardinality constraint. At time $t$, the agent selects a set $S_{t} \subset[n]$ where $\left|S_{t}\right| \leq k$, and observes reward $f\left(S_{t}\right)+\eta_{t}$ where $\eta_{t}$ is i.i.d. mean-zero 1-sub-Gaussian noise, and $f: 2^{[n]} \rightarrow[0,1]$ is an unknown monotone non-decreasing submodular function defined for all sets of cardinality at most $k$.

Ideally, our goal would be to minimize the regret relative to pulling the best set $S^{*}:=$ arg $\max _{|S| \leq k} f(S)$ at each time. In general, even if we had the ability to evaluate the true function $f(\cdot)$ (i.e. without noise), maximizing a submodular function with a cardinality constraint is NP-hard. However, greedy algorithms which sequentially add points, i.e. $S^{(i+1)}=\arg \max _{a \notin S^{(i)}} f\left(S^{(i)} \cup a\right), 1 \leq i \leq k$ guarantee that $f\left(S^{(k)}\right) \geq \alpha f\left(S^{\star}\right)$ with $\alpha \geq 1-1 / e$. Unfortunately, since we do not know $f(\cdot)$ and instead only have access to noisy observations, running the greedy algorithm on any estimate $\hat{f}(\cdot)$ may not necessarily guarantee an $\alpha=1-1 / e$-approximation to $f\left(S^{*}\right)$.

Consequently, a critical notion for us is an $\boldsymbol{\epsilon}$-approximate greedy set with $\epsilon \in[0,1]^{k}$. We define the following collection of sets of size $k$

$$
\mathcal{S}^{k, \epsilon}=\left\{S=S^{(k)} \supset \cdots \supset S^{(1)},\left|S^{(i)}\right|=i, \max _{a \notin S^{(i)}} f\left(S^{(i)} \cup\{a\}\right)-f\left(S^{(i+1)}\right) \leq \epsilon_{i}\right\}
$$

Intuitively, any $S \in \mathcal{S}^{k, \epsilon}$ can be thought of as being constructed from a process that adds an element at stage $i$ which is $\epsilon_{i}$-optimal compared to the Greedy algorithm run on $f$. Such a set naturally arises as the output of the Greedy algorithm run on an approximation $\hat{f}$. This set enjoys the following guarantee.

Lemma 1. (Generalization of Theorem 6 in Streeter and Golovin (2007) For any $\boldsymbol{\epsilon} \geq \mathbf{0} \in \mathbb{R}^{k}$, and $S_{g r}^{k, \epsilon} \in \mathcal{S}^{k, \epsilon}$, we have

$$
f\left(S_{g \boldsymbol{g}}^{k, \epsilon}\right)+\mathbf{1}^{T} \boldsymbol{\epsilon} \geq \frac{1}{c}\left(1-e^{-c}\right) f\left(S^{*}\right)
$$

where $c:=1-\min _{S, a \notin S} \frac{f(S \cup\{a\})-f(S)}{f(a)}$ is the total submodular curvature of $f$.

Lemma 1 is a noise-resistant equivalent result to the approximation ratio of the perfect greedy algorithm. Based off this Lemma we define robust greedy regret

$$
\begin{equation*}
R_{\mathrm{gr}}:=\min _{\epsilon \geq 0, S_{\mathrm{gr}}^{k, \epsilon} \in \mathcal{S}^{k, \epsilon}} R\left(S_{\mathrm{gr}}^{k, \epsilon}\right)+T \mathbf{1}^{T} \boldsymbol{\epsilon} \tag{3}
\end{equation*}
$$

where

$$
R(S):=\sum_{t=1}^{T} f(S)-f\left(S_{t}\right)
$$

This notion of regret captures the fact that the approximate greedy set is the natural set to compare to and that this approximation inevitably results in additional regret. Another notion of regret commonly found in the literature is $\alpha$-Regret: for an $\alpha \in[0,1]$, define $\alpha$-regret by,

$$
R_{\alpha}:=\sum_{t=1}^{T} \alpha f\left(S^{*}\right)-f\left(S_{t}\right)
$$

where $S^{*}:=\arg \max _{|S| \leq k} f(S)$. Using Lemma 1, one immediately has that $R_{\alpha} \leq \min _{\epsilon \geq 0, S_{\mathbf{s r}}^{k, \epsilon} \in \mathcal{S}^{k, \epsilon}} R\left(S_{\mathbf{g r}}^{k, \epsilon}\right)+$ $k \epsilon T$ for $\alpha=\frac{1}{c}\left(1-e^{-c}\right)$. Thus, an upper bound on (3) immediately results in an upper bound on $R_{\alpha}$, which all previous works exploit to obtain their upper bounds on $R_{\alpha}$. Deviating somewhat from previous works, instead of reporting our results in terms of $R_{\alpha}$, we choose to report our results in terms of the more critical quantity (3), which can also be related to a lower bound.

| Function Assumptions | Stochastic | Regret | Upper Bound | Lower Bound |
| :---: | :---: | :---: | :---: | :---: |
| Submodular + monotone | $\checkmark$ | $R_{\mathrm{gr}}$ | $k n^{1 / 3} T^{2 / 3}$ <br> Pedramfar and Aggarwal 2023] | $\min _{\text {(This work) }}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)$ |
| Submodular + monotone | $x$ | $R_{\mathrm{gr}}$ | $k n^{2 / 3} T^{2 / 3}$ <br> Niazadeh et al. 2023 | $\min _{\text {(This work) }}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)$ |
| Degree d Polynomial | $x$ | $R\left(S^{*}\right)$ | $\min \left(\sqrt{n^{d} T}, \sqrt{n^{k} T}\right)$ <br> Chen et al. 2021$]$ | $\min \left(\sqrt{n^{d} T}, \sqrt{n^{k} T}\right)$ |
| Submodular + monotone <br> (This work) | $\checkmark$ | $R_{\mathrm{gr}}$ | $\min _{i}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)$ | $\min _{i}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)$ |

Table 1: Best known regret bounds for combinatorial multiarmed bandits under different assumptions. By lemma 1 our upperbound can also be stated for $R_{1-e^{-1}}$. We note that our lower bound proven for the stochastic setting immediately applies to the non-stochastic setting in the table.

### 1.2 Related Work

There has been several works on combinatorial multi-armed bandits with submodular assumptions and different feedback assumptions. Table 1 summarizes of the most relevant results as well as the results of this paper. For monotonic submodular maximization specifically, previous work use Lemma 1 with appropriate $\boldsymbol{\epsilon}$ to prove an upper bound on expected $R_{\alpha}$-regret when the greedy result with perfect information gives an $\alpha$-approximation of the actual maximum value.

Stochastic In the stochastic setting, when the expected reward function is submodular and monotonic, Nie et al. 2022 proposed an explore-then-commit algorithm with full-bandit feedback that achieves $R_{\mathrm{gr}}=\mathcal{O}\left(k^{4 / 3} T^{2 / 3} n^{1 / 3}\right)^{1}$. Recently, Pedramfar and Aggarwal [2023| showed with the same ETC algorithm with different parameters, $R_{\mathrm{gr}}=\mathcal{O}\left(k n^{1 / 3} T^{2 / 3}+k n^{2 / 3} T^{1 / 3} d\right)$ is possible with delay feedback parameter of $d$. Without the monotonicity, Fourati et al. [2023 achieves $R_{\alpha}=\mathcal{O}\left(n T^{2 / 3}\right)$ with bandit feedback for $\alpha=1 / 2$. There have also been several works in the semi-bandit feedback setting (Wen et al. [2017], Zhu et al. [2021), and others such as getting the marginal gain of each element after each query.

Adversarial In the adversarial setting, the environment chooses a sequence of monotone submodular functions $\left\{f_{1}, \ldots, f_{T}\right\}$. Streeter and Golovin 2008 showed $\mathcal{O}(k \sqrt{\operatorname{Tn} \log n}) R_{\left(1-e^{-1}\right)^{-}}$ regret is possible with partially transparent feedback(where after each round, $f\left(S^{(i)}\right)$ for all $i$ is revealed instead of only $\left.f\left(S^{(k)}\right)\right)$ Niazadeh et al. 2023 proposed an algorithm with $\tilde{\mathcal{O}}\left(k n T^{2 / 3}\right)$ $R_{\left(1-e^{-1}\right) \text {-regret with full bandit feedback. Without the monotone assumption, Niazadeh et al. [2023 }}$ gets $\mathcal{O}\left(n T^{2 / 3}\right) R_{(1 / 2}$-regret with bandit feedback. The upper-bound results in the adversarial setting doesn't naturally lead to results in the stochastic setting as the function is submodular and monotone only in expectation in the stochastic setting.

Continuous Submodular There are several works on online maximization of the continuous extensions of submodular set functions to a compact subspace such as Lovasz extension. Sadeghi et al. 2021 showed that under the stronger assumption of DR-submodularity, and smoothness of the function, $\tilde{\mathcal{O}}(\sqrt{T})\left(1-e^{-1}\right)$-regret is possible.

Low-degree polynomial In general reward functions without the submodular assumption, Chen et al. |2021] Showed if the reward function is a $d$-degree polynomial, $\Theta\left(\min \left(\sqrt{n^{d} T}, \sqrt{n^{k} T}\right)\right)$ regret is optimal.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_0cc955a551f427f5befcg-05.jpg?height=656&width=876&top_left_y=325&top_left_x=581)

Figure 1: $\mathcal{H}_{0}$ expected reward plot for general $k$. In (a) the expected reward of $\mathcal{H}_{0}$ for sets $\{1, \ldots, i\}$ are elevated by $\Delta / k$ for $i<k$, and expected reward of set $\{1, \ldots, k$ is elevated by $\Delta$.

## 2 LOWER BOUND

Theorem 2. For any $n \geq 4, k \leq\lfloor n / 3\rfloor$, satisfying $512 k^{9} n \leq T \in \mathbb{N}$, let $\mathcal{F}$ denote the set of submodular functions that are non-decreasing and bounded by $[0,1]$ for sets of size $k$ or less, and $f(\emptyset)=0$. Then

$$
\begin{array}{r}
\inf _{\text {Alg }}^{f \in \mathcal{F}} \sup _{f \in} \mathbb{E}\left[R_{g r}\right] \geq \frac{1}{16}\left(k-i^{*}\right) T^{2 / 3} n^{1 / 3} e^{-16-2} \sqrt[3]{16} \\
\quad+\frac{1}{4} T^{1 / 2} \sqrt{\binom{n-k}{i^{*}}} e^{-2}
\end{array}
$$

where the infimum is over all randomized algorithms and the supremum is over the functions in $\mathcal{F}$, and $i^{*} \in[k]$ is the largest value satisfying $\frac{16}{n^{2} k^{6}}\binom{n-k}{i^{*}}^{3} \leq T$.

The lowerbound is intuitively a mix of the greedy explore-then-commit algorithm for the first $k-i^{*}$ arms, and then a standard MAB algorithm between all superarms of cardinality $k$ that include those elements. For small $T$ (i.e. $T=\mathcal{O}\left(n^{4}\right)$ ) the regret would be $\mathcal{O}\left(k n^{1 / 3} T^{2 / 3}\right)$, and for large $T$ (i.e. $T=\Omega\left(n^{3 k-2}\right)$ ) the regret would be $\left.\mathcal{O}\binom{n}{k}^{1 / 2} T^{1 / 2}\right)$. This lowerbound also immediately gives a lower bound for the adversarial setting where $f_{i}=f+\mathcal{N}(0,1)$ is the function chosen by the environment at time $i$.

Proof Sketch We construct a hard instance so that at each cardinality a single set gives an elevated reward. Focusing on $k=2$ for illustration, the instance would be the following:

$$
\mathbf{H}_{0}:= \begin{cases}f(\{i\})=1 & \text { if } i \in\{1\} \\ f(\{i\})=1-\frac{\Delta}{2} & \text { if } i \in[n] \backslash\{1\} \\ f(\{i, j\})=3 / 2 & \text { if }(i, j)=(1,2) \\ f(\{i, j\})=3 / 2-\Delta & \text { if }(i, j) \in\binom{[n]}{2} \backslash\{(1,2)\}\end{cases}
$$

where $\Delta$ is the gap of the best set that we will tune based on $T$. Pulling any arm of cardinality less than 2 would incur $O(1)$ regret, however, since there are only $n$ such sets (compared to $\binom{n}{2}$ sets of size 2), pulling these simple arms give more information on the optimal set.

For a set of alternative instances, we choose a set of size $k$ and elevate its reward by $2 \Delta$. We also elevate every prefix set of a permutation of this set by $2 \Delta / k$ so that the new set can be found by a greedy algorithm. Again, for $k=2$, an $1 \notin\{\hat{i}, \hat{j}\}$

$$
\mathbf{H}_{\hat{i}, \hat{j}}:= \begin{cases}f(\{i\})=1 & \text { if } i \in\{1\} \\ f(\{i\})=1+\frac{\Delta}{2} & \text { if } i \in\{\hat{i}\} \\ f(\{i\})=1-\frac{\Delta}{2} & \text { if } i \in[n] \backslash\{1, \widehat{i}\} \\ f(\{i, j\})=3 / 2 & \text { if }(i, j)=(1,2) \\ f(\{i, j\})=3 / 2+\Delta & \text { if }(i, j)=(\widehat{i}, \widehat{j}) \\ f(\{i, j\})=3 / 2-\Delta & \text { if }(i, j) \in\binom{[n]}{2} \backslash\{(1,2),(\widehat{i}, \widehat{j})\}\end{cases}
$$

For small enough $\Delta,\left(\Delta<\frac{1}{8}\right.$ for $\left.k=2\right)$, All the functions would be submodular, as $f(\{a, b\})-f(\{b\}) \leq$ $\frac{1}{2}+2 \Delta \leq 1-\Delta \leq f(\{a\})-f(\{\phi\})$ for any $a, b \in[n]$.

For $\mathbf{H}_{0}$, if $\epsilon<\Delta$, then $f_{\mathcal{H}_{0}}\left(S_{g r}^{2, \epsilon}\right)=\frac{3}{2}$, so $\min _{\epsilon \geq 0} f_{\mathbf{H}_{0}}\left(S_{g r}^{2, \epsilon}\right)+2 \epsilon=\frac{3}{2}$. Similarly, $\min _{\epsilon \geq 0} f_{\mathbf{H}_{\hat{i}, \hat{j}}}\left(S_{g r}^{2, \epsilon}\right)+$ $2 \epsilon=\frac{3}{2}+\Delta$. So for these instances $R_{\mathrm{gr}}=R\left(S^{*}\right)$.

We show that if the KL divergence between an alternate instance and $H_{0}$ is small, then the algorithm cannot distinguish between the two environments and the maximum regret of the two would be $\mathcal{O}(\Delta T)$. Let $\mathbb{P}_{\hat{i}, \hat{j}}, \mathbb{E}_{\widehat{i}, \hat{j}}$ be the probability and expectation under $\mathbf{H}_{\hat{i}, \hat{j}}$, respectively when executing some fixed algorithm with observations being corrupted by standard Gaussian noise. Then $K L\left(\mathbb{P}_{0} \mid \mathbb{P}_{\hat{i}, \hat{j}}\right)=\frac{\Delta^{2}}{2}\left(\mathbb{E}_{0}\left[T_{\hat{i}}\right]+4 \mathbb{E}_{0}\left[T_{\hat{i}, j}\right]\right)$ for $k=2$, and

$$
\begin{aligned}
& 2 \max \left\{\mathbb{E}_{0}\left[R_{T}\right], \mathbb{E}_{\widehat{i}, j}\left[R_{T}\right]\right\} \geq \mathbb{E}_{0}\left[R_{T}\right]+\mathbb{E}_{\widehat{i}, j}\left[R_{T}\right] \\
& \geq \frac{1}{2} \sum_{i=1}^{n} \mathbb{E}_{0}\left[T_{i}\right]+\frac{\Delta T}{2}\left(\mathbb{P}_{0}\left(T_{1,2} \leq \frac{T}{2}\right)+\mathbb{P}_{\hat{i}, \hat{j}}\left(T_{1,2}>\frac{T}{2}\right)\right) \\
& \geq \frac{1}{2} \sum_{i=1}^{n} \mathbb{E}_{0}\left[T_{i}\right]+\frac{\Delta T}{4} \exp \left(-K L\left(\mathbb{P}_{0} \mid \mathbb{P}_{\widehat{i}, \hat{j}}\right)\right) \\
& =\frac{1}{2} \sum_{i=1}^{n} \mathbb{E}_{0}\left[T_{i}\right]+\frac{\Delta T}{4} \exp \left(-\frac{\Delta^{2}}{2}\left(\mathbb{E}_{0}\left[T_{\hat{i}}\right]+4 \mathbb{E}_{0}\left[T_{\widehat{i}, j}\right]\right)\right)
\end{aligned}
$$

Since $\widehat{i}, \widehat{j}$ were arbitrary, we can show that there exist such pair that $\mathbb{E}_{0}\left[T_{\hat{i}}\right]+4 \mathbb{E}_{0}\left[T_{\hat{i}, \hat{j}}\right] \leq$ $\frac{\sum_{i} \mathbb{E}_{0}\left[T_{i}\right]}{n-2}+\frac{4 T}{\binom{n-2}{2}}$ (see Lemma 4 for general $k$ ). If $i^{*}=1$, then for $\Delta=T^{-1 / 3} n^{2 / 3}$, we have $\frac{2 \Delta^{2} T}{\binom{n-2}{2}} \leq 1$. So for KL divergence to be large, we would have $\sum_{i} \mathbb{E}_{0}\left[T_{i}\right] \geq \frac{1}{4} T^{2 / 3} n^{1 / 3}$, which from the above equation shows the regret is $\mathcal{O}\left(T^{2 / 3} n^{1 / 3}\right)$. This can be extended for expected value of pulls of each cardinality lower than $i^{*}+1$ for general $k$. If $i^{*}=2$, then it can be shown that $\frac{1}{2} \sum_{i=1}^{n} \mathbb{E}_{0}\left[T_{i}\right]+\frac{\Delta T}{4} \exp \left(-\frac{\Delta^{2}}{2}\left(\frac{1}{n-2} \sum_{i} \mathbb{E}_{0}\left[T_{i}\right]+4 T /\binom{n-2}{2}\right)\right)$ with $\Delta=\sqrt{\binom{n-2}{2} / T}$ minimizes when $\sum_{i} \mathbb{E}_{0}\left[T_{i}\right]=0$, so the regret would be $T^{1 / 2}\binom{n-2}{2}^{1 / 2} \exp (-2)$. This shows that the expected regret is $O\left(\min _{i}\left(i n^{1 / 3} T^{2 / 3}+\sqrt{n^{k-i} T}\right)\right)$. The instance of general $k$, and the detailed proof is in appendix 6.1.

## 3 UCB UPPER BOUND

```
Algorithm 1 SUB-UCB algorithm for set bandits with cardinality constraints
    Input: $T, m$, greedy stop level $l$
    Initialization: $S^{(0)}=\emptyset, T_{A}=0$ for all $A \subset[n]$
    For each $a \in[n]$, pull $\{a\}$ exactly $m$ times and update $T_{\{a\}} \leftarrow m$. Update $t \leftarrow m n$.
    for $i=1,2, \ldots, l$ do
        $U_{a} \leftarrow \infty$ for all $a \notin S^{(i-1)}$
        while $T_{S^{(i-1)} \cup \operatorname{arg~max} U_{a}}<m$ do
            Pull arm $S_{t}=S^{(i-1)} \cup \arg \max _{a} U_{a}$
            Observe $r_{t}$.
            $T_{S_{t}} \leftarrow T_{S_{t}}+1$
            for each $a \notin S^{(i-1)}$ do
                $S_{a} \leftarrow S^{(i-1)} \cup\{a\}$
                $\hat{\mu}_{S_{a}} \leftarrow \frac{1}{T_{S_{a}}} \sum_{t: I_{t}=S_{a}} r_{t}$
                Compute UCB: $U_{a}=\hat{\mu}_{S_{a}}+\sqrt{\frac{8 \log t}{T_{S_{a}}}}$
            end for
            $t \leftarrow t+1$
        end while
        Update the base set: $S^{(i)} \leftarrow S^{(i-1)} \cup\left\{a_{i}\right\}$ where $a_{i}:=\arg \max _{a} U_{a}$
    end for
    while $t<T$ do
        Run UCB on all size $k$ super-arms $A$ where $S^{(l)} \in A$.
    end while
```

A natural approach to minimizing regret is to take an Explore-Then-Commit strategy motivated by the greedy algorithm. Such an algorithm would be the following - proceed in $k$ rounds. Set $S^{0}=\emptyset$. In round $i$ pull each set in the collection $\left\{S^{i-1} \cup\{a\}: a \in[n] \backslash S^{i-1}\right\}, m$ times. Use these samples to update our estimate $\hat{f}$ of $f$ on these sets, and set $S^{(i)} \leftarrow \arg \max _{a \in[n] \backslash S^{i-1}} \hat{f}\left(S^{i-1} \cup\{a\}\right)$. This approach has been pursued by existing works Nie et al. [2022], and with an appropriate choice of $m$ results in $O\left(k n^{1 / 3} T^{2 / 3}\right)$ regret.

The disadvantage of this approach is that it can not achieve the correct trade-off between $\sqrt{n^{k} T}$ and $k n^{1 / 3} T^{2 / 3}$ exhibited by the lower bound. Motivated by the statement of the lower bound, our algorithm SUB-UCB attempts to interpolate between these different regret regimes. The critical quantity is $i^{*}$. For the first $k-i^{*}$ cardinalities, our algorithm plays a UCB style strategy which more or less follows the ETC strategy described in the previous paragraph. After that, it defaults to a UCB algorithm on all subsets containing $S^{k-i^{*}}$, a total of $\binom{n-k+i^{*}}{i^{*}}$ possible arms.

Theorem 3. For any $l \leq k$, SUB-UCB guarantees

$$
\mathbb{E}\left[R_{\boldsymbol{g r}}\right] \leq(1+4 \sqrt{2}) l T^{2 / 3} n^{1 / 3}(\log T)^{1 / 3}+65 \sqrt{T\binom{n-k}{k-l} \log T}+\frac{32}{15}\binom{n-k}{k-l}
$$

when $m=T^{2 / 3} n^{-2 / 3} \log T^{1 / 3}$.

Proof Sketch We show that for $\epsilon:=2 \sqrt{2 \log \left(2 k n T^{2}\right) / m}$, the greedy part of SUB-UCB with high probability adds an $\epsilon$-optimal arm in each step. Defining event $G$ to be $\left|\hat{\mu}_{S}-f(S)\right| \leq$ $\sqrt{2 T_{S} \log \left(2 k n T^{2}\right)}$ for all iterations, we prove that this event is true with a probability of at least $1-\frac{1}{T}$.

On Event $G$, We show that an $\epsilon$-good arm is selected at each step of the greedy algorithm for $\epsilon=2 \sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}$. Let $a$ be a sub-optimal arm with expected reward value more than $2 \sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}$ from the best arm in the $i$-th step i.e. $\Delta_{S^{(i)}, a}:=\max _{a^{\prime}} f\left(S^{(i)} \cup\left\{a^{\prime}\right\}\right)-f\left(S^{(i)} \cup\{a\}\right) \geq 2 \sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}$. Then if arm $a$ is added in $i$-th step, we have $U_{a}(t) \geq U_{a^{*}}(t) \geq f\left(S^{(i)) \cup\left\{a^{*}\right\}}\right.$, and therefore,

$$
U_{a}(t)-f\left(S^{(i)} \cup\{a\}\right) \geq \Delta_{S^{(i)}, a}>2 \sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}
$$

so $\hat{\mu}_{S^{(i)} \cup\{a\}}-f\left(S^{(i)} \cup\{a\}\right)>\sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}$. This is a contradiction with event $G$, so on event $G$ such an arm cannot be selected. Lastly, we expand the regret of two stages. As UCB in the second part of the algorithm has the regret of $65 \sqrt{T\binom{n-k}{k-l} \log T}+\frac{32}{15}\binom{n-k}{k-l}$ against the best arm containing $S^{(l)}$ (see Lattimore and Szepesvari 2017|), it is an upper bound for the regret against the greedy solution were the last $k-l$ steps select the best arm, so

$$
\mathbf{1}^{T} \boldsymbol{\epsilon}=l \boldsymbol{\epsilon}=2 l \sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}
$$

Therefore, the expected regret $\mathbb{E}\left[R_{\mathrm{gr}}\right]$ on event $G$ can be written as

$$
2 T l \sqrt{\frac{2 \log \left(2 k n T^{2}\right)}{m}}+m n\left(k-i^{*}\right)+65 \sqrt{T\binom{n-k}{k-l} \log T}+\frac{32}{15}\binom{n-k}{k-l}
$$

for any choice of $m$ and $l$. So for $m=T^{2 / 3} n^{-2 / 3} \log ^{1 / 3}\left(2 k n T^{2}\right)$ the above term becomes $\tilde{\mathcal{O}}\left(l T^{2 / 3} n^{1 / 3}+\right.$ $\sqrt{T\binom{n}{k-l}}$. The detailed proof is in Appendix 6.2

## 4 EXPERIMENTS

For the experiments we compare SUB-UCB $(l)$ for different greedy stop levels $l$, SUB-UCB $\left(k-i^{*}\right)$ which selects the best stop level based on the regret analysis, the ETCG (explore-then-commit greedy) algorithm from Nie et al. 2022, and UCB on all size $k$ arms. Each arm pull has a 1-Gaussian noise, with 50 trials for each setting. The expected reward functions are the following.

## Functions:

- The Unique greedy path hard instance i.e.

$$
f(S)=\left\{\begin{array}{l}
\sum_{i=1}^{|S|} \frac{1}{k+i} \quad S=\{1, \ldots,|S|\} \\
\sum_{i=1}^{|S|} \frac{1}{k+i}+\frac{1}{100} \quad S=\{1, \ldots,|S|\}
\end{array}\right.
$$

This function is inspired by the hard instance in the proof of our lower-bound. Note that this particular parameterization is submodular when $k \leq 7$, not for general $k$.

- Weighted set cover function i.e. $f_{\mathcal{C}}(S)=\sum_{C \in \mathcal{C}} w(C) \mathbf{1}\{S \cap C \neq \emptyset\}$ for a partition $\mathcal{C}$ of $[n]$ and weight function $w$ on the partition. For $n=15$, we use the partitions of size $5,5,4,1$ with weights of $1 / 10,1 / 10,2 / 10,6 / 10$ respectively.

Results: As illustrated in 3, we observe that our algorithm with the level selection of $k-i^{*}$ outperforms both ETCG and naive UCB on all size $k$ arms, as it combines the advantages of greedy approach for small $T$ s and UCB on many super arms for large $T$. For smaller $T$ s compared to $\binom{n}{k}$, both SUB-UCB and ETCG outperform normal UBC as it doesn't have enough budget to find

![](https://cdn.mathpix.com/cropped/2024_06_04_0cc955a551f427f5befcg-09.jpg?height=551&width=745&top_left_y=310&top_left_x=663)

Figure 2: Regret of SUb-UCB (i) for the unique greedy path reward function. The optimal stop greedy cardinality $l=k-i^{*}$ is highlighted

![](https://cdn.mathpix.com/cropped/2024_06_04_0cc955a551f427f5befcg-09.jpg?height=556&width=729&top_left_y=1088&top_left_x=663)

Figure 3: Regret comparison for weighted set cover with $n=15$ and $k=4$

![](https://cdn.mathpix.com/cropped/2024_06_04_0cc955a551f427f5befcg-09.jpg?height=585&width=1575&top_left_y=1800&top_left_x=253)

![](https://cdn.mathpix.com/cropped/2024_06_04_0cc955a551f427f5befcg-09.jpg?height=496&width=656&top_left_y=1817&top_left_x=257)

(a) unique greedy path function

![](https://cdn.mathpix.com/cropped/2024_06_04_0cc955a551f427f5befcg-09.jpg?height=498&width=675&top_left_y=1819&top_left_x=1148)

(b) weighted set cover function

Figure 4: Comparison between all Sub-UCB greedy stop cardinality choices for the weighted set cover function and unique greedy path function
optimal sets of size $k$, so it gets linear regret(as the other two get $O\left(T^{2 / 3}\right)$ ). However, as $T$ becomes larger the reverse happens as $\binom{n}{k} T^{1 / 2}$ becomes smaller than $T^{2 / 3}$, but SUB-UCB adopts to $T$ and continues to outperform the two until it converges with naive UCB for very large $T$.

In figures 2 and 4a, we compare the performance of SUB-UCB for different choices of greedy stop cardinality, and observe that the best choice gradually decreases from $k$ to 0 as $T$ gets larger, and $k-i^{*}$ is a practical selection of the best stop cardinality before running the algorithm.

In Figure 4b, as $\binom{n}{k}$ is relatively small, the budget that naive UCB (SUB-UCB with $l=0$ ) outperforms all other choices for $T \geq 10^{7}$. Also, the empirical standard derivation is much smaller than $\mathcal{O}\left(T^{1 / 2}\right)$ due to the regret symmetry of non-optimal sets at each cardinality, and it's not visible in the plots.

## 5 CONCLUSION

In this paper we showed that $\min _{i}\left(i T^{2 / 3} n^{1 / 3}+\sqrt{n^{k-i} T}\right)$, ignoring logarithmic factors, is a lower bound on the regret against robust greedy solutions of stochastic submodular functions, and matched this bound with an algorithm. This work is the first minimax lower bound for submodular bandits, and it remains open to prove similar minimax optimal bounds in the adversarial setting, and in general, any offline-to-online greedy procedure that is robust to noise (e.g. Non-monotonic submodular maximization were the greedy approach gets a 1/2-approximation of the function).

## Acknowledgements

This paper is based on work supported by Microsoft Grant for Customer Experience Innovation.

## References

Mridul Agarwal, Vaneet Aggarwal, Christopher J. Quinn, and Abhishek Umrawal. DART: aDaptive Accept RejecT for non-linear top-K subset identification, November 2020. URL http://arxiv. org/abs/2011.07687, arXiv:2011.07687 [cs, stat].

Mridul Agarwal, Vaneet Aggarwal, Christopher J. Quinn, and Abhishek K. Umrawal. Stochastic Top-\$K\$ Subset Bandits with Linear Space and Non-Linear Feedback, October 2021. URL http://arxiv.org/abs/1811.11925, arXiv:1811.11925 [cs, stat].

Maria-Florina Balcan and Nicholas J A Harvey. Learning Submodular Functions.

Eric Balkanski and Yaron Singer. The adaptive complexity of maximizing a submodular function. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1138-1151, Los Angeles CA USA, June 2018. ACM. ISBN 978-1-4503-5559-9. doi: 10.1145/ 3188745.3188752. URL https://dl.acm.org/doi/10.1145/3188745.3188752.

Andrew An Bian, Joachim M. Buhmann, Andreas Krause, and Sebastian Tschiatschek. Guarantees for Greedy Maximization of Non-submodular Functions with Applications, May 2019. URL http://arxiv.org/abs/1703.02100, arXiv:1703.02100 [cs, math].

Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. Combinatorial Multi-Armed Bandit with General Reward Functions. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016a. URL https://proceedings.neurips.cc/paper/2016/hash/ aa169b49b583a2b5af89203c2b78c67c-Abstract.html

Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms, March 2016b. URL http://arxiv.org/abs/1407. 8339, arXiv:1407.8339 [cs].

Xi Chen, Yanjun Han, and Yining Wang. Adversarial Combinatorial Bandits with General Non-linear Reward Functions, January 2021. URL http://arxiv.org/abs/2101.01301. arXiv:2101.01301 [cs, stat].

Uriel Feige. A threshold of $\ln \mathrm{n}$ for approximating set cover. J. ACM, 45(4):634-652, jul 1998. ISSN 0004-5411. doi: 10.1145/285055.285059. URL https://doi.org/10.1145/285055.285059.

Fares Fourati, Vaneet Aggarwal, Christopher Quinn, and Mohamed-Slim Alouini. Randomized greedy learning for non-monotone stochastic submodular maximization under full-bandit feedback. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 7455-7471. PMLR, 25-27 Apr 2023. URL https://proceedings.mlr.press/v206/fourati23a.html.

Michel X. Goemans, Nicholas J. A. Harvey, Satoru Iwata, and Vahab Mirrokni. Approximating Submodular Functions Everywhere. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 535-544. Society for Industrial and Applied Mathematics, January 2009. ISBN 978-0-89871-680-1 978-1-61197-306-8. doi: 10.1137/1.9781611973068.59. URL https://epubs.siam.org/doi/10.1137/1.9781611973068.59.

Botao Hao, Tor Lattimore, and Mengdi Wang. High-Dimensional Sparse Linear Bandits, September 2021. URL http://arxiv.org/abs/2011.04020, arXiv:2011.04020 [cs, math, stat].

Nicholas Harvey, Christopher Liaw, and Tasuku Soma. Improved Algorithms for Online Submodular Maximization via First-order Regret Bounds. In Advances in Neural Information Processing Systems, volume 33, pages 123-133. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/hash/ 0163cceb20f5ca7b313419c068abd9dc-Abstract.htm1.

Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. An efficient bandit algorithm for realtime multivariate optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1813-1821, 2017.

Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the Complexity of Best Arm Identification in Multi-Armed Bandit Models, November 2016. URL http://arxiv.org/abs/1407.4443. arXiv:1407.4443 [cs, stat].

Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2):209-232, 2002.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. In Lucas Bordeaux, Youssef Hamadi, and Pushmeet Kohli, editors, Tractability, pages 71-104. Cambridge University Press, 1 edition, February 2014. ISBN 978-1-107-02519-6 978-1-139-17780-1. doi: 10.1017/CBO9781139177801. 004. URL https://www.cambridge.org/core/product/identifier/CB09781139177801A031/ type/book_part

Tor Lattimore and Csaba Szepesvari. Bandit algorithms. 2017. URL https://tor-lattimore.com/ downloads/book/book.pdf.

Tatsuya Matsuoka, Shinji Ito, and Naoto Ohsaka. Tracking Regret Bounds for Online Submodular Optimization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 3421-3429. PMLR, March 2021. URL https://proceedings.mlr.press/v130/ matsuoka21a.html. ISSN: 2640-3498.

George L Nemhauser and Laurence A Wolsey. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Combinatorica, 3(3-4):257-268, 1978.

Rad Niazadeh, Negin Golrezaei, Joshua Wang, Fransisca Susan, and Ashwinkumar Badanidiyuru. Online Learning via Offline Greedy Algorithms: Applications in Market Design and Optimization, February 2023. URL http://arxiv.org/abs/2102.11050, arXiv:2102.11050 [cs, math, stat].

Guanyu Nie, Mridul Agarwal, Abhishek Kumar Umrawal, Vaneet Aggarwal, and Christopher John Quinn. An Explore-then-Commit Algorithm for Submodular Maximization Under Full-bandit Feedback. 2022.

Mohammad Pedramfar and Vaneet Aggarwal. Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback, March 2023. URL http://arxiv.org/abs/2303.13604. arXiv:2303.13604 [cs].

Omid Sadeghi, Prasanna Raut, and Maryam Fazel. Improved Regret Bounds for Online Submodular Maximization, June 2021. URL http://arxiv.org/abs/2106.07836, arXiv:2106.07836 [cs, math, stat].

Max Simchowitz, Kevin Jamieson, and Benjamin Recht. Best-of-K Bandits, March 2016. URL http://arxiv.org/abs/1603.02752, arXiv:1603.02752 [cs, stat].

Matthew Streeter and Daniel Golovin. An Online Algorithm for Maximizing Submodular Functions:. Technical report, Defense Technical Information Center, Fort Belvoir, VA, December 2007. URL http://reports-archive.adm.cs.cmu.edu/anon/2007/CMU-CS-07-171.pdf.

Matthew Streeter and Daniel Golovin. An Online Algorithm for Maximizing Submodular Functions. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008. URL https://papers.nips.cc/paper_files/paper/2008/hash/ 5751ec3e9a4feab575962e78e006250d-Abstract.html

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermodular optimization with bounded curvature, December 2014. URL http://arxiv.org/ abs/1311.4728. arXiv:1311.4728 [cs].

Zoya Svitkina and Lisa Fleischer. Submodular approximation: sampling-based algorithms and lower bounds, May 2010. URL http://arxiv.org/abs/0805.1071. arXiv:0805.1071 [cs].

Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. Online influence maximization under independent cascade model with semi-bandit feedback. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 3026-3036, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

Junlong Zhu, Qingtao Wu, Mingchuan Zhang, Ruijuan Zheng, and Keqin Li. Projection-free decentralized online learning for submodular maximization over time-varying networks. Journal of Machine Learning Research, 22(51):1-42, 2021. URL http://jmlr.org/papers/v22/18-407. html.
