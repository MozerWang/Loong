# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? 

Shaoxiong Ji<br>University of Helsinki<br>shaoxiong.ji@helsinki.fi

Pinzhen Chen<br>University of Edinburgh<br>pinzhen.chen@ed.ac.uk


#### Abstract

Fine-tuning large language models for multilingual downstream tasks requires a diverse set of languages to capture the nuances and structures of different linguistic contexts effectively. While the specific number varies depending on the desired scope and target languages, we argue that the number of languages, language exposure, and similarity that incorporate the selection of languages for fine-tuning are some important aspects to examine. By fine-tuning large multilingual models on 1 to 52 languages, this paper answers one question: How many languages are needed in instruction fine-tuning for multilingual tasks? We investigate how multilingual instruction fine-tuned models behave on multilingual benchmarks with an increasing number of languages and discuss our findings from the perspective of language exposure and similarity.


## 1 Introduction

Multilinguality in the context of language models refers to the ability of a model to understand and generate texts in multiple languages. Large language models are often designed to be multilingual, meaning they can handle text in various languages and even perform translation between them. Large language models (LLMs), like mGPT (Shliazhko et al. 2022), BLOOM (Scao et al., 2022), and LLaMA (Touvron et al., 2023), are characterized by their vast size and extensive pre-training on a collection of diverse multilingual texts from the Internet. They exhibit impressive capabilities to comprehend and generate texts across multiple languages to solve downstream tasks. However, mastering multilingual proficiency requires an understanding of the unique grammatical structures, idiomatic expressions, and cultural contexts inherent to each language. Fine-tuning a model for multilingual prowess demands meticulous consideration of these linguistic nuances to ensure accurate and contextually appropriate responses.

Recent research has shed light on the efficacy of multilingual instruction fine-tuning (mIT) in enhancing multilingual performance. Chen et al. (2024) compared monolingual and multilingual instruction tuning under a resource-fair scenario. Kew et al. (2023) experimented with English-centric LLMs such as Llama 2 (Touvron et al. 2023) and Falcon (Almazrouei et al. 2023) and found that multilingual instruction fine-tuning with as few as three languages improves models' cross-lingual transfer abilities. Similarly, Shaham et al. (2024) studied instruction tuning for multilingual LLMs and observed that enhancing the diversity of languages in the instruction tuning set, even by just adding 2,3 , or 4 languages, leads to improved cross-lingual generalization. Moreover, Chirkova \& Nikoulina (2024b) explored zero-shot cross-lingual transfer, where an LLM is trained on English-only data but tested on prompts in the other four languages, highlighting the importance of learning rate in achieving effective cross-lingual transfer.

Nevertheless, it is essential to note the variations in the number and diversity of languages used in these studies. Kew et al. (2023) considered five non-English languages (Spanish,[^0]

Russian, German, Chinese, and French) but also trained models on the full Guanaco dataset with more than 30 languages. Shaham et al. (2024) experimented with a machine-translated instruction dataset in 11 diverse languages. Chirkova \& Nikoulina (2024b) studied finetuning with English data and tested on four languages (English, French, Portuguese, and Russian). These works are not strictly comparable due to different training configurations, data, model choices, and evaluation tasks and benchmarks. More importantly, while it has been demonstrated that a small number of languages can elicit (zero-shot) cross-lingual transfer, it is in no way associated with achieving the optimal downstream task performance.

We aim to fill in the gap by scaling up the number of languages in the instruction tuning phase, offering insights into the broader implications of multilingual instruction fine-tuning. This paper employs instruction fine-tuning using the multilingual BLOOM model (Scao et al. 2022), and a parallel instruction dataset named Bactrain-X in 52 languages ( $\mathrm{Li}$ et al. 2023). We progressively add a language during instruction fine-tuning at each time, and train 52 models in total. Then, we evaluate those models using three multilingual benchmarks. Our experimental results show that:

- Contrary to prior research, adding more languages beyond a handful can further improve accuracy, although with some outlier cases and diminishing returns.
- Given the considered 52 studied languages, there is no consistent answer regarding the optimal number of languages for mIT. The optimal number of instruction languages depends on the language similarity and downstream evaluation.
- The impact of mIT can vary, potentially aiding or hindering multilingual performance. Additionally, the cross-lingual transfer ability of mIT exists, though both phenomena are contingent upon the benchmark and languages involved.

Our study emphasizes the importance of a closer look at the tasks, benchmarks, languages, and evaluation metrics. And we advocate for more consistent future studies focused on mIT. For example, in the prior works that explore open-ended chat (Shaham et al. 2024: Kew et al. 2023: Chirkova \& Nikoulina, 2024b, Chen et al. 2024), there is variation in the evaluation set, number of samples, and evaluation criteria. Our study highlights the necessity for more systematic experimental studies on various variables. These variables include but are not limited to base LLMs, training recipes, instruction data and languages, evaluation tasks and benchmarks, and evaluation criteria for different tasks. Such comprehensive and consistent investigations are crucial for advancing our understanding of mIT and its implications.

## 2 Multilingual Instruction Fine-tuning

We perform multilingual instruction fine-tuning to large language models, aiming to study their proficiency across multiple languages. By fine-tuning the model using diverse linguistic datasets, it adapts to various languages, allowing it to generate more contextually relevant and accurate responses in a multilingual context. This section describes the methodological settings of multilingual instruction fine-tuning, multilingual instruction data, and the base language model, which are necessary for addressing our research inquiries.

### 2.1 Instruction Fine-tuning with an Increasing Number of Languages

Our setup is supervised fine-tuning, where an instruction and a task input are fed to an LLM, and the LLM is trained to produce a response. To expand the multilingual capabilities of the model, we employ a strategy that progressively incorporates additional languages during instruction fine-tuning. We start from English and Chinese, which are extensively resourced languages that are prominently presented in both pretraining corpora and evaluation benchmarks and also represent distinct written scripts. Then, we progressively add the other languages in alphabetical order.

It is important to acknowledge a potential limitation stemming from the increasing data size as the number of languages expands. This can introduce an additional variable that might impact multilingual performance. To mitigate this effect, we opt for parallel instruction
data in which English instructions are translated into different languages. This ensures consistency in the instruction information across languages while minimizing the overall increase in data size. Moreover, the increase in data size also amplifies the number of optimization steps when utilizing stochastic gradient descent to update the model parameters on the same device. The number of updates can be expressed as: $U=\left\lceil\frac{N \times L \times E}{B \times W}\right\rceil$, where $N$ is the number of instruction data, $L$ is the number of languages, $E$ is the number of training epochs, $B$ is the batch size, and $W$ is the world size (i.e., the number of GPUs). We increase the number of GPUs proportionally when increasing the number of languages to maintain a manageable range of updates.

### 2.2 Multilingual Instruction Data

The choice of multilingual instruction data lies in its comprehensive language coverage and data quality, thus enabling robust and inclusive multilingual instruction fine-tuning experiments. We use the Bactrian-X dataset (Li et al., 2023) that addresses the scarcity of high-quality multilingual instruction-response data. This dataset comprises 3.4 million instruction-response pairs across 52 languages. Instructions were collected from Alpaca (52K) (Taori et al. 2023) and Dolly (15K) (Conover et al. 2023) datasets, with Alpaca instructions generated by a text-davinci-003 model and Dolly instructions contributed by humans. These $67 \mathrm{~K}$ instructions in English were then translated into 51 different languages using the Google Translate API, aligned with the languages supported by the mBART-50 model (Liu et al. 2020). To ensure translation accuracy and relevance, instructions containing programming-related content were identified and excluded. To assess translation quality, 100 randomly selected sentences per language were backtranslated into English using the same API, with original English sentences as references. Automatic evaluation with standard metrics indicates reliable and high-quality translations. For further comprehensive details regarding the dataset creation process and evaluation, readers are encouraged to refer to the original paper. In our experimentation, we utilize the Bactrian- $\mathrm{X}$ dataset as the foundation and iteratively augment it by adding one language at a time for multilingual instruction fine-tuning. The languages selected for instruction tuning follow the specified order: en (English), zh (Chinese), af (Afrikaans), ar (Arabic), az (Azerbaijani), bn (Bengali), cs (Czech), de (German), es (Spanish), et (Estonian), fa (Farsi), fi (Finnish), fr (French), gl (Galician), gu (Gujarati), he (Hebrew), hi (Hindi), hr (Croatian), id (Indonesian), it (Italian), ja (Japanese), ka (Georgian), kk (Kazakh), km (Khmer), ko (Korean), lt (Lithuanian), lv (Latvian), mk (Macedonian), ml (Malayalam), mn (Mongolian), mr (Marathi), my (Burmese), ne (Nepali), nl (Dutch), pl (Polish), ps (Pashto), pt (Portuguese), ro (Romanian), ru (Russian), si (Sinhala), sl (Slovenian), sv (Swedish), sw (Swahili), ta (Tamil), te (Telugu), th (Thai), tl (Tagalog), $\operatorname{tr}$ (Turkish), uk (Ukrainian), ur (Urdu), vi (Vietnamese), xh (Xhosa). We note that a recent multilingual instruction dataset called Aya (Singh et al. 2024) was released at the time of writing. We leave it as a future study.

### 2.3 Base Language Model

Multilingual language models can inherit biases present in the training data, which may affect their responses when fine-tuned. We adopt a representative multilingual language model: BLOOM (Scao et al. 2022) that is developed with careful consideration in multiple natural and coding languages. Its multilingual capacity makes it well-suited for a wide range of natural language processing tasks across multiple languages. In our specific application, we opt for the BLOOM-7B1 variant with 7.1B parameters. This parameter count ensures necessary capabilities for text understanding and generation across a multitude of languages and contexts, and it is affordable considering our computing budget.

## 3 Experimental Setup

### 3.1 Training Details

We use the Hugging Face transformers framework (Wolf et al. 2019) with the DeepSpeed integration (Rasley et al. 2020) as the training software. The learning rate is set to $3 \mathrm{e}-5$. The
batch size was established at 4 per device. Gradient accumulation, with a step size of 4 , enables the aggregation of gradients over multiple steps. The number of training epochs is fixed at 3. The maximum model length is set to 768 , the same as the Bactrian- $\mathrm{X}$ paper $(\overline{\mathrm{Li}}$ et al., 2023). Models are trained on the cluster with 4 AMD MI250X GPUs (8 GPU dies) in each node. We adopt distributed training on multiple nodes from 2 to 10 nodes with the increase in the number of languages, making the global batch size from 256 to 1280.

### 3.2 Benchmarks and Evaluation

We evaluate the instruction-tuned models on three multilingual benchmarks. XCOPA (Ponti et al. 2020) is a multilingual dataset for causal commonsense reasoning in 11 languages XStoryCloze (Lin et al. 2021) is a multilingual dataset derived from the English StoryCloze dataset (Mostafazadeh et al. 2017) for commonsense reasoning in the story, translated into 10 non-English languages. The test involves a system selecting the appropriate ending for a four-sentence story. XWinograd (Tikhonov \& Ryabinin, 2021) is a multilingual compilation of Winograd Schemas (Levesque et al. 2012) available in six languages, designed for assessing cross-lingual commonsense reasoning abilities. We utilize zero-shot evaluation techniques facilitated by the Language Model Evaluation Harness (lm-evaluation-harness) (Gao et al. 2023). Different models, trained with progressively added languages, are evaluated on these benchmarks using accuracy (\%) as the evaluation metric.

## 4 Results and Discussion

This section presents the experimental results and our findings.

### 4.1 Overall Performance with the Increasing Number of Languages

We first answer the effect of the number of languages on multilingual performance - how much multilingualism is needed for instruction-tuning a large language model, i.e., BLOOM7B1 in our case study. Figure 1a illustrates the average accuracy on three benchmarks with the increase in the number of languages. The figure shows fluctuating results. For XCOPA and XStoryCloze, there is a slightly increasing trend with the increasing number of instruction languages, except for a notable drop when Korean is added into the instruction languages. For XWinograd, the trend is not clear, and the performance also drops when Korean is added. We checked the training curve of the model trained with Korean added to instruction data and found that the training and validation loss decreases as training goes on and the model converges as expected.

Moving to the performance in reference languages, i.e., English and Chinese, we notice a similar drop in accuracy when Korean is added, as displayed in Figures 1b to 1d, but no obvious trend for cross-lingual transfer capacity with the increase in the number of languages. For XCOPA, instruction fine-tuning on Chinese improves the accuracy of the target language, and the accuracy peaks at the model when Bangla (bn), the 6th language, is added for instruction fine-tuning. For English and Chinese XStoryCloze, Latvian (lv, the 27th language added) and Malaysian (ml, the 29th language added) are helpful to boost the performance in these languages respectively. Instruction fine-tuning turns out to be harmful on the XWinograd dataset, as shown in Figure $1 \mathrm{~d}$ that instruction fine-tuned models are less performant than the base LLM. Multilingual instruction fine-tuning does not generalize on the three benchmarks studied; sometimes, multilingual instruction tuning is detrimental.

Overall, the performance of multilingual evaluation does not increase linearly with the number of languages of instruction data. The effect of the number of languages on multilingual performance is dependent on the task and the language added for training. Our results show that instruction fine-tuning with a few languages is good for cross-lingual transfer but not the most performant. Further, adding more languages, even in alphabetical order, can sometimes improve the performance and is occasionally harmful in XCOPA and XStoryCloze, while in XWinograd, multilingual instruction fine-tuning is detrimental.

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-05.jpg?height=848&width=1391&top_left_y=270&top_left_x=367)

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-05.jpg?height=333&width=650&top_left_y=289&top_left_x=388)

(a) All languages on three benchmarks

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-05.jpg?height=344&width=639&top_left_y=695&top_left_x=388)

(c) English and Chinese XStoryCloze

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-05.jpg?height=328&width=639&top_left_y=297&top_left_x=1098)

(b) Chinese XCOPA

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-05.jpg?height=351&width=646&top_left_y=692&top_left_x=1092)

(d) English and Chinese XWinograd

Figure 1: Overall accuracy on all languages evaluated (Figure 1a and accuracy on English and Chinese that act as reference languages (Figures $1 \mathrm{~b}$ to $1 \mathrm{~d}$. 'instruct' in the round brackets indicates the scores are obtained by instruction-tuned models. 'base' stands for that obtained by the base model.

### 4.2 Language Exposure

Assessing the performance of multilingual models after instruction fine-tuning can be challenging, as it may involve evaluating their proficiency across multiple languages and tasks. The multilingual benchmarks do not cover the full list of languages used during pretraining and instruction tuning. We group the languages into four categories according to base and instruction-tuned models' language coverage:

1. languages are seen by the base and instruction models, including id, sw, ta, vi, zh (XCOPA); ar, en, es, hi, id, sw, te, zh (XStoryCloze); en, fr, pt, zh (Winograd)
2. languages are seen by the base model but not the instruction model, including ht (XCOPA) and eu (XStoryCloze)
3. languages are unseen by the base model but seen by the instruction model, including et, it, th, $\operatorname{tr}$ (XCOPA); my, ru (XStoryCloze); ru (XWinograd)
4. languages are unseen by the base nor instruction model, including qu (XCOPA) and jp (XWinograd)

We analyze the performance of multilingual instruction tuning with the increase in the number of languages in different groups with a focus on unseen languages by the base or instruction-tuned model. Cross-lingual transfer does happen in some cases, depending on benchmarks and languages.

Figure 2 displays the accuracy of languages seen by the base model but unseen during instruction fine-tuning. Multilingual instruction fine-tuning on languages other than the target languages (i.e., ht and eu) shows good cross-lingual transfer ability in most cases. And we also see a notable drop in eu in the XStoryCloze when Korean is added for instruction tuning.

Then, we look at whether multilingual instruction fine-tuning can adapt the base LLM to unseen languages during pretraining. Figure 3 shows the accuracy of languages that have not been used for training the base model but are used during instruction tuning. We find

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-06.jpg?height=353&width=615&top_left_y=298&top_left_x=430)

(a) XCOPA

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-06.jpg?height=368&width=610&top_left_y=285&top_left_x=1126)

(b) XStoryCloze

Figure 2: Accuracy on languages seen by the base model but unseen by instruction-tuned model

that adding a language to instruction tuning can immediately improve the performance on that language as anticipated, especially for th in Figure 3c, my in Figure 3e and ru in Figure $3 \mathrm{f}$, the improvement is remarkable. But for ru in the XWinograd dataset in Figure $3 g$, the improved performance is still worse than the base model. We also observe that cross-lingual transfer is happening in the evaluated languages. For example, the performance on et and it in XCOPA shown in Figures $3 \mathrm{a}$ and $3 \mathrm{~b}$ can be further improved with the increase of instruction languages, and instruction fine-tuning on languages other than the target language tr can contribute to an improved performance than the base model as in Figure $3 \mathrm{~d}$

There are two languages that are not covered by the base model and instruction-tuned models, i.e., qu and jp. Figure 4 shows the results. The accuracy on qu fluctuates in general, but with a steadily increasing trend when adding $\mathrm{lt}, \mathrm{lv}, \mathrm{mk}, \mathrm{ml}, \mathrm{mn}$, and $\mathrm{mr}$. The accuracy on jp also fluctuates, but with more than 20 languages for instruction tuning is better than tuning with fewer than 20 languages in many cases. Moreover, the performance on jp shows an opposite result to the performance on other languages, and the average performance instruction fine-tuning is beneficial to jp in XWinograd that is completely unseen during pretraining or fine-tuning.

### 4.3 Language Similarity

We conduct a post-hoc analysis on how language closeness affects cross-lingual transfer. Instead of studying the relation between the number of fine-tuning languages and test set performance, we define an aggregated similarity measure between all languages present in a fine-tuning corpus and a test language $L_{\text {test }}$ :

$$
\text { similarity }_{\text {train, test }}=\sum_{L \in \text { corpus }} \operatorname{sim}\left(L, L_{\text {test }}\right)
$$

where $\operatorname{sim}($,$) is a similarity metric between two languages. We measure "aggregated$ similarity" instead of "average similarity" because we argue that, given their giant sizes, LLMs have the capacity to model all language data in the training set simultaneously.

We adopt different similarity measures based on syntactic, geographic, phonological, genetic, inventory, and featural distances scored by lang2vec (Littell et al. 2017, Malaviya et al., 2017) 1 In addition, we gathered from another source a language closeness score derived from sound (consonants) overlap which is deemed to reflect genetic similarity (Beaufils $\&$ Tomin, 2020) ${ }^{2}$ In total, we test out seven measures, where the similarity is always normalized to between 0 and 1, indicating the least and most similar. The choice of language closeness features is similar to a contemporaneous study on language transferability and similarity (Philippy et al. 2024). For comparison, we provide Pearson correlation coefficients[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=1263&width=1288&top_left_y=282&top_left_x=424)

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=353&width=596&top_left_y=298&top_left_x=431)

(a) et, the 10th mIT language, in XCOPA

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=363&width=593&top_left_y=735&top_left_x=430)

(c) th, the 46th mIT language, in XCOPA

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=360&width=593&top_left_y=1175&top_left_x=430)

(e) my, the 32nd mIT language, in XStoryCloze

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=352&width=596&top_left_y=301&top_left_x=1098)

(b) it, the 20th mIT language, in XCOPA

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=363&width=591&top_left_y=735&top_left_x=1100)

(d) tr, the 48th mIT language, in XCOPA

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=366&width=594&top_left_y=1172&top_left_x=1096)

(f) ru, the 39th mIT language, in XStoryCloze

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-07.jpg?height=356&width=609&top_left_y=1619&top_left_x=758)

(g) ru, the 39 th mIT language, in XWingrad

Figure 3: Accuracy on languages unseen by the base model but seen by the instruction-tuned model. The subcaptions indicate the order of evaluated languages that are added to the instruction languages. The square mark in black indicates the target language evaluated.

between the number of languages and performance. Also, since empirically we observe that the checkpoint with the addition of Korean has an outlier performance, we also compute coefficients without that particular checkpoint.

The following observations are made on the benchmarks. Many factors contribute to the correlation: different languages, different tasks, and different similarity measures. lang2vec genetic features seem to stand out among all language features we test for, usually resulting

![](https://cdn.mathpix.com/cropped/2024_06_04_85b064e912216fe4b4f5g-08.jpg?height=412&width=697&top_left_y=301&top_left_x=714)

Figure 4: Accuracy on languages unseen by the base model but unseen by instruction-tuned model. Only XCOPA and XWinograd have languages under this category.

in a higher correlation than simply the number of languages. We also find that certain languages are affected by other languages more, e.g. th and sw, whereas certain languages have weak correlations with these features, e.g. en and zh. Also, across different test sets, the same language could display distinct behaviours such as ru in XStoryCloze and XWinograd.

|  | et | id | it | sw | ta | th | tr | vi | zh |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| num. lang. | 0.44 | 0.44 | 0.63 | 0.54 | -0.80 | 0.53 | 0.45 | -0.46 | -0.20 |
| num. lang. w/o ko | 0.44 | 0.50 | 0.64 | 0.54 | -0.80 | 0.53 | 0.46 | -0.50 | -0.39 |
| sound correspondence | 0.51 | 0.48 | 0.64 | 0.64 | -0.83 | 0.62 | 0.45 | -0.36 | -0.20 |
| lang2vec featural | 0.46 | 0.45 | 0.63 | 0.56 | -0.81 | 0.55 | 0.45 | -0.44 | -0.19 |
| lang2vec genetic | $\mathbf{0 . 6 7}$ | $\mathbf{0 . 5 8}$ | $\mathbf{0 . 6 7}$ | $\mathbf{0 . 9 3}$ | $\mathbf{- 0 . 8 4}$ | $\mathbf{0 . 8 2}$ | 0.47 | 0.02 | 0.01 |
| lang2vec geographic | 0.43 | 0.46 | 0.64 | $\mathbf{0 . 9 3}$ | -0.80 | 0.55 | 0.45 | -0.45 | 0.01 |
| lang2vec inventory | 0.46 | 0.45 | 0.64 | 0.52 | -0.80 | 0.55 | 0.45 | -0.45 | -0.19 |
| lang2vec phonological | 0.45 | 0.45 | 0.62 | 0.54 | -0.80 | 0.55 | 0.44 | -0.45 | -0.19 |
| lang2vec syntactic | 0.45 | 0.45 | 0.63 | 0.54 | -0.81 | 0.54 | 0.45 | -0.45 | -0.19 |

Table 1: Pearson correlation between XCOPA performance and training data similarity

|  | ar | en | es | hi | id | my | ru | sw | te | zh |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| num. lang. | -0.07 | 0.15 | 0.46 | 0.51 | 0.53 | 0.75 | 0.81 | 0.56 | -0.47 | 0.11 |
| num. lang. w/o ko | 0.08 | $\mathbf{0 . 4 1}$ | $\mathbf{0 . 7 3}$ | $\mathbf{0 . 6 6}$ | 0.63 | 0.75 | 0.86 | 0.56 | $\mathbf{- 0 . 5 3}$ | 0.31 |
| sound correspondence | -0.06 | 0.15 | 0.48 | 0.52 | 0.57 | 0.82 | 0.83 | 0.67 | -0.43 | 0.12 |
| lang2vec featural | -0.05 | 0.15 | 0.47 | 0.51 | 0.53 | 0.77 | 0.83 | 0.58 | -0.46 | 0.13 |
| lang2vec genetic | 0.17 | 0.16 | 0.50 | 0.54 | $\mathbf{0 . 6 6}$ | $\mathbf{0 . 9 6}$ | $\mathbf{0 . 8 7}$ | $\mathbf{0 . 9 6}$ | -0.26 | $\mathbf{0 . 3 7}$ |
| lang2vec geographic | 0.17 | 0.15 | 0.47 | 0.51 | 0.54 | 0.76 | 0.81 | $\mathbf{0 . 9 6}$ | -0.48 | $\mathbf{0 . 3 7}$ |
| lang2vec inventory | -0.06 | 0.15 | 0.46 | 0.51 | 0.54 | 0.76 | 0.83 | 0.55 | -0.46 | 0.13 |
| lang2vec phonological | -0.05 | 0.15 | 0.47 | 0.51 | 0.53 | 0.76 | 0.83 | 0.57 | -0.45 | 0.13 |
| lang2vec syntactic | -0.05 | 0.15 | 0.47 | 0.51 | 0.53 | 0.78 | 0.82 | 0.57 | -0.45 | 0.13 |

Table 2: Pearson correlation between XStoryCloze performance and training data similarity

## 5 Related Work

Multilingual LLMs Multilingual large language models have been trained to handle multiple languages, leveraging shared linguistic features across diverse language data. Many multilingual LLMs have emerged such as BLOOM (Scao et al., 2022), PaLM (Anil et al. 2023), MADLAD-400 (Kudugunta et al., 2024), and MaLA-500 (Lin et al., 2024). The following studies have been conducted to explore the capacity of multilingual LLMs. For example, Ye et al. (2023) examined the multilingual transfer ability of English-centric models compared to multilingual pretrained models, and Yuan et al. (2023) assessed the multilingual

|  | en | fr | jp | pt | ru | zh |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| num. lang. | -0.02 | 0.01 | 0.62 | -0.32 | -0.07 | 0.49 |
| num. lang. w/o ko | -0.03 | 0.00 | 0.66 | -0.35 | -0.07 | $\mathbf{0 . 5 0}$ |
| sound correspondence | -0.01 | -0.01 | 0.66 | -0.33 | -0.06 | 0.45 |
| lang2vec featural | -0.01 | 0.00 | 0.62 | -0.31 | -0.06 | 0.47 |
| lang2vec genetic | -0.02 | -0.08 | $\mathbf{0 . 7 2}$ | -0.35 | -0.05 | -0.31 |
| lang2vec geographic | -0.02 | -0.01 | 0.62 | -0.33 | -0.07 | -0.31 |
| lang2vec inventory | -0.01 | 0.00 | 0.62 | -0.31 | -0.06 | 0.48 |
| lang2vec phonological | -0.01 | 0.01 | 0.63 | -0.32 | -0.06 | 0.48 |
| lang2vec syntactic | -0.02 | 0.00 | 0.62 | -0.32 | -0.06 | 0.47 |

Table 3: Pearson correlation between XWinograd performance and training data similarity

capacity across 101 languages and revealed that existing LLMs exhibit greater multilingual capabilities than anticipated.

Multilingual Instruction Tuning Fine-tuning large multilingual language models has attracted much attention in recent years due to their capability to handle diverse linguistic contexts effectively. Kew et al. (2023) investigated the impact of multilinguality during finetuning on cross-lingual generalization. Chen et al. (2024) explored cost-efficient strategies for multilingual instruction tuning of large language models and compared monolingual and multilingual instruction tuning using different tuning methods. Shaham et al. (2024) studied the number of languages for instruction tuning using up to 11 languages and found that monolingual tuning transfers instruction-following capabilities across languages. Chirkova \& Nikoulina (2024a) explored the effectiveness of zero-shot cross-lingual generation by fine-tuning multilingual pretrained language models on generative tasks. Then, Chirkova \& Nikoulina (2024b) further expanded the study to zero-shot cross-lingual transter of large language models. Both studies emphasized the importance of model configuration. While previous works tried to constrain the amount of training data and squeeze the number of languages, this work attempts to scale up the number of languages, yielding additional findings to (zero-shot) language transfer. In a small-scale experiment, the authors of the Bactrian-X dataset discovered that monolingual models fine-tuned with low-rank adaptation are better than a single multilingual model (Li et al., 2023), which is different from our findings albeit with very different experimental setups.

## 6 Conclusion and Future Work

Instruction fine-tuning of large multilingual language models presents both opportunities and challenges. While it can enable versatile language processing capabilities, it also demands careful handling to address issues related to language-specific nuances. Various studies analyzed multilingualism and cross-lingual transferability in different settings, leading to different conclusions. This paper conducts an experimental analysis on yet another set of settings and reveals different findings from prior works, showing that even with the same base model, instruction data, and training recipe, the performance is still dependent on the evaluation tasks. Building upon the findings of prior works and our study, we conclude that the performance of multilingual instruction fine-tuning is highly dependent on factors such as base models, instruction data, tasks, and evaluation protocols. Our study stresses the importance of the theoretical study on the effectiveness and generalizability of multilingual instruction fine-tuning, as well as the need for more systematic experimental studies to validate the theory.

## Limitations

Our work studies multilingual instruction fine-tuning in 52 languages, which might be small compared to thousands of living languages. We did not conduct a human evaluation
due to limited budget constraints. Future work would be to conduct a more systematic assessment with more rigorously controlled variables during instruction tuning.

## Ethical Considerations

Multilingual instruction fine-tuning also raises ethical questions about the responsible use of AI models, especially when dealing with diverse linguistic and cultural contexts. This paper investigates the challenge in instruction fine-tuning large multilingual language models and learning lessons to facilitate future research to harness the full potential of multilingual models while minimizing potential drawbacks.

## Reproducibility Statement

We make the following efforts to ensure reproducible research. Our base model, instruction dataset, and evaluation benchmarks are all open-source. We release the fine-tuned model weights. Our results are reproducible using the provided model weights and evaluation scripts.

## Acknowledgements

The work has received funding from the European Union's Horizon Europe research and innovation programme under grant agreement No 101070350 and from UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee [grant number 10052546]. Shaoxiong Ji also received funding from UTTER's Financial Support for Third Parties. We acknowledge CSC-IT Center for Science, Finland for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through Finnish extreme scale call (project LumiNMT and MOOMIN) and the call from the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254).

## References

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The Falcon series of open language models. arXiv preprint, 2023.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv preprint, 2023.

Vincent Beaufils and Johannes Tomin. Stochastic approach to worldwide language classification: the signals and the noise towards long-range exploration. SocArXiv, 2020.

Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry Haddow, and Kenneth Heafield. Monolingual or multilingual instruction tuning: Which makes a better alpaca. In Findings of the Association for Computational Linguistics: EACL 2024, 2024.

Nadezhda Chirkova and Vassilina Nikoulina. Key ingredients for effective zero-shot crosslingual knowledge transfer in generative tasks. arXiv preprint, 2024a.

Nadezhda Chirkova and Vassilina Nikoulina. Zero-shot cross-lingual transfer in instruction tuning of large language model. arXiv preprint, 2024b.

Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free Dolly: Introducing the world's first truly open instruction-tuned LLM. https://www.databricks.com. 2023.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation. Zenodo, 2023.

Tannon Kew, Florian Schottmann, and Rico Sennrich. Turning english-centric llms into polyglots: How much multilinguality is needed? arXiv preprint, 2023.

Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. Madlad-400: A multilingual and documentlevel large audited dataset. Advances in Neural Information Processing Systems, 2024.

Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.

Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint, 2023.

Peiqin Lin, Shaoxiong Ji, Jörg Tiedemann, André FT Martins, and Hinrich Schütze. MaLA-500: Massive language adaptation of large language models. arXiv preprint arXiv:2401.13303, 2024.

Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learning with multilingual language models. arXiv preprint, 2021.

Patrick Littell, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 2017.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 2020.

Chaitanya Malaviya, Graham Neubig, and Patrick Littell. Learning language representations for typology prediction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017.

Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, 2017.

Fred Philippy, Siwen Guo, Shohreh Haddadan, Cedric Lothritz, Jacques Klein, and Tegawendé F. Bissyandé. Soft prompt tuning for cross-lingual transfer: When less is more. In Proceedings of the 1st Workshop on Modular and Open Multilingual NLP (MOOMIN 2024), 2024.

Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery $\mathcal{E}$ Data Mining, 2020.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint, 2022.

Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. Multilingual instruction tuning with just a pinch of multilinguality. arXiv preprint, 2024.

Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. mGPT: Few-shot learners go multilingual. arXiv preprint, 2022.

Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint, 2024.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Alexey Tikhonov and Max Ryabinin. It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint, 2023.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint, 2019.

Jiacheng Ye, Xijia Tao, and Lingpeng Kong. Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability. arXiv preprint, 2023.

Fei Yuan, Shuai Yuan, Zhiyong Wu, and Lei Li. How multilingual is multilingual llm? arXiv preprint, 2023.


[^0]:    We release all model weights to Hugging Face at this link

    * Fun fact: Lucky 52 was a famous variety show in China in the late 1990s and early 2000s. In this work, the term is used to denote the 52 languages used in multilingual instruction tuning.

[^1]:    ${ }^{1}$ https://github.com/antonisa/lang2vec

    2 http://www.elinguistics.net/language_evolution.html

