# SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression 

Tim Dettmers ${ }^{* \dagger}$<br>University of Washington

Ruslan Svirschevski ${ }^{*}$<br>HSE University \& Yandex

Saleh Ashkboos<br>ETH Zurich

Vage Egiazarian ${ }^{*}$<br>HSE University \& Yandex

Alexander Borzunov
HSE University \& Yandex
Torsten Hoefler

ETH Zurich

Dan Alistarh<br>IST Austria \& NeuralMagic


#### Abstract

Recent advances in large language model (LLM) pretraining have led to highquality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. $\mathrm{SpQR}$ works by identifying and isolating outlier weights, which cause particularlylarge quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than $1 \%$ in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single $24 \mathrm{~GB}$ consumer GPU without any performance degradation at $15 \%$ speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime ${ }^{3}$. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than $4 x$.


## 1 Introduction

Pretrained large language models (LLMs) improved rapidly from task-specific performance \$\left[\mathrm{WSM}^{+}\right.\$18, DCLT19, RWC \${ }^{+}\$19], to performing well on general tasks if prompted with instructions \$\left[\mathrm{BMR}^{+} 20, \mathrm{WBZ}^{+}\right.\$21, Ope23]. While the improved performance can be attributed to scaling in training data and parameters $\left[\mathrm{KMH}^{+} 20, \mathrm{CND}^{+} 22\right]$ recent trends focused on smaller models trained on more data, that are easier to use at inference time $\left[\mathrm{HBM}^{+} 22, \mathrm{BSA}^{+} 23, \mathrm{TLI}^{+} 23\right]$. For example, the 7B parameter LLaMA model trained on 1T tokens achieved an average performance only slightly lower than GPT-3 \$\left[\mathrm{BMR}^{+}\right.\$20] despite being $25 \mathrm{x}$ smaller. Current techniques for LLM compression can shrink these models further by a factor of about $4 \mathrm{x}$, while preserving their performance[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-02.jpg?height=458&width=1294&top_left_y=229&top_left_x=404)

Figure 1: Compressed LLM performance for LLaMA models. (left) LM loss on WikiText2 vs model size. (right) Average performance on zero-shot tasks vs model size.

[DLBZ22, XLS \${ }^{+}\$22, FAHA22, DZ22]. This yields performance levels comparable to the largest GPT-3 model, with major reductions in terms of memory requirements. With such improvements, well-performing models could be efficiently served on end-user devices, such as laptops.

The main challenge is to compress models enough to fit into such devices while also preserving generative quality. Specifically, studies show that, although accurate, existing techniques for 3 to 4-bit quantization still lead to significant accuracy degradation [DZ22, FAHA22]. Since LLM generation is sequential, depending on previously-generated tokens, small relative errors can accumulate and lead to severely corrupted outputs. To ensure reliable quality, it is critical to design low-bitwidth quantization that does not degrade predictive performance compared to the 16-bit model.

In this work, we introduce Sparse-Quantized Representations (SpQR), a hybrid sparse-quantized format which can compress accurate pretrained LLMs to 3-4 bits per parameter while staying nearlossless: specifically, $\mathrm{SpQR}$ is the first weight quantization method which is able to reach such compression ratios while inducing end-to-end accuracy error as measured in perplexity of less than $1 \%$ relative to the dense baseline. SpQR works by combining two innovations. First, we isolate outlier weights, whose quantization we show to induce disproportionately high errors: these weights are kept in high precision, while the other weights are stored in a much lower, e.g. 3-bit, format. Second, we implement a variant of grouped quantization with very small group size, e.g. 16 contiguous elements, but we show that one can quantize the quantization scales themselves to a 3-bit representation.

To convert a given pretrained LLM into SpQR format, we adopt an extended version of the posttraining quantization (PTQ) approach recently introduced by GPTQ [FAHA22]. Specifically, the method passes calibration data through the uncompressed model; to compress each layer, it applies a layer-wise solver with respect to the L2 error between the outputs of the uncompressed model, and those of the quantized weights. Our approach splits this process into two steps: an "outlier detection" step, in which we isolate weights whose direct quantization has outsize impact on layer output behavior, and an actual compression step, in which most $(\geq 99 \%)$ of weights are compressed to low-bitwidth, the outliers are extracted, and the whole representation is rendered more efficient by further compressing the quantization metadata.

Our method is motivated by a new analysis showing that LLM weight quantization errors exhibit both vertical and horizontal group correlations, corresponding to systematic large errors corresponding to input feature dimensions and output hidden dimensions. While outlier input features have been observed before [DLBZ22, \$\mathrm{XLS}^{+}\$22], our work is the first to demonstrate that similar outliers occur in the weights, for particular output hidden dimensions. Unlike input feature outliers, the output hidden dimension outliers occur only in small segments for a particular output hidden dimension.

Our quantization algorithm isolates such outliers and efficiently encodes a given model in SpQR format. To exploit the resulting structure, we develop a specialized sparse-matrix multiplication algorithm based on the compressed sparse row (CSR) format. To use SpQR for token-by-token generation, we combine this sparse algorithm together with a dense-quantized matrix multiplication for 3-4 bit weights. With this, $\mathrm{SpQR}$ reduces the memory footprint of LLMs by a factor of about $3.4 \mathrm{x}$ or more without degradation in accuracy, measured as language modeling loss or perplexity, while also being $20-30 \%$ faster for LLM generation compared to 16-bit inference.

## 2 Related Work

We focus our discussion on related post-training quantization (PTQ) methods \$\left[\mathrm{NAVB}^{+}\right.\$20], referring the reader to the recent survey of Gholami et al. $\left[\mathrm{GKD}^{+} 21\right]$ for full background on quantization. PTQ methods are a popular approach for one-shot compression of models with various sizes, based on a limited amount of calibration data, using accurate solvers, usually focused on layeror group-wise compression sub-problems. Most PTQ methods, such as AdaRound [NAVB \${ }^{+}\$20], BitSplit [WCHC20], AdaQuant [HNH \${ }^{+}\$21], BRECQ [LGT \${ }^{+}\$21], or OBQ [FSA22] were designed for vision models or small-scale language models, with less than 100M parameters. All these recent approaches tend to use accurate solvers, which would not scale to GPT-scale models in terms of computational or memory cost, as they are 10-1000x larger in size.

Recently, there has been significant interest in obtaining accurate post-training methods that scale to such massive models. Due to computational constraints, early work such as ZeroQuant [ \$\mathrm{YAZ}^{+}\$22], LLM.int8() [DLBZ22], and nuQmm [PPK \${ }^{+}\$22] used direct rounding of weights to the nearest quantization level, while customizing the quantization granularity (i.e., group size) to trade off space for increased accuracy. LLM.int8() [DLBZ22] suggested isolating "outlier features" which would be quantized separately to higher bit-width. These approaches are able to induce relatively low quantization error, e.g. $5.5 \%$ relative LM Loss increase for LLaMA-7B at 4-bit weight quantization, provided that the quantization granularity is low enough. GPTQ [FAHA22] proposed a higheraccuracy approach (e.g., 4\% LM Loss increase in the above setting), which works via an approximate large-scale solver for the problem of minimizing the layer-wise squared error.

Dettmers et al. [DZ22] provided an in-depth overview of the accuracy-compression trade-offs underlying these methods, establishing that 4-bit quantization is an optimal point for round-to-nearest-based methods, whereas higher compression can be achieved via data-aware methods such as GPTQ. SparseGPT [FA23] presented an approach to jointly sparsify LLM weights to medium sparsities, together with quantization of the remaining weights to a fixed given bit-width. One common drawback of existing methods is that the accuracy loss relative to the original model is still significant (see Table 1). This is especially relevant to relatively small but easily deployable models, e.g. in the 7-13B parameter range, where existing methods show drastic accuracy drops. We investigate this question here, and provide a new compression format which can lead to near-lossless 3-4 bits compression in this regime.

A related question is that of performing both activation and weight quantization. There is early work [DLBZ22, $\left.\mathrm{XLS}^{+} 22, \mathrm{YAZ}^{+} 22\right]$, showing that both activations and weights could be quantized to 8 -bits with relatively low accuracy impact. These complementary investigations yield interesting insights into the causes of compression error in the case of LLMs. Specifically, [DLBZ22, XLS \${ }^{+}\$22] observe the presence of "outlier features" with significantly higher values in the input/output of large LLMs, which induce higher quantization error, and propose different mitigation strategies.

We analyze this phenomenon from the point of view of weight quantization. In particular, we investigate the outlier structure, beyond input feature outliers in the weight matrix. While we find that input feature outliers of the current layer are correlated to hidden unit outliers weight in the previous layer there is not a strict correspondence. Such partially-structured outlier patterns necessitate a fine-grained hybrid compression format that goes beyond algorithms that exploit the column structure of outlier features found in previous work.

Hybrid sparse-quantized formats have been investigated generally for deep networks. Some efficient CPU inference engines [Neu22, GFS \${ }^{+}\$19] support a different block sparse-and-quantized format, in which each block of 4 consecutive weights is either completely sparse or quantized to 8 -bit format, whereas GPUs support a similar compound format in which every group of 4 weights contains 2 zero weights, and the non-zero weights could be quantized. The FBGEMM package $\left[\mathrm{KHB}^{+} 21\right]$ proposed a format in which certain "outlier" weights are quantized separately, to reduce their impact on normalization. However, in this format, "outlier" weights are still quantized to exactly the same bit-width (8-bit) as regular weights; moreover, no procedure is given for converting a model to this format post-training. By contrast, 1) we provide an efficient and accurate post-training compression algorithm which identifies outliers as weights inducing high output error, 2) we propose a format compressing outliers to a higher bit-width relative to regular weights, and 3) our format stores outliers in blocks, allowing for efficient implementation of GPU kernels, which we provide as well.

## 3 Quantization sensitivity of LLM weights

### 3.1 Parameter sensitivity under quantization

Not all parameters in a neural network are equally important. Intuitively, a weight could be seen as sensitive to quantization if its rounding error is large, i.e. it is not close to a quantization point, and/or the inputs it is usually multiplied with a large, amplifying even a small rounding error. These simple notions of sensitivity however disregard the fact that LLMs operate on very large vectors with significant correlations: a weight $w_{a}$ may have a large rounding error while being strongly correlated to another weight $w_{b}$, meaning that the error of rounding up $w_{a}$ can be well compensated by rounding down $w_{b}$. This idea is exploited by modern quantization algorithms [FAHA22, YAZ \${ }^{+}\$22] and can lead to major improvements over vanilla rounding, especially a low bitwidths. Properly capturing this aspect of sensitivity requires a more robust definition.

For computational tractability, we assess sensitivity on a per-layer level using a small set of calibration inputs $X$, collected by running them through the model up to the particular layer. We define the sensitivity $s_{i j}$ of some weight $w_{i j}$ in the layer's weight matrix $W$ as the minimum squared difference between the original predictions on $X$ and those of any weight matrix $W^{\prime}$ where this weight is quantized, i.e. $w_{i j}^{\prime}=$ quant $\left(w_{i j}\right)$ :

$$
\begin{equation*}
s_{i j}=\min _{W^{\prime}}\left\|W X-W^{\prime} X\right\|_{2}^{2} \quad \text { s.t. } \quad w_{i j}^{\prime}=\text { quant }\left(w_{i j}\right) \tag{1}
\end{equation*}
$$

Crucially, all weights of $W^{\prime}$ except for $w_{i j}^{\prime}$ may take on arbitrary, not necessarily quantized, values in order to compensate for the quantization error incurred by rounding $w_{i j}$, thus capturing the correlation aspect discussed above. Further, as we allow continuous values, this problem admits a closed-form solution. This can be determined by following the generalized Optimal Brain Surgeon framework [FSA22], where $\left(X X^{\top}\right)^{-1}$ is the inverse Hessian matrix corresponding to the optimization problem:

$$
\begin{equation*}
s_{i j}=\frac{\left(w_{i j}-\text { quant }\left(w_{i j}\right)\right)^{2}}{2\left(X X^{\top}\right)^{-1}} \tag{2}
\end{equation*}
$$

This saliency measure can be approximated efficiently by quantization solvers, such as GPTQ [FAHA22]. In more detail, GPTQ quantizes weight matrices column-by-column while in each step adjusting the not-yet-quantized part to compensate for the quantization error in a similar sense as defined above. Consequentially, instead of statically deciding all sensitivities in advance, they can be computed dynamically as the algorithm processes each column, by using the inverse of the Hessian subselection corresponding to all not yet quantized weights. This matrix is already efficiently computed by GPTQ and thus does not impose any additional overheads. The main advantage of this approach is that $s_{i j}$ is always determined based on the most current value of $w_{i j}$ and thus accounts for adjustments due to previously quantized weights as well.

### 3.2 Exploring parameter sensitivity

Before we define out main method, $\mathrm{SpQR}$, we provide a motivating analysis of parameter sensitivity which uncovers that the location of sensitive weights in the weight matrix are not random but have particular structures. To highlight these structural elements during the quantization process, we calculate the the per-weight sensitivities and visualize them for the popular and highly-accurate LLaMA-65B model \$\left[\mathrm{TLI}^{+}\right.\$23]. As the quantization method, we use GPTQ quantization to 3-bit, without weight grouping, following [FAHA22]. We use \$\mathrm{C} 4\left[\mathrm{RSR}^{+}\right.\$20] as the calibration dataset, and we estimate the error on 128 sequences of 2048 tokens each. Figure 2 depicts the output projection of the last self-attention layer of LLaMA-65B.

Using the sensitivity analysis, we observe several patterns in the weight matrix, often in a single row or column. Since the large weight matrices in LLaMA-65B have too many rows/columuns to be respresentable in a compact image (default: $8 \mathrm{k} \times 32 \mathrm{k}$ pixels) we perform max pooling to visualize the matrices, that is we take the maximum sensitivity in each square of $32 \times 32$ rows and columns. This max pooling only affects the leftmost image. Using this visualization, we observe that the quantization error patterns vary both by layer type, for example attention vs multilayer perceptron (MLP), and layer depth. In particular, we find that more sensitive outliers are present for deeper layers. (Please see Appendix A for additional results.) We now proceed to categorize outlier structures, taking this attention weight matrix as an exemplar. We make the following observations:
![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-05.jpg?height=552&width=1304&top_left_y=230&top_left_x=410)

Figure 2: Weight log-sensitivities from the last attention layer of LLaMA-65B. Dark-blue shades indicate higher sensitivity. The image on the left is a high-level view, resized to $1: 32$ scale with max-pooling. The two images in the middle are zoomed in from the main figure. The two images on the right are taken from other weight matrices.

- Row outliers are shown in Figure 2 bottom-center as regions of high sensitivity within one output unit. Some of these patterns span the entire row, while others are partial. In attention layers, some of the partial row outliers correspond to some subset of attention heads. Column outliers appear in Figure 2, bottom-right, showing high sensitivity in select input dimensions (columns) across all rows. The latter are correlated to the "outlier feature" phenomenon reported in Dettmers et al. [DLBZ22].
- Sensitive attention heads. (Figure 2, top-center) - regular stripes of width 128 highlight all weights corresponding to one attention head. This could be related to some attention heads having more important functions [VTM ${ }^{+} 19$, Vig \$19, \mathrm{OEN}^{+}\$22]. The corresponding "stripes" are horizontal for attention $\mathrm{Q} \& \mathrm{~K}$ projections, vertical in output projection, and absent from value projections and any MLP weights. Of note, there is significant variation in individual weight sensitivity even within the sensitive heads.
- The Rotary embedding pattern, a repeating vertical pattern of sensitivity with a period of 64 units. We attribute this to the use of rotary embeddings [SLP \${ }^{+}\$21]: each attention head $(\operatorname{dim}=128)$ is split into two halves: the first 64 are "rotated" with cosine, and the other 64 use sine. Both sine and cosine rotation use the same set of frequencies. Typically, the weights that correspond to low-frequency sines and cosines are more sensitive than their high-frequency counterparts, as shown in Figure 2 (top-right). As expected, this pattern is absent from any layer not using rotary embeddings.
- Unstructured outliers. Besides the above, each layer has a number of individual sensitivity weights that do not fit into any of the above patterns. These unstructured outliers occur more frequently for columns with largest input index (i.e. on the right side of the images). This effect is difficult to see on a heatmap, so we provide additional figures and statistical tests in Appendix A. We believe is probably an artefact of the GPTQ algorithm, which compresses one by one, using yet-uncompressed weights to compensate the error. Thus, the rightmost batch of weights accumulates the most error.

Next, we will leverage these findings to propose a compressed representation which can support all these different outlier types.

## 4 SpQR: A Sensitivity-aware compressed representation

### 4.1 Overview

Existing LLM quantization algorithms treat low- and high-sensitivity weights equally; however, our above discussion suggests that this may lead to sub-optimal quantization. Ideally, we would want the representation to assign more of its "size budget" to sensitive weights. However, these weights
are scattered in the weight matrix as either individual weights or small groups, for example, partial rows or attention head. To capture this structure, we are introducing two changes to the quantization procedure: one for capturing small sensitive groups, and another for capturing individual outliers.

Capturing small groups of weights with bilevel quantization. In the previous section, we observed several cases where weights behave similarly in small consecutive groups, with abrupt changes between groups, for example for some attention head and partial row outliers (see Figure 4 left, bottom-center). When applying a standard approach, there will be many cases where these weights will be grouped together, sharing the same quantization statistics. To reduce the number of such cases, we use groupwise quantization with extremely small groups, typically of $\beta_{1}=8-32$ weights. That is, for every $\beta_{1}$ consecutive weights, there is a separate quantization scale and zero-point. This choice runs contrary to current intuition: for instance, the recent work of Yao et al. [YLW \${ }^{+}\$23] explicitly recommends against small groups, arguing that the overhead for storing quantization statistics would outweigh the precision advantages.

To circumvent this issue, we quantize the groupwise statistics themselves using the same quantization algorithm as for weights - asymmetric (min-max) quantization. Because of how min-max quantization works, the range of quantized values will fit to the groups with largest (or smallest) quantization scale, quantizing them perfectly. In other words, we group groupwise statistics from $\beta_{2}=16$ consecutive values and quantize them together in the same number of bits, such that groups with atypical quantization parameters end up using more of the "quantization budget". Finally, both first and second-level quantization is directly within the quantization process, allowing the algorithm to compensate the second-level quantization error where possible.

High-sensitivity outliers. Our analysis showed the existence of cases where a small percentage of sensitive weights come in small groups (in the self-attention) or individual "outliers" (in the MLP). In some cases, $1 \%$ of the weights account for over $75 \%$ of the total quantization error. Since these weights appear to lead to high, irreducible error, we choose to keep these outliers in high precision (16-bit). As these outliers are often unstructured, we encode them individually in a rowwise arrangement similar to a compressed-sparse-row (CSR) representation \$\left[\mathrm{HABN}^{+}\right.\$21]. This can encode both individual outliers and small structures that do not fit into the above definition of groups.

The procedure for detecting the outliers is described in detail in Alg. 1. If follows a rough two-step procedure: (1) find and isolate outliers as 16-bit weights, (2) quantize the non-outlier "base" weights into 3-4 bit and transfer the remaining quantization into the the 16-bit outliers weights. For the outlier isolation step, the algorithm implements a filtering technique based on the sensitivity criterion in Eq. (2), which is used to isolate and separate outliers from base weights. Globally, for each matrix, the algorithm aims to pick a sensitivity threshold $\tau$ to obtain the desired number of outliers across the whole model, usually around $1 \%$ of weights. Specifically, a particular weight is considered an outlier if keeping the weight in 16 -bit reduces the error in Eq. (2) by at least $\tau$.

Following this first outlier detection step, we quantize the base weights ignoring all outliers that occur in the same quantization group. As such, the quantization statistics (e.g. scales) are computed by excluding outliers. This results in significant improvements in terms of error, since e.g. the min-max scales will be significantly reduced. The algorithm then proceeds to apply GPTQ to quantize the remaining weights. Interestingly, unlike [DLBZ22], a weight can be chosen to be an outlier not only if it causes error by itself, but also if the GPTQ algorithm can employ this weight to compensate errors from many other weights. Thus, the resulting 16 -bit value will contain not the original weight, but a weight that was adjusted to minimize the output error. As such, $\mathrm{SpQR}$ goes beyond mere detection of outliers towards the more general notion of isolating and treating outliers that occur during the quantization process. Finally, the algorithm gathers and compresses sparse outlier matrix as well as the final quantization statistics with bilevel quantization and returns the compressed weights and their metadata.

Implementation details. Our algorithm also contains several optimizations. As we are using small group sizes, it is often the case that a group contains all positive (or all negative) values. Standard quantizers [FSA22, FAHA22] require the maximum value to be positive and the minimum value to be negative. For small group sizes, removing this requirement results in slightly better quality. As a by-product of quantizing the quantization statistics, our algorithm allows non-integer zero points. We ablate these and other $\mathrm{SpQR}$ components in Section 5.

```
Algorithm 1 SpQR quantization algorithm: the left snippet describes the full procedure, the right
side contains subroutines for bilevel quantization and finding outliers.
func $\operatorname{SPQRQUANTIZE~}\left(W, X, b, \beta_{1}, \beta_{2}, \tau, \lambda\right)$
func fit_quantizer $(M, \beta)$
Input: $W \in \mathcal{R}^{m \times n}$ - weight matrix,
$\vec{m}:=$ flatten $(M)$
$X \in \mathcal{R}^{n \times d}$ _ calibration data,
$\vec{s}, \vec{z}:=$ vectors ()
$b$ - the base number of quantization bits,
for $i=1, \beta_{1}, 2 \beta_{1}, \ldots \operatorname{dim}(\mathrm{m})$ do
$\beta_{1}, \beta_{2}$ - quantization group sizes,
$s_{i}:=\frac{\max \left(\vec{m}_{i: i+\beta}\right)-\min \left(\vec{m}_{i: i+\beta}\right)}{2^{b}-1}$
$\tau$ - sensitivity outlier threshold
$z_{i}:=-\min \left(\vec{m}_{i: i+\beta}\right) / s_{i}$
$\lambda$ - hessian regularizer,
return $\vec{s}, \vec{z}$
$E:=$ float_matrix $(m, n) \quad / / \mathrm{L} 2$ error
error $\left(W, H^{\text {ic }}\right)$
$H:=2 X X^{T} \quad / / \mathrm{L} 2$ error hessian, $\mathcal{R}^{n \times n}$
$\vec{s}, \vec{z}:=$ fit_quantizer $\left(W, \beta_{1}\right)$
$H^{\mathrm{ic}}:=\operatorname{Cholesky}\left((H+\lambda \mathbf{I})^{-1}\right)$
$W_{q}:=$ quantize $(W, \vec{s}, \vec{z})$
$Q:=$ int_matrix $(m, n) \quad / /$ quantized weight
$E:=\left(W-W_{q}\right) / H^{\text {ic }}$
$\mathcal{O}:=\emptyset \quad / /$ a set of all outliers
return $E^{2}$
$\mathcal{S}:=\emptyset \quad / /$ a set of quantization statistics
c outliers $\left(W, H^{\text {ic }}, \mathcal{O}\right)$
for $i=1, \beta_{1}, 2 \beta_{1}, \ldots n$ do
$E_{\text {base }}=\operatorname{error}\left(W, H^{\text {ic }}\right)$
$W_{:, i: i+\beta_{1}}, \mathcal{O}:=$ outliers $\left(W_{:, i: i+\beta_{1}}, H_{i:\left(i+\beta_{1}\right), i:\left(i+\beta_{1}\right)}^{\text {ic }} \mathcal{O}\right)$
for $i=1, \ldots, \beta_{1}$ do
$\hat{s}, \hat{z}, \mathcal{S}:=$ fit_statistics $\left(W_{:, i: i+\beta_{1}}, \mathcal{S}, \mathcal{O}\right)$
$l o o:=\left\{1,2, \ldots, \beta_{1}\right\} /\{i\}$
for $j=i, \ldots, i+\beta_{1}$ do
$E_{\mathrm{ol}}=\operatorname{error}\left(W_{:, \text {loo }}, H_{\text {loo,loo }}^{\text {ic }}\right)$
$Q_{:, j}:=$ quantize $\left(W_{:, j}, \hat{s}, \hat{z}\right)$
$I_{o}=\operatorname{select}\left(E_{\mathrm{base}}-E_{\mathrm{ol}}>\tau\right)$
$\vec{w}_{q}:=$ dequantize $\left(Q_{:, j}, \hat{s}, \hat{z}\right)$
$\mathcal{O}:=\mathcal{O} \cup I_{o}$
$E_{:, j}:=\left(W_{:, j}-\vec{w}_{q}\right) / H_{j, j}^{\text {ic }} \cdot\left(1-\right.$ is_outlier $\left.\left(W_{:, j}, \mathcal{O}\right)\right)$
return $W, \mathcal{O}$
$W_{:, j:\left(i+\beta_{1}\right)}:=W_{:, j:\left(i+\beta_{1}\right)}-E \cdot H_{j, j:\left(i+\beta_{1}\right)}^{\text {ic }}$
unc fit_statistics $(W, \mathcal{S}, \mathcal{O})$
$W_{:,\left(i+\beta_{1}\right): n}:=W_{:,\left(i+\beta_{1}\right): n}-E \cdot H_{i:\left(i+\beta_{1}\right), i:\left(i+\beta_{1}\right)}^{\mathrm{ic}}$
$W:=W \cdot(1-$ is_outlier $(W, O))$
$S_{q}, Z_{q}, S_{s}, Z_{s}, S_{z}, Z_{z}:=$ gather_statistics $(\mathcal{S})$
$\vec{s}, \vec{z}:=$ fit_quantizer $\left(W, \beta_{1}\right)$
$/ / \vec{s}$ for scales, $\vec{z}$ for zero points
$\vec{s}_{s}, \vec{z}_{s}:=$ fit_quantizer $\left(\vec{s}, \beta_{2}\right)$
$\vec{s}_{z}, \vec{z}_{z}:=$ fit_quantizer $\left(\vec{z}, \beta_{2}\right)$
$\vec{s}_{q}:=$ quantize $\left(\vec{s}, \vec{s}_{s}, \vec{z}_{s}\right)$
$\vec{z}_{q}:=$ quantize $\left(\vec{z}, \vec{s}_{z}, \vec{z}_{z}\right)$
$\mathcal{S}:=\mathcal{S} \cup\left\{s_{q}, s_{s}, s_{z}, z_{q}, s_{z}, z_{z}\right\}$
func dequantize $(Q, \vec{s}, \vec{z})$
$\hat{s}:=$ dequantize $\left(s_{q}, s_{s}, s_{z}\right)$
$\hat{z}:=$ dequantize $\left(z_{q}, z_{s}, z_{z}\right)$
return $\vec{s} \cdot(Q-\vec{z})$
1: return $\hat{s}, \hat{z}, \mathcal{S}$
```

![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-07.jpg?height=401&width=1414&top_left_y=1575&top_left_x=342)

input dimension (8192)

![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-07.jpg?height=344&width=650&top_left_y=1622&top_left_x=347)

weight matrix split into $\beta_{1} \times \beta_{2}$ blocks

![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-07.jpg?height=273&width=265&top_left_y=1647&top_left_x=1014)

quantized weights

![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-07.jpg?height=300&width=182&top_left_y=1628&top_left_x=1275)

scales and zeros

![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-07.jpg?height=344&width=274&top_left_y=1622&top_left_x=1465)

Figure 3: A high-level overview of the $\mathrm{SpQR}$ representation for a single weight tensor. The right side of the image depicts all stored data types and their dimensions.

### 4.2 Implementing and Leveraging the Sparse Quantized Representation

Our algorithm converts homogeneous weights into several data structures of various sizes and precisions. Overall, the representation consists of (1) quantized weights, (2) first level quantized quantization statistics, second level quantization statistics, and (3) the CSR outlier indices and values. We summarize the overall structure of $\mathrm{SpQR}$ in Figure 3 and describe each component below.

Storing quantized groups. All non-outlier weights are encoded as a structure that contains:

- a $b_{w}$-bit individual weight;
- a $b_{q}$-bit scale and zero point for each group of size $B$;
- 16-bit statistics for quantizing groups of $B_{q}$ quantization scales and zero-points.

As a particular example for a $\mathrm{SpQR}$ representation, consider $b_{w}=b_{q}=3$ and $B_{w}=B_{q}=16$. The weight matrix is split into groups of $B_{q} \times B_{w}=256$ weights. A group contains 256 individual $b_{w}=3$-bit codes. Every 16 weights use a separate 3-bit scale and zero-point. Finally, there are four 16-bit scalars for the entire group used for second level quantization. To simplify GPU memory access, we keep the quantized values for outlier weights in place and adjust the 16-bit versions to compensate for that. We also store both quantized weights and quantized quantization statistics in a contiguous memory region for each group. When running on a different hardware (e.g. mobile CPUs), it is possible to further reduce the memory footprint by removing the quantized version of outliers. We leave this direction for future work.

Storing outliers. Recall that our outliers are unstructured; for storage, we sort them by their row first and column second, so that outliers in the same row are contiguous in memory. For each outlier, we store two scalars: the 16 -bit weight value and the 16-bit column index. For each row, we also store a single 32-bit number-the total number of outliers in the rows up to the current one for efficient inference. This results in an average storage cost of 32.03 to 32.1 bits per sensitive weight. This could be reduced significantly by grouping outliers, which we leave as future work.

Inference with SpQR. To illustrate the practicality of our approach, we design an efficient GPUbased decoding implementation for the $\mathrm{SpQR}$ format, focused on the popular token-by-token LLM generation as a use-case.

We leverage the fact that autoregressive inference on GPUs is memory-bound, so high compression rates can hide decoding overheads, to a significant extent. At a high level, our algorithm loads group statistics and the quantized weights into shared memory (SRAM), dequantizes to 16-bits, and then performs matrix multiplication with 16-bit inputs. For handling outliers, we design a sparse matrix algorithm that takes advantage of outliers that occur in rows. Roughly, the algorithm works as follows

First, (1) we divide the matrix into equally sized blocks. Then, each GPU core (thread block) (2) loads a large slice of outliers into shared memory (SRAM), and each GPU core (3) determines if outliers are part of the segment or not. The corresponding weights are (4) loaded from main memory; finally, the matrix multiplication is performed.

This algorithm essentially performs load balancing through steps (1-3), while step (4) tends to have contiguous memory access due to the row-like patterns for the outliers. We will show in Section 5 that this custom approach is faster than the sparse matrix algorithms in PyTorch.

## 5 Experimental Validation

Experimental setup. We focus on three main settings: 1) evaluating what is the most compact representation with which $\mathrm{SpQR}$ can replicate the performance of a 16 -bit model within $1 \%$ perplexity, 2) controlling for the average number of bits per parameter across methods and assess the performance of SpQR compared to round-to-nearest and GPTQ baselines, 3) what is the best trade-off in terms of model size and performance. For these settings, we evaluate the full $\mathrm{SpQR}$ algorithm on publicly-available LLMs. We focus on the LLaMA $\{7,13,30,65\}$ B model family [TLI \${ }^{+}\$23] and Falcon $\{7,40\}$ B model family [UAE23a]. We quantize LLaMa models using the RedPajama dataset and Falcon models on RefinedWeb dataset [UAE23b], publicly-available replicas of the LLaMA and Falcon training data, respectively. In addition, we provide perplexity results for OPT models in Appendix F.

We compare SpQR against two other post-training quantization schemes: GPTQ [FAHA22] and simple rounding-to-nearest (RTN) quantization, which is used by most other LLM compression methods [DLBZ22, \$\mathrm{YAZ}^{+}\$22]. Both baselines use 4-bit quantization since it provides the best quality to size trade-off [DZ22]. For SpQR, we consider both 3-bit and 4-bit base quantization, though the resulting model size can be slightly larger due to the presence of outliers.

We evaluate quantized model performance by two metrics. Firstly, we measure perplexity, measured on the WikiText2 [MXBS16], Penn Treebank $\left[\mathrm{MKM}^{+} 94\right]$ and \$\mathrm{C} 4\left[\mathrm{RSR}^{+}\right.\$20] datasets. Secondly, we measure zero-shot accuracy on five tasks: WinoGrande [SBBC21], PiQA [TP03], HellaSwag, ARC-easy and ARC-challenge \$\left[\mathrm{CCE}^{+}\right.\$18]. We use the LM Evaluation Harness [GTB \${ }^{+}\$21] with

LLaMa

| Size | Method | Avg bits | Wiki2 | C4 | PTB |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 7B | - | 16.00 | 5.68 | 7.08 | 8.80 |
|  | SpQR | 4.63 | 5.73 | 7.13 | 8.88 |
|  | RTN | 4 | 6.43 | 7.93 | 10.30 |
|  | GPTQ | 4 | 6.13 | 7.43 | 9.27 |
|  | SpQR | 3.94 | 5.87 | 7.28 | 9.07 |
| 13B | - | 16.00 | 5.09 | 6.61 | 8.07 |
|  | SpQR | 4.63 | 5.13 | 6.64 | 8.13 |
|  | RTN | 4 | 5.55 | 6.98 | 8.65 |
|  | GPTQ | 4 | 5.40 | 6.84 | 8.44 |
|  | SpQR | 3.96 | 5.22 | 6.72 | 8.22 |


| Size | Method | Avg bits | Wiki2 | C4 | PTB |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 30 | - | 16.00 | 4.10 | 5.98 | 7.30 |
|  | $\mathrm{SpQR}$ | 4.69 | 4.14 | 6.01 | 7.33 |
|  | RTN | 4 | 4.57 | 6.34 | 7.75 |
|  | GPTQ | 4 | 48 | 6.20 | 7.54 |
|  | $\mathrm{SpQR}$ | 3.89 | 4.25 | 6.08 | 7.38 |
| 65B | _ | 16.0 | 3.53 | 5.62 | 6.91 |
|  | $\mathrm{SpQR}$ | 4.71 | 3.57 | 5.64 | 6.93 |
|  | $\mathrm{R}^{\prime}$ | 4 | 3.87 | 5.8 | 7.17 |
|  | GPT( | ![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-09.jpg?height=43&width=126&top_left_y=705&top_left_x=1291) | 3.83 | 5.8 | 7.07 |
|  | SpQR | 3.90 | 3.68 | 5.70 | 6.99 |

Table 1: Perplexity on WikiText2 [MXBS16], C4 [RSR \${ }^{+}\$20] and Penn Treebank \$\left[\mathrm{MKM}^{+}\right.\$94] for $\mathrm{SpQR}$ and round-to-nearest (RTN) and GPTQ baselines with LLaMa. We can see that SpQR reaches performances within $1 \%$ of the perplexity with less than 4.71 bits per parameter. We also see that for 4-bits per parameter SpQR significantly improves on GPTQ with an improvement as large as the improvement from RTN to GPTQ.

recommended parameters. We provide full configurations in Appendix B, as well as code which we plan to release publicly. Our implementation takes around 4.5 hours on the largest model size (65B) on an NVIDIA A100 and about 6 on an A6000.

To control for model size, we evaluate RTN and GPTQ with 4-bit base quantization. For SpQR we use 3-bit base quantization, a group size of 8 with 3-bit for the first quantization, a group size of 64 for the second quantization, and as many outliers as possible to still reach less than 4-bits per parameter on average. We aim to achieve near-lossless compression, for which we adopt the definition of the MLCommons benchmark \$\left[\mathrm{RCK}^{+}\right.\$20]: $1 \%$ error relative to the uncompressed baseline. In all $\mathrm{SpQR}$ evaluations, we choose $\tau$ such that the proportion of outliers is under $1 \%$.

Main Results. Figure 1 measures actual model size versus perplexity on LLaMa models on WikiText2, and accuracy on zero-shot tasks. We observe that SpQR outperforms GPTQ (and correspondingly RTN) at similar model size by a significant margin, especially on smaller models. This improvement comes from both $\mathrm{SpQR}$ achieving more compression, while also reducing loss degradation. In addition, if we measure the bits per parameter needed to come within $1 \%$ of the 16 -bit performance in terms of perplexity, Figure 1 shows that $\mathrm{SpQR}$ with 4.6 to 4.71 bits per parameter approaches the non-quantized models with at most $1 \%$ margin of error for all models (see Table 1 and Table 2 for exact values).

The second set of results, presented in Table 1 for LLaMa and Table 2 for Falcon family models, controls model size by comparing SpQR and baseline methods with 4 bits per parameter. These results show that $\mathrm{SpQR}$ improves over previous methods, with the gap between $\mathrm{SpQR}$ and the next best method GPTQ being as large as the improvement of GPTQ over naive RTN. For 4-bit, SpQR halves the error relative to the 16 -bit baseline compared to GPTQ.[^1]

Falcon

| Size | Method | Avg bits | Wiki2 | $\mathrm{C} 4$ | PTB | Size | Method | Avg bits | Wiki2 | $\mathrm{C} 4$ | PTB |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 7B | - | 16.00 | 6.59 | 9.50 | 9.90 | 40B | - | 16.00 | 5.23 | 7.76 | 7.83 |
|  | SpQR | 4.44 | 6.64 | 9.58 | 9.97 |  | $\mathrm{SpQR}$ | 4.46 | 5.26 | 7.79 | 7.86 |
|  | RTN | 4 | 8.73 | 12.56 | 13.76 |  | RTN | 4 | 6.52 | 9.76 | 10.63 |
|  | GPTQ | 4 | 6.91 | 9.93 | 10.33 |  | GPTQ | 4 | 5.36 | 7.95 | 8.01 |
|  | $\mathrm{SpQR}$ | 3.92 | 6.74 | 9.70 | 19.114 |  | SpQR | 3.90 | 5.29 | 7.85 | 7.91 |

Table 2: Perplexity on WikiText2 [MXBS16], C4 [RSR \${ }^{+}\$20] and Penn Treebank [MKM \${ }^{+}\$94] for $\mathrm{SpQR}$ and round-to-nearest (RTN) and GPTQ baselines on Falcon model. We can see that SpQR reaches performances within $1 \%$ of the perplexity with less than 4.5 bits per parameter. We also see that for 4-bits per parameter SpQR significantly improves on GPTQ with an improvement as large as the improvement from RTN to GPTQ.

| Name | Wiki2 | C4 | PTB | Avg bits |
| :--- | :---: | :---: | :---: | :---: |
| Uncompressed | 3.53 | 5.62 | 6.91 | 16 |
| GPTQ (4 bit) | 3.83 | 5.80 | 7.07 | 4 |
| 3-bit statistics | 3.74 | 5.73 | 7.02 | 3.63 |
| 16-bit statistics | 3.84 | 5.83 | 7.12 | 3.67 |
| Round zero | 3.75 | 5.76 | 7.01 | 3.63 |
| w/o act order | 3.74 | 5.76 | 7.05 | 3.63 |

Table 3: Perplexity for LLaMA-65B model.

![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-10.jpg?height=371&width=631&top_left_y=885&top_left_x=1148)

Figure 4: Different outlier types, LLaMA-65B.

MSE between layer predictions) and treat the entire rows as 16 -bit outliers. We compare the three outlier types on top of 3-bit $\mathrm{SpQR}$ and report the results in Figure 4. Overall, unstructured outliers reduce perplexity significantly faster than their row counterpart and the criterion of [DZ22], even after accounting for the different memory footprint.

Finally, we analyze the impact of the minor hyperparameter changes that we introduced at the end of Section 4. In Table 3 (bottom), we evaluate quantization errors without these changes. The "Round zero" entry corresponds to a version of $\mathrm{SpQR}$ where the zero-point is a 3-bit integer. This reduces the memory footprint of $\mathrm{SpQR}$, but results in a moderate increase in perplexity. Similarly, we evaluate $\mathrm{SpQR}$ without the "act order" flag. This option re-orders the input dimensions by the diagonal of the inverse hessian, which was introduced as a part of the GPTQ algorithm. Using this heuristic slightly improves loss, though not as much as from quantized groups.

To summarize, both small quantized groups and unstructured outliers independently improve perplexity and perform better than alternative strategies. SpQR also benefits from using the GPTQ activation order heuristic, though the gain is smaller than from outliers or small groups. Still, we opt to use the same activation order heuristic in the GPTQ baselines to ensure a fair comparison. To further explore the design space of $\mathrm{SpQR}$, we provide an additional hyperparameter study in Appendix C.

Inference Time. Finally, we evaluate the inference speed of $\mathrm{SpQR}$ for autoregressive inference with a focus on measuring the token generation latency with batch size 1 on a single A100 GPU. We measure inference speed in two setups: i) generating 100 tokens from scratch and ii) adding 100 tokens on top of a 1024-token prefix (prompt). We compare our specialized sparse matrix multiplication algorithm with the algorithm implemented in PyTorch (cuSPARSE). We also compare against a 16-bit baseline. We measure the end-to-end latency as inference steps per second for the full $\mathrm{SpQR}$ algorithm, that is for both the dense and sparse multiplication part together.

Results are shown in Table 4. We can see that while standard sparse matrix multiplication in PyTorch is not faster than 16-bit inference, our specialized sparse matrix multiplication algorithm yields speedups of about $20-30 \%$.

| Method <br> IJ_aMA | fp16 (baseline) |  |  |  | SpQR (PyTorch) |  |  |  | SpQR (optimized) |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA | $7 \mathrm{~B}$ <br> $47+23$ | $13 B$ <br> $37+08$ | $30 B$ <br> $10+11$ | 65B | $7 B$ <br> $30+22$ | $13 \mathrm{~B}$ <br> $24+12$ | $30 \mathrm{~B}$ <br> $88+0.4$ | 65B | {f05e5220a-3bc8-43a2-9545-9e085d649a3d}$7 \mathrm{~B}$ <br> $57+21$\right. | ![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-11.jpg?height=52&width=107&top_left_y=269&top_left_x=1425) | ![](https://cdn.mathpix.com/cropped/2024_06_04_a6d82b51079fde65d04fg-11.jpg?height=52&width=114&top_left_y=269&top_left_x=1528) | 65B |
| prefix 1024 | $46 \pm 2.4$ | $31 \pm 0.9$ | $17 \pm 0.8$ | OOM | $27 \pm 1.6$ | $21 \pm 1.1$ | $6.5 \pm 0.7$ | OOM | $55 \pm 2.1$ | $\mathbf{3 7} \pm 0.8$ | $22 \pm 1.3$ | $\mathbf{1 1} \pm 0.6$ |

Table 4: Inference speed comparison (tokens/s), OOM means the model did not fit in an A100 GPU. We see that our optimized $\mathrm{SpQR}$ algorithm is faster than the 16-bit baseline and almost $2.0 \mathrm{x}$ faster than quantized matrix multiplication + standard PyTorch sparse matrix multiplication baseline.

## 6 Discussion \& Limitations

We have presented $\mathrm{SpQR}$, an quantization approach which quantizes sensitive outliers in higher precision, to achieve near-lossless 16-bit accuracy with less than 4.75 bits per parameter on average. We achieve even better quality-size-tradeoff when compressing to as little as 3.36 bits which makes $\mathrm{SpQR}$ an ideal method for compressing models for memory-limited devices. Despite our promising results, there are several limitations. The main limitation is that we do not evaluate the generative quality of quantized LLMs, but only the predictive performance in terms of zero-shot accuracy and perplexity. While we believe that perplexity measurements and generation quality are strongly related, this is a hypothesis we aim to investigate in future work. While we devise a sparse matrix multiplication algorithm to accelerate the computation with outliers, another limitation is that we do not fuse sparse matrix multiplication with regular quantized matrix multiplication. Such an approach would yield even better inference time performance. However, such an approach is also very difficult to implement. We leave the implementation of such an algorithm to future work.

## 7 Acknowledgements

D.K. was supported by Russian Science Foundation, grant 21-11-00373. D.A. and E.F. gratefully acknowledge funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 805223 ScaleML). Authors also thank Ivan Komarov for his help in profiling and understanding the performance bottlenecks of $\mathrm{SpQR}$ on GPU.

## References

\$\left[\mathrm{BMR}^{+}\right.\$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.

\$\left[\mathrm{BSA}^{+}\right.\$23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.

\$\left[\mathrm{CCE}^{+}\right.\$18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

\$\left[\mathrm{CND}^{+}\right.\$22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pretraining of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.

[DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.

[DZ22] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.

[FA23] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.

[FAHA22] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[FSA22] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal Brain Compression: A framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022. Accepted to NeurIPS 2022, to appear.

\$\left[\mathrm{GFS}^{+}\right.\$19] Yury Gorbachev, Mikhail Fedorov, Iliya Slavutin, Artyom Tugarev, Marat Fatekhov, and Yaroslav Tarkan. Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0-0, 2019.

\$\left[\mathrm{GKD}^{+}\right.\$21] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.

\$\left[\mathrm{GTB}^{+}\right.\$21] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.

\$\left[\mathrm{HABN}^{+}\right.\$21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprint arXiv:2102.00554, 2021.

\$\left[\mathrm{HBM}^{+}\right.\$22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

\$\left[\mathrm{HNH}^{+}\right.\$21] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning (ICML), 2021.

\$\left[\mathrm{KHB}^{+}\right.\$21] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615, 2021.

\$\left[K M H^{+}\right.\$20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

\$\left[\mathrm{LGT}^{+}\right.\$21] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations (ICLR), 2021.

[MKM \${ }^{+}\$94] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.

[MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

\$\left[\mathrm{NAVB}^{+}\right.\$20] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.

[Neu22] NeuralMagic. DeepSparse, 2022.

\$\left[\mathrm{OEN}^{+}\right.\$22] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

[Ope23] OpenAI. Gpt-4 technical report. arXiv, 2023.

\$\left[\mathrm{PGM}^{+}\right.\$19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Conference on Neural Information Processing Systems (NeurIPS). 2019.

\$\left[\mathrm{PPK}^{+}\right.\$22] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuQmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.

\$\left[\mathrm{RCK}^{+}\right.\$20] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, et al. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 446-459. IEEE, 2020 .

\$\left[\mathrm{RSR}^{+}\right.\$20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.

\$\left[\mathrm{RWC}^{+}\right.\$19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[SBBC21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, 2021.

[SLP \${ }^{+}\$21] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

[TLI \${ }^{+}\$23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[TP03] Sandeep Tata and Jignesh M Patel. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.

[UAE23a] TII UAE. The falcon family of large language models. https://huggingface.co/ tiiuae/falcon-40b, May 2023.

[UAE23b] TII UAE. The refined web dataset. https://huggingface.co/datasets/tiiuae/ falcon-refinedweb, May 2023.

[Vig19] Jesse Vig. A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714, 2019.

\$\left[\mathrm{VTM}^{+}\right.\$19] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, Florence, Italy, July 2019. Association for Computational Linguistics.
\$\left[\mathrm{WBZ}^{+}\right.\$21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.

[WCHC20] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate posttraining network quantization via bit-split and stitching. In International Conference on Machine Learning (ICML), 2020.

[WMR \${ }^{+}\$21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models, 2021.

\$\left[\mathrm{WSM}^{+}\right.\$18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[XLS \${ }^{+}\$22] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.

\$\left[\mathrm{YAZ}^{+}\right.\$22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.

[YLW \${ }^{+}\$23] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study on post-training quantization for large language models, 2023.
