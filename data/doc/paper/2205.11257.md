# MANIFOLD-ALIGNED NEIGHBOR EMBEDDING 

Mohammad Tariqul Islam, Jason W. Fleischer*<br>Department of Electrical and Computer Engineering<br>Princeton University<br>Princeton, NJ 08544<br>\{mtislam,jasonf\}@princeton.edu


#### Abstract

In this paper, we introduce a neighbor embedding framework for manifold alignment. We demonstrate the efficacy of the framework using a manifold-aligned version of the uniform manifold approximation and projection algorithm. We show that our algorithm can learn an aligned manifold that is visually competitive to embedding of the whole dataset.


## 1 INTRODUCTION

Unsupervised clustering algorithms, like t-distributed stochastic neighbor embedding (tSNE) (Maaten \& Hinton, 2008) and uniform manifold approximation and projection (UMAP) (McInnes et al., 2018), map points in high-dimensional data space to a more visualizable low-dimensional space. They are growing in popularity, as the learning is unsupervised and the resulting clusters are often interpretable. However, the methods break down when points are taken from disparate datasets, as there is no link for pairwise comparison as the neighborhood graph networks are disconnected. The problem of mapping correlated points in unrelated ways, known as manifold alignment (Ma \& Fu, 2012), occurs whenever post-analysis data is added, e.g., for longitudinal studies, data compilations, and multi-modal measurements. It also arises with privacy and proprietary concerns, when source data cannot be transferred off-site. In each case, silos of datasets are created that cannot be part of a central representation learning scheme.

Previous attempts at manifold alignment have used a variety of methods, including semi-supervised learning (Ham et al., 2005), spectral techniques (Wang \& Mahadevan, 2009), and Procrustes analysis (Wang \& Mahadevan, 2008). While the former uses known labels or bases for correlation, the latter is topological and integrates well with UMAP. Nevertheless, Procrustes transformations are not sufficient to align shared information among the datasets. This is because, Procrustes transformation is composed of various linear transforamtions which work on all the samples as an envelope of the data instead of individual samples. Here, we overcome this issue by iteratively optimizing the neighbor analysis for each dataset and jointly embedding the shared points. We mathematically characterize the new approach and experimentally validate it on several widely used datasets.

## 2 MANIFOLD-ALIGNED NEIGHBOR EMBEDDING (MANE) FRAMEWORK

Assume the individual $n$-dimensional local datasets $\mathcal{D}^{(m)}=\left\{\mathbf{z}_{i}^{(m)}\right\}$, where $\mathbf{z}_{i} \in \mathbb{R}^{n}$ and $m=1,2,3, \ldots, M$, cannot interact with each other. The index $i=1,2,3, \ldots, N_{m}$ indexes each data point in the dataset. These datasets are local as in the datasets my be located in different physical systems that cannot interact with each other or hidden from each other. We construct a seeding dataset $\mathcal{D}^{(0)}=\left\{\mathbf{z}_{i}\right\}$, where $i=1,2,3, \ldots, N_{0}$, in order to construct extended datasets $D^{(m)}=\mathcal{D}^{(0)} \cup \mathcal{D}^{(m)}=\left\{\mathbf{x}_{i}^{(m)}\right\}$, where $i=1,2,3, \ldots, N_{0}, N_{0}+1, N_{0}+2, \ldots, N_{0}+N_{m}$. Alternatively, $\mathcal{D}^{(0)}$ is the shared data among all the datasets $D^{(m)}$. For notational simplicity and without loss of generality, we assume that $\mathbf{x}_{0}^{(m)}=\mathbf{z}_{0}, \mathbf{x}_{1}^{(m)}=\mathbf{z}_{1}, \ldots, \mathbf{x}_{N_{0}}^{(m)}=\mathbf{z}_{N} \in \mathcal{D}^{(0)}$ and[^0]$\mathbf{x}_{N_{0}+1}^{(m)}, \mathbf{x}_{N_{0}+2}^{(m)}, \ldots, \mathbf{x}_{N_{0}+N_{m}}^{(m)} \in \mathcal{D}^{(m)}$. Now we can define a high-dimensional weighted graph $p^{(m)}$ for each of the datasets using pairwise metric $d_{H}(\cdot, \cdot)$. The entries of the adjacency matrix are given by

$$
\begin{equation*}
p_{i, j}^{(m)}=f_{H}\left(d_{H}\left(\mathbf{x}_{i}^{(m)}, \mathbf{x}_{j}^{(m)}\right), D^{(m)}\right) \tag{1}
\end{equation*}
$$

where $f_{H}(\cdot)$ is a function that describes the weighted relation between points $\mathbf{x}_{i}, \mathbf{x}_{j} \in D^{(m)}$ subject to the distance metric. In t-SNE the function $f_{H}(\cdot)$ is a Gaussian, whereas in UMAP it's k-nearest neighbor based affinity function. It is to be noted that, this construction of graph $p^{(m)}$ can be computed in the same location where dataset $\mathcal{D}^{(m)}$ is located. In what follows, the embedding algorithm or the global part of the algorithm effectively requires this adjacency matrix $p^{(m)}$ in order to compute the low-dimensional embedding, and any access to individual data points is not necessary.

We initialize the low-dimensional (d-dimensional) embedding $\left|D^{(m)}\right|=\left\{\mathbf{y}_{i}^{(m)}\right\}$ for each dataset, where $\mathbf{y}_{i}^{(m)} \in \mathbb{R}^{d}$ is the corresponding low-dimensional mapping of the high-dimensional point $\mathbf{x}_{i}^{(m)} \in D^{(m)}$. Now we can define a low-dimensional weighted graph for each of the datasets using the following pairwise relation

$$
\begin{equation*}
q_{i, j}^{(m)}=f_{L}\left(d_{L}\left(\mathbf{y}_{i}^{(m)}, \mathbf{y}_{j}^{(m)}\right),\left|D^{(m)}\right|\right) \tag{2}
\end{equation*}
$$

where $f_{L}(\cdot)$ is a function of the weighted relation in the low dimension and $d_{L}(\cdot)$ is some pairwise metric. Typically, $d \ll n$. In t-SNE $f_{L}(\cdot)$ is the normalized Student's t-distribution function with one degree of freedom. UMAP skips the normalization step and uses a modified function similar to Student's t-distribution function of t-SNE's but has tunable parameters.

Finally the relation between the high-dimensional graphs and their joint low-dimensional embedding is established by optimizing the following problem

$$
\min _{\left|D^{(1)}\right|, \ldots,\left|D^{(m)}\right|} \sum_{m} \sum_{i, j} l\left(p_{i, j}^{(m)}, q_{i, j}^{(m)}\right)
$$

s.t.

$$
\begin{equation*}
\mathbf{y}_{i}^{(0)}=\mathbf{y}_{i}^{(1)}=\cdots=\mathbf{y}_{i}^{(M)}, \forall i=1,2, \ldots N_{0} \tag{3}
\end{equation*}
$$

where, $l(\cdot, \cdot)$ is the loss function. The constraint in Eq. 3 indicates that in MANE, the seeding dataset captures the shared manifold and works as anchor points, around which other data points are aligned in the low-dimensional embedding. To the best of our knolwedge this is the first time the constraint in Eq. 3 is being used for manifold alignment.

## 3 EXPERIMENTS

In this section, we briefly describe the implementation of the framework in an algorithm and the evaluation metrics to validate our results. In the main text, we describe our results for FashionMNIST Xiao et al., 2017). Results for additional datasets are described in the Appendix C

This framework can be implemented using the principles of most modern dimensionality reduction methods. Our implementation of the framework is based on the UMAP algorithm. We follow the UMAP principles to build the high-dimensional graphs $p_{i, j}^{(m)}$, low-dimensional graphs $q_{i, j}^{(m)}$ for each dataset $D^{(m)}$. Then we define the loss function $l\left(p_{i, j}^{(m)}, q_{i, j}^{(m)}\right)$ to be the cross-entropy loss function. We optimize the embeddings of each dataset jointly using the negative sampling Mikolov et al. (2013) approach. We sample one point from one of the datasets and optimize the loss function employing the constraint in Eq. 3. More details of the implementation are provided in Appendix $\mathrm{A}$.

We compare our results to two other UMAP based methods: 1) individual UMAP: when the datasets are embedded individually and 2) aligned UMAP: a software package of UMAP that aligns the learned manifolds using Procrustes analysis, then optimizes each of the embeddings with a regularizer constraint on the shared points. For each comparison, the number of nearest neighbors, minimum distance parameter, and negative sampling rate have been set to $30,0.1$, and 1.0 , respectively, for all embeddings. Aligned UMAP optimizes each dataset for 200 epochs individually and then

![](https://cdn.mathpix.com/cropped/2024_06_04_a4717bb663c0499eeba3g-03.jpg?height=602&width=1399&top_left_y=260&top_left_x=363)

Figure 1: Two-dimensional embedding of Fashion-MNIST data. (Left) UMAP embedding of 60,000 points $(T=0.9773)$. (Right) Top row: embedding of $D^{(1)}$ and bottom row: embedding of $D^{(2)}$ for the individual UMAP, aligned UMAP, and MANE. Individual UMAPs naturally cannot align the manifolds which can be seen from misalignment of the large cluster consisting of images of ankle boot, sandal and sneaker in the two embeddings $\left(T^{(1)}=0.9776, T^{(2)}=0.9779\right)$. Aligned UMAP $\left(T^{(1)}=0.9771, T^{(2)}=0.9775\right)$ and MANE $\left(T=0.9769, T^{(1)}=0.9775, T^{(2)}=0.9770\right)$ show very good alignment.
![](https://cdn.mathpix.com/cropped/2024_06_04_a4717bb663c0499eeba3g-03.jpg?height=416&width=1392&top_left_y=1224&top_left_x=364)

Figure 2: Shared data points from the two-dimensional embedding of the Fashion-MNIST dataset of Figure 1. (Left) Individual UMAPs of sets $D^{(1)}$ and $D^{(2)}$ show that the shared information is not aligned between two datasets $\left(d_{p}=0.0323\right.$ ). (Middle) Aligned UMAP shows close alignment between the shared points ( $d_{p}=0.0013$ ). (Right) MANE shows best alignment between the shared points as ensured by the constraint in $\mathrm{Eq} 3\left(d_{p}=0\right)$.

optimizes the aligned embeddings for further 200 epochs. For UMAP implementation, the precomputed distance matrix has been used. Our MANE embeddings were obtained using 200 epochs and no pre-embeddings were required. We initialized the embeddings using the axes obtained from the principal component analysis (PCA) of the shared data. To compare the embeddings numerically, we use Procrustes distance $\left(d_{p}\right)$ and trustworthiness $(T)$ Venna \& Kaski, 2001) metrics. Procrustes distance is the minimum euclidean distance between two datasets under translation, scaling, rotation, and reflection. Trustworthiness is a measure that gives a sense of how much of the nearest neighbors in the high dimension are preserved in the low dimension after embedding. More details of these metrics are provided in Appendix B The PCA initializatin scheme can be used as a linear baseline, which has been further illustrated in Appendix $\mathrm{D}$.

The Fashion-MNIST dataset contains 70,000 gray-scale images of fashion items. The training data contains 60,000 images, which have been used in the experiments. First, we show results by randomly splitting the Fashion-MNIST dataset into three datasets $\mathcal{D}^{(m)}$, where $m=1,2,3$, where $\mathcal{D}^{(0)}$ contains 10,000 samples and the other splits contains 25,000 samples each. Then we construct

![](https://cdn.mathpix.com/cropped/2024_06_04_a4717bb663c0499eeba3g-04.jpg?height=450&width=1390&top_left_y=271&top_left_x=365)

Figure 3: MANE output of Fashion-MNIST data by setting the number of shared points, $N_{0}$, to (from left to right) $100,1000,5000,10000$, and 20000 respectively. The data has been split into two datasets with $N_{0}$ shared points and then aligned using our implementation. Top row: $\left|D^{(1)}\right|$ and bottom row: $\left|D^{(2)}\right|$. (from left to right) Trustworthiness values are $T=$ $0.9722,0.9754,0.9765,0.9769,0.9771$. These embeddings show that the shared points have to sample the manifold enough to obtain better alignment.

![](https://cdn.mathpix.com/cropped/2024_06_04_a4717bb663c0499eeba3g-04.jpg?height=216&width=1391&top_left_y=1044&top_left_x=367)

Figure 4: MANE output of Fashion-MNIST data which is split into 5 datasets of 14400 data points. Each dataset shares 3000 points. The embeddings show good alignment $(T=0.9760)$.

$D^{(1)}=\mathcal{D}^{(0)} \cup \mathcal{D}^{(1)}$ and $D^{(1)}=\mathcal{D}^{(0)} \cup \mathcal{D}^{(2)}$. The embeddings obtained using different schemes are shown in Figure 1 Since Individual UMAP and aligned UMAP provide separate embeddings for the shared points, we compute the trustworthiness metric of each dataset $\left(T^{(1)}\right.$ and $T^{(2)}$ ). On the other hand, our method embeds both datasets into the same metric space and thus we can compute trustworthiness as if the whole Fashion-MNIST dataset has been embedded. Thus in addition to reporting $T^{(1)}$ and $T^{(2)}$, we also report trustworthiness of union of the two embeddings $(T)$. We can observe from the figure that, all embeddings capture the general manifold of the Fashion-MNIST data as shown in the UMAP of the Fashion-MIST dataset. However, in individual UMAPs the embeddings $\left|D^{(1)}\right|$ and $\left|D^{(2)}\right|$ are not aligned to each other. This can be realized by observing the large cluster on the left involving the labels sneaker, ankle boot and sandal (colored teal, violet and greenish-yellow, respectively) which is oriented differently in the two embeddings indicating misalignment. Moreover, the other cluster on the lower right involving the labels $t$-shirt, pullover and dress (colored maroon, orange, and light green, respectively) is more compact in $\left|D^{(2)}\right|$ than it is in $\left|D^{(1)}\right|$. On the other hand, both aligned UMAP and MANE produce embeddings that are aligned to each other as the discrepencies described above are absent for these two. The trustworthiness of MANE is closer to the trustworthiness metric when the whole dataset has been embedded using UMAP. The trustworthiness metric $T^{(1)}$ and $T^{(2)}$ are similar for all three metric, which indicates all of them have similar performance for embedding $D^{(1)}$ and $D^{(2)}$ whereas, the added benefit of aligned UMAP and MANE is the manifold alignment. While aligned UMAP uses regularization constraint for alignment (along with some costly pre-processing), MANE uses a hard constraint described in Eq3. This shows that jointly aligning and embedding using a hard constraint of Eq. 3 is a viable way to obtain aligned embedding. In Figure 2, we examine how the shared points have been embedded. We can see that the shared points are not in agreement for individual UMAP and aligned UMAP, but they are in agreement for MANE.

Now we use the same scheme of the previous experiment but vary the value of $N_{0}$, the number of shared points, to see how much the shared information influences the alignment. We kept the total number of unique points to 60,000 . The resultant embeddings are shown in Figure 3, It can be
observed that except for $N_{0}=1000$ and $N_{0}=100$, all other embeddings are in good alignment. For $N_{0}=1000$ and $N_{0}=100$, the alignment is somewhat lost. For these embeddings, the large structures are in similar places having similar orientation, however, finer structures are not aligned. For example, for $N_{0}=1000$ and $N_{0}=100$, the larger cluster in the lower right is compact for $\left|D^{(2)}\right|$ compared to that of $\left|D^{(1)}\right|$. For $N_{0}=100$, the spike like structure in the cluster of sandal (colored greenish-yellow in the left of the figure) is different for $\left|D^{(1)}\right|$ and $\left|D^{(2)}\right|$. This is due to the fact that the shared points work as anchors around which the manifold of both datasets is arranged. However, with merely 1000 or 100 points, the shared points cannot sample enough of the manifold.

Finally, we look into a case where more than two datasets are involved. We split the Fashion-MNIST data into 5 datasets of equal size ( 14400 data points each). The number of shared points is 3000 . The resultant aligned embeddings are shown in Figure 4. The figures show that all 5 datasets are aligned.

## 4 CONCLUSIONS AND FUTURE WORK

In this paper, we introduce MANE, a neighbor embedding approach for aligning the manifold of multiple datasets that use the same underlying manifold. We modeled the problem in terms of shared data points among multiple datasets. We implemented our approach using UMAP principles. The efficacy of our method in aligning datasets is demonstrated by several experiments. We showed the shared points can be in perfect alignment yet produce embeddings that are comparable to the embedding of the combined dataset. We compared our method with aligned UMAP in which the shared data are not in perfect alignment. Overall, MANE produce emeddings that are comparable to mebedding of combined dataset, and provides perfect alignment for the shared points. This method can be used in practice where shared data is known or a seeding dataset is available.

In the future, a more interesting demonstration would be in a case where two datasets are of different modalities (say x-ray image and its corresponding report or two different medical image modalities). The dataset $\mathcal{D}^{(0)}$ contains pairs of x-ray image and reports, whereas, $\mathcal{D}^{(1)}$ and $\mathcal{D}^{(2)}$ are datasets of $\mathrm{x}$-rays, and reports, respectively. Thus, using manifold-aligned neighbor embedding one can jointly obtain the embedding of x-rays and reports, which will be of great importance in medical AI.

## ACKNOWLEDGEMENT

This material is based upon work supported by the Air Force Office of Scientific Research (AFOSR) under Grant FA9550-18-1-0219 and by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112090123. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFOSR, or the U.S. Government.

## CODE AND DATA AVAILABILITY

The codebase of the paper can be obtained from/https://github.com/tariqul-islam/mane_paper.

All the data used in this paper are publicly availble.

Fashion-MNIST is available athttps://github.com/zalandoresearch/fashion-mnist

MNIST is available at http://yann. lecun.com/exdb/mnist/.

Swiss roll data can be generated fromhttps://scikit-learn.org/stable/modules/generated/sklearn.d

Single cell transcriptomes data is available athttps://github.com/biolab/tsne-embedding.

## REFERENCES

Jihun Ham, Daniel Lee, and Lawrence Saul. Semisupervised alignment of manifolds. In International Workshop on Artificial Intelligence and Statistics, pp. 120-127. PMLR, 2005.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.

Yunqian Ma and Yun Fu. Manifold learning theory and applications, Volume 434. CRC press Boca Raton, 2012.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov):2579-2605, 2008.

Evan Z Macosko, Anindita Basu, Rahul Satija, James Nemesh, Karthik Shekhar, Melissa Goldman, Itay Tirosh, Allison R Bialas, Nolan Kamitaki, Emily M Martersteck, et al. Highly parallel genome-wide expression profiling of individual cells using nanoliter droplets. Cell, 161(5):1202$1214,2015$.

Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111-3119, 2013.

Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323-2326, 2000.

Jarkko Venna and Samuel Kaski. Neighborhood preservation in nonlinear projection methods: An experimental study. In International Conference on Artificial Neural Networks, pp. 485-491. Springer, 2001.

Chang Wang and Sridhar Mahadevan. Manifold alignment using procrustes analysis. In Proceedings of the 25th International Conference on Machine Learning, pp. 1120-1127, 2008.

Chang Wang and Sridhar Mahadevan. A general framework for manifold alignment. In 2009 AAAI Fall Symposium Series, 2009.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. 2017.
