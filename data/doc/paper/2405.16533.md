# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner 

Zhengliang Shi $\|_{||}$Shen Gao ${ }^{2}$ Xiuyi Chen ${ }^{3}$ Yue Feng ${ }^{4}$ Lingyong Yan ${ }^{3}$<br>Haibo Shi ${ }^{3}$ Dawei Yin ${ }^{3}$ Zhumin Chen ${ }^{1}$ Suzan Verberne ${ }^{5}$ Zhaochun Ren ${ }^{5}$<br>${ }^{1}$ Shandong University ${ }^{2}$ University of Electronic Science and Technology of China<br>${ }^{3}$ Baidu Inc., Beijing, China ${ }^{4}$ University of Birmingham, Birmingham, UK<br>${ }^{5}$ Leiden University, Leiden, The Netherlands<br>shizhl@mail.sdu.edu.cn z.ren@liacs.leidenuniv.nl


#### Abstract

Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, empowering them to solve practical tasks. Existing work typically empowers LLMs as tool users with a manually designed workflow, where the LLM plans a series of tools in a step-by-step manner, and sequentially executes each tool to obtain intermediate results until deriving the final answer. However, they suffer from two challenges in realistic scenarios: (1) The handcrafted control flow is often ad-hoc and constraints the LLM to local planning; (2) The LLM is instructed to use only manually demonstrated tools or well-trained Python functions, which limits its generalization to new tools. In this work, we first propose Automatic Tool Chain (ATC), a framework that enables the LLM to act as a multi-tool user, which directly utilizes a chain of tools through programming. To scale up the scope of the tools, we next propose a black-box probing method. This further empowers the LLM as a tool learner that can actively discover and document tool usages, teaching themselves to properly master new tools. For a comprehensive evaluation, we build a challenging benchmark named ToolFlow, which diverges from previous benchmarks by its long-term planning scenarios and complex toolset. Experiments on both existing datasets and ToolFlow illustrate the superiority of our framework. Analysis on different settings also validates the effectiveness and the utility of our black-box probing algorithm.


## 1 Introduction

Large language models (LLMs) have shown promising capabilities such as in-context learning and real-world planning [1-3]. To further increase their utility, the tool learning task [4, 5] is proposed to augment LLMs with external tools, e.g., a Weather App, enabling them to interact with the physical world [6-8]. With the assistance of tools, LLMs can serve as agents to automatically solve practical tasks [9-11], such as check the weather in London. More recent work further uses Python code as a unified interface to access diverse tools, coming with advantages like seamlessly reusing massive built-in functions and performing for-loop operations [12-15].

Given a practical task, prior work grounds the tool-use process in an iterative plan-execute-observe pipeline [16-19]. As shown in Figure 1 (a), the LLM first plans a series of tools in a step-by-step manner [20-22]. For each step, the LLM generates arguments in a handcrafted format [23, 24] or code snippets [12, 19] for execution, continuously incorporating intermediate results into the context for subsequent actions. However, they suffer from two challenges in realistic scenarios. First, their[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-02.jpg?height=726&width=1347&top_left_y=255&top_left_x=386)

Figure 1: The comparison between previous plan-execute-observe pipeline (left) and our proposed framework (right).

workflow to interact with tools is typically manually designed and ad-hoc, struggling to generalize to different tool-use scenarios. The handcrafted workflow also constrains the LLM to local planning, leading to diminished performance in long-term planning tasks. Second, the LLM can only use manually demonstrated tools or built-in Python functions, which limits the toolset scope. To this end, we address the above challenges with a focus on two research objectives in our work: (1) Empower the LLM as an automatic Multi-tool user to generate a tool-use chain; (2) Further empower the LLM as an active Multi-tool learner to instruct themselves to master new tools.

To address the first objective, we propose Automatic Tool Chain (ATC), a framework that enables the LLM to utilize a chain of tools through programming. As shown in Figure 1 (b), the LLM directly learns the input-output schema and data flow dependency of various tools from tool protocols (a.k.a., tool documentation). Different from the short-form docstring of simple Python functions in previous work [13, 25, 15], the protocol comprehensively outlines meta-information about a complex tool, such as arguments requirement, structural response specifications (i.e., a general schema elaborating expected execution results) and possible execution statuses. With the assistance of the provided protocols, we instruct the LLM to generate a program that sequentially calls a chain of tools, parses the tool response to cache useful information and derives the final answer. To correct runtime errors in the generated programs, we introduce an attributable reflection mechanism, which allows the LLM to track faulty snippets, pinpoint incorrect tool usage, and calibrate the programs accordingly.

In realistic scenarios, a potential challenge that limits the scope of the toolset in our framework is the continuous crafting of documented protocols for diverse and fast-paced tools, which is typically done by software developers [26, 27]. Therefore, we propose a black-box probing method to address the second objective. This approach enables the LLM to be an active tool learner that can probe the input-output schema of new tools and teach itself how to use them. Initially, the LLM is instructed to generate testing instances that target the functionality of a tool, including relevant tasks and tool-use program solutions. While executing the generated program, we transform the task-specific tool response into a general schema and leverage these instances as practical usage demonstrations, thereby documenting the tool protocol. Considering that a single tool may fail to probe due to the absence of private arguments, which are only acquired through other tools, we introduce a chain of probing algorithms. This algorithm effectively optimizes the cooperation among tools that have a strong input-output dependency.

We first investigate the capability of LLMs to generate a chain of tools on two well-established datasets from RestBench [14]. For a comprehensive evaluation, we also create a new benchmark testbed named ToolFlow, including 224 tasks across 107 real-world tools. ToolFlow diverges from the existing benchmarks by its more long-term planning tasks, the thorough protocol of the toolset, and complex data flow interdependency among tools, which evaluates our method under more challenging scenarios. The results show that (1) the LLM can well understand the tool protocol; (2)
the LLM exhibits strong capability in planning a chain of tools programmatically; and (3) despite the straightforward design, our framework substantially surpasses previous baselines with higher efficiency. In addition, the proposed black-box probing method effectively instructs LLMs to probe tool protocols and teach themselves to master new tools, extending the scope of the tools in our ATC.

Our contributions are summarized as follows: (i) We propose the Automatic tool chain (ATC), a framework to empower the LLM as a multi-tool user. (ii) We introduce a black-box probing method, which further enables the LLM to act as an active tool learner to the scope of the toolset in our ATC. (iii) We release a new benchmark, ToolFlow, to evaluate tool learning methods in more challenging scenarios. (iv) Extensive experiments on three datasets validate the superiority of our method.

## 2 Related Work

Tool learning with foundation models. Augmenting LLMs with external tools has been proven a promising method for enhancing their utility and enabling interactions with the physical world [8. 6, 28, 29]. As the commonly-used methods, the LLM first breaks down complex tasks and plans a series of tools in a step-by-step manner [24, 30, 19]. For each step, the LLM separately executes the tools and incorporates the full response into context, which contains the required arguments to invoke subsequent tools due to the data flow dependency [20, 31, 22]. Despite advancements, this iterative workflow is typically manually designed and ad-hoc, struggling to generalize across various tool-use scenarios. In this work, we propose the ATC, enabling the LLM as an automatic multi-tool learner to directly integrate a chain of tools.

Programming-enhanced LLMs. Recent work has shown the potential of using programming languages (PLs) to enhance the planning and reasoning capability of LLMs [32--34]. For example, previous work enables LLMs to generate a programmatic chain of thought to solve complex numeric reasoning tasks [35, 36], which exhibits remarkable performance. In the tool learning task, compared with nature languages (NLs), recent work also shows that LLMs can generate Python code snippets as actions, with advantages like integrating widely used Python functions and simplifying lengthy for-loop operations [12]. Additionally, previous work limited the LLM to only use well-documented tools [14, 27, 25] or the Python function learned from the pre-training stage [12] . In this work, we further investigate the LLM as a multi-tool learner, teaching themselves to master new tools.

Learning from external feedback. Learning from feedback is a prevailing strategy to mitigate undesired behaviors of LLMs [37, 38], mirroring a typical human learning strategy where individuals refine their behaviors through trial, error, and correction [39-42]. Previous studies such as Reflexion [43] show the capability of LLMs to reflect verbal signals from the environment and revise their mistakes [44]. Recent work prompts LLMs to use automated feedback (e.g., runtime errors) implemented in software to self-debug its generated code in each step [44, 45, 17]. Despite the progress, this feedback typically reflects straightforward faults while failing to address the snowballing issue [46] in multi-step planning, where an initial error can lead to a series of subsequent accumulated errors 47]. In the tool learning task, pinpointing the exact tool triggering the error is crucial [48]. In this work, the proposed attributable reflection mechanism guides LLMs to track the faulty program snippet, attribute it to a specific tool calling, and revise generated programs.

## 3 Automatic Tool Chain

### 3.1 Preliminaries

Solving practical tasks with the assistance of tools can be conceptualized as a planning process. Formally, the LLM, denoted as $M_{\theta}$, is equipped with access to a set of tools $\mathcal{T}=\left\{t_{1}, t_{2}, \ldots, t_{|\mathcal{T}|}\right\}$ and corresponding documented protocols $\mathcal{D}=\left\{d_{1}, d_{2}, \ldots, d_{|\mathcal{D}|}\right\}$. The protocol $d_{i}$ provides detailed meta information about tool $t_{i}$ such as argument requirements, tool description, and the specification of execution result (a.k.a., schema). Given a natural language task $x \in \mathcal{X}$ from the task space $\mathcal{X}$, the object is to generate a sequence of tool callings paired with corresponding arguments to derive the final answer. Previous work configures the LLM with customized control flow and tool-use templates, whereby the LLM iteratively interacts with single tools following a plan-execute-observe pipeline. In this work, we enable the LLM to automatically utilize a chain of tools by generating a program $\mathcal{C}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-04.jpg?height=524&width=1396&top_left_y=236&top_left_x=364)

(a) Overall framework

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-04.jpg?height=233&width=659&top_left_y=241&top_left_x=1080)

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-04.jpg?height=230&width=623&top_left_y=476&top_left_x=1095)

(b) Black-box Tool probing

Figure 2: Left: An overview of our framework with the proposed attributable reflection mechanism ( $\$ 3.3$. Right: Our black-box probing method ( $\$ 4.1$.

### 3.2 Chain of Tools Generation

Given a task $x$, we first provide the LLM with the documented protocol $d_{i} \in \mathcal{D}$ for each tool $t_{i}$ in the candidate toolset $\mathcal{T}$. The documented protocol $d_{i}$ records meta information, including the description to outline what the tool $t_{i}$ can be used for, the argument requirements to describe how to invoke it, and the response schema to specify the expected type of execution results. With the assistance of this meta information, the LLM can automatically learn the tool usage, and master the detailed input-output schema as well as the data flow relation among various tools. Then, we instruct the LLM $\mathcal{M}_{\theta}$ to directly generate an executable program $\mathcal{C}$ to utilize multiple tools and solve the input task $x$. Formally, it can be formulated as:

$$
\begin{equation*}
\mathcal{C}=\mathcal{M}_{\theta}\left(x, \mathcal{T}, \mathcal{D}, \mathcal{I}_{c}\right) \tag{1}
\end{equation*}
$$

Here, the $\mathcal{I}_{c}$ indicates a concise instruction for program generation operation, which is provided in Appendix A.7. The $\mathcal{T}$ and $\mathcal{D}$ represent the candidate toolset and corresponding tool protocols, respectively. The generated program sequentially calls multiple tools to acquire useful information, parses lengthy execution results for subsequent utilization, and simplifies the lengthy task-solving trajectory with concise programmatic planning. The final result $r$ is derived by executing the generated program through a code interpreter, which can be formulated as $r=\operatorname{Execute}(\mathcal{C})$.

### 3.3 Programming with Attributable Reflection

The generated program effectively integrates multi-step tool utilization. However, runtime errors, such as passing redundant arguments, are frequently observed during the execution. Therefore, we introduce an attributable reflection mechanism, guiding the LLM to first attribute the raised error to a specific tool, and then adaptively revise the generated program. As shown in Figure 2 , if a runtime error is raised, we capture the error message in the result $r$, including both the faulty code snippet and the error trace. Then, we instruct the LLM to localize the specific tool calling which triggers the error and exactly generate the tool name, represented as $t_{j}=\mathcal{M}_{\theta}\left(x, \mathcal{T}, \mathcal{I}_{a}, r_{j}\right)$. Here the $j$ indicates the $j$ th iteration reflection and the $\mathcal{I}_{a}$ indicates the instruction for this error attribution operation. The identified tool $t_{j}$ paired with its documentation $d_{j}$ as well as the error message is taken as input, assisting the LLM to revise the generated program, which can be formulated as:

$$
\begin{equation*}
\mathcal{C}_{j}=\mathcal{M}_{\theta}\left(x, \mathcal{T}, \mathcal{D}, \mathcal{I}_{c},\left\{\left(\mathcal{C}_{<j}, r_{<j}\right)\right\}, d_{j}\right) \tag{2}
\end{equation*}
$$

Our attributable reflection mechanism is operated until the generated program is executed successfully or up to the maximum iteration $\alpha$.

## 4 Black-box Probing Enable Toolset Extension

Our framework ATC enables the LLM to directly operate a chain of well-documented tools through programming. However, manually crafting and maintaining documented protocols for diverse and fast-paced tools is cost-intensive, which poses a potential limitation to the scope of the toolset in our framework. Therefore, we propose a black-box probing method, which enables the LLM to act as an active tool learner, instructing themselves to master new tools. Due to the relation among the data flow of tools, we also introduce a chain of probing algorithms to enhance our probing process.

### 4.1 Tool Probing

As shown in Figure 2(b), our probing method contains two phases, including Instance discovery and Protocol documenting. The core idea of the former is to generate tool-use instances through self-exploration, examining the expected input-output mechanism for each tool, while the latter transforms specific instances into general tool protocol.

Instance discovery. We instruct the LLM to formulate a question $q$ targeting the functionality of a tool $t$ and generate a program utilizing the tool $t$ to solve the formulated question. Formally, it can be represented as $(q, \mathcal{C})=\mathcal{M}_{\theta}\left(t, \mathcal{I}_{p}\right)$, where $\mathcal{I}_{p}$ is the instruction for our instance discovery operation. The response $r$ of the tool $t$ can be examined while executing the generated program $\mathcal{C}$ as $r=\operatorname{Execute}(\mathcal{C})$, which represents a specific instance to demonstrate the output of the tool $t$. Since the LLM may hallucinate to formulate unsolvable questions or fail to generate a correct program, we repeat the above operation multiple times until the response $r$ can be examined correctly or up to the maximum of sampling times $N$. Thus, we obtain a tool-use instance denoted as $\left(\left(q_{i}, \mathcal{C}\right), r\right)$.

Protocol documenting. On top of of sampled instance $\left(\left(q_{i}, \mathcal{C}_{i}, r_{i}\right)\right)$, we construct the tool protocol. Since the response of real-world tools is typically lengthy with intricate structures, we first transform the query-specific response $r$ into a general schema $s$ to demonstrate the expected output specification. This process is automatically performed by recursively decomposing each element in $r$, representing the hierarchical structure of $r$, and listing the type of the corresponding value. Then, we utilize the question-program pair $(q, \mathcal{C})$ as a usage demonstration of the tool $t$, pairing it with $s$ to construct the documented protocol $d$, denoted as $d=((q, \mathcal{C}), s, t)$. We provide an example and detailed procedure to demonstrate the above transformation process in Appendix Alg. A.1.

### 4.2 Chain of Probing

During the probing, some tools may not be callable exclusively due to the absence of specific private arguments, which are only accessible through other tools, i.e., the unique ID. To address the strong data flow interconnection, we propose the chain of probing algorithm that enables the cooperation of tools. Formally, we denote the black-box toolset as $\mathcal{B}$, which contains unprobed tools and is initialized with the entire candidate toolset. The successfully probed tools are cached in the list $\mathcal{H}$, which is initialized with an empty list.

Initial Iteration for single tool probing. As illustrated in Figure 3, our initial iteration starts by probing each single tool $t$ within the black-box toolset $\mathcal{B}$, represented as $d=\operatorname{LLMProb}(t)$. The $\operatorname{LLMProb}(*)$ indicates the tool probing operation in $\S 4.1$. If a tool $t$ is successfully probed, i.e., no exceptional errors are raised, it is moved from black-box toolset $\mathcal{B}$ to list $\mathcal{H}$, formulated as:

$$
\begin{equation*}
\mathcal{B}=\mathcal{B} \backslash\{t\}, \quad \mathcal{H}=\mathcal{H} \cup\{t\} \tag{3}
\end{equation*}
$$

After the initial iteration, $\mathcal{H}$ contains tools that are directly callable like the tool $\mathrm{C}$ and $\mathrm{D}$ in Figure 3 while the remaining tools in $\mathcal{B}$ are interconnected with other tools.

Probing with dependency chain. For the remaining tools $t$ in $\mathcal{B}$, we probed them with the assistance of tools from $\mathcal{H}$. Specifically, we instruct the LLM to select a subset of tools from $\mathcal{H}$ based on their relevance to the tool $t$, which can be formulated as $\hat{\mathcal{T}}=\mathcal{M}_{\theta}\left(t, \mathcal{H}, \mathcal{I}_{s}\right)$. Here, the $\mathcal{I}_{s}$ denotes the instruction for tool selection. The selected subset $\hat{\mathcal{T}}$ serves as the prerequisite which facilitates the acquisition of necessary arguments to invoke the tool $t$ during the probing process, thereby deriving the tool protocol, rep-

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-05.jpg?height=395&width=677&top_left_y=1903&top_left_x=1060)
resented as $d=\operatorname{LLMProb}(\hat{\mathcal{T}} \cup\{t\})$. As shown in Figure 3, the tool $\mathrm{C}$ is selected to assist in probing the tool $\mathrm{B}$. Our probing process continues for multiple iterations until all tools in $\mathcal{B}$ are successfully probed or reach the maximum of iteration $\beta$. A detailed pseudo algorithm of the overall process is provided in Appendix Alg. 1 .

## 5 Dataset and Evaluation Setup

Existing Datasets. We first conduct experiments on widely used RestBench [14], including two datasets: TMDB and Spotify. The TMDB contains 100 tasks across 54 tools for movie scenarios while the Spotify contains 57 tasks across 40 tools for music scenarios. Each tool in the RestBench is paired with a lengthy documented protocol, making it inherently appropriate to benchmark the protocol utilization capability of LLMs.

A new benchmark - ToolFlow. As shown in Appendix Table 8, to the best of our knowledge, no existing benchmarks containing complex tools with comprehensive tool protocol (e.g., arguments requirement and input-output schema) while involving long-term planning tool-use tasks. Therefore, we build a new test set named ToolFlow to fill this gap. We first collect 107 tools with long protocols across 4 real-world domains, e.g., Weather and Game, from $16 \mathrm{k}$ public tools of the ToolBench [9] dataset. Then, we invite 7 well-trained experts working on NLP research to provide solutions for 224 complex tasks in the form of tool interaction sequences, including the tool name and corresponding arguments. Each task requires long-term reasoning and at least 7 times interacting with tools. ToolFlow also diverges from existing benchmarks by its strong interconnection among the tools (the arguments of subsequent tools can only be extracted from the response of previous tools) and stability (the task solution is not time-varying). We provide more details of ToolFlow in A.2.

Evaluation metrics. Following previous work [49, 50], we use three evaluation metrics, including: (1) Success Rate (Success\%), which measures the proportion of successful query completions; (2) Correct Path Rate (Path\%), which calculates the proportion of ground truth tools in model-generated tool callings; (3) Correct Tool Precision (Prec\%), which calculates the precision score between the model-generated tool callings and ground truth tool sequence. We also conduct the human evaluation to evaluate our method and the details can be found in Appendix A.1.2.

Baselines. We mainly compare our method with the well-known baselines, including: (1) ReAct [18], which prompts LLM to generate the chain-of-thought and actions in an interleaved manner; (2) CodeAct [12], which prompts LLM to iteratively generate code snippets as actions to call external tools; (3) ToolLLM-DFSDT[9], which enhances LLMs with the Depth First Search-based Decision Tree (DFSDT) to select tools to solve a task; (4) RestGPT [14], which includes a coarse-to-fine planning module and a tool executor; (5) ConAgents [26], which enables the cooperation of three specialized LLMs to solve complex tasks. For further comparison, We also establish two baselines, i.e., ReAct@3 and ToolLLM@3, which are up to three times runs of their vanilla method (ReAct or ToolLLM) until the input task is successfully completed.

## 6 Experiment Results

### 6.1 Results of RQ1 - Enable the LLM as an automatic muti-tool user

We utilize three widely used LLMs for different baselines and our method: OpenAI's gpt-3.5-turbo$16 \mathrm{k}$ and gpt-4-turbo, and the open-source model Mixtral- $8 x 7 B[51]^{3}$ The decoding temperature is set to 0 for deterministic generation. The trial number $\alpha$ in our reflection mechanism $(\S 3.3$ is set to 3 . Following previous work [26, 14], we provide all the methods with 20 candidate tools for each task in the test set, which contains the required tools and randomly sampled tools.

Results on RestBench. As shown in Table 1, the LLM, when equipped with our framework, surpasses all the baselines on the RestBench benchmark in terms of all metrics. For example, our method achieves 89.00 in success rate metrics on the RestBench-TMDB dataset, which substantially improves over the commonly used baseline ReAct and ToolLLM. Table 2 and Table 3 further illustrate that our framework can achieve the best performance with various backbone LLMs, i.e., the Mistral8x7B and GPT-4. These results indicate our framework effectively enables LLM to master external tools and directly generate a program for utilization. The performance of two runs is tested using a two-tailed paired t-test where no significant difference is found ( $p>0.05$ ), showing the stability of our method. In addition, human evaluation indicates that our method performs substantially better on executability and utility than strong baselines. See Appendix A.1.2 for details.[^1]

Table 1: Experiment results on three datasets with gpt-3.5-turbo as backbone. The Path\%, Prec\% and Success\% indicate Correct Path Rate, Correct Path Precision and Successful Rate metrics.

| Method | RestBench-TMDB |  |  | RestBench-Spotify |  |  | ToolFlow |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Succ\% | Path\% | Prec\% | Succ\% | Path\% | Prec\% | Succ\% | Path\% | Prec\% |
| gpt-3.5-turbo |  |  |  |  |  |  |  |  |  |
| ReAct [18] | 61.00 | 77.13 | 52.30 | 50.88 | 74.64 | 44.79 | 22.76 | 60.75 | 68.03 |
| CodeAct [26] | 63.00 | 80.91 | 83.72 | 54.30 | 76.64 | 79.81 | 27.82 | 57.93 | 66.23 |
| ToolLLM $[9]$ | 72.00 | 78.29 | 49.41 | 61.40 | 82.82 | 25.33 | 42.14 | 71.02 | 65.24 |
| RestGPT [14] | 65.00 | 77.49 | 80.15 | 64.91 | 73.94 | 88.71 | 26.83 | 40.95 | 62.21 |
| ConAgents [26 | 76.00 | 78.29 | 82.31 | 63.16 | 78.21 | 82.71 | - | - | - |
| $\overline{\operatorname{ReAct}} \mathrm{C} 3$ | $\overline{70.00}$ | $8 \overline{0.96}$ | $4 \overline{8} .01$ | $\overline{59} . \overline{65}$ | $\overline{8} 1 . \overline{8} 0$ | 30.48 | $-28.3 \overline{5}$ | $6 \overline{6} .6 \overline{6}$ | $\overline{66} . \overline{21}$ |
| ToolLLM@3 | 74.00 | 83.29 | 45.41 | 66.67 | 83.41  | 23.73 | 44.70 | 73.85 | 60.77 |
| $\overline{\mathbf{A T}} \overline{\mathbf{C}}(\overline{\text { ours }})$ | $\mathbf{8 9 . 0 0}$ | $8 \overline{4} .7 \overline{1}$ | $8 \overline{3} . \overline{87}$ | $\overline{78 .} \overline{95}$ | $\overline{7} 8 . \overline{5} 4$ | 91.46 | $60.2 \overline{1}$ | $7 \overline{8} .3 \overline{1}$ | $\overline{72.45}$ |

Table 2: Experiment with the Mistral-8x7B.

| Method | TMDB |  |  | ToolFlow |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Succ\% | Path\% |  | Succ\% | Path\% |
| mixtral-8x7B-instruct-v0.1 |  |  |  |  |  |
| ReAct | 24.74 | 73.34 |  | 10.53 | 41.37 |
| ReAct@3 | 37.88 | 76.85 |  | 18.95 | 52.40 |
| ToolLLM@3 | 45.00 | 74.40 |  | 22.54 | 51.85 |
| Ours | $\mathbf{5 8 . 0 0}$ | $\mathbf{7 8 . 1 7}$ |  | $\mathbf{2 9 . 8 7}$ | $\mathbf{5 9 . 1 4}$ |

Table 3: Experiment with the GPT-4.

| Method | TMDB |  |  | ToolFlow |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Succ\% | Path\% |  | Succ\% | Path\% |
| gpt-4-turbo |  |  |  |  |  |
| ReAct | 77.00 | 86.05 |  | 25.99 | 65.98 |
| ReAct@3 | 80.00 | 89.21 |  | 30.98 | 67.55 |
| ToolLLM@3 | 82.00 | 90.62 |  | 50.46 | 76.73 |
| Ours | $\mathbf{9 4 . 0 0}$ | $\mathbf{9 2 . 6 8}$ |  | $\mathbf{6 5 . 7 4}$ | $\mathbf{8 3 . 5 4}$ |

Results on ToolFlow. Table 1 presents the results on our ToolFlow benchmark. We find that our ToolFlow poses a significant challenge for previous baselines, where the best performance only achieves a 44.70 success rate with GPT-3.5 as the backbone. Our method pushes the success rate to 60.21 with a 15.51 point improvement. The potential reason for our improvement is that our ATC enables the LLM to generate a chain of tools programmatically, which is more effective in controlling workflow and consolidating lengthy task-solving trajectories into a concise program.

Ablation for our attributable reflection. We compare our attributable reflection method with two ablative variants: (1) w/o reflect, which allows the LLMs to generate a program as the final solution without further revision, and (2) w/ naive reflection, which enables the LLMs to revise generated programs directly using error messages from code interpreter. The results are shown in Table4, We observe that our attributable reflection outperforms the two variants with a $5-10 \%$ point improvement. It demonstrates the superiority of the reflection mechanism and the effectiveness of our error attribution strategy.

### 6.2 Results of RQ2 - Enable the LLM as an active Multi-tool learner

We evaluate our black-box probing method on three datasets using different backbone LLMs, i.e., RestBench-TMDB, RestBench-Spotify, and ToolFlow, respectively. The sampling number $N$ ( $\$ 4$ ) is set to 3 and the maximum iteration number $\beta$ ( $\$ 4.2$ is set to 4 . We mainly evaluate our probing method by computing the number of successfully probed tools. To evaluate the utility of the autodocumented protocol, we compare the performance of our ATC supported by standard protocol (std protocol) and synthetic protocol (auto protocol). Considering that our synthetic documentation contains a usage example for each tool, we further set a zero-shot experiment, which only remains the transformed schema (auto schema).

Success rate of tool probing. Table 5 shows the number of successfully probed tools. We find that the open-source model Mixtral-8x7B, when equipped with our probing method, can probe $82.5 \%$ to $88.2 \%$ of tools and synthesize their tool documentation. The number of successfully probed tools also increases when alternating more powerful backbone LLMs, specifically GPT-4. These results validate the effectiveness of our tool probing method. We further analyse the cases where the LLM

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-08.jpg?height=545&width=1395&top_left_y=237&top_left_x=365)

Figure 4: The comparison of our framework when equipped with different documentation.

fails to probe successfully. A potential reason is that the code interpreter only examines compile errors and runtime faults, failing to calibrate the correctness of the program's output. This limitation can lead to the False Success phenomena.


#### Abstract

Ablation for the black-box probing. We compare our tool probing with two ablative variants: (1) w/o multi-sample, which replaces the multiple sampling strategy in $\S 4$ with sampling only one instance; and (2) w/o chain, which ignore the dependency chain among tools in $\S 4.2$ and separately probes single tools. As shown in Table 5 , in terms of the number of successfully probed tools, we observe a 3-7 point decrease for w/o multi-sample, which indicates that the LLMs may fail to generate a correct program at

Table 5: The number of successfully probed tools using our vanilla probing method and two variants.

| Method | TMDB | Spotify | NovelTools |
| :---: | :---: | :---: | :---: |
| Totally | 54 | 40 | 107 |
| Probing (mixtral) | 47 | 33 | 90 |
| Probing (gpt-4) | 54 | 38 | 102 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-08.jpg?height=43&width=231&top_left_y=1254&top_left_x=1078) | $\overline{54}$ | 38 | $9 \overline{9}$ |
| - w/o multi-sample | $50_{\downarrow 4}$ | $35_{\downarrow 3}$ | $91_{\downarrow 7}$ |
| - w/o chain | $47_{\downarrow 7}$ | $17_{\downarrow 21}$ | $87_{\downarrow 11}$ |


one pass. We also find a substantial decrease between our vanilla probing method and the w/o chain variant. These results demonstrate the necessity of optimizing the combination of the tools with strong interconnection.

Utility of Auto-documented Protocol. Figure 4 shows the performance of our proposed framework in different settings. Compared with using standard protocol crafted manually (i.e., Ours w/ std protocol), the LLM achieves comparable performance with the assistance of auto-documented protocol (i.e., Ours w/ auto protocol), which illustrates the utility of our synthetic protocol. We also observe that our framework substantially outperforms the best baseline even when only using the transformed schema, i.e., Ours w/ auto schema. This result further demonstrates the effectiveness of our tool probing and protocol documenting methods which can extend our proposed framework into diverse new tools without handcrafted protocols. In addition, we also conduct the case study to evaluate the quality of the synthetic protocol and show a concrete example in A.3.

## 7 Discussion

The impact of iteration count in our attribution reflection. Our attribution reflection mechanism enables LLMs to adaptively revise their generated programs according to error messages raised by the program interpreter. We further alternate the maximum reflection count $\alpha$ from 1 to 5 and evaluate the Success Rate with the same setting as Table 1 (ours). As shown in Figure 5, we observe an increasing Success Rate when $\alpha$ shifts from 1 to 3, which illustrates that LLMs can adapt their generation accordingly. We also find a relatively stable trend when the $\alpha$ keeps increasing (from 3 to 5), which indicates that the LLMs can revise most of the errors in 3 iterations. We also analyse the cases of unsuccessful corrections and we find that the generated program may be grammatically

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-09.jpg?height=200&width=702&top_left_y=255&top_left_x=365)

(a) Error distribution

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-09.jpg?height=208&width=190&top_left_y=243&top_left_x=1054)

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-09.jpg?height=200&width=488&top_left_y=247&top_left_x=1255)

(b) Error type distribution

Figure 6: The statistics of the error of our framework. Left: We calculate the success and failure rates for tasks that require different numbers of tools. Right: The percentage of different type of error.

correct but yields incorrect answers and therefore cannot be detected by the compiler (i.e., the False Success phenomena).

Statistics of error cases. To further evaluate the potential advancement and drawback of our method (Table 1), we count the success and failure rates for tasks with different complexity. We first randomly sample a total of 130 tasks from the RestBench and our ToolFlow. Following previous work [14, 9], we assess the task complexity using the number of tools calling in the ground truth solution. Figure 6.a) represents the results. We find that the LLM when equipped with our framework, can effectively solve both short-term planning (i.e., $3 \geq$ tool num.) and long-term planning (i.e., 7 $\leq$ tool num.). We also analyse the type of failure tasks and divide them into four categories shown in Figure 6.b). Most of the errors are derived from misunderstanding the tool documentation or the mismatch between a task and selected tools. A potential solution is to enrich the tool documentation, further clarify the distinction among similar tools, and append negative examples as prior experience in the tool documentation to instruct LLMs to well master a tool. We take it as our future work.

Efficiency at inference. The intensive parameters of LLMs typically raise the concern about inference cost. Thus, we compare the token consumption between our framework (auto $d o c$ ) and strong baselines on the TMDB and ToolFlow datasets and show the results in Figure 7 to explain more intuitively. We observe that although our framework achieves better performance, we spend fewer tokens compared with all baselines. The potential reason is that our framework benefits from the inherent advancement of programming language which sup-

![](https://cdn.mathpix.com/cropped/2024_06_04_20442f848b1cffa6777ag-09.jpg?height=377&width=697&top_left_y=1145&top_left_x=1061)
ports control of the workflow and allows the composition of multiple tools to perform complex logical operations. By contrast, the previous baseline interacts with tools in a step-by-step manner, leading to a long task-solving trajectory with substantial inference costs. We also compute the token consumption for our probing process, where each tool costs 2703 tokens to prob on average. More details can be found in A. 1

Case study. We conduct the case studies and find that our proposed framework is more effective at utilizing various tools to solve complex tasks. We also provide concrete examples to intuitively explain each component of our method in A. 3

## 8 Conclusions

We presented Automatic Tool Chain (ATC), a framework that enables the LLM to act as a multi-tool user. ATC enables LLMs to learn input-output schemas and data flow dependency of various tools from documented tool protocols, programmatically generating a chain of tools to solve complex tasks. ATC overcomes the limitations of existing tool learning methods, including relying on manually designed workflows and lengthy inference steps. On top of ATC, we propose a black-box probing method, empowering the LLM to act as a multi-tool learner that can automatically discover tool protocols and teach itself to master new tools. Extensive experiments conducted on existing datasets and a newly created challenging benchmark demonstrate that an LLM, when equipped with our framework, achieves the best performance compared with all the baselines. We expect future research to further calibrate the output of generated programs, mitigating the false success phenomena, i.e., the program triggers no runtime error but still gives an incorrect answer. We are also interested in exploring the integration of our framework into vision foundation models, to develop a multi-modal agent to solve complex practical tasks.

## References

[1] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Association for Computational Linguistics: ACL, 2023.

[2] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. A Survey on Knowledge Distillation of Large Language Models. arXiv, 2024.

[3] Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903, 2023.

[4] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint $\underline{\operatorname{arXiv}: 2304.08354,2023 .}$

[5] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools. Neural Information Processing Systems: NeurIPS, 2023.

[6] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. WebCPM: Interactive web search for Chinese long-form question answering. In Association for Computational Linguistics: ACL, 2023.

[7] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.

[8] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.

[9] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. International Conference on Learning Representations: ICLR, 2023.

[10] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024.

[11] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

[12] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024.

[13] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with Massive APIs. arXiv preprint arXiv:2305.15334, 2023.

[14] Yifan Song, Weimin Xiong, Dawei Zhu, Chengzu Li, Ke Wang, Ye Tian, and Sujian Li. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs. arXiv, 2023.

[15] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating and retrieving from specialized toolsets. International Conference on Learning Representations: ICLR, 2024.

[16] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772, 2023.

[17] Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. In Findings of the Association for Computational Linguistics: EMNLP, 2023.

[18] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations: ICLR, 2023.

[19] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.

[20] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Lumos: Learning agents with unified data, modular design, and open-source llms. arXiv preprint arXiv:2311.05657, 2023.

[21] Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. LLMs in the Imaginarium: tool learning through simulated trial and error. arXiv preprint arXiv:2403.04746, 2024.

[22] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Neural Information Processing Systems: NeurIPS, 2023.

[23] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.

[24] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.

[25] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv:2308.00675, 2023.

[26] Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. Learning to use tools via cooperative and interactive agents. arXiv preprint arXiv:2403.03031, 2024.

[27] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201, 2024.

[28] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Bioinformatics, 2024.

[29] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324, 2024.

[30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 2024.

[31] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. arXiv preprint arXiv:2401.05268, 2024.

[32] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, et al. If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. arXiv preprint $\underline{\operatorname{arXiv}: 2401.00812,2024 .}$

[33] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.

[34] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.

[35] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.

[36] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Proceedings of Machine Learning Research: PMLR, 2023.

[37] Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Experiential co-learning of software-developing agents. arXiv preprint arXiv:2312.17025, 2023.

[38] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19632-19642, 2024.

[39] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv, 2023.

[40] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.

[41] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023 .

[42] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.

[43] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Neural Information Processing Systems: NeurIPS, 2023.

[44] Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. Leti: Learning to generate from textual interactions. arXiv preprint arXiv:2305.10314, 2023.

[45] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.

[46] Haoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023.

[47] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023.

[48] Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, and Jane Dwivedi-Yu. Toolverifier: Generalization to new tools via self-verification. arXiv preprint arXiv:2402.14158, 2024.

[49] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction. Neural Information Processing Systems: NeurIPS, 2023.

[50] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun Ren. Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. In Proceedings of the AAAI Conference on Artificial Intelligence: AAAI, 2024.

[51] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[52] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504, 2023.

[53] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. API-bank: A comprehensive benchmark for tool-augmented LLMs. In Association for Computational Linguistics: EMNLP, 2023.

[54] Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, et al. Tooleyes: Fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios. arXiv preprint arXiv:2401.00741, 2024.

[55] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714, 2024.

[56] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024.
