# TRUSTLLM: TRUSTWORTHINESS IN LARGE LANGUAGE MODELS 

\author{
Lichao Sun ${ }^{1 * \dagger}$ Yue Huang ${ }^{1,2 \dagger * \ddagger}$ Haoran Wang ${ }^{3 *}$ Siyuan Wu ${ }^{1 * \ddagger}$ Qihui Zhang ${ }^{1 * \ddagger}$ Yuan $^{1 *}{ }^{4 * \ddagger}$ <br> Chujie Gao ${ }^{1 * \ddagger}$ Yixin Huang $^{5 *}$ Wenhan Lyu ${ }^{6 *}$ Yixuan Zhang ${ }^{6 *}$ Xiner Li $^{7 *}$ <br> Hanchi Sun ${ }^{1 *}$ Zhengliang Liu ${ }^{8 *}$ Yixin Liu $^{1 *}$ Yijue Wang $^{9 *}$ Zhikun Zhang ${ }^{10 *}$ <br> Bertie Vidgen $^{11} \quad$ Bhavya Kailkhura $^{12}$ Caiming Xiong ${ }^{13}$ Chaowei Xiao ${ }^{14}$ <br> Chunyuan Li ${ }^{15}$ Eric Xing ${ }^{16,43}$ Furong Huang ${ }^{17}$ Hao Liu ${ }^{18}$ Heng Ji ${ }^{19}$ Hongyi Wang ${ }^{16}$ <br> Huan Zhang ${ }^{19}$ Huaxiu Yao ${ }^{20}$ Manolis Kellis ${ }^{21}$ Marinka Zitnik ${ }^{22} \quad$ Meng Jiang $^{2}$ <br> Mohit Bansal ${ }^{20}$ James Zou ${ }^{10} \quad$ Jian Pei $^{23}$ Jian Liu ${ }^{24}$ Jianfeng Gao ${ }^{15} \quad$ Jiawei Han $^{19}$ <br> Jieyu Zhao ${ }^{25}$ Jiliang Tang ${ }^{26}$ Jindong Wang ${ }^{27}$ Joaquin Vanschoren ${ }^{28}$ <br> John Mitchell ${ }^{10}$ Kai Shu $^{3}$ Kaidi Xu ${ }^{29}$ Kai-Wei Chang ${ }^{30}$ Lifang He ${ }^{1}$ Lifu Huang ${ }^{31}$ <br> Michael Backes ${ }^{32}$ Neil Zhenqiang Gong ${ }^{23} \quad$ Philip S. Yu $^{33} \quad$ Pin-Yu Chen ${ }^{34}$ <br> Quanquan Gu ${ }^{30}$ Ran $X^{13} \quad$ Rex Ying $^{35}$ Shuiwang Ji ${ }^{7}$ Suman Jana $^{36} \quad$ Tianlong Chen $^{20}$ <br> Tianming Liu ${ }^{8}$ Tianyi Zhou ${ }^{17}$ Willian Wang ${ }^{37}$ Xiang Li $^{38} \quad$ Xiangliang Zhang $^{2}$

![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-001.jpg?height=52&width=1341&top_left_y=1080&top_left_x=392) <br> Yinzhi Cao $^{41}$ Yong Chen ${ }^{42} \quad$ Yue Zhao $^{25}$ <br> ${ }^{1}$ Lehigh University ${ }^{2}$ University of Notre Dame ${ }^{3}$ Illinois Institute of Technology <br> ${ }^{4}$ University of Cambridge ${ }^{5}$ Institut Polytechnique de Paris <br> ${ }^{6}$ William \& Mary ${ }^{7}$ Texas A\&M University ${ }^{8}$ University of Georgia <br> ${ }^{9}$ Samsung Research America ${ }^{10}$ Stanford University ${ }^{11}$ University of Oxford <br> ${ }^{12}$ Lawrence Livermore National Laboratory ${ }^{13}$ Salesforce Research <br> ${ }^{14}$ University of Wisconsin, Madison ${ }^{15}$ Microsoft Research ${ }^{16}$ Carnegie Mellon University <br> ${ }^{17}$ University of Maryland ${ }^{18}$ University of California, Berkeley <br> ${ }^{19}$ University of Illinois Urbana-Champaign ${ }^{20}$ UNC Chapel Hill <br> ${ }^{21}$ Massachusetts Institute of Technology ${ }^{22}$ Harvard University <br> ${ }^{23}$ Duke University ${ }^{24}$ University of Tennessee, Knoxville ${ }^{25}$ University of Southern California <br> ${ }^{26}$ Michigan State University ${ }^{27}$ Microsoft Research Asia ${ }^{28}$ Eindhoven University of Technology <br> ${ }^{29}$ Drexel University ${ }^{30}$ University of California, Los Angeles <br> ${ }^{31}$ Virginia Tech ${ }^{32}$ CISPA ${ }^{33}$ University of Illinois Chicago ${ }^{34}$ IBM Research AI <br> ${ }^{35}$ Yale University ${ }^{36}$ Columbia University ${ }^{37}$ University of California, Santa Barbara <br> ${ }^{38}$ Massachusetts General Hospital ${ }^{39}$ Northwestern University <br> ${ }^{40}$ Florida International University ${ }^{41}$ Johns Hopkins University <br> ${ }^{42}$ University of Pennsylvania ${ }^{43}$ Mohamed Bin Zayed University of Artificial Intelligence
}


#### Abstract

Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TRUSTLLM, a


[^0]comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating $\mathbf{1 6}$ mainstream LLMs in TRUSTLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. For instance, LLMs like GPT-4, ERNIE, and Llama2, which exhibit strong performance in stereotype categorization, tend to reject stereotypical statements more reliably. Similarly, Llama2-70b and GPT-4, known for their proficiency in natural language inference, demonstrate enhanced resilience to adversarial attacks. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Notably, Llama2 demonstrates superior trustworthiness in several tasks, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like moderator, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs, such as Llama2, may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we've uncovered key insights into the multifaceted trustworthiness in LLMs. In terms of truthfulness, LLMs often struggle to provide truthful responses due to the noise, misinformation, or outdated information in their training data. Notably, LLMs enhanced with external knowledge sources show a marked performance improvement. For safety, most open-source LLMs significantly lag behind that of proprietary LLMs, particularly in areas like jailbreak, toxicity, and misuse. Also, the challenge of balancing safety without over-caution remains. Regarding fairness, most LLMs exhibit unsatisfactory performance in stereotype recognition, with even the best-performing (GPT-4) having an overall accuracy of only $65 \%$. The robustness of LLMs shows significant variability, especially in open-ended tasks and out-of-distribution tasks. Regarding privacy, while LLMs show an awareness of privacy norms, the understanding and handling of private information vary widely, with some models even demonstrating information leakage when tested on the Enron Email Dataset. Lastly, in machine ethics, LLMs exhibit a basic moral understanding but fall short in complex ethical scenarios. These insights underscore the complexity of trustworthiness in LLMs and highlight the need for continued research efforts to enhance their reliability and ethical alignment. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness. We advocate that the establishment of an $\mathrm{AI}$ alliance between industry, academia, the open-source community as well as various practitioners to foster collaboration is imperative to advance the trustworthiness of LLMs. Our dataset, code, and toolkit will be available at $($ ) https://github.com/HowieHwong/TrustLLM and the leaderboard is released at https://trustllmbenchmark.github.io/TrustLLM-Website/.

Content Warning: This paper may contain some offensive content generated by LLMs.

## Contents

1 Introduction ..... 5
2 Observations and Insights ..... 9
2.1 Overall Observations ..... 9
2.2 Novel Insights into Individual Dimensions of Trustworthiness ... ..... 9
3 Background ..... 12
3.1 Large Language Models (LLMs) ..... 12
3.2 Evaluation on LLMs ..... 12
3.3 Developers and Their Approaches to Enhancing Trustworthiness in LLMs ..... 13
3.4 Trustworthiness-related Benchmarks ..... 14
4 Guidelines and Principles for Trustworthiness Assessment of LLMs ..... 16
4.1 Truthfulness ..... 16
4.2 Safety ..... 17
4.3 Fairness ..... 17
4.4 Robustnesss ..... 18
4.5 Privacy ..... 18
4.6 Machine Ethics ..... 18
4.7 Transparency ..... 19
4.8 Accountability ..... 19
4.9 Regulations and Laws ..... 20
5 Preliminaries of TrustLLM ..... 21
5.1 Curated List of LLMs ..... 21
5.2 Experimental Settings ..... 23
6 Assessment of Truthfulness ..... 26
6.1 Misinformation Generation ..... 26
6.1.1 Using Merely Internal Knowledge ..... 26
6.1.2 Integrating External Knowledge ..... 27
6.2 Hallucination ..... 28
6.3 Sycophancy in Responses ..... 30
6.3.1 Persona-based Sycophancy . . ..... 30
6.3.2 Preference-driven Sycophancy ..... 31
6.4 Adversarial Factuality ..... 32
7 Assessment of Safety ..... 34
7.1 Jailbreak ..... 34
7.2 Exaggerated Safety ..... 38
7.3 Toxicity ..... 39
7.4 Misuse ..... 40
8 Assessment of Fairness ..... 42
8.1 Stereotypes ..... 42
8.2 Disparagement ..... 44
8.3 Preference Bias in Subjective Choices ..... 46
9 Assessment of Robustness ..... 48
9.1 Robustness against Input with Natural Noise ..... 48
9.1.1 Ground-Truth Labeled Task Performance ..... 48
9.1.2 Performance in Open-ended Tasks ..... 50
9.2 Assessing Out of Distribution (OOD) Task Resilience ..... 51
9.2.1 OOD Detection ..... 52
9.2.2 OOD Generalization ..... 53
10 Assessment of Privacy Preservation ..... 55
10.1 Privacy Awareness ..... 55
10.2 Privacy Leakage ..... 58
11 Assessment of Machine Ethics ..... 60
11.1 Implicit Ethics ..... 60
11.2 Explicit Ethics ..... 62
11.3 Awareness ..... 63
12 Discussion of Transparency ..... 69
13 Discussion of Accountability ..... 71
14 Open Challenges ..... 73
15 Future Work ..... 75
16 Conclusion ..... 78
17 Acknowledgement ..... 78

## 1 Introduction

The advent of large language models (LLMs) marks a significant milestone in natural language processing (NLP) and generative AI, as evidenced by numerous foundational studies [1, 2]. The exceptional capabilities of these models in NLP have garnered widespread attention, leading to diverse applications that impact every aspect of our lives. LLMs are employed in a variety of language-related tasks, including automated article writing [3], the creation of blog and social media posts, and translation [4]. Additionally, they have improved search functionalities, as seen in platforms like Bing Chat [5, 6, 7], and other applications [8]. The efficacy of LLMs is distinctly evident in various other areas of human endeavor. For example, models such as Code Llama [9] offer considerable assistance to software engineers [10]. In the financial domain, LLMs like BloombergGPT [11] are employed for tasks including sentiment analysis, named entity recognition, news classification, and question answering. Furthermore, LLMs are increasingly being applied in scientific research [12, 13, 14, 15], spanning areas like medical applications [16, 17, 18, 19, 20, 21, 22, 23, 24, 25], political science [26], law [27, 28], chemistry [29, 30], oceanography [31, 32], education [33], and the arts [34], highlighting their extensive and varied impact.

The outstanding capabilities of LLMs can be attributed to multiple factors, such as the usage of large-scale raw texts from the Web as training data (e.g., PaLM $[35,36]$ was trained on a large dataset containing more than 700 billion tokens [37]), the design of transformer architecture with a large number of parameters (e.g., GPT-4 is estimated to have in the range of 1 trillion parameters [38]), and advanced training schemes that accelerate the training process, e.g., low-rank adaptation (LoRA) [39], quantized LoRA [40], and pathway systems [41]. Moreover, their outstanding instruction following capabilities can be primarily attributed to the implementation of alignment with human preference [42]. Prevailing alignment methods use reinforcement learning from human feedback (RLHF) [43] along with various alternative approaches [44, 45, 46, 47, 48, 49, $50,51,52,53,54,55]$. These alignment strategies shape the behavior of LLMs to more closely align with human preferences, thereby enhancing their utility and ensuring adherence to ethical considerations.

However, the rise of LLMs also introduces concerns about their trustworthiness. Unlike traditional language models, LLMs possess unique characteristics that can potentially lead to trustworthiness issues. 1) Complexity and diversity of outputs from LLMs, coupled with their emergent generative capabilities. LLMs demonstrate an unparalleled ability to handle a broad spectrum of complex and diverse topics. Yet, this very complexity can result in unpredictability and, consequently, the possibility of generating inaccurate or misleading outputs $[56,57,58]$. Simultaneously, their advanced generative capabilities open avenues for misuse by malicious actors, including the propagation of false information [59] and facilitating cyberattacks [60]. For instance, attackers might use LLMs to craft deceptive and misleading text that lures users to click on malicious links or download malware. Furthermore, LLMs can be exploited for automated cyberattacks, such as generating numerous fake accounts and comments to disrupt the regular operation of websites. A significant threat also comes from techniques designed to bypass the safety mechanisms of LLMs, known as jailbreaking attacks [61], which allows attackers to misuse LLMs illicitly. 2) Data biases and private information in large training datasets. One primary challenge to trustworthiness arises from potential biases in training datasets, which have significant implications for the fairness of content generated by LLMs. For example, a male-centric bias in the data may yield outputs that mainly reflect male perspectives, thereby overshadowing female contributions and viewpoints [62]. In a similar vein, a bias favoring a particular cultural background can result in responses biased toward that culture, thus disregarding the diversity present in other cultural contexts [63]. Another critical issue concerns the inclusion of sensitive personal information within training datasets. In the absence of stringent safeguards, this data becomes susceptible to misuse, potentially leading to privacy breaches [64]. This issue is especially acute in the healthcare sector, where maintaining the confidentiality of patient data is of utmost importance [65]. 3) High user expectations. Users may have high expectations regarding the performance of LLMs, expecting accurate and insightful responses that emphasize the model's alignment with human values. Many researchers are expressing concerns about whether LLMs align with human values. A misalignment could significantly impact their broad applications across various domains. For instance, an LLM considers a behavior appropriate in some situations. Still, humans may view it as inappropriate, leading to conflicts and contradictions in its applications, as highlighted in specific cases [66].

The developers of LLMs have undertaken significant efforts to address the concerns mentioned above. OpenAI [67] has taken measures to ensure LLMs' trustworthiness in the training data phase, training methods, and
downstream applications. WebGPT [7] is introduced to assist human evaluation in identifying inaccurate information in LLM responses. Meta [68], dedicated to responsible AI, bases its approach on five pillars: privacy, fairness, robustness, transparency, and accountability. The introduction of Llama2 [69] sets new safety alignment benchmarks for LLMs, encompassing extensive safety investigations in pretraining, fine-tuning, and red teaming. Further discussion on the various strategies employed by developers to ensure the trustworthiness of LLMs can be found in Section 3.3. Despite these concerted efforts, a persistent question remains: To what extent can we genuinely trust LLMs?

To tackle these crucial questions, it is essential to address the fundamental issue of benchmarking how trustworthy LLMs are. What key elements define the trustworthiness of large language models, and from various perspectives, how should this trustworthiness be assessed? Furthermore, exploring methodologies to practically evaluate trustworthiness across these dimensions is vital. However, answering these questions is far from straightforward. The primary challenges include: 1) Definition of comprehensive aspects. One of the main obstacles is the absence of a universally accepted set of criteria that comprehensively encapsulates all facets of trustworthiness. This lack of standardized metrics makes it difficult to uniformly assess and compare the trustworthiness of different LLMs. 2) Scalability and generalizability: Creating benchmarks that are scalable across different sizes and types of LLMs and generalizable across various domains and applications is a complex task; 3) Practical evaluation methodologies. Effective prompts need to be designed to test obvious trustworthiness issues and uncover more subtle biases and errors that might not be immediately apparent. This requires a deep understanding of both the technology and the potential societal impacts of its outputs.

Previous studies $[70,71,72]$, have established foundational insights into the trustworthiness of LLMs. These studies have proposed approaches for evaluating LLMs and formulated taxonomies to measure their trustworthiness. However, certain taxonomies $[70,73]$ have not fully encompassed all aspects related to LLM trustworthiness. Additionally, some taxonomies $[71,72]$ focus on fine-grained distinctions, resulting in overlapping subcategories that complicate the establishment of clear evaluation benchmarks. Consequently, there is a need for a more comprehensive and nuanced approach to accurately assess the trustworthiness of LLMs.

Here, we present TRUSTLLM, a unified framework to support a comprehensive analysis of trustworthiness in LLM, including a survey of existing work, organizing principles of different dimensions of trustworthy LLMs, a novel benchmark, and a thorough evaluation of trustworthiness for mainstream LLMs. Specifically, we address the three challenges above as follows.

- Identification of eight facets of trustworthiness. To explore how trustworthy LLMs are, we incorporated domain knowledge from across AI, machine learning, data mining, human-computer interaction (HCI), and cybersecurity. We conducted an extensive review of 600 papers on LLM trustworthiness published in the past five years and identified eight key aspects that define the trustworthiness of LLMs, which are truthfulness, safety, fairness, robustness, privacy, machine ethics, transparency, and accountability. In this work, to facilitate our investigation, we separate utility (i.e., functional effectiveness) from the eight identified dimensions and define trustworthy LLMs as "to be trustworthy, LLMs must appropriately reflect characteristics such as truthfulness, safety, fairness, robustness, privacy, machine ethics, transparency, and accountability." The detailed discussion can be found in Section 4.
- Selection of comprehensive and diverse LLMs for investigation. By evaluating 16 LLMs, encompassing both proprietary and open-source models, we cover a broad spectrum of model sizes, training strategies, and functional capabilities. This diversity guarantees that TRUSTLLM is not confined to a specific type or size of LLM. It also establishes a comprehensive evaluation framework for assessing the trustworthiness of future LLMs.
- Benchmarking and evaluation across various tasks and datasets: We benchmark 30 datasets to comprehensively evaluate the functional capabilities of LLMs, ranging from simple classification to complex generation tasks. Each dataset presents unique challenges and benchmarks the LLMs across multiple dimensions of trustworthiness. Meanwhile, diverse evaluation metrics are employed for understanding the capabilities of LLMs. This approach ensures that the evaluation is thorough and multifaceted.

Contributions. The outcomes of TRUSTLLM evaluation are summarized in Figure 1, with observations and insights presented in Section 2. We briefly highlight our contributions to this work as follows. (1) Firstly, we have proposed a set of guidelines based on a comprehensive literature review for evaluating the trustworthiness

![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-007.jpg?height=1561&width=1651&top_left_y=233&top_left_x=237)

Figure 1: Ranking card of 16 LLMs' trustworthiness performance on TRUSTLLM. If the model's performance ranks among the top eight, we display its ranking, with darker blue indicating a better performance. In each subsection, all the ranking is based on the overall performance if not specified otherwise.

of LLMs, which is a taxonomy encompassing eight aspects, including truthfulness, safety, fairness, robustness, privacy, machine ethics, transparency, and accountability. (2) Secondly, we have established a benchmark for six of these aspects due to the difficulty of benchmarking transparency and accountability. This is the first comprehensive and integrated benchmark comprising over 18 subcategories, covering more than 30 datasets and 16 LLMs, including proprietary and open-weight ones. Besides the trustworthiness ranking of these models illustrated in Figure 1, we present the evaluation details in each subsequent section. (3) Last but not least, drawing from extensive experimental results, we have derived insightful findings (detailed in Section 2). Our evaluation of trustworthiness in LLMs takes into account both the overall observation and individual findings based on each dimension, emphasizing the relationship between effectiveness and trustworthiness, the prevalent lack of alignment in most LLMs, the disparity between proprietary and open-weight LLMs, and the opacity of current trustworthiness-related technologies. We aim to provide valuable insights for future research, contributing to a more nuanced understanding of the trustworthiness landscape in large language models.

Roadmap. First, in Section 2, we summarize and present the empirical findings of TrustLLM. Then, in Section 3, we review LLMs and related work on trustworthiness, including current trustworthy technologies and benchmarks. Following this, in Section 4, we propose guidelines and principles for trustworthy LLMs. Section 5 introduces the selected LLMs, tasks, datasets, and experimental settings used in our benchmark. Sections 6-13 offer an overview and assessment of trustworthy LLMs from eight different perspectives. In Section 14, we identify and discuss the current and upcoming challenges that TrustLLM faces. Section 15 is dedicated to discussing future directions. Finally, our conclusions are presented in Section 16.

TRUSTLLM

## 2 Observations and Insights

To facilitate the overall understanding of our study, in this section, we first present the observations and insights we have drawn based on our extensive empirical studies in this work.

### 2.1 Overall Observations

Trustworthiness is closely related to utility". Our findings indicate a positive correlation between trustworthiness and utility, particularly evident in specific tasks. For example, in moral behavior classification (Section 11.1) and stereotype recognition tasks (Section 8.1), LLMs like GPT-4 that possess strong language understanding capabilities tend to make more accurate moral judgments and reject stereotypical statements more reliably. Similarly, Llama2-70b and GPT-4, known for their proficiency in natural language inference, demonstrate enhanced resilience against adversarial attacks. Furthermore, we observed that the trustworthiness rankings of LLMs often mirror their positions on utility-focused leaderboards, such as MT-Bench [74], OpenLLM Leaderboard [75], and others. This observation underscores the intertwined nature of trustworthiness and utility, highlighting the importance for both developers and users to consider these aspects simultaneously when implementing and utilizing LLMs.

Most LLMs are "overly aligned". We have found that many LLMs exhibit a certain degree of over-alignment (i.e., exaggerated safety), which can compromise their overall trustworthiness. Such LLMs may identify many innocuous prompt contents as harmful, thereby impacting their utility. For instance, Llama2-7b obtained a $57 \%$ rate of refusal in responding to prompts that were, in fact, not harmful. Consequently, it is essential to train LLMs to understand the intent behind a prompt during the alignment process, rather than merely memorizing examples. This will help in lowering the false positive rate in identifying harmful content.

Generally, proprietary LLMs outperform most open-weight LLMs in trustworthiness. However, a few open-source LLMs can compete with proprietary ones. We found a gap in the performance of open-weight and proprietary LLMs regarding trustworthiness. Generally, proprietary LLMs (e.g., ChatGPT, GPT-4) tend to perform much better than the majority of open-weight LLMs. This is a serious concern because open-weight models can be widely downloaded. Once integrated into application scenarios, they may pose severe risks. However, we were surprised to discover that Llama2 [69], a series of open-weight LLMs, surpasses proprietary LLMs in trustworthiness in many tasks. This indicates that open-weight models can demonstrate excellent trustworthiness even without adding external auxiliary modules (such as a moderator [76]). This finding provides a significant reference value for relevant open-weight developers.

Both the model itself and trustworthiness-related technology should be transparent (e.g., open-sourced). Given the significant gap in performance regarding trustworthiness among different LLMs, we emphasize the importance of transparency, both in the models themselves and in the technologies aimed at enhancing trustworthiness. As highlighted in recent studies [77, 78], a thorough understanding of the training mechanisms of models, including aspects such as parameter and architecture design, forms the cornerstone of researching LLMs. Our experiments found that while some proprietary LLMs exhibit high trustworthiness (e.g., ERNIE [79]), the specifics of the underlying technologies remain undisclosed. Making such trustworthy technologies transparent or open-source can promote the broader adoption and improvement of these techniques, significantly boosting the trustworthiness of LLMs. This, in turn, makes LLMs more reliable and strengthens the AI community's overall trust in these models, thereby contributing to the healthy evolution of AI technology.

### 2.2 Novel Insights into Individual Dimensions of Trustworthiness

Truthfulness. Truthfulness in AI systems refers to the accurate representation of information, facts, and results. Our findings indicate that: 1) Proprietary LLMs like GPT-4 and open-source LLMs like LLama2 often struggle to provide truthful responses when relying solely on their internal knowledge. This issue is primarily due to noise in their training data, including misinformation or outdated information, and the lack of generalization capability in the underlying Transformer architecture [80]. 2) Furthermore, all LLMs face challenges in zero-shot commonsense reasoning tasks, suggesting difficulty in tasks that are relatively[^1]straightforward for humans. 3) In contrast, LLMs with augmented external knowledge demonstrate significantly improved performance, surpassing state-of-the-art results reported on original datasets. 4) We observe a notable discrepancy among different hallucination tasks. Most LLMs show fewer hallucinations in multiple-choice question-answering tasks compared to more open-ended tasks such as knowledge-grounded dialogue, likely due to prompt sensitivity (Section 14). 5) Additionally, we find a positive correlation between sycophancy and adversarial actuality. Models with lower sycophancy levels are more effective in identifying and highlighting factual errors in user inputs.

Safety. Safety in LLMs is crucial for avoiding unsafe or illegal outputs and ensuring engagement in healthy conversations [72]. In our experiments (Section 7), we found that: 1) The safety of most open-source LLMs remains a concern and significantly lags behind that of proprietary LLMs, particularly in areas like jailbreak, toxicity, and misuse. 2) Notably, LLMs do not uniformly resist different jailbreak attacks. Our observations revealed that various jailbreak attacks, particularly leetspeak attacks [61], vary in their success rates against LLMs. This underscores the need for LLM developers to adopt a comprehensive defense strategy against diverse attack types. 3) Balancing safety is a challenge for most LLMs; those with stringent safety protocols often show exaggerated caution, as evident in the Llama2 series and ERNIE. This suggests that many LLMs are not fully aligned and may rely on superficial alignment knowledge.

Fairness. Fairness is the ethical principle of ensuring that LLMs are designed, trained, and deployed in ways that do not lead to biased or discriminatory outcomes and that they treat all users and groups equitably. In our experiments (Section 8), we have found that 1) The performance of most LLMs in identifying stereotypes is not satisfactory, with even the best-performing GPT-4 having an overall accuracy of only $65 \%$. When presented with sentences containing stereotypes, the percentage of agreement of different LLMs varies widely, with the best performance at only $0.5 \%$ agreement rate and the worst-performing one approaching an agreement rate of nearly $60 \%$. 2) Only a few LLMs, such as Oasst-12b [81] and Vicuna-7b [82], exhibit fairness in handling disparagement; most LLMs still display biases towards specific attributes when dealing with questions containing disparaging tendencies. 3) Regarding preferences, most LLMs perform very well on the plain baseline, maintaining objectivity and neutrality or refusing to answer directly. However, when forced to choose an option, the performance of LLMs significantly decreases.

Robustness. Robustness is defined as a system's ability to maintain its performance level under various circumstances [83]. In our experiments (Section 9), we found that: 1) The Llama2 series and most proprietary LLMs surpass other open-source LLMs in traditional downstream tasks. 2) However, LLMs exhibit significant variability in open-ended task performance. The least effective model shows an average semantic similarity of only $88 \%$ before and after perturbation, substantially lower than the top performer at $97.64 \%$. 3) In terms of OOD robustness, LLMs demonstrate considerable performance variation. The top-performing model, GPT-4, exhibits a RtA (Refuse to Answer) rate of over $80 \%$ in OOD detection and an average F1 score of over $92 \%$ in OOD generalization. In contrast, the least effective models show an RtA rate of merely $0.4 \%$ and an F1 score of around $30 \%$. 4) Additionally, our observations reveal no consistent positive correlation between parameter size and OOD performance, as evidenced by the varied performance levels of Llama2 models regardless of their parameter size.

Privacy. Privacy encompasses the norms and practices aimed at protecting human autonomy, identity, and dignity [83]. In our experiments (Section 10), we found that: 1) Most LLMs demonstrate a certain level of privacy awareness, as evidenced by a significant increase in the likelihood of these models refusing to respond to queries about private information when informed that they must adhere to privacy policy. 2) The Pearson correlation coefficient measuring agreement between humans and LLMs on the use of privacy information varies greatly. The best-performing model, ChatGPT, achieves a correlation of 0.665, while Oass-12b exhibits a surprising negative correlation, less than zero, indicating a divergent understanding of privacy compared to humans. 3) We observed that nearly all LLMs show some degree of information leakage when tested on the Enron Email Dataset $[84]$.

Machine Ethics. Machine ethics ensure the moral behaviors of man-made machines utilizing AI, commonly referred to as AI agents $[85,86]$. In our experiments (Section 11), we found that: 1) LLMs have developed a specific set of moral values, yet there remains a significant gap in fully aligning with human ethics. The accuracy of most LLMs in implicit tasks within low-ambiguity scenarios falls below $70 \%$, irrespective of the

## TRUSTLLM

dataset. In high-ambiguity scenarios, performance varies considerably among different LLMs; for instance, the Llama2 series achieves an RtA of $99.9 \%$, while others score less than $70 \%$. 2) In terms of awareness, the best-performing model GPT-4 achieves an average accuracy rate of $94 \%$ over four awareness datasets. Other LLMs exhibit decent but not substantial awareness.

TRUSTLLM

## 3 Background

### 3.1 Large Language Models (LLMs)

A language model (LM) aims to predict the probability distribution over a sequence of tokens. Scaling the model size and data size, large language models (LLMs) have shown "emergent abilities" $[87,88,89]$ in solving a series of complex tasks that cannot be dealt with by regular-sized LMs. For instance, GPT-3 can handle few-shot tasks by learning in context, in contrast to GPT-2, which struggles in this regard. The success of LLMs is primarily attributed to the Transformer architecture [80]. Specifically, almost all the existing LLMs employ a stack of transformer blocks, each consisting of a Multi-Head Attention layer followed by a feedforward layer interconnected by residual links. Built upon this transformer-based architecture, there are three primary designs of LLMs: encoder-decoder architecture [90], causal-decoder architecture, and prefix-decoder architecture. Among them, the most widely used architecture is the causal decoder, which employs an attention mask to ensure that each input token only attends to previous tokens and itself. In this survey, we mainly focus on the causal-decoder architecture. The training of LLMs is usually composed of three steps: pre-training, instruction finetuning, and alignment tuning. We will introduce each step in detail.

During pre-training, LLMs learn world knowledge and basic language abilities on large-scale corpora. To improve model capacity, researchers established some scaling laws to show the compute-optimal ratio between the model size and data size, including KM scaling law [91] and Chinchilla scaling law [92]. When the scale reaches certain levels, LLMs show emergent abilities to solve complex tasks, instruction following, in-context learning, and step-by-step reasoning. These abilities endow LLMs to be general-purpose task solvers. To further elicit the instruction-following and in-context learning ability of LLMs, instruction tuning suggests creating appropriate task instructions or particular in-context learning methods to enhance the ability of LLMs to generalize to tasks they have not encountered before. During the alignment training phase, LLMs are trained to align with human values, e.g., being helpful, honest, and harmless, instead of producing harmful content. For this purpose, two kinds of alignment training methods, including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), are proposed in InstructGPT, which is the fundamental algorithm behind the ChatGPT.

SFT guides the LLMs to understand the prompts and generate meaningful responses, which can be defined as follows. Given an instruction prompt $x$, we want the LLM to generate a response aligned with the humanwritten response $y$. The SFT loss is defined as the cross-entropy loss between the human-written response and the LLM-generated response, i.e., $\mathcal{L}_{\mathrm{SFT}}=-\sum_{t} \log p\left(y_{t} \mid x, y_{<t}\right)$, where $y_{<t}$ represents the sequence of tokens up to but not including the current token $y_{t}$. However, the limitation of SFT is that it only provides a single human-written response for each prompt, which is insufficient to provide a fine-grained comparison between the sub-optimal ones and capture the diversity of human responses. To address this issue, RLHF [43] is proposed to provide fine-grained human feedback with pair-wise comparison labeling. Typical RLHF includes three main steps: 1) SFT on high-quality instruction set; 2) collecting manually ranked comparison response pairs and training a reward model for quality assessment; 3 ) optimizing the SFT model under the PPO [93] reinforcement learning framework with the reward model from the second step. To prevent over-optimization in step 3), a KL-divergence regularization term between the current and SFT models is added to the loss function. However, the PPO algorithm is not stable during training. Thus, Reward rAnked Fine-Tuning (RAFT) [94] is proposed to replace Proximal Policy Optimization (PPO) training with direct learning on the high-ranked samples filtered by the reward model. Nevertheless, these online algorithms require interaction between policy, behavior policy, reward, and value model, which requires fine-grained tuning on the hyper-parameters to achieve stability and generalizability. To prevent this, offline algorithms like ranking-based approaches, including Direct Preference Optimization (DPO) and Preference Ranking Optimization (PRO), and language-based approaches, including Conditional Behavior Cloning [95], Chain of Hindsight [96], and Stable Alignment [97] are proposed. These methods eliminate the risk of overfitting a reward model and improve training stability using preference ranking data.

### 3.2 Evaluation on LLMs

Evaluation of LLMs is a fast-evolving field involving multi-dimensional evaluation across various tasks, datasets, and benchmarks [98]. It encompasses a wide range of domains, starting with traditional NLP tasks,
where LLMs are assessed for natural language understanding, including tasks like sentiment analysis [99, 100, 101], text classification [102, 103], natural language inference [101, 104], etc. The evaluation of LLMs also extends to reasoning tasks [98], covering mathematical reasoning [101, 105], logical reasoning [106, 107], and other reasoning parts; alongside natural language generation tasks like summarization $[101,108]$ and question answering [101, 109]; as well as including multilingual tasks [110]. The evaluation also requires careful studies on robustness, especially in challenging situations such as out-of-distribution (OOD) and adversarial robustness [98, 111, 112], and learning rate tuning [113]. For trustworthiness, some work indicates that LLMs tend to absorb and express harmful biases and toxic content in their training data $[114,115]$. This underscores the need for comprehensive evaluation methodologies and a heightened focus on various trustworthiness aspects of LLMs [71], and we will discuss them in section 3.4. Moreover, the application of LLMs expands into many other fields [116] including computational social science [117], legal task [118, 119, 120], and psychology [121]. Besides, evaluating LLMs in natural science and engineering provides insights into their capabilities in mathematics $[122,123]$, general science $[29,124]$, and engineering $[125,126]$ domains. In the medical field, LLMs have been evaluated for their proficiency in addressing medical queries [127, 128], medical examinations $[129,130]$, and functioning as medical assistants [131, 132]. In addition, some benchmarks are designed to evaluate specific language abilities of LLMs like Chinese [133, 134, 135, 136]. Besides, agent applications [137] underline their capabilities for interaction and using tools [138, 139, 140, 141]. Beyond these areas, LLMs contribute to different domains, such as education [142], finance [143, 144, 145, 146], search and recommendation [147, 148], personality testing [149]. Other specific applications, such as game design [150] and log parsing [151], illustrate the broad scope of the application and evaluation of LLMs. In addition to conventional text generation evaluations, the evaluations of LLMs have expanded to include their code generation capabilities [152]. Recent studies have highlighted this emerging direction, revealing both the potential and the challenges in LLM-driven code synthesis $[152,153,154,155]$.

In text generation evaluation, diverse untrained automatic evaluation metrics are utilized, including metrics based on n-gram overlap, distance-based measures, diversity metrics, content overlap metrics, and those with grammatical features [156]. Standard traditional metrics, such as BLEU [157] and ROUGE [158] classified as n-gram overlap metrics, estimate between the reference text and a text generated by the model. However, these metrics face limitations, particularly in scenarios where multiple correct methods of text generation exist, as often seen in tasks involving latent content planning or selection, which can also lead to accurate solutions receiving low scores $[159,160]$.

LLM evaluation datasets and benchmarks are vital in evaluating various language models for tasks, reflecting complex real-world language processing scenarios. Benchmarks like GLUE [161] and SuperGLUE [162] encompass various tasks from text categorization and machine translation to dialogue generation. These evaluations are crucial for understanding the capabilities of LLMs in general-purpose language tasks. Additionally, automatic and human evaluations serve as critical methods for LLM evaluation [98].

### 3.3 Developers and Their Approaches to Enhancing Trustworthiness in LLMs

Since trustworthiness has emerged as a critical concern, leading LLM developers have employed various strategies and methodologies to enhance the trustworthiness of their models. This section explores the diverse approaches taken by industry giants like OpenAI, Meta, Anthropic, Microsoft, and Google, highlighting their unique contributions and the shared challenges they face in this vital endeavor.

OpenAI. As one of the most renowned companies in the field of LLMs, OpenAI [67] has taken various measures to ensure the trustworthiness of LLMs in the phase of training data, training methods, and downstream applications. In terms of pre-training data, OpenAI implements management and filtering [163] to remove harmful content. During the alignment phase, OpenAI has introduced WebGPT [7] to assist human evaluation in identifying inaccurate information in LLM responses. Additionally, a Red Teaming Network [164] is established to ensure LLMs' security. They have also defined usage policies [165] for users and referenced moderation [76] for review purposes.

Meta. Meta [68], dedicated to responsible AI, bases its approach on five pillars: privacy, fairness, robustness, transparency, and accountability. The introduction of Llama2 [69] sets new safety alignment benchmarks for LLMs, encompassing extensive safety investigations in pretraining, fine-tuning, and red teaming. Llama2's

TRUSTLLM

safety fine-tuning involves supervised techniques, RLHF, and safe context distillation. This includes queryanswer pair assessments and extensive red teaming efforts by a large team aiming to identify and mitigate unsafe model responses. Recently, Meta proposed LLama Guard [166], demonstrating performance on par with or surpassing existing content moderation tools.

Anthropic. Anthropic [167] has introduced the excellent Claude model [168], which has made significant contributions to the field of trustworthiness. For instance, Anthropic has released a dataset of 38,961 red team attacks for others to analyze [169]. In addition, their researchers have proposed the Self-Correction method, which enables language models to learn complex normative harm concepts, such as stereotypes, biases, and discrimination. Furthermore, Anthropic has put forth General Principles for Constitutional AI [170] and found that relying solely on a list of written principles can replace human feedback.

Microsoft. Microsoft has developed, assessed, and deployed AI systems in a safe, trustworthy, and ethical way by proposing a Responsible AI Standard [171], which includes fairness, reliability\&safety, privacy\&security, inclusiveness, transparency, and accountability. Moreover, it has proposed DecodingTrust [71], a comprehensive assessment of trustworthiness in GPT models, which considers diverse perspectives, including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Moreover, PromptBench [172] comprehensively evaluated the robustness of LLMs on prompts with both natural (e.g., typos and synonyms) and adversarial perturbations.

Google. Google has also proposed many measures to improve the trustworthiness of their LLMs. For instance, for the Palm API, Google provides users with safety filters [173] to prevent generating harmful content. Regarding responsible AI practices, Google's work focuses on promoting the fairness [174], privacy [175], and safety [176]. For instance, their seminal work, "Ethical and social risks of harm from Language Models," delves into the potential adverse effects and underscores the necessity for responsible AI development [177]. Furthering their commitment to ethical AI, DeepMind has formulated a framework to evaluate AI systems in the face of novel threats $[178,179]$. Gemini, described as Google's most advanced and versatile model, has been enhanced with various technologies to ensure its trustworthiness. Google has thoroughly researched potential risks [179] to ensure Gemini is trustworthy, applying advanced techniques from Google Research for adversarial testing [180]. This helps identify and resolve key safety issues during Gemini's deployment.

Baichuan. Baichuan [181], a rising company in multilingual LLMs, is adopting a multi-stage development process to bolster the trustworthiness of its models. Baichuan2 enforces strict data filtering for safety in its Pre-training Stage, employs expert-driven red-teaming for robustness in the Alignment Stage, and integrates DPO and PPO for ethical response tuning in the Reinforcement Learning Optimization Stage [182].

IBM. Before the prevalence of foundation models and generative AI applications, IBM has developed several trustworthy AI products and open-source libraries, such as AIF360, AIX360, ART360, and AI FactSheets 360. Recently, IBM announced Watsonx.ai [183] as an enterprise studio to facilitate the development and deployment of foundation models. Specifically, to assist with building trustworthy and responsible LLMs and generative AI applications, IBM also introduced Watsonx.governance framework [184] for automated performance assessment and risk mitigation in the lifecycle of foundation models.

### 3.4 Trustworthiness-related Benchmarks

Currently, in the domain of trustworthiness-related evaluation, there are many related works. For example, DecodingTrust [185] aims to thoroughly assess several perspectives of trustworthiness in GPT models. The recent study [186] proposes a prompting strategy by designing malicious demonstrations, and conducts an assessment of open-source LLMs on trustworthiness. Do-Not-Answer [73] introduces a dataset specifically designed to test the safeguard mechanisms of LLMs by containing only prompts that responsible models should avoid answering. SafetyBench [187] is a comprehensive benchmark for evaluating the safety of LLMs comprising diverse multiple-choice questions that span seven distinct categories of safety concerns. The HELM [70] is dedicated to enhancing the transparency of language models by comprehensively examining their capabilities and limitations by assessing various scenarios and metrics. Concurrently, the Red-Teaming benchmark [188] conducts security tests on LLMs to investigate their responses to potential threats. CVALUES [189] focuses on measuring the safety and responsibility of Chinese Language Large Models, while PromptBench [172] examines the robustness of these models against adversarial prompts. Moreover, the GLUE-x [190] is centered

Table 1: Comparison between TRUSTLLM and other trustworthiness-related benchmarks.

| Ben |  | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=46&top_left_y=315&top_left_x=536) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=58&top_left_y=315&top_left_x=592) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=51&top_left_y=315&top_left_x=649) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=705) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=761) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=817) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=46&top_left_y=315&top_left_x=873) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=46&top_left_y=315&top_left_x=929) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=46&top_left_y=315&top_left_x=985) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=46&top_left_y=315&top_left_x=1041) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=46&top_left_y=315&top_left_x=1097) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=52&top_left_y=315&top_left_x=1153) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=1209) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=1265) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=58&top_left_y=315&top_left_x=1321) | $\sqrt{2}$ <br> $\underset{\Delta}{\bar{u}}$ <br> 凷 |  | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=1487) | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=1543) |  | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=57&top_left_y=315&top_left_x=1655) |  | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-015.jpg?height=304&width=52&top_left_y=315&top_left_x=1764) | $\stackrel{5}{\mathrm{C}}$ <br> $\stackrel{1}{\mathrm{~T}}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Truthfulness | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $x$ | $v$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $V$ |
| Safety | $v$ | $V$ | $v$ | $\checkmark$ | $\checkmark$ | $x$ | $\checkmark$ | $x$ | $\checkmark$ | $x$ | $\checkmark$ | $x$ | $v$ | $\checkmark$ | $V$ | $x$ | $x$ | $v$ | $x$ | $x$ | $v$ | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Fairness | $\checkmark$ | $\checkmark$ | $v$ | $x$ | $x$ | $x$ | $x$ | $x$ | $\checkmark$ | $x$ | $x$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $v$ | $x$ | $v$ | $v$ | $\checkmark$ |
| Robustness | $V$ | $V$ | $v$ | $x$ | $x$ | $V$ | $x$ | $v$ | $x$ | $x$ | $v$ | $x$ | $x$ | $v$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $v$ | $x$ | $x$ | $v$ | $x$ |
| Privacy | $v$ | $x$ | $v$ | $x$ | $x$ | $x$ | $x$ | $x$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $V$ | $x$ | $x$ | $x$ | $x$ | $v$ | $v$ | $v$ | $x$ | $v$ | $x$ | $x$ |
| Machine Ethics | $\checkmark$ | $x$ | $v$ | $x$ | $x$ | $x$ | $\checkmark$ | $x$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $x$ | $v$ | $v$ | $x$ | $x$ |

on the open-domain robustness of language models. HaluEval [191] assesses the performance of LLMs in generating misinformation, and Latent Jailbreak [192] tests the safety and output robustness of models when presented with text containing malicious instructions. Finally, SC-Safety [193] engages Chinese LLMs with multi-turn open-ended questions to test their safety and trustworthiness. However, most of these benchmarks cover specific sections about trustworthiness, which are not comprehensive enough. We have compared these studies without TRUSTLLM in Table 1.

## 4 Guidelines and Principles for Trustworthiness Assessment of LLMs

To create guidelines for assessing the trustworthiness of LLMs, we conducted an extensive literature review. First, we searched multiple acedemic databases, including ACM, IEEE Xplore, and arXiv, focusing on papers published in the past five years. We utilized a range of keywords such as "Large Language Models" or "LLM", "Trustworthy" and "Trustworthiness". Two researchers independently screened the publications to determine their relevance and methodological soundness. This process helped us distill the literature that most accurately defines and contextualizes trustworthiness in LLMs. We then conducted a qualitative analysis of the selected papers. We coded the literature for emerging themes and concepts, categorizing them into different areas, such as "safety mechanisms," "ethical considerations," and "fairness implementations." Our coding was cross-verified by two team members to ensure analytical consistency. Our review work leads to a set of guidelines to evaluate the trustworthiness of LLMs.

In the following sections, we present the principal dimensions of trustworthy LLMs, outlining their respective definitions and descriptions. The keywords of each principal dimension are cataloged within Table 2.

Table 2: The definitions of the eight identified dimensions.

| Dimension | Definition | Section |
| :--- | :--- | :---: |
| Truthfulness | The accurate representation of information, facts, and results by an AI system. | $\S 6$ |
| Safety | The outputs from LLMs should only engage users in a safe and healthy conversation [72]. | $\S 7$ |
| Fairness | The quality or state of being fair, especially fair or impartial treatment [208]. | $\S 8$ |
| Robustness | The ability of a system to maintain its performance level under various circumstances [83]. | $\S 9$ |
| Privacy | The norms and practices that help to safeguard human and data autonomy, identity, and dignity [83]. | $\S 10$ |
| Machine ethics | Ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as <br> artificial intelligent agents [85, 86]. | $\S 11$ |
|  | The extent to which information about an AI system and its outputs is available to individuals <br> interacting with such a system [83]. | $\S 12$ |
| Accountability | An obligation to inform and justify one's conduct to an authority [209, 210, 211, 212, 213]. | $\S 13$ |

### 4.1 Truthfulness

Intricately linked to factuality, truthfulness stands out as an essential challenge for Generative AI models, including LLMs. It has garnered extensive discussion and scholarly attention [58, 214, 215, 216]. To critically evaluate LLMs' adherence to truthfulness, datasets and benchmarks, such as MMLU [217], Natural Questions [218], TriviaQA [219], and TruthfulQA [220], have been employed in prior works [221]. Some tools also assessed some specific aspects of general truthfulness: HaluEval [191] assesses hallucinations; SelfAware [222] explores awareness of knowledge limitations; FreshQA [223] and Pinocchio [224] inspect the adaptability to rapidly evolving information.

While accuracy remains a predominant metric for evaluating truthfulness [217, 191, 222, 223], the need for human evaluation is also recognized, particularly in benchmarks like TruthfulQA [220] and FreshQA [223]. However, the challenge of ensuring truthfulness is compounded by the inherent imperfections in training data [225]. LLMs, being trained on vast troves of text on the Internet, are susceptible to absorbing and propagating misinformation, outdated facts, and even intentionally misleading content embedded within their training datasets [226, 227], making the pursuit of truthfulness in LLMs an ongoing and intricate challenge.

In this work, we define the truthfulness of LLMs as the accurate representation of information, facts, and results. Our assessment of the truthfulness of LLMs focuses on 1) evaluating their inclination to generate misinformation under two scenarios: relying solely on internal knowledge and retrieving external knowledge; 2) testing LLMs' propensity to hallucinate across four tasks: multiple-choice question-answering, open-ended question-answering, knowledge-grounded dialogue, and summarization; 3) assessing the extent of sycophancy in LLMs, encompassing two types: persona sycophancy and preference sycophancy; and 4) testing the capabilities of LLMs to correct adversarial facts when, e.g., a user's input contains incorrect information. More details are presented in section 6.

TRUSTLLM

### 4.2 Safety

With the pervasive integration of LLMs into various domains, safety and security concerns have emerged, necessitating comprehensive research and mitigation strategies [228, 229, 230, 231, 188, 232, 233, 234, 193, $235,236,197,237,238,239,240,241,69,242]$. Although LLMs should be designed to be safe and harmless, their vulnerability to adversarial behaviors, such as jailbreaking, has been extensively documented [61]. Some commonly used jailbreaking methods include generation exploitation attacks [243] and straightforward queries [244] to sophisticated techniques involving genetic algorithms [245].

The repercussions of jailbreaking extend to the generation of toxic content and the misuse of LLMs, with the potential to significantly impact user interactions and downstream applications [246]. Furthermore, the role assigned to LLMs, dictated by their system parameters, can profoundly influence their propensity to generate toxic content, underscoring the need for vigilant role assignment and parameter tuning [247]. A prevalent form of misuse is misinformation, which exemplifies the potential harms associated with LLMs, and has been shown to result in tangible negative outcomes $[227,226,248]$.

Prior work has attempted to analyze the safety issues surrounding LLMs, tracing the origins of these issues and evaluating their impacts. Tools and datasets, such as Toxigen [249] and Realtoxicityprompts [250] have been developed to facilitate the detection of toxic content and assess the harm posed by LLMs. Integrating these tools into LLMs' development and deployment pipelines is crucial for ensuring that these powerful models are used safely and responsibly.

In TrustLLM, we define Safety as the ability of LLMs to avoid unsafe, illegal outputs and only engage users in a healthy conversation [72]. We first assess LLMs' safety against jailbreak attacks, by introducing a comprehensive taxonomy of jailbreak attacks comprising five major classes and 13 subclasses. Secondly, we evaluate the issue of over-alignment (i.e., exaggerated safety). Furthermore, we measure the toxicity levels in the outputs of LLMs that have been compromised by jailbreak attacks. Finally, we assess the LLMs' resilience against various misuse scenarios using the Do-Not-Answer dataset [73], the Do-Anything-Now dataset [251], and an additional dataset specifically curated for this study. The details can be found in section 7.

### 4.3 Fairness

Ensuring fairness in LLMs is crucial, as it encapsulates the ethical principle that necessitates the equitable design, training, and deployment of LLMs and related AI systems, preventing biased or discriminatory outcomes [252]. The significance of this issue is underscored by the increasing number of countries implementing legal frameworks that mandate adherence to fairness and anti-discrimination principles in AI models [72, 253].

There is a growing body of research dedicated to understanding the stages of model development and deployment where fairness could be jeopardized, including training data preparation, model building, evaluation, and deployment phases $[254,255,256]$. Fairness compromised due to the prevalence of bias in training datasets is often considered a top concern and has been the subject of extensive recent scrutiny [257, 258, 259]. Various strategies have been proposed to improve fairness issues of LLMs, ranging from holistic solutions to reducing specific biases, like biases in internal components of LLMs and biases from user interactions [257, 260, 261]. Other work has unearthed pervasive biases and stereotypes in LLMs, particularly against individuals from certain demographic groups, such as different genders [262], LGBTQ+ communities [263], and across various political spectrums [264]. The fairness of specific LLMs like GPT-3 and GPT-4 has also been extensively examined $[265,194]$.

We define fairness as the ethical principle of ensuring that LLMs are designed, trained, and deployed in ways that do not lead to biased or discriminatory outcomes and that they treat all users and groups equitably. In TRUSTLLM, we assess the fairness of LLMs in three main aspects: stereotypes, disparagement, and preference biases. As detailed in Section 8, our initial focus is on identifying potential stereotypes embedded within LLMs. This is achieved through three tasks: analyzing agreement on stereotypes, recognizing stereotypical content, and conducting stereotype query tests. Next, we investigate the issue of disparagement by examining how LLMs might attribute different salaries to individuals based on various characteristics, thus revealing potential biases. Finally, we explore LLMs' tendencies for preference bias by observing their decision-making in scenarios presenting contrasting opinion pairs.

TRUSTLLM

### 4.4 Robustnesss

Robustness refers to the ability of AI systems to perform well under varying conditions and to properly handle exceptions, anomalies, or unexpected inputs. Recent benchmarks and studies [266, 267, 172, 268, 244, 269, 270] on LLMs have collectively underscored a critical consensus: robustness is not an inherent quality of current LLMs. For instance, GPT-3.5 is not robust with seemingly simple inputs, such as emojis [271].

In the context of TRUSTLLM, we assess the robustness regarding the stability and performance when LLMs are faced with various input conditions. Note that that we distinguish robustness from the concept of resilience against malicious attacks, which is covered under the safety dimension (Section 7). Here, we specifically explore robustness in the context of ordinary user interactions. This involves examining how LLMs cope with natural noise in inputs (as detailed in Section 9.1) and how they handle out-of-distribution (OOD) challenges (discussed in Section 9.2). These aspects provide a comprehensive view of an LLM's stability and reliability under typical usage scenarios.

### 4.5 Privacy

The privacy challenges associated with LLMs have garnered significant attention due to their ability to memorize and subsequently (unintentionally) leak private information, a concern that we have for traditional machine learning models [272]. This issue is exacerbated by the heavy reliance of LLMs training on Internetsourced data, which inevitably includes personal information. Once such information is embedded within LLMs, it becomes susceptible to extraction through malicious prompts, posing a substantial risk [273].

Recent studies have delved into various aspects of privacy risks in LLMs. These include efforts of revealing personal data from user-generated text, employing predefined templates to probe and unveil sensitive information, and even attempting to jailbreaking LLMs to access confidential information [274, 275, 276, 71, 277]. To address these challenges, a range of frameworks and tools have been proposed and developed [278, 279, 280, 281, 282], alongside the methods of differential privacy, to mitigate the risk of privacy breaches and enhance the privacy of LLMs [283, 284]. Using cryptographic techniques like secure computation [285], recent works also explored ways to provide privacy by putting the LLM-related computation in secure computation protocols $[286,287]$.

Our Privacy guideline refers to the norms and practices that help to safeguard human and data autonomy, identity, and dignity. Specifically, we focus on evaluating LLMs' privacy awareness and potential leakage. We first assess how well LLMs recognize and handle privacy-sensitive scenarios, including their tendency to inadvertently disclose learned information (section 10.1). Then, we investigate the risk of privacy leakage from their training datasets, examining if sensitive data might be unintentionally exposed when LLMs are prompted in certain ways (section 10.2). Overall, this analysis aims to understand LLMs' ability to safeguard privacy and the inherent risks of private data exposure in their outputs.

### 4.6 Machine Ethics

Machine ethics is ethics for machines, where machines, instead of humans, are the subjects. The most famous machine ethics principle is the "three laws of robotics" proposed and investigated by Isaac Asimov [288]. Earlier research in this field focused on discussing the emerging field of machine ethics and the challenges faced in representing ethical principles in machines [85, 86]. These foundational investigations have also explored the motivations behind the need for machine ethics, highlighting the pursuit of ethical decision-making abilities in computers and robots [289], and examined the nature and significance of machine ethics, discussing the challenges in defining what constitutes machine ethics and proposing potential implementation strategies [290].

Subsequent research has expanded the discourse, providing nuanced analyses of contemporary ethical dilemmas and the particular challenges that arise in the context of LLMs. While specific studies have concentrated on individual models, such as Delphi [291], GPT-3 [292], and GPT-4 [293], others have interrogated the responses of LLMs across specific domains. Two sectors frequently subject to scrutiny are the academic realm $[294,295,296]$ and healthcare research $[297,298,299]$.

Defining the term of machines ethics for LLMs is rendered nearly infeasible by our current insufficient grasp of a comprehensive ethical theory [290]. Instead, we divide it into three segments: implicit ethics, explicit ethics, and emotional awareness. Implicit ethics refers to the internal values of LLMs, such as the judgment of moral situations. In section 11.1, we assess LLMs' alignment with human ethical standards by evaluating their moral action judgments. In contrast, explicit ethics focuses on how LLMs should react in different moral environments. In section 11.2, we evaluate how LLMs should behave in various moral contexts. The assessment of LLMs' ability to take morally appropriate actions in ethical scenarios is a crucial aspect, because LLMs increasingly serve as intelligent agents, engaging in action planning and decision-making. Lastly, awareness reflects LLMs' capacity to understand their abilities and mission, recognize human emotions, and consider other perspectives. In section 11.3, we evaluate four dimensions of awareness through complex scenarios, drawing insights from psychology and sociology.

### 4.7 Transparency

Transparency was not a problem when linear classifiers and decision trees dominated AI systems. Conversely, they were considered interpretable as any observer can examine the inferred tree from the root to the leaves and understand how input variables influence the output [300]. However, with the development of high-dimensional machine learning models (e.g., deep neural networks) and the pursuit of accuracy, transparency is often sacrificed due to the opaque, "black-box" nature of complex machine learning systems [301]. Systems with opaque decision-making processes are challenging to trust, particularly in critical areas such as finance, autonomous driving, and aerospace engineering, where decisions have significant ethical and safety implications. To address these concerns, various interpretation methods have been developed in recent years [302], aiming to explain how deep learning models form their predictions. These methods are crucial for ensuring transparency and fostering trust in the predictions of advanced models in critical sectors.

As for LLMs, the lack of transparency is still noted as a core challenge [303] and a potential pitfall [304]. Reasons for their absence are often associated with some characteristics of LLMs, like complexity and massive architecture [305]. Transparency is also hard to evaluate as not all situations require the same level of transparency [305]. The evaluation should also involve human factors, like why people seek information [306, 307]. Thus, transparency is often not evaluated directly in prior works of LLMs.

In this work, transparency of LLMs refers to how much information about LLMs and their outputs is available to individuals interacting with them. In section 12, we first contextualize various perspectives on transparency. Then, we delve into specific aspects of LLM transparency, examining the unique challenges it presents and reviewing the existing research aimed at addressing these issues.

### 4.8 Accountability

In 1996, Nissenbaum [308] described four barriers to accountability that computerization presented. Developing machine learning systems requires revisiting those concepts and bringing new challenges [309]. For LLMs and their powered AI systems, the lack of transparency often leads to a lack of accountability [300]. Besides, major scholarly and societal credit is deserved for data openness, as data work is often seen as low-level grunt work [310], and data citation is a crucial but missing component in LLMs [311]. Current works on the accountability of LLMs often focus on the healthcare $[312,313]$ and academic $[314]$ domains. However, achieving overall accountability is still far from practical.

For a personal or an organization, accountability is a virtue [315]. We believe this is also applicable to LLMs. LLMs should autonomously provide explanations and justifications for their behavior. In section 13, we follow the framework of the four barriers to the accountability of computer systems as identified by Helen Nissenbaum [308], and discuss these barriers in the context of LLMs. The "problem of many hands" makes it difficult to pinpoint responsibility within the collaborative development of LLMs, while the inherent "bugs" in these systems further complicate accountability. The tendency to use the computer as a "scapegoat" and the issue of "ownership without liability" where companies disclaim responsibility for errors, further blur the lines of accountability. Furthermore, as LLMs become more sophisticated, differentiating their output from human text grows more challenging. Concurrently, the extensive use of training data in LLMs raises
significant copyright concerns, underscoring the urgent need for a clear legal framework to navigate the intricate relationship between technology, ethics, and law in the AI domain.

### 4.9 Regulations and Laws

LLMs and other Large Generative AI Models (LGAIMS) dramatically change how we interact, depict, and create information and technologies. However, current AI regulation has primarily focused on conventional AI models [316, 317]. The EU Artificial Intelligence Act defines four risk categories for general-purpose AI: unacceptable, high, limited, and minimal. However, it is inadequate to regulate LLMs [318]. Concerns have been raised regarding their compliance with existing data privacy legislation, such as the General Data Protection Regulation (GDPR) [319] for LLMs, as they might unintentionally disclose private information or reconstruct protected data from their training datasets. As a result, Italy blocked ChatGPT temporarily in April 2023 due to privacy concerns and the lack of proper regulation [320]. The EU also drafted the Digital Services Act to curb the spread of misinformation and harmful material, though LLMs were not the center of public interest then. The blueprint for an AI Bill of Rights was released in 2022 as a non-binding white paper in the US. The AI Risk Management Framework released by the National Institute of Standards and Technology provides guidelines to better manage the potential risks of LLMs and other AI systems. However, its use is still voluntary. The most recent executive order from the White House on the development and use of AI has the force of law, representing the first major binding government action on AIs of the United States [321]. The Food And Drug Administration (FDA) started regulating Software as a Medical Device (SaMD) but does not have specific categories exclusively for AI-based technologies. Instead, they evaluate them within the existing regulatory framework for medical devices [322].

![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-021.jpg?height=716&width=1648&top_left_y=379&top_left_x=236)

Figure 2: The design of benchmark in TRUSTLLM. Building upon the evaluation principles in prior research [323, 71], we design the benchmark to evaluate the trustworthiness of LLMs on six aspects: truthfulness, safety, fairness, robustness, privacy, and machine ethics. We incorporate both existing and new datasets first proposed (as shown in Table 4). The benchmark involves categorizing tasks into classification and generation, as detailed in Table 5. Through diverse metrics and evaluation methods, we assess the trustworthiness of a range of LLMs, encompassing both proprietary and open-weight variants.

## 5 Preliminaries of TrustLLM

In this section, we will introduce the design of our benchmark. As shown in Figure 2, we will introduce the model selection of LLMs in Section 5.1, including proprietary and open-weight LLMs. We will introduce our experimental setup in Section 5.2, including datasets, tasks, prompt templates, and evaluation methods.

Ethical consideration. In illustrating the examples within the assessment tasks, certain outputs produced by LLMs may be disconcerting for individuals. We emphasize that our work is solely for research purposes, and no one should misuse the datasets/methods of TRUSTLLM in illegal ways. The ultimate goal of our work is to foster the development of more reliable and trustworthy LLMs.

### 5.1 Curated List of LLMs

In this study, we meticulously curate a diverse set of 16 LLMs, encompassing proprietary and open-weight examples. This collection represents a broad spectrum of model size, training data, methodologies employed, and functional capabilities, offering a comprehensive landscape for evaluation. We summarize the information of each LLM in Table 3.

ChatGPT \& GPT-4 [325]. ChatGPT and GPT-4, developed by OpenAI, represent specialized adaptations of the GPT architecture explicitly tailored for conversational AI tasks. These models signify the dawn of the authentic era of LLMs. Trained on extensive collections of internet text data, they can generate responses that closely mimic human conversational patterns. Further refinement is achieved through fine-tuning with RLHF [43], which enhances their proficiency in producing coherent and contextually appropriate responses. GPT models represent a monumental leap in conversational AI, establishing a benchmark for future LLM developments and solidifying their position at the forefront of this technological revolution.

Vicuna [82]. The Vicuna series (7b, 13b, and 33b) are developed by researchers from LMSYS [326], targeting a wide array of natural language processing tasks. Central to Vicuna is an emphasis on intricate performance and structural nuance, with models fine-tuned on a substantial dataset comprising approximately 70,000 user-

Table 3: The details of LLMs in the benchmark. For the use of the PaLM 2 API, we have removed the safety restrictions [324], as its safety restrictions resulted in many of the returned content being none.

| Model | Model Size | Open-Weight | Version | Creator | Source |
| :---: | :---: | :---: | :---: | :---: | :---: |
| GPT-3.5-turbo (ChatGPT) | unknown | ![](https://cdn.mathpix.com/cropped/2024_05_26_bbfee58f0b6823897adbg-022.jpg?height=46&width=203&top_left_y=401&top_left_x=977) | - | OpenAI | OpenAI API |
| GPT-4 | unknown | ( | - |  | OpenAI API |
| ERNIE-3.5-turbo | unknown | $\otimes$ | - | Baidu Inc. | ERNIE API |
| text-bison-001 (PaLM 2) | unknown | ( | - | Google | Google API |
| Llama2-7b-chat | $7 \mathrm{~b}$ | V | - | Meta | HuggingFace |
| Llama2-13b-chat | $13 b$ |  | - |  | HuggingFace |
| Llama2-70b-chat | $70 \mathrm{~b}$ |  | - |  | HuggingFace |
| Mistral-7b | $7 \mathrm{~b}$ | V | v0.1 | Mistral AI | HuggingFace |
| Vicuna-33b | $33 b$ | V | v1.3 | LMSYS | HuggingFace |
| Vicuna-13b | $13 b$ | V | v1.3 |  | HuggingFace |
| Vicuna-7b | $7 \mathrm{~b}$ |  | v1.3 |  | HuggingFace |
| ChatGLM2 | $6 \mathrm{~b}$ | V | $\mathrm{v} 1.0$ | Tsinghua \& Zhipu | HuggingFace |
| Baichuan-13b | $13 \mathrm{~b}$ | 0 | - | Baichuan Inc. | HuggingFace |
| Wizardlm-13b | $13 b$ | $\theta$ | $\mathrm{v} 1.2$ | Microsoft | HuggingFace |
| Koala-13b | $13 b$ | 0 | - | UCB | HuggingFace |
| Oasst-12b | $12 \mathrm{~b}$ | 0 | - | LAION | HuggingFace |

shared ChatGPT conversations. Vicuna-33b employs advanced memory optimization techniques to manage longer conversational content during training, achieving cost-effective efficiency.

ChatGLM2 [327]. ChatGLM2 is released by the KEG Lab [328] of Tsinghua University and Zhipu AI [329] in 2023, advancing from its predecessor ChatGLM. With 6 billion parameters and the General Language Model (GLM) architecture, it supports various NLP tasks like natural language generation, text classification, and machine translation. ChatGLM2-6B benefits from robust pre-training on 1.4T Chinese and English tokens and fine-tuning aligning with human preferences, which lead to substantial performance boosts on several benchmarks. The model also adopts flash attention [330] and multi-query attention, extending the context length to $32 \mathrm{~K}$ and improving inference efficiency, respectively. These enhancements make ChatGLM2-6B a competitive model in the open-source community, with more extended context handling and efficient inference, marking a notable evolution in the ChatGLM series.

Koala-13b [331]. Koala-13b is developed by BAIR [332] for academic research with a parameter count of 13 billion. It has undergone extensive human evaluations on various test sets, including real user queries, showcasing its effectiveness in assistant-like applications.

Llama2 [69]. The Llama2 series, developed by Meta [68], consists of models ranging from 7b to 70b parameters. These models are notable for being trained on 2 trillion tokens. The series includes specialized variants like Llama Chat, fine-tuned with over 1 million human annotations. Llama2 excels in external benchmarks, showcasing its proficiency in reasoning, coding, and knowledge tests. To bolster the safety aspect of Llama2, measures such as a toxicity filter, context distillation learning, and red teaming are incorporated.

WizardLM-13b [333]. WizardLM-13b is a powerful language model developed by Microsoft Research [334]. Unlike traditional training methods, WizardLM-13b leverages an innovative process known as EvolInstruct [333], which utilizes LLMs to automatically generate various open-domain instructions of varying complexity levels. This process involves evolving existing instructions to increase complexity and difficulty and creating new instructions to enhance diversity.

Oasst-12b [81]. Oasst(Open Assistant), developed by the LAION organization [335], represents the initial English SFT iteration of the Open-Assistant project. Its training data is based on the basic data structure of conversation trees, and the model is fine-tuned on approximately 22,000 human demonstrations of assistant conversations.

TRUSTLLM

Baichuan-13b [336]. Baichuan-13b is developed by Baichuan AI [181]. With a parameter count of 13 billion, Baichuan-13b is a large-scale language model known for its exceptional performance on Chinese benchmarks. It distinguishes itself by being trained on a massive corpus of 1.4 trillion tokens and supports both Chinese and English, using ALiBi [337] position coding with a context window length of 4096.

ERNIE [79]. Ernie is an LLM developed by Baidu [338], which exemplifies a generative AI product that is augmented with a knowledge-enhanced framework. This model's robust pre-training on numerous Chinese and English tokens, combined with its fine-tuning in line with human preferences, highlights its pivotal contribution to the advancement of AI in China. Ernie's versatile applications range from everyday household tasks to industrial and manufacturing innovations.

Mistral 7B [339]. Mistral 7B, a 7b-parameter LLM by Mistral AI [340], effectively handles text generation and diverse NLP tasks, whose benchmark covers areas like commonsense reasoning, world knowledge, math and reading comprehension, showcasing its broad applicability. It utilizes a sliding window attention mechanism [341, 342], supports English and coding languages, and operates with an $8 \mathrm{k}$ context length.

PaLM 2 [36]. PaLM 2 is a capable language model developed by Google [343]. It shows strong multilingual language processing, code generation, and reasoning capabilities, reflecting advancements in computational scaling, dataset diversity, and architectural improvements.

### 5.2 Experimental Settings

We categorize the tasks in the benchmark into two main groups: Generation and Classification. Drawing from prior studies [71], we employ a temperature setting of 0 for classification tasks to ensure more precise outputs. Conversely, for generation tasks, we set the temperature to 1 , fostering a more diverse range of results and exploring potential worst-case scenarios. For instance, recent research suggests that elevating the temperature can enhance the success rate of jailbreaking [243]. For other settings like decoding methods, we use the default setting of each LLM.

Datasets. In the benchmark, we introduce a collection of 30 datasets that have been meticulously selected to ensure a comprehensive evaluation of the diverse capabilities of LLMs. Each dataset provides a unique set of challenges. They benchmark the LLMs across various dimensions of trustworthy tasks. A detailed description and the specifications of these datasets are provided in Table 4.

Tasks. In specific subsections, we have crafted a variety of tasks and datasets to augment the thoroughness of our findings. Additionally, in light of the expansive and diverse outputs generated by LLMs compared to conventional LMs, we have incorporated a range of new tasks to evaluate this unique aspect. Table 5 lists all the tasks encompassed in the benchmark.

Prompts. In most tasks, particularly for classification, our prompts are designed for LLMs to incorporate specific keywords, aiding our evaluation process. For example, we expect LLMs to generate relevant category labels (such as "yes" or "no"), which allows for efficient regular expression matching in automated assessments. Furthermore, except for privacy leakage evaluation (where we aim to increase the probability of LLMs leaking privacy information), we deliberately exclude few-shot learning from the prompts. A key reason for this is the complexity involved in choosing examples [ $363,364,365]$, as varying exemplars may significantly influence the final performance of LLMs. Moreover, even though there are various prompt methods proposed in prior studies like Chain of Thoughts (CoT) [366, 367, 368, 369], Tree of Thoughts (ToT) [370], and so on [371], we do not involve these methods in our benchmark as the benchmark aims at a plain result of LLMs.

Evaluation. Our benchmark includes numerous generative tasks, posing the challenge of defining a standard ground-truth for assessment. To avoid manual evaluation's high cost and low efficiency, we've integrated a specialized classifier [73] and ChatGPT/GPT-4 into our evaluation framework.

For the tasks with ground-truth labels, our evaluation focuses on keyword matching and regular expressions. When the approach fails to assess particular responses accurately, we utilize ChatGPT/GPT-4 to extract keywords in answers before the evaluation process.

Regarding generative tasks, they yield various answers, often including reasoning and explanations, making traditional keyword/regex matching ineffective. Recent studies have validated the effectiveness of LLMs

## 16 Conclusion

In this paper, we introduce the TRUSTLLM, a comprehensive study of the trustworthiness of LLMs, including principles for different dimensions of trustworthiness, established benchmarks, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. The study presents the principles across eight key dimensions and establishes the related benchmark for six of them. By assessing 16 mainstream LLMs across diverse datasets, we emphasize the interconnection between trustworthiness and utility in LLMs. The findings underscore the prevalence of excessive trustworthiness in many LLMs and reveal notable performance variations between open-weight and proprietary counterparts. The identified challenges highlight the necessity for collaboration among LLM developers to enhance the overall reliability of these models. The advocacy for increased transparency in trustworthy-related technologies is a central theme, aiming to foster a more human-trusted landscape in the evolving realm of LLMs. As LLMs play a pivotal role in natural language processing and a variety of real-world applications, addressing trustworthiness concerns is essential to maximize their utility and ensure responsible deployment in various domains. Only through collective effort, can we build trustworthy LLMs.


## References

[1] Tshephisho Joseph Sefara, Mahlatse Mbooi, Katlego Mashile, Thompho Rambuda, and Mapitsi Rangata. A toolkit for text extraction and analysis for natural language processing tasks. In 2022 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD), pages $1-6,2022$.

[2] Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. Natural language processing: State of the art, current trends and challenges. Multimedia tools and applications, 82(3):3713-3744, 2023.

[3] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with large language models. In 27th International Conference on Intelligent User Interfaces, pages 841-852, 2022.

[4] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis, 2023.

[5] Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web, 2023. https://blogs.microsoft.com/blog/2023/02/07/ reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/.

[6] Enhancing search using large language models, 2023. https://medium.com/whatnot-engineering/ enhancing-search-using-large-language-models-f9dcb988bdb9.

[7] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

[8] 7 top large language model use cases and applications, 2023. https://www.projectpro.io/article/ large-language-model-use-cases-and-applications/887.

[9] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[10] MintMesh. Large language models: The future of b2b software, 2023.

[11] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance, 2023.

[12] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.

[13] Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, and Shuiwang Ji. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023.

[14] Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4, 2023.

[15] Xianjun Yang, Junfeng Gao, Wenxin Xue, and Erik Alexandersson. Pllama: An open-source large language model for plant science, 2024.

[16] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger,

Gregory P Veldhuizen, et al. The future landscape of large language models in medicine. Communications Medicine, 3(1):141, 2023.

[17] Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, and Yongdong Zhang. ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences. arXiv preprint arXiv:2311.06025, 2023.

[18] Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, and Linda Ruth Petzold. Alpacare:instruction-tuned large language models for medical application, 2023.

[19] Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, Lifang He, Brian D. Davison, Quanzheng Li, Yong Chen, Hongfang Liu, and Lichao Sun. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks, 2023.

[20] Yirong Chen, Zhenyu Wang, Xiaofen Xing, huimin zheng, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, and Xiangmin Xu. Bianque: Balancing the questioning and suggestion ability of health 11 ms with multi-turn health conversations polished by chatgpt, 2023.

[21] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. Huatuogpt, towards taming language models to be a doctor. arXiv preprint arXiv:2305.15075, 2023.

[22] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023.

[23] Ming Xu. Medicalgpt: Training medical gpt model. https://github.com/shibing624/MedicalGPT, 2023.

[24] Soumen Pal, Manojit Bhattacharya, Sang-Soo Lee, and Chiranjib Chakraborty. A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineering and research. Annals of Biomedical Engineering, pages 1-4, 2023.

[25] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334, 2023.

[26] Mitchell Linegar, Rafal Kocielnik, and R Michael Alvarez. Large language models and political science. Frontiers in Political Science, 5:1257092, 2023.

[27] fuzi.mingcha. https://github.com/irlab-sdu/fuzi.mingcha, 2023.

[28] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, and Zhongyu Wei. Disc-lawllm: Fine-tuning large language models for intelligent legal services, 2023.

[29] Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. In NeurIPS, 2023.

[30] Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Jiawei Han, and Lianhui Qin. Structured chemistry reasoning with large language models. arXiv preprint arXiv:2311.09656, 2023.

[31] Ziqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him Wong Tim, and Sai-Kit Yeung. Marinegpt: Unlocking secrets of "ocean" to the public, 2023.

[32] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. Oceangpt: A large language model for ocean science tasks, 2023.

[33] Jingsi Yu, Junhui Zhu, Yujie Wang, Yang Liu, Hongxiang Chang, Jinran Nie, Cunliang Kong, Ruining Chong, XinLiu, Jiyuan An, Luming Lu, Mingwei Fang, and Lin Zhu. Taoli llama. https://github.com/ blcuicall/taoli, 2023.

[34] Zhengqing Yuan, Huiwen Xue, Xinyi Wang, Yongming Liu, Zhuanzhe Zhao, and Kun Wang. Artgpt-4: Artistic vision-language understanding with adapter-enhanced minigpt-4, 2023.

TRUSTLLM

[35] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.

[36] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.

[37] Towards Data Science. Palm: Efficiently training massive language models, 2023.

[38] Wired. How chatgpt works: A look inside large language models, 2023.

[39] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[40] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

[41] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. Pathways: Asynchronous distributed dataflow for ml. Proceedings of Machine Learning and Systems, 4:430-449, 2022.

[42] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, and Wen Gao. Ai alignment: A comprehensive survey, 2023.

[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730$27744,2022$.

[44] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023.

TRUSTLLM

[45] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023.

[46] Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. RI4f: Generating natural language feedback with reinforcement learning for repairing model outputs, 2023.

[47] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilè Lukošiūté, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.

[48] Ethan Perez, Sam Ringer, Kamilè Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.

[49] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.

[50] Micah Carroll, Alan Chan, Henry Ashton, and David Krueger. Characterizing manipulation from ai systems. arXiv preprint arXiv:2303.09387, 2023.

[51] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

[52] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.

[53] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[54] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544, 2022.

[55] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. Advances in neural information processing systems, $29,2016$.

[56] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.

[57] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.

[58] Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, et al. Factuality challenges in the era of large language models. arXiv preprint arXiv:2310.05189, 2023.

[59] Canyu Chen and Kai Shu. Combating misinformation in the age of llms: Opportunities and challenges. arXiv preprint arXiv:2311.05656, 2023.

[60] Forbes Tech Council. 10 ways cybercriminals can abuse large language models, 2023.

[61] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does $11 \mathrm{~m}$ safety training fail? arXiv preprint arXiv:2307.02483, 2023.

[62] Appen. Unraveling the link between translations and gender bias in $11 \mathrm{~ms}, 2023$.

[63] Forbes Tech Council. Navigating the biases in $1 \mathrm{~lm}$ generative ai: A guide to responsible implementation, 2023.

[64] Slator. Large language models may leak personal data, 2022. https://slator.com/ large-language-models-may-leak-personal-data/.

TRUSTLLM

[65] Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu, and Xiang Li. Deid-gpt: Zero-shot medical text de-identification by gpt-4, 2023.

[66] Quanta Magazine. What does it mean to align ai with human values?, 2022.

[67] OpenAI. Openai, 2023. https://www.openai.com.

[68] Meta. Ai at meta, 2023. https://ai.meta.com.

[69] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[70] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.

[71] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. arXiv preprint arXiv:2306.11698, 2023.

[72] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. arXiv preprint arXiv:2308.05374, 2023.

[73] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: A dataset for evaluating safeguards in llms. arXiv preprint arXiv:2308.13387, 2023.

[74] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, and Hao Zhang. Chatbot arena leaderboard week 8: Introducing mt-bench and vicuna-33b. https://lmsys.org/ chatbot-arena-leaderboard-week-8-introducing-mt-bench-and-vicuna-33b/, 2023.

[75] Hugging Face. The big benchmarks collection - a open-llm-leaderboard collection. https://huggingface. co/spaces/OpenLLMBenchmark/The-Big-Benchmarks-Collection.

[76] Openai moderation api, 2023. https://platform.openai.com/docs/guides/moderation.

[77] Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang. The foundation model transparency index, 2023.

[78] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023.

[79] Baidu. Ernie - baidu yiyan, 2023. https://yiyan.baidu.com/.

[80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[81] Andreas Köpf, Yannic Kilcher, Huu Nguyen (ontocord), and Christoph Schuhmann. an open assistant for everyone by laion, 2023. https:///open-assistant.io/.

[82] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng andZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. vicuna, 2023. https://lmsys.org/blog/2023-03-30-vicuna/.

[83] Artificial intelligence risk management framework (ai rmf 1.0), 2023. https://nvlpubs.nist.gov/nistpubs/ ai/NIST.AI.100-1.pdf.

[84] CMU. Enron email dataset, 2015. https://www.cs.cmu.edu/ enron/.

[85] Michael Anderson and Susan Leigh Anderson. Guest editors' introduction: machine ethics. IEEE Intelligent Systems, 21(4):10-11, 2006.

TRUSTLLM

[86] Michael Anderson and Susan Leigh Anderson. Machine ethics: Creating an ethical intelligent agent. AI magazine, 28(4):15-15, 2007.

[87] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.

[88] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

[89] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

[90] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

[91] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

[92] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training computeoptimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[93] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv: 1707.06347, 2017.

[94] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.

[95] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023.

[96] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676, 2023.

[97] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023.

[98] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models, 2023.

[99] Alejandro Lopez-Lira and Yuehua Tang. Can chatgpt forecast stock price movements? return predictability and large language models, 2023.

[100] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check, 2023.

[101] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver?, 2023.

[102] Kai-Cheng Yang and Filippo Menczer. Large language models can rate news outlet credibility, 2023.

[103] Ruohong Zhang, Yau-Shian Wang, and Yiming Yang. Generation-driven contrastive self-training for zero-shot text classification with instruction-tuned gpt, 2023.

[104] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. Sources of hallucination by large language models on inference tasks, 2023.

[105] Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. Mathematical capabilities of chatgpt, 2023.

TRUSTLLM

[106] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4, 2023.

[107] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning, 2023.

[108] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. Benchmarking large language models for news summarization, 2023.

[109] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets, 2023.

[110] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models, 2023.

[111] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, and Xing Xie. On the robustness of chatgpt: An adversarial and out-of-distribution perspective, 2023.

[112] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization, 2022.

[113] Hongpeng Jin, Wenqi Wei, Xuyu Wang, Wenbin Zhang, and Yanzhao Wu. Rethinking learning rate tuning in the era of large language models. arXiv preprint arXiv:2309.08859, 2023.

[114] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020.

[115] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity, 2023.

[116] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, Wenhao Huang, Zili Wang, Shusen Wang, Weiguo Zheng, Hongwei Feng, and Yanghua Xiao. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation, 2023.

[117] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science?, 2023.

[118] John J. Nay, David Karamardian, Sarah B. Lawsky, Wenting Tao, Meghana Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H. Choi, and Jungo Kasai. Large language models as tax attorneys: A case study in legal capabilities emergence, 2023.

[119] Neel Guha, Julian Nyarko, Daniel E. Ho, Christopher Ré, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N. Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory M. Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan H. Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, and Zehua Li. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models, 2023.

[120] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. Lawbench: Benchmarking legal knowledge of large language models. arXiv preprint arXiv:2309.16289, 2023.

[121] Michael Frank. Baby steps in evaluating the capacities of large language models. Nature Reviews Psychology, 2, 062023.

[122] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks?, 2023.

[123] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.

TRUSTLLM

[124] Cayque Nascimento and Andre Pimentel. Do large language models understand chemistry? a conversation with. Journal of Chemical Information and Modeling, 63, 032023.

[125] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia. Understanding the capabilities of large language models for automated planning, 2023.

[126] Giriprasad Sridhara, Ranjani H. G., and Sourav Mazumdar, 2023.

[127] Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T. Sio, Lisa A. McGee, Jonathan B. Ashman, Xiang Li, Tianming Liu, Jiajian Shen, and Wei Liu. Evaluating large language models on a highly-specialized topic, radiation oncology physics. Frontiers in Oncology, 13, jul 2023.

[128] Jamil Samaan, Yee Yeo, Nithya Rajeev, Lauren Hawley, Stuart Abel, Wee Han Ng, Nitin Srinivasan, Justin Park, Miguel Burch, Rabindra Watson, Omer Liran, and Kamran Samakar. Assessing the accuracy of responses by the language model chatgpt to questions regarding bariatric surgery. Obesity Surgery, $33: 1-7,042023$.

[129] Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Taylor, and David Chartash. How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment. JMIR medical education, 9:e45312, 022023.

[130] Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLOS Digital Health, 2(2):1-12, 022023.

[131] Zhuo Wang, Rongzhen Li, Bowen Dong, Jie Wang, Xiuxing Li, Ning Liu, Chenhui Mao, Wei Zhang, Liling Dong, Jing Gao, and Jianyong Wang. Can llms like gpt-4 outperform traditional ai tools in dementia diagnosis? maybe, but not today, 2023.

[132] Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz, Benjamin Glicksberg, and Eyal Klang. Evaluating the use of large language model in identifying top research questions in gastroenterology. Scientific Reports, 13, 032023.

[133] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.

[134] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models, 2023.

[135] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark, 2023.

[136] Xun Liang, Shichao Song, Simin Niu, Zhiyu Li, Feiyu Xiong, Bo Tang, Zhaohui Wy, Dawei He, Peng Cheng, Zhonghao Wang, and Haiying Deng. Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation, 2023.

[137] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model evaluation. arXiv preprint arXiv:2308.04026, 2023.

[138] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.

[139] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.

TRUSTLLM

[140] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128, 2023.

[141] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms, 2023.

[142] Wei Dai, Jionghao Lin, Flora Jin, Tongguang Li, Yi-Shan Tsai, Dragan Gasevic, and Guanliang Chen. Can large language models provide feedback to students? a case study on chatgpt, 042023.

[143] Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks. arXiv preprint arXiv:2305.05862, 2023.

[144] Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, Zhoufan Zhu, Anbo Wu, Xin Guo, and Yun Chen. Fineval: A chinese financial domain knowledge evaluation benchmark for large language models, 2023.

[145] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: A new benchmark for financial question answering, 2023.

[146] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu: A large language model, instruction data and evaluation benchmark for finance, 2023.

[147] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. Recommender systems in the era of large language models (llms), 2023.

[148] Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large language models for recommendation model interpretability, 2023.

[149] Greg Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, and Maja Matarić. Personality traits in large language models, 2023.

[150] Pier Luca Lanzi and Daniele Loiacono. Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design, 2023.

[151] Van-Hoang Le and Hongyu Zhang. Log parsing: How far can chatgpt go?, 2023.

[152] Li Zhong and Zilong Wang. Can chatgpt replace stackoverflow? a study on robustness and reliability of large language model code generation, 2023.

[153] Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach D. Le, and David Lo. Refining chatgpt-generated code: Characterizing and mitigating code quality issues, 2023.

[154] Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, and Yong Yu. Codeapex: A bilingual programming evaluation benchmark for large language models, 2023.

[155] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023.

[156] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: A survey, 2021.

[157] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002.

[158] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004.

[159] ADVAITH SIDDHARTHAN. Ehud reiter and robert dale. building natural language generation systems. cambridge university press, 2000. Natural Language Engineering, 7(3):271-274, 2001.

TRUSTLLM

[160] Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The gem benchmark: Natural language generation, its evaluation and metrics, 2021.

[161] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[162] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020.

[163] OpenAI. Lessons learned on language model safety and misuse, 2023.

[164] OpenAI. Openai red teaming network, 2023.

[165] OpenAI. Usage policies, 2023.

[166] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llm-based input-output safeguard for human-ai conversations, 2023.

[167] Anthropic. Anthropic, 2023. https://www.anthropic.com.

[168] Anthropic. Claude model, 2023.

[169] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022.

[170] Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general principles for constitutional ai. arXiv preprint arXiv:2310.13798, 2023.

[171] Microsoft. What is responsible ai?, 2023.

[172] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023.

[173] Safety filters and attributes, 2023. https://cloud.google.com/vertex-ai/docs/generative-ai/learn/ responsible-ai\#safety_filters_and_attributes.

[174] Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models. arXiv preprint arXiv:2010.06032, 2020.

[175] Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, Keith Rush, and Sushant Prakash. Federated reconstruction: Partially local federated learning, 2021.

[176] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness, 2019.

[177] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models (2021). arXiv preprint arXiv:2112.04359, 2021.

[178] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324, 2023.

[179] Google. An early warning system for novel ai risks, 2023. https://deepmind.google/discover/blog/ an-early-warning-system-for-novel-ai-risks/.

[180] Google. Responsible ai at google research: Adversarial testing for generative ai safety, 2023. https: //blog.research.google/2023/11/responsible-ai-at-google-research_16.html.

[181] Baichuan AI. Baichuan model, 2023. https://www.baichuan-ai.com/home.

[182] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.

[183] IBM. Watsonx.ai, 2023. http://watsonx.ai/.

[184] IBM. Watsonx.governance, 2023. https://www.ibm.com/products/watsonx-governance.

[185] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. 2023.

[186] Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun. How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities, 2023.

[187] Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436, 2023.

[188] Rishabh Bhardwaj and Soujanya Poria. Red-teaming large language models using chain of utterances for safety-alignment, 2023.

[189] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. Cvalues: Measuring the values of chinese large language models from safety to responsibility, 2023.

[190] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. arXiv preprint arXiv:2211.08073, 2022.

[191] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models. arXiv e-prints, pages arXiv-2305, 2023.

[192] Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. Latent jailbreak: A test suite for evaluating both text safety and output robustness of large language models, 2023.

[193] Liang Xu, Kangkang Zhao, Lei Zhu, and Hang Xue. Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese. arXiv preprint arXiv:2310.05818, 2023.

[194] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.

[195] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023.

[196] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2023.

[197] Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen tse Huang, Wenxiang Jiao, and Michael R. Lyu. All languages matter: On the multilingual safety of large language models, 2023.

TRUSTLLM

[198] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang2 Siyin Wang1 Xiangyang Liu, Mozhi Zhang1 Junliang Hel Mianqiu Huang, Zhangyue Yin, and Kai Chen2 Xipeng Qiu. Evaluating hallucinations in chinese large language models.

[199] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. Felm: Benchmarking factuality evaluation of large language models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.

[200] Mi Zhang, Xudong Pan, and Min Yang. Jade: A linguistics-based safety evaluation platform for llm, 2023.

[201] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, and Yangqiu Song. P-bench: A multi-level privacy evaluation benchmark for language models, 2023.

[202] Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin Choi. Can llms keep a secret? testing privacy implications of language models via contextual integrity theory, 2023.

[203] Yanyang Li, Jianqiao Zhao, Duo Zheng, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael R Lyu, et al. Cleva: Chinese language models evaluation platform. arXiv preprint arXiv:2308.04813, 2023.

[204] Allen Nie, Yuhui Zhang, Atharva Amdekar, Chris Piech, Tatsunori Hashimoto, and Tobias Gerstenberg. Moca: Measuring human-language model alignment on causal and moral judgment tasks, 2023.

[205] Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, and Dahua Lin. Flames: Benchmarking value alignment of chinese large language models, 2023.

[206] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Michael Smith. Robbie: Robust bias evaluation of large generative language models, 2023.

[207] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity, 2023.

[208] What does "fairness" mean for machine learning systems?, 2023. https://haas.berkeley.edu/wp-content/ uploads/What-is-fairness_-EGAL2.pdf.

[209] Essien E Akpanuko and Ikenna E Asogwa. Accountability: A synthesis. International Journal of Finance and Accounting, 2(3):164-173, 2013.

[210] Staffan I Lindberg. Mapping accountability: core concept and subtypes. International review of administrative sciences, 79(2):202-226, 2013.

[211] Richard Mulgan. 'accountability': an ever-expanding concept? Public administration, 78(3):555-573, 2000.

[212] Ian Thynne and John Goldring. Accountability and control: Government officials and the exercise of power. (No Title), 1987.

[213] Claudio Novelli, Mariarosaria Taddeo, and Luciano Floridi. Accountability in artificial intelligence: what it is and how it works. AI \& SOCIETY, pages 1-12, 2023.

[214] Ali Borji. A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494, 2023.

[215] Sajed Jalil, Suzzana Rafi, Thomas D LaToza, Kevin Moran, and Wing Lam. Chatgpt and software testing education: Promises \& perils. In 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW), pages 4130-4137. IEEE, 2023.

[216] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does chatgpt fall short in answering questions faithfully? arXiv preprint arXiv:2304.10513, 2023.

[217] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

TRUSTLLM

[218] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.

[219] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.

[220] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.

[221] Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and Yue Zhang. Evaluating open question answering evaluation. arXiv preprint arXiv:2305.12421, 2023.

[222] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language models know what they don't know? arXiv preprint arXiv:2305.18153, 2023.

[223] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214, 2023.

[224] Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S Yu, and Zhijiang Guo. Do large language models know about facts? arXiv preprint arXiv:2310.05177, 2023.

[225] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

[226] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. On the risk of misinformation pollution with large language models. arXiv preprint arXiv:2305.13661, 2023.

[227] Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-20, 2023.

[228] Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. Tricking $11 m$ s into disobedience: Understanding, analyzing, and preventing jailbreaks. arXiv preprint arXiv:2305.14965, 2023.

[229] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023.

[230] Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models. arXiv preprint arXiv:2307.08487, 2023.

[231] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442, 2023.

[232] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et al. Cvalues: Measuring the values of chinese large language models from safety to responsibility. arXiv preprint arXiv:2307.09705, 2023.

[233] Xi Zhiheng, Zheng Rui, and Gui Tao. Safety and ethical concerns of large language models. In Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 4: Tutorial Abstracts), pages $9-16,2023$.

[234] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of $1 \mathrm{~lm}$ via a human-preference dataset. arXiv preprint arXiv:2307.04657, 2023.

[235] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models, 2023.

TRUSTLLM

[236] Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach. Low-resource languages jailbreak gpt-4, 2023.

[237] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts, 2023.

[238] Dongyu Yao, Jianshu Zhang, Ian G. Harris, and Marcel Carlsson. Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models, 2023.

[239] Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks, 2023.

[240] Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking attacks via robustly aligned $11 \mathrm{~m}, 2023$.

[241] Mansi Phute, Alec Helbling, Matthew Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, and Duen Horng Chau. Llm self defense: By self examination, llms know they are being tricked, 2023.

[242] Pin-Yu Chen and Payel Das. AI Maintenance: A robustness perspective. Computer, 56(2):48-56, 2023.

[243] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of open-source llms via exploiting generation. arXiv preprint arXiv:2310.06987, 2023.

[244] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. Prompt injection attacks and defenses in $11 \mathrm{~m}$-integrated applications. arXiv preprint arXiv:2310.12815, 2023.

[245] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023.

[246] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in detoxifying language models. arXiv preprint arXiv:2109.07445, 2021.

[247] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335, 2023.

[248] Julian Hazell. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint arXiv:2305.06972, 2023.

[249] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022.

[250] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020.

[251] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023.

[252] Lu Wang, Max Song, Rezvaneh Rezapour, Bum Chul Kwon, and Jina Huh-Yoo. People's perceptions toward bias and related concepts in large language models: A systematic review. arXiv preprint arXiv:2309.14504, 2023.

[253] Jessica Fjeld, Nele Achten, Hannah Hilligoss, Ádám Nagy, and Madhulika Srikumar. Principled artificial intelligence: Mapping consensus in ethical and rights-based approaches to principles for ai. SSRN Electronic Journal, 2020.

[254] Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: A survey, 2023.

[255] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1-35, 2021.

[256] Harini Suresh and John Guttag. A framework for understanding sources of harm throughout the machine learning life cycle. In Equity and access in algorithms, mechanisms, and optimization, pages 1-9. 2021.

TRUSTLLM

[257] Jintang Xue, Yun-Cheng Wang, Chengwei Wei, Xiaofeng Liu, Jonghye Woo, and C-C Jay Kuo. Bias and fairness in chatbots: An overview. arXiv preprint arXiv:2309.08836, 2023.

[258] Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, and Emma Strubell. Queer people are people first: Deconstructing sexual identity stereotypes in large language models. arXiv preprint arXiv:2307.00101, 2023.

[259] Yanhong Bai, Jiabao Zhao, Jinxin Shi, Tingjiang Wei, Xingjiao Wu, and Liang He. Fairbench: A four-stage automatic framework for detecting stereotypes and biases in large language models, 2023.

[260] Sunipa Dev, Akshita Jha, Jaya Goyal, Dinesh Tewari, Shachi Dave, and Vinodkumar Prabhakaran. Building stereotype repositories with llms and community engagement for scale and depth. CrossCultural Considerations in NLP@ EACL, page 84, 2023.

[261] UBC. Reducing bias in 1lms, 2023. https://www.ischool.berkeley.edu/projects/2023/ reducing-bias-large-language-models.

[262] Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and Nanyun Peng. " kelly is a warm person, joseph is a role model": Gender biases in llm-generated reference letters. arXiv preprint arXiv:2310.09219, 2023.

[263] Virginia K Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models. arXiv preprint arXiv:2306.15087, 2023.

[264] Fabio Motoki, Valdemar Pinho Neto, and Victor Rodrigues. More human than human: Measuring chatgpt political bias. Public Choice, pages 1-21, 2023.

[265] Gabriel Simmons. Moral mimicry: Large language models produce moral rationalizations tailored to political identity. arXiv preprint arXiv:2209.12106, 2022.

[266] Wentao Ye, Mingfeng Ou, Tianyi Li, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu, Gang Chen, Junbo Zhao, et al. Assessing hidden risks of llms: An empirical study on robustness, consistency, and credibility. arXiv preprint arXiv:2305.10235, 2023.

[267] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840, 2021.

[268] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against $11 m$-integrated applications, 2023.

[269] Pin-Yu Chen and Cho-Jui Hsieh. Adversarial Robustness for Machine Learning. Academic Press, 2022.

[270] Pin-Yu Chen and Sijia Liu. Holistic adversarial robustness of deep learning models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 15411-15420, 2023.

[271] Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan Kankanhalli. An $11 \mathrm{~m}$ can fool itself: A prompt-based adversarial attack. arXiv preprint arXiv:2310.13345, 2023.

[272] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr. What does it mean for a language model to preserve privacy? In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 2280-2292, 2022.

[273] Sunder Ali Khowaja, Parus Khuwaja, and Kapal Dev. Chatgpt needs spade (sustainability, privacy, digital divide, and ethics) evaluation: A review. arXiv preprint arXiv:2305.03123, 2023.

[274] Robin Staab, Mark Vero, Mislav Balunović, and Martin Vechev. Beyond memorization: Violating privacy via inference with large language models, 2023.

[275] Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models leaking your personal information?, 2022.

[276] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile: Probing privacy leakage in large language models, 2023.

[277] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197, 2023.

TRUSTLLM

[278] Rouzbeh Behnia, Mohammadreza Reza Ebrahimi, Jason Pacheco, and Balaji Padmanabhan. Ew-tune: A framework for privately fine-tuning large language models with differential privacy. In 2022 IEEE International Conference on Data Mining Workshops (ICDMW), pages 560-566. IEEE, 2022.

[279] Sara Montagna, Stefano Ferretti, Lorenz Cuno Klopfenstein, Antonio Florio, and Martino Francesco Pengo. Data decentralisation of $11 \mathrm{~m}$-based chatbot systems in chronic disease self-management. In Proceedings of the 2023 ACM Conference on Information Technology for Social Good, pages 205-212, 2023.

[280] Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, and Xiaolin Zheng. Federated large language model: A position paper. arXiv preprint arXiv:2307.08925, 2023.

[281] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile: Probing privacy leakage in large language models. arXiv preprint arXiv:2307.01881, 2023.

[282] Saiteja Utpala, Sara Hooker, and Pin Yu Chen. Locally differentially private document generation using zero shot prompting. arXiv preprint arXiv:2310.16111, 2023.

[283] Fatemehsadat Mireshghallah, Huseyin A Inan, Marcello Hasegawa, Victor Rühle, Taylor BergKirkpatrick, and Robert Sim. Privacy regularization: Joint privacy-utility optimization in language models. arXiv preprint arXiv:2103.07567, 2021.

[284] Aldo Gael Carranza, Rezsa Farahani, Natalia Ponomareva, Alex Kurakin, Matthew Jagielski, and Milad Nasr. Privacy-preserving recommender systems with synthetic query generation using differentially private large language models. arXiv preprint arXiv:2305.05973, 2023.

[285] Andrew Chi-Chih Yao. How to generate and exchange secrets. In 27th Annual Symposium on Foundations of Computer Science (sfcs 1986), pages 162-167, 1986.

[286] Kanav Gupta, Neha Jawalkar, Ananta Mukherjee, Nishanth Chandran, Divya Gupta, Ashish Panwar, and Rahul Sharma. Sigma: Secure gpt inference with function secret sharing. Cryptology ePrint Archive, Paper 2023/1269, 2023. https://eprint.iacr.org/2023/1269.

[287] Xiaoyang Hou, Jian Liu, Jingyu Li, Yuhan Li, Wen jie Lu, Cheng Hong, and Kui Ren. Ciphergpt: Secure two-party gpt inference. Cryptology ePrint Archive, Paper 2023/1147, 2023. https://eprint.iacr. org/2023/1147.

[288] Vincent C. Müller. Ethics of Artificial Intelligence and Robotics. In Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2023 edition, 2023.

[289] Wendell Wallach, Colin Allen, and Iva Smit. Machine morality: bottom-up and top-down approaches for modelling human moral faculties. Ai \& Society, 22:565-582, 2008.

[290] James H Moor. The nature, importance, and difficulty of machine ethics. IEEE intelligent systems, 21(4):18-21, 2006

[291] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. A word on machine ethics: A response to jiang et al.(2021). arXiv preprint arXiv:2111.04158, 2021.

[292] Philip Feldman, Aaron Dant, and David Rosenbluth. Ethics, rules of engagement, and ai: Neural narrative mapping using large transformer language models. arXiv preprint arXiv:2202.02647, 2022.

[293] Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, and Helen Meng. Rethinking machine ethics-can llms perform moral reasoning through the lens of moral theories? arXiv preprint arXiv:2308.15399, 2023.

[294] Sebastian Porsdam Mann, Brian D Earp, Nikolaj Møller, Suren Vynn, and Julian Savulescu. Autogen: A personalized large language model for academic enhancement-ethics and proof of principle. The American Journal of Bioethics, pages 1-14, 2023.

[295] Brady D Lund, Ting Wang, Nishith Reddy Mannuru, Bing Nie, Somipam Shimray, and Ziang Wang. Chatgpt and a new academic reality: Artificial intelligence-written research papers and the ethics of the large language models in scholarly publishing. Journal of the Association for Information Science and Technology, 74(5):570-581, 2023.

TRUSTLLM

[296] Jesse G Meyer, Ryan J Urbanowicz, Patrick CN Martin, Karen O'Connor, Ruowang Li, Pei-Chen Peng, Tiffani J Bright, Nicholas Tatonetti, Kyoung Jae Won, Graciela Gonzalez-Hernandez, et al. Chatgpt and large language models in academia: opportunities and challenges. BioData Mining, 16(1):20, 2023.

[297] Hanzhou Li, John T Moon, Saptarshi Purkayastha, Leo Anthony Celi, Hari Trivedi, and Judy W Gichoya. Ethics of large language models in medicine and medical research. The Lancet Digital Health, 5(6):e333-e $335,2023$.

[298] Hanzhou Li, John T Moon, Saptarshi Purkayastha, Leo Anthony Celi, Hari Trivedi, and Judy W Gichoya. Ethics of large language models in medicine and medical research. The Lancet Digital Health, $5(6): \mathrm{e} 333-\mathrm{e} 335,2023$.

[299] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930-1940, 2023.

[300] Paul B De Laat. Algorithmic decision-making based on machine learning from big data: can transparency restore accountability? Philosophy \& technology, 31(4):525-541, 2018.

[301] Kacper Sokol and Peter Flach. One explanation does not fit all: The promise of interactive explanations for machine learning transparency. KI-Künstliche Intelligenz, 34(2):235-250, 2020.

[302] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1):18, 2020.

[303] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In Proceedings of the 2022 CHI conference on human factors in computing systems, pages 1-22, 2022.

[304] Daniel Buschek, Lukas Mecke, Florian Lehmann, and Hai Dang. Nine potential pitfalls when designing human-ai co-creative systems. arXiv preprint arXiv:2104.00358, 2021.

[305] Q Vera Liao and Jennifer Wortman Vaughan. Ai transparency in the age of llms: A human-centered research roadmap. arXiv preprint arXiv:2306.01941, 2023.

[306] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, and Kevin Baum. What do we want from explainable artificial intelligence (xai)?-a stakeholder perspective on xai and a conceptual model guiding interdisciplinary xai research. Artificial Intelligence, 296:103473, 2021.

[307] Harini Suresh, Steven R Gomez, Kevin K Nam, and Arvind Satyanarayan. Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-16, 2021.

[308] Helen Nissenbaum. Accountability in a computerized society. Science and engineering ethics, 2:25-42, 1996.

[309] A Feder Cooper, Emanuel Moss, Benjamin Laufer, and Helen Nissenbaum. Accountability in an algorithmic society: relationality, responsibility, and robustness in machine learning. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 864-876, 2022.

[310] Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse. Opening up chatgpt: Tracking openness, transparency, and accountability in instruction-tuned text generators. In Proceedings of the 5th International Conference on Conversational User Interfaces, pages 1-6, 2023.

[311] Jie Huang and Kevin Chen-Chuan Chang. Citation: A key to building responsible and accountable large language models. arXiv preprint arXiv:2307.02185, 2023.

[312] Edward Guo, Mehul Gupta, Sarthak Sinha, Karl Rössler, Marcos Tatagiba, Ryojo Akagami, Ossama AlMefty, Taku Sugiyama, Phillip E Stieg, Gwynedd E Pickett, et al. neurogpt-x: Towards an accountable expert opinion tool for vestibular schwannoma. medRxiv, pages 2023-02, 2023.

[313] Jin K Kim, Michael Chua, Mandy Rickard, and Armando Lorenzo. Chatgpt and large language model (llm) chatbots: the current state of acceptability and a proposal for guidelines on utilization in academic medicine. Journal of Pediatric Urology, 2023.

TRUSTLLM

[314] Daniel H Solomon, Kelli D Allen, Patricia Katz, Amr H Sawalha, and Ed Yelin. Chatgpt, et al... artificial intelligence, authorship, and medical publishing. ACR Open Rheumatology, 5(6):288, 2023.

[315] Mark Bovens. Two concepts of accountability: Accountability as a virtue and as a mechanism. West European Politics, 33(5):946-967, 2010.

[316] Philipp Hacker, Andreas Engel, and Marco Mauer. Regulating chatgpt and other large generative ai models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages $1112-1123,2023$.

[317] Ensuring safe, secure, and trustworthy ai, 2023. https://www.whitehouse.gov/wp-content/uploads/2023/ 07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf.

[318] Carlos I Gutierrez, Anthony Aguirre, Risto Uuk, Claire C Boine, and Matija Franklin. A proposal for a definition of general purpose artificial intelligence systems. Digital Society, 2(3):36, 2023.

[319] Zhongxiang Sun. A short survey of viewing large language models in legal aspect. arXiv preprint arXiv:2303.09136, 2023.

[320] Shiona McCallum. Chatgpt banned in italy over privacy concerns, Apr 2023.

[321] Lauren Feiner Hayden Field. Biden issues u.s.' first ai executive order, requiring safety assessments, civil rights guidance, research on labor market impact, Oct 2023.

[322] Bertalan Meskó and Eric J Topol. The imperative for regulatory oversight of large language models (or generative ai) in healthcare. $n p j$ Digital Medicine, 6(1):120, 2023.

[323] Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, and Douwe Kiela. Dynaboard: An evaluation-as-a-service platform for holistic nextgeneration benchmarking. Advances in Neural Information Processing Systems, 34:10351-10367, 2021.

[324] Google. Safety settings, 2023.

[325] OpenAI. Chatgpt, 2023. https://openai.com/product/chatgpt.

[326] Large Model Systems Organization. Lmsys org, 2023.

[327] Knowledge Engineering Group (KEG) \& Data Mining at Tsinghua University. Chatglm2-6b: An open bilingual chat llm, 2023. https://github.com/THUDM/ChatGLM2-6B.

[328] Tsinghua University Knowledge Engineering Group (KEG). Chatglm2-6b: An open bilingual chat 1lm, 2023. https://github.com/THUDM.

[329] Zhipu AI. Zhipu ai, 2023. https://www.zhipuai.cn/.

[330] Dao-AILab. Flash-attention, 2023. https://github.com/Dao-AILab/flash-attention.

[331] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023.

[332] Berkeley Artificial Intelligence Research Lab. Koala: A dialogue model for academic research, 2023. https://bair.berkeley.edu/.

[333] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.

[334] Microsoft. Ai for good research lab, 2023. https://www.microsoft.com/en-us/research/group/ ai-for-good-research-lab/.

[335] LAION. Laion: Ai and natural language processing lab, 2023. https://laion.ai/.

[336] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models, 2023.

TRUSTLLM

[337] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv, 2021. https://arxiv.org/abs/2108.12409.

[338] Baidu. Baidu qian fan model, 2023. https://cloud.baidu.com/product/wenxinworkshop.

[339] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.

[340] Mistral 7b, November 2023.

[341] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.

[342] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020.

[343] Google AI. Google ai palm 2, 2023. https://ai.google/discover/palm2/.

[344] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[345] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[346] Michael Chen, Mike D'Arcy, Alisa Liu, Jared Fernandez, and Doug Downey. Codah: An adversarially authored question-answer dataset for common sense. arXiv preprint arXiv:1904.04365, 2019.

[347] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

[348] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662-678, 2020.

[349] Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. arXiv preprint arXiv:2012.00614, 2020.

[350] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. arXiv preprint arXiv:2004.14974, 2020.

[351] Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan. COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2116-2129, Online, August 2021. Association for Computational Linguistics.

[352] Mourad Sarrouti, Asma Ben Abacha, Yassine M'rabet, and Dina Demner-Fushman. Evidence-based fact-checking of health-related claims. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3499-3512, 2021.

[353] nrimsky. Sycophancy dataset. https://github.com/nrimsky/LM-exp/blob/main/datasets/sycophancy/ sycophancy.json.

[354] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. Crows-pairs: A challenge dataset for measuring social biases in masked language models. arXiv preprint arXiv:2010.00133, 2020.

[355] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020.

[356] UCI. Adult dataset. https://archive.ics.uci.edu/dataset/2/adult.

[357] Nirali Vaghani. Flipkart products review dataset, 2023.

TRUSTLLM

[358] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A new dataset for automatic medical diagnosis. Advances in Neural Information Processing Systems, 35:31306-31318, 2022.

[359] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275, 2020.

[360] Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. Social chemistry 101: Learning to reason about social and moral norms. arXiv preprint arXiv:2011.00620, 2020.

[361] Nino Scherrer, Claudia Shi, Amir Feder, and David M Blei. Evaluating the moral beliefs encoded in llms. arXiv preprint arXiv:2307.14324, 2023.

[362] Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models, 2023.

[363] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.

[364] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021.

[365] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.

[366] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.

[367] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.

[368] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models, 2022.

[369] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-ofthought prompting, 2023.

[370] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.

[371] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented code emulator, 2023.

[372] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.

[373] Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928, 2023.

[374] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models, 2023.

[375] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. arXiv preprint arXiv:2311.18702, 2023.

[376] Xingwei He, Qianru Zhang, A-Long Jin, Jun Ma, Yuan Yuan, and Siu Ming Yiu. Improving factual error correction by learning to inject factual errors, 2023.

TRUSTLLM

[377] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity, 2023.

[378] Haoqin Tu, Bingchen Zhao, Chen Wei, and Cihang Xie. Sight beyond text: Multi-modal training enhances $11 \mathrm{~ms}$ in truthfulness and ethics. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.

[379] Canyu Chen, Haoran Wang, Matthew Shapiro, Yunyu Xiao, Fei Wang, and Kai Shu. Combating health misinformation in social media: Characterization, detection, intervention, and open issues. arXiv preprint arXiv:2211.05289, 2022.

[380] Yupeng Cao, Aishwarya Muralidharan Nair, Elyon Eyimife, Nastaran Jamalipour Soofi, K. P. Subbalakshmi, John R. Wullert II au2, Chumki Basu, and David Shallcross. Can large language models detect misinformation in scientific news reporting?, 2024.

[381] Aman Rangapur, Haoran Wang, and Kai Shu. Investigating online financial misinformation and its consequences: A computational perspective. arXiv preprint arXiv:2309.12363, 2023.

[382] Yue Huang and Lichao Sun. Harnessing the power of chatgpt in fake news: An in-depth exploration in generation, detection and explanation. arXiv preprint arXiv:2310.05046, 2023.

[383] Canyu Chen and Kai Shu. Can llm-generated misinformation be detected? arXiv preprint arXiv:2309.13788, 2023.

[384] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022.

[385] Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007, 2023.

[386] De Choudhury et al. Ask me in english instead: Cross-lingual evaluation of large language models for healthcare queries. arXiv preprint arXiv:2310.13132, 2023.

[387] Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037, 2022.

[388] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.

[389] Yi Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep Polisetty, Heng Ji, Shih-Fu Chang, Kathleen McKeown, Mohit Bansal, and Avi Sil. Infosurgeon: Cross-media fine-grained information consistency checking for fake news detection. In Proc. The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), 2021.

[390] Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi, and Heng Ji. Faking fake news for real fake news detection: Propaganda-loaded training data generation. In Proc. The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023) Findings, 2023.

[391] Kung-Hsiang Huang, ChengXiang Zhai, and Heng Ji. Improving cross-lingual fact checking with cross-lingual retrieval. In Proc. The 29th International Conference on Computational Linguistics (COLING2022), 2022.

[392] Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. Fact-checking complex claims with program-guided reasoning. arXiv preprint arXiv:2305.12744, 2023.

[393] Haoran Wang and Kai Shu. Explainable claim verification via knowledge-grounded reasoning with large language models. arXiv preprint arXiv:2310.05253, 2023.

TRUSTLLM

[394] Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. Zero-shot faithful factual error correction. In Proc. The 61 st Annual Meeting of the Association for Computational Linguistics (ACL2023), 2023.

[395] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR, 2020.

[396] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR, 2022.

[397] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023.

[398] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.

[399] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.

[400] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.

[401] Ohad Rubin and Jonathan Berant. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421, 2023.

[402] Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models, 2023.

[403] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language models: A survey. arXiv preprint arXiv:2310.16218, 2023.

[404] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.

[405] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. arXiv preprint arXiv:2210.07229, 2022.

[406] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341, 2023.

[407] Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[408] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.

[409] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization, 2023.

[410] Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh R Menon, Md Rizwan Parvez, and Zhe Feng. Delucionqa: Detecting hallucinations in domain-specific question answering, 2023.

[411] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. On early detection of hallucinations in factual question answering, 2023.

[412] Priyesh Vakharia, Devavrat Joshi, Meenal Chavan, Dhananjay Sonawane, Bhrigu Garg, Parsa Mazaheri, and Ian Lane. Don't believe everything you read: Enhancing summarization interpretability through automatic identification of hallucinations in large language models, 2023.

TRUSTLLM

[413] Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi. Alleviating hallucinations of large language models through induced hallucinations, 2023.

[414] Shreyas Verma, Kien Tran, Yusuf Ali, and Guangyu Min. Reducing llm hallucinations using epistemic neural networks, 2023.

[415] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.

[416] Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. arXiv preprint arXiv:2305.13712, 2023.

[417] Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the uncertainty estimation of large language models. arXiv preprint arXiv:2307.01379, 2023.

[418] Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, and Luoyi Fu. Enhancing uncertainty-based hallucination detection with stronger focus, 2023.

[419] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987, 2023.

[420] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.

[421] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. Mitigating language model hallucination with interactive question-knowledge alignment. arXiv preprint arXiv:2305.13669, 2023.

[422] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. arXiv preprint arXiv:2305.14739, 2023.

[423] Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting. arXiv preprint arXiv:2311.13314, 2023.

[424] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-of-note: Enhancing robustness in retrieval-augmented language models, 2023.

[425] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Fine-tuning language models for factuality, 2023.

[426] Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen tse Huang, Wenxiang Jiao, and Michael R. Lyu. The earth is flat? unveiling factual errors in large language models, 2024.

[427] Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, and Monica S. Lam. Wikichat: Stopping the hallucination of large language model chatbots by few-shot grounding on wikipedia, 2023.

[428] Shiyue Zhang, David Wan, and Mohit Bansal. Extractive is not faithful: An investigation of broad unfaithfulness problems in extractive summarization. arXiv preprint arXiv:2209.03549, 2022.

[429] David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, and Mohit Bansal. Faithfulness-aware decoding strategies for abstractive summarization. arXiv preprint arXiv:2303.03278, 2023.

[430] David Wan and Mohit Bansal. Evaluating and improving factuality in multimodal abstractive summarization. arXiv preprint arXiv:2211.02580, 2022.

[431] David Wan and Mohit Bansal. Factpegasus: Factuality-aware pre-training and fine-tuning for abstractive summarization. arXiv preprint arXiv:2205.07830, 2022.

[432] Leonardo FR Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, and Mohit Bansal. Factgraph: Evaluating factuality in summarization with semantic graph representations. arXiv preprint arXiv:2204.06508, 2022.

TRUSTLLM

[433] Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. Evaluating the factual consistency of large language models through summarization. arXiv preprint arXiv:2211.08412, 2022.

[434] Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023.

[435] Leonardo Ranaldi and Giulia Pucci. When large language models contradict humans? large language models' sycophantic behaviour, 2023.

[436] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophancy in language models, 2023.

[437] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.

[438] Nina Rimsky. Reducing sycophancy and improving honesty via activation steering, 2023.

[439] Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation, 2023.

[440] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation, 2023.

[441] Nanna Inie, Jonathan Stray, and Leon Derczynski. Summon a demon and bind it: A grounded theory of $11 \mathrm{~m}$ red teaming in the wild, 2023.

[442] Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, and Yingchun Wang. Fake alignment: Are llms really aligned well?, 2023.

[443] Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. Can llms follow simple rules?, 2023.

[444] Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, and Jordan Boyd-Graber. Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition, 2023.

[445] Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, and Muhao Chen. Cognitive overload: Jailbreaking large language models with overloaded logical thinking, 2023.

[446] Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity, 2023.

[447] Yu Fu, Yufei Li, Wen Xiao, Cong Liu, and Yue Dong. Safety alignment in nlp tasks: Weakly aligned summarization as an in-context attack, 2023.

[448] Wei Zhao, Zhe Li, and Jun Sun. Causality analysis for evaluating the security of large language models, 2023.

[449] Jason Vega, Isha Chaudhary, Changming Xu, and Gagandeep Singh. Bypassing the safety training of open-source llms with priming attacks, 2023.

[450] Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models, 2023.

[451] Aleksander Buszydlik, Karol Dobiczek, Michał Teodor Okoń, Konrad Skublicki, Philip Lippmann, and Jie Yang. Red teaming for large language models at scale: Tackling hallucinations on mathematics tasks, 2023.

[452] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023.

TRUSTLLM

[453] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, and Himabindu Lakkaraju. Certifying llm safety against adversarial prompting, 2023.

[454] Zeyang Sha and Yang Zhang. Prompt stealing attacks against large language models, 2024.

[455] Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. Defending jailbreak prompts via in-context adversarial game, 2024.

[456] Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. Llm jailbreak attack versus defense techniques - a comprehensive study, 2024.

[457] Yueqi Xie, Minghong Fang, Renjie Pi, and Neil Gong. Gradsafe: Detecting unsafe prompts for llms via safety-critical gradient analysis, 2024.

[458] Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, and Christopher Leckie. Round trip translation defence against large language model jailbreaking attacks, 2024.

[459] Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, and Yang Liu. Pandora: Jailbreak gpts by retrieval augmented generation poisoning, 2024.

[460] Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu. Cold-attack: Jailbreaking llms with stealthiness and controllability, 2024.

[461] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding, 2024.

[462] Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, and Yang Liu. Play guessing game with 1lm: Indirect jailbreak attack with implicit clues, 2024.

[463] Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, and Yu Qiao. Attacks, defenses and evaluations for $11 \mathrm{~m}$ conversation safety: A survey, 2024.

[464] Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning, 2023.

[465] Kellin Pelrine, Mohammad Taufeeque, Michał Zając, Euan McLean, and Adam Gleave. Exploiting novel gpt-4 apis, 2023.

[466] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models, 2023.

[467] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.

[468] George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, and Eitan Farchi. Unveiling safety vulnerabilities of large language models, 2023.

[469] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. On the exploitability of instruction tuning, 2023.

[470] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Zhuojun Jiang, Zhaoheng Zheng, Zhuofeng Wu, Muhao Chen, and Chaowei Xiao. Adversarial demonstration attacks on large language models, 2023.

[471] Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, and Chaowei Xiao. On the exploitability of reinforcement learning with human feedback for large language models, 2023.

[472] Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, and Chaowei Xiao. Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger, 2023.

[473] Javier Rando and Florian Tramèr. Universal jailbreak backdoors from poisoned human feedback, 2023.

[474] Yuanpu Cao, Bochuan Cao, and Jinghui Chen. Stealthy and persistent unalignment on large language models via backdoor injections, 2023.

[475] Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. Composite backdoor attacks against large language models, 2023.

TRUSTLLM

[476] Hongwei Yao, Jian Lou, and Zhan Qin. Poisonprompt: Backdoor attack on prompt-based large language models, 2023.

[477] Wencong You, Zayd Hammoudeh, and Daniel Lowd. Large language models are better adversaries: Exploring generative clean-label backdoor attacks against text classifiers, 2023.

[478] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models, 2023.

[479] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for large language models. In NeurIPS 2023 Workshop on Backdoors in Deep Learning - The Good, the Bad, and the Ugly, 2023.

[480] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. arXiv preprint arXiv:2305.00944, 2023.

[481] Xuan Sheng, Zhicheng Li, Zhaoyang Han, Xiangmao Chang, and Piji Li. Punctuation matters! stealthy backdoor attack for language models, 2023.

[482] Jiachen Zhao, Zhun Deng, David Madras, James Zou, and Mengye Ren. Learning and forgetting unsafe examples in large language models, 2023.

[483] Anonymous. On the safety of open-sourced large language models: Does alignment really prevent them from being misused?, 2023.

[484] Fangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, and Xing Xie. Defending chatgpt against jailbreak attack via self-reminder. 2023.

[485] Ahmed Salem, Andrew Paverd, and Boris Köpf. Maatphor: Automated variant analysis for prompt injection attacks, 2023.

[486] Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, and Chao Shen. A mutation-based method for multi-modal jailbreaking attack detection, 2023.

[487] Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, and Muhao Chen. Test-time backdoor mitigation for black-box large language models with defensive demonstrations. arXiv preprint arXiv:2311.09763, 2023.

[488] Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, and Sinno Jialin Pan. Backdoor attacks on dense passage retrievers for disseminating misinformation, 2024.

[489] Jiang Zhang, Qiong Wu, Yiming Xu, Cheng Cao, Zheng Du, and Konstantinos Psounis. Efficient toxic content detection by bootstrapping and distilling large language models, 2023.

[490] Heegyu Kim and Hyunsouk Cho. Gta: Gated toxicity avoidance for Im performance preservation, 2023.

[491] Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, and Bryan Catanzaro. Exploring the limits of domain-adaptive training for detoxifying large-scale language models. Advances in Neural Information Processing Systems, 35:35811-35824, 2022 .

[492] Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. Fundamental limitations of alignment in large language models, 2023.

[493] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733, 2023.

[494] Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. 2022.

[495] Dan is my new friend, 2022. https://old.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_ friend/.

[496] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with 1lms via cipher. arXiv preprint arXiv:2308.06463, 2023.

TRUSTLLM

[497] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models, 2023.

[498] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on large language models, 2023.

[499] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.

[500] Perspective api, 2023. https://www.perspectiveapi.com.

[501] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[502] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions, 2023.

[503] Neeraj Varshney, Pavel Dolin, Agastya Seth, and Chitta Baral. The art of defending: A systematic evaluation and analysis of $11 \mathrm{~m}$ defense strategies on safety and over-defensiveness, 2023.

[504] Yau-Shian Wang and Yingshan Chang. Toxicity detection with generative prompt-based inference. arXiv preprint arXiv:2205.12390, 2022.

[505] Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. Probing toxic content in large pre-trained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4262-4274, 2021.

[506] Youngwook Kim, Shinwoo Park, Youngsoo Namgoong, and Yo-Sub Han. ConPrompt: Pre-training a language model with machine-generated data for implicit hate speech detection. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10964-10980, Singapore, December 2023. Association for Computational Linguistics.

[507] Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang. Unveiling the implicit toxicity in large language models, 2023.

[508] Facebook content moderation, 2023. https://transparency.fb.com/policies/community-standards/ hate-speech/.

[509] Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et al. Can machines learn morality? the delphi experiment. arXiv e-prints, pages arXiv-2110, 2021.

[510] Machine learning can help reduce toxicity, improving online conversation, 2023. https://jigsaw.google. com/the-current/toxicity/countermeasures/.

[511] Jigsaw toxicity dataset, 2023. https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge.

[512] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867, 2023.

[513] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. Understanding the capabilities, limitations, and societal impact of large language models. ArXiv, abs/2102.02503, 2021.

[514] Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274, 2023.

[515] Ning Bian, Peilin Liu, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, and Le Sun. A drop of ink may make a million think: The spread of false information in large language models. arXiv preprint arXiv:2305.04812, 2023.

TRUSTLLM

[516] Alessandro Pegoraro, Kavita Kumari, Hossein Fereidooni, and Ahmad-Reza Sadeghi. To chatgpt, or not to chatgpt: That is the question! arXiv preprint arXiv:2304.01487, 2023.

[517] PV Charan, Hrushikesh Chunduri, P Mohan Anand, and Sandeep K Shukla. From text to mitre techniques: Exploring the malicious use of large language models for generating cyber attack payloads. arXiv preprint arXiv:2305.15336, 2023.

[518] Mithun Das, Saurabh Kumar Pandey, and Animesh Mukherjee. Evaluating chatgpt's performance for multilingual and emoji-based hate speech detection. arXiv preprint arXiv:2305.13276, 2023.

[519] Fan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736, 2023.

[520] Yanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. Investigating the fairness of large language models for predictions on tabular data, 2023.

[521] Jiaxu Zhao, Meng Fang, Shirui Pan, Wenpeng Yin, and Mykola Pechenizkiy. Gptbias: A comprehensive framework for evaluating bias in large language models, 2023.

[522] Yueqing Liang, Lu Cheng, Ali Payani, and Kai Shu. Beyond detection: Unveiling fairness vulnerabilities in abusive language models, 2023.

[523] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models' selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023.

[524] Guanqun Bi, Lei Shen, Yuqiang Xie, Yanan Cao, Tiangang Zhu, and Xiaodong He. A group fairness lens for large language models, 2023.

[525] Hannah Kirk, Yennie Jun, Haider Iqbal, Elias Benussi, Filippo Volpin, Frederic A. Dreyer, Aleksandar Shtedritski, and Yuki M. Asano. Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models, 2021.

[526] Hadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language models. CI '23, page 12-24, New York, NY, USA, 2023. Association for Computing Machinery.

[527] Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and Nanyun Peng. "kelly is a warm person, joseph is a role model": Gender biases in $11 \mathrm{~m}$-generated reference letters, 2023.

[528] Abel Salinas, Louis Penafiel, Robert McCormack, and Fred Morstatter. "im not racist but...": Discovering bias in the internal knowledge of large language models, 2023.

[529] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models, 2021.

[530] Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, et al. On measures of biases and harms in nlp. arXiv preprint arXiv:2108.03362, 2021.

[531] Naomi Ellemers. Gender stereotypes. Annual Review of Psychology, 69(1):275-298, 2018. PMID: 28961059.

[532] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods, 2018.

[533] Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online, August 2021. Association for Computational Linguistics.

[534] Religious stereotyping and voter support for evangelical candidates. Political Research Quarterly, $62(2): 340-354,2009$.

[535] Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, and Kai-Wei Chang. On measures of biases and harms in nlp, 2022.

[536] Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. On measuring and mitigating biased inferences of word embeddings. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 2020.

TRUSTLLM

[537] Lucas Dixon, John Li, Jeffrey Scott Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 2018.

[538] SSA.gov. National average wage index. https://www.ssa.gov/oact/cola/AWI.html.

[539] Alan Agresti. An introduction to categorical data analysis. 1990.

[540] David Rozado. The political biases of chatgpt. Social Sciences, 12(3):148, 2023.

[541] Robert W McGee. Is chat gpt biased against conservatives? an empirical study. An Empirical Study (February 15, 2023), 2023.

[542] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524, 2023.

[543] Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. Rethinking the evaluation for conversational recommendation in the era of large language models. arXiv preprint arXiv:2305.13112, 2023.

[544] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. Uncovering chatgpt's capabilities in recommender systems. arXiv preprint arXiv:2305.02182, 2023.

[545] Yichen Jiang and Mohit Bansal. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa, 2019.

[546] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding, 2020.

[547] Tong Niu and Mohit Bansal. Adversarial over-sensitivity and over-stability strategies for dialogue models, 2018.

[548] Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. A survey of adversarial defenses and robustness in nlp. ACM Computing Surveys, 55(14s):1-39, 2023.

[549] Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, and Christopher Ré. Robustness gym: Unifying the NLP evaluation landscape. In Avi Sil and Xi Victoria Lin, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 42-55, Online, June 2021. Association for Computational Linguistics.

[550] Nanyang Ye, Kaican Li, Lanqing Hong, Haoyue Bai, Yiting Chen, Fengwei Zhou, and Zhenguo Li. OoDBench: Benchmarking and understanding out-of-distribution generalization datasets and algorithms. arXiv preprint arXiv:2106.03721, 2021.

[551] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, and Fatemeh Shiri. On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. arXiv preprint arXiv:2301.12868, 2023.

[552] Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, and Shiyu Chang. Certified robustness for large language models with self-denoising, 2023.

[553] Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong. Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert, 2020.

[554] OpenAI. New and improved embedding model, 2023.

[555] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.

[556] Pieter Muysken, Norval Smith, et al. The study of pidgin and creole languages. Pidgins and creoles: An introduction, pages 3-14, 1995.

TRUSTLLM

[557] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J Liu. Out-of-distribution detection and selective generation for conditional language models. arXiv preprint arXiv:2209.15558, 2022.

[558] Maxime Peyrard, Sarvjeet Singh Ghotra, Martin Josifoski, Vidhan Agarwal, Barun Patra, Dean Carignan, Emre Kiciman, and Robert West. Invariant language modeling. arXiv preprint arXiv:2110.08413, 2021.

[559] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. Anomalous example detection in deep learning: A survey. IEEE Access, 8:132330-132347, 2020.

[560] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334, 2021.

[561] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.

[562] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

[563] Lei Shu, Hu Xu, and Bing Liu. Doc: Deep open classification of text documents. arXiv preprint arXiv:1709.08716, 2017.

[564] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.

[565] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31,2018 .

[566] Di Jin, Shuyang Gao, Seokhwan Kim, Yang Liu, and Dilek Hakkani-Tür. Towards textual out-ofdomain detection without in-domain labels. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:1386-1395, 2022.

[567] Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty, 2023.

[568] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022.

[569] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378-1406, 2021.

[570] Zheyan Shen, Peng Cui, Tong Zhang, and Kun Kunag. Stable learning via sample reweighting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5692-5699, 2020.

[571] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization. In International Conference on Machine Learning, pages 6804-6814. PMLR, 2021.

[572] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big data, 3(1):1-40, 2016.

[573] Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pages 242-264. IGI global, 2010.

[574] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43-76, 2020.

[575] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:135153,2018

[576] Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, and Shuiwang Ji. Joint learning of label and environment causal independence for graph out-of-distribution generalization. arXiv preprint arXiv:2306.01103, 2023.

TRUSTLLM

[577] Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure and feature extrapolation for out-of-distribution generalization. arXiv preprint arXiv:2306.08076, 2023.

[578] Judea Pearl. Causality. Cambridge university press, 2009.

[579] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.

[580] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.

[581] Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008.

[582] Jose G Moreno-Torres, Troy Raeder, Rocío Alaiz-Rodríguez, Nitesh V Chawla, and Francisco Herrera. A unifying view on dataset shift in classification. Pattern Recognition, 45(1):521-530, 2012.

[583] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.

[584] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.

[585] Gerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift and hidden contexts. Machine learning, 23(1):69-101, 1996.

[586] Linyi Yang, Yaoxiao Song, Xuan Ren, Chenyang Lyu, Yidong Wang, Lingqiao Liu, Jindong Wang, Jennifer Foster, and Yue Zhang. Out-of-distribution generalization in text classification: Past, present, and future. arXiv preprint arXiv:2305.14104, 2023.

[587] Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, and Ruihai Dong. Exploring the efficacy of automatically generated counterfactuals for sentiment analysis. arXiv preprint arXiv:2106.15231, 2021.

[588] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.

[589] Milad Moradi, Kathrin Blagec, and Matthias Samwald. Deep learning models are not robust against noise in clinical text. arXiv preprint arXiv:2108.12242, 2021.

[590] Zhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages $14024-14031,2021$.

[591] Xilun Chen and Claire Cardie. Multinomial adversarial networks for multi-domain text classification. arXiv preprint arXiv:1802.05694, 2018.

[592] Chenyang Lyu, Jennifer Foster, and Yvette Graham. Extending the scope of out-of-domain: Examining qa models in multiple subdomains. arXiv preprint arXiv:2204.04534, 2022.

[593] Pouya Pezeshkpour, Sarthak Jain, Sameer Singh, and Byron C Wallace. Combining feature and instance attribution to detect artifacts. arXiv preprint arXiv:2107.00323, 2021.

[594] Barbara Plank. Cross-lingual cross-domain nested named entity evaluation on english web texts. In Findings of ACL 2021, page 1808. Association for Computational Linguistics, 2021.

[595] Xiner Li, Jing Zhao, Wei-Qiang Zhang, Zhiqiang Lv, and Shen Huang. Keyword search based on unsupervised pre-trained acoustic models. International Journal of Asian Language Processing, $31(03 n 04): 2250005,2021$.

[596] Xuezhi Wang, Haohan Wang, and Diyi Yang. Measure and improve robustness in nlp models: A survey. arXiv preprint arXiv:2112.08313, 2021.

[597] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations. arXiv preprint arXiv:2306.04618, 2023.

TRUSTLLM

[598] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are sometimes inversely correlated on real-world datasets. arXiv preprint arXiv:2209.00613, 2022.

[599] Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, and Lei Fan. Policygpt: Automated analysis of privacy policies with large language models, 2023.

[600] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633-2650, 2021.

[601] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives for defending against extraction attacks, 2023.

[602] Seth Neel and Peter Chang. Privacy issues in large language models: A survey, 2023.

[603] Liang Niu, Shujaat Mirza, Zayd Maradni, and Christina Pöpper. \{CodexLeaks\}: Privacy leaks from code generation language models in \{GitHub $\}$ copilot. In 32nd USENIX Security Symposium (USENIX Security 23), pages 2133-2150, 2023.

[604] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable extraction of training data from (production) language models, 2023.

[605] Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, and Zheng Xu. User inference attacks on large language models, 2023.

[606] Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, and Wei Xu. Reducing privacy risks in online self-disclosures with language models, 2023.

[607] Yansong Li, Zhixing Tan, and Yang Liu. Privacy-preserving prompt tuning for large language model services, 2023.

[608] Yejin Bang, Tiezheng Yu, Andrea Madotto, Zhaojiang Lin, Mona Diab, and Pascale Fung. Enabling classifiers to make judgements explicitly aligned with human values. arXiv preprint arXiv:2210.07652, 2022.

[609] Aida Ramezani and Yang Xu. Knowledge of cultural moral norms in large language models, 2023.

[610] Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindřich Libovický, Alexander Fraser, and Kristian Kersting. Do multilingual language models capture differing moral norms?, 2022.

[611] Michal Kosinski. Theory of mind might have spontaneously emerged in large language models, 2023.

[612] Max J. van Duijn, Bram M. A. van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R. Spruit, and Peter van der Putten. Theory of mind in large language models: Examining performance of 11 state-of-the-art models vs. children aged 7-10 on advanced tests, 2023.

[613] Shalom H Schwartz. An overview of the schwartz theory of basic values. Online readings in Psychology and Culture, 2(1):11, 2012.

[614] Jing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, and Xing Xie. Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values, 2023.

[615] James Moor et al. Four kinds of ethical robots. Philosophy Now, 72:12-14, 2009.

[616] Machine ethics, 2023. https:///en.wikipedia.org/wiki/Machine_ethics.

[617] Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu. Denevil: Towards deciphering and navigating the ethical values of large language models via instruction learning. arXiv preprint arXiv:2310.11053, 2023.

[618] Xiaoyuan Yi, Jing Yao, Xiting Wang, and Xing Xie. Unpacking the ethical value alignment in big models, 2023.

[619] David J Chalmers. Could a large language model be conscious? arXiv preprint arXiv:2303.07103, 2023.

TRUSTLLM

[620] Jen tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R. Lyu. Emotionally numb or empathetic? evaluating how llms feel using emotionbench, 2023.

[621] Per Carlbring, Heather Hadjistavropoulos, Annet Kleiboer, and Gerhard Andersson. A new era in internet interventions: The advent of chat-gpt and ai-assisted therapist guidance. Internet Interventions, $32,2023$.

[622] Yue Huang, Qihui Zhang, Lichao Sun, et al. Trustgpt: A benchmark for trustworthy and responsible large language models. arXiv preprint arXiv:2306.11507, 2023.

[623] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.

[624] Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.

[625] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.

[626] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427, 2023.

[627] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.

[628] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.

[629] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents, 2023.

[630] Dan Hendrycks, Mantas Mazeika, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, and Jacob Steinhardt. What would jiminy cricket do? towards agents that behave morally. NeurIPS, 2021.

[631] Shelley Duval and Robert A Wicklund. A theory of objective self awareness. 1972.

[632] Alain Morin. Self-awareness part 1: Definition, measures, effects, functions, and antecedents. Social and personality psychology compass, 5(10):807-823, 2011.

[633] Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. Towards empathetic opendomain conversation models: a new benchmark and dataset, 2019.

[634] Richard D Lane, Donald M Quinlan, Gary E Schwartz, Pamela A Walker, and Sharon B Zeitlin. The levels of emotional awareness scale: A cognitive-developmental measure of emotion. Journal of personality assessment, 55(1-2):124-134, 1990.

[635] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. Towards interpretable mental health analysis with large language models, 2023.

[636] Kristina Schaaff, Caroline Reinig, and Tim Schlippe. Exploring chatgpt's empathic abilities, 2023.

[637] T Goode. Promoting cultural diversity and cultural competency: self-assessment checklist for personnel providing behavioral health services and supports to children, youth and their families. Retrieved August, 24:2006, 2006.

[638] Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, and Lichao Sun. I think, therefore i am: Awareness in large language models. arXiv preprint arXiv:2401.17882, 2024.

TRUSTLLM

[639] AyseKok Arslan. A benchmark model for language models towards increased transparency. International Journal of Latest Engineering Research and Applications (IJLERA), 7:42-48, 2022.

[640] Heike Felzmann, Eduard Fosch-Villaronga, Christoph Lutz, and Aurelia Tamò-Larrieux. Towards transparency by design for artificial intelligence. Science and Engineering Ethics, 26(6):3333-3361, 2020.

[641] Albert Meijer. Understanding the complex dynamics of transparency. Public administration review, $73(3): 429-439,2013$.

[642] Richard W. Oliver. What is transparency? New York: McGraw-Hill, 2004.

[643] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220-229, 2019.

[644] Anamaria Crisan, Margaret Drouhard, Jesse Vig, and Nazneen Rajani. Interactive model cards: A human-centered approach to model documentation. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 427-439, 2022.

[645] Emily M Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587-604, 2018.

[646] Kasia S Chmielinski, Sarah Newman, Matt Taylor, Josh Joseph, Kemi Thomas, Jessica Yurkofsky, and Yue Chelsea Qiu. The dataset nutrition label (2nd gen): Leveraging context to mitigate harms in artificial intelligence. arXiv preprint arXiv:2201.03954, 2022.

[647] Tobin South, Robert Mahari, and Alex Pentland. Transparency by design for large language models. Computational Legal Futures, Network Law Review.(2023), 2023.

[648] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion, 58:82-115, 2020.

[649] Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià GarrigaAlonso. Towards automated circuit discovery for mechanistic interpretability, 2023.

[650] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: A circuit for indirect object identification in gpt-2 small, 2022.

[651] Nadia Burkart and Marco F Huber. A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245-317, 2021.

[652] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages $1747-1764,2022$.

[653] Sungsoo Ray Hong, Jessica Hullman, and Enrico Bertini. Human factors in model interpretability: Industry practices, challenges, and needs. Proceedings of the ACM on Human-Computer Interaction, $4(\mathrm{CSCW} 1): 1-26,2020$.

[654] Gagan Bansal, Zana Buçinca, Kenneth Holstein, Jessica Hullman, Alison Marie Smith-Renner, Simone Stumpf, and Sherry Wu. Workshop on trust and reliance in ai-human teams (trait). In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-6, 2023.

[655] Clifford Nass and Youngme Moon. Machines and mindlessness: Social responses to computers. Journal of social issues, 56(1):81-103, 2000.

[656] Sandra Wachter and Brent Mittelstadt. A right to reasonable inferences: re-thinking data protection law in the age of big data and ai. Colum. Bus. L. Rev., page 494, 2019.

[657] Aimee Van Wynsberghe. Designing robots for care: Care centered value-sensitive design. In Machine ethics and robot ethics, pages 185-211. Routledge, 2020.

TRUSTLLM

[658] Tal Z Zarsky. Transparent predictions. U. Ill. L. Rev., page 1503, 2013.

[659] Cass R Sunstein. Output transparency vs. input transparency. In Troubling transparency: The history and future of freedom of information, pages 187-205. Columbia University Press, 2018.

[660] Joshua Alexander Kroll. Accountable algorithms. PhD thesis, Princeton University, 2015.

[661] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[662] OpenAI. Gpt-4, 2023. https://openai.com/product/gpt-4.

[663] Nancy G Leveson and Clark S Turner. An investigation of the therac-25 accidents. Computer, 26(7):18$41,1993$.

[664] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694, 2023.

[665] Eugene Volokh. Large libel models? liability for ai output. J. Free Speech L., 3:489, 2023.

[666] Protection for private blocking and screening of offensive material. 47 U.S.C. $\S 230,1996$.

[667] Matt Perault. Section 230 won't protect chatgpt. J. Free Speech L., 3:363, 2023.

[668] Will Knight. Openai's ceo says the age of giant ai models is already over, Apr 2023.

[669] Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. Mgtbench: Benchmarking machine-generated text detection, 2023.

[670] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156, 2023.

[671] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Frederick Wieting, and Mohit Iyyer. Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[672] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint arXiv:2301.11305, 2023

[673] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. arXiv preprint arXiv:2306.05540, 2023.

[674] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better black-box machine-generated text detectors. arXiv preprint arXiv:2305.09859, 2023.

[675] Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130, 2023.

[676] Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. arXiv preprint arXiv:2305.17359, 2023.

[677] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597, 2023.

[678] Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, and Bhiksha Ramakrishnan. Gpt-sentinel: Distinguishing human and chatgpt generated content. arXiv preprint arXiv:2305.07969, 2023.

[679] Jan Hendrik Kirchner, Lama Ahmad, Scott Aaronson, and Jan Leike. New ai classifier for indicating ai-written text, 2023.

[680] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. arXiv preprint arXiv:2301.10226, 2023.

[681] Scott Aaronson. Watermarking of large language models. Online Video, 2023. https://www.youtube. com/watch?v=2Kx9jbSMZqA.

[682] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. arXiv preprint arXiv:2306.04634, 2023.

[683] Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. A semantic invariant robust watermark for large language models. arXiv preprint arXiv:2310.06356, 2023.

[684] Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, and Boaz Barak. Watermarks in the sand: Impossibility of strong watermarking for generative models, 2023.

[685] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models. arXiv preprint arXiv:2310.10669, 2023.

[686] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models, 2023.

[687] Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, and Makoto Yamada. Necessary and sufficient watermark for large language models. arXiv preprint arXiv:2310.00833, 2023.

[688] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim. Who wrote this code? watermarking for code generation, 2023.

[689] Michael M. Grynbaum and Ryan Mac. The times sues openai and microsoft over a.i. use of copyrighted work, 2023. https://www.nytimes.com/2023/12/27/business/media/ new-york-times-open-ai-microsoft-lawsuit.html.

[690] SAVANNAH FORTIS. Evidence mounts as new artists jump on stability ai, midjourney copyright lawsuit, 2023. https://cointelegraph.com/news/ evidence-mounts-new-artists-join-stability-ai-mid-journey-copyright-lawsuit.

[691] George Lawton. Is ai-generated content copyrighted?, 2023.

[692] The court recognized the ai-generated content as a work and entitled to copyright, 2020.

[693] Aida Davani, Mark Díaz, Dylan Baker, and Vinodkumar Prabhakaran. Disentangling perceptions of offensiveness: Cultural and moral correlates, 2023.

[694] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.

[695] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210-31227. PMLR, 2023.

[696] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. arXiv preprint arXiv:2108.13161, 2021.

[697] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021.

[698] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023.

[699] Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: A multi-level fine-grained constraints following benchmark for large language models, 2023.

[700] Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, and Xuezhe Ma. Evaluating large language models on controlled generation tasks, 2023.

TRUSTLLM

[701] Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems, 33:1129-1141, 2020.

[702] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pages 97-117. Springer, 2017.

[703] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. Advances in neural information processing systems, 31, 2018.

[704] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In international conference on machine learning, pages 1310-1320. PMLR, 2019.

[705] Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. Branch and bound for piecewise linear neural network verification. Journal of Machine Learning Research, $21(42): 1-39,2020$.

[706] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1-30, 2019.

[707] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Betacrown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. Advances in Neural Information Processing Systems, 34:29909-29921, 2021.

[708] Maksym Andriushchenko and Matthias Hein. Provably robust boosted decision stumps and trees against adversarial attacks. Advances in Neural Information Processing Systems, 32, 2019.

[709] Hongge Chen, Huan Zhang, Si Si, Yang Li, Duane Boning, and Cho-Jui Hsieh. Robustness verification of tree-based models. Advances in Neural Information Processing Systems, 32, 2019.

[710] Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T. Johnson. The fourth international verification of neural networks competition (vnn-comp 2023): Summary and results, 2023.

[711] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International conference on machine learning, pages 5286-5295. PMLR, 2018.

[712] Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for provably robust image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4842-4851, 2019.

[713] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316, 2019.

[714] Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Fast certified robust training with short warmup. Advances in Neural Information Processing Systems, 34:18335-18349, 2021.

[715] Kai Hu, Andy Zou, Zifan Wang, Klas Leino, and Matt Fredrikson. Scaling in depth: Unlocking robustness certification on imagenet. Advances in Neural Information Processing Systems, 2023.

[716] Robin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang. Certified robustness to adversarial word substitutions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4129-4142, Hong Kong, China, November 2019. Association for Computational Linguistics.

[717] Mao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for certified robustness to adversarial word substitutions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3465-3475, Online, July 2020. Association for Computational Linguistics.

TRUSTLLM

[718] Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving verified robustness to symbol substitutions via interval bound propagation. In Empirical Methods in Natural Language Processing (EMNLP), pages 4081-4091, 2019.

[719] Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang. Certified robustness to text adversarial attacks by randomized [mask]. Computational Linguistics, 49(2):395-427, 2023.

[720] Zhuoqun Huang, Neil G Marchant, Keane Lucas, Lujo Bauer, Olga Ohrimenko, and Benjamin IP Rubinstein. Rs-del: Edit distance robustness certificates for sequence classifiers via randomized deletion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[721] Pengfei Yu and Heng Ji. Self information update for large language models through mitigating exposure bias. In arxiv, 2023.

[722] Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, and Tsung-Yi Ho. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing Systems, 2023.

[723] Brian R Bartoldson, Bhavya Kailkhura, and Davis Blalock. Compute-efficient deep learning: Algorithmic trends and opportunities. Journal of Machine Learning Research, 24:1-77, 2023.

[724] Wensheng Gan, Zhenlian Qi, Jiayang Wu, and Jerry Chun-Wei Lin. Large language models in education: Vision and opportunities, 2023.

[725] Daniel Leiker. White paper: The generative education (gened) framework, 2023.

[726] Mingze Yuan, Peng Bao, Jiajia Yuan, Yunhao Shen, Zifan Chen, Yi Xie, Jie Zhao, Yang Chen, Li Zhang, Lin Shen, and Bin Dong. Large language models illuminate a progressive pathway to artificial healthcare assistant: A review, 2023.

[727] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. Large language models in finance: A survey, 2023.

[728] Haoqiang Kang and Xiao-Yang Liu. Deficiency of large language models in finance: An empirical examination of hallucination, 2023.

[729] Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, and Joshua Saxe. Purple llama cyberseceval: A secure coding benchmark for language models, 2023.

[730] Sanghak Oh, Kiho Lee, Seonhye Park, Doowon Kim, and Hyoungshick Kim. Poisoned chatgpt finds work for idle hands: Exploring developers' coding practices with insecure suggestions from poisoned ai models, 2023.

[731] Fangzhou Wu, Qingzhao Zhang, Ati Priya Bajaj, Tiffany Bao, Ning Zhang, Ruoyu "Fish" Wang, and Chaowei Xiao. Exploring the limits of chatgpt in software security applications, 2023.

[732] James Boyko, Joseph Cohen, Nathan Fox, Maria Han Veiga, Jennifer I-Hsiu Li, Jing Liu, Bernardo Modenesi, Andreas H. Rauch, Kenneth N. Reid, Soumi Tribedi, Anastasia Visheratina, and Xin Xie. An interdisciplinary outlook on large language models for scientific research, 2023.

[733] Michal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 2023.

[734] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 1, 2023.

[735] Fei Dou, Jin Ye, Geng Yuan, Qin Lu, Wei Niu, Haijian Sun, Le Guan, Guoyu Lu, Gengchen Mai, Ninghao Liu, et al. Towards artificial general intelligence (agi) in the internet of things (iot): Opportunities and challenges. arXiv preprint arXiv:2309.07438, 2023.

TRUSTLLM

[736] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models for time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196, 2023.

[737] Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, et al. Rethinking mobile AI ecosystem in the LLM era. arXiv preprint arXiv:2308.14363, 2023.

[738] Xingyu Chen and Xinyu Zhang. RF Genesis: Zero-shot generalization of mmwave sensing through simulation-based data synthesis and generative diffusion models. In ACM Conference on Embedded Networked Sensor Systems (SenSys' 23), 2023.

[739] Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Abbas Jamalipour, Dong In Kim, Victor Leung, et al. Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services. arXiv preprint arXiv:2303.16129, 2023.

[740] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.

[741] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.

[742] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023.

[743] Joonhyun Jeong. Hijacking context in large multi-modal models, 2023.

[744] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models, 2023.

[745] Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. Sneakyprompt: Jailbreaking text-toimage generative models. arXiv preprint arXiv:2305.12082, 2023.

[746] Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y. Zhao. Prompt-specific poisoning attacks on text-to-image generative models, 2023.

[747] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback, 2023.

[748] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models, 2023.

[749] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning, 2023.

[750] Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, and Xing Xie. ToViLaG: Your visual-language generative model is also an evildoer. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3508-3533, Singapore, December 2023. Association for Computational Linguistics.

[751] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models, 2023.

[752] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In The Second Workshop on New Frontiers in Adversarial Machine Learning, 2023.

[753] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.

[754] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models, 2023.

[755] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. arXiv preprint arXiv:2305.16934, 2023.

[756] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023 .

[757] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.

[758] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023.

[759] Youpeng Li, Xuyu Wang, and Lingling An. Hierarchical clustering-based personalized federated learning for robust and fair human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 7(1):1-38, 2023.

[760] Peng Liao, Xuyu Wang, Lingling An, Shiwen Mao, Tianya Zhao, and Chao Yang. TFSemantic: A time-frequency semantic GAN framework for imbalanced classification using radio signals. ACM Transactions on Sensor Networks, 2023.

[761] Syed Saqib Ali and Bong Jun Choi. State-of-the-art artificial intelligence techniques for distributed smart grids: A review. Electronics, 9(6):1030, 2020.

[762] Wenjuan Sun, Paolo Bocchini, and Brian D Davison. Applications of artificial intelligence for disaster management. Natural Hazards, 103(3):2631-2689, 2020.
