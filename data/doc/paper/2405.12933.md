# Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs 

Bilgehan Sel *<br>Virginia Tech<br>bsel@vt.edu

Kun Zhou<br>Amazon<br>zhouku@amazon.com

Priya Shanmugasundaram *<br>Virginia Tech<br>priyas@vt.edu

Ming Jin<br>Virginia Tech<br>jinming@vt.edu


#### Abstract

Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives. Central to SKIG's mechanism is simulating accountability for actions, which, alongside empathy exercises and risk assessment, is pivotal to its effectiveness. We validate SKIG's performance across various moral reasoning benchmarks with proprietary and opensource LLMs, and investigate its crucial components through extensive ablation analyses.


## 1 Introduction

In recent years, large language models (LLMs) (Vaswani et al., 2017; Radford et al., 2018; Devlin et al., 2018) have showcased an unprecedented degree of performance in reasoning (Wei et al., 2021; Huang and Chang, 2022; Srivastava et al., 2022), optimization (Li et al., 2023; Guo et al., 2023; Jin et al., 2023), education (Kung et al., 2023; Kasneci et al., 2023), and instruction following (Ouyang et al., 2022). Most prior works focused on standard prompting where we expect answer from the model right away; later work has shown that generating step-by-step reasoning can be superior (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Zhang et al., 2022). However, ethical decision-making in the face of potential risks to society still encounters stumbling blocks (Hendrycks et al., 2020; Weidinger et al., 2021; Pan et al., 2023).

Moral reasoning, unlike general problemsolving, involves charting the intricate landscape[^0]

of human values and ethics. This complexity is partly due to the influence of culture and political ideologies on morality (Haidt, 2013) and social biases (Fraser et al., 2022; Weidinger et al., 2022). However, there also exist universal moral values that transcend cultural differences (Dogruyol et al., 2019).

To address these challenges, most approaches have focused on aligning LLMs with human values through top-down approaches such as finetuning (Ganguli et al., 2022; Bai et al., 2022a,c) or prompting (Bang et al., 2022). Recent works have turned to deliberate thinking by counterfactual reasoning to enhance the deduction abilities of LLMs (Ma et al., 2023). Following recent advancements in planning with LLMs, we argue that the current limitations stem from two main issues: under-exploration of the consequences of probable decisions (Long, 2023; Yao et al., 2023; Sel et al., 2023) and a lack of accountability for the LLMs' choices (Sun et al., 2024, Sec. 13). (Taleb and Sandis, 2013) argue that bearing the outcomes of one's decisions lead to more ethical and responsible choices that minimizes the risky tail events that can be detrimental to every stakeholder affected. Inspired by these insights, we present the Skin-inthe-Game (SKIG) framework for LLMs to enhance their moral reasoning capabilities.

In our SKIG framework, we leverage LLMs to explore different scenarios based on given situations and potential actions. This approach facilitates a deeper understanding of the decision impacts on various stakeholders. We make the language model envision itself as each character in a situation and simulate accountability for its actions as shown in figure 1. This perspective shift has led to marked improvements, with substantial performance enhancements of up to $70 \%$ across a wide array of benchmarks. These improvements are consistent across various types of LLMs, including both proprietary and open-source models.

![](https://cdn.mathpix.com/cropped/2024_05_26_46412929a8fd4e82c72eg-02.jpg?height=777&width=1313&top_left_y=231&top_left_x=360)

Figure 1: Illustration outlining various strategies for tackling reasoning problems with LLMs. The red box contains existing methods that use single-turn methods Standard Prompting and zero-shot Chain-of-Thought. The blue box contains Thought Experiment, a multi-turn single-perspective framework. The green box contains SKIG, our proposed multi-turn multi-perspective reasoning framework.

## 2 Related Work

Morality in LLMs The investigation of morality in LLMs has attracted significant attention, reflecting numerous viewpoints and methodologies. LLMs are scrutinized regarding their societal impacts and ethical decision-making (Bender et al., 2021), as well as widespread social biases they harbor (Bordia and Bowman, 2019; Abid et al., 2021). The practical challenges of overcoming these are attributed to the vague goal of alignment to human values due to wide range of moral beliefs (Gabriel, 2020; Floridi et al., 2021). Finetuning on specialized datasets on top of pretraining improves alignment (Bai et al., 2022b; Bakker et al., 2022; Liu et al., 2023) along with counterfactual reasoning (Ma et al., 2023). Our work differs by promoting exploration of decisions and their potential impacts on each involved party through simulated accountability-raising awareness of the LLMs' own actions for the stakeholders as a whole.

Decision making with LLMs LLMs can be adapted to many downstream tasks such as planning and recommendations by prompting (Yang et al., 2023). Chain-of-Thought (Wei et al., 2022) and recent advancements (Long, 2023; Yao et al., 2023; Sel et al., 2023) improve performance on multi-step reasoning tasks. Self-consistency (Wang et al., 2022) samples many rationales to help with covering a larger decision landscape. Our SKIG framework is complementary to these approaches but adds the critical dimension of analyzing stakeholder impacts for a given decision under various scenarios. The key is asking LLMs to "put skin in the game" by explicitly imagining and tracing the impact of any decision or recommendation it makes. From an alignment perspective, we aim to change the intrinsic optimization objective (in mesa-optimization (Hubinger et al., 2019)) to incorporate multiple stakeholder objectives (see $\mathrm{Sec}$. 3.1 for a formal discussion).

The key notion of simulated accountability is along the lines of discussions of accountability (Bovens, 2014; Sun et al., 2024), but it differs in the critical way that we do not actually hold LLMs accountable, but prompt them to consider all the impacts their decisions may have. This perspective frame is shown to significantly boost their moral reasoning capabilities (see Sec. 4).

## 3 Method

Our approach draws inspiration from the Skin-inthe-Game concept introduced by Taleb and Sandis (2014). The essence of our method lies in aligning decision-makers with both the potential rewards and risks inherent in their choices. By integrating principles derived from psychology, skin-inthe-game philosophy, and ethical decision-making, our proposed approach not only enhances moral reasoning but also cultivates a more nuanced and
conscientious decision-making process.

### 3.1 Skin-in-the-Game Framework

We frame the moral decision making process as an implicit optimization (a.k.a. mesa-optimization (Hubinger et al., 2019)) of various aggregate welfare functions consisting of individual stakeholder utilities. These should reflect the impact of the scenarios stemming from the world setting and the decision we make. In order to guide our prompting design process, we first formulate the problem and present our prompts together with their motivations as to how they fit to the problem setup.

We denote the overall decision process by $F^{p}$ : $\mathcal{Q} \rightarrow \mathcal{A}$, where $\mathcal{Q}$ is the query space, $\mathcal{A}$ is the action space and $p$ is the prompting system. The decision made by $F^{p}$ for a query $q \in \mathcal{Q}$ is found by the following optimization:

$$
\begin{equation*}
F^{p}(q)=\underset{a \in \mathcal{A}}{\arg \max } \mathbb{E}_{x \sim h_{S}^{p}(q, a)} \operatorname{Agg}_{q}^{p}\left(\mathbf{h}_{\mathbf{u}}^{p}(x)\right) \tag{1}
\end{equation*}
$$

where $h_{S}^{p}: \mathcal{Q} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{X})$ is the counterfactual scenario generator reasoning about possible scenarios given a query and a decision prompted by $p, \mathcal{P}(\mathcal{X})$ is all the probability distributions on the scenario space $\mathcal{X}, \operatorname{Agg}_{q}^{p}: \mathbb{R}^{n_{q}} \rightarrow \mathbb{R}$ represents the aggregation mechanism that takes in the individual stakeholder utilities for a particular scenario and returns the overall utility we would like to maximize, $\mathbf{h}_{\mathbf{u}}^{p}=\left(h_{u_{1}^{q}}^{p}(x), \ldots, h_{u_{n q}^{q}}^{p}(x)\right)$ is the collection of $n_{q}$ stakeholders involved pertaining to the situation/query $q$ with $h_{u_{k}^{q}}^{p}$ being the individual utility function for stakeholder $k \in 1, \ldots, n_{q}$. Note that in this mesa-optimization (1), all the components $h_{S}^{p}, \mathbf{h}_{\mathbf{u}}^{p}, \mathrm{Agg}_{q}^{p}$ that influence $F^{p}$ explicitly depend on prompting strategy $p$, the main focus of our study. Indeed, we expect to see considerable differences between various LLMs in terms of their capability to be aligned to these essential ingredients by the guidance of the prompts.

Scenario generator $h_{S}^{p}(q, a)$. Given query $q$ and action $a$, the model should have enough information to contemplate the probable future unfolding events. Prompting LLMs to consider numerous possible continuations serves as a meaningful tool in decision-making due to its ability to obtain a broader depiction of the decision space (Long, 2023; Yao et al., 2023; Sel et al., 2023). In addition, since we can only sample limited number of times, it is imperative that the prompts should lead to a thorough coverage to ensure the reliable representation of consequences of its decisions.
Aggregation $\mathrm{Agg}^{p}$ After considering various stakeholder outcomes for a particular scenario, reflecting on the overall community benefit or harm is requisite. For instance, we may want to maximize the outcome of the worst-off stakeholder as in the Veil of Ignorance [(Rawls, 1971)], or use the Nash bargaining solution to simulate negotiation between non-cooperative agents [(Nash, 1953; Thomson, 1994)].

Scenario evaluator $\mathbf{h}_{\mathbf{u}}^{p}$. LLMs can embody individuals (Binz and Schulz, 2023; Argyle et al., 2023), political ideologies (Simmons, 2022; Jiang et al., 2022) or justice system (Cui et al., 2023). This is the starting point for the LLM to "put skin in the game" by depicting the interests of the stakeholders from their viewpoints. For instance, as discussed in (Taleb and Sandis, 2014), it could be the long and short-term monetary gain of the investors. Similarly, for a digital assistant, it involves alignment with the diverse user priorities such as helpfulness, harmlessness and honesty (Bai et al., 2022b). This positions the LLM not just as a tool, but as an active participant in addressing the inclusive needs of various stakeholders.

### 3.2 Generalization Guarantees

A core aspect of evaluating our SKIG framework is analyzing how well it generalizes-that is, how accurately can an LLM represent the true underlying scenario distributions and corresponding stakeholder utilities given a particular decision query. In this section, we aim to theoretically examine two key dimensions that control generalization performance: 1) the LLM's intrinsic capability to accurately model complex scenario distributions, and 2) the number of scenario simulations sampled when estimating expected outcomes. Understanding performance as a function of these factors provides insights into trade-offs in prompt design. More capable LLMs can produce reliable decisions with fewer samples, reducing computation costs. However, improved prompting strategies can also enhance generalization in weaker models.

To isolate the effects of the LLM's ability to model scenarios $h_{S}^{p}(q, a)$ and the number of simulations, we assume that the scoring is consistent, i.e. $\operatorname{Agg}_{q}^{p}\left(\mathbf{h}_{\mathbf{u}}^{p}(x)\right)$ represent the true utility $G^{p}(x)$ we want to optimize by the prompt $p$. We believe this not to be a strong assumption, since if the scenarios are detailed enough, the tasks of Aggregation and the scenario evaluator will be relatively easy.

Theorem 3.1. Assume that $\operatorname{Agg}_{q}^{p}\left(\mathbf{h}_{\mathbf{u}}^{p}(x)\right)$ is consistent. Let $X_{1}^{q, a}, \ldots, X_{n}^{q, a}$ be the i.i.d. samples from the distribution $h_{S}^{p}(q, a)$ given query $q$ and decision a. Define the total variation between two distributions as $D_{\mathrm{TV}}\left(Z_{1} \| Z_{2}\right):=\sup _{A \subseteq \mathcal{Z}} \mid Z_{1}(A)-$ $Z_{2}(A) \mid$. Then, we have

$$
\begin{gather*}
P\left(\left|\mathbb{E}_{x \sim X, a} G^{p}(x)-\mathbb{E}\left[\frac{1}{n} \sum_{i \in[n]} G^{p}\left(X_{i}^{q, a}\right)\right]\right| \geq\right.  \tag{2}\\
\left.\|G\|_{\infty} D_{\mathrm{TV}}\left[X^{q, a} \| h_{S}^{p}(q, a)\right]+t\right) \leq \frac{\sigma^{2}}{n t^{2}}
\end{gather*}
$$

for any query $q \in \mathcal{Q}$, any decision $a \in \mathcal{A}$ and $t \in \mathbb{R}^{+}$.

Theorem 3.1 shows that as we use more scenarios together with a more capable LLM that can represent the true scenario distribution more accurately, the performance discrepancy will be decrease as given in (2).

### 3.3 Prompting Mechanism

We provide the steps in our framework together with their relation to scenario generation, aggregation, scenario evaluation.

1. Stakeholder Identification: Firstly, to determine all the potential stakeholders in the situation, we supply the following prompt "For each scenario, identify the different stakeholders including the main character. Imagine you are the main character".
2. Motivation Analysis: We request to discern the motives behind the actions of the main character for facilitating a reliable scenario generation and alignment with societal norms. The model is prompted by "What are the motivations for the main character's actions in each of the scenarios, and are the motivations as per acceptable societal norms?".
3. Consequence Exploration: This step performs scenario generation by considering all the possible consequences for the stakeholders. We instruct the model with, "Imagine all possible consequences of the main character's actions on each of the stakeholders in the scenarios."
4. Empathy Exercise: We simulate accountability by prompting the model to envision itself as each stakeholder, representing scenario evaluation component of our framework. We extend the prompt with, "Emulate yourself as each of the stakeholders, including the main character, for each stakeholder and scenario. Identify the degree of impact of the main character's action on you."
5. Risk Assessment: Informed decision-making is enhanced by aggregation of the spectrum of outcomes to reason overall benefit/harm. We prompt the model as follows: "What is the absolute best-case and worst-case consequence that could result from the main character's actions in each scenario, and how likely is it to happen?"
6. Outcome Summary: We aim to distill key insights before arriving at a final decision. We prompt the model with, "Considering the different consequences and their likelihood of happening, summarize the outcomes of the main character's actions in each scenario."

The model assesses the morality of the main character's actions and makes a definitive choice, drawing on the observed outcomes.

## 4 Experiments

We demonstrate that the Skin-in-the-Game framework outperforms moral reasoning on various baselines standard prompting, zero-shot CoT (Wei et al., 2023) and the state-of-the-art Thought Experiment (TE) (Ma et al., 2023), across benchmarks MMLU Moral Scenarios (Hendrycks et al., 2021), Moral Stories (Emelin et al., 2021), ETHICS Commonsense Morality (Hendrycks et al., 2023) and Social Chemistry 101 (Forbes et al., 2020). This is observed for proprietary models TEXT-ADA, TEXTBABBAGE, TEXT-CURIE, TEXT-DAVINCI (Brown et al., 2020), GPT-3.5 Turbo and GPT-4 (OpenAI et al., 2023), as well as the open-source, instruction-finetuned MISTRAL-7B model (Jiang et al., 2023) with 7 billion parameters. The parameter count of other models are unknown. We used a single H100 with 80 GB VRAM to conduct our experiments with local LLMs for less than 10 hours.

Error Baselines We perform error analysis and categorize errors into bins representing their root causes: pessimism bias, assumption bias, and binary bias. Pessimism bias stems from excessive

| Method | GPT-3.5 TURBO | GPT-4 | ADA | BABBAGE | CURIE | DAVINCI | MISTRAL-7B |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| I/O | $42 \%$ | $78 \%$ | $23 \%$ | $25 \%$ | $19 \%$ | $38 \%$ | $38 \%$ |
| CoT | $52 \%$ | $80 \%$ | $23 \%$ | $21 \%$ | $21 \%$ | $39 \%$ | $37 \%$ |
| TE | $54 \%$ | $60 \%$ | $21 \%$ | $20 \%$ | $\mathbf{2 8 \%}$ | $35 \%$ | $50 \%$ |
| SKIG | $\mathbf{7 1 \%}$ | $\mathbf{8 6 \%}$ | $\mathbf{2 4 \%}$ | $\mathbf{2 7 \%}$ | $26 \%$ | $\mathbf{5 1 \%}$ | $\mathbf{5 8 \%}$ |

Table 1: Accuracy of the prompting baselines and SKIG in the MMLU Moral Scenarios task with various LLMs.

caution when the model overestimates the likelihood of negative outcomes. Assumption bias arises when the model makes decisions based on unsupported assumptions. Binary bias occurs when the model defaults to binary judgments for moral gray areas. We instruct human annotators to classify errors into the above bias categories. We additionally evaluate improvements using the error correction rate (Patel et al., 2022) and compositionality gap (Press et al., 2022) metrics to assess the performance of our method compared to other baselines.

### 4.1 MMLU Moral Scenarios

MMLU (Hendrycks et al., 2021) is an extensively monitored benchmark for state-of-the-art large language models (Chung et al., 2022; Touvron et al., 2023; Anil et al., 2023). Our experiments focus on the Moral Scenarios sub-task within the MMLU benchmark which is particularly challenging, with a considerable scope for improvement (Brown et al., 2020). The sub-task contains questions designed to evaluate a model's grasp of normative statements across a range of everyday scenarios.

Task Setup. In this task, the model is presented with two unrelated situations that have different context and main character. The model is required to select the most appropriate option from four presented choices, regarding the morality of the actions of the main character in each of the situations. ${ }^{1}$

Results. SKIG significantly outperforms I/O, CoT and TE across different large language models. Our method shows consistent accuracy improvements ranging from $+16 \%$ to $+70 \%$. Zero-shot CoT methods effective in mathematical reasoning (Wei et al., 2023), struggle to generalize to the intricate domain of moral reasoning, exhibiting lower accuracy than I/O prompting in GPT-4, as observed in Ma et al. (2023). Probing the decision space[^1]

with exploratory questions by scenario generation enables SKIG and TE to outperform CoT, which only uses information available in the query.

Ablation Analysis. The incremental integration of different SKIG components consistently improved accuracy, with empathy exercise and risk assessment providing the most substantial improvements. These components show an uptick of $+15 \%$ and $+6 \%$ in accuracy upon integration in GPT3.5-TURBO and DAVINCI models, while similar trends but smaller magnitudes of improvements are observed in MISTRAL-7B due to the smaller overall improvement in the latter model. Outcome summary component is of least importance in this benchmark, the pair of situations presented in the question are completely unrelated to each other.

Error Analysis The major portion of errors in SKIG can be attributed to pessimism bias followed by assumption bias. Risk-averse choices and preconceptions are common in language models, however, SKIG is able to reduce the error levels significantly in comparison to baselines. The Compositionality Gap reduces significantly for SKIG in comparison to TE despite it having more subquestions than the latter. SKIG improves the error in TE by $54 \%$ and introduces errors in it by $22 \%$ which can be attributed to the error categories identified above.

| Error Type | Error |
| :--- | ---: |
| Assumption Bias | $28 \%$ |
| Pessimism Bias | $31 \%$ |
| Binary Bias | $20 \%$ |
| Others | $21 \%$ |
| Method | Comp. Gap |
| CoT | $81 \%$ |
| TE | $91 \%$ |
| SKIG | $21 \%$ |

Table 2: MMLU Moral Scenarios Error Analysis

![](https://cdn.mathpix.com/cropped/2024_05_26_46412929a8fd4e82c72eg-06.jpg?height=491&width=1600&top_left_y=237&top_left_x=228)

Figure 2: Ablation Analysis on MMLU Moral Scenarios, Moral Stories and ETHICS Commonsense Morality datasets comparing the improvement in accuracy resulting from each of the components in SKIG framework.

| Method | GPT-3.5 TURbO | GPT-4 | ADA | BAbBaGe | CURIE | DAVINCI | MISTRAL-7B |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| I/O | $86 \%$ | $84 \%$ | $48 \%$ | $46 \%$ | $51 \%$ | $82 \%$ | $60 \%$ |
| CoT | $88 \%$ | $88 \%$ | $39 \%$ | $\mathbf{5 2 \%}$ | $\mathbf{5 3 \%}$ | $79 \%$ | $54 \%$ |
| TE | $89 \%$ | $91 \%$ | $36 \%$ | $49 \%$ | $50 \%$ | $85 \%$ | $81 \%$ |
| SKIG | $\mathbf{9 4 \%}$ | $\mathbf{9 6 \%}$ | $\mathbf{4 8 \%}$ | $50 \%$ | $51 \%$ | $\mathbf{9 1 \%}$ | $\mathbf{8 5 \%}$ |

Table 3: Accuracy of the prompting baselines and SKIG in the Moral Stories task with various LLMs.

Key Insight SKIG is able to reason through multiple steps independently for unrelated scenarios with reduced compositionality gap than baselines.

### 4.2 Moral Stories

Moral Stories is a crowd-sourced dataset which contains stories with various descriptors of a situation and a main character's actions to evaluate normative moral reasoning of language models in social situations (Emelin et al., 2021). The intention and norm samples describe the context of a social situation with normative actions and divergent actions representing conventional and unconventional social behavior respectively.

Task Setup. In this task, the model is presented with two situations with the same context that represent broadly endorsed and generally disapproved social behavior. The situations are morally ambiguous and lack a clear delineation between right and wrong. The model is required to choose from two answer choices regarding the morality of the situations. ${ }^{2}$

Results. The results on the Moral Stories benchmark follow similar trends to our previous findings,[^2]

with SKIG exhibiting higher accuracy levels than all the baselines across language models. The improvements are most pronounced in MISTRAL-7B which sees an improvement of $+40 \%$, followed by GPT-3.5-TURBO and GPT-4 with improvement of around $10 \%$ over standard prompting method. SKIG outperforms TE mainly because the benchmark contains morally nuanced that depend on context-based detailed analysis to arrive at a decisive conclusion.

Ablation Analysis. Experiments highlight the critical roles of the empathy exercise and risk assessment components within our framework, following trends observed in other benchmarks. Risk assessment proves especially critical for this benchmark, as judicious evaluation of worst-case and best-case consequences helps circumvent reasoning errors commonly observed in TE. Such errors stem from TE's inability to disambiguate superficially moral situations from truly immoral scenarios. Additionally, the morality evaluation component shows pronounced effects on this dataset. Consolidating prior insights and focused analysis of a situation's morality reveals subtle but significant ethical distinctions overlooked by other methods.

Error Analysis. Binary bias is a predominant source of mistakes in TE and SKIG under moral ambiguity. However, SKIG demonstrates superior error correction by mitigating over $80 \%$ of errors
present in TE, with $30 \%$ error correction for binarybias based mistakes. Given this benchmark's emphasis on ambiguity, assumption bias proves more prevalent than pessimism bias. SKIG demonstrates significantly lowered compositionality gaps across all baselines.

Key Insight SKIG's integration of calibrated risk analysis and morality probing enables better reasoning on morally ambiguous situations by reducing binary bias.

| Error Type | Error |
| :--- | ---: |
| Assumption Bias | $23 \%$ |
| Pessimism Bias | $12 \%$ |
| Binary Bias | $54 \%$ |
| Others | $11 \%$ |
| Method | Comp. Gap |
| I/O | $93 \%$ |
| TE | $91 \%$ |
| SKIG | $45 \%$ |

Table 4: Moral Stories Error Analysis

### 4.3 ETHICS Commonsense Morality

The ETHICS benchmark is widely used to evaluate a language model's knowledge of concepts in justice, well-being, duties, virtues and commonsense morality. Language models experience difficulty in predicting basic human ethical judgements (Hendrycks et al., 2023) and to improve this, we have chosen the Commonsense Morality sub-task for our experiments.

Task Setup. In this task, the model is presented with two situations that share the same context but are clearly different in terms of the morality of the main-character's actions. The model is required to select the most appropriate option from two presented choices regarding the morality of the situations. $^{3}$

Results. The commonsense morality task contains relatively unambiguous scenarios with actions by the main character that clearly delineate moral versus immoral behavior. Due to this nature of the benchmark, higer-order models demonstrate good performance even with standard prompting, with[^3]

slight improvements resulting from SKIG. Lowerorder and open-source language models showcase SKIG's effectiveness at enhancing task accuracy. Especially, MistRaL-7B exhibits a substantial performance boost under SKIG, increasing accuracy by $+40 \%$ in comparison to standard prompting and around $+10 \%$ in comparison to TE. Even ADA shows better than random-choice performance with SKIG. These results validate SKIG's efficacy in aiding models to discern morality and immorality of actions, especially for models that struggle on clearly delineated scenarios.

Ablation Analysis. The ablation study corroborates the vital roles of the empathy exercise and risk assessment components in boosting accuracy, aligning with trends across benchmarks. The empathy exercise proves especially critical for this dataset, where scenarios solely differ based on the protagonist's actions and resulting stakeholder impact. Meanwhile, the risk assessment and morality evaluation components demonstrate smaller impacts versus other benchmarks given this dataset's morally unambiguous examples. With clear-cut ethical judgements, these components contribute less to the overall evaluation outcome.

Key Insight SKIG enables lower-order language models with lower proficiency even on morally unambiguous commonsense questions to achieve accuracy on par with higher order LLMs.

## 5 Discussion

In this section, we perform a critical analysis of our framework, using the MMLU Moral Scenarios bencmark as the primary case study. ${ }^{4}$

## How accurate is the stakeholder identification in

 SKIG? We use the Social Chemistry 101 dataset to assess stakeholder identification - a crucial step for multi-stakeholder alignment. Employing fewshot learning, we prompt a GPT-4 "Judge" model with multiple choice questions to evaluate SKIG stakeholder identification versus Social Chemistry annotations. SKIG was correctly able to identify all the primary stakeholders and additional secondary stakeholders with more than $90 \%$ accuracy across LLMs.[^4]| Method | GPT-3.5 TURBO | GPT-4 | ADA | BABBAGE | CURIE | DAVINCI | MISTRAL-7B |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| I/O | $81 \%$ | $97 \%$ | $45 \%$ | $46 \%$ | $50 \%$ | $82 \%$ | $66 \%$ |
| CoT | $92 \%$ | $94 \%$ | $49 \%$ | $48 \%$ | $\mathbf{5 2 \%}$ | $75 \%$ | $67 \%$ |
| TE | $96 \%$ | $95 \%$ | $41 \%$ | $\mathbf{5 3 \%}$ | $45 \%$ | $85 \%$ | $89 \%$ |
| SKIG | $\mathbf{9 6 \%}$ | $\mathbf{9 9 \%}$ | $\mathbf{5 6 \%}$ | $51 \%$ | $50 \%$ | $\mathbf{8 7 \%}$ | $\mathbf{9 4 \%}$ |

Table 5: Accuracy of the prompting baselines and SKIG in the ETHICS Commonsense Morality benchmark with various LLMs.

## Does SKIG generate consistent reasoning paths?

Candidate reasoning paths generated at hightemperature setting for identical questions are presented to a GPT-4 "Judge" model to evaluate consistency. We observe high component-wise and overall consistency across different sample rationales. The Empathy Exercise component shows high consistency of $93 \%$, closely followed by the Risk Assessment and Outcome Summary components, which show consistency rates of $92 \%$ and $91 \%$, respectively. Strong consistency rates within a tight range for all components emphasizes reliable performance of SKIG.

How robust is SKIG reasoning to different prompts? We conducted an ablation study to assess the robustness of our methodology to variations in linguistic expression. We evaluated ten additional prompt sets with altered lexical choices and syntax versions of the standard prompt. We observe an average accuracy of $70.5 \%$ on all the runs. The results show consistent and similar accuracy levels for all the prompt variants. The efficacy of our method lies predominantly in the underlying strategy itself rather than specific prompt wording.

Does conditioning SKIG for optimism/pessimism during risk-assessment improve/degrade performance? Our analysis of different risk assessment objectives reveals higher accuracy with best-case-only versus worstcase-only goals, both at the aggregated overall level and at individual stakeholder-level. This is due to higher error-correction rates for pessimism bias in best-case. The risk-assessment component has stakeholder level insights from empathy exercise as context, making risk-assessment at overall-level more favorable for the reasoning process. $A$ balanced assessment weighing both best-case and worst-case objectives across stakeholders proves conducive for nuanced risk analysis.

| Risk Objective | Accuracy |
| :--- | ---: |
| Best-case only (Overall) | $65 \%$ |
| Worst-case only (Overall) | $62 \%$ |
| Best-case only (Stakeholder) | $60 \%$ |
| Worst-case only (Stakeholder) | $59 \%$ |
| Best-case + Worst-case (Overall) | $\mathbf{7 1 \%}$ |

Table 6: Risk Assessment for different objectives at Overall level and per-Stakeholder level.

Does this method necessitate a multi-turn framework or can a single-turn approach suffice? To understand the impact of multi-turn reasoning, we test variants of SKIG. We observe that single-turn variants performed poorly, with accuracy levels below standard prompting levels at $20 \%$ and $22 \%$ for all sub-questions in single-turn (ST-All) and best performing sub-questions in a single-turn (STBest) respectively. Multi-turn variants with shorter reasoning paths resulted in improved accuracy of $59 \%$ for best performing sub-questions in multiturn (MT-Best) than single-turn variants, baselines, but were not able to match SKIG (MT-All) accuracy levels. A shorter reasoning path might be chosen when some reduction in accuracy levels are acceptable for multi-stakeholder alignment.

How does the number of scenario samples affect performance? We prompt the LLMs to consider some of the possible scenarios instead of all in the Consequence Exploration step in SKIG. We see consistent performance drops of $8 \%$ with GPT3.5-Turbo. This is also motivated by Theorem 3.1, showing the significance of good coverage of the consequences of the actions.

## 6 Conclusion

We introduced the Skin-in-the-Game (SKIG) framework, significantly enhancing LLMs' moral reasoning by simulating accountability and evaluating impacts from multiple perspectives, particularly emphasizing multi-stakeholder alignment in the decision-making process. Key components
like empathy exercise and risk assessment reduce common biases, leading to more ethically sound outcomes. Our results demonstrate SKIG's superiority, surpassing previous methods across various benchmarks and LLMs, and marking a substantial improvement in ethical decision-making.

## 7 Limitations

The proposed method has been extensively studied for moral reasoning. The extension of reasoning using SKIG in domains like negotiation which require multi-stakeholder alignment are yet to be studied. Also, the reasoning path could generate harmful responses for scenarios rarely, strategies to address such responses need to be improved.

## References

Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298-306.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.

Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate. 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337-351.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, J Landau, Kamal Ndousse, Kamilè Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem'i Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, T. J. Henighan, Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B. Brown, and Jared Kaplan. 2022c. Constitutional ai: Harmlessness from ai feedback. ArXiv, abs/2212.08073.

Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. 2022. Fine-tuning language models to find agreement among humans
with diverse preferences. Advances in Neural Information Processing Systems, 35:38176-38189.

Yejin Bang, Nayeon Lee, Tiezheng Yu, Leila Khalatbari, Yan Xu, Samuel Cahyawijaya, Dan Su, Bryan Wilie, Romain Barraud, Elham J. Barezi, Andrea Madotto, Hayden Kee, and Pascale Fung. 2022. Towards answering open-ended ethical quandary questions.

Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610-623.

Marcel Binz and Eric Schulz. 2023. Turning large language models into cognitive models. arXiv preprint arXiv:2306.03917.

Shikha Bordia and Samuel R Bowman. 2019. Identifying and reducing gender bias in word-level language models. arXiv preprint arXiv:1904.03035.

Mark Bovens. 2014. Two concepts of accountability: Accountability as a virtue and as a mechanism. In Accountability and European governance, pages 18 39. Routledge.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.

Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. Chatlaw: Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv:2306.16092.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Burak Dogruyol, Sinan Alper, and Onurcan Yilmaz. 2019. The five-factor model of the moral foundations theory is stable across weird and non-weird cultures. Personality and Individual Differences, 151:109547.

Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 698-718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, et al. 2021. An ethical framework for a good ai society: Opportunities, risks, principles, and recommendations. Ethics, governance, and policies in artificial intelligence, pages 19-39.

Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653-670, Online. Association for Computational Linguistics.

Kathleen C. Fraser, Svetlana Kiritchenko, and Esma Balkir. 2022. Does moral code have a moral code? probing delphi's moral philosophy. ArXiv, abs/2205.12771.

Iason Gabriel. 2020. Artificial intelligence, values, and alignment. Minds and machines, 30(3):411-437.

Deep Ganguli, Liane Lovitt, John Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Benjamin Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zachary Dodds, T. J. Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli TranJohnson, Dario Amodei, Tom B. Brown, Nicholas Joseph, Sam McCandlish, Christopher Olah, Jared Kaplan, and Jack Clark. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv, abs/2209.07858.

Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, and ShouDe Lin. 2023. Towards optimizing with large language models. arXiv preprint arXiv:2310.05204.

J. Haidt. 2013. The Righteous Mind: Why Good People Are Divided by Politics and Religion. Knopf Doubleday Publishing Group.

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275.

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2023. Aligning ai with shared human values.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding.

Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.

Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019. Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.

Hang Jiang, Doug Beeferman, Brandon Roy, and Deb Roy. 2022. Communitylm: Probing partisan worldviews from language models. arXiv preprint arXiv:2209.07065.

Ming Jin, Bilgehan Sel, Fnu Hardeep, and Wotao Yin. 2023. A human-on-the-loop optimization autoformalism approach for sustainability. ArXiv, abs/2308.10380.

Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. 2023. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, $103: 102274$.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.

Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel DiazCandido, James Maningo, et al. 2023. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198.

Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. 2023. Large language models for supply chain optimization. arXiv preprint arXiv:2307.03875.

Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi. 2023. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960.

Jieyi Long. 2023. Large language model guided tree-ofthought. arXiv preprint arXiv:2305.08291.
Xiao Ma, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. Let's do a thought experiment: Using counterfactuals to improve moral reasoning.

John Nash. 1953. Two-person cooperative games. Econometrica: Journal of the Econometric Society, pages 128-140.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.

OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob

Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Alexander Pan, Chan Jun Shern, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. 2023. Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. In International Conference on Machine Learning.

Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Is a question decomposition unit all we need? arXiv preprint arXiv:2205.12538.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.

John Rawls. 1971. Atheory of justice. Cambridge (Mass.).

Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, and Ming Jin. 2023. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379.

Gabriel Simmons. 2022. Moral mimicry: Large language models produce moral rationalizations tailored to political identity. arXiv preprint arXiv:2209.12106.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.

Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561.

Nassim N Taleb and Constantine Sandis. 2014. The skin in the game heuristic for protection against tail events. Review of Behavioral Economics, 1(1-2):115-135.

Nassim Nicholas Taleb and Constantine Sandis. 2013. The skin in the game heuristic for protection against tail events.

William Thomson. 1994. Cooperative models of bargaining. Handbook of game theory with economic applications, 2:1237-1284.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.

Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John F. J. Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sande Minnich Brown, Zachary Kenton, William T. Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William S. Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2022. Taxonomy of risks posed by language models. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.

Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. 2023. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.

Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.
