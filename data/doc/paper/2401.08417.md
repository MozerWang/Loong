# Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation 

Haoran $\mathrm{Xu}^{\star}$ Amr Sharaf ${ }^{\ominus}$ Yunmo Chen ${ }^{\star}$ Weiting Tan ${ }^{\star}$ Lingfeng Shen ${ }^{\star}$ Benjamin Van Durme ${ }^{\star}$<br>Kenton Murray* ${ }^{*}$ Young Jin Kim* ${ }^{\circ}$


#### Abstract

Moderate-sized large language models (LLMs) - those with 7B or 13B parameters - exhibit promising machine translation (MT) performance. However, they do not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4 (OpenAI, 2023). In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to supervised fine-tuning which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA (Xu et al., 2023) models with only $22 \mathrm{~K}$ parallel sentences and tuning only $0.1 \%$ parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.


## 1. Introduction

Machine translation (MT) predominantly utilizes transformer encoder-decoder architectures (Vaswani et al., 2017), which is evident in prominent models such as NLLB-200 (NLLB TEAM et al., 2022), M2M100 (Fan et al., 2021), BiBERT (Xu et al., 2021), and MT5 (Xue et al., 2021). However, the emergence of decoder-only large language models (LLMs) such as the GPT series (Brown et al., 2020; OpenAI,[^0]

2023), Mistral (Jiang et al., 2023), LLaMA series (Touvron et al., 2023a;b), Falcon (Almazrouei et al., 2023), inter alia, which have shown remarkable efficacy in various NLP tasks, which attracts the interest of developing machine translation with these decoder-only LLMs. Recent studies (Zhu et al., 2023a; Jiao et al., 2023b; Hendy et al., 2023; Kocmi et al., 2023; Freitag et al., 2023) indicate that larger LLMs such as GPT-3.5 (175B) and GPT-4 exhibit strong translation abilities. However, the performance of smaller-sized LLMs (7B or 13B) still falls short when compared to conventional translation models (Zhu et al., 2023a).

Therefore, there are studies intend to enhance the translation performance for these smaller LLMs (Yang et al., 2023; Zeng et al., 2023; Chen et al., 2023; Zhu et al., 2023b; Li et al., 2023; Jiao et al., 2023a; Zhang et al., 2023), but their improvements are relatively modest, primarily due to the predominant pre-training of LLMs on English-centric datasets, resulting in limited linguistic diversity (Xu et al., 2023). Addressing this limitation, Xu et al. (2023) initially fine-tune LLaMA-2 (Touvron et al., 2023b) with extensive non-English monolingual data to enhance their multilingual abilities, and then perform supervised fine-tune (SFT) with high-quality parallel data to instruct the model to generate translations. Their model, named ALMA, outperforms all prior moderated-size LLMs, and even larger models such as GPT-3.5, in the translation task. Nonetheless, the performance still lags behind leading translation models such as GPT-4 and WMT competition winners. Our study bridges this gap by further fine-tuning ALMA models with our novel training method Contrastive Preference Optimization (CPO) and minimal costs, i.e., only $12 \mathrm{M}$ learnable parameters (equivalent to $0.1 \%$ of the original model size) and a $22 \mathrm{~K}$ dataset for 10 directions. The fine-tuned model is referred to as ALMA-R. A detailed performance comparison is illustrated in Figure 1.

CPO aims to mitigate two fundamental shortcomings of SFT. First, SFT's methodology of minimizing the discrepancy between predicted outputs and gold-standard references inherently caps model performance at the quality level of the training data. This limitation is significant, as even humanwritten data, traditionally considered high-quality, is not immune to quality issues (more details in Section 2). For in-

![](https://cdn.mathpix.com/cropped/2024_06_04_bbbd0a340fa9072f51d2g-02.jpg?height=613&width=808&top_left_y=230&top_left_x=192)

Figure 1. A performance comparison featuring our proposed model ALMA-13B-R against other recently released 13B LLMbased models, as well as top-performing translation systems like GPT-4 and WMT winners. This evaluation covers the WMT'22 test data across 8 directions, involving translations to and from English for German, Czech, Chinese, and Russian. Scores are averaged by three different reference-free models: wmt23-cometkiwi-da-xxl, XCOMET-XXL, and wmt22-cometkiwi-da, and are also averaged across all directions. The gold reference is also evaluated due to the reference-free approach. Our model, ALMA-13B-R, developed by further training ALMA-13B-LoRA using our proposed CPO method, either matches or surpasses the most advanced translation models. We show the detailed numerical data for all systems presented in the figure in Appendix A.

stance, one may notice that some strong translation models are capable of producing translations superior to the gold reference, as illustrated in Figure 1. Secondly, SFT lacks a mechanism to prevent the model from rejecting mistakes in translations. While strong translation models can produce high-quality translations, they occasionally exhibit minor errors, such as omitting parts of the translation. Preventing the production of these near-perfect but ultimately flawed translations is essential. To overcome these issues, we introduce Contrastive Preference Optimization (CPO) to train the ALMA model using specially curated preference data. After CPO training, the ALMA-R model shows marked improvements, achieving performance levels that match or even surpass those of GPT-4 and WMT competition winners.

Our main contributions are summarized as follows:

Are reference Gold or Gilded? We conducted an in-depth analysis of the training data (FLORES-200 data) utilized by the ALMA model. We meticulously compared the quality of the reference translations with those generated by strong translation models. Our findings reveal that, in numerous instances, the quality of human-written parallel data is even inferior to that of system-generated translations. This observation underscores a critical insight: training models exclu- sively towards replicating reference translations may not be the most effective approach, and reliance on reference-based evaluation could be flawed.

Pushing the Performance Boundary of SFT We introduce Contrastive Preference Optimization, which offers advantages in terms of memory efficiency, speed, and, crucially, enhanced effectiveness in improving translation quality. CPO breaks the performance bottleneck inherent in SFT's reference-mimicking learning process and pushes the performance boundary of models that have reached saturation through SFT training. ${ }^{1}$

Preference Data We build and release a high-quality preference dataset for the machine translation area.

## 2. Gold or Gilded? Scrutinizing Gold Reference Quality

The significance of target references is paramount in machine translation tasks. The paradigm of training models on the machine translation task heavily relies on the quality of the references since the model is commonly optimized using a loss that is defined to minimize the difference between the predicted outputs and gold reference. Consider a dataset $\mathcal{D}$, comprising pairs of source sentences $x$ and their corresponding target sentences (gold references) $y$, represented as $\mathcal{D}=\left\{x^{(i)}, y^{(i)}\right\}_{i=1}^{N}$, where $N$ is the total number of parallel sentences. The negative log-likelihood loss for these parallel sentences, in relation to a model $\pi_{\theta}$ parameterized by $\theta$, is defined as follows:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{NLL}}=-\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\log \pi_{\theta}(y \mid x)\right] \tag{1}
\end{equation*}
$$

Hence, the ability of models to effectively translate is contingent upon the availability of high-quality translation pairs (Xu et al., 2023; Maillard et al., 2023). Furthermore, prevalent evaluation tools such as BLEU (Papineni et al., 2002) and COMET-22 (Rei et al., 2022) predominantly rely on reference-based metrics. However, the precision of these evaluations is sensitive to and compromised by substandard references (Kocmi et al., 2023; Freitag et al., 2023). Recent research (Xu et al., 2023; Kocmi et al., 2023; Freitag et al., 2023) has shifted attention towards assessing the quality of parallel datasets, indicating that target references may not consistently represent the highest quality. In Figure 2, we take a translation example from the FLORES-200 dataset, and compare the gold reference translation with outputs from the best ALMA model and GPT-4. This comparison reveals that the gold reference is a flawed translation, as it omits part of information, whereas the system-generated outputs demonstrate superior quality. This prompts an inquiry: Are references (even though human-written) truly[^1]equivalent to gold standards? To thoroughly assess the quality of both the gold standard references and the outputs from contemporary high-performance translation models, we propose evaluating these outputs utilizing reference-free evaluation frameworks.

```
Source: 这是马特利 (Martelly) 四年来第五次入选海地临时选举委员会 (CEP)。
Reference: It is Martelly's fifth CEP in four years.
ALMA-13B-LoRA: This is Martelly's fifth time being selected by the Provisional
Electoral Council (CEP) in four years.
GPT-4: This is the fifth time Martelly has been selected for Haiti's Provisional
Electoral Council (CEP) in four years.
```

Figure 2. An example demonstrating that a human-written gold reference may not always be flawless, and could be surpassed by translations from advanced translation models. In this case, the reference retains the abbreviation "CEP" but fails to provide its full name. The highlighted phrases in the model-generated translations indicate the portions omitted by the gold reference.

Models We scrutinize the translation outputs from ALMA13B-LoRA ${ }^{2}$, as well as zero-shot translations from the most recent GPT-4 (gpt-4-1106-preview). To assess the quality of these outputs, we employ two of the latest and largest reference-free models, each with a 10B parameter size and demonstrating very high correlation with human judgements (Freitag et al., 2023). These models are Unbabel/wmt23-cometkiwi-da-xxl (henceforth referred to as KIWI-XXL) (Rei et al., 2023) and Unbabel / XCOMET-XXL (subsequently referred to as XCOMET) (Guerreiro et al., 2023).

Data we consider the high-quality and human-written FLORES-200 dataset (NLLB TEAM et al., 2022), comprising both development and test data, amounting to a total of 2009 samples for each language direction, to compare the gold references with the outputs generated by the models. We employed ALMA-13B-LoRA and GPT-4 to perform translations across five English-centric language pairs, covering both translations from and to English. These pairs include German (de), Czech (cs), Icelandic (is), Chinese (zh), and Russian (ru), with Icelandic (is) categorized as a low-resource language and the others as high-resource languages.

Prompt The prompt employed for generating translations with ALMA models is consistent with the one used in $\mathrm{Xu}$ et al. (2023). For GPT-4 translation generation, we follow the guidelines suggested by Hendy et al. (2023). The specifics of these prompts are detailed in Appendix B.[^2]

Table 1. A performance comparison between gold references and outputs from advanced translation models, as assessed by two 10B-size reference-free evaluation models with the highest correlation to human preferences. The results indicate that the average performance of these strong translation models can even exceed that of the gold references, achieving a high success rate in beating the reference.

|  | KIWI-XXL | Win Ratio (\%) | XCOMET | Win Ratio (\%) |
| :---: | :---: | :---: | :---: | :---: |
| Translating to English $(\mathrm{xx} \rightarrow \mathrm{en})$ |  |  |  |  |
| Reference | 85.31 | - | 88.82 | - |
| ALMA-13B-LoRA | 88.33 | 73.24 | 92.68 | 60.17 |
| GPT-4 | 89.21 | 79.43 | 94.66 | 54.25 |
| Translating from English $(\mathrm{en} \rightarrow \mathrm{xx})$ |  |  |  |  |
| Reference | 87.85 | - | 94.42 | - |
| ALMA-13B-LoRA | 85.62 | 42.15 | 93.07 | 35.46 |
| GPT-4 | 87.30 | 49.13 | 94.21 | 38.09 |

Model Outputs Can Be Better References In Table 1, we present the evaluation scores of KIWI-XXL and XCOMET for the gold references, ALMA-13B-LoRA outputs, and GPT-4 outputs. Additionally, we report Win Ratio, reflecting the proportion of instances where model outputs surpass the gold standard references. These metrics are calculated as an average across five languages. Remarkably, even comparing with the high-quality Flores-200 dataset, the average performance of translation models in $x x \rightarrow e n$ translations significantly exceeds that of the references, showing approximately 3-4 point increases in KIWI-XXL and 4-6 point gains in XCOMET. Notably, a significant proportion of outputs are rated higher than the references by KIWI-XXL (e.g., $\mathbf{7 3 . 2 4 \%}$ for ALMA), with a slightly reduced yet still substantial percentage when assessed using XCOMET ( $\mathbf{6 0 . 1 7 \%}$ for ALMA). In the en $\rightarrow \mathrm{xx}$ direction, while the overall performance between the translations from reference and two systems is comparable, approximately $40 \%$ are still deemed superior to the reference translations.

Motivation: Help The Model Learn Rejection The aforementioned findings illustrate that translations produced by advanced models can sometimes surpass the quality of gold standard references. This raises the question of how to effectively utilize such data. A straightforward approach would involve fine-tuning the model using the source and the superior translations as references. While this could enhance the model's translation abilities, it does not equip the model with the discernment to identify and avoid generating suboptimal translations, exemplified by the "good but not perfect" translations depicted in Figure 2. Consequently, this situation motivates us to develop a new training objective, which aims to instruct the model in prioritizing the generation of higher-quality translations and rejecting lesser ones, in a style of contrastive learning with hard negative examples (Oord et al., 2018; Chen et al., 2020; He et al., 2020; Robinson et al., 2021; Tan et al., 2023). This objective moves beyond the traditional focus on merely minimizing cross-entropy loss towards the reference.

## 3. Contrastive Preference Optimization

To learn an objective that fosters superior translations and rejects inferior ones, access to labeled preference data is essential, yet such data is scarce in machine translation. In this section, we first describe the construction of our preference data and then introduces a preference learning technique, contrastive preference optimization (CPO).

### 3.1. Triplet Preference Data

We here detail our methodology for constructing preference data $\mathcal{D}$. This dataset is developed using the FLORES-200 data (both development and test sets) and encompasses the same language pairs as discussed in Section 2. For each language pair, the dataset comprises 2009 parallel sentences.

For a given source sentence $x$, whether translated from or to English, we utilize both GPT-4 and ALMA-13B-LoRA to generate respective translations, denoted as $y_{\text {gpt-4 }}$ and $y_{\text {alma }}$. Together with the original target reference $y_{\text {ref }}$, this forms a triplet $\mathbf{y}=\left(y_{\text {ref }}, y_{\text {gpt-4 }}, y_{\text {alma }}\right)$, representing three different translation outputs for the input $x$. The referencefree evaluation models KIWI-XXL and XCOMET are then employed to score these translations, with the average scores represented as $\mathbf{s}=\left(s_{\text {ref }}, s_{\text {gpt-4 }}, s_{\text {alma }}\right) .{ }^{3}$ The highest-scoring translation is labeled as the preferred translation $y_{w}$, and the lowest-scoring as the dis-preferred translation $y_{l}$, i.e., $y_{w}=\mathbf{y}_{\arg \max _{i}(\mathbf{s})}, y_{l}=\mathbf{y}_{\arg \min _{i}(\mathbf{s})}$, where $i$ represents the index in the triplet. Translations with intermediate scores are not considered. An illustrative example of this selection process is depicted in Figure 3. It is important to note that even the dis-preferred translations may be of high-quality. The designation 'dis-preferred' indicates that there is still room for improvement, perhaps through the addition of minor details. This approach of using high-quality but not flawless translations as dis-preferred data aids in training the model to refine details and achieve perfection in generated translations.

### 3.2. Deriving the CPO Objective

We discuss the derivation of CPO objective, beginning with an analysis of Direct Preference Optimization (DPO) (Rafailov et al., 2023). DPO represents a more direct optimization objective utilized in reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022). Given a set of source sentences $x$, alongside preferred translation targets $y_{w}$ and less preferred ones $y_{l}$, we can access a static dataset of comparisons, denoted as $\mathcal{D}=\left\{x^{(i)}, y_{w}^{(i)}, y_{l}^{(i)}\right\}_{i=1}^{N}$. The loss function for DPO is constructed as a maximum likelihood objective for a param-[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_bbbd0a340fa9072f51d2g-04.jpg?height=426&width=792&top_left_y=218&top_left_x=1076)

Figure 3. A triplet of translations, either model-generated or derived from a reference, accompanied by their respective scores as assessed by reference-free models. For a given source sentence, the translation with the highest score is designated as the preferred translation, while the one with the lowest score is considered dispreferred, and the translation with a middle score is disregarded.

eterized policy $\pi_{\theta}$ :

$$
\begin{align*}
\mathcal{L}\left(\pi_{\theta} ; \pi_{\mathrm{ref}}\right)= & -\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\operatorname { l o g } \sigma \left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{w} \mid x\right)}\right.\right. \\
& \left.\left.-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{l} \mid x\right)}\right)\right] \tag{2}
\end{align*}
$$

where $\pi_{\text {ref }}$ is a pre-trained language (translation) model, $\sigma$ is the Sigmoid function, and $\beta$ is a hyperparameter. The DPO loss is derived a reparameterization process of the groundtruth reward and the corresponding optimal policy in the Proximal Policy Optimization (PPO) framework (Schulman et al., 2017). As a result, DPO training can be conducted in a supervised fine-tuning style, as it relies exclusively on labeled preference data and does not require interaction between agents and their environment.

However, DPO has notable drawbacks compared to common SFT. Firstly, DPO is memory-inefficient: it necessitates twice the memory capacity to simultaneously store both the parameterized policy and the reference policy. Secondly, it is speed-inefficient: executing the model sequentially for two policies doubles the processing time. To address these inefficiencies, we introduce contrastive preference optimization.

The memory- or speed- inefficiency can be resolved when $\pi_{\text {ref }}$ is set as a uniform prior $U$, as the terms $\pi_{\text {ref }}\left(y_{w} \mid x\right)$ and $\pi_{\text {ref }}\left(y_{l} \mid x\right)$ cancel each other out. This negates the need for additional computations and storage beyond the policy model itself. Thus, we initially demonstrate that the DPO loss can be effectively approximated using a uniform reference model:

$$
\begin{align*}
\mathcal{L}\left(\pi_{\theta} ; U\right)= & -\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\operatorname { l o g } \sigma \left(\beta \log \pi_{\theta}\left(y_{w} \mid x\right)\right.\right. \\
& \left.\left.-\beta \log \pi_{\theta}\left(y_{l} \mid x\right)\right)\right] \tag{3}
\end{align*}
$$

Specifically, we prove the below Theorem in Appendix C.

Theorem 1. When $\pi_{r e f}$ is defined as $\pi_{w}$, an ideal policy that precisely aligns with the true data distribution of preferred data, the DPO loss $\mathcal{L}\left(\pi_{\theta} ; \pi_{w}\right)+C$ is upper bounded by $\mathcal{L}\left(\pi_{\theta} ; U\right)$, where $C$ is a constant.

The approximation in Equation 3 is effective because it minimizes the upper boundary of the DPO loss. The proof relies on an important assumption of $\pi_{\text {ref }}=\pi_{w}$. Contrary to common practices where $\pi_{\text {ref }}$ is set as the initial SFT checkpoint, our approach considers it as the ideal policy we aim to reach. Although the ideal policy $\pi_{w}$ is unknown and unattainable during model training, it is not engaged in the loss after our approximation.

Furthermore, we incorporate a behavior cloning (BC) regularizer (Hejna et al., 2023) to ensure that $\pi_{\theta}$ does not deviate from the preferred data distribution:

$$
\begin{align*}
& \min _{\theta} \mathcal{L}\left(\pi_{\theta}, U\right) \\
& \text { s.t. } \mathbb{E}_{\left(x, y_{w}\right) \sim \mathcal{D}}\left[\mathbb{K} \mathbb{L}\left(\pi_{w}\left(y_{w} \mid x\right) \| \pi_{\theta}\left(y_{w} \mid x\right)\right)\right]<\epsilon \tag{4}
\end{align*}
$$

where $\epsilon$ is a small positive constant and $\mathbb{K} \mathbb{L}$ is Kullback-Leibler (KL) divergence. The regularizer can boil down to adding a SFT term on the preferred data (a detailed explanation is provided in Appendix C):

$$
\begin{equation*}
\min _{\theta} \underbrace{\mathcal{L}\left(\pi_{\theta}, U\right)}_{\mathcal{L}_{\text {prefer }}} \underbrace{-\mathbb{E}_{\left(x, y_{w}\right) \sim \mathcal{D}}\left[\log \pi_{\theta}\left(y_{w} \mid x\right)\right]}_{\mathcal{L}_{\mathrm{NLL}}} \tag{5}
\end{equation*}
$$

The above is the formulation of our CPO loss, which includes one preference learning term $\mathcal{L}_{\text {prefer }}$ and one negative $\log$ likelihood term $\mathcal{L}_{\mathrm{NLL}}$.

## 4. Experiments

### 4.1. Data

Following Section 2, we consider 10 translation directions in the paper: cs $\leftrightarrow e n, d e \leftrightarrow e n$, is $\leftrightarrow e n, z h \leftrightarrow e n, r u \leftrightarrow e n$. Building on the ALMA models' (Xu et al., 2023) insights that a small quantity of high-quality data can yield impressive translation results, our training dataset is even more compact. As detailed in Section 3.1, our preference training data is derived from the FLORES-200 dataset, a subset of which has been also employed in the training of ALMA models. This results in a total of $2 \mathrm{~K} \times 10$ directions $=20 \mathrm{~K}$ paired sentences. We detail the provenance distribution for each language pair from ALMA-13B-LoRA, GPT4, and reference as presented in Table 2. In addition to preference data assessed by large evaluation models, our dataset incorporates $1 \mathrm{~K}$ internal human-labeled preference data, containing preferred and dis-preferred translations along with human preference. However, the human-labeled data is limited to just two translation directions: $\mathrm{en} \rightarrow \mathrm{zh}$ and
Table 2. The provenance distribution for each language pair in the preference data.

|  | ALMA-13B-LoRA | GPT-4 | Reference |
| :--- | :---: | :---: | :---: |
| en $\leftrightarrow$ de | $46 \%$ | $37 \%$ | $17 \%$ |
| en $\leftrightarrow$ cs | $32 \%$ | $41 \%$ | $27 \%$ |
| en↔is | $36 \%$ | $40 \%$ | $24 \%$ |
| en $\leftrightarrow$ zh | $45 \%$ | $35 \%$ | $20 \%$ |
| en $\leftrightarrow$ ru | $31 \%$ | $44 \%$ | $25 \%$ |

en $\rightarrow$ de. The details regarding the composition and influence of human-labeled data are explored in Appendix D. ${ }^{4}$ In alignment with $\mathrm{Xu}$ et al. (2023), our primary focus is on the test set drawn from WMT'21 for is and WMT'22 for other languages. Additionally, we conduct auxiliary experiments evaluating models on WMT'23, covering six directions: de $\leftrightarrow$ en, zh en, and ru $r$ en.

### 4.2. Training Setup

We train the model in a many-to-many multilingual machine translation manner, starting with ALMA-13B-LoRA as the initial checkpoint. During the training phase, we focus exclusively on updating the weights of the added LoRA parameters. These weights have a rank of 16 and only add an additional $12 \mathrm{M}$ parameters to the original 13B size of the model. We adhere to the default $\beta$ value of 0.1 as suggested by Rafailov et al. (2023). The fine-tuning process of ALMA13B-LoRA involves a batch size of 128 , a warm-up ratio of 0.01 , spanning a single epoch, and accommodating sequences with a maximum length of 512 tokens. To optimize training efficiency, we integrate the deepspeed tool (Rasley et al., 2020). We utilize the same prompt as Xu et al. (2023) and do not compute the loss for the prompt. While our primary focus is on the performance of 13B models, CPO markedly benefits 7B models as well. Consequently, we also release ALMA-7B-R and provide a detailed discussion of its performance in Appendix A.

### 4.3. Baselines

SoTA Models In this category, our benchmarks are established against, to the best of our knowledge, the strongest publicly available translation models. We first compare with ALMA-13B-LoRA, recognized as one of the top moderatesize language-model based translation systems, surpassing notable conventional models such as NLLB-54B in both WMT'21 and WMT'22. We also compare our results with TowerInstruct ${ }^{5}$, a recently released LLM-based translation[^4]

Table 3. The overall results in en $\rightarrow \mathrm{xx}$ for WNT'21 and WMT'22. The application of the CPO method to fine-tune the ALMA-13BLoRA model leads to a significant enhancement in performance, equalling or surpassing that of WMT competition winners and GPT-4. Bold numbers denote the highest scores across all systems. Dark blue boxes indicates that the improvement over the original ALMA model achieves at least $80 \%$ estimated accuracy with the human judgement (Kocmi et al., 2024). Specifically, this denotes that for an agreement rate of $80 \%$ with human decisions, the improvement needs a minimum of $\geq 1.24$ for both KIWI-XXL and XCOMET, and $\geq 0.53$ for KIWI-22. Further details on estimatied accuracy are provided in Appendix F. The lesser improvements are highlighted in shallow blue boxes. Decreases in performance are marked with yellow boxes.

| Models | $\mathrm{de}$ |  |  | CS |  |  | is |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET |
| Gold Reference | 82.67 | 84.01 | $97.85 \quad$ | 83.19 | 81.83 | 90.27 | 80.51 | 85.20 | 91.52 |
| WMT Winners | 83.56 | 83.70 | 96.99 | 85.31 | ![](https://cdn.mathpix.com/cropped/2024_06_04_bbbd0a340fa9072f51d2g-06.jpg?height=34&width=146&top_left_y=660&top_left_x=1134) | 94.38 | 81.77 | $84.94 \quad$ | 91.61 |
| GPT-4 | 83.48 | 84.91 | 97.56 | 84.81 | 85.35 | 93.48 | 81.03 | 81.21 | 90.00 |
| ALMA-13B-LoRA | 82.62 | 81.64 | 96.49 | 84.14 | 84.24 | 92.38 | 81.71 | 83.31 | 91.20 |
| $+\mathrm{SFT}$ on preferred data | $8 \overline{2} . \overline{7}-$ | $\overline{81} \overline{85}$ | $\overline{96} . \overline{67}$ | 84.14 | $83 . \overline{46}$ | 91.99 | 81.48 | 82.11 | 90.30 |
| $+\mathrm{DPO}$ | 82.40 | 81.20 | 96.40 | 83.86 | 83.45 | 91.68 | 81.43 | 82.66 | 90.33 |
| + CPO (Ours, ALMA-13B-R) | 83.28 | 84.25 | 97.48 | ![](https://cdn.mathpix.com/cropped/2024_06_04_bbbd0a340fa9072f51d2g-06.jpg?height=35&width=119&top_left_y=839&top_left_x=1002) | 87.06 | 93.61 | 82.18   | 85.68   | 91.93 |
| Models | $\mathrm{zh}$ |  |  | $r u$ |  |  | Avg. |  |  |
|  | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET |
| Gold Reference | 80.92 | 81.70 | 90.42 | 82.96 | 84.62 | 94.17 | 82.05 | 83.47 | $92.85 \quad$ |
| WMT Winners | 82.04 | 81.13 | 91.14 | $84.35 \quad$ | 87.01 | 94.79 | 83.41   | 84.81 | 93.78 |
| GPT-4 | 81.73 | 81.53 | 90.79 | 83.64 | 86.15 | 94.3 | 82.94 | 83.83 | 93.23 |
| ALMA-13B-LoRA | 80.82 | 79.96 | 89.92 | 83.10 | 84.17 | 93.79 | 82.48 | 82.66 | 92.76 |
| $+\mathrm{SFT}$ on preferred data | 81.25 | $\overline{80.51}$ | 90.18 | 83.23 | 84.15 | 93.54 | 82.57 | $82 . \overline{42}$ | 92.54 |
| $+\mathrm{DPO}$ | 80.74 | 79.64 | 89.58 | 82.94 | 83.40 | 93.25 | 82.27 | 82.07 | 92.25 |
| + CPO (Ours, ALMA-13B-R) | 82.25 | 84.32 | 92.03 | 83.98 | 87.37 | 95.22 | 83.34 | 85.74 | 94.05 |

model and a contemporary work in the field. ${ }^{6}$ Additionally, we evaluate against the zero-shot performance of the latest GPT-4 (gpt-4-1106-preview), currently shown to be the best translation model among all LLM-based translation systems (Xu et al., 2023; Zhang et al., 2023; Zeng et al., 2023; Jiao et al., 2023a). Lastly, we include comparisons with the WMT competition winners, representing the highest standard of translation models within the competition, though it is noted that the winning models vary across different language directions. ${ }^{7}$

SFT and DPO We also compare different training objectives. Given that CPO is designed to steer learning towards preferred data, a straightforward benchmark is to compare its performance against directly SFT on the same preferred data set. Furthermore, considering that CPO is an evolution of DPO, we also include a comparative analysis with DPO.

### 4.4. WMT'21 and WMT'22 Results

We present the primary results for $e n \rightarrow x x$ and $x x \rightarrow e n$ in Table 3 and Table 4, respectively. Our emphasis is primarily on reference-free evaluation models, due to our analysis in Section 2, which questions the reliability of gold references and highlights that evaluations can be compromised by poorquality references (Kocmi et al., 2023; Freitag et al., 2023).[^5]

However, we are not rejecting the use of reference-based models for evaluation but cautioning the potential pitfalls of poor-quality references. The reference-free models used for evaluation include KIWI-XXL, XCOMET, and a smaller yet popular model, Unbabel/wmt22-cometkiwi-da (hereinafter referred to as KIWI-22). Scores highlighted in bold represent the highest achieved across all systems. For a comprehensive comparison, we also include reference-based evaluations using sacreBLEU (Post, 2018) and COMET22 (Un.babel/wmt22-comet-da) (Rei et al., 2022) in Appendix A.

Comparing With SoTA Models While ALMA-13BLoRA ranks as one of the top moderate-size LLM translation models, it slightly trails behind GPT-4 and the WMT competition winners. However, the incorporation of CPO significantly enhances ALMA's capabilities, bringing its performance to a level that is comparable to or even surpasses that of GPT-4 and WMT winners. For example, ALMA13B-R achieves an average score of 85.74 on KIWI-XXL and 94.05 on XCOMET for $\mathrm{en} \rightarrow \mathrm{xx}$ translations. These scores outperform GPT-4, which scores 83.83 on KIWIXXL and 93.23 on XCOMET, as well as the WMT winners, who score 84.81 on KIWI-XXL and 93.78 on XCOMET.

Comparing With SFT and DPO All training objectives in our study are fine-tuned using the ALMA-13B-LoRA model as a base. In Table 3 and 4, we observe that SFT on preferred data marginally enhances the ALMA model's translation capability for $\mathrm{x} x \rightarrow e n$, and results in a slight de-

Table 4. The overall results in $x \mathrm{x} \rightarrow$ en for WMT'21 and WMT'22. The usage of color and boldface are the same in Table 3 .

| Models | de |  |  | cS |  |  | is |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET |
| Gold Reference | $78.74 \quad$ | 78.56 | 88.82 | 82.08 | 83.11 | 84.60 | 80.88 | 85.04 | 76.16 |
| WMT Winners | 81.38 | 83.59 | 93.74 | 82.47 | 82.53 | 85.65 | 81.39 | 85.60 | 78.14 |
| GPT-4 | 81.50   | 84.58   | 94.47 | 82.52 | 83.55 | 88.48   | 81.49 | 85.90   | 81.11    |
| ALMA-13B-LoRA | 81.14 | 83.57 | 93.30 | 81.96 | 82.97 | 83.95 | 80.90 | 85.49 | 76.68 |
| + SFT on preferred data | 81.36 | 83.98 | 93.84 | 82.36 | 83.15 | 86.67 | 81.32 | 85.61 | 80.20 |
| $+\mathrm{DPO}$ | 81.13 | 83.52 | 93.25 | 81.82 | 82.69 | 83.84 | 80.89 | 85.22 | 76.09 |
| + CPO (Ours, ALMA-13B-R) | 81.50   | 83.97 | 94.20 | 82.63   | 83.75  | 88.03 | 81.57   | 85.73 | 80.49 |
| Models | $\mathrm{zh}$ |  |  | $r u$ |  |  | Avg. |  |  |
|  | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET | KIWI-22 | KIWI-XXL | XCOMET |
| Gold Reference | 77.09 | 74.19 | 90.70 | 80.74 | 79.59 | 88.56 | 79.91 | 80.10 | 85.77 |
| WMT Winners | 77.66 | 73.28 | 87.2 | 81.71 | 80.97 | 90.91 | 80.92 | 81.19 | 87.13 |
| GPT-4 | 79.33 | 77.65 | 92.06 | 81.57 | 81.34 | 90.95 | 81.28 | 82.60   | 89.41  |
| ALMA-13B-LoRA | 77.32 | 74.41 | 89.88 | 81.31 | 81.05 | 89.89 | 80.53 | 81.50 | 86.74 |
| + SFT on preferred data | 78.32 | 76.03 | 90.65 | 81.46 | 81.17 | 90.65 | 80.96 | 81.99 | 88.40 |
| $+\mathrm{DPO}$ | 77.50 | 74.50 | 89.94 | 81.19 | 80.88 | 89.76 | 80.51 | 81.36 | 86.58 |
| + CPO (Ours, ALMA-13B-R) | 79.24 | 77.17 | 91.65 | 81.72 8  | 81.54   | 91.18 | 81.33 | 82.43 | 89.11 |

Table 5. The average performance in WMT'23 across all 6 directions, with the highest score highlighted in bold.

|  | KIWI-22 | KIWI-XXL | XCOMET |
| :--- | :---: | :---: | :---: |
| Gold Reference | 78.74 | 75.56 | 86.30 |
| WMT Winners | $\mathbf{8 0 . 5 7}$ | 77.72 | 88.24 |
| TowerInstruct | 80.31 | 77.18 | 88.11 |
| ALMA-13B-LoRA | 79.48 | 76.00 | 87.16 |
| $+\overline{\mathrm{C}} \overline{\mathrm{P} O} \overline{(\bar{O}} \overline{\mathrm{Ou} s}, \overline{\mathrm{A}} \overline{\mathrm{L}} \overline{\mathrm{M}} \overline{\mathrm{A}}-\overline{1} \overline{3} \overline{\mathrm{B}} \overline{\mathrm{R}})$ | $-\overline{80.55}$ | $\mathbf{7 8 . 9 7}$ | $\mathbf{8 9 . 7} \overline{\mathbf{4}}$ |

terioration for $\mathrm{en} \rightarrow \mathrm{xx}$. Similarly, DPO slightly decreases model performance. In contrast, CPO demonstrates significant improvements across all translation directions.

### 4.5. WMT'23 Results

We show the average results across all six directions in Table 5, and provide the performance in each direction in Appendix G due to the space constraint. Consistent with observations from WMT'21 and WMT'22, ALMA-13B-R surpasses contemporary moderate-size LLM-based translators such as ALMA-13B-LoRA and TowerInstruct, and either matches or exceeds WMT winners.

## 5. Analyses

All analyses use the WMT'21 and WMT'22 test sets, with their averaged performance being reported.

### 5.1. Are Translations Really Better or Just Metric-Preferred?

In our study, since the preferred data is selected by referencefree models and the same models are used for evaluation, we investigate the potential for "cheating" in the scoring process. Specifically, we question whether the translations become genuinely better or they simply align more closely with the evaluation model's preferences. This inquiry is addressed in two parts:

At the metric level, we examine if training a model on data preferred by a specific metric (such as KIWI-XXL) yields improvements that are consistent across other metrics. To investigate this, we reconstruct the preference data using only KIWI-XXL or XCOMET and re-train the ALMA-13BLoRA model using the CPO method. The results, presented in Table 6, do not indicate a significant bias towards the metric used for selecting preferred data. We observed similar and consistent improvements across all metrics, regardless of the specific metric used to select the preferred data. Considering Comet-series models may be positive correlated, we further evaluate ALMA-R using a non-comet metric, BLEURT (Sellam et al., 2020), and also observe significant improvements in Appendix H. The inclusion of a third-party evaluation metric further substantiates the superior translation quality of ALMA-R.

At the method level, we question whether training on metricpreferred data always leads to better scores on that metric, regardless of the method we use. Intriguingly, we observe that generating translations favored by the metric - without true improvement — is not easy. For example, fine-tuning the model solely using DPO or SFT on metric-preferred data can even paradoxically lower its performance on this metric (in Table 3). This prompts us to question whether the improvements observed with CPO, an alternative objective that approximates DPO, when trained on the same data, are merely a result of metric bias. Our stance is that if both DPO and SFT fail to achieve improvements through metric bias, it stands to reason that CPO would similarly not benefit solely from such bias.

Table 6. The influence of employing various reference-free models for creating preference data. The results illustrates that the final performance disparities are minimal whether using solely KIWIXXL, XCOMET, or their combined ensemble.

| Models for Building Preference Data | KIWI-22 | KIWI-XXL | XCOMET |
| :--- | :---: | :---: | :---: |
| Translating to |  |  |  |
| Nnglish $(\mathrm{xx} \rightarrow \mathrm{en})$ |  |  |  |
| N/A (ALMA-13B-LoRA baseline) | 80.53 | 81.50 | 86.74 |
| KIWI-XXL | $\mathbf{8 1 . 3 3}$ | $\mathbf{8 2 . 5 9}$ | 88.82 |
| XCOMET | 81.27 | 82.33 | $\mathbf{8 9 . 1 7}$ |
| Ensemble of above (Original) | $\mathbf{8 1 . 3 3}$ | 82.43 | 89.11 |
| Translating from English $(\mathrm{en} \rightarrow \mathrm{xx})$ |  |  |  |
| N/A (ALMA-13B-LoRA baseline) | 82.48 | 82.66 | 92.76 |
| KIWI-XXL | 83.31 | $\mathbf{8 5 . 8 7}$ | 93.97 |
| XCOMET | 83.09 | 85.43 | $\mathbf{9 4 . 0 9}$ |
| Ensemble of above (Original) | $\mathbf{8 3 . 3 4}$ | 85.74 | 94.05 |

### 5.2. Human Evaluation

The preceding analysis provides indirect evidence underscoring the absence of bias. Here, we incorporate human evaluation as direct proof.

we focused on the $\mathrm{zh} \rightarrow$ en direction, which aligns with the example presented in Section 2. We selected 400 samples from a total of 1875 test sentences, each sample including a Chinese source and two English translations, one from our base model ALMA-13B-LoRA and the other from ALMA13B-R. Four bilingual (English and Chinese) speakers were enlisted to rate each translation on a scale from 0 to 6 , as per the methodology outlined in Kocmi et al. (2022). We provide clarity on the evaluation criteria used for scoring in our study:

$\mathbf{0}$ : it signifies that the translation is nonsensical, failing to convey any coherent meaning.

2: it indicates that the translation partially preserves the meaning of the source text, albeit with substantial inaccuracies or omissions.

4: it denotes that the translation largely maintains the source text's meaning, with only minor issues such as slight grammatical errors.

6: it represents a perfect translation, accurately conveying the full meaning of the source text without any errors.

To ensure impartiality, each annotator was assigned 100 samples to score, with the order of the translations randomized to conceal their origin. In Table 7, we report the mean scores, rank position (with rank 1 indicating better translation and rank 2 indicating worse translation since we only have two translations to compare), and win ratio (note that both of them win if there is a tie) of ALMA and ALMA-R:

The human evaluation results clearly demonstrate that ALMA-13-R outperforms the original ALMA-13B-LoRA. Consequently, our analysis supports the robustness and validity of using reference-free models like KIWI-XXL and XCOMET both for constructing preference data and for
Table 7. The results of human evaluation on sampled $\mathrm{zh} \rightarrow \mathrm{en}$ WMT'22 test data. $\uparrow$ indicates that higher values are better, while $\downarrow$ indicates that lower values are better.

|  | Avg. score $\uparrow$ | Avg. rank $\downarrow$ | Avg. win ratio (\%) | Ties (\%) |
| :--- | :---: | :---: | :---: | :---: |
| ALMA-13B-LoRA | 4.86 | 1.60 | 62.50 | 40.30 |
| ALMA-13B-R | $\mathbf{5 . 1 6}$ | $\mathbf{1 . 4 0}$ | $\mathbf{7 7 . 8 0}$ | 40.30 |

Table 8. An examination of the impact of dis-preferred data quality, contrasting noised data with natural, high-quality translations receiving the lowest scores as dis-preferred data. The findings underscore the importance of the quality of dis-preferred data.

| Dis-Preferred Data | KIWI-22 | KIWI-XXL | XCOMET |
| :--- | :---: | :---: | :---: |
| Translating to English $(\mathrm{x} \times \rightarrow \mathrm{en})$ |  |  |  |
| Manually Noised | 81.01 | 82.18 | 88.23 |
| Natural (Ours) | $\mathbf{8 1 . 3 3}$ | $\mathbf{8 2 . 4 3}$ | $\mathbf{8 9 . 1 1}$ |
| Translating from English $(\mathrm{en} \rightarrow \mathrm{xx})$ |  |  |  |
| Manually Noised | 82.71 | 83.13 | 92.80 |
| Natural (Ours) | $\mathbf{8 3 . 3 4}$ | $\mathbf{8 5 . 7 4}$ | $\mathbf{9 4 . 0 5}$ |

evaluation purposes.

### 5.3. Ablation Study

CPO Loss Components The CPO loss function consists of two components: $\mathcal{L}_{\text {prefer }}$ for preference learning, and $\mathcal{L}_{\text {NLL }}$, which ensures the model does not deviate significantly from the preferred data distribution. To illustrate the significance of each term, we re-train the model exclusively with one of the components. It is important to note that training solely with $\mathcal{L}_{\text {NLL }}$ equates to the baseline scenario of SFT on preferred data. As depicted in the left of Figure 4, the inclusion of both terms yields the optimal performance, while the absence of either leads to a decrease in performance. In Appendix I, we also show that incorporating $\mathcal{L}_{\mathrm{NLL}}$ into the DPO loss yields significant improvements.

Preference Data Components Our preference data selection involves choosing preferred and dis-preferred translations from a triplet consisting of outputs from GPT-4, ALMA, and the gold reference. In the right of Figure 4, we emphasize the significance of the data generated by both ALMA and GPT-4. The results indicate a notable decline in performance when ALMA data is excluded in the en $\rightarrow \mathrm{x} x$ direction. Conversely, omitting GPT-4 data leads to a significant performance decrease in the $x \mathrm{x} \rightarrow$ en direction. This demonstrates that data generated by both systems plays a helpful role in enhancing model performance.

### 5.4. Does The Quality of Dis-preferred Data Matter?

In our experimental setup, dis-preferred data, though originating from strong translation models, receives the lowest scores when compared with two other translation outputs. A pertinent question arises: does the quality of dis-preferred
![](https://cdn.mathpix.com/cropped/2024_06_04_bbbd0a340fa9072f51d2g-09.jpg?height=470&width=1526&top_left_y=220&top_left_x=278)

Figure 4. Left: an ablation study evaluating the significance of individual components in the CPO loss function, specifically analyzing how the preference learning loss $\mathcal{L}_{\text {prefer }}$ and the log-likelihood loss $\mathcal{L}_{\mathrm{NLL}}$ each contribute to enhancing translation performance. Right: An ablation study assessing the significance of each component in the translation triplet. By excluding either ALMA or GPT-4 generated data from the preference triplet and re-training the model, we evaluate their respective impacts. The findings highlight the importance of ALMA-generated data for $e n \rightarrow x x$ translations and GPT-4 generated data for $x \mathrm{x} \rightarrow$ en translations.

data significantly impact model performance, and can highquality (albeit imperfect) dis-preferred data aid in translation improvement? To explore this, we constructed a new set of preference data where the dis-preferred translations $\left(y_{l}\right)$ are artificially generated, as opposed to being naturally derived high-quality translations.

In this new dataset, the preferred translation $\left(y_{w}\right)$ remains the best of the three translation candidates, selected in the same manner as in Section 3.1. However, the dis-preferred translation is intentionally modified to be a noised version of $y_{w}$. We applied random deletions of words with a probability of 0.15 and word swaps within a range of 1 with a probability of 0.3 , following the method suggested by Zeng et al. (2023) for creating manually noised dis-preferred data. This approach produces worse translations that are artificial.

Table 8 compares the performance when using these manually noised dis-preferred data versus the original, naturally occurring high-quality dis-preferred data. The results show a substantial decline in performance across all three metrics and both translation directions when the dis-preferred data is manually noised, underscoring the importance of the quality of dis-preferred data in enhancing translation performance.

## 6. Conclusion

In this study, we initially proposed the potential quality issues of gold references in the MT task, highlighting instances where advanced translation models can outperform these references. This finding not only challenges model training via SFT, but also the evaluation procedure that uses reference-based metrics. Subsequently, we introduce Contrastive Preference Optimization, a more efficient variant of of DPO. This method leverages both model-generated and reference data to guide the model in avoiding nearperfect yet flawed translations and learning superior ones. Our developed model, ALMA-13B-R, stands out as the first moderate-size LLM-based translation model to match, and in some cases surpass, the performance of GPT-4 and WMT competition winners, marking a significant advancement in the field of MT.

## Impact Statement

This paper presents work whose goal is to advance the field of Machine Translation and Large Language Model. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## Acknowledgements

We express our profound appreciation to annoymous reviewers for their helpful suggestions. We also thank Tianjian Li, Hieu Hoang, Marcin Junczys-Dowmunt, Huda Khayrallah, Thamme Gowda, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Roman Grundkiewicz, Philipp Koehn, Hany Hassan Awadalla, Arul Menezes, and Vishal Chowdhary for their engaging and valuable discussions that greatly enriched our work. Special thanks to Tom Kocmi for his innovative suggestion to enhance numerical data visibility using a dynamic threshold determined by estimated accuracy. Our gratitude also extends to Pushpendre Rastogi and Joey Hejna for their insightful recommendations on the CPO theory. Furthermore, we acknowledge the Unbabel Team for their valuable advice on incorporating non-COMET metrics into our analysis.

## References

Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo, G. Falcon-40B: an open large language model with stateof-the-art performance. 2023.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020 .

Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020.

Chen, Y., Liu, Y., Meng, F., Chen, Y., Xu, J., and Zhou, J. Improving translation faithfulness of large language models via augmenting instructions. arXiv preprint arXiv:2308.12674, 2023.

Fan, A., Bhosale, S., Schwenk, H., Ma, Z., El-Kishky, A., Goyal, S., Baines, M., Celebi, O., Wenzek, G., Chaudhary, V., et al. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22 (107):1-48, 2021.

Freitag, M., Mathur, N., Lo, C.-k., Avramidis, E., Rei, R., Thompson, B., Kocmi, T., Blain, F., Deutsch, D., Stewart, C., Zerva, C., Castilho, S., Lavie, A., and Foster, G. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Koehn, P., Haddow, B., Kocmi, T., and Monz, C. (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 578-628, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.wmt-1.51. URL https://aclanthology. org/2023.wmt-1.51.

Guerreiro, N. M., Rei, R., van Stigt, D., Coheur, L., Colombo, P., and Martins, A. F. xcomet: Transparent machine translation evaluation through fine-grained error detection. arXiv preprint arXiv:2310.10482, 2023.

He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729-9738, 2020 .

Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W. B., and Sadigh, D. Contrastive prefence learning: Learning from human feedback without rl. arXiv preprint arXiv:2310.13639, 2023.

Hendy, A., Abdelrehim, M., Sharaf, A., Raunak, V., Gabr, M., Matsushita, H., Kim, Y. J., Afify, M., and Awadalla, H. H. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210, 2023.
Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https: / openreview.net/forum?id=nZeVKeeFYf9.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Jiao, W., Huang, J.-t., Wang, W., He, Z., Liang, T., Wang, X., Shi, S., and Tu, Z. ParroT: Translating during chat using large language models tuned with human translation and feedback. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15009-15020, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 1001. URL https://aclanthology.org/2023. findings-emnlp. 1001 .

Jiao, W., Wang, W., Huang, J.-t., Wang, X., and Tu, Z. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745, 2023b.

Kocmi, T., Bawden, R., Bojar, O., Dvorkovich, A., Federmann, C., Fishel, M., Gowda, T., Graham, Y., Grundkiewicz, R., Haddow, B., Knowles, R., Koehn, P., Monz, C., Morishita, M., Nagata, M., Nakazawa, T., Novák, M., Popel, M., and Popović, M. Findings of the 2022 conference on machine translation (WMT22). In Koehn, P., Barrault, L., Bojar, O., Bougares, F., Chatterjee, R., Costajussà, M. R., Federmann, C., Fishel, M., Fraser, A., Freitag, M., Graham, Y., Grundkiewicz, R., Guzman, P., Haddow, B., Huck, M., Jimeno Yepes, A., Kocmi, T., Martins, A., Morishita, M., Monz, C., Nagata, M., Nakazawa, T., Negri, M., Névéol, A., Neves, M., Popel, M., Turchi, M., and Zampieri, M. (eds.), Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 1-45, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https : / aclanthology.org/2022.wmt-1.1.

Kocmi, T., Avramidis, E., Bawden, R., Bojar, O., Dvorkovich, A., Federmann, C., Fishel, M., Freitag, M., Gowda, T., Grundkiewicz, R., Haddow, B., Koehn, P., Marie, B., Monz, C., Morishita, M., Murray, K., Nagata, M., Nakazawa, T., Popel, M., Popović, M., and Shmatova, M. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Koehn, P., Haddow, B., Kocmi, T., and Monz, C. (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 1-42, Singapore, December 2023. Association for Computational Linguistics. URL https : //aclanthology.org/2023.wmt-1.1.

Kocmi, T., Zouhar, V., Federmann, C., and Post, M. Navigating the metrics maze: Reconciling score magnitudes and accuracies. arXiv preprint arXiv:2401.06760, 2024.

Kudugunta, S., Caswell, I., Zhang, B., Garcia, X., Choquette-Choo, C. A., Lee, K., Xin, D., Kusupati, A., Stella, R., Bapna, A., and Firat, O. Madlad-400: A multilingual and document-level large audited dataset, 2023.

Li, J., Zhou, H., Huang, S., Chen, S., and Chen, J. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. arXiv preprint arXiv:2305.15083, 2023.

Maillard, J., Gao, C., Kalbassi, E., Sadagopan, K. R., Goswami, V., Koehn, P., Fan, A., and Guzman, F. Small data, big impact: Leveraging minimal data for effective machine translation. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2740-2756, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.154. URL https: //aclanthology.org/2023.acl-long.154.

NLLB TEAM, Costa-jussà, M. R., Cross, J., Çelebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.

Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

OpenAI. Gpt-4 technical report, 2023.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In Isabelle, P., Charniak, E., and Lin, D. (eds.), Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.

Post, M. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https: / /aclanthology.org/W18-6319.
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining, pp. 3505-3506, 2020 .

Rei, R., C. de Souza, J. G., Alves, D., Zerva, C., Farinha, A. C., Glushkova, T., Lavie, A., Coheur, L., and Martins, A. F. T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. wmt-1. 52 .

Rei, R., Guerreiro, N. M., Pombal, J., van Stigt, D., Treviso, M., Coheur, L., de Souza, J. G., and Martins, A. F. Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task. arXiv preprint arXiv:2309.11925, 2023.

Robinson, J. D., Chuang, C.-Y., Sra, S., and Jegelka, S. Contrastive learning with hard negative samples. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum? id=CR1XOQ0UTh-.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Sellam, T., Das, D., and Parikh, A. BLEURT: Learning robust metrics for text generation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https : //aclanthology.org/2020.acl-main. 704.

Tan, W., Heffernan, K., Schwenk, H., and Koehn, P. Multilingual representation distillation with contrastive learning. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1469-1482, 2023.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.

$\mathrm{Wu}, \mathrm{Y}$. and Hu, G. Exploring prompt engineering with GPT language models for document-level machine translation: Insights and findings. In Koehn, P., Haddow, B., Kocmi, T., and Monz, C. (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 166-169, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.15. URL https: //aclanthology.org/2023.wmt-1.15.

Xu, H., Van Durme, B., and Murray, K. BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6663-6675, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.534. URL https://aclanthology . org/2021.emnlp-main. 534.

Xu, H., Kim, Y. J., Sharaf, A., and Awadalla, H. H. A paradigm shift in machine translation: Boosting translation performance of large language models, 2023.

Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. mT5: A massively multilingual pre-trained text-to-text transformer. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483-498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https: //aclanthology.org/2021.naacl-main. 41.

Yang, W., Li, C., Zhang, J., and Zong, C. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. arXiv preprint arXiv:2305.18098, 2023.

Zeng, J., Meng, F., Yin, Y., and Zhou, J. Tim: Teaching large language models to translate with comparison. arXiv preprint arXiv:2307.04408, 2023.
Zhang, S., Fang, Q., Zhang, Z., Ma, Z., Zhou, Y., Huang, L., Bu, M., Gui, S., Chen, Y., Chen, X., et al. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint arXiv:2306.10968, 2023.

Zhu, W., Liu, H., Dong, Q., Xu, J., Kong, L., Chen, J., Li, L., and Huang, S. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675, 2023a.

Zhu, W., Lv, Y., Dong, Q., Yuan, F., Xu, J., Huang, S., Kong, L., Chen, J., and Li, L. Extrapolating large language models to non-english by aligning languages. arXiv preprint arXiv:2308.04948, 2023b.

Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.
