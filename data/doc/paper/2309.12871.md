# ANGLE-OPTIMIZED TEXT EMBEDDINGS 

Xianming Li, Jing Li *<br>Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR<br>xianming.li@connect.polyu.hk, jing-amelia.li@polyu.edu.hk


#### Abstract

High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.


## 1 INTRODUCTION

The development of text embeddings (Kiros et al., 2015, Hill et al., 2016, Conneau et al. 2017, Cer et al., 2018, Reimers \& Gurevych, 2019: Gao et al., 2021) is an essential research challenge in the NLP community. Text embeddings effectively feature key semantic and syntactic information in language, which broadly affects the performance of downstream tasks, such as text classification (Li et al., 2021), sentiment analysis (Suresh \& Ong, 2021; Zhang et al., 2022), semantic matching Grill et al., 2020, Lu et al. 2020), clustering (Reimers \& Gurevych, 2019; Xu et al., 2023), and question-answering (QA) system (Yue et al., 2021). In particular, text embedding models play a crucial role in LLMs such as ChatGPT (OpenAI 2022, 2023), LLaMA (Touvron et al., 2023a b), and ChatGLM (Du et al., 2022)-based applications. These LLM-based applications heavily rely on high-quality text embeddings for tasks such as vector search, where related documents are retrieved for LLM QA (Asai et al., 2023).

Recent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani \& Huang, 2023, Zhuo et al. 2023) have utilized pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al. 2019) in combination with contrastive learning to enhance the quality of text embeddings. These approaches involve pulling semantically similar samples together and pushing apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semantically similar can be generated by data augmentation, while negative samples that are dissimilar are selected from different texts within the same mini-batch (in-batch negatives). However, supervised negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without annotation, which can lead to performance degradation. Although some models such as (Gao et al. 2021) optimize hard negative samples, they rely on strict triple formats $\left(x_{i}, x_{i}^{+}, x_{i}^{-}\right)$. While most existing supervised STS datasets only provide pairs $\left(x_{i}, x_{i}^{+}\right)$or $\left(x_{j}, x_{j}^{-}\right)$, where $x_{i}^{+}$refers to the positive sample of $x_{i}$ while $x_{j}^{-}$the negative sample of $x_{j}$. Thus, most contrastive models are used in unsupervised settings yet might not benefit from human supervision.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-02.jpg?height=284&width=637&top_left_y=297&top_left_x=728)

Figure 1: The saturation zones of the cosine function. The gradient at saturation zones is close to zero. During backpropagation, if the gradient is very small, it could kill the gradient and make the network difficult to learn.

For supervised STS (Reimers \& Gurevych, 2019; Su, 2022), most efforts to date employed the cosine function in their training objective to measure the pairwise semantic similarity. However, the cosine function has saturation zones, as shown in Figure 1. It can impede the optimization due to the gradient vanishing issue and hinder the ability to learn subtle distinctions between texts in backpropagation. Additionally, many STS datasets such as MRPC 1 and $\mathrm{QQP}^{2}$ provide binary labels representing dissimilar (0) and similar (1), which naturally fall within the saturation zone of the cosine function. To overcome this challenge, this paper proposes a novel angle-optimized text embedding. It optimizes not only the cosine similarity between texts but also the angle to mitigate the negative impact of the saturation zones of the cosine function on the learning process. Specifically, it first divides the text embedding into real and imaginary parts in a complex space. Then, it follows the division rule in complex space to compute the angle difference between two text embeddings. After normalization, the angle difference becomes an objective to be optimized. It is intuitive to optimize the normalized angle difference, because if the normalized angle difference between two text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space, i.e., their similarity is larger.

In the STS experimental setup, we observed that the majority of existing STS benchmarks focus on evaluating models on short texts. Unfortunately, there is a lack of datasets specifically designed to evaluate the STS performance of models on long texts. Long texts are prevalent in real-world applications such as financial documents, legal documents, and health reports (Li et al. 2023). To tackle this challenge, this paper presents a new high-quality long-text STS dataset. This dataset allows for a more thorough evaluation of model performance on long texts. Specifically, the dataset is collected from GitHub Issues with roughly $21 \mathrm{~K}$ samples, we use the duplicate issues as the positive samples and the non-duplicate issues as the negative samples.

We first experimented with both short and long-text datasets and showed that AnglE outperforms the SOTA STS models in both transfer and non-transfer STS tasks. For example, AnglE shows an average Spearman correlation of $73.55 \%$ in non-transfer STS tasks, compared to $68.03 \%$ for SBERT. Then, an ablation study shows that all components contribute positively to AnglE's superior performance. Next, we discuss the domain-specific scenarios with limited annotated data that are challenging for AnglE-like supervised STS, where it is observed that AnglE can work well with LLM-supervised data. Finally, we find that AnglE can benefit downstream retrieval applications and can learn representations closer to actual representations.

In summary, the contributions of this paper are listed as follows:

- We investigate the negative effects of saturation zone in the cosine function widely applied in STS and propose a novel angle-optimized text embedding model to mitigate this issue.
- We extend the existing STS benchmark with a newly collected long-text dataset from Github Issues to allow a more comprehensive empirical study in STS.
- We present extensive experiments on STS and demonstrate that AngIE can substantially improve the text embedding quality in various scenarios.[^1]


## 2 RELATED WORK

This section is organized as follows: we first introduce the unsupervised approaches, then the supervised approaches, and finally give a summary.

Unsupervised Approaches Early studies (Hill et al., 2016; Pagliardini et al. 2018) have demonstrated the efficacy of augmenting word2vec (Mikolov et al. 2013) with n-gram embeddings, yielding strong results in text embeddings. Recently, BERT-flow (Li et al. 2020) has introduced a flowbased approach that maps BERT embeddings to a standard Gaussian latent space. On the other hand, BERT-whitening (Su et al. 2021) applies the whitening operation to BERT embeddings to enhance text embeddings. Furthermore, very recent research (Carlsson et al., 2020, Zhang et al., 2020, Giorgi et al., 2021; Gao et al. 2021; Yan et al., 2021; Chuang et al.||2022; Jiang et al., 2022b; Zhuo et al., 2023) has focused on leveraging contrastive objectives to improve the quality of text embeddings.

Supervised Approaches Supervised text embeddings usually perform better than their unsupervised counterparts (Gao et al. 2021). Various studies have effectively utilized supervised datasets to enhance the learning of text embeddings. In particular, Conneau et al. (2017) introduced a method that leverages supervised Natural Language Inference (NLI) tasks for this purpose. Building on a transformer backbone, USE (Cer et al. 2018) incorporates the SNLI dataset to augment unsupervised training, resulting in improved performance. Furthermore, SBERT (Reimers \& Gurevych. 2019) enhances text embedding by combining BERT with a siamese architecture. Jiang et al. (2022a, 2023) proposed the use of prompt engineering to improve text embeddings.

However, most existing models optimize the cosine similarity but neglect the negative effect of the saturation zone of the cosine function. To address this issue, this paper proposes a novel angleoptimized text embedding model to improve the quality of text embedding.

## 3 METHODOLOGY

This section will introduce the components of the proposed angle-optimized text embedding model, including the input layer, cosine objective, in-batch negative objective, and angle objective.

### 3.1 INPUT LAYER

For the input sentences, we first apply padding to ensure a consistent length $l$. Next, we map each word to a continuous $d$-dimensional space to produce word embeddings $\mathbf{e}_{i} \in \mathbb{R}^{d}$. These word embeddings are then concatenated to form the model input: $\mathbf{E}=\left[\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{l}\right] \in \mathbb{R}^{l \times d}$. Subsequently, the model input is passed through an encoder such as BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), and LLaMA (Touvron et al., 2023a b) to obtain the contextual representation $\mathbf{X}$.

### 3.2 Cosine OBJECTIVE

Following the prior study ( $\mathrm{Su}, 2022$ ), we employ the cosine objective function for end-to-end optimization of cosine similarity between representations, as follows:

$$
\begin{equation*}
\mathcal{L}_{\text {cos }}=\log \left[1+\sum_{s\left(\mathbf{X}_{i}, \mathbf{X}_{j}\right)>s\left(\mathbf{X}_{m}, \mathbf{X}_{n}\right)} e^{\frac{\cos \left(\mathbf{X}_{m}, \mathbf{x}_{n}\right)-\cos \left(\mathbf{X}_{i}, \mathbf{x}_{j}\right)}{\tau}}\right] \tag{1}
\end{equation*}
$$

where $\tau$ is a temperature hyperparameter, $\cos (\cdot)$ is the cosine similarity function, and $s(u, v)$ is the similarity between $u$ and $v$. By optimizing the $\mathcal{L}_{\text {cos }}$, we expect the cosine similarity of the high similarity pair to be greater than that of the low similarity pair.

### 3.3 IN-BATCH NEGATIVE OBJECTIVE

To further improve performance, we integrate the in-batch negative objective function. Because in-batch negative samples can serve as a data augmentation technique, which can benefit the generalization. Unlike existing contrastive learning models (Gao et al., 2021, Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recognizing that there might be identical sentences within a batch that are not explicitly labeled as positive samples, causing them to become in-batch negatives, we identify these duplicate sentences and assign them as positive samples, thereby reducing potential noise. The formulation for the in-batch negative objective function (ibn) is as follows:

$$
\begin{equation*}
\mathcal{L}_{i b n}=-\sum_{b} \sum_{i}^{m} \log \left[\frac{e^{\cos \left(\mathbf{X}_{b_{i}}, \mathbf{X}_{b_{i}}^{+}\right) / \tau}}{\sum_{j}^{N} e^{\cos \left(\mathbf{X}_{b_{i}}, \mathbf{X}_{b_{j}}^{+}\right) / \tau}}\right] \tag{2}
\end{equation*}
$$

where $\tau$ is a temperature hyperparameter, $b$ stands for the $b$-th batch, $\mathbf{X}_{b_{i}}^{+}$and $\mathbf{X}_{b_{j}}^{+}$are the respective positive samples of $\mathbf{X}_{b_{i}}$ and $\mathbf{X}_{b_{j}}$, $m$ represents the number of positive pairs in $b$-th batch, $N$ is the batch size, and $\cos (\cdot)$ is the cosine similarity function.

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-04.jpg?height=349&width=388&top_left_y=850&top_left_x=543)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-04.jpg?height=333&width=401&top_left_y=842&top_left_x=1168)

(b)

Figure 2: (a) Division in complex space. $\Delta \theta$ is the angle difference between dividend $z$ and divisor $w$ in complex space. (b) Angle optimization in cosine saturation zones. Even though $\Delta y \approx 0$ could kill the gradient, the corresponding angle difference in complex space is still distinct for optimization.

### 3.4 ANGLE OBJECTIVE

We found that both the cosine and in-batch negative objectives employ the cosine function to measure similarity. However, it is important to note that the cosine function includes saturation zones, which can hinder the optimization process. We optimize the angle difference in complex space to mitigate these adverse effects. Figure $2 \mathrm{a}$ draws the division in complex space, and Figure $2 \mathrm{~b}$ depicts how angle optimization works in cosine saturation zones. To optimize the angle difference, we define $\mathbf{X}^{r e}$ and $\mathbf{X}^{i m}$ are the real part and the imaginary part of $\mathbf{X}$. We follow the implementation of (Sun et al. 2019) to obtain $\mathbf{X}^{r e}$ and $\mathbf{X}^{i m}$ by the chunking strategy. Specifically, for the pair $\left(\mathbf{X}_{i}, \mathbf{X}_{j}\right)$, their representations in the complex space are defined as follows:

$$
\begin{align*}
\mathbf{z} & =\mathbf{a}+\mathbf{b} i \in \mathbb{C} \\
\mathbf{w} & =\mathbf{c}+\mathbf{d} i \in \mathbb{C} \tag{3}
\end{align*}
$$

where $\mathbf{a}=\mathbf{X}_{i}^{r e} \in \mathbb{R}, \mathbf{b}=\mathbf{X}_{i}^{i m} \in \mathbb{R}, \mathbf{c}=\mathbf{X}_{j}^{r e} \in \mathbb{R}$, and $\mathbf{d}=\mathbf{X}_{j}^{i m} \in \mathbb{R}$. To compute the angle difference between $\mathbf{z}$ and $\mathbf{w}$, we calculate division in complex space in polar coordinates, as follows:

$$
\begin{align*}
\frac{\mathbf{z}}{\mathbf{w}} & =\gamma \Delta \theta_{z w} \\
\gamma & =\frac{r_{\mathbf{z}}}{r_{\mathbf{w}}}=\frac{\sqrt{\mathbf{a}^{2}+\mathbf{b}^{2}}}{\sqrt{\mathbf{c}^{2}+\mathbf{d}^{2}}}  \tag{4}\\
\Delta \theta_{z w} & =\theta_{\mathbf{z}}-\theta_{\mathbf{w}}
\end{align*}
$$

where $r_{\mathbf{z}}$ and $r_{\mathbf{w}}$ represent the magnitudes of $\mathbf{z}$ and $\mathbf{w}$, while $\theta_{\mathbf{z}}$ and $\theta_{\mathbf{w}}$ denote the respective angles of $\mathbf{z}$ and $\mathbf{w}$. Next, we compute the value of $\frac{\mathbf{z}}{\mathbf{w}}$ by the division rule in complex space, as follows:

$$
\begin{equation*}
\frac{\mathbf{z}}{\mathbf{w}}=\frac{\mathbf{a}+\mathbf{b} i}{\mathbf{c}+\mathbf{d} i}=\frac{(\mathbf{a c}+\mathbf{b d})+(\mathbf{b c}-\mathbf{a d}) i}{\mathbf{c}^{2}+\mathbf{d}^{2}} \tag{5}
\end{equation*}
$$

By employing Eq. 4 and Eq. 5, we can calculate the angle difference between $\mathbf{z}$ and $\mathbf{w}$ by multiplying both sides by $\frac{1}{\gamma}$, which can be seen as a normalization operation. In this paper, we determine the absolute normalized angle difference using the following expression:

$$
\begin{align*}
\Delta \theta_{z w} & =\operatorname{abs}\left(\frac{\mathbf{z}}{\mathbf{w}} \times \frac{1}{\gamma}\right) \\
& =\operatorname{abs}\left[\frac{(\mathbf{a c}+\mathbf{b d})+(\mathbf{b c}-\mathbf{a d}) i}{\mathbf{c}^{2}+\mathbf{d}^{2}} \times \frac{\sqrt{\mathbf{c}^{2}+\mathbf{d}^{2}}}{\sqrt{\mathbf{a}^{2}+\mathbf{b}^{2}}}\right]  \tag{6}\\
& =\operatorname{abs}\left[\frac{(\mathbf{a c}+\mathbf{b d})+(\mathbf{b c}-\mathbf{a d}) i}{\sqrt{\left(\mathbf{c}^{2}+\mathbf{d}^{2}\right)\left(\mathbf{a}^{2}+\mathbf{b}^{2}\right)}}\right]
\end{align*}
$$

Then, the angle difference can be optimized by the following objective function:

$$
\begin{equation*}
\mathcal{L}_{\text {angle }}=\log \left[1+\sum_{s\left(\mathbf{X}_{i}, \mathbf{X}_{j}\right)>s\left(\mathbf{X}_{m}, \mathbf{X}_{n}\right)} e^{\frac{\Delta \theta_{i j}-\Delta \theta_{m n} n}{\tau}}\right] \tag{7}
\end{equation*}
$$

where $\tau$ is a temperature hyperparameter and $s(u, v)$ is the similarity between $u$ and $v$. By optimizing the $\mathcal{L}_{\text {angle }}$, our objective is to minimize the normalized angle difference for pairs with high similarity compared to those with low similarity.

Finally, we combine the aforementioned three objective functions in the following manner to form the final objective function:

$$
\begin{equation*}
\mathcal{L}=w_{1} * \mathcal{L}_{\text {cos }}+w_{2} * \mathcal{L}_{i b n}+w_{3} * \mathcal{L}_{\text {angle }} \tag{8}
\end{equation*}
$$

where $w_{1}, w_{2}$, and $w_{3}$ are constants.

## 4 EXPERIMENT

### 4.1 DATASEtS and EvalUATION METRICS

Existing STS Benchmarks We mainly evaluate our model on several widely-adopted STS datasets, namely: MRPC, QQP, QNLI ${ }^{3}$, STS 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), SICK-R (Marelli et al., 2014), and STS-B (Cer et al. 2017). These datasets mainly consist of short text, but real-world scenarios often involve long text documents. Thus, we introduce a newly long-text dataset called GitHub Issues Similarity Dataset to comprehensively evaluate the STS task.

GitHub Issues Similarity Dataset We observed the presence of many duplicate issues on GitHub. Typically, the maintainers of open source organizations tend to mark these duplicate issues as closed with a comment like "closing as a duplicate of \#id". Consequently, these duplicate issues inherently serve as a source of the STS task. It is also worth noting that most issues contain long texts because of the inclusion of extensive code within the issues. To compile the dataset, we extracted duplicated issues from 55 popular open-source projects (see A.1) on GitHub using GitHub API 4 . The duplicated issues were used as positive samples, while the remaining issues were considered negative samples. Table 1 presents statistics of the GitHub Issues Similarity Dataset, while Figure 3 shows a violin plot illustrating the token-level text length distribution. The visualization reveals a substantial number of long texts. Specifically, the proportion of long texts (token length $>512$ ) for the train, validation, and test sets is at $61.03 \%, 60.85 \%$, and $60.50 \%$, respectively.

Evaluation Metrics To ensure a fair comparison, we follow previous studies and use Spearman's correlation for evaluation. We use SentEval (Conneau \& Kiela, 2018) to compute Spearman's correlation and report the results in the "all" setting, which is consistent with the baselines.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-06.jpg?height=306&width=480&top_left_y=281&top_left_x=408)

Figure 3: Log token length distribution of the GitHub Issue Similarity Dataset.
Table 1: Statistics of the proposed GitHub Issues Similarity Dataset. \#Positive denotes the count of positive pairs, and \#Negative represents the number of negative pairs.

| Split | Train | Validation | Test |
| :---: | :---: | :---: | :---: |
| \#Positive | 9457 | 774 | 807 |
| \#Negative | 9108 | 773 | 741 |
| Total | 18565 | 1547 | 1548 |

### 4.2 IMPLEMENTATION DETAILS

In this paper, we use the pre-trained uncased BERT base model (110M parameters) as the backbone model. For a fair comparison, all BERT-based baselines also adopt this setting. We set the value of $\tau$ for the cosine objective and the in-batch negative objective to 0.05 , based on prior research. Additionally, we determined the value of $\tau$ for the angle objective to be 1.0 through grid search.

### 4.3 MAIN RESULTS

In this section, we will first introduce the baselines, then the results of the transfer STS tasks, then the results of the non-transfer STS tasks, and finally a summary.

Baselines We compare our proposed model with widely used baselines, encompassing both unsupervised and supervised models. The unsupervised models are average GloVe (Pennington et al. 2014), BERT-flow (Li et al. 2020), BERT-whitening (Su et al., 2021), LLaMA2 (Touvron et al. 2023b), and contrastive learning models including IS-BERT (Zhang et al., 2020), CT-BERT (Carlsson et al., 2020), SimCSE (Gao et al. 2021), ConSERT (Yan et al., 2021), and DiffCSE (Chuang et al. 2022). On the other hand, the chosen supervised models are InferSent (Conneau et al. 2017), USE (Cer et al. 2018), SBERT (Reimers \& Gurevych, 2019), CoSENT (Su, 2022), as well as supervised versions of SimCSE and ConSERT.

Transfer STS Tasks For a fair comparison, we train AnglE with the NLI datasets MNLI (Williams et al. 2018) and SNLI (Bowman et al. 2015) and then transfer it to evaluate seven STS benchmark datasets. The evaluation results are presented in Table 2 It is evident that AnglE-BERT and AnglELLaMA consistently outperform the baselines with a gain of $0.80 \%$ and $0.72 \%$ in average score, respectively, over the previous SOTA SimCSE-BERT and SimCSE-LLaMA. Note that supervised SBERT and CoSENT show lower results than other unsupervised contrastive learning models like SimCSE and DiffCSE. This difference might arise from the difference in data distributions between the training and test data in the transfer STS tasks. They struggle to effectively generalize to STS tasks when trained solely with NLI datasets. In contrast, contrastive learning models exhibit better generalization capabilities due to their alignment and uniformity features. Because AnglE optimizes both the supervised cosine objective and the in-batch negative objective. This can allow AnglE to generalize well in transfer STS tasks. Additionally, the angle optimization in AnglE mitigates the negative impact of the saturation zone in the cosine function to produce better performance than other baselines.

Non-transfer STS Tasks To provide a comprehensive analysis, we also evaluate the performance of the baselines in the non-transfer setting. We train the baselines on the train set and evaluate them on the test or validation set. Two typical models, SimCSE and SBERT, representing contrastive and supervised learning, are compared with our model. The results of the non-transfer STS tasks are listed in Table 3 , where we evaluate the baselines on four short-text datasets (MRPC, STS-B, QQP, and QNLI) and one long-text dataset (GitHub Issues Similarity Dataset). SimCSE notably performs poorly compared to SBERT and AnglE in the non-transfer setting. This is due to the limitation of the small-scale training set, as there are not enough samples for SimCSE to effectively learn representations. Furthermore, the datasets only provide pair-supervised data, namely $\left(x, x^{+}\right)$or $\left(x, x^{-}\right)$, which prevents SimCSE from utilizing its hard negative objective that relies on triple-supervised

| Model | STS12 | STS13 | STS14 | STS15 | STS16 | STS-B | SICR-R | Avg. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Unsupervised Models |  |  |  |  |  |  |  |  |
| GloVe (avg.) $\dagger$ | 55.14 | 70.66 | 59.73 | 68.25 | 63.66 | 58.02 | 53.76 | 61.32 |
| BERT-flow $\ddagger$ | 58.40 | 67.10 | 60.85 | 75.16 | 71.22 | 68.66 | 64.47 | 66.55 |
| BERT-whitening $\ddagger$ | 57.83 | 66.90 | 60.90 | 75.08 | 71.31 | 68.24 | 63.73 | 66.28 |
| IS-BERT $\ddagger$ | 56.77 | 69.24 | 61.21 | 75.23 | 70.16 | 69.21 | 64.25 | 66.58 |
| CT-BERT $\ddagger$ | 61.63 | 76.80 | 68.47 | 77.50 | 76.48 | 74.31 | 69.19 | 72.05 |
| ConSERT-BERT | 64.64 | 78.49 | 69.07 | 79.72 | 75.95 | 73.97 | 67.31 | 72.74 |
| DiffCSE-BERT | 72.28 | 84.43 | 76.47 | 83.90 | 80.54 | 80.59 | 71.23 | 78.49 |
| SimCSE-BERT | 68.40 | 82.41 | 74.38 | 80.91 | 78.56 | 76.85 | 72.23 | 76.25 |
| LLaMA2-7B $\star$ | 50.66 | 73.32 | 62.76 | 67.00 | 70.98 | 63.28 | 67.40 | 65.06 |
| Supervised Models |  |  |  |  |  |  |  |  |
| InferSent-GloVe $\dagger$ | 52.86 | 66.75 | 62.15 | 72.77 | 66.87 | 68.03 | 65.65 | 65.01 |
| USE $\dagger$ | 64.49 | 67.80 | 64.61 | 76.83 | 73.18 | 74.92 | 76.69 | 71.22 |
| ConSERT-BERT | 74.07 | 83.93 | 77.05 | 83.66 | 78.76 | 81.36 | 76.77 | 79.37 |
| CoSENT-BERT $\star$ | 71.35 | 77.52 | 75.05 | 79.68 | 76.05 | 78.99 | 71.19 | 75.69 |
| SBERT $\dagger$ | 70.97 | 76.53 | 73.19 | 79.09 | 74.30 | 77.03 | 72.91 | 74.89 |
| SimCSE-BERT | 75.30 | 84.67 | 80.19 | 85.40 | 80.82 | 84.25 | 80.39 | 81.57 |
| SimCSE-LLaMA2-7B $\star$ | 78.39 | 89.95 | 84.80 | 88.50 | 86.04 | 87.86 | 81.11 | 85.24 |
| AnglE-BERT | 75.09 | 85.56 | 80.66 | 86.44 | 82.47 | 85.16 | 81.23 | 82.37 |
| AnglE-LLaMA2-7B | $\mathbf{7 9 . 0 0}$ | $\mathbf{9 0 . 5 6}$ | $\mathbf{8 5 . 7 9}$ | $\mathbf{8 9 . 4 3}$ | $\mathbf{8 7 . 0 0}$ | $\mathbf{8 8 . 9 7}$ | $\mathbf{8 0 . 9 4}$ | $\mathbf{8 5 . 9 6}$ |

Table 2: Text embedding performance on STS tasks. We report the Spearman's correlation $\rho \times$ 100 of the "all" setting computed by SentEval. For supervised LLaMA-based models, we finetuned them using the LoRA (Hu et al. 2021) technique and used the prompt "Summarize sentence \{sentence \} in one word:" sparked by (Jiang et al. 2023). Results marked with $\dagger$ are obtained from (Reimers \& Gurevych, 2019), while results marked with $\ddagger$ are retrieved from (Gao et al. 2021). Additionally, results marked with $\star$ denote our own implementation using official code. For the remaining baselines, we refer to the corresponding original papers to obtain their results.

data $\left(x, x^{+}, x^{-}\right)$. This limitation might affect its performance. On the other hand, AnglE consistently outperforms SBERT, achieving an absolute gain of $5.52 \%$. This can support the idea that angle-optimized text embedding can mitigate the negative impact of the cosine function, resulting in better performance. Furthermore, we explore applying the long text model $\mathrm{RAN}_{\text {base }}$ ( $86 \mathrm{M}$ parameters) (Li et al. 2023) as the backbone to test the performance on long text. The results show that AnglE-BERT outperforms AnglE-RAN across all short text datasets. This advantage might be attributed to the larger parameter size of BERT and its proficiency in handling short texts. However, we observe a remarkable shift in long-text STS. AnglE-RAN outperforms AngIE-BERT in this scenario, suggesting that AnglE-RAN can handle long texts well despite having fewer parameters.

| Model | MRPC | STS-B | QQP | QNLI | GitHub Issues. | Avg. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | test | test | validation | validation | test |  |
| SimCSE-BERT | 48.13 | 76.27 | 65.84 | 33.00 | 60.38 | 56.72 |
| SBERT | 46.19 | 84.67 | 73.80 | 65.98 | 69.50 | 68.03 |
| AngIE-RAN | 58.70 | 80.23 | 74.87 | 63.04 | $\mathbf{7 1 . 2 5}$ | 69.62 |
| AngIE-BERT | $\mathbf{6 2 . 2 0}$ | $\mathbf{8 6 . 2 6}$ | $\mathbf{7 6 . 5 4}$ | $\mathbf{7 2 . 1 9}$ | 70.55 | $\mathbf{7 3 . 5 5}$ |

Table 3: Results on the STS tasks. All baseline results are our implementation using the official code. Spearman's correlation $(\rho \times 100)$ serves as the reported metric.

In short, this evidence suggests AngIE's superiority in transfer and non-transfer settings, its ability to produce high-quality text embeddings, and its robustness and adaptability to different backbones.

### 4.4 ABLATION STUDY

To gain a deeper understanding of AnglE, we conducted an ablation study examining different objectives and their effects. The results in table 4 indicate that AnglE shows improved performance with all three objectives. In particular, we observe that AnglE experiences a greater drop in performance without the angle objective than without the in-batch negative (ibn) objective. This suggests that angle optimization is more important than ibn in improving text embedding. Additionally, we find that using the angle objective alone yields performance close to that of using the cosine objective alone, demonstrating the effectiveness of angle optimization. We also evaluated five different pooling strategies and found that the "cls" strategy performed the best. Finally, we compared the ibn with/without identical sentence pair (ISP) detection and found that ibn without ISP detection has about $0.18 \%$ performance drop than with. This indicates that ibn with ISP detection is effective.

| Model | Spearman's Correlation |
| :--- | :---: |
| Objective |  |
| AnglE-BERT-all | $\mathbf{8 6 . 2 6}$ |
| - w/o ibn | 86.00 |
| - w/o angle | 85.30 |
| only cosine | 85.28 |
| only ibn | 72.48 |
| only angle | 85.15 |
| Pooling Strategy |  |
| cls | $\mathbf{8 6 . 2 6}$ |
| cls-last-avg | 85.81 |
| last-avg | 84.15 |
| last-max | 79.76 |
| first-last-avg | 81.99 |

Table 4: Ablation study of AnglE. The results are Spearman's correlations on the STS-B test set.
Table 5: Results of unsupervised and LLM supervised models on the STS-B test set. For ChatGPT, LLaMA, and ChatGLM, we use the gpt-turbo-3.5, 7B LLaMA2, and 6B ChatGLM, respectively.

| Model | Spearman's |
| :--- | :---: |
| Unsupervised Models |  |
| SimCSE-BERT | 76.85 |
| ConSERT-BERT | 73.97 |
| DiffCSE-BERT | 80.59 |
| LLM-supervised Models |  |
| AnglE-BERT + ChatGPT | 81.52 |
| AnglE-BERT + LLaMA | 79.29 |
| AngIE-BERT + ChatGLM | 81.11 |
| AngIE-BERT + Ensemble | $\mathbf{8 2 . 0 1}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-08.jpg?height=358&width=1247&top_left_y=1504&top_left_x=431)

Figure 4: The procedures of the LLM-supervised learning. For the STS task, we use the prompt "You are a highly smart same-meaning/opposite-meaning sentence-generating system. Your job is to generate $\{$ size $\}$ synonymous/antonym sentences of a given input sentence. Input sentence: $\{$ text $\}$. Output:" to generate positive/negative pairs. \{size\} and $\{$ text $\}$ are placeholders for the generated size and the input text, respectively.

### 4.5 DISCUSSION AND ANALYSIS

Discussion of LLM-supervised Learning AnglE, a supervised learning model, must be trained on labeled data. However, the limited availability of domain-supervised data poses a challenge in real-world applications. To overcome this problem, we propose LLM-supervised learning. This approach applies LLMs as data annotators to label the pseudo-supervised data for AnglE training. Figure 4 outlines the procedures involved in LLM-supervised learning. In this study, we compare the LLM-supervised AnglE and unsupervised contrastive learning models. To simulate domain application, we extract all "sentence1" texts from the STS-B train set and employ LLM-supervised learning to train the AnglE model. Table 5 shows the results on the STS-B test set. It is evident from

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-09.jpg?height=396&width=555&top_left_y=279&top_left_x=403)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_680054468d59702a3382g-09.jpg?height=388&width=656&top_left_y=283&top_left_x=1057)

(b)

Figure 5: (a) Density plots of cosine similarities between sentence pairs in the STS-B test set. The pairs have been categorized into 6 groups, reflecting the ground truth ratings (where higher ratings indicate a higher degree of similarity), visually represented on the $y$-axis. The $x$-axis represents the cosine similarity. (b) Density plots of golden scores between sentence pairs in the STS-B test set.

the results that LLM-supervised AnglE performs better than unsupervised contrastive baselines, and the ensemble of LLMs shows the best results. This evidence suggests the effectiveness of LLMsupervised learning and indicates that it can alleviate the domain-supervised data scarcity problem.

Discussion of Text Retrieval We also evaluate the performance of the text retrieval task by experimenting on the test split of the flickr30k dataset (Young et al. 2014). This dataset consists of five caption texts for each photo, and these texts are similar to each other. We use the first caption text vector to retrieve the top 5 similar sentences using faiss ${ }^{5}$. The strict accuracy ${ }^{6}$ of AngIE, SimCSE (supervised), and SBERT are $12.9 \%, 10.4 \%$, and $5.2 \%$, respectively. This evidence indicates the effectiveness of using AnglE for the retrieval task.

Discussion of Transfer Tasks In addition, we evaluate the performance of text embedding in transfer tasks. In particular, our approach involves training text embedding on STS tasks and then transferring it to seven other kinds of tasks. Notably, AnglE outperforms baselines, showing a significant improvement of $4.34 \%$ and $4.48 \%$ over DiffCSE and SimCSE, respectively. These results suggest that AnglE can produce better embeddings that effectively improve performance in various tasks. A more detailed description of the experiment can be found in section A.2.

Analysis of Text Embedding Distribution Figure 5a depicts the density plots of the cosine similarities between sentence pairs in the STS-B test set to provide an intuitive visualization of the text embedding quality. Figure 5 b displays the golden scores for the same sentence pairs. Analyzing the overall density of cosine similarities, we find that AnglE's distribution resembles the golden distribution more closely than SimCSE (supervised) and SBERT's. Figure 5b illustrates a peak in the $0-1$ range; however, only AnglE shows a distinct peak in this range in Figure $5 \mathrm{a}$ Also, Figure $5 \mathrm{~b}$ portrays a higher peak around 4 than around 4.8 in the 4-5 range, only AnglE demonstrates this feature properly in Figure $5 \mathrm{a}$ Notably, the 0-1 and 4-5 ranges in Figure $5 \mathrm{~b}$ represent two saturation zones of the cosine function. This evidence suggests that AnglE can mitigate the negative effect of the saturation zone. In conclusion, we can confidently assert that AnglE produces better text embeddings with a cosine similarity density closely resembling the actual distribution than the baselines.

## 5 CONCLUSION AND FUTURE WORK

In this paper, we have presented a novel text embedding model called AnglE, which optimizes the angle difference in complex space to overcome the adverse impact of the saturation zone of the cosine function, thereby improving text embeddings. To comprehensively evaluate the STS tasks, we have introduced the GitHub Issues Similarity Dataset to evaluate model performance on the[^3]long-text STS task. Furthermore, we have proposed an LLM-supervised learning method to cope with the scarcity of domain-supervised data. Extensive experimental results have demonstrated that AnglE outperforms baselines, indicating that AnglE can handle both short and long-text STS tasks and work effectively in various scenarios. In future work, we plan to explore the application of AnglE in real-world scenarios and provide further insights into AnglE.

## REFERENCES

Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 385-393. Association for Computational Linguistics, 2012.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pp. 32-43, Atlanta, Georgia, USA, 2013. Association for Computational Linguistics.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pp. 81-91. Association for Computational Linguistics, 2014.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pp. 252-263. Association for Computational Linguistics, 2015.

Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pp. 497-511. Association for Computational Linguistics, 2016.

Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pp. 41-46. Association for Computational Linguistics, July 2023.

Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 632-642. Association for Computational Linguistics, 2015.

Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylipää Hellqvist, and Magnus Sahlgren. Semantic re-tuning with contrastive tension. In International conference on learning representations, 2020.

Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1-14. Association for Computational Linguistics, August 2017.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Universal sentence encoder for English. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 169-174. Association for Computational Linguistics, 2018.

Sachin Chanchani and Ruihong Huang. Composition-contrastive learning for sentence embeddings. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pp. 15836-15848. Association for Computational Linguistics, 2023.

Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, and James Glass. DiffCSE: Difference-based contrastive learning for sentence embeddings. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4207-4218. Association for Computational Linguistics, 2022.

Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA), 2018.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670-680. Association for Computational Linguistics, 2017.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171-4186, 2019.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pp. 320-335, 2022.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894-6910. Association for Computational Linguistics, 2021.

John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 879-895. Association for Computational Linguistics, 2021.

Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271-21284, 2020.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 13671377. The Association for Computational Linguistics, 2016.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi Zhang. PromptBERT: Improving BERT sentence embeddings with prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 8826-8837. Association for Computational Linguistics, December 2022a. doi: 10.18653/v1/2022.emnlp-main.603. URL https://aclanthology.org/2022. emnlp-main. 603

Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling sentence embeddings with large language models. arXiv preprint arXiv:2307.16645, 2023.

Yuxin Jiang, Linhan Zhang, and Wei Wang. Improved universal sentence embeddings with promptbased contrastive learning and energy-based learning. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 3021-3035. Association for Computational Linguistics, 2022b.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, pp. 3294-3302, 2015.

Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence embeddings from pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9119-9130. Association for Computational Linguistics, 2020.

Xianming Li, Zongxi Li, Haoran Xie, and Qing Li. Merging statistical feature via adaptive gate for improved text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 13288-13296, 2021.

Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, and Qing Li. Recurrent attention networks for long-text modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3006-3019. Association for Computational Linguistics, 2023.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692

Wenpeng Lu, Xu Zhang, Huimin Lu, and Fangfang Li. Deep hierarchical encoding model for sentence semantic matching. Journal of Visual Communication and Image Representation, 71: $102794,2020$.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pp. 216-223. European Language Resources Association (ELRA), 2014.

Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In 27th Annual Conference on Neural Information Processing Systems 2013., pp. 3111-3119, 2013.

OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.

OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings using compositional n-gram features. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 528-540. Association for Computational Linguistics, 2018.

Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543. Association for Computational Linguistics, 2014.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pp. 3980-3990. Association for Computational Linguistics, 2019.

Jianlin Su. Cosent (1): A more effective sentence vector scheme than sentence bert, Jan 2022. URL https://kexue.fm/archives/8847

Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. Whitening sentence representations for better semantics and faster retrieval. arXiv preprint arXiv:2103.15316, 2021.

Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019.

Varsha Suresh and Desmond Ong. Not all negatives are equal: Label-aware contrastive loss for fine-grained text classification. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 4381-4394. Association for Computational Linguistics, 2021.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112-1122, New Orleans, Louisiana, 2018. Association for Computational Linguistics.

Lingling Xu, Haoran Xie, Zongxi Li, Fu Lee Wang, Weiming Wang, and Qing Li. Contrastive learning models for sentence representations. ACM Trans. Intell. Syst. Technol., 14(4), jun 2023. ISSN 2157-6904. doi: 10.1145/3593590. URL https://doi.org/10.1145/3593590

Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. Consert: A contrastive framework for self-supervised sentence representation transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pp. 5065-5075. Association for Computational Linguistics, 2021.

Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78, 2014.

Zhenrui Yue, Bernhard Kratzwald, and Stefan Feuerriegel. Contrastive domain adaptation for question answering using limited text corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 9575-9593. Association for Computational Linguistics, November 2021.

Han Zhang, Zongxi Li, Haoran Xie, Raymond YK Lau, Gary Cheng, Qing Li, and Dian Zhang. Leveraging statistical information in fine-grained financial sentiment analysis. World Wide Web, 25(2):513-531, 2022.

Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. An unsupervised sentence embedding method by mutual information maximization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1601-1610. Association for Computational Linguistics, 2020.

Wenjie Zhuo, Yifan Sun, Xiaohan Wang, Linchao Zhu, and Yi Yang. WhitenedCSE: Whiteningbased contrastive learning of sentence embeddings. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pp. 12135-12148. Association for Computational Linguistics, 2023.
