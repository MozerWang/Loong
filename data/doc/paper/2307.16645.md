# Scaling Sentence Embeddings with Large Language Models 

Ting Jiang Shaohan Huang Zhongzhi Luan<br>Deqing Wang ${ }^{\dagger} \quad$ Fuzhen Zhuang<br>Beihang University<br>royokong@uaa.edu.cn


#### Abstract

Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling_sentemb.


## 1 Introduction

Sentence embeddings is a fundamental problem in natural language processing, requiring language models to project sentences into a vector space based on their semantics. Current methods based on contrastive learning, such as SimCSE [GYC21], have successfully leveraged pretrained language models to generate high-quality embeddings. A significant amount of research has been devoted to refining the contrastive learning framework in order to further improve sentence embeddings $\left[\mathrm{CDL}^{+} 22\right.$, $\left.\mathrm{WTS}^{+} 22, \mathrm{WGL}^{+} 22, \mathrm{CYS}^{+} 23\right]$.

Recently, large language models (LLMs), such as GPT-3 [BMR \${ }^{+}\$20] and LLaMA [TLI \${ }^{+}\$23], have demonstrated significant potential on various natural language processing tasks such as translation, question answering, and text classification. Current research has also explored the application of LLMs for data augmentation in sentence embeddings. By generating better sentence pairs for contrastive learning, LLMs can help alleviate the scarcity of labeled data [CYS ${ }^{+} 23$, ZLH23]. However, directly utilizing LLMs to generate sentence embeddings presents two primary challenges. Firstly, LLMs, as autoregressive models, produce text instead of vectors, which necessitates vectorizing the output. Secondly, it is crucial to determine an effective approach for incorporating the capabilities of in-context learning into sentence embeddings.

In this work, we aim to investigate the capabilities of current LLMs for sentence embeddings, facilitated by the availability of open-source LLMs $\left[\mathrm{TLI}^{+} 23, \mathrm{ZRG}^{+} 22\right]$. We address the following[^0]research questions: 1) How can LLMs be used to represent sentence embeddings, and does prompt engineering, as demonstrated by PromptBERT [JJH \${ }^{+}\$22]? 2) Can in-context learning [LYF \${ }^{+}\$23] enhance the quality of sentence embeddings? 3) Does the scaling up the model parameters stil work when the number of parameters exceeds billions? 4) What improvements can be achieved by incorporating the current contrastive learning framework into LLMs?

To address these questions, we conduct a systematic study by evaluating LLaMA [TLI \${ }^{+}\$23] and OPT \$\left[\mathrm{ZRG}^{+}\right.\$22] on both semantic textual similarity (STS) tasks and transfer tasks. Following \$\left[\mathrm{JJH}^{+}\right.\$22], we utilize a prompt such as This sentence: " [text] " means to enable LLMs to generate sentence embeddings, where [text] serves as the input slot. This method outperforms traditional representation methods, such as averaging output tokens to represent sentences. Considering the causal architecture and pretraining tasks of LLMs compared to BERT, we can refine the prompt to generate better representations by instructing LLMs to encapsulate as much semantic information of the sentences as possible within the target token.

Inspired by [TST21], which uses definition sentences from a word dictionary to learn sentence embeddings, we find that performance can be further improved by adding definition sentences and corresponding words as examples to perform in-context learning. To mitigate the gap between examples and input sentences, we also use sentences from the STS-B [CDA \${ }^{+}\$17] training set as examples by instructing ChatGPT to generate a single word to represent the meaning of sentences. By evaluating the demonstration examples based on the STS-B development set, LLMs can outperform previous contrastive learning-based sentence models, which were fine-tuned on unsupervised data.

By scaling up the parameters of LLMs, we find that transitioning from millions to billions of parameters results in improvements on STS tasks. However, continue scaling up may not yield further improvements. Even with in-context learning, 66B OPT still underperforms 6.7B OPT on STS tasks. Nonetheless, scaling up improves performance on transfer tasks. LLMs with tens of billions parameters exhibit strong performances, achieving state-of-the-art performance even without any fine-tuning.

With the advancement of parameter-efficient fine-tuning techniques[HSW $\left.{ }^{+} 21, \mathrm{DPHZ} 23\right]$ and posttraining quantization methods[FAHA22], we can also fine-tune LLMs with large batch sizes to conduct contrastive learning, even with limited computational resources. For instance, fine-tuning 7B parameter LLMs can be accomplished using the same hardware employed for previous BERT-based models like SimCSE [GYC21]. Even without fine-tuning the full parameters and using the 4-bit quantized method [DPHZ23], 2.7B OPT with our sentence embeddings method outperforms a 4.8B ST5 \$\left[\mathrm{NÁC}^{+}\right.\$21] and achieves the state-of-the-art results on STS tasks.

Our main contributions are as follows:

1. We propose a sentence embeddings method that leverages LLMs to enhance the representation of sentences. Additionally, we incorporate in-context learning to further improve the quality of sentence embeddings. Our approach demonstrates that LLMs can generate high-quality sentence embeddings without the need for fine-tuning.
2. We conduct an analysis of scaling up the parameters of LLMs from millions to tens of billions in sentence embeddings. We observe scaling to more than tens of billion parameters may harm the performance on STS tasks. However, the largest model can outperform other counterparts on transfer tasks.
3. Based on our method, we discover that performance can be further enhanced by employing contrastive learning. By adopting efficient fine-tuning techniques, LLMs achieve state-ofthe-art performance on STS tasks, even with limited computational resources.

## 2 Related Work

Sentence Embeddings Sentence embeddings is to convert a sentence into a fixed-size vector, which captures the semantic meaning and context of the sentence. It allows for the efficient retrieval of similar sentences through the similarity between vectors. Recently, SimCSE [GYC21] demonstrated that contrastive learning is an effective approach for learning sentence embeddings using BERT in both unsupervised and supervised settings. In the unsupervised setting, SimCSE predicts the input sentence itself from in-batch negatives, with different dropout \$\left[\mathrm{SHK}^{+}\right.\$14] masks applied. In the supervised
setting, Natural Language Inference (NLI) datasets $\left[\mathrm{CKS}^{+} 17, \mathrm{RG} 19\right]$ are used to provide positive and negative pairs. Following the success of SimCSE, there has been a surge of work exploring contrastive learning-based methods. DiffCSE \$\left[\mathrm{CDL}^{+}\right.\$22] incorporates a replaced token detection loss into the contrastive learning framework. PromptBERT \$\left[\mathrm{JJH}^{+}\right.\$22] reveals that prompts can enhance BERT's ability to represent sentences. Additionally, several studies [CYS \${ }^{+}\$23, ZLH23] have investigated data augmentation for sentence embeddings using LLMs. SentenceT5 (ST5) [NÁC \${ }^{+}\$21] leverages the encoder-decoder structure of models, such as T5 \$\left[\mathrm{RSR}^{+}\right.\$20], for generating sentence embeddings and demonstrates improvements by scaling T5 from millions to billions of parameters. However, directly using large language models (LLMs) to generate sentence embeddings remains an area of ongoing research.

Large Language Models LLMs [ \$\mathrm{ZRG}^{+}\$22, SAW22, $\left.\mathrm{CND}^{+} 22, \mathrm{TLI}^{+} 23\right]$ recently show impressive performance on various natural language process, benefiting from their large parameter sizes compared to previous pretrained language models. LLMs can efficiently learn a new task with in-context learning by using training data as demonstrations \$\left[\mathrm{BMR}^{+}\right.\$20]. Without any gradient updates, LLMs with in-context learning can solve challenging tasks like multitask language understanding [ \$\mathrm{HBB}^{+}\$20], commonsense reasoning [LHE21], and math problems [CKB \${ }^{+}\$21]. This performance can be further improved by scaling up language models $\left[\mathrm{HBM}^{+} 22, \mathrm{KMH}^{+} 20\right]$.

## 3 Methodology

In this section, we first discuss current sentence embeddings methods with LLMs, and then introduce a new Prompt-based method with Explicit One word Limitation (PromptEOL) for LLMs in Section 3.1. Based on this method, we describe two settings: without and with fine-tuning. For the setting without fine-tuning, we utilize the in-context learning ability of LLMs to enhance sentence embeddings. To address the issue of lacking textual outputs, we propose two methods to automatically generate demonstrations for in-context learning in Section 3.2. For the setting with fine-tuning, we employ contrastive learning framework, and combine it with the efficient fine-tuning method to alleviate substantial memory requirement in Section 3.3.

### 3.1 Represent Sentence with LLMs

Previous works \$\left[\mathrm{LZH}^{+}\right.\$20, SCLO21, \$\mathrm{JJH}^{+}\$22] have extensively studied on improving sentence embeddings from encoder-based pretrained models, like BERT without fine-tuning. Recently, PromptBERT \$\left[\mathrm{JJH}^{+}\right.\$22] leverage a prompt-based method to represent sentence. It uses manual templates like This sentence: " [text] " means [MASK]., where [text] is the placeholder for a sentence. The output vector of [MASK] token is used as sentence embeddings. It demonstrates superior results compared to previous sentence representation methods like averaging output hidden vectors or the output vector of [CLS] token.

Considering to LLMs as autoregression models, which do not have special tokens like [CLS] or [MASK], we modify the prompt-based method in [JJH \${ }^{+}\$22] to make it compatible with LLMs We use This sentence: " [text] " means to prompt LLMs generate next token and extract the hidden vectors of the final token as sentence embeddings. To validate the prompt-based method with LLMs, we compare it with two other methods, such as averaging or using the last token as sentence embeddings. For LLMs, we use OPT [ZRG \${ }^{+}\$22] from 125 million parameters to 66 billions and evaluate it on STSB development set in Figure 1. Following the results in \$\left[\mathrm{JJH}^{+}\right.\$22], we observe that promptbased method can enhance sentence representation across all OPTs, ranging from millions to billions parameters. Despite that the previous

![](https://cdn.mathpix.com/cropped/2024_06_04_eaa668ce392e3280de53g-03.jpg?height=526&width=613&top_left_y=1838&top_left_x=1100)

Figure 1: Performances of OPT in STS-B development set with three representation methods. Dash lines represent the results of BERT.

![](https://cdn.mathpix.com/cropped/2024_06_04_eaa668ce392e3280de53g-04.jpg?height=591&width=1391&top_left_y=209&top_left_x=367)

Figure 2: An illustration of in-context learning based sentence embeddings. The green sentences denote the demonstration sentence, and the blue words denote the demonstration words. The corresponding color blocks refer to their slots in the template.

prompt-based method also improved LLMs like OPT on sentence representations, OPT, even with significantly more parameters, still fails to outperform BERT.

Consider to bidirectional attention in BERT, we hypothesize that BERT can implicitly condense the entire semantic information corresponding to a sentence into a single [MASK] token when using templates like "This sentence: " [text] " means [MASK].". Since the [MASK] token follows a period, this implicitly restricts BERT to explain meaning into one word. However, this template fails to add the similar "one word limitation" when it is used in autoregression models like OPT with unidirectional attention. To validate this, we simply remove the period in template to transfer it into "This sentence: " [text] " means [MASK]". Despite only one word difference, and no modification to meaning of the template, the performance of BERT on STS-B development set plummeted from 73.44 to 33.89 Spearman correlation, which means BERT without this implicit "one word limitation" fails to represent sentence.

Inspired by this, our objective is to enhance prompt-based method for LLMs by introducing a "one word limitation". We propose a new Prompt-based method with Explicit One word Limitation (PromptEOL) for LLMs. PromptEOL is simple and straightforward by directly adding some tokens in the template to instruct LLMs in predicting the meaning of sentence in one word. The template we used after modification is following:

This sentence: " [text] " means in one word: "

Compared to the template in $\left[\mathrm{JJH}^{+} 22\right]$, we introduce two simple modifications for LLMs. First, we append in one word to the prompt to constrain LLMs in predicting semantic information in next token. Secondly, we incorporate : " at the end of template to prevent model form generating punctuations in next token, as This sentence: " is used to indicate the input of a sentence. We find this template improve all OPT models and allow them to match or even outperform BERT with prompt-based method in Figure 4.

### 3.2 Improve Sentence Embeddings with In-context Learning

In-context learning is widely utilized as an effective method to help LLMs understand problems. It improves their comprehension of inputs and outputs by directly adding a few examples in the prompts. However, when considering the problem of sentence embeddings, we need to project sentences into vectors based on their semantic information, separately. In other word, sentence embeddings lack textual outputs that could be used as examples to perform in-context learning, such as answers for question answer problems or labels for text classification problems. Moreover, there are also no predetermined gold vectors for a given sentence.

To leverage in-context learning in sentence embeddings, we propose an framework to automatically build demonstration sets and search demonstration to improve LLMs sentence embeddings in Figure 2.
![](https://cdn.mathpix.com/cropped/2024_06_04_eaa668ce392e3280de53g-05.jpg?height=622&width=1392&top_left_y=194&top_left_x=366)

Figure 3: Distribution of Spearman correlations on the STS-B development set with different incontext learning demonstrations. The red dash line represents the Spearman correlation of the corresponding model without any demonstration. The blue area represents demonstrations that negatively impact the performance, and the percentage refers to the proportion of these demonstrations to the total number of demonstrations.

For the demonstration set, the goal is to create sentence and word pairs, where the word can represents the semantic information of the sentence. We propose two methods to generate pairs.

The first method involves using ChatGPT to generate corresponding words according to the semantic information of given sentences from STS-B training set. By asking ChatGPT with same template in Figure 2, ChatGPT outputs one word summary for the given sentence. We also find "one word limitation" in Section 3.1 is important for ChatGPT. Consider to our prompt-based representation method, we employ the hidden state of the next token as the sentence embeddings. By removing in one word from the template, it tends to explain the meaning of a sentence in a lengthy way, and the first word often becomes an article such as "The", which lacks clear meaning. For example, given the sentence "A jockey riding a horse.", the hidden state achieves the highest dot product similarity for "Equestrain" among its word embeddings. However, without "one word limitation", it will achieve the highest dot product similarity for word without specific meaning such as "The" among its word embeddings, which can not represent sentence properly. Inspired by DefSent [TST21], which leverages definition sentences with their words as labels to train unsupervised sentence embedding, our second method is also based on a word dictionary. We directly use words and their definition sentences in the Oxford dictionary as word-sentence pairs.

Based on these methods, we construct a demonstration set consisting of 300 pairs of sentences and words. 100 pairs are from STS-B training set, with words labeled by ChatGPT, while the remaining are from the Oxford dictionary. To find demonstration that help model to represent sentences, we directly evaluate each demonstration on the STS-B development set and use the demonstration with the best Spearman correlation as the demonstration for corresponding models. We also visualize the distribution of Spearman correlations for OPT from 125M to 66B parameters in Figure 3. Following the previous study $\left[\mathrm{KMH}^{+} 20\right]$, we notice that in-context learning achieves better performance, when increasing model parameter from $125 \mathrm{M}$ to $2.7 \mathrm{~B}$. For example, there are only one demonstration that helps the $125 \mathrm{M}$ OPT achieve better performance compared to without demonstration. However, around $98 \%$ of demonstrations improve the performance of the $2.7 \mathrm{~B}$ OPT. In-context learning significantly enhance the sentence embeddings, especially for OPT with more than 1B parameters. With only in-context learning, OPT with more than 1.3B parameters even achieve better results on STS tasks compared to contrastive learning based method like SimCSE [GYC21] in Table 1.

### 3.3 Contrastive Learning with Efficient Fine-tuning

Since in-context learning boosts sentence embeddings performances without any gradient update, we also exploit contrastive learning on LLMs, which has been demonstrated as an efficient way to learn sentence embeddings [GYC21]. It can be divided into unsupervised and supervised settings, according to the datasets. For unsupervised setting, the sentences in dataset lack corresponding
positive and negative sentences to perform contrastive learning. For supervised setting, natural language inference (NLI) datasets are used as the datasets, and each sentence has corresponding positive and negative sentences. In this section, we focus on the supervised setting to fully leverage LLMs for sentence embeddings.

However, contrastive learning requires a large batch size to increase the number of negative samples, which demands a high amount of GPU memory, especially in the supervised setting. For example, SimCSE uses a batch size of 512 to fine-tune 110M BERT in the supervised setting. Each batch includes 1536 sentences, containing both their positive and hard negative sentences. It requires 58GB of GPU memory on 4 GPUs. As a result, fine-tuning LLMs with contrastive learning becomes challenging due to the memory requirements, particularly for models with significantly larger parameter sizes than BERT.

To solve this problem, we leverage current efficient fine-tuning method QLoRA [DPHZ23]. QLoRA combines two techniques to significantly reduces memory usage: 4-bit quantization and parameter efficient fine-tuning. Quantization reduces the memory usage of LLMs by quantizing their weight from 16-bit to 4-bit. Parameter efficient fine-tuning with LoRA [HSW $\left.{ }^{+} 21\right]$ significantly reduces the memory usage of optimizer compared to full fine-tuning by only fine-tuning small proportion of weight.

Following [GYC21], we use SNLI and MNLI datasets where each sentence $x_{i}$ has corresponding a positive sentence $x_{i}^{+}$and a hard negative sentence $x_{i}^{-}$. To represent sentence, we use our promptbased method in Section 3.1. Formally, given sentence $x_{i}$, we first add $x_{i}$ to the template and get hidden states

$$
\begin{equation*}
\mathbf{h}_{i 1}, \ldots, \mathbf{h}_{i l}=\operatorname{LLM}\left(\text { This sentence: " } x_{i}\right. \text { " means in one word: ") } \tag{1}
\end{equation*}
$$

where $l$ is the number of hidden states. We then use last token hidden state as its sentence embedding $\mathbf{h}_{i}=\mathbf{h}_{i l}$. Since we can represent the sentence pair $\left(x_{i}, x_{i}^{+}, x_{i}^{-}\right)$to their embeddings $\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}, \mathbf{h}_{i}^{-}\right)$. Our training objective is following:

$$
\begin{equation*}
\ell_{i}=-\log \frac{e^{\cos \left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\cos \left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}+e^{\cos \left(\mathbf{h}_{i}, \mathbf{h}_{j}^{-}\right) / \tau}\right)} \tag{2}
\end{equation*}
$$

where $N$ is the batch size and $\tau$ is the temperature hyperparameter in contrastive learning.

## 4 Experiment

### 4.1 Implementation Details

For the setting without fine-tuning, we use OPT from 125M to 66B parameters, and LLaMA from 7B to 65B parameters. All models use the same template in Section 3.1. We use 300 pairs of sentences and words as demonstration set for in-context learning. Among these, 100 pairs are from the STS-B training set, and we use gpt-3.5-turbo to label their words. The remaining 200 pairs are from the Oxford dictionary. We provide all demonstrations in Appendix A. For each model, we choose only one demonstration that has the highest Spearman correlation on the STS-B development set as their demonstration for evaluation. All results from models with 16-bit weights. We also present results using quantization methods in Appendix B.

For the setting with fine-tuning, we use QLoRA [DPHZ23] to fine-tune OPT and LLaMA with contrastive learning. Following QLoRA, we use LoRA $r=64, \alpha=16$, dropout $=0.05$, and add LoRA modules on all linear layers of the 4-bit quantized model. We fine-tune models on the NLI datasets [GYC21] with one epoch, temperature $\tau=0.5$ and learning rate 5e-4. Due to hardware limitations, we only conduct our experiments with model parameters less than or equal to $13 \mathrm{~B}$ with 8 RTX-3090 GPUs. For models with fewer than 7B parameters, we fine-tune them on 2 GPUs with a batch size of 256 . For 7B models, we use 4 GPUs with a batch size of 256 . For 13B models, we use 8 GPUs with a batch size of 200 .

| Method | Params | STS12 | STS13 | STS14 | STS15 | STS16 | STS-B | SICK-R | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Fine-tuning on unsupervised datasets |  |  |  |  |  |  |  |  |  |
| SimCSE-BERT $^{\dagger}$ | $110 \mathrm{M}$ | 68.40 | 82.41 | 74.38 | 80.91 | 78.56 | 76.85 | 72.23 | 76.25 |
| SimCSE-RoBERTa $^{\dagger}$ | $123 \mathrm{M}$ | 70.16 | 81.77 | 73.24 | $81.36 \quad$ | 80.65 | 80.22 | 68.56 | $76.57 \quad$ |
| PromptBERT ${ }^{\ddagger}$ | $110 \mathrm{M}$ | 71.56 | 84.58 | 76.98 | $84.47 \quad$ | 80.60 | 81.60 | 69.87 | $78.54 \quad$ |
| PromptRoBERTa ${ }^{\ddagger}$ | $123 \mathrm{M}$ | 73.94 | 84.74 | 77.28 | 84.99 | 81.74 | 81.88 | 69.50 | 79.15 |
| Without fine-tuning |  |  |  |  |  |  |  |  |  |
| BERT avg. ${ }^{\dagger}$ | $110 \mathrm{M}$ | 30.87 | 59.89 | 47.73 | 60.29 | 63.73 | 47.29 | 58.22 | 52.57 |
| BERT prompt ${ }^{\ddagger}$ | $110 \mathrm{M}$ | 60.96 | 73.83 | 62.18 | 71.54 | 68.68 | 70.60 | 67.16 | $67.85 \quad$ |
| ST5-Enc ${ }^{\S}$ | $4.8 \mathrm{~B}$ | 34.97 | 60.19 | 47.59 | 66.40 | 70.62 | 62.83 | 63.57 | 58.02 |
| PromptEOL <br> OPT | $\overline{125 \mathrm{M}}$ | 59.90 | 71.55 | 60.93 | 70.76 | 72.83 | 67.89 | $\overline{65.14}$ | $\overline{67.00}$ |
|  | $350 \mathrm{M}$ | 54.70 | 71.52 | 59.99 | 64.51 | 71.39 | 66.55 | 66.58 | ![](https://cdn.mathpix.com/cropped/2024_06_04_eaa668ce392e3280de53g-07.jpg?height=43&width=77&top_left_y=730&top_left_x=1647) |
|  | $1.3 \mathrm{~B} \quad$ | 64.59 | 79.06 | 68.46 | 78.88 | 78.64 | 73.22 | 69.41 | $73.18 \quad$ |
|  | $2.7 \mathrm{~B} \quad$ | 60.03 | 75.51 | 64.30 | 74.56 | 77.62 | 67.73 | 65.35 | $69.30 \quad$ |
|  | $6.7 \mathrm{~B}$ | 60.91 | 80.05 | 67.65 | 75.49 | 80.11 | 72.91 | 67.57 | $72.10 \quad$ |
|  | 13B | 60.21 | 81.36 | 69.69 | 75.46 | 79.58 | 70.73 | 65.99 | $71.86 \quad$ |
|  | 30B | 59.99 | 80.52 | 69.80 | 75.20 | 78.03 | 73.57 | 69.87 | 72.43 |
|  | 66B | 55.66 | 74.62 | 64.90 | 72.34 | 75.21 | 71.72 | 67.43 | 68.84 |
| PromptEOL+ICL <br> OPT | $125 \mathrm{M}$ | 62.22 | 73.10 | 61.84 | 71.09 | 72.08 | 67.80 | 64.10 | 67.46 |
|  | $350 \mathrm{M}$ | 63.87 | 73.85 | 63.41 | 72.45 | 73.13 | 70.84 | 65.61 | 69.02 |
|  | $1.3 \mathrm{~B}$ | 72.78 | 83.77 | 73.61 | 83.42 | 80.60 | 78.80 | 69.69 | 77.52 |
|  | $2.7 \mathrm{~B}$ | 68.49 | 84.72 | 75.15 | 83.62 | 81.34 | 80.94 | 72.97 | 78.18 |
|  | $6.7 \mathrm{~B}$ | 70.65 | 84.51 | 75.01 | 83.51 | 82.00 | 81.12 | 76.77 | 79.08 |
|  | $13 \mathrm{~B}$ | 71.99 | 85.22 | 76.04 | 82.23 | 81.38 | 81.42 | 75.00 | 79.04 |
|  | 30B | 69.99 | 83.35 | 74.75 | 83.14 | 82.42 | 81.45 | 77.46 | 78.94 |
|  | 66B | 69.93 | 83.29 | 74.88 | 80.10 | 81.11 | 81.76 | 76.26 | 78.19 |

Table 1: Performances of our method on STS tasks without fine-tuning. ICL denotes in-context learning with our demonstration set. †: results from [GYC21]. ‡: results from [JJH \${ }^{+}\$22]. §: results from $\left[\mathrm{NÁ} \mathrm{C}^{+} 21\right]$.

### 4.2 Dataset

Following previous works [GYC21, \$\mathrm{JJH}^{+}\$22], We use the SentEval toolkit [CK18] to conduct our experiments on seven STS datasets and seven transfer learning datasets. The STS datasets include STS tasks 2012-2016 [ACDGA12, \$\mathrm{ACD}^{+} 13, \mathrm{ABC}^{+} 14, \mathrm{ABC}^{+} 15, \mathrm{ABC}^{+}\$16] STS-B [CDA \${ }^{+}\$17], SICK-R \$\left[\mathrm{MMB}^{+}\right.\$14]. Sentence pairs in each STS dataset are scored from 0 to 5 to indicate semantic similarity. Spearman correlation is used as a metric to evaluate the correlation between the cosine similarity of sentence embeddings and the golden similarity scores. The transfer learning datasets include MR [PL05], CR [HL04], SUBJ [PL04], MPQA [WWC05], SST-2 [SPW \${ }^{+}\$13], TREC [VT00] and MRPC [DB05]. Sentence embeddings are used as input feature to train corresponding logistic regression classification.

### 4.3 Results

We compare our method with BERT-based methods such as SBERT [RG19], SimCSE [GYC21], and PromptBERT \$\left[\mathrm{JJH}^{+}\right.\$22]. In addition, we include other sentence methods based on LLMs as baselines, such as ST5 $\left[\right.$ NÁC \${ }^{+}\$21] and SGPT [Mue22]. Among these baselines, ST5 achieves state-of-the-art results on both STS and transfer learning tasks by further fine-tuning 4.8B parameters T5 encoder with contrastive learning.

STS tasks without fine-tuning Table 1 shows the results of PromptEOL with and without incontext learning on STS tasks. Even without corresponding textual outputs for sentence embeddings, in-context learning still helps model to generate better embeddings. As the model size grows, improvements from in-context learning also increase. Moreover, in-context learning shows significantly improvements on STS tasks for model with more than billions parameters. For instances, it raises the Spearman correlation from 68.84 to 78.19 on 66B OPT. Our method with in-context learning also outperforms among methods without fine-tuning. Even if we do not use any method to avoid

| Method | Params | STS12 | STS13 | STS14 | STS15 | STS16 | STS-B | SICK-R | Avg. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Fine-tuning on supervised datasets |  |  |  |  |  |  |  |  |  |
| SBERT-NL $^{\dagger}$ | $220 \mathrm{M}$ | 72.27 | 78.46 | 74.90 | 80.99 | 76.25 | 79.23 | 73.75 | 76.55 |
| SimCSE-RoBERTa $^{\dagger}$ | $123 \mathrm{M}$ | 76.53 | 85.21 | 80.95 | 86.03 | 82.57 | 85.83 | 80.50 | 82.52 |
| PromptRoBERTa $^{\ddagger}$ | 1254M | 77.46 | 87.27 | 82.36 | 86.66 | 83.93 | 86.70 | 81.95 | 83.76 |
| SGPT $^{\boldsymbol{q}}$ | 123. | 76.75 | 85.93 | 82.28 | 86.69 | 82.80 | 86.14 | 80.04 | 82.95 |
| ST5-Enc $^{\S}$ | 5.8B | 74.28 | 85.35 | 79.21 | 85.52 | 82.54 | 85.50 | 79.53 | 81.70 |
|  | 4.8B | 80.10 | 88.75 | 84.70 | 88.86 | 85.17 | 86.77 | 80.39 | 84.96 |
| PromptEOL+CSE | 1.3B | 79.01 | 89.26 | 84.10 | 88.30 | 84.62 | 87.71 | 80.52 | 84.79 |
| OPT | 2.7B | 79.49 | 89.64 | 84.80 | 89.51 | 85.91 | 88.33 | 81.64 | 85.62 |
|  | 13B | 80.14 | 90.02 | 84.94 | 89.78 | 85.84 | 88.75 | 81.29 | 85.82 |
| PromptEOL+CSE | 7B | 79.16 | 90.22 | 85.40 | 88.99 | 86.25 | 88.37 | 81.51 | 85.70 |
| LLaMA | 13B | 78.63 | 90.03 | 85.46 | 89.48 | 86.18 | 88.45 | 82.69 | 85.85 |

Table 2: Performances of our method on STS tasks with fine-tuning. CSE denotes contrastive learning for sentence embeddings. †: results from [GYC21]. §: results from [NÁC \${ }^{+}\$21]. 『: results from evaluation the public checkpoint [Mue22] on STS tasks.

| Method | Params | MR | CR | SUBJ | MPQA | SST | TREC | MRPC | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Fine-tuning on supervised datasets |  |  |  |  |  |  |  |  |  |
| $\operatorname{SimCSE-RoBERTa~}^{\dagger}$ | $123 \mathrm{M}$ | 84.92 | 92.00 | 94.11 | 89.82 | 91.27 | 88.80 | 75.65 | 88.08 |
|  | $220 \mathrm{M}$ | 81.12 | 92.37 | 95.11 | 90.49 | 92.75 | 91.80 | 76.64 | 89.61 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_eaa668ce392e3280de53g-08.jpg?height=84&width=251&top_left_y=1189&top_left_x=390) | $123 \mathrm{M}$ | 85.74 | $91.47 \quad$ | 94.81 | 90.93 | 92.53 | 90.40 | 77.10 | 89.00 |
|  | $4.8 \mathrm{~B}$ | 90.83 | 94.44 | 96.33 | 91.68 | 94.84 | 95.40 | 77.91 | 91.63 |
| Without fine-tuning |  |  |  |  |  |  |  |  |  |
| BERT avg. <br> ST5-Enc ${ }^{\S}$ | $110 \mathrm{M}$ | 78.66 | 86.25 | $94.37 \quad$ | 88.66 | 84.40 | 92.80 | 69.54 | 84.94 |
|  | $4.8 \mathrm{~B}$ | 91.15 | 93.33 | 97.55 | 90.20 | 94.07 | 94.40 | 74.26 | 90.71 |
| PromptEOL <br> OPT | $1.3 \mathrm{~B}$ | 88.06 | 91.55 | 95.90 | 91.55 | 93.08 | 95.00 | 73.97 | 89.87 |
|  | $2.7 \mathrm{~B}$ | 88.83 | 92.29 | 95.93 | 91.76 | 94.62 | 96.00 | 75.94 | 90.77 |
|  | $6.7 \mathrm{~B}$ | 90.26 | 92.50 | 96.67 | 91.39 | 94.67 | 96.00 | 77.91 | 91.34 |
|  | 13B | 90.73 | 92.90 | 96.69 | 91.48 | 94.01 | 96.80 | 75.59 | 91.17 |
|  | $30 \mathrm{~B}$ | 90.95 | 92.77 | 96.99 | 91.79 | 95.28 | 97.00 | 73.97 | 91.25 |
|  | $66 \mathrm{~B}$ | 90.96 | 93.40 | 97.01 | 91.93 | 95.22 | 96.40 | 75.25 | 91.45 |
| PromptEOL <br> LLaMA | 7B | 90.40 | 92.90 | 96.88 | 91.57 | 95.11 | 95.40 | 75.13 | 91.06 |
|  | 13B | 92.02 | 93.22 | 97.29 | 91.40 | 95.66 | 95.80 | 76.46 | 91.69 |
|  | 30B | 91.64 | 93.27 | 97.10 | 91.86 | 95.99 | 95.80 | 78.43 | 92.01 |
|  | 65B | 92.13 | 93.43 | 97.16 | 91.91 | 95.33 | 97.40 | 77.28 | 92.09 |

Table 3: Performances of our method on transfer learning tasks. $\dagger$ : results from [GYC21]. ‡: results from \$\left[J \mathrm{JJ}^{+}\right.\$22]. §: results from \$\left[\mathrm{NÁC}^{+}\right.\$21].

anisotropy [Eth19], which is widely regarded as the main reason for poor performance on STS tasks [GYC21, NÁC \${ }^{+}\$21], our method still outperforms unsupervised methods such as SimCSE and PromptBERT, which use contrastive learning to avoid anistoropy. Additionally, we find the performance is not sensitive to the model size while scaling model beyond a billion parameters. Smaller models, such as 1.3B OPT, even outperforms SimCSE without fine-tuning.

STS tasks with fine-tuning Table 2 shows the results by fine-tuning with PromptEOL on the supervised dataset. Compared to ST5-Enc, which fine-tuned all 4.8B parameters on Community QA and NLI datasets, our method with 2.7B OPT achieves superior results through parameter-efficient fine tuning on the 4-bit model with only NLI datasets. Keep scaling up the parameters size, 13B OPT and LLaMA achieve the best performance on STS tasks. However, the improvement in scaling model parameters from 2.7B to 13B is not significant.

Transfer tasks We also report the results of our method on the transfer learning tasks in Table 3 . Unlike STS tasks, we observe that LLMs achieve better performance as the model size increases.

Specifically, the 66B OPT and 65B LLaMA models outperform their smaller counterparts with our representation method. Based on our representation method, LLMs show good performance without in-context learning and contrastive learning. Following ST5 [NÁC + 21], we find that applying contrastive learning solely on NLI datasets can even harm performance on transfer tasks. To solve this problem, ST5 utilizes additional datasets, such as the Community QA dataset, to enhance its performance in transfer tasks. For in-context learning, as it is widely used in text classification, we find that using examples not relevant to tasks, such as STS-B or the dictionary, does not enhance transfer task performance. We present these results in Appendix C.

## 5 Analysis

### 5.1 Sentence Representation Methods

We present the results obtained using three sentence representation methods, across models ranging in size from 125M to 66B parameters, as shown in Figure 4. Different representation methods can yield significantly different results. Prompt-based methods outperform direct averaging in three settings. Among these methods, PromptEOL exhibits the best performance, as it introduces an explicit "one-word limitation". More detail results can be find in Appendix D.
![](https://cdn.mathpix.com/cropped/2024_06_04_eaa668ce392e3280de53g-09.jpg?height=404&width=1390&top_left_y=996&top_left_x=365)

Figure 4: Influence of different sentence representation methods on three settings. "avg." refers to use averaging output tokens as sentence embeddings. "prompt" refers to extract sentence embeddings using the template from \$\left[\mathrm{JJH}^{+}\right.\$22] . Dash lines represent the results from the base-size BERT.

### 5.2 In-context Learning

We demonstrate in-context learning examples that were obtained from each model on the STSB development set, along with corresponding improvements on Spearman correlation for STS tasks. As the size of the model increases to 2.7B, the improvements in in-context learning become more and more pronounced, and related examples are usually more implicit. For instance, the $125 \mathrm{M}$ OPT uses examples where words are incorporated within the sentence.

|  | Sentence | Word | Improve |
| :--- | :--- | :---: | :---: |
| 125M | A man is smoking. | Smoking | 0.46 |
| 350M | A man is playing on a guitar and <br> singing. | Music | 3.99 |
| 1.3B | relating to switzerland or its peo- | Swiss | 4.34 |
|  | ple. | Equestrian | 8.88 |
| 2.7B | A jockey riding a horse. | Horseback-riding | 6.98 |
| 6.7B | The man is riding a horse. | Venison | 7.18 |
| 13B | meat from a deer. | Motorcycling | 6.51 |
| 30B | The man is riding a motorcycle | Tutorial | 9.35 |

Table 4: In-context learning examples used in various model size.

## 6 Conclusion

In this paper, we focus on exploiting Large Language Models (LLMs) to improve sentence embeddings. To achieve this, we propose a new sentence embeddings method called PromptEOL, which adapts previous prompt-based methods to autoregression models. Furthermore, we leverage in-context learning to generate superior sentence embeddings by utilizing ChatGPT and the Oxford dictionary to create sentence embeddings demonstrations. It demonstrates in-context learning allows LLMs to achieve performance comparable to current contrastive learning methods. With our promtpbased method, we also discover that further fine-tuning of LLMs can achieve the state-of-the-art performance using only efficient fine-tuning methods.

## References

\$\left[\mathrm{ABC}^{+}\right.\$14] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014), pages 81-91, 2014.

\$\left[\mathrm{ABC}^{+}\right.\$15] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 252-263, 2015.

\$\left[\mathrm{ABC}^{+}\right.\$16] Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, German Rigau Claramunt, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (Association for Computational Linguistics), 2016.

\$\left[\mathrm{ACD}^{+}\right.\$13] Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013 shared task: Semantic textual similarity. In Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity, pages 32-43, 2013.

[ACDGA12] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385-393, 2012.

\$\left[\mathrm{BMR}^{+}\right.\$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.

[CDA \${ }^{+}\$17] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.

\$\left[\mathrm{CDL}^{+}\right.\$22] Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljačić, Shang-Wen Li, Wen-tau Yih, Yoon Kim, and James Glass. Diffcse: Difference-based contrastive learning for sentence embeddings. arXiv preprint arXiv:2204.10298, 2022.

[CK18] Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449, 2018.

\$\left[\mathrm{CKB}^{+}\right.\$21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

\$\left[\mathrm{CKS}^{+}\right.\$17] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In emnlp, pages 670-680, 2017.

\$\left[\mathrm{CND}^{+}\right.\$22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
\$\left[\mathrm{CYS}^{+}\right.\$23] Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang Li, and Xipeng Qiu. Improving contrastive learning of sentence embeddings from ai feedback. arXiv preprint arXiv:2305.01918, 2023.

[DB05] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.

[DPHZ23] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

[Eth19] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 55-65, 2019.

[FAHA22] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[GYC21] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.

\$\left[\mathrm{HBB}^{+}\right.\$20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

\$\left[\mathrm{HBM}^{+}\right.\$22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[HL04] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In ACM SIGKDD international conference on Knowledge discovery and data mining, 2004.

\$\left[\mathrm{HSW}^{+}\right.\$21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

\$\left[\mathrm{JJH}^{+}\right.\$22] Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi Zhang. Promptbert: Improving bert sentence embeddings with prompts. arXiv preprint arXiv:2201.04337, 2022.

\$\left[\mathrm{KMH}^{+}\right.\$20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

[LHE21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.

\$\left[\mathrm{LYF}^{+}\right.\$23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.

\$\left[\mathrm{LZH}^{+}\right.\$20] Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence embeddings from pre-trained language models. arXiv preprint arXiv:2011.05864, 2020.

\$\left[\mathrm{MMB}^{+}\right.\$14] Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, Roberto Zamparelli, et al. A sick cure for the evaluation of compositional distributional semantic models. In Lrec, pages 216-223. Reykjavik, 2014.

[Mue22] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022.

\$\left[\mathrm{NÁC}^{+}\right.\$21] Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021.

[PL04] Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In acl, pages 271-278, 2004.

[PL05] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In acl, pages 115-124, 2005.

[RG19] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.

\$\left[\mathrm{RSR}^{+}\right.\$20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

[SAW22] Teven Le Scao, 388 Authors, and Thomas Wolf. BLOOM: A 176B-parameter openaccess multilingual language model. ArXiv, abs/2211.05100, 2022.

[SCLO21] Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. Whitening sentence representations for better semantics and faster retrieval. arXiv preprint arXiv:2103.15316, 2021.

\$\left[\mathrm{SHK}^{+}\right.\$14] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.

\$\left[\mathrm{SPW}^{+}\right.\$13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In emnlp, pages 1631-1642, 2013.

\$\left[\mathrm{TLI}^{+}\right.\$23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[TST21] Hayato Tsukagoshi, Ryohei Sasano, and Koichi Takeda. DefSent: Sentence embeddings using definition sentences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 411-418, Online, August 2021. Association for Computational Linguistics.

[VT00] Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In the 23 rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200-207, 2000.

[WGL+22] Xing Wu, Chaochen Gao, Zijia Lin, Jizhong Han, Zhongyuan Wang, and Songlin Hu. InfoCSE: Information-aggregated contrastive learning of sentence embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3060-3070, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

[WTS \${ }^{+}\$22] Qiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, and Daxin Jiang. Pcl: Peercontrastive learning with diverse augmentations for unsupervised sentence embeddings. arXiv preprint arXiv:2201.12093, 2022.

[WWC05] Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165-210, 2005.

[ZLH23] Junlei Zhang, Zhenzhong Lan, and Junxian He. Contrastive learning of sentence embeddings from scratch. arXiv preprint arXiv:2305.15077, 2023.

\$\left[Z R G^{+}\right.\$22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
