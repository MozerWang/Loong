# Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly 

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-01.jpg?height=55&width=529&top_left_y=475&top_left_x=752) <br> ${ }^{a}$ College of Design and Engineering, National University of Singapore <br> ${ }^{\mathrm{b}}$ Department of Mathematics, National University of Singapore <br> ORCID (Yijun Bian): https://orcid.org/0000-0002-5926-7100, ORCID (Yujie Luo): <br> https://orcid.org/0000-0002-5970-3371
}


#### Abstract

Providing various machine learning (ML) applications in the real world, concerns about discrimination hidden in ML models are growing, particularly in high-stakes domains. Existing techniques for assessing the discrimination level of ML models include commonly used group and individual fairness measures. However, these two types of fairness measures are usually hard to be compatible with each other, and even two different group fairness measures might be incompatible as well. To address this issue, we investigate to evaluate the discrimination level of classifiers from a manifold perspective and propose a "harmonic fairness measure via manifolds (HFM)" based on distances between sets. Yet the direct calculation of distances might be too expensive to afford, reducing its practical applicability. Therefore, we devise an approximation algorithm named "Approximation of distance between sets (ApproxDist)" to facilitate accurate estimation of distances, and we further demonstrate its algorithmic effectiveness under certain reasonable assumptions. Empirical results indicate that the proposed fairness measure $H F M$ is valid and that the proposed ApproxDist is effective and efficient.


## 1 Introduction

Numerous real-world applications nowadays have been advanced to assist humans thanks to the flourishing development of machine learning (ML) techniques, such as healthcare, transportation, recruitment, and jurisdiction. For instance, many scholars have attempted to develop ML systems to alleviate resource constraints and reduce overall healthcare costs, such as the early diagnosis of Alzheimer's disease and automated detection of suspicious mammographic lesions for biopsy [13]. However, deploying ML systems in highrisk scenarios raises concerns regarding unresolved safety issues. These issues include erroneous predictions, lack of confidence in predictions, imperfect generalisability, black-box decision-making, and insensitivity to impact and automation complacency [13]. Among them, an area to investigate trustworthy $\mathrm{AI}$ is gradually gaining researchers' attention, for example, fairness in machine learning.

The reason why people worry about the fairness and reliability of ML systems is that discriminative models may perpetuate or even exacerbate inappropriate human prejudices, which harms not only model performance but also society. Taking the ML systems deployed for clinical use for example, shifted distributions exist within[^0]

the collected medical datasets thus causing prediction failures, as a higher incidence of a disease may be reported in people seeking medical treatment in hospitals, of which the characteristics are different from that of a dataset collected for population studies [3]. Besides, potential sources of dataset bias also exist, such as imbalanced proportions for different groups, specific measurement biases captured via imaging devices, systematic biases introduced by human annotators, and distorted research subject to dataset availability. Last but not least, inappropriate evaluation for ML models may miss the desired target, and a strong focus on benchmark performance would lead to diminishing returns, where the performance gains of increasingly large efforts are actually smaller and smaller [40]. It is worthwhile to choose suitable metrics in evaluation as improvements in traditional metrics such as accuracy that do not necessarily translate to practical clinical outcomes. Therefore, it is crucial in the responsible deployment of ML systems to evaluate model performance from both accuracy and fairness.

Unlike precise accuracy, however, the concept of "fairness" is interdisciplinary with broadly varying definitions across disciplines, such as law, politics, social sciences, quantitative fields (mathematics, computer science, statistics, economics), and philosophy. Even in the field of computer science that we mainly discuss in this paper, there exist so many fairness explanations or measures to choose from when applied to the deployment of ML systems, of which two typical categories are group- and individual-level fairness. While individuallevel fairness follows the intuition that similar predicted outcomes are expected for similar individuals (in other words, similar treatment) given some specific similarity metrics [19. 22], group-level fairness cares more about the statistical disparity among groups divided by certain sensitive/unprivileged/protected characteristics, including age, race, gender, disability, pregnancy, and sexual orientation [10, 14]. Typical group-level fairness measures include demographic parity (aka. statistical parity) [8, 19, 21, 46], equalised odds [26], and predictive parity [18], and they respectively correspond to independence, separation, and sufficiency in scenarios for sensitive attributes with binary values [5]. However, it is hard to meet all three criteria (aka. independence, separation, and sufficiency) simultaneously, as imposing any two of them at the same time would overconstrain the space to the point where only degenerate solutions remain [5]. Moreover, satisfying group fairness such as statistical parity may violate individual fairness terribly [19, 22, 39]; the inherent trade-off between fairness and accuracy also draws researchers' attention [47].

Therefore, it is necessary to deliberate the incompatibility of these criteria when evaluating ML model performance.

To this end, we investigate the possibility of assessing the discrimination level of ML models from both individual and group fairness aspects. In this paper, we propose a novel fairness measure from a manifold perspective, named "harmonic fairness measure via manifolds (HFM)", to fulfil this purpose. However, the calculation of $H F M$ lies on a core distance between two sets, which might be pretty costly. To speed up the calculation and increase its practical applicable values, we propose an approximation algorithm to quickly estimate the distance between sets, named as "Approximation of distance between sets (ApproxDist)." We also further investigate its algorithmic properties under certain reasonable assumptions, in other words, how effective it is to achieve the approximation goal. Our contribution in this work is four-fold:

- We propose a harmonic fairness measure via manifolds (HFM) that could reflect the discrimination level of classifiers from both individual and group fairness aspects, which is built on a concept of distances between sets.
- We propose an approximation algorithm named "Approximation of distance between sets (ApproxDist)" to speed up the estimation of distances between sets, in order to mitigate the disadvantage of its costly direct calculation.
- We further investigate the algorithmic effectiveness of ApproxDist under certain assumptions and provide detailed explanations.
- Comprehensive experiments are conducted to demonstrate the effectiveness of the proposed HFM and ApproxDist.


## 2 Related Work

In this section, we first introduce the sources of biases and then summarise existing techniques to measure and enhance fairness in turn.

Sources of biases Before addressing fairness concerns, it is necessary to understand the origins of biases. Existing literature identifies two primary sources of unfairness: biases from the data and biases from the algorithm [41]. Biassed data collected from various sources, like device measurements and historically biassed human decisions, directly influence ML algorithms, perpetuating these biases. Additionally, missing data, such as instances or values, can introduce disparities between the dataset and the target population, leading to biassed outcomes. Even with clean data, learning algorithms might yield unfair results due to proxy attributes for sensitive features or tendentious algorithmic objectives. For example, optimising aggregated prediction errors can advantage privileged groups over marginalised ones.

Mechanisms to enhance fairness Numerous mechanisms have been proposed to enhance fairness and mitigate biases in ML models, typically categorised as pre-processing, in-processing, and postprocessing mechanisms [35, 39] 31]. Pre-processing mechanisms [4, 11, 9] usually take place when the algorithm is allowed to modify the training data, manipulating features or labels before instances are fed into the algorithm, in order to align the distributions of unprivileged and privileged groups to minimise discrimination. In contrast, post-processing mechanisms [20, 15, 26] are normally used when the learned model can only be treated as a black box without any ability to modify the training data or learning algorithm, manipulating output scores or decoupling predictions for each group. While pre-processing and post-processing mechanisms offer flexibility across classification tasks, they have drawbacks. Pre-processing mechanisms suffer from high uncertainty in final accuracy, while post-processing mechanisms often yield inferior results due to the late-stage application in the learning process. Additionally, they may not fully achieve individual fairness. In-processing mechanisms [44, 45, 43, 38] incorporate fairness constraints during training by using penalty/regularisation terms, while some of them [2, 1] adjust fairness constraints in minimax or multi-objective optimisation settings. They usually impose explicit trade-offs between fairness and accuracy in the objective function, yet the designated ML algorithm itself is tightly coupled with them as well. It is hard to say which one outperforms the others in all cases; the results depend on the fairness measures, datasets, and even the handling of training-test splits [23].

Types of fairness measures Various fairness measures have been proposed to facilitate the design of fair ML models, generally divided into distributive and procedural fairness measures. Procedural fairness concerns decision-making processes and includes featureapriori fairness, feature-accuracy fairness, and feature-disparity fairness [25]. However, these measures are dependent on features and a set of users who view the corresponding features as fair ones and thus still risk hidden discrimination in the data. Distributive fairness refers to decision-making outcomes (predictions) and includes unconscious/unawareness fairness, group fairness, individual fairness, and counterfactual fairness [19, 6, 49, 28, 33, 30]. As the simplest, unawareness fairness avoids explicitly using sensitive attributes but does not address biases stemming from associations between protected and unprotected attributes. Group fairness focuses on statistical equality among groups defined by sensitive attributes, including but not limited to demographic parity, equalised odds, equality of opportunity/equal opportunity, predictive quality parity, and disparate impact [21]. In contrast, individual fairness is based on the principle that "similar individuals should be evaluated or treated similarly," where similarity is measured by some certain distance between individuals while the specified distance also matters a lot. Besides, counterfactual fairness aims to explain the sources of discrimination and qualitative equity through causal inference tools.

However, the group fairness measures introduced in different types of fairness may hardly be compatible with each other, for example, the occurrence between equalised odds and demographic parity, or that between equalised calibration and equalised odds [6, 36, 26]. Individual and group fairness such as demographic parity are also incompatible except in trivial degenerate cases. Moreover, meeting all three fairness criteria (i.e., independence, separation, and sufficiency) simultaneously is demonstrated to be challenging, often only achievable in degenerate scenarios [5]. Furthermore, achieving higher fairness can often come at the cost of compromised accuracy [32]. Although some unique scenarios [34, 42] are recently proposed where fairness and accuracy can be concurrently improved, constrained optimisation with additional fairness constraints likely results in reduced accuracy compared to accuracy-focused optimisation.

## 3 Methodology

In this section, we formally study the measurement of fairness from a manifold perspective. Here is a list of some of the standard notations we use. In this paper, we denote

- the scalars by italic lowercase letters (e.g., $x$ ),
- the vectors by bold lowercase letters (e.g., $\boldsymbol{x}$ ),
- the matrices/sets by italic uppercase letters (e.g., $X$ ),
- the random variables by serif uppercase letters (e.g., $\mathrm{X}$ ),
- the real numbers (resp. the integers, and the positive integers) by $\mathbb{R}$ (resp. $\mathbb{Z}$, and $\mathbb{Z}_{+}$),
- the probability measure (resp. the expectation, and the variance of one random variable by $\mathbb{P}(\cdot)$ (resp. $\mathbb{E}(\cdot)$, and $\mathbb{V}(\cdot)$ ),
- the hypothesis space (resp. models in this space) by $\mathcal{F}$ (resp. $f(\cdot)$ ).

In this paper, $i \in[n]$ represents $i \in\{1,2, \ldots, n\}$ for brevity. We use $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$ to denote a dataset where the instances are iid. (independent and identically distributed), drawn from an feature/input-label/output space $\mathcal{X} \times \mathcal{Y}$ based on an unknown distribution. The feature space $\mathcal{X}$ is arbitrary, and the label space $\mathcal{Y}=$ $\left\{1,2, \ldots, n_{c}\right\}\left(n_{c} \geqslant 2\right)$ is finite, which could be binary or multiclass classification depending on the number of labels (aka. the value of $n_{c}$ ). Presuming that the considered dataset $S$ is composed of the instances including sensitive attributes, the features of one instance including sensitive attributes $\boldsymbol{a}=\left[a_{1}, a_{2}, \ldots, a_{n_{a}}\right]^{\top}$ is represented as $\boldsymbol{x} \triangleq(\breve{\boldsymbol{x}}, \boldsymbol{a})$, where $n_{a} \geqslant 1$ is the number of sensitive attributes allowing multiple attributes and $a_{i} \in \mathbb{Z}_{+}\left(1 \leqslant i \leqslant n_{a}\right)$ allows both binary and multiple values. A function $f \in \mathcal{F}: \mathcal{X} \mapsto \mathcal{Y}$ represents a hypothesis in a space of hypotheses $\mathcal{F}$, of which the prediction for one instance $\boldsymbol{x}$ is denoted by $f(\boldsymbol{x})$ or $\hat{y}$ for brevity.

### 3.1 Model fairness assessment from a manifold perspective

Given the dataset $S=\left\{\left(\breve{\boldsymbol{x}}_{i}, \boldsymbol{a}_{i}, y_{i}\right)\right\}_{i=1}^{n}$ composed of instances including sensitive attributes, here we denote one instance by $\boldsymbol{x}=$ $(\breve{\boldsymbol{x}}, \boldsymbol{a})=\left[x_{1}, \ldots, x_{n_{x}}, a_{1}, \ldots, a_{n_{a}}\right]^{\top}$ for clarity, where $n_{x}$ is the number of insensitive attributes in $\breve{\boldsymbol{x}}$. In this paper, we mainly discuss the fairness measure in scenarios for sensitive attributes with binary values, that is, $a_{i} \in \mathcal{A}=\{0,1\}$ and $a_{i}=1$ represents the majority or privileged group for $i \in\left[n_{a}\right]$. Note that the observations could be extended to sensitive attributes with multiple values as well.

In the case of instances with only one sensitive attribute, that is, $n_{a}=1$ and $\boldsymbol{x}=\left(\breve{\boldsymbol{x}}, a_{1}\right)$, the original dataset $S$ can be divided into a few disjoint sets according to the value of the sensitive attribute $a_{1}$, that is, $S_{j}=\left\{(\boldsymbol{x}, y) \in S \mid a_{1}=j\right\}, \forall j \in \mathcal{A}$. As $\mathcal{A}=\{0,1\}$ in this paper, we could get $S_{0}$ and $S_{1}$, while the division also works for sensitive attributes with multiple values, where $\mathcal{A}=\{0,1,2, \ldots\}$ is a finite set. Inspired by the principle of individual fairness-similar treatment for similar individuals, if we view the instances as data points on manifolds, the manifold representing members from the unprivileged group(s) is supposed to be as close as possible to that representing members from the privileged group. Then given a specific distance metric on instances $\mathbf{d}(\cdot, \cdot)^{2}$ we can define the distance between sets (that is, $S_{0}$ and $S_{1}$ here) by

$$
\begin{array}{r}
\mathbf{D}\left(S_{0}, S_{1}\right) \triangleq \max \left\{\max _{(\boldsymbol{x}, y) \in S_{0}} \min _{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \mathbf{d}\left((\breve{\boldsymbol{x}}, y),\left(\breve{\boldsymbol{x}}^{\prime}, y^{\prime}\right)\right)\right. \\
\left.\max _{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \min _{(\boldsymbol{x}, y) \in S_{0}} \mathbf{d}\left((\breve{\boldsymbol{x}}, y),\left(\breve{\boldsymbol{x}}^{\prime}, y^{\prime}\right)\right)\right\} \tag{1}
\end{array}
$$

and view it as an approximation of the distance between the manifold of unprivileged groups and that of the privileged group. Notice that the distance defined above satisfies the following basic properties:

(1) For any two data sets $S_{0}, S_{1} \in \mathcal{X} \times \mathcal{Y}, \mathbf{D}\left(S_{0}, S_{1}\right)=0$ if and only if $S_{0}$ equals $S_{1}$; and

(2) For any sets $S_{0}, S_{1}$, and $S_{2}$, we have the triangle inequality

$$
\mathbf{D}\left(S_{0}, S_{2}\right) \leqslant \mathbf{D}\left(S_{0}, S_{1}\right)+\mathbf{D}\left(S_{1}, S_{2}\right)
$$[^1]

Analogously, for a trained classifier $f(\cdot)$, we can calculate

$$
\begin{array}{r}
\mathbf{D}_{f}\left(S_{0}, S_{1}\right)=\max \left\{\max _{(\boldsymbol{x}, y) \in S_{0}} \min _{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \mathbf{d}\left((\breve{\boldsymbol{x}}, \hat{y}),\left(\breve{\boldsymbol{x}}^{\prime}, \hat{y}^{\prime}\right)\right)\right. \\
\left.\max _{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \min _{(\boldsymbol{x}, y) \in S_{0}} \mathbf{d}\left((\breve{\boldsymbol{x}}, \hat{y}),\left(\breve{\boldsymbol{x}}^{\prime}, \hat{y}^{\prime}\right)\right)\right\} . \tag{2}
\end{array}
$$

We remark that $\mathbf{D}\left(S_{0}, S_{1}\right)$ reflects the biases from the data and $\mathbf{D}_{f}\left(S_{0}, S_{1}\right)$ reflects the biases from the algorithm. Then the following value could be used to reflect the fairness degree of this classifier, that is,

$$
\begin{equation*}
\mathbf{d f}(f)=\frac{\mathbf{D}_{f}\left(S_{0}, S_{1}\right)}{\mathbf{D}\left(S_{0}, S_{1}\right)}-1 \tag{3}
\end{equation*}
$$

We name the fairness degree of one classifier by Eq. (3) as "harmonic fairness measure via manifolds (HFM)". We give the following remarks on $\mathbf{d f}(f)$ defined in Eq. (3):

- Notice that $\mathbf{d}\left(\left(\breve{\boldsymbol{x}}, y_{1}\right),\left(\breve{\boldsymbol{x}}, y_{2}\right)\right)=\left|y_{1}-y_{2}\right|$. We set $X_{i}=\{\breve{\boldsymbol{x}} \mid$ $\left.(\boldsymbol{x}, y) \in S_{i}\right\}$ and $Y_{i}=\left\{y \mid(\boldsymbol{x}, y) \in S_{i}\right\}$ for $i=0,1$. If $X \triangleq X_{1}=X_{2}$, then $\mathbf{D}\left(S_{0}, S_{1}\right) \leqslant \max _{\breve{\boldsymbol{x}} \in X}\left\{\left|y-y^{\prime}\right|\right\}$. Similarly we have $\mathbf{D}_{f}\left(S_{0}, S_{1}\right) \leqslant \max _{\breve{\boldsymbol{x}} \in X}\left\{\left|f(\boldsymbol{x})-f\left(\boldsymbol{x}^{\prime}\right)\right|\right\}$. In particular, if $\mathbf{D}\left(S_{0}, S_{1}\right)>0$ (resp. $\mathbf{D}_{f}\left(S_{0}, S_{1}\right)>0$ ), then there exist two data points with exactly the same insensitive attributes, while their sensitive attributes and corresponding labels (predictions) are different.
- $\mathbf{d f}(f)$ measures the bias of the classifier concerning the biases from the data. In particular, if $\mathbf{d f}(f)=0$, then $\mathbf{D}_{f}\left(S_{0}, S_{1}\right)=$ $\mathbf{D}\left(S_{0}, S_{1}\right)$, indicating that the classifier does not introduce additional biases beyond those inherent in the data. If $\mathbf{d f}(f)>0$, then $\mathbf{D}_{f}\left(S_{0}, S_{1}\right)>\mathbf{D}\left(S_{0}, S_{1}\right)$, suggesting that the algorithm introduces biases beyond those initially present in the dataset. And as $\mathbf{d f}(f)$ grows, the relative bias of the classifier with respect to the biases from the dataset increases. Lastly, if $\mathbf{d f}(f)<0$, then the classifier somehow reduces the bias arising from the original data.
- Assume that $\mathbf{D}\left(S_{0}, S_{1}\right)=0$, which means that the original dataset is perfectly unbiased. If $\mathbf{D}_{f}\left(S_{0}, S_{1}\right)=0$ as well, then we write $\mathbf{d f}(f)=\frac{0}{0}-1=0$, indicating that the classifier is also unbiased. On the other hand, if $\mathbf{D}_{f}\left(S_{0}, S_{1}\right)>0$, then $\mathbf{d f}(f)=+\infty$, indicating that the classifier introduces biases by providing different outputs for identical data with differing sensitive attributes.
- As we can see in the definition, $\mathbf{d f}(f)$ is achieved by $\mathbf{d}\left((\breve{\boldsymbol{x}}, y),\left(\breve{\boldsymbol{x}}^{\prime}, y^{\prime}\right)\right)$ for some $(\boldsymbol{x}, y) \in S_{0}$ and $\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}$, and this implies that $\mathbf{d f}(f)$ is highly sensitive to the instances used in its computation.

Note that $n_{0}$ and $n_{1}$ are the number of instances in $S_{0}$ and $S_{1}$, respectively. Then the computational complexity of directly calculating Equations 1 or would be $\mathcal{O}\left(n_{0} n_{1}\right)$, that is, $\mathcal{O}\left(n_{0}\left(n-n_{0}\right)\right)$, which is expensive and less practical to be applied on large datasets.

### 3.2 A prompt approximation of distances between sets for Euclidean spaces

Given the high computational complexity of directly calculating Equations $\sqrt{17}$ and $\sqrt{2}$, it would be necessary to speed up the calculation if we intend to use them to measure the discriminative level of classifiers in practice.

Notice that the core operation in Equations (1) and (2) is evaluating the distance between data points inside $\mathcal{X} \times \mathcal{Y}$. To reduce the number of distance evaluation operations involved in Equations 11) and (2), we observe that the distance between similar data points tends to be closer than others after projecting them onto a general

```
Algorithm 1 Approximation of distance between sets, aka.
ApproxDist $\left(\left\{\left(\breve{\boldsymbol{x}}_{i}, \boldsymbol{a}_{i}\right)\right\}_{i=1}^{n},\left\{\ddot{y}_{i}\right\}_{i=1}^{n} ; m_{1}, m_{2}\right)$
Input: Dataset $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}=\left\{\left(\breve{\boldsymbol{x}}_{i}, \boldsymbol{a}_{i}, y_{i}\right)\right\}_{i=1}^{n}$, prediction
    of $S$ by the classifier $f(\cdot)$ that has been trained, that is, $\left\{\hat{y}_{i}\right\}_{i=1}^{n}$,
    and two hyperparameters $m_{1}$ and $m_{2}$ as the designated numbers
    for repetition and comparison respectively
Output: Approximation of the distance, aka. D. $\left(S_{0}, S_{1}\right)$ in Eq. 6)
    1: for $j$ from 1 to $m_{1}$ do
        Take a random vector $\boldsymbol{w}$ from the space $\mathcal{W}=\{\boldsymbol{w}=$
        $\left.\left[w_{0}, w_{1}, \ldots, w_{n_{x}}\right]^{\top}\left|\sum_{i=0}^{n_{x}}\right| w_{i} \mid=1\right\} \subseteq[-1,1]^{1+n_{x}}$
        $d_{\max }^{j}=$ AcceleDist $\left(\left\{\left(\breve{\boldsymbol{x}}_{i}, \boldsymbol{a}_{i}\right)\right\}_{i=1}^{n},\left\{\ddot{y}_{i}\right\}_{i=1}^{n}, \boldsymbol{w} ; m_{2}\right)$
    end for
    return $\min \left\{d_{\max }^{j} \mid j \in\left[m_{1}\right]\right\}$
```

one-dimensional linear subspace. In fact, let $g: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}$ be a projection, then

$$
\begin{equation*}
\left|g(\boldsymbol{x}, y)-g\left(\boldsymbol{x}^{\prime}, y^{\prime}\right)\right| \leqslant \mathbf{d}\left((\breve{\boldsymbol{x}}, y),\left(\breve{\boldsymbol{x}}^{\prime}, y^{\prime}\right)\right) \tag{4}
\end{equation*}
$$

To be concrete, one possible candidate for the projection $g$ could be

$$
\begin{align*}
& g(\boldsymbol{x}, y ; \boldsymbol{w})=g(\breve{\boldsymbol{x}}, \boldsymbol{a}, y ; \boldsymbol{w})=\left[y, x_{1}, \ldots, x_{n_{x}}\right]^{\top} \boldsymbol{w}  \tag{5a}\\
& g(\boldsymbol{x}, f ; \boldsymbol{w})=g(\breve{\boldsymbol{x}}, \boldsymbol{a}, f(\boldsymbol{x}) ; \boldsymbol{w})=\left[\hat{y}, x_{1}, \ldots, x_{n_{x}}\right]^{\top} \boldsymbol{w} \tag{5b}
\end{align*}
$$

where $\boldsymbol{w}=\left[w_{0}, w_{1}, \ldots, w_{n_{x}}\right]^{\top}$ is a random vector that meets $w_{i} \in$ $[-1,1]$ for any $i \in\left\{0,1, \ldots, n_{x}\right\}$ and $\sum_{i=0}^{n_{x}}\left|w_{i}\right|=1$.

Now, we choose a random projection $g: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}$, then we sort all the projected data points on $\mathbb{R}$. According to Eq. 4, it is likely that for the instance $(\boldsymbol{x}, y)$ in $S_{0}$, the desired instance $\operatorname{argmin}_{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \mathbf{d}\left((\breve{\boldsymbol{x}}, y),\left(\breve{\boldsymbol{x}}^{\prime}, y^{\prime}\right)\right)$ would be someone near it after the projection, and vice versa. Thus, by using the projections in Eq. 5a, we could accelerate this process in Eq. 1) by checking several adjacent instances rather than traversing the whole dataset. Analogously, we could accelerate the process in Eq. (2) through using the projection in Eq. 5b as well.

By recording $\left\{y_{i}\right\}_{i=1}^{n}$ and $\left\{\hat{y}_{i}\right\}_{i=1}^{n}$ as one denotation for simplification, that is, $\ddot{y}$ could be the true label $y$ or the prediction by the classifier $f(\cdot)$ or $\hat{y}$, we could rewrite Equations (1), 2, and (5) as

$$
\begin{align*}
\mathbf{D} .\left(S_{0}, S_{1}\right)=\max \left\{\max _{(\boldsymbol{x}, y) \in S_{0}} \min _{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \mathbf{d}\left((\breve{\boldsymbol{x}}, \ddot{y}),\left(\breve{\boldsymbol{x}}^{\prime}, \ddot{y}^{\prime}\right)\right)\right. \\
\left.\max _{\left(\boldsymbol{x}^{\prime}, y^{\prime}\right) \in S_{1}} \min _{(\boldsymbol{x}, y) \in S_{0}} \mathbf{d}\left((\breve{\boldsymbol{x}}, \ddot{y}),\left(\breve{\boldsymbol{x}}^{\prime}, \ddot{y}^{\prime}\right)\right)\right\}  \tag{6}\\
g(\boldsymbol{x}, \ddot{y} ; \boldsymbol{w})=g(\breve{\boldsymbol{x}}, \boldsymbol{a}, \ddot{y} ; \boldsymbol{w})=\left[\ddot{y}, x_{1}, \ldots, x_{n_{x}}\right]^{\top} \boldsymbol{w} \tag{7}
\end{align*}
$$

respectively. Note that $\mathbf{D} .(\cdot, \cdot)$ is symmetrical.

Then we could propose an approximation algorithm to estimate the distance between sets, named as "Approximation of distance between sets (ApproxDist)", shown in Algorithm 1 Note that there exists a sub-route within ApproxDist to obtain an approximated distance between sets, which is named as "Acceleration sub-procedure in approximation (AcceleDist)" and shown in Algorithm 2 As the time complexity of sorting in line 2 of Algorithm 2 could reach $\mathcal{O}(n \log n)$, we could get the computational complexity of Algorithm 2 as follows: 1) The complexity of line 1 is $\mathcal{O}(n)$; and 2) The complexity from line 4 to line 9 is $\mathcal{O}\left(2 m_{2} n+1\right)$. Thus the overall time complexity of Algorithm 2 would be $\mathcal{O}\left(n\left(1+\log n+2 m_{2}\right)\right)$ and that of Algorithm 1 be $\mathcal{O}\left(m_{1} n\left(\log n+m_{2}\right)\right)$. As both $m_{1}$ and $m_{2}$ are the designated constants, the time complexity of computing the distance is down to $\mathcal{O}(n \log n)$, which is more welcome than $\mathcal{O}\left(n_{0} n_{1}\right)$ for the direct computation in Section 3.1

It is also worth noting that in line 5 of Algorithm 1 we use the minimal instead of their average value. The reason is that in each
Algorithm 2 Acceleration sub-procedure in approximation, aka. AcceleDist $\left(\left\{\left(\breve{\boldsymbol{x}}_{i}, \boldsymbol{a}_{i}\right)\right\}_{i=1}^{n},\left\{\ddot{y}_{i}\right\}_{i=1}^{n}, \boldsymbol{w} ; m_{2}\right)$

Input: Data points $\left\{\left(\breve{\boldsymbol{x}}_{i}, \boldsymbol{a}_{i}\right)\right\}_{i=1}^{n}$, its corresponding value $\left\{\ddot{y}_{i}\right\}_{i=1}^{n}$, where $\ddot{y}_{i}$ could be its true label $y_{i}$ or prediction $\hat{y}_{i}$ by the classifier $f(\cdot)$, a random vector $\boldsymbol{w}$ for projection, and a hyperparameter $m_{2}$ as the designated number for comparison

Output: Approximation of the distance, aka. D. $\left(S_{0}, S_{1}\right)$ in Eq. 6.

1: Project data points onto a one-dimensional space based on Eq. (7), in order to obtain $\left\{g\left(\boldsymbol{x}_{i}, \ddot{y}_{i} ; \boldsymbol{w}\right)\right\}_{i=1}^{n}$

2. Sort original data points based on $\left\{g\left(\boldsymbol{x}_{i}, \ddot{y}_{i} ; \boldsymbol{w}\right)\right\}_{i=1}^{n}$ as their corresponding values, in ascending order

:or $i$ from 1 to $n$ do

Set the anchor data point $\left(\boldsymbol{x}_{i}, \ddot{y}_{i}\right)$ in this round

Compute the distances $\mathbf{d}\left(\left(\breve{\boldsymbol{x}}_{i}, \ddot{y}_{i}\right), \cdot\right)$ for at most $m_{2}$ nearby data points that meets $\boldsymbol{a} \neq \boldsymbol{a}_{i}$ and $g(\boldsymbol{x}, \ddot{y} ; \boldsymbol{w}) \leqslant g\left(\boldsymbol{x}_{i}, \ddot{y}_{i} ; \boldsymbol{w}\right)$

Find the minimum among them, recorded as $d_{\min }^{s}$

Compute the distances $\mathbf{d}\left(\left(\breve{\boldsymbol{x}}_{i}, \ddot{y}_{i}\right), \cdot\right)$ for at most $m_{2}$ nearby data points that meets $\boldsymbol{a} \neq \boldsymbol{a}_{i}$ and $g(\boldsymbol{x}, \ddot{y} ; \boldsymbol{w}) \geqslant g\left(\boldsymbol{x}_{i}, \ddot{y}_{i} ; \boldsymbol{w}\right)$

Find the minimum among them, recorded as $d_{\min }^{r}$

$d_{\text {min }}^{(i)}=\min \left\{d_{\min }^{s}, d_{\text {min }}^{r}\right\}$

## end for

return $\max \left\{d_{\text {min }}^{(i)} \mid i \in[n]\right\}$

projection, the exact distance for one instance would not be larger than the calculated distance for it via AcceleDist; and the same observation holds for all of the projections in ApproxDist. Thus, the calculated distance via ApproxDist is always no less than the exact distance, and the minimal operator should be taken finally after multiple projections.

### 3.3 Algorithmic effectiveness analysis of ApproxDist

As ApproxDist in Algorithm 1 is devised to facilitate the approximation of direct calculation of the distance between sets, in this subsection, we detail more about its algorithmic effectiveness under some conditions.

Before delving into the main result (Proposition 27, we first introduce an important Lemma that confirms the observation that 'the distance between similar data points tends to be closer than others after projecting them onto a general one-dimensional linear subspace'. Note that all proofs in this subsection are presented in Appendix A to save space.

Lemma 1. Let $\boldsymbol{v}_{1}\left(\right.$ resp. $\left.\boldsymbol{v}_{2}\right)$ be a vector in the n-dimensional Euclidean space $\mathbb{R}^{n}$ with length $r_{1}$ (resp. $r_{2}$ ) such that $r_{1} \leqslant r_{2}$. Let $\boldsymbol{w} \subset \mathbb{R}^{n}$ be a unit vector. We define $\mathbb{P}\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2}\right)$ as the probability that $\left|\left\langle\boldsymbol{w}, \boldsymbol{v}_{1}\right\rangle\right| \geqslant\left|\left\langle\boldsymbol{w}, \boldsymbol{v}_{2}\right\rangle\right|$. Then,

$$
\begin{equation*}
\frac{\sin \phi}{\pi} \cdot \frac{r_{1}}{r_{2}} \leqslant \mathbb{P}\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2}\right) \leqslant\left(1+\frac{r_{1}^{2}}{r_{2}^{2}}\right)^{-\frac{1}{2}} \cdot \frac{r_{1}}{r_{2}} \tag{8}
\end{equation*}
$$

here $\phi$ represents the angle between $\boldsymbol{v}_{1}$ and $\boldsymbol{v}_{2}$.

By Eq. 8, when the ratio $r_{1} / r_{2}$ goes to zero, the probability $\mathbb{P}\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2}\right)$ goes to zero. Also, when $r_{1}=r_{2}$, it is easy to calculate that $\mathbb{P}\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2}\right)=\frac{1}{2}$.

Proposition 2. Let $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n} \subset \mathcal{X} \times \mathcal{Y}$ be a $(k+1)$ dimensional (instances have $k+1$ features) dataset, evenly distributed dataset with a size of $n$ that is a random draw of the training instances. For any two subsets of $S$ with distance d (ref. Eq. (6),
suppose further that the scaled density

$$
\begin{equation*}
\sup _{r}\left\{\inf _{\mathbf{B} \text { ball of radius } r} \frac{1}{\operatorname{Vol}(\mathbf{B})} \#(\mathbf{B} \cap S)\right\}=\frac{\mu}{\operatorname{Vol}(\mathbf{B}(d))} \tag{9}
\end{equation*}
$$

for some positive real number $\mu$ (here $\#$ denotes the number of points of a finite set and $\mathbf{B}(d)$ denotes a ball of radius $d$ ). Then, with probability at least

$$
\begin{equation*}
1-\left(\frac{\pi \mu}{m_{2} \operatorname{Vol}(\mathbf{B}(1))}\left(\left(1+\frac{n}{\mu}\right)^{\frac{1}{k+1}}-\alpha\right)\right)^{m_{1}} \tag{10}
\end{equation*}
$$

ApproxDist could reach an approximate solution that is at most $\alpha$ times of the distance between these two subsets.

Note that by Eq. 10, the efficiency of ApproxDist decreases as the scaled density $\mu$ of the original dataset increases. Meanwhile, when dealing with a large scale dataset, the more insensitive attributes we have, the more efficient ApproxDist is. In general, the efficiency of ApproxDist depends on the shape of these two subsets of $S$. Roughly speaking, the more these two sets are separated from each other, the more efficient ApproxDist is.

Now we discuss our choice of $m_{1}$ and $m_{2}$ according to Eq. 10. In fact, Eq. 10 can be approximately written as $1-c \cdot n^{\frac{m_{1}}{k+1}} / m_{2}^{m_{1}}$. We can calculate the order of magnitude of $n^{\frac{m_{1}}{k+1}} / m_{2}^{m_{1}}$ by taking the logarithm:

$$
\begin{equation*}
-\lambda \triangleq \lg \left(n^{\frac{m_{1}}{k+1}} / m_{2}^{m_{1}}\right)=m_{1}\left(\frac{\lg n}{k+1}-\lg m_{2}\right) \tag{11}
\end{equation*}
$$

The ApproxDist could reach an approximate solution with probability at least $1-c \cdot 10^{-\lambda}$. In practice, we choose positive integers $m_{2}$ and $m_{1}$ such that $\lambda$ is reasonably large, ensuring that the algorithm will reach an approximate solution with high probability.

## 4 Empirical Results

In this section, we elaborate on our experiments to evaluate the effectiveness of the proposed HFM in Eq. 3) and ApproxDist in Algorithm 1 These experiments are conducted to explore the following research questions: RQ1. Compared with the state-of-the-art (SOTA) baseline fairness measures, does the proposed HFM capture the discriminative degree of one classifier effectively, and can it capture the discrimination level from both individual and group fairness aspects? RQ2. Can ApproxDist approximate the direct computation of distances in Eq. 6 precisely, and how efficient is ApproxDist compared with the direct computation of distances? RQ3. Will the choice of hyper-parameters (that is, $m_{1}$ and $m_{2}$ in ApproxDist) affect the approximation results, and if the answer is yes, how?

### 4.1 Experimental setups

In this subsection, we present the experimental setting, including datasets, evaluation metrics, baseline fairness measures, and implementation details.

Datasets Five public datasets were adopted in the experiments: Ricci $\square^{3}$ Credit ${ }^{4}$ Income $5^{5}$ PPR, and PPVR $6^{6}$ Each of them has two sensitive attributes except Ricci, with more details in Appendix B[^2]

Evaluation metrics As data imbalance usually exists within unfair datasets, we consider several criteria to evaluate the prediction performance, including accuracy, precision, recall (aka. sensitivity), $\mathrm{f}_{1}$ score, and specificity. For efficiency metrics, we directly compare the time cost of different methods.

Baseline fairness measures To evaluate the validity of $H F M$ in capturing the discriminative degree of classifiers, we compare it with three commonly-used group fairness measures (that is, demographic parity (DP) [21, 24], equality of opportunity (EO) [26], and predictive quality parity (PQP) [12, 41]) and discriminative risk (DR) [7].

Implementation details We mainly use bagging, AdaBoost, LightGBM [29], FairGBM [16], and AdaFair [27] as learning algorithms, where FairGBM and AdaFair are two fairness-aware ensemble-based methods. Plus, certain kinds of classifiers are used in Section 4.2-including decision trees (DT), naive Bayesian (NB) classifiers, $k$-nearest neighbours (KNN) classifiers, Logistic Regression (LR), support vector machines (SVM), linear SVMs (linSVM), and multilayer perceptrons (MLP)—so that we have a larger learner pools to choose from based on different fairness-relevant rules. Standard 5-fold cross-validation is used in these experiments, in other words, in each iteration, the entire dataset is divided into two parts, with $80 \%$ as the training set and $20 \%$ as the test set. Also, features of datasets are scaled in preprocessing to lie between 0 and 1 . Except for the experiments for RQ3, we set the hyperparameters $m_{1}=25$ and $m_{2}=\lceil 2 \lg n\rceil$ in other experiments.

### 4.2 Comparison between HFM and baseline fairness measures

In this experiment, we evaluate the performance of the proposed $H F M$ compared with baseline fairness measures. As groundtruth discriminative levels of classifiers remain unknown and it is hard to directly compare different methods from that perspective, we compare the test performance of the classifier that is chosen on the training set based on fairness-relevant rules. The empirical results are reported in Tables 1 and 3 of which Table 3 is presented in the Appendix to save space. In other words, as the comparison results reported in Tables 1 and 3 show, the third column named "Accuracy" means that it reports the test performance of the classifier that is chosen purely based on the training accuracy (aka. not considering fairness at all), and the following six columns mean that they report the test performance of the classifier that is chosen based on a combination of error rates and fairness. To fairly compare different fairness-relevant rules (columns), we examine their test performance by two-tailed paired $t$ tests at a 5\% significance level to tell if choosing classifiers based on two distinct fairness-relevant rules has significantly different results. Specifically, two fairness-relevant rules end in a tie if there is no significant statistical difference; otherwise, the one with better performance (or the same performance with lower standard deviation) will win. The statistical performance of each fairness-relevant rule is reported in the last two rows of Tables 1 and 3 compared with $H F M$ (i.e., $\hat{\mathbf{d f}}$ ) in terms of the average rank and the number of $\hat{\mathbf{d f}}$ winning, ending with ties, or losing, respectively. Analogously, we also report the test performance of corresponding classifiers in Tables 4 and 5 of Appendix B, where the classifier is chosen based on a pure $f_{1}$ score or a combination of $f_{1}$ score and fairness.

As we can see from Table 1(a) and Figures 2(a) and 2(b) HFM (i.e., $\mathbf{d f}$ and $\hat{\mathbf{d f}}$ ) achieves the first and second best of the test accuracy over three group fairness measures and DR, and DR also achieves relatively good performance, which means it is indeed useful to consider the discriminative level of classifiers from both individual- and

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=345&width=445&top_left_y=250&top_left_x=380)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=346&width=443&top_left_y=655&top_left_x=378)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=343&width=422&top_left_y=251&top_left_x=817)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=303&width=394&top_left_y=291&top_left_x=1251)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=342&width=420&top_left_y=660&top_left_x=818)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=337&width=394&top_left_y=665&top_left_x=1251)

(f)

Figure 1. Comparison of baseline fairness measures and the proposed HFM, evaluated on test data. (a,d) Scatter plots 16 showing fairness and accuracy (resp. $\mathrm{f}_{1}$ score) of each fairness measure; (b,e) Scatter plots showing fairness and the variation of test accuracy (resp. $\mathrm{f}_{1}$ score) performance between the raw and disturbed data, where in the disturbed data, only sensitive attributes are changed if there are any. (c) Plot [16] of trade-offs between fairness and error rates per fairness measure, and (f) Plot of trade-offs between fairness and $\left(1-\mathrm{f}_{1}\right.$ score) per fairness measure; Lines in (c) and (f) show the mean value, and shades show $95 \%$ confidence intervals; Also note that in (c) and (f) the smaller the better.

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=320&width=457&top_left_y=1276&top_left_x=411)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=325&width=440&top_left_y=1665&top_left_x=405)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=334&width=369&top_left_y=1261&top_left_x=889)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=329&width=379&top_left_y=1663&top_left_x=867)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=337&width=368&top_left_y=1259&top_left_x=1278)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-06.jpg?height=329&width=374&top_left_y=1660&top_left_x=1269)

(f)

Figure 2. Comparison of baseline fairness measures and the proposed HFM. (a) Friedman test chart (non-overlapping means significant difference) on the test accuracy, which rejects the null hypothesis that "all fairness-relevant rules have the same evaluation performance" at the significance level of $5 \%$, and where CD means the critical difference of average rank difference, calculated by Nemenyi post-hoc test 48]; (b) The aggregated rank of each fairness-relevant rule (the smaller the better) [37] on the test accuracy; (d-e) Friedman test chart and the aggregated rank of each fairness-relevant rule on the $f_{1}$ score, evaluated on test data; (c) and (f) The aggregated rank of each fairness-relevant rule on the discriminative risk (DR), where the ranking rule in (a-c) is $0.05 \times$ error rate $+0.95 \times$ fairness, and that in (d-f) is $0.1 \times\left(1-\mathrm{f}_{1}\right.$ score $)+0.9 \times$ fairness.

group-fairness aspects, while DP, EO, and PQP only consider groupfairness aspects. It also demonstrates that HFM could measure the discriminative level of classifiers somehow from both individual- and group-fairness perspectives, just like DR does. HFM also demonstrates superior performance in Table 4(a) and Figures 2(d) and 2(e) As for Tables 1(b) and 4(b) and Figures 2(c) and 2(f) $H F M$ (i.e., $\hat{\mathbf{d f}}$ ) presents relatively weaker fairness performance, especially when compared with DR, which is understandable because, unlike all other baseline fairness measures, HFM only measures the newlyintroduced bias by classifiers in the learning, without reflecting the bias within the data. In other words, $H F M$ is unlikely to recognise the primitive bias hidden in the data if no extra bias is introduced by the learner/classifier. Therefore, HFM probably reflects partial bias information in the learning compared with other fairness measures,

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=300&width=354&top_left_y=267&top_left_x=268)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=297&width=377&top_left_y=268&top_left_x=634)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=311&width=377&top_left_y=256&top_left_x=1019)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=311&width=377&top_left_y=256&top_left_x=1385)

(d)

Figure 3. Comparison of approximation distances between sets with precise distances that are calculated directly by definition, evaluated on test data. (a) Scatter plot showing approximated values and precise values of distances between sets, and correlation here refers to the Pearson correlation coefficient; (b) Relative difference comparison of ApproxDist with direct computation concerning distance values. (c-d) Comparison of time cost (second) between ApproxDist and direct computation based on Eq. 6 .

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=337&width=397&top_left_y=802&top_left_x=401)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=340&width=400&top_left_y=1212&top_left_x=400)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=329&width=414&top_left_y=812&top_left_x=821)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=334&width=403&top_left_y=1215&top_left_x=815)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=334&width=417&top_left_y=804&top_left_x=1251)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_e318c9ab660c0d8ca8c0g-07.jpg?height=343&width=414&top_left_y=1208&top_left_x=1255)

(f)

Figure 4. Effects of hyperparameters $m_{1}$ and $m_{2}$ in ApproxDist. (a-b) The effect of the hyperparameter $m_{2}$ on the distance value; (c) The effect of the hyperparameter $m_{2}$ on the time cost, where $m_{1}$ is set to 20 . (d-e) The effect of the hyperparameter $m_{1}$ on the distance value;(f) The effect of the hyperparameter $m_{1}$ on the time cost, where $m_{2}$ is set to $\lceil 2 \lg n\rceil$ in terms of $n$-the size of the corresponding dataset.

leading to relatively inferior performance. However, it is worth noting that $H F M$ shows pretty good results when evaluated from the perspective of PQP in Tables 3(c) and 5(c) and Figures 6(c) and 6(f) Except them, similar observations could be found in other sub-tables of Tables 3 and 5 and other sub-figures of Figure 6 as well.

Moreover, we evaluate fairness-performance trade-offs per fairness measure and report the empirical results in Figures 1 and 8 of which Figure 8 is presented in the Appendix to save space. As Figure 1(c) shows, $H F M$ (that is, $\mathbf{d f}$ and $\mathbf{d f}$ ) achieves the best of tradeoffs between fairness and error rates in most of cases for $\alpha \in[0,1]$. Similar observations could be found in Figures $1(\mathrm{f})$ 8(c) $8(\mathrm{f})$ and 8(i). where $\mathrm{f}_{1}$ score, precision, recall/sensitivity, and specificity are respectively used as the corresponding performance metric.

### 4.3 Validity of HFM to approximate distances between sets for Euclidean spaces

In this experiment, we evaluate the performance of the proposed $A p$ proxDist compared with the precise distance that is computed by definition directly. To verify whether ApproxDist could achieve the true distance between sets accurately and timely, we employ scatter plots to compare their values and time cost, presented in Figures 3 to 4 and Figure 7 in Appendix B As we can see from Figure 3(a) the approximated values of distance using ApproxDist are highly correlated with their correspondingly precise values; Besides, their linear fit line and the identity line (that is, $f(x)=x$ ) are near and almost parallel. Both observations mean that the approximation values of distance via ApproxDist are validly acceptable. Figure 3(b) also presents that the relative difference between the approximation values and direct computation of distances is comparatively small and acceptable. It also shows that the relative difference between them becomes smaller as the distance $\mathbf{D}$. increases. As for the time cost of ApproxDist shown in Figure 3(c) all approximated values of distance cost less time than their direct computation of precise distance, and we find in practice that the time saving is usually more satisfactory when the used dataset is pretty large, such as on the Income dataset. Similar observations could also be found in Figures 3(d) 4(c) 4(f), 7(e) and 7(g) where in most cases of large datasets that need longer time to proceed with the direct computation of distances, the time cost of ApproxDist would not be worse than that of direct computation; yet in smaller dataset cases, the time cost of direct computation is less and completely acceptable, and using ApproxDist would be less necessary.

Table 1. Comparison of the test performance of different fairness-relevant rules, where the used fairness measures include baseline fairness measures (i.e., DP, EO, PQP, and DR) and our proposed $H F M$ (that is, $\mathbf{d f}$ and $\hat{\mathbf{d f}}$ ). Note that the best of them with lower standard deviation are indicated with italic fonts for each dataset (row) and $\alpha=0.05$ in this experiment. (a) Comparison of the test accuracy (\%); (b) Comparison of discriminative risk (DR).

| Dataset | $\mathbf{A t t r}_{\text {sens }}$ | Accuracy | $\alpha \cdot$ Error rate $+(1-\alpha)$. Fairness |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | DP | $\mathrm{EO}$ | $\mathrm{PQP}$ | DR | df | $\hat{d f}$ |
| ricci | race | $98.2609 \pm 2.1300$ | $91.3043 \pm 3.8888$ | $98.2609 \pm 2.1300$ | $98.2609 \pm 2.1300$ | $90.4348 \pm 5.0704$ | $98.2609 \pm 2.1300$ | $98.2609 \pm 2.1300$ |
| credit | $\operatorname{sex}$ | $68.5000 \pm 2.1679$ | $70.0000 \pm 0.0000 \dagger$ | $68.5000 \pm$ | $68.5000 \pm$ | $70.0000 \pm 0.0000 \dagger$ | $72.5000 \pm 3.0000$ | $72.4000 \pm 2.9563$ |
|  | age | $68.5000 \pm 2.1679 \div$ | $70.0000 \pm 0.0000 \ddagger$ | $68.5000 \pm 2.1679 \ddagger$ | $68.5000 \pm 2.1679 \div$ | $70.0000 \pm 0.0000 \ddagger$ | $71.0000 \pm 1.8439$ | $72.4000 \pm 1.0198$ |
| income | race | $81.7642 \pm 0.4886 \div$ | $83.4356 \pm 0.3514 \ddagger$ | $83.4488 \pm 0.3653 \neq$ | $84.7156 \pm 0.2348$ | $83.4356 \pm 0.3514 \ddagger$ | $84.7621 \pm 0.2036$ | $84.7156 \pm 0.2348$ |
|  | $\operatorname{sex}$ | $81.7642 \pm 0.4886$ | $83.4654 \pm 0.3527 \dagger$ | $83.2797 \pm 0.1345 \dagger$ | $84.5863 \pm 0.1812 \dagger$ | $83.4356 \pm 0.3514 \dagger$ | $84.7156 \pm 0.2348 \dagger$ | $81.1574 \pm 0.1989$ |
| ppr | sex | $62.2565 \pm 1.2127$ | $46.8669 \pm 0.8175 \ddagger$ | $62.1104 \pm 0.8347$ | $62.1104 \pm 0.8347$ | $59.4318 \pm 3.2752$ | $63.7175 \pm 1.7310$ | $59.4318 \pm 3.2752$ |
|  | race | $62.2565 \quad$ | $46.8669 \pm 0.8175 \dagger$ | $62.2565 \pm 1.2127 ‡$ | $62.1104 \pm 0.8347 \div$ | $65.0162 \pm 1.3178 \ddagger$ | $62.2565 \pm 1.2127 \ddagger$ | $66.3636 \pm 0.9883$ |
| ppvr | $\operatorname{sex}$ | $79.6005 \pm 1.1621$ | $83.9451 \pm 0.1868 \dagger$ | $83.9451 \pm 0.1868 \dagger$ | $79.6005 \pm 1.1621$ | $83.9451 \pm 0.1868 \dagger$ | $83.0462 \pm 1.4330 \dagger$ | $79.6005 \pm 1.1621$ |
|  | race | $79.6005 \pm 1.1621$ | $83.9451 \pm 0.1868 \dagger$ | $83.9451 \pm 0.1868 \dagger$ | $79.6005 \pm 1.1621$ | $83.9451 \pm 0.1868 \dagger$ | $79.6005 \pm 1.1621$ | $81.2235 \pm 0.5993$ |
| {W/T/LL <br> avg.rank} |  | 216 | $1 / 1$ | ו⿰㇒一 |  |  |  |  |
|  |  | 5.1111 | 4.3889 | 3.9444 | 4.5556 | 3.8889 | 2.5556 | $3.5556 \quad$ |

${ }^{1}$ The reported results are the average values of each method and the corresponding standard deviation under 5 -fold cross-validation on each dataset based on one sensitive attribute. By two-tailed paired $t$-test at $5 \%$ significance level, $\ddagger$ and $\dagger$ denote that the performance of using df is superior to and inferior to that of the comparative baseline method, respectively.

${ }^{2}$ The last two rows show the results of $t$-test and average rank, respectively. The "W/T/L" in $t$-test indicates the numbers that using doff is superior to, not significantly different from, or inferior to the corresponding comparative baseline methods. The average rank is calculated according to the Friedman test [17].

(b)

| Dataset | Attr $_{\text {sens }}$ | Accuracy | $\alpha$. Error rate $+(1-\alpha)$. Fairness |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $\overline{D P}$ | EO | $\overline{P Q P}$ | $\overline{\mathrm{DR}}$ | df | $\overline{d f}$ |
| ric | race | $0.5217 \pm 0.0275$ | $0.5391 \pm 0.0443$ | $0.5217 \pm 0.0275$ | $0.5217 \pm 0.0275$ | $0.5043 \pm 0.0348$ | $0.5217 \pm 0.0275$ | $0.5217 \pm 0.0275$ |
| credit | $\operatorname{sex}$ | $0.4430 \pm 0.0575 \dagger$ | $0.0000 \pm 0.0000 \dagger$ | $0.4430 \pm 0.0$ | $0.4430 \pm 0.0575 \dagger$ | $0.0000 \pm 0.0000 \dagger$ | $0.8050 \pm 0.0283$ | $0.8040 \pm 0.0275$ |
|  | age | $0.4430 \pm 0.0575 \dagger$ | $0.0000 \pm 0.0000 \dagger$ | $0.4430 \pm 0.0575 \dagger$ | $0.4430 \pm 0.0575 \dagger$ | $0.0000 \pm 0.0000 \dagger$ | $0.5010 \pm 0.0753 \dagger$ | $0.7650 \pm 0.0230$ |
| income | race | $0.5708 \pm 0.1181 \dagger$ | $0.1206 \pm 0.0216 \dagger$ | $0.1637 \pm 0.0851 \dagger$ | $0.7945 \pm 0.0063$ | $0.1206 \pm 0.0216 \dagger$ | $0.7945 \pm 0.0063$ | $0.7945 \pm 0.0063$ |
|  | sex | $0.5708 \pm 0.1181 \neq$ | $0.1629 \pm 0.1024$ | $0.4761 \pm 0.1766 \div$ | $0.7959 \pm 0.0063$ ‡ | $0.1206 \pm 0.0216 \dagger$ | $0.7945 \pm 0.0063$ ‡ | $0.2551 \pm 0.0107$ |
| $\mathrm{ppr}$ | sex | $0.4140 \pm 0.0167$ | $0.8511 \pm 0.0280 \neq$ | $0.3648 \pm 0.0$ | $0.3648 \pm 0$. | $0.2560 \pm 0.0166 \dagger$ | $0.4140 \pm c$ | $0.4140 \pm 0.0167$ |
|  | race | $0.4140 \pm 0.0167 \ddagger$ | $0.8511 \pm 0.0280 \ddagger$ | $0.4140 \pm 0.0167 \ddagger$ | $0.3648 \pm 0.0332$ | $0.2933 \pm 0.0102 \dagger$ | $0.4140 \pm 0.0167 \div$ | $0.3648 \pm 0.0332$ |
| ppvr | sex | $0.3278 \pm 0.1560$ | $0.0022 \pm 0.0028 \dagger$ | $0.0022 \pm 0.0028 \dagger$ | $0.3278 \pm 0.1560$ | $0.0022 \pm 0.0028 \dagger$ | $0.0597 \pm 0.0419 \dagger$ | $0.3278 \pm 0.1560$ |
|  | race | $0.3278 \pm 0.1560 \ddagger$ | $0.0022 \pm 0.0028 \dagger$ | $0.0022 \pm 0.0012 \dagger$ | $0.3278 \pm 0.1560 \ddagger$ | $0.0022 \pm 0.0028 \dagger$ | $0.3278 \pm 0.1560 \ddagger$ | $0.0797 \pm 0.0085$ |
| {W/T/LL <br> avg.rank} |  | $3 / 3 / 3$ | $2 /$ | 2/1/6 | $2 / 4 / 3$ | $0 / 1 / 8$ | $3 / 4 / 2$ | - |
|  |  | 4.7778 | 3.5556 | 3.2778 | 4.6667 | 1.4444 | 5.4444 | 4.8333 |

### 4.4 Effect of hyperparameters $m_{1}$ and $m_{2}$

In this subsection, we investigate whether different choices of hyperparameters (that is, $m_{1}$ and $m_{2}$ ) would affect the performance of ApproxDist or not. Different $m_{2}$ values are tested when $m_{1}$ is set, and vice versa, with empirical results presented in Figure 4 As we can see from Figures 4(c) and 4(f) obtaining approximated values of distance distinctly costs less time than that of precise values, while increasing $m_{2}$ (or $m_{1}$ ) would cost more time but still be less than that of precise values. As for the approximation performance shown in Figures 4(a) and 4(d) all approximated values are highly correlated to the precise values of distance no matter how small $m_{1}$ or $m_{2}$ is, which means the effect of less proper choices of hyperparameters is relatively inapparent; As $m_{1}$ or $m_{2}$ increases, the approximated values would be closer to the precise values of distance.

## 5 Conclusion

In this paper, we investigate how to measure the discrimination level of classifiers from both individual and group fairness aspects and present a novel harmonic fairness measure (HFM) based on distances between sets. To facilitate the calculation of distances and reduce its time cost from $O\left(n^{2}\right)$ to $O(n \log n)$, we propose an approximation algorithm ApproxDist and further present its algorithmic effectiveness analysis. The empirical results have demonstrated that the proposed HFM and ApproxDist are valid and effective.

## Ethics Statement

This work is relevant to fair and non-discriminatory topics, because it investigates whether extra discrimination is introduced in the learning process and aims to measure the discrimination level of classifiers from a manifold perspective. No extra human bias is involved except for that hiding in the raw data if there is any. The reason is that no distinctive operation concerning sensitive attributes is devised in the methodology and that no additional human annotations are included in the experiments. All necessary implementation details have been detailed in Section 4.1 and Appendix B

## References

[1] A. Agarwal, A. Beygelzimer, M. Dudík, J. Langford, and H. Wallach. A reductions approach to fair classification. In ICML, volume 80 , pages 60-69. PMLR, 2018

[2] A. Agarwal, M. Dudík, and Z. S. Wu. Fair regression: Quantitative definitions and reduction-based algorithms. In ICML, volume 97, pages 120-129. PMLR, 2019.

[3] S. Azizi, L. Culp, J. Freyberg, B. Mustafa, S. Baur, S. Kornblith, T. Chen, N. Tomasev, J. Mitrović, P. Strachan, et al. Robust and dataefficient generalization of self-supervised machine learning for diagnostic imaging. Nat Biomed Eng, pages 1-24, 2023.

[4] A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner. Scalable fair clustering. In ICML, volume 97, pages 405-413. PMLR, 2019 .

[5] S. Barocas, M. Hardt, and A. Narayanan. Fairness and machine learning: Limitations and opportunities. MIT Press, 2023.

[6] R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth. Fairness in criminal justice risk assessments: The state of the art. Sociol Methods Res, 50(1):3-44, 2021.

[7] Y. Bian, K. Zhang, A. Qiu, and N. Chen. Increasing fairness via combination with learning guarantees. arXiv preprint arXiv:2301.10813, 2023.

[8] T. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints. In ICDM workshops, pages 13-18. IEEE, 2009.

[9] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney. Optimized pre-processing for discrimination prevention. In NIPS, volume 30. Curran Associates, Inc., 2017.

[10] Centers for Disease Control and Prevention. Types of discrimination.

[EB/OL]. URL https://www.cdc.gov/eeo/faqs/discrimination.htm Latest accessed August 29, 2022.

[11] F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. In NIPS, volume 30. Curran Associates, Inc., 2017.

[12] A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2):153-163, 2017.

[13] M. Chua, D. Kim, J. Choi, N. G. Lee, V. Deshpande, J. Schwab, M. H. Lev, R. G. Gonzalez, M. S. Gee, and S. Do. Tackling prediction uncertainty in machine learning for healthcare. Nat Biomed Eng, pages 1-8, 2022.

[14] U. E. E. O. Commission. Discrimination by type. [EB/OL]. URL https://www.eeoc.gov/discrimination-type Latest accessed August 29, 2022.

[15] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. Algorithmic decision making and the cost of fairness. In SIGKDD, pages 797-806. ACM, 2017.

[16] A. F. Cruz, C. Belém, J. Bravo, P. Saleiro, and P. Bizarro. Fairgbm: Gradient boosting with fairness constraints. In ICLR, 2023.

[17] J. Demšar. Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res, 7:1-30, 2006.

[18] W. Dieterich, C. Mendoza, and T. Brennan. Demonstrating accuracy equity and predictive parity performance of the compas risk scales in broward county. Technical report, 2016.

[19] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In ITCS, pages 214-226. ACM, 2012.

[20] C. Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson. Decoupled classifiers for group-fair and efficient machine learning. In FAT, volume 81, pages 119-133. PMLR, 2018.

[21] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and removing disparate impact. In SIGKDD, pages 259-268, 2015 .

[22] W. Fleisher. What's fair about individual fairness? In AIES, pages 480490,2021 .

[23] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamilton, and D. Roth. A comparative study of fairness-enhancing interventions in machine learning. In FAT, pages 329-338. ACM, 2019.

[24] P. Gajane and M. Pechenizkiy. On formalizing fairness in prediction with machine learning. In FAT/ML, 2018.

[25] N. Grgić-Hlača, M. B. Zafar, K. P. Gummadi, and A. Weller. Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning. In AAAI, volume 32, 2018.

[26] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. In NIPS, volume 29, pages 3323-3331. Curran Associates Inc., 2016.

[27] V. Iosifidis and E. Ntoutsi. Adafair: Cumulative fairness adaptive boosting. In CIKM, pages 781-790, New York, NY, USA, 2019. ACM.

[28] M. Joseph, M. Kearns, J. H. Morgenstern, and A. Roth. Fairness in learning: Classic and contextual bandits. In NIPS, volume 29. Curran Associates, Inc., 2016.

[29] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu. Lightgbm: A highly efficient gradient boosting decision tree. In NIPS, volume 30, pages 3146-3154, 2017.

[30] M. J. Kusner, J. Loftus, C. Russell, and R. Silva. Counterfactual fairness. In NIPS, volume 30, pages 4069-4079. Curran Associates, Inc., 2017.

[31] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. A survey on bias and fairness in machine learning. ACM Comput Surv, 54 (6): $1-35,2021$.

[32] A. K. Menon and R. C. Williamson. The cost of fairness in binary classification. In FAT, volume 81, pages 107-118. PMLR, 2018.

[33] H. Nilforoshan, J. D. Gaebler, R. Shroff, and S. Goel. Causal conceptions of fairness and their consequences. In ICML, volume 162, pages 16848-16887. PMLR, 2022.

[34] D. Pessach and E. Shmueli. Improving fairness of artificial intelligence algorithms in privileged-group selection bias data settings. Expert Syst Appl, 185:115667, 2021.

[35] D. Pessach and E. Shmueli. Algorithmic fairness. In Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, pages 867-886. Springer, 2023.

[36] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger. On fairness and calibration. In NIPS, volume 30, 2017.

[37] C. Qian, Y. Yu, and Z.-H. Zhou. Pareto ensemble pruning. In AAAI, volume 29, pages 2935-2941, 2015.

[38] N. Quadrianto and V. Sharmanska. Recycling privileged learning and distribution matching for fairness. In NIPS, volume 30. Curran Associates, Inc., 2017.

[39] Z. Tang, J. Zhang, and K. Zhang. What-is and how-to for fairness in machine learning: A survey, reflection, and perspective. ACM Comput Surv, 55(13s):1-37, 2023.

[40] G. Varoquaux and V. Cheplygina. Machine learning for medical imaging: Methodological failures and recommendations for the future. NPJ Digit Med, 5(1):48, 2022.

[41] S. Verma and J. Rubin. Fairness definitions explained. In FairWare, pages 1-7, 2018 .

[42] M. Wick, S. Panda, and J.-B. Tristan. Unlocking fairness: a trade-off revisited. In NeurIPS, volume 32, pages 8783-8792. Curran Associates, Inc., 2019.

[43] B. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro. Learning non-discriminatory predictors. In COLT, volume 65, pages 19201953. PMLR, 2017.

[44] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness beyond disparate treatment and disparate impact: Learning classification without disparate mistreatment. In $W W W$, pages 1171-1180, 2017.

[45] M. B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi. Fairness constraints: Mechanisms for fair classification. In AISTATS, volume 54, pages 962-970. PMLR, 2017.

[46] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In ICML, pages 325-333. PMLR, 2013.

[47] H. Zhao and G. J. Gordon. Inherent tradeoffs in learning fair representations. J Mach Learn Res, 23(1):2527-2552, 2022.

[48] Z.-H. Zhou. Machine learning. Springer Nature, 2021.

[49] I. Žliobaitė. Measuring discrimination in algorithmic decision making. Data Min Knowl Discov, 31(4):1060-1089, 2017.
