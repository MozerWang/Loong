# Increasing Fairness via Combination with Learning Guarantees 

Yijun Bian, Kun Zhang, Anqi Qiu, Member, IEEE, and Nanguang Chen


#### Abstract

The concern about underlying discrimination hidden in machine learning (ML) models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect-either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk to reflect both individual and group fairness aspects. Furthermore, we investigate the properties of the proposed measure and propose first- and secondorder oracle bounds to show that fairness can be boosted via ensemble combination with theoretical learning guarantees. The analysis is suitable for both binary and multi-class classification. A pruning method is also proposed to utilise our proposed measure and comprehensive experiments are conducted to evaluate the effectiveness of the proposed methods.


Index Terms-Fairness, machine learning, weighted vote, learning bound

## I. INTRODUCTION

MACHINE learning (ML) applications in sensitive decision-making domains, such as recruitment, jurisdiction, or credit evaluation, are rapidly expanding. As ML is promptly developed and widely applied in many real-world scenarios nowadays, it raises concerns about the fairness and reliability of ML models as discriminative models may perpetuate or even exacerbate improper human prejudices, which harms not only model performance but also society. Possible causes of unfair models identified in the literature can be generally divided into biases from data and biases from algorithms [1]. Data collected from biassed device measurements, erroneous reports, historically biassed human

Manuscript received September 23, 2023

Y. Bian and N. Chen are with the Department of Biomedical Engineering, College of Design and Engineering, National University of Singapore, Singapore 119276 (e-mails: yjbian@nus.edu.sg; biecng @nus.edu.sg).

K. Zhang is with School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230009, China (e-mail: zhang1028kun@gmail.com).

A. Qiu is with the Department of Biomedical Engineering, National University of Singapore, Singapore. She is also with the NUS (Suzhou) Research Institute, National University of Singapore, Suzhou, China, the Department of Biomedical Engineering, the Johns Hopkins University, Baltimore, MD, USA, the Department of Health Technology and Informatics, the Hong Kong Polytechnic University, Hung hom, Hong Kong (e-mail: anqi.qiu @ polyu.edu.hk). decisions, or other reasons will directly affect ML algorithms to replicate them continually. In some cases, missing data, such as missing values or instances, can also differentiate the distribution in a dataset from the target population and bring in biases. Even when the data are purely clean, learning algorithms may still produce unfair results, sometimes as a result caused by proxy attributes for sensitive attributes, or sometimes as a result of tendentious algorithmic objectives. For example, the aim of minimising the aggregated prediction errors may benefit the privileged group over unprivileged minorities.

Numerous mechanisms have been proposed to mitigate biases and enhance fairness in ML models, which are typically categorised as pre-processing, inprocessing, and postprocessing mechanisms. Pre- and post-processing mechanisms normally function by manipulating input or output, while inprocessing mechanisms introduce fairness constraints into training procedures or algorithmic objectives. It is hard to decide which one outperforms the others in all cases, since the results depend on applied fairness measures, datasets, and even the handling of training-test-splits [2], [3]. Moreover, various fairness measures have been proposed to facilitate the design of fair ML models, such as group and individual fairness measures. Group fairness usually focuses on statistical/demographic equality among groups defined by sensitive attributes, including but not limited to demographic parity (DP) [4], [5], equality of opportunity (EO) [6], and predictive quality parity (PQP) [1], [7]. In contrast, individual fairness is based on a principle that "similar individuals should be evaluated or treated similarly," where similarity is measured by some certain distance between individuals while the specified distance also matters a lot [8], [9]. However, the hard compatibility among these measures means that unfair decisions may still exist even if one of them is satisfied, such as the incompatibility of group fairness measures themselves and incompatibility between individual and group fairness measures [6], [10]-[12].

To address these issues, we propose a novel fairness quality measure named "discriminative risk (DR)" that could reflect the discriminative degree of learners from both individual and group fairness aspects. We further investigate its properties and propose that the fairness quality of learners can benefit from combination with theoretical learning guarantees, inspired by a cancellation-of-errors effect of combination, which is the reason why gathering multiple weak individual learners can conduct a more powerful learner. In other words, we explore the possibility of a cancellation-of-biases effect in combination, and the question that we endeavour to answer is: Will combination help mitigate discrimination in multiple biassed individual classifiers? We claim that the answer is positive according to our pro-
posed oracle bounds regarding fairness, demonstrating that the fairness quality of an ensemble is better than that of one single individual classifier. Our contribution in this work is four-fold:

- We propose a novel fairness quality measure that could reflect the discriminative degree of classifiers from both individual and group fairness sides.
- We propose the first- and second-order oracle bounds regarding fairness to demonstrate the existence of a cancellation-of-biases effect in combination, providing theoretical foundations for boosting fairness via ensemble methods. We also propose two PAC-Bayesian bounds regarding fairness to facilitate the demonstration of the cancellation-of-biases effect.
- We further propose a pruning method by combining the proposed fairness quality measure and the concepts of domination and Pareto optimality with the target of improving fairness without much accuracy damage.
- Comprehensive experiments are conducted to demonstrate the validity of the proposed bounds and the effectiveness of the proposed pruning method.


## II. RELATED WORK

In this section, we first introduce existing techniques to mitigate bias issues in ML models and then summarise relevant fairness-aware ensemble-based methods in turn.

## A. Mechanisms to enhance fairness

Typical mechanisms to mitigate biases and enhance fairness in ML models include pre-processing, inprocessing, and postprocessing mechanisms. Pre-processing mechanisms usually manipulate features or labels of instances before they are fed into the algorithm to assimilate the distributions of unprivileged groups to that of the privileged group so that they become as close as possible, making it difficult for the algorithm to distinguish between them [4], [13]-[18]. In contrast, post-processing mechanisms normally manipulate output scores or decouple predictions for each group [3], [6], [19], [20]. Neither preprocessing nor post-processing mechanisms have restrictions for specific classification tasks, which is advantageous. However, pre-processing mechanisms suffer from the high uncertainty associated with the accuracy level obtained at the end. As for post-processing mechanisms, they are usually accompanied by inferior results because the applied stage is relatively late in the overall learning process. Besides, differentiated treatment may also exist and therefore be unsatisfactory in terms of individual fairness. Inprocessing mechanisms favour taking fairness constraints into account during the training by incorporating penalty/regularisation terms [21]-[26], while some of them also adjust fairness constraints in minimax optimisation or multi-objective optimisation settings [27]-[29]. They usually impose explicit trade-offs between accuracy and fairness in the objective function, yet the designated ML algorithm itself is tightly coupled with them as well. It is hard to decide which one outperforms the others in all cases, since the results depend on the fairness measures, datasets, and even the handling of training-test splits [2], [3].

## B. Types of fairness measures

Various fairness measures have been proposed to facilitate the design of fair ML models, which can be generally divided into distributive and procedural fairness measures. Procedural fairness refers to the fairness of decision-making processes that lead to the outcomes, including feature-apriori fairness, feature-accuracy fairness, and feature-disparity fairness [30]. All of these are dependent on features and a set of users who view the corresponding features as fair ones, which may still introduce discrimination hidden in the data. Distributive fairness refers to the fairness of decision-making outcomes (predictions), including unconscious/unawareness fairness, group fairness, individual fairness, and counterfactual fairness. As the simplest, unawareness fairness means making predictions without explicitly using any protected sensitive attributes, which does not prevent biases that stem from the association between unprotected and protected attributes [9]. Group fairness usually focuses on statistical/demographic equality among groups defined by sensitive attributes, including but not limited to demographic parity, equality of opportunity, and predictive quality parity [4], [6], [11], [20], [31]-[34]. In contrast, individual fairness is based on the principle that "similar individuals should be evaluated or treated similarly," where similarity is measured by some certain distance between individuals while the specified distance also matters a lot [8], [9]. Besides, counterfactual fairness aims to explain the sources of discrimination and qualitative equity through causal interference tools [35], [36].

However, the group fairness measures may hardly be compatible with each other, for example, the occurrence between equalised odds and demographic parity, or that between equalised calibration and equalised odds [6], [11], [12]. Individual and group fairness such as demographic parity are also incompatible except in the case of trivial degenerate solutions. Moreover, three fairness criteria (i.e., independence, separation, and sufficiency) are demonstrated not to be satisfied concurrently unless in degenerate cases [10]. Furthermore, significant attention has been paid to compromising accuracy in the pursuit of higher levels of fairness [2], [19], [20], [37]. It is widely accepted that a constrained optimisation problem including additional fairness constraints likely results in worse accuracy than that solely aiming at maximising accuracy, although some researchers recently propose a few unique scenarios in which fairness and accuracy can be simultaneously improved [38], [39].

## C. Fairness-aware ensemble-based methods

One of the greatest challenges in the design of fair learning algorithms is that algorithm accuracy may decrease if only increasing fairness is paid much attention to. Recent scholarship has been devoted to investigating multiple fairness-enhancing techniques and their effect on ML models, including a few methods using a typical boosting mechanism in ensemble learning, such as AdaFair [40], FARF [41], and FairGBM [42]. For instance, FARF and AdaFair combined different fairness-related criteria into the training phase, while FairGBM transformed non-differentiable fairness constraints into a proxy inequality
constraint to use gradient-based optimisation. However, few of them discussed whether their proposed methods could bring theoretical guarantees to boost fairness. Instead, they usually provided empirical results to show their effectiveness in practice.

## III. MeThODOLOGY

In this section, we formally study the properties of fairness in ensemble methods with majority vote.

We first introduce the necessary notations used in this paper. Let scalars, vectors, matrices/sets, and random variables be denoted by italic lowercase letters (e.g., $x$ ), bold lowercase letters (e.g., $\boldsymbol{x}$ ), italic uppercase letters (e.g., $X$ ), and serif uppercase letters (e.g., X), respectively. The notations $\mathbb{R}, \mathbb{P}, \mathbb{E}, \mathbb{V}$, and $\mathbb{I}$ are used to denote the real space, the probability measure, the expectation of a random variable, the variance of a random variable, and the indicator function, respectively. Additionally, the notation $f(\cdot)$ is used to denote one model/hypothesis in the hypotheses space, and $i \in[n]$ represents $i \in\{1,2, \ldots, n\}$ for brevity.

In this paper, we use $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$ to denote a dataset where the instances are independent identically distributed (i.i.d.) drawn from an input/feature-output/label space $\mathcal{X} \times \mathcal{Y}$ according to an unknown distribution $\mathcal{D}$. The label space $\mathcal{Y}=\left\{1,2, \ldots, n_{c}\right\}\left(n_{c} \geqslant 2\right)$ is finite, which could be binary or multi-class classification as well, and the feature space $\mathcal{X}$ is arbitrary. An instance including sensitive attributes $a$ is represented as $\boldsymbol{x} \stackrel{\text { def }}{=}(\breve{\boldsymbol{x}}, \boldsymbol{a})$, and $\tilde{\boldsymbol{a}}$ indicates the disturbed sensitive attributes. A hypothesis in a space of hypotheses $\mathcal{F}$ is represented by a function $f \in \mathcal{F}: \mathcal{X} \mapsto \mathcal{Y}$, and the weighted voting prediction by an ensemble of $m$ individual classifiers parameterised by a weight vector $\rho=$ $\left[w_{1}, \ldots, w_{m}\right]^{\top} \in[0,1]^{m}$, such that $\sum_{j=1}^{m} w_{j}=1$, wherein $w_{j}$ is the weight of individual classifier $f_{j}(\cdot)$, is given by $\mathbf{w v}_{\rho}(\boldsymbol{x})=\operatorname{argmax}_{y \in \mathcal{Y}} \sum_{j=1}^{m} w_{j} \mathbb{I}\left(f_{j}(\boldsymbol{x})=y\right)$. Note that ties are resolved arbitrarily, and both parametric and non-parametric models could work as individual classifiers here.

## A. Fairness quality from both individual and group fairness aspects

To discuss the properties of fairness, we need a proper measure to reflect the prediction quality of hypotheses. Lots of fairness measures have been proposed (in Section II-B), including three that are quite commonly used, that is, DP [4], [5], EO [6], and PQP [1], [7]. $1^{1}$ However, the hard compatibility among them means that each one of them can only focus on one aspect (either group or individual fairness). Therefore, to reflect the discriminative degree of hypotheses from both individual and group sides, we propose one fairness quality measure named discriminative risk $(D R)$ in this subsection.

Presume that the considered dataset $S$ is composed of the instances including sensitive attributes, wherein the features[^0]

of one instance $\boldsymbol{x}=(\breve{\boldsymbol{x}}, \boldsymbol{a})$ include the sensitive attributes $\boldsymbol{a}$ and general attributes $\breve{\boldsymbol{x}}$. Note that $\boldsymbol{a}=\left[a_{1}, \ldots, a_{n_{a}}\right]^{\top}$ allows multiple attributes, with $n_{a}$ as the number of sensitive attributes, and for each attribute, $a_{i} \in \mathbb{Z}_{+}\left(1 \leqslant i \leqslant n_{a}\right)$ allows binary and multiple values. Following the principle of individual fairness, the treatment/evaluation on one instance should not be changed only due to minor changes of sensitive attributes within the instance, which means there will exist underlying discriminative risks if one hypothesis/classifier makes different predictions for one instance with the only changes in sensitive attributes. Therefore, the fairness quality of one hypothesis $f(\cdot)$ could be evaluated by

$$
\begin{equation*}
\ell_{\text {fair }}(f, \boldsymbol{x})=\mathbb{I}(f(\breve{\boldsymbol{x}}, \boldsymbol{a}) \neq f(\breve{\boldsymbol{x}}, \tilde{\boldsymbol{a}})) \tag{2}
\end{equation*}
$$

similarly to the $0 / 1$ loss. Note that Eq. (2) is evaluated on only one instance. To describe this characteristic of the hypothesis on multiple instances (aka. from a group level), then the empirical discriminative risk on $S$ and the true discriminative risk of the hypothesis are expressed as

$$
\begin{equation*}
\hat{\mathcal{L}}_{\text {fair }}(f, S)=\frac{1}{n} \sum_{i=1}^{n} \ell_{\text {fair }}\left(f, \boldsymbol{x}_{i}\right) \tag{3}
\end{equation*}
$$

and

$$
\begin{equation*}
\mathcal{L}_{\text {fair }}(f)=\mathbb{E}_{(\boldsymbol{x}, y) \sim \mathcal{D}}\left[\ell_{\text {fair }}(f, \boldsymbol{x})\right] \tag{4}
\end{equation*}
$$

respectively. By $\ell_{\text {fair }}(f, \boldsymbol{x})$, we are able to measure the discriminative risk of one hypothesis on one instance, reflecting the individual fairness; Meanwhile, by $\hat{\mathcal{L}}_{\text {fair }}(f, S)$ and $\mathcal{L}_{\text {fair }}(f)$, we are able to weigh the discriminative risk of the hypothesis from a group perspective as well. Overall, the proposed fairness quality could provide benefits to properly measure the discriminative risk of one hypothesis from both individual and group fairness aspects.

Moreover, it is worth noting that, unlike three commonlyused group fairness measures in Eq. (1), the proposed Equations (3) and (4) do not need to split different subgroups and calculate them separately. The discriminative risk can be gotten for a dataset as a whole including the privileged group and unprivileged groups. The reason is that in Eq.(2), if one hypothesis/classifier makes different predictions for one instance with only changes in sensitive attributes, there will exist underlying discriminative risks, regardless of which group this instance/member belongs to. A bonus advantage of the proposed fairness quality measure is that it is suitable for not only binary but also multi-class classification scenarios, enlarging its applicable fields.

Ensemble classifiers predict by taking a weighted combination of predictions by hypotheses from $\mathcal{F}$, and the $\rho$-weighted majority vote $\mathbf{w v}_{\rho}(\cdot)$ predicts $\mathbf{w v}_{\rho}(\boldsymbol{x})=$ $\operatorname{argmax}_{y \in \mathcal{Y}} \mathbb{E}_{\rho}[\mathbb{I}(f(\boldsymbol{x})=y)]$. The fairness quality of the ensemble will be evaluated corresponding to the total weight assigned to individual classifiers, that is, $\ell_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}, \boldsymbol{x}\right)=$ $\mathbb{I}\left(\mathbf{w v}_{\rho}(\breve{\boldsymbol{x}}, \boldsymbol{a}) \neq \mathbf{w} \mathbf{v}_{\rho}(\breve{\boldsymbol{x}}, \tilde{\boldsymbol{a}})\right)$. Then we can obtain the empirical discriminative risk on $S$ and the true discriminative risk of the ensemble analogously. Note that $\mathbb{E}_{(\boldsymbol{x}, y) \sim \mathcal{D}}[\cdot]$ and $\mathbb{E}_{f \sim \rho}[\cdot]$ can be respectively abbreviated as $\mathbb{E}_{\mathcal{D}}[\cdot]$ and $\mathbb{E}_{\rho}[\cdot]$ for brevity when the context is unambiguous.

## B. Oracle bounds regarding fairness for weighted vote

In this subsection, we discuss oracle bounds concerning fairness for the weighted vote, inspired by the work [43]. Following the notations described above, if the weighted vote makes a discriminative decision, then at least a $\rho$-weighted half of the classifiers have made a discriminative decision and, therefore, $\ell_{\text {fair }}\left(\mathbf{w v}_{\rho}, \boldsymbol{x}\right) \leqslant \mathbb{I}\left(\mathbb{E}_{\rho}[\mathbb{I}(f(\breve{\boldsymbol{x}}, \boldsymbol{a}) \neq f(\breve{\boldsymbol{x}}, \tilde{\boldsymbol{a}}))] \geqslant 0.5\right)$. This observation leads to our proposed first- and second-order oracle bounds for the fairness quality of weighted vote. Note that there are no more assumptions except the aforementioned notations in the following theorems.

Theorem 1 (First-order oracle bound).

$$
\begin{equation*}
\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right) \leqslant 2 \mathbb{E}_{\rho}\left[\mathcal{L}_{\text {fair }}(f)\right] \tag{5}
\end{equation*}
$$

To investigate the bound deeper, we introduce here the tandem fairness quality of two hypotheses $f(\cdot)$ and $f^{\prime}(\cdot)$ on one instance $(\boldsymbol{x}, y)$, adopting the idea of the tandem loss [43], by

$$
\begin{equation*}
\ell_{\text {fair }}\left(f, f^{\prime}, \boldsymbol{x}\right)=\mathbb{I}\left(f(\breve{\boldsymbol{x}}, \boldsymbol{a}) \neq f(\breve{\boldsymbol{x}}, \tilde{\boldsymbol{a}}) \wedge f^{\prime}(\breve{\boldsymbol{x}}, \boldsymbol{a}) \neq f^{\prime}(\breve{\boldsymbol{x}}, \tilde{\boldsymbol{a}})\right) \tag{6}
\end{equation*}
$$

The tandem fairness quality counts a discriminative decision on the instance $(\boldsymbol{x}, y)$ if and only if both $f(\cdot)$ and $f^{\prime}(\cdot)$ give a discriminative prediction on it. Note that in the degeneration case $\ell_{\text {fair }}(f, f, \boldsymbol{x})=\ell_{\text {fair }}(f, \boldsymbol{x})$. Then the expected tandem fairness quality is defined by $\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)=\mathbb{E}_{\mathcal{D}}\left[\ell_{\text {fair }}\left(f, f^{\prime}, \boldsymbol{x}\right)\right]$. Also, note that a prominent difference between our work and the work of Masegosa et al. [43] is that they investigate the expected risk of accuracy rather than fairness quality. Lemma 2 relates the expectation of the second moment of the standard fairness quality to the expected tandem fairness quality. Note that $\mathbb{E}_{f \sim \rho, f^{\prime} \sim \rho}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right]$ for the product distribution $\rho \times \rho$ over $\mathcal{F} \times \mathcal{F}$ can be abbreviated as $\mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right]$ for brevity.

Lemma 2. In multi-class classification,

$$
\begin{equation*}
\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\rho}\left[\ell_{\text {fair }}(f, \boldsymbol{x})\right]^{2}\right]=\mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right] \tag{7}
\end{equation*}
$$

Theorem 3 (Second-order oracle bound). In multi-class classification

$$
\begin{equation*}
\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right) \leqslant 4 \mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right] \tag{8}
\end{equation*}
$$

Furthermore, we can also obtain an alternative second-order bound based on Chebyshev-Cantelli inequality, presented in Theorem 4 .

Theorem 4 (C-tandem oracle bound). If $\mathbb{E}_{\rho}\left[\mathcal{L}_{\text {fair }}(f)\right]<1 / 2$, then

$$
\begin{equation*}
\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right) \leqslant \frac{\mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right]-\mathbb{E}_{\rho}\left[\mathcal{L}_{\text {fair }}(f)\right]^{2}}{\mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right]-\mathbb{E}_{\rho}\left[\mathcal{L}_{\text {fair }}(f)\right]+\frac{1}{4}} \tag{9}
\end{equation*}
$$

Up to now, we have gotten the first- and second-order oracle bounds of the fairness quality for the weighted vote, which will help us further investigate fairness. Note that all proofs in this subsection are provided in Appendix A-A.

Furthermore, it is worth noting that, despite the similar names of "first- and second-order oracle bounds" from our inspiration [43], the essences of our bounds are distinct from theirs. To be specific, their work investigates the bounds for generalisation error and is not relevant to fairness issues, while ours focus on the theoretical support for bias mitigation. In other words, their bounds are based on the $0 / 1$ loss $\ell_{\mathrm{err}}(f, \boldsymbol{x})=\mathbb{I}(f(\boldsymbol{x}) \neq y)$, while ours are built on $\ell_{\text {fair }}(f, x)$ in Eq. (2).

## C. PAC-Bayesian bounds for the weighted vote

All oracle bounds described in Section III-B are expectations that can only be estimated on finite samples instead of being calculated precisely. They could be transformed into empirical bounds via PAC-Bayesian analysis as well to ease the difficulty of giving a theoretical guarantee of the performance on any unseen data, which we discuss in this subsection. Based on Hoeffding's inequality, we can deduct generalisation bounds presented in Theorems 5 and 6 Note that all proofs in this subsection are provided in Appendix A-B

Theorem 5. For any $\delta \in(0,1)$, with probability at least $(1-\delta)$ over a random draw of $S$ with a size of $n$, for a single hypothesis $f(\cdot)$,

$$
\begin{equation*}
\mathcal{L}_{\text {fair }}(f) \leqslant \hat{\mathcal{L}}_{\text {fair }}(f, S)+\sqrt{\frac{1}{2 n} \ln \frac{1}{\delta}} \tag{10}
\end{equation*}
$$

Theorem 6. For any $\delta \in(0,1)$, with probability at least $(1-\delta)$ over a random draw of $S$ with a size of $n$, for all distributions $\rho$ on $\mathcal{F}$,

$$
\begin{equation*}
\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right) \leqslant \hat{\mathcal{L}}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}, S\right)+\sqrt{\frac{1}{2 n} \log \frac{|\mathcal{F}|}{\delta}} \tag{11}
\end{equation*}
$$

## D. Application: Constructing fairer ensembles without much accuracy degradation

To use the aforementioned bounds to construct strong ensemble classifiers, we discuss the accuracy performance indicated by corresponding loss functions in this subsection.

We evaluate the accuracy quality of a hypothesis $f(\cdot)$ by the $0 / 1$ loss $\ell_{\text {err }}(f, \boldsymbol{x})$, the empirical loss by $\hat{\mathcal{L}}_{\text {err }}(f, S)=$ $\frac{1}{n} \sum_{i=1}^{n} \ell_{\text {err }}(f, \boldsymbol{x})$, and the expected loss by $\mathcal{L}_{\text {err }}(f)=$ $\mathbb{E}_{\mathcal{D}}\left[\ell_{\text {err }}(f, \boldsymbol{x})\right]$, respectively. A concept of domination [44] is introduced to take fairness and accuracy into account simultaneously during pruning. Note that the domination relationship is used to achieve a Pareto optimal solution for this bi-objective minimisation case, where two sub-objectives are viewed as improving fairness and accuracy, respectively.

Definition 7 (Domination). Let $\mathcal{L}_{\text {err }}(\cdot)$ and $\mathcal{L}_{\text {fair }}(\cdot)$ be two sub-objectives to be minimised, and let $\mathcal{G}=\left(\mathcal{L}_{\text {err }}, \mathcal{L}_{\text {fair }}\right)$ be the objective for a Pareto optimal solution. For two probability distributions $\rho$ and $\pi$ on $\mathcal{F}$ that are independent of $S$ : 1) $\rho$ weakly dominates $\pi$ if $\mathcal{L}_{\text {err }}\left(\mathbf{w} \mathbf{v}_{\rho}\right) \leqslant \mathcal{L}_{\text {err }}\left(\mathbf{w} \mathbf{v}_{\pi}\right)$ and $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right) \leqslant \mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\pi}\right)$, denoted as $\succeq_{\mathcal{G}}$; 2) $\rho$ dominates $\pi$ if $\rho \succeq_{\mathcal{G}} \pi$ and either $\mathcal{L}_{\text {err }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)<\mathcal{L}_{\text {err }}\left(\mathbf{w} \mathbf{v}_{\pi}\right)$ or $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)<\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\pi}\right)$, denoted as $\succ_{\mathcal{G}}$.

An alternative way to compare two hypotheses by considering both fairness and accuracy is to define an objective function based on an adaptive weighted sum method, i.e.,

$$
\begin{equation*}
\mathcal{L}\left(f, f^{\prime}\right)=\frac{\lambda}{2}\left(\mathcal{L}_{\text {err }}(f)+\mathcal{L}_{\text {err }}\left(f^{\prime}\right)\right)+(1-\lambda) \mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right) \tag{12}
\end{equation*}
$$

wherein $\lambda \in(0,1)$ represents a regularisation factor introduced to balance fairness and accuracy and to indicate their relative importance as well. Note that this is a symmetrical function, that is, $\mathcal{L}\left(f, f^{\prime}\right)=\mathcal{L}\left(f^{\prime}, f\right)$, and that in the degeneration case $\mathcal{L}(f, f)=\lambda \mathcal{L}_{\text {err }}(f)+(1-\lambda) \mathcal{L}_{\text {fair }}(f)$ when two hypotheses $f(\cdot)$ and $f^{\prime}(\cdot)$ are identical. Then we can obtain the objective for the weighted vote as

$$
\begin{equation*}
\mathcal{L}\left(\mathbf{w} \mathbf{v}_{\rho}\right)=\lambda \mathbb{E}_{\rho}\left[\mathcal{L}_{\text {err }}(f)\right]+(1-\lambda) \mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right] \tag{13}
\end{equation*}
$$

```
Algorithm 1 Pareto Optimal Ensemble Pruning via Improving
Accuracy and Fairness Concurrently (POAF)
Input: training set $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$, original ensemble $F=$
    $\left\{f_{j}(\cdot)\right\}_{j=1}^{m}$ via weighted vote, and threshold $k$ as maximum size
    of the sub-ensemble after pruning
Output: Pruned sub-ensemble $H$ ( $H \subset F$ and $|H| \leqslant k$ )
    Randomly pick $k$ individual members from $F$, indicated by $\mathbf{r}$
    Initialise a candidate set for pruned sub-ensembles $\mathcal{P}=\{\mathbf{r}\}$
    for $i=1$ to $k$ do
        Randomly choose $\mathbf{r}$ from $\mathcal{P}$ with equal probability
        Generate $\mathbf{r}^{\prime}$ by flipping each bit of $\mathbf{r}$ with probability $1 / m$
        if $\exists \mathbf{z} \in \mathcal{P}$ such that $\mathbf{z} \succ_{\mathcal{G}} \mathbf{r}^{\prime}$ then
            continue
        $\mathcal{P}=\left(\mathcal{P} \backslash\left\{\mathbf{z} \in \mathcal{P} \mid \mathbf{r}^{\prime} \succeq_{\mathcal{G}} \mathbf{z}\right\}\right) \bigcup\left\{\mathbf{r}^{\prime}\right\}$
        Let $\mathcal{V}=\mathcal{N}_{-}\left(\mathbf{r}^{\prime}\right) \cup \mathcal{N}_{+}\left(\mathbf{r}^{\prime}\right)$
        Sort $\mathcal{V}$ by $\operatorname{argmin}_{\mathbf{v} \in \mathcal{V}} \hat{L}(\mathbf{v}, S)$ in ascending order
        for $\mathrm{v} \in \mathcal{V}$ do
            if $\exists \mathbf{z} \in \mathcal{P}$ such that $\mathbf{z} \succ_{\mathcal{G}} \mathbf{v}$ then
            continue
        $\mathcal{P}=\left(\mathcal{P} \backslash\left\{\mathbf{z} \in \mathcal{P} \mid \mathbf{v} \succeq_{\mathcal{G}} \mathbf{z}\right\}\right) \bigcup\{\mathbf{v}\}$
    $H=\operatorname{argmin}_{\mathbf{r} \in \mathcal{P}} \hat{\mathcal{L}}(\mathbf{r}, S)$
```

which is expected to be minimised to improve fairness and accuracy concurrently.

Based on the domination concept and this objective $\mathcal{L}\left(f, f^{\prime}\right)$, we propose an ensemble pruning method, named as "Pareto Optimal Ensemble Pruning via Improving Accuracy and Fairness Concurrently (POAF)," presented in Algorithm 1 . aiming to construct stronger ensemble classifiers with less accuracy damage. Note that a selector vector $\mathbf{r} \in\{0,1\}^{m}$ in line 1 is used to indicate potential sub-ensembles, wherein $|\mathbf{r}|=k$ and the picked sub-ensemble can be denoted by $F_{\mathbf{r}} \stackrel{\text { def }}{=} \mathbf{r}$ as well. The notations $\mathcal{N}_{-}\left(\mathbf{r}^{\prime}\right)$ and $\mathcal{N}_{+}\left(\mathbf{r}^{\prime}\right)$ in line 9 signify the sets of neighbour solutions of $\mathbf{r}^{\prime}$, wherein each element in $\mathcal{N}_{-}\left(\mathbf{r}^{\prime}\right)$ has one less individual member than $\mathbf{r}^{\prime}$ and each element in $\mathcal{N}_{+}\left(\mathbf{r}^{\prime}\right)$ has one more individual member than $\mathbf{r}^{\prime}$, respectively. The used bi-objective here is denoted by $\mathcal{G}(\mathbf{r})=\left(\hat{\mathcal{L}}_{\text {err }}(\mathbf{r}, S), \hat{\mathcal{L}}_{\text {fair }}(\mathbf{r}, S)\right)$, wherein $\mathbf{r}$ indicates the

```
Algorithm 2 Centralised Version of Ensemble Pruning via Improving
Accuracy and Fairness Concurrently (EPAF-C)
Input: training set $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$, original ensemble $F=$
    $\left\{f_{j}(\cdot)\right\}_{j=1}^{m}$, and threshold $k$ as maximum size after pruning
Output: Pruned sub-ensemble $H$ ( $H \subset F$ and $|H| \leqslant k$ )
    $H \leftarrow$ an arbitrary individual member $f_{i} \in F$
    for $i=2$ to $k$ do
        $f^{*} \leftarrow \operatorname{argmin}_{f_{i} \in F \backslash H} \sum_{f_{j} \in H} \hat{\mathcal{L}}\left(f_{i}, f_{j}, S\right)$
        Move $f^{*}$ from $F$ to $H$
$\overline{\text { Algorithm } 3 \text { Distributed Version of Ensemble Pruning via Improving }}$
Accuracy and Fairness Concurrently (EPAF-D)
Input: training set $S=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$, original ensemble $F=$
    $\left\{f_{j}(\cdot)\right\}_{j=1}^{m}$, threshold $k$ as maximum size after pruning, and
    number of machines $n_{m}$
Output: Pruned sub-ensemble $H$ ( $H \subset F$ and $|H| \leqslant k$ )
    1: Partition $F$ randomly into $n_{m}$ groups as equally as possible, i.e.,
        $F_{1}, \ldots, F_{n_{m}}$
    for $i=1$ to $n_{m}$ do
        $H_{i} \leftarrow E P A F-C\left(F_{i}, k\right)$
    $H^{\prime} \leftarrow E P A F-C\left(\bigcup_{i=1}^{n_{m}} H_{i}, k\right)$
    $H \leftarrow \operatorname{argmin}_{T \in\left\{H_{1}, \ldots, H_{n_{m}}, H^{\prime}\right\}} \hat{\mathcal{L}}(T, S)$
```

individual members that are selected to join the pruned subensemble. Also note that a bi-objective optimisation problem may have multiple solutions that are Pareto optimal, rather than one single Pareto optimal solution only. A solution $\mathbf{w v}_{\rho}(\cdot)$ is Pareto optimal if there is no other solution in $\mathcal{F}$ that dominates $\mathbf{w} \mathbf{v}_{\rho}(\cdot)$. It is worth mentioning that $P O A F$ is not the only way to achieve a strong sub-ensemble, and two other easilyimplemented pruning methods are also able to achieve this goal (presented as follows).

TABLE I

DATASET STATISTICS. NOTE THAT THE COLUMNS NAMED “\#INST” AND “\#SENS ATTR" REPRESENT THE NUMBER OF INSTANCES AND THE NUMBER OF SENSITIVE ATTRIBUTES IN THE DATASET, RESPECTIVELY. THE JOINT SENSITIVE ATTRIBUTE OF ONE INSTANCE REPRESENTS BOTH TWO OF THE SENSITIVE ATTRIBUTES OF IT BELONG TO THE CORRESPONDING PRIVILEGED GROUP.

| Dataset | \#inst | \#raw <br> feature | \#feature <br> binarized | \#sens <br> attr | \#privileged group for sensitive attribute |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | 1st sens attr | 2nd sens attr | joint |
| ricci | 118 | 6 | 7 | 1 | 68 in race | _ | - |
| credit | 1000 | 22 | 60 | 2 | 690 in sex | 851 in age | 625 |
| income | 30162 | 15 | 100 | 2 | 25933 in race | 20380 in sex | 18038 |
| $\mathrm{ppr}$ | 6167 | 12 | 403 | 2 | 4994 in sex | 2100 in race | 1620 |
| ppvr | 4010 | 12 | 329 | 2 | 3173 in sex | 1452 in race | 1119 |

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-05.jpg?height=338&width=418&top_left_y=1067&top_left_x=1081)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-05.jpg?height=347&width=420&top_left_y=1060&top_left_x=1508)

(b)
Fig. 1. Comparison of the proposed discriminative risk (DR) with three group fairness measures, that is, DP, EO, and PQP. (a) Scatter diagrams with the degree of correlation, where the $x$ - and $y$-axes are different fairness measures and the variation of accuracy between the raw and disturbed data. (b) Correlation among multiple criteria. Note that correlation here is calculated for all experiments, and refer to Figure 11 in Appendix B-A if audiences would like to see empirical results on each dataset.

Two extra easily-implemented pruning methods: There are many methods to solve the aforementioned bi-objective optimisation problem, and $P O A F$ using the concept of Pareto optimal is just one of them. Inspired by the work [46], [47], we also come up with two extra pruning methods that could be easily implemented, named as "Ensemble Pruning via Improving Accuracy and Fairness Concurrently (EPAF)," presented in Algorithms 2 and 3. They utilise the balanced objective function in Eq. (12) as well yet with one key difference from $P O A F$, that is, they do not have the concepts of domination nor Pareto optimality. The objective used in EPAF adopts the weighted sum method in bi-objective optimisation problems, which is simple and easy to implement. However, it also comes along with the disadvantage of the difficulty of setting an appropriate weight for each factor. Besides, it cannot find certain Pareto-optimal solutions in the case of a non-convex objective space as well. EPAF has two versions of it, that is, a centralised version ( $E P A F-C)$ and a distributed one $(E P A F-D)$. The key difference between them is that a sub-procedure (line 3) in EPAF-D could be parallelized and therefore greatly contribute to a speedup for the whole pruning

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=360&width=1759&top_left_y=156&top_left_x=194)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=282&width=295&top_left_y=171&top_left_x=205)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=282&width=298&top_left_y=171&top_left_x=491)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=282&width=299&top_left_y=171&top_left_x=791)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=280&width=268&top_left_y=169&top_left_x=1080)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=285&width=293&top_left_y=172&top_left_x=1358)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-06.jpg?height=285&width=288&top_left_y=172&top_left_x=1645)

(f)

Fig. 2. Correlation for oracle bounds in Section III-B and generalisation bounds in Section III-C (a-c) Correlation between $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ and oracle bounds, where $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ is indicated on the vertical axis and the horizontal axes represent the right-hand sides of inequalities (5), 8, and , 9 , respectively. Note that the horizontal and vertical axes in (d) denote the right- and left-hand sides in Eq. 77, respectively. (e-f) Correlation between $\mathcal{L}_{\text {fair }}(\cdot)$ and generalisation bounds, where $\mathcal{L}_{\text {fair }}(\cdot)$ is indicated on the vertical axis and the right-hand sides of inequalities 10 and 11) are indicated on the horizontal axes, respectively.

TABLE II

COMPARISON OF THE FAIRNESS-AWARE ENSEMBLE METHODS WITH POAF. (A-B) COMPARISON ON THE TEST ACCURACY (\%) AND F1 SCORE.

| Dataset | Attr $_{\text {sens }}$ | lightGBM | FairGBM | AdaFair | Bagging | $E P A F-C$ | $E P A F-D$ | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | race | $98.26 \pm 2.13$ | $98.26 \pm 2.13$ | $98.26 \pm 2.13$ | $98.26 \pm 2.13$ | $98.26 \pm 2.13$ | $98.26 \pm 2.13$ | $99.13 \pm 1.74$ |
| credit | sex | $75.00 \pm 3.21$ | $75.00 \pm 3.21$ | $70.00 \pm 0.00 \dagger$ | $74.00 \pm 2.47$ | $71.60 \pm 2.96$ | $71.00 \pm 1.76$ | $71.80 \pm 4.14$ |
|  | age | $75.00 \pm 3.21$ | $74.80 \pm 3.01$ | $70.00 \pm 0.00 \dagger$ | $74.00 \pm 2.47$ | $71.60 \pm 2.96$ | $71.00 \pm 1.76$ | $71.80 \pm 4.14$ |
| income | race | $\mathbf{8 6 . 1 0} \pm \mathbf{0 . 4 2}$ | $86.02 \pm 0.45 \dagger$ | $24.89 \pm 0.00 \ddagger$ | $84.15 \pm 0.44$ | $83.53 \pm 0.53$ | $83.55 \pm 0.42$ | $83.80 \pm 0.68$ |
|  | sex | $\mathbf{8 6 . 1 0} \pm \mathbf{0 . 4 2} \dagger$ | $86.08 \pm 0.51 \dagger$ | $24.89 \pm 0.00 \neq$ | $84.15 \pm 0.44$ | $83.53 \pm 0.53$ | $83.55 \pm 0.42$ | $83.80 \pm 0.68$ |
| ppr | sex | $\mathbf{6 7 . 9 7} \pm \mathbf{0 . 6 2}$ | $67.50 \pm 0.86 \div$ | $45.54 \pm 0.00 \ddagger$ | $65.41 \pm 0.54$ | $64.45 \pm 1.20$ | $64.56 \pm 0.78$ | $62.73 \pm 0.69$ |
|  | race | $67.97 \pm 0.62 \div$ | $67.48 \pm 0.72 \div$ | $45.54 \pm 0.00 \pm$ | $65.41 \pm 0.54$ | $64.45 \pm 1.20$ | $64.56 \pm 0.78$ | $62.73 \pm 0.69$ |
| ppvr | $\operatorname{sex}$ | $\mathbf{8 4 . 2 4} \pm \mathbf{0 . 7 6}$ | $84.17 \pm 0.90 \div$ | $49.39 \pm 19.77 \neq$ | $81.67 \pm 1.64$ | $81.10 \pm 1.28 \div$ | $81.30 \pm 1.36$ | $82.60 \pm 0.74$ |
|  | race | $84.24 \pm 0.76$ | $84.24 \pm 0.76 \div$ | $69.01 \pm 26.42$ | $81.67 \pm 1.64$ | $81.10 \pm 1.28 \div$ | $81.30 \pm 1.36$ | $82.60 \pm 0.74$ |
| {W/T/L <br> avg.rank} |  | $0 / 3 / 6$ | $0 / 3 / 6$ | 5/2/2 | $0 / 7 / 2$ | $2 / 5 / 2$ | $0 / 7 / 2$ | - |
|  |  | 1.50 | 2.17 | 6.72 | 3.39 | 5.39 | 4.94 | 3.89 |

(a)

| Dataset | Attr sens | lightGBM | FairGBM | AdaFair | Bagging | EPAF-C | $E P A F-D$ | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | race | $0.9818 \pm 0.0224$ | $0.9818 \pm 0.0224$ | $0.9818 \pm 0.0224$ | $0.9818 \pm 0.0224$ | $0.9818 \pm 0.0224$ | $0.9818 \pm 0.0224$ | $0.9913 \pm 0.0174$ |
| credit | sex | $0.8347 \pm 0.0190$ | $0.8347 \pm 0.0190$ | $0.8235 \pm 0.0000 \dagger$ | $0.8232 \pm 0.0164$ | $0.8048 \pm 0.0189$ | $0.8017 \pm 0.0101$ | $0.8093 \pm 0.0311$ |
|  | age | $0.8347 \pm 0.0190$ | $0.8326 \pm 0.0178$ | $0.8235 \pm 0.0000 \dagger$ | $0.8232 \pm 0.0164$ | $0.8048 \pm 0.0189$ | $0.8017 \pm 0.0101$ | $0.8093 \pm 0.0311$ |
| income | race <br> race | $0.6766 \pm 0.0095$ | $0.6740 \pm 0.0102 \div$ | $0.3986 \pm 0.0000$ | $0.6615 \pm 0.0073$ | $0.6530 \pm 0.0084$ | $0.6523 \pm 0.0052$ | $0.6412 \pm 0.0267$ |
|  | sex | $0.6766 \pm 0.0095$ | $0.6742 \pm 0.0114$ | $0.3986 \pm 0.0000$ | $0.6615 \pm 0.0073$ | $0.6530 \pm 0.0084$ | $0.6523 \pm 0.0052$ | $0.6412 \pm 0.0267$ |
| ppr | sex | $0.6202 \pm 0.0117 \dagger$ | $0.6118 \pm 0.0144$ | $\mathbf{0 . 6 2 5 8} \pm 0.0000$ | $0.6066 \pm 0.0112 \dagger$ | $0.5970 \pm 0.0197 \dagger$ | $0.5968 \pm 0.0144 \dagger$ | $0.5346 \pm 0.0341$ |
|  | race <br> race | $0.6202 \pm 0.0117 \div$ | $0.6177 \pm 0.0185$ | $\mathbf{0 . 6 2 5 8} \pm \mathbf{0 . 0 0 0 0}$ | $0.6066 \pm 0.0112 \uparrow$ | $0.5970 \pm 0.0197$ | $0.5968 \pm 0.0144 \dagger$ | $0.5346 \pm 0.0341$ |
| ppvr | sex | †.2388土0.0061 | $0.2380 \pm 0.0068 \div$ | $\mathbf{0 . 3 3 4 1} \pm \mathbf{0 . 0 3 0 8}$ | $0.3007 \pm 0.0283 \dagger$ | $0.2894 \pm 0.0325 \dagger$ | $0.2969 \pm 0.0383 \dagger$ | $0.2356 \pm 0.0325$ |
|  | race | $0.2388 \pm 0.0061 \div$ | $0.2388 \pm 0.0061 \div$ | $\mathbf{0 . 3 3 7 4} \pm \mathbf{0 . 0 5 7 0}$ | $0.3007 \pm 0.0283 \dagger$ | $0.2894 \pm 0.0325 \dagger$ | $0.2969 \pm 0.0383 \dagger$ | $0.2356 \pm 0.0325$ |
| {W/T/L <br> avg,rank} |  | $0 / 3$ | $0 / 3$ | 2/1/6 | $0 / 5 / 4$ | $0 / 5 / 4$ | $0 / 5 / 4$ | - |
|  |  | 2.61 | 3.28 | 3.17 | 3.39 | 4.72 | 5.17 | 5.67 |

(b)

procedure, which is also verified in Figure 8 of Section IV-E providing a comparison between them over time cost. Note that the sub-procedure could be replaced with any other existing pruning method as well.

## IV. EMPIRICAL RESULTS

In this section, we elaborate on our experiments to evaluate the proposed methods. Five public datasets that we use include Ricci $\sqrt{2}$ Credit $\sqrt[3]{3}$ Income ${ }^{4}$ PPR, and PPVR $\sqrt[5]{5}$ with more details about the dataset statistics provided in Table I Standard 5fold cross-validation is used in these experiments, i.e., in each iteration, the entire dataset is split into two parts, with $80 \%$ as the training set and $20 \%$ as the test set. Baselines include three group fairness measures (i.e., DP [4], [5], EO [6], and PQP [1], [7]), two fairness-aware ensemble-based methods (that is, FairGBM [42] and AdaFair [40]), and various ensemble pruning methods. More experimental details are provided in the corresponding subsections and Appendix B[^1]

## A. Validating the proposed fairness quality measure

In this subsection, we evaluate the validity of the proposed fairness quality measure (namely discriminative risk, DR) in Section III-A, compared with three group fairness measures. Classifiers are conducted using several ensemble methods including bagging [49], AdaBoost [50], [51], lightGBM [52], FairGBM, and AdaFair. The empirical results are reported in Figure 1 and Section B-A

As we can see from Figure 11 a), compared with three group fairness measures, DR has the highest value of correlation (namely the Pearson correlation coefficient) between itself and the variation of accuracy. It means that DR captures better the characteristic of changed treatment than three other group fairness measures when sensitive attributes are disturbed, as the drop in accuracy indicates the existence of underlying discrimination hidden in models. Figure 1|(b) reports the correlation between the variation of other criteria (such as precision, recall, $\mathrm{f}_{1}$ score, sensitivity, and specificity) including accuracy and multiple fairness measures. It shows that DR also has a high correlation with the variation of specificity.

## B. Validating the oracle bounds and PAC-Bayesian bounds

In this subsection, experiments are conducted to verify the proposed oracle bounds in Section III-B and generalisation

TABLE III

COMPARISON OF THE FAIRNESS-AWARE ENSEMBLE METHODS WITH POAF . (A-D) COMPARISON ON THE PROPOSED DISCRIMINATIVE RISK (DR) MEASURE, AND THREE GROUP FAIRNESS MEASURES (THAT IS, DP, EO, AND PQP), RESPECTIVELY.

| Dataset | Attr sens | lightGBM | FairGBM | AdaFair | Bagging | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ri | race | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0 0}$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm 0.0000$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ |
| credit | sex | $0.0420 \pm 0.0234$ | $0.0450 \pm 0.0257$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \div$ | $0.0770 \pm 0.0312$ | $0.0730 \pm 0.0312$ | $0.0620 \pm 0.0147$ | $0.0380 \pm 0.0194$ |
|  | age | $0.0420 \pm 0.0234$ | $0.0360 \pm 0.0196$ | $0.0000 \pm 0.0000 \dagger$ | $0.0770 \pm 0.0312$ | $0.0730 \pm 0.0312$ | $0.0620 \pm 0.0147$ | $0.0380 \pm 0.0194$ |
| income | race | $0.0011 \pm 0.0006 \div$ | $0.0006 \pm 0.0002 \div$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} 0^{\dagger}$ | $0.0671 \pm$ | $0.0738 \pm 0.0073$ | $0.0733 \pm 0.0042 \neq$ | $0.0644 \pm 0.0032$ |
|  | sex | $0.0011 \pm 0.0006 \dagger$ | $0.0013 \pm 0.0005 \dagger$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.0671 \pm 0$ | $0.0738 \pm 0.0073$ | $0.0733 \pm 0.0042 \ddagger$ | $0.0644 \pm 0.0032$ |
| ppr | $\operatorname{sex}$ | $0.0735 \pm 0.0193 \dagger$ | $0.1153 \pm 0.0129 \dagger$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.2114 \pm$ | $\pm 0.0135$ | $1 \pm 0.0112$ | $0.2000 \pm 0.0165$ |
|  | race | $0.0735 \pm 1$ | 0.1 | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | 0.2114 | 0.2148 | .0112 | $0.2000 \pm 0.0165$ |
| ppvr | sex | $0.0275 \pm 0.0110$ | $0.0282 \pm 0.0111$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0})^{\dagger}$ | $0.0699 \pm 0.0092 \neq$ | $0.0659 \pm 0.0105 \ddagger$ | $0.0642 \pm 0.0091 \neq$ | $0.0407 \pm 0.0060$ |
|  | race | $0.0275 \pm 0.0110$ | $0.0275 \pm 0.0110$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} 0^{\dagger}$ | $0.0699 \pm 0.0092 \neq$ | $0.0659 \pm 0.0105 \ddagger$ | $0.0642 \pm 0.0091 \neq$ | $0.0407 \pm 0.0060$ |
| {W/T/L <br> avg.r.ank} |  | $0 / 5$ | 00 | 01 | $2 \pi$ | $2 \prime \quad$ | $4 / 1$ | - |
|  |  | 2.72 | 2.94 | 1.33 | 5.78 | 6.00 | 5.56 | 3.67 |

(a)

| Dataset | Attr sens | lightGBM | FairGBM | AdaFair | Bagging | EPAF-C | $E P A F-D$ | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | race | $0.3093 \pm 0.1497$ | $0.3093 \pm 0.1497$ | $0.3150 \pm 0.1368$ | $0.3150 \pm 0.1368$ | $0.3150 \pm 0.1368$ | $0.3150 \pm 0.1368$ | $0.3293 \pm 0.1237$ |
| credit | sex | $0.0602 \pm 0.0794$ | $0.0556 \pm 0.0702$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.0807 \pm 0.0762$ | $0.0739 \pm 0.0519$ | $0.0664 \pm 0.0538$ | $0.0406 \pm 0.0270$ |
|  | age | $0.1291 \pm 0.0890$ | $0.1130 \pm 0.0932$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.1762 \pm 0.0612 \ddagger$ | $0.1148 \pm 0.0856$ | $0.1006 \pm 0.0646$ | $0.0588 \pm 0.0428$ |
| income | race | $0.0738 \pm 0.0078 \div$ | $0.0730 \pm 0.0068 \div$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.0962 \pm 0.0086$ | $0.0958 \pm 0.0140$ | $0.0956 \pm 0.0159$ | $0.0932 \pm 0.0137$ |
|  | sex | $0.1689 \pm 0.0066$ | $0.1594 \pm 0.0067 \dagger$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.1899 \pm 0.0112$ | $0.1924 \pm 0.0101$ | $0.1910 \pm 0.0099$ | $0.1757 \pm 0.0104$ |
| ppr | $\operatorname{sex}$ | $0.1646 \pm 0.0202 \div$ | $0.0853 \pm 0.0391$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0})^{\dagger}$ | $0.1506 \pm 0.0332$ | $0.1490 \pm 0.0374$ | $0.1450 \pm 0.0355$ | $0.1186 \pm 0.0136$ |
|  | race | $0.2029 \pm 0.0482 \stackrel{\ddagger}{*}$ | $0.1091 \pm 0.0481$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0} \dagger$ | $0.1359 \pm 0.0324 \ddagger$ | $0.1419 \pm 0.0302 \ddagger$ | $0.1331 \pm 0.0249$ | $0.1189 \pm 0.0370$ |
| ppvr | sex | $0.0440 \pm 0.0147$ | $0.0450 \pm 0.0133$ | $0.0436 \pm 0.0289$ | $0.0683 \pm 0.0151 \ddagger$ | $0.0521 \pm 0.0161 \neq$ | $0.0600 \pm 0.0088 \ddagger$ | $0.0342 \pm 0.0190$ |
|  | race | $\mathbf{0 . 0 3 5 7} \pm \mathbf{0 . 0 1 2 5}$ | $0.0357 \pm 0.0125$ | $0.0494 \pm 0.0324$ | $0.0361 \pm 0.0175$ | $0.0428 \pm 0.0146$ | $0.0448 \pm 0.0136$ | $0.0361 \pm 0.0099$ |
| {W/T/L <br> avg.rank} |  | 2/6/1 | $0 / 7 / 2$ | $0 / 3 / 6$ | $3 / 6 / 0$ | $2 / 7 / 0$ | 1/8/0 | - |
|  |  | 4.00 | 2.44 | 2.17 | 5.72 | 5.50 | 4.83 | 3.33 |

(b)

| Dataset | Attr $_{\text {sens }}$ | lightGBM | FairGBM | AdaFair | Bagging | $E P A F-C$ | $E P A F-D$ | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ri | race | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ |
| credit | sex | $0.0646+0$ | $0.0579+0.0359$ | $0.0000 \pm 0.0000 \div$ | $0.0798+0.0683$ | $0.0983 \pm 0.0561$ | $0.0808+0.0422$ | $0.0363+0.0261$ |
|  | age | $0.0945 \pm$ |  | $\mathbf{0 . 0 0 0 0 \pm 1}$ |  |  | $0.0813 \pm 0.0588$ | $0.0649 \pm 0.0409$ |
| income | race | 0.0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=28&width=128&top_left_y=1060&top_left_x=807) | 0.0000 |  | 0.0 | 0.044 | 0.0563 |
|  | sex | $0.1097 \pm 0.0205 \div$ | $0.0760 \pm 0.0244$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $0.0804=$ | $0.0788 \pm 0.0257$ | $0.0743 \pm 0.0163$ | $0.0688 \pm 0.0307$ |
| ppr | se | 0.150 |  | 0.00 |  |  | 0480 | 0.0420 |
|  | race | $0.2202 \pm$ | 0.11 | $\mathbf{0 . 0 0 0 0} \pm 1$ | 0.16 | 0.15 | .0223 | $0.1285 \pm 0.0404$ |
| ppvr |  | $0.1466 \pm 0.0455$ | $0.1466 \pm 0.0455$ | $0.0633 \pm 0.0466$ | $0.1458 \pm 0.0785$ | $0.1260 \pm 0.0620$ | $0.1244 \pm 0.0678$ | $0.1060 \pm 0.0506$ |
|  | race | $0.1096 \pm 0.0457$ | $0.1096 \pm 0.0457$ | $0.1197 \pm 0.1399$ | $0.0886 \pm 0.0462$ | $0.1225 \pm 0.0617$ | $0.1159 \pm 0.0364$ | $0.1102 \pm 0.0417$ |
| {W/T/L <br> avg_rank} |  | $2 / 7 / 0$ | 0/9/0 | $0 / 4 / 5$ | $0 / 9 / 0$ | $0 / 8$ | $0 / 9$ | - |
|  |  | 5. | 3. | 1.94 | 4.83 | 5.39 | 4.50 | 2.89 |

(c)

| Dataset | Attrsens | lightGBM | FairGBM | AdaFair | Bagging | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | race | $\mathbf{0 . 0 1 8 2} \pm \mathbf{0 . 0 3 6 4}$ | $\mathbf{0 . 0 1 8 2} \pm \mathbf{0 . 0 3 6 4}$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ | $0.0250 \pm 0.0500$ |
| credit | sex | $0.088 \pm 0.0545$ | $0.0893 \pm 0.0538$ | $0.0754 \pm 0.0586$ | $\mathbf{0 . 0 5 6 6} \pm \mathbf{0 . 0 3 8 4} \dagger$ | $0.0845 \pm 0.0521$ | $0.066 \pm 0.0479$ | $0.0875 \pm 0.0331$ |
|  | age | $0.1354 \pm 0.0995$ | $0.1351 \pm 0.1036$ | $0.1385 \pm 0.1243$ | $0.1340 \pm 0.0896$ | $\mathbf{0 . 1 0 1 0} \mathbf{0 . 1 0 9 0}$ | $0.1154 \pm 0.1154$ | $0.1146 \pm 0.0817$ |
| income | race | $0.0615 \pm 0.0301$ | $0.0591 \pm 0.0299$ | $0.1052 \pm 0.0109 \neq$ | $0.0390 \pm 0.0156$ | $\mathbf{0 . 0 3 5 6} \pm \mathbf{0 . 0 3 4 3}$ | $0.0419 \pm 0.0364$ | $0.0454 \pm 0.0302$ |
|  | sex | $0.0367 \pm 0.0271 \neq$ | $0.0274 \pm 0.0161$ | $0.2003 \pm 0.0052 \neq$ | $0.0189 \pm 0.0140$ | $0.0319 \pm 0.0108 \neq$ | $0.0282 \pm 0.0156$ | $\mathbf{0 . 0 0 9 5} \pm \mathbf{0 . 0 0 8 2}$ |
| ppr | sex | $\mathbf{0 . 0 6 8 1} \pm \mathbf{0 . 0 4 4 9}$ | $0.1122 \pm 0.0717$ | $0.1284 \pm 0.0263$ | $0.0955 \pm 0.0502$ | $0.1097 \pm 0.0474$ | $0.1172 \pm 0.0541$ | $0.1093 \pm 0.0471$ |
|  | race | $\mathbf{0 . 0 4 2 6} \pm \mathbf{0 . 0 3 7 9} \dagger$ | $0.0930 \pm 0.0422$ | $0.0977 \pm 0.0099$ | $0.1057 \pm 0.0465$ | $0.0947 \pm 0.0410$ | $0.0991 \pm 0.0461$ | $0.0866 \pm 0.0313$ |
| ppvr | sex | $0.5270 \pm 0.1920$ | $0.5324 \pm 0.1819$ | $\mathbf{0 . 1 1 2 8} \pm \mathbf{0 . 0 4 1 9}$ | $0.2159 \pm 0.1211$ | $0.2122 \pm 0.1322$ | $0.2138 \pm 0.1067$ | $0.2870 \pm 0.1582$ |
|  | race | $0.3218 \pm 0.2237$ | $0.3218 \pm 0.2237$ | $0.2134 \pm 0.2030$ | $\mathbf{0 . 1 6 1 8} \pm \mathbf{0 . 1 0 0 8}$ | $0.1899 \pm 0.1060$ | $0.1652 \pm 0.0976$ | $0.2238 \pm 0.1016$ |
| W/T/L | $1 / 7 / 1$ | $0 / 9 / 0$ | $2 / 7 / 0$ | $008 / 1$ | $1 / 8 / 0$ | $0 / 9 / 0$ | - |  |
| avg.rank | 4.44 | 4.78 | 5.11 | $\mathbf{3 . 1 1}$ | 3.22 | 3.78 | 3.56 |  |

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=301&width=390&top_left_y=1687&top_left_x=304)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=304&width=382&top_left_y=2027&top_left_x=313)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=298&width=374&top_left_y=1689&top_left_x=691)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=304&width=377&top_left_y=2027&top_left_x=690)

(f)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=312&width=393&top_left_y=1676&top_left_x=1061)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=310&width=401&top_left_y=2019&top_left_x=1057)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-07.jpg?height=295&width=372&top_left_y=1685&top_left_x=1451)

(d)

(h)

Fig. 3. Comparison of the fairness-aware ensemble-based methods with $P O A F$. (a) Friedman test chart (non-overlapping means significant difference) on the test accuracy, which rejects the null hypothesis that "all methods have the same evaluation performance" at $5 \%$ significance level, and where CD means the critical difference of average rank difference, calculated by Nemenyi post-hoc test [48]; (b) The aggregated rank of each method (the smaller the better) 44] on the test accuracy; (c) Friedman test chart on the DR; (d) The aggregated rank of each method on the DR; (d-h) The aggregated rank on the $\mathrm{f}_{1}$ score, DP $\mathrm{EO}$, and $\mathrm{PQP}$, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=303&width=374&top_left_y=217&top_left_x=323)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=303&width=374&top_left_y=564&top_left_x=323)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=303&width=371&top_left_y=217&top_left_x=693)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=309&width=380&top_left_y=561&top_left_x=691)

(f)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=303&width=374&top_left_y=217&top_left_x=1060)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=307&width=377&top_left_y=562&top_left_x=1058)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=307&width=377&top_left_y=215&top_left_x=1427)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=320&width=383&top_left_y=553&top_left_x=1424)

(h)

Fig. 4. Comparison of the state-of-the-art pruning method with $P O A F$, using bagging to conduct homogeneous ensembles. (a-c) Friedman test chart on the test accuracy, precision, and $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$, respectively, of which each rejects the null hypothesis at the significance level of $5 \%$; (d-h) The aggregated rank for each pruning method over the $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$, test accuracy, precision, recall, and $\mathcal{L}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$, respectively.

TABLE IV

COMPARISON OF THE STATE-OF-THE-ART PRUNING METHODS WITH POAF ON THE PROPOSED FAIRNESS QUALITY MEASURE (AKA. DR). THE COLUMN NAMED "ENSEM" IS THE CORRESPONDING RESULT FOR THE ENTIRE ENSEMBLE WITHOUT PRUNING. THE BEST DR WITH A LOWER STANDARD DEVIATION IS INDICATED WITH BOLD FONTS FOR EACH DATASET (ROW). (A-C) USING BAGGING, ADABOOSTM1, AND SAMME TO PRODUCE HOMOGENEOUS ENSEMBLES, RESPECTIVELY

| Dataset | Ensem | KP | oO | DREP | SEP | OEP | PEP | MRMC | Disc.EP | TSP.AP | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0 \pm 0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm \mathbf{0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ |
| credit | $0.0450 \pm 0.0215$ | $0.0610 \pm 0.0243 \neq$ | $0.0550 \pm 0.0215$ | $0.0390 \pm 0.0185$ | $0.0430 \pm 0.0217$ | $0.0410 \pm 0.0156$ | $0.0350 \pm 0.0235$ | $0.0500 \pm 0.0276$ | $0.0590 \pm 0.0311$ | $0.0400 \pm 0.0187$ | $0.0460 \pm 0.0195$ | $0.0450 \pm 0.0200$ | $0.0330 \pm 0.0130$ |
| income | $0.0453 \pm 0.0053 \pm$ | $0.0507 \pm 0.0054 \ddagger$ | $0.0460 \pm 0.0056 \neq$ | $0.0535 \pm 0.0030 \neq$ | $0.0484 \pm 0.0050 \neq$ | $0.0453 \pm 0.0053 \div$ | $0.0453 \pm 0.0053 \neq$ | $0.0502 \pm 0.0094 \ddagger$ | $0.0498 \pm 0.0041 \div$ | $0.0444 \pm 0.0054 \div$ | $0.0481 \pm 0.0042 \ddagger$ | $0.0471 \pm 0.0041 \neq$ | $\mathbf{0 . 0 3 5 8} \pm 0.0058$ |
| ppr | $0.1367 \pm 0.0132$ | $0.1539 \pm 0.0143 \div$ | $0.1399 \pm 0.0111 \div$ | $0.1427 \pm 0.0127 \mp$ | $0.1489 \pm 0.0167$ | $0.1367 \pm 0.0132$ | $0.1367 \pm 0.0132$ | $0.1406 \pm 0.0070 \neq$ | $0.1451 \pm 0.0100 \div$ | $0.1380 \pm 0.0121 \div$ | $0.1370 \pm 0.0222$ | $0.1391 \pm 0.0205$ | $\mathbf{0 . 1 2 0 9} \pm \mathbf{0 . 0 1 1 1}$ |
| ppvr | $0.0507 \pm 0.0073 \div$ | $0.0522 \pm 0.0066 \%$ | $0.0492 \pm 0.0117 \pm$ | $0.0639 \pm 0.0111 \div$ | $0.0534 \pm 0.0121 \div$ | $0.0507 \pm 0.0073$ | $0.0514 \pm 0.0046 \neq$ | $0.0532 \pm 0.0097 \Psi$ | $0.0549 \pm 0.0119 \ddagger$ | $0.0427 \pm 0.0090$ | $0.0494 \pm 0.0084 \div$ | $0.0427 \pm 0.0082 \neq$ | $\mathbf{0 . 0 2 7 5} \pm 0.0056$ |
| W/T/L | 2/3/0 | 4/1/0 | $3 / 2 / 0$ | 3/2/0 | $3 / 2 / 0$ | $2 / 3 / 0$ | $2 / 3 / 0$ | 3/2/0 | $3 / 2 / 0$ | $3 / 2 / 0$ | $2 / 3 / 0$ | $2 / 3 / 0$ | - |
| avg.rank | 5.6 | 10.8 | 7.2 | 9.2 | 9.0 | 5.1 | 4.8 | 9.4 | 11 | 4.4 | 6.8 | 6.1 | 2.2 |

${ }^{1}$ The reported results are the average values of each method and the corresponding standard deviation under 5-fold cross-validation on each dataset.

${ }^{2}$ By two-tailed paired $t$-test at $5 \%$ significance level, $\ddagger$ and $\dagger$ denote that the performance of $P O A F$ is superior to and inferior to that of the comparative baseline method, respectively.

${ }^{3}$ The last two rows show the results of $t$-test and average rank, respectively. The "W/T/L" in $t$-test indicates the numbers that $P O A F$ is superior to, not significantly different from, or inferior to the corresponding comparative pruning methods. The average rank is calculated according to the Friedman test 45 .

(a)

| Dataset | Ensem | KP | $\mathrm{OO}$ | DREP | SEP | OEP | ![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-08.jpg?height=24&width=118&top_left_y=1588&top_left_x=1030) | MRMC | Disc.EP | TSP.AP | EPAF-C | $E P A F-D$ | $\quad \overline{P O A F}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm \mathbf{0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $0.0000 \pm 0.0000$ |
|  | $0.0610 \pm 0.0263$ |  | $0.0650 \pm 0.0166$ | $0.0820 \pm 0.0451$ | $0.0680 \pm 0.0091$ | $0.0600 \pm 0.0221$ | $0.0710 \pm 0.0331 \ddagger$ | $0.0540 \pm 0.0383$ | $0.0830 \pm 0.0076$ | $0.0820 \pm 0.0303 \div$ | $0.0580 \pm 0.0325$ | $0.0440 \pm 0.0167$ | $\mathbf{0 . 0 3 8 0} \pm 0.0305$ |
| income | $0.0592 \pm 0.0026$ | $0.0608 \pm 0.0041$ | $0.0596 \pm 0.0025$ | $0.0645 \pm 0.0133$ | $0.0683 \pm 0.0107$ | $0.0609 \pm 0.0048$ | $0.0608 \pm 0.0041$ | $0.0608 \pm 0.0045$ | $0.0608 \pm 0.0041$ | $0.0598 \pm 0.0022$ | $0.0602 \pm 0.0022$ | $0.0660 \pm 0.0121$ | $0.0523 \pm 0.0060$ |
| ppr | $0.1607 \pm 0.0188$ | $0.1836 \pm 0.0065 \neq$ | $0.1685 \pm 0.0175$ | $0.1636 \pm 0.0216$ | $0.1805 \pm 0.0267$ | $0.1703 \pm 0.0169$ | $0.1703 \pm 0.0169$ | $0.1812 \pm 0.0134$ | $0.1849 \pm 0.0107$ | $0.1682 \pm 0.0171$ | $0.1708 \pm 0.0128 \neq$ | $0.1843 \pm 0.0185 \div$ | $0.1490 \pm 0.0094$ |
| $\mathrm{ppvr}$ | $0.0474 \pm 0.0095$ | $0.0494 \pm 0.0104$ | $0.0494 \pm 0.0104$ | $0.0519 \pm 0.0075 \neq$ | $0.0592 \pm 0.0175$ | $0.0497 \pm 0.0103$ | $0.0494 \pm 0.0104$ | $0.0459 \pm 0.0067$ | $0.0494 \pm 0.0104$ | $0.0494 \pm 0.0053 \neq$ | $0.0472 \pm 0.0051 \neq$ | $0.0582 \pm 0.0098$ | $0.0432 \pm 0.0049$ |
| W/T/L | $0 / 5 / 0$ | $1 / 4 / 0$ | $0 / 5 / 0$ | $1 / 4 / 0$ | $0 / 5 / 0$ | $0 / 5 / 0$ | $1 / 4 / 0$ | $1 / 4 / 0$ | $2 / 3 / 0$ | $2 / 3 / 0$ | $2 / 3 / 0$ | $2 / 3 / 0$ |  |
| avg.rank | 4.2 | 8.1 | 5.9 | 8.7 | 10.2 | 7.7 | 7.7 | 5.6 | 9.6 | 6.7 | 5.4 | 9.0 | 2.2 |

(b)

| Dataset | Ensem | KP | oo | DREP | SEP | OEP | PEP | MRMC | c.EP | SP.AP | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | $0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $0.0000 \pm 0.0000$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0}$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $\mathbf{0 . 0 0 0 0} \pm \mathbf{0 . 0 0 0 0}$ | $0 \pm 0.0000$ | $0 \pm 0.1$ | 00 |
|  |  |  |  |  |  |  |  |  |  | 48 |  |  |  |
|  |  |  |  |  |  |  |  |  | .056 | 0.0580 | 089 | 0.060 |  |
| ppr | $0.1547 \pm 0$. | $0.1797 \pm 0 . C$ | $0.1591 \pm 0.0078$ | $0.1529 \pm$ | $0.1773 \pm 0.0147$ | $0.1606 \pm 0.0092$ | $0.1578 \pm 0.0094$ | $0.1^{\circ}$ | $0.1838 \pm 0.0128$ | $0.1584 \pm 0.0062$ | $\pm 0.0149$ | $.0232 \div$ | 0.147 |
| $\mathrm{ppvr}$ | $0.0487 \pm 0.0089$ | $0.0512 \pm 0.0090$ | $0.0512 \pm 0.0090$ | $0.0707 \pm 0.0229$ | $0.0549 \pm 0.0201$ | $0.0512 \pm 0.0090$ | $0.0519 \pm 0.0096$ | $0.0512 \pm 0.0140$ | $0.0512 \pm 0.0090$ | $0.0594 \pm 0.0270$ | $0.0574 \pm 0.0170$ | $0.0499 \pm 0.0176$ | $0.0424 \pm 0.0188$ |
|  |  |  |  |  |  |  |  |  |  |  | $5 / 0$ |  |  |
| avg. |  |  |  | 6. | 9 | 6.9 | 7.0 | 1 | 8.3 | 8.6 | 7.2 | 6.8 | 2.2 |

(C)

bounds in Section III-C, of which the validity is evaluated on scatter diagrams with the degree of correlation, namely the Pearson correlation coefficient, reported in Figure 2 and Section B-B. For the experiments in this subsection, bagging, AdaBoostM1, and SAMME are used to constitute an ensemble classifier on various kinds of individual classifiers including decision trees (DT), naive Bayesian (NB) classifiers, $k$-nearest neighbours (KNN) classifiers, Logistic Regression (LR), support vector machines (SVM), linear SVMs (linSVM), and multilayer perceptrons (MLP).

As we may see from Figure $2(\mathrm{a})$, it shows a high level of correlation between $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ and $2 \mathbb{E}_{\rho}\left[\mathcal{L}_{\text {fair }}(f)\right]$ and that $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ is indeed smaller than $2 \mathbb{E}_{\rho}\left[\mathcal{L}_{\text {fair }}(f)\right]$ in most cases, indicating the inequality (5) is faithful. Similar results are presented in Figures 2 (b) and $21(\mathrm{c})$ as well for inequalities (8) and 9, respectively. Note that the correlation between $\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\rho}\left[\ell_{\text {fair }}(f, \boldsymbol{x})\right]^{2}\right]$ and $\mathbb{E}_{\rho^{2}}\left[\mathcal{L}_{\text {fair }}\left(f, f^{\prime}\right)\right]$ in Eq. (7) is close to one, indicating that Eq. (7) is faithful. As for Figure $2 \mid(\mathrm{f})$, it shows a relatively high level of correlation between $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ and the generalisation bound in inequality (11) and that $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ is indeed smaller than the generalisation bound in most cases, indicating the inequality 11) for an ensemble

TABLE V

COMPARISON OF THE STATE-OF-THE-ART PRUNING METHODS WITH POAF ON THE TEST ACCURACY (\%). THE COLUMN NAMED "ENSEM" IS THE CORRESPONDING RESULT FOR THE ENTIRE ENSEMBLE WITHOUT PRUNING. THE BEST ACCURACY WITH A LOWER STANDARD DEVIATION IS INDICATED WITH BOLD FONTS FOR EACH DATASET (ROW). (A-C) USING BAGGING, ADABOOSTM1, AND SAMME TO PRODUCE HOMOGENEOUS ENSEMBLES, RESPECTIVELY

| Dataset | Ensem | KP | oO | DREP | SEP | ![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=28&width=100&top_left_y=370&top_left_x=965) | PEP | MRMC | Disc.EP | TSP.AP | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $99.13 \pm 1.94$ | $9.13 \pm 1.94$ |
| credit | $73.90 \pm 1.19$ | $73.10 \pm 3.23$ | $72.70 \pm 2.46$ | $74.10 \pm 2.72$ | $73.70 \pm 2.02$ | $73.90 \pm 1.19$ | $74.10 \pm 1.14$ | $73.20 \pm 3.38$ | $73.10 \pm 2.90$ | $74.30 \pm 3.42$ | $71.50 \pm 0.94$ | $72.70 \pm 2.36$ | $74.10 \pm 3.07$ |
| income | $83.82 \pm 0.44$ | $83.24 \pm 0.57$ | $83.41 \pm 0.63$ | $82.64 \pm 0.95$ | $83.61 \pm 0.34 \dagger$ | $83.82 \pm 0.44$ | $\mathbf{8 3 . 8 2} \pm \mathbf{0 . 4 4}$ | $83.28 \pm 0.40$ | $83.14 \pm 0.49$ | $83.79 \pm 0.48 \dagger$ | $83.20 \pm 0.52$ | $83.17 \pm 0.44$ | $3.04 \pm 0.48$ |
| ppr | $64.68 \pm 0.36$ | $63.96 \pm 0.83$ | $64.42 \pm 0.87$ | $62.47 \pm 1.46$ | $64.24 \pm 1.37 \dagger$ | $64.68 \pm 0.36 \dagger$ | $64.68 \pm 0.36$ | $64.04 \pm 1.06+$ | $63.62 \pm 0.87$ | $64.51 \pm 1.14$ | $63.77 \pm 1.05$ | $63.75 \pm 0.99$ | $62.74 \pm 1.13$ |
| ppvr | $81.17 \pm 0.88$ | $80.95 \pm 0.94$ | $81.37 \pm 1.75$ | $79.40 \pm 2.19$ | $80.65 \pm 1.40 \neq$ | $81.17 \pm 0.88$ | $81.37 \pm 0.92$ | $80.35 \pm 0.97 \neq$ | $80.45 \pm 1.57$ | $81.65 \pm 0.83$ | $81.15 \pm 0.80$ | $80.97 \pm 0.96$ | $82.15 \pm 1.03$ |
| W/T/L | $0 / 3 / 2$ | $0 / 4 / 1$ | $0 / 4$ | $0 / 5 / 0$ | $1 / 2 / 2$ | $0 / 3 / 2$ | $0 / 3 \quad 3$ | $1 / 3 / 1$ | $0 / 5 / 0$ | $0 / 3 / 2$ | $0 / 5 / 0$ | $0 / 5 / 0$ |  |
| av | {fcbbb31cc-764b-4895-a559-d1c5727499fc}4 | 8.3 | 6.6 | 9.8 | 7.0 | 4.4 | 3.6 | 8.2 | 9.9 | 3.6 | 9.0 | 9.2 | 7.0 |

(a)

| Dataset | Ensem | KP | oO | DREP | SEP | OEP | PEP | MRMC | Disc.EP | TSP.AP | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | $\mathbf{1 0 0 . 0 0} \pm 0.00$ | $100.00 \pm 0.00$ | $100.00 \pm 0.00$ | $\mathbf{1 0 0 . 0 0} \pm 0.00$ | $100.00 \pm 0.00$ | $100.00 \pm 0.00$ | $100.00 \pm 0.00$ | $\mathbf{1 0 0 . 0 0} \pm 0.00$ | $\mathbf{1 0 0 . 0 0} \pm 0.00$ | $\mathbf{1 0 0 . 0 0} \pm \mathbf{0 . 0 0}$ | $\mathbf{1 0 0 . 0 0} \pm \mathbf{0 . 0 0}$ | $100.00 \pm 0.00$ | $\mathbf{1 0 0 . 0 0} \pm \mathbf{0 . 0 0}$ |
| credit | $\mathbf{7 1 . 6 0} \pm \mathbf{1 . 7 8}$ | $69.50 \pm 2.62$ | $71.00 \pm 1.94$ | $70.00 \pm 3.26$ | $68.50 \pm 1.58$ | $71.50 \pm 2.47 \dagger$ | $70.40 \pm 2.53$ | $69.80 \pm 2.33$ | $69.60 \pm 1.08 \dagger$ | $70.30 \pm 3.55$ | $71.40 \pm 2.01 \dagger$ | $70.10 \pm 1.08 \dagger$ | $67.50 \pm 3.89$ |
| income | $83.32 \pm 0.28 \dagger$ | $83.29 \pm 0.28 \dagger$ | $83.14 \pm 0.27 \dagger$ | $80.96 \pm 0.47$ | $82.07 \pm 1.57$ | $83.05 \pm 0.40 \dagger$ | $83.29 \pm 0.28 \dagger$ | $82.91 \pm 0.44 \dagger$ | $83.29 \pm 0.28 \dagger$ | $83.11 \pm 0.23 \dagger$ | $83.11 \pm 0.23 \dagger$ | $80.65 \pm 0.82$ | $80.51 \pm 2.05$ |
| ppr | $63.30 \pm 1.54$ | $62.79 \pm 1.77$ | $62.86 \pm 0.97$ | $61.59 \pm 2.40$ | $60.96 \pm 1.44$ | $62.74 \pm 1.37$ | $62.74 \pm 1.37$ | $62.87 \pm 1.37 \dagger$ | $62.01 \pm 2.04$ | $62.86 \pm 1.23$ | $62.86 \pm 1.60 \dagger$ | $60.71 \pm 2.00$ | $58.49 \pm 3.41$ |
| ppvr | $81.57 \pm \mathbf{0 . 4 6}$ | $81.40 \pm 0.51$ | $81.40 \pm 0.51$ | $78.55 \pm 0.91 \neq$ | $79.58 \pm 1.75$ | $81.52 \pm 0.59$ | $81.55 \pm 0.72$ | $81.25 \pm 0.74$ | $81.40 \pm 0.51$ | $80.25 \pm 1.77$ | $79.75 \pm 2.13$ | $78.68 \pm 1.97$ | $80.85 \pm 1.64$ |
| $\underset{\text { Wरor rank }}{\text { W/T }}$ | $0 / 3 / 2$ | 0/4/1/1 | $0 / 4 / 1$ | $1 / 4 / 0$ <br> 08 | $0 / 5 / 0$ <br> 102 | $0 / 3 / 2$ | $0 / 4 / 1$ <br> 40 | $0 / 3 / 2$ | $0 / 3 / 2$ | $0 / 4 / 1$ | $0 / 2 / 3$ <br> 50 | $0 / 4 / 1$ <br> 100 | $\overline{108}$ |

(b)

| Dataset | Ensem | KP | $\mathrm{OO}$ | DREP | SEP | OEP | PEP | MRMC | Disc.EP | TSP.AP | EPAF-C | EPAF-D | POAF |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ricci | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $99.13 \pm 1.94$ | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $98.26 \pm 2.38$ | $99.13 \pm 1.94$ | $98.26 \pm 2.38$ | $26 \pm 2.38$ |
|  | $0 \pm 2.33$ | $\pm 2.17$ | $00 \pm 3.94$ | $70.70 \pm 2.41$ | $70.50 \pm 2.09$ | $73.30 \pm 3.19$ | $73.50 \pm 3.94$ | $71.00 \pm 1.06$ | $70.30 \pm 3.13$ | $72.30 \pm 2.17$ | $70.80 \pm 2.28$ | $\pm 1.95$ |  |
| income | $83.04 \pm 0.43$ | $82.98 \pm 0.43 \dagger$ | $82.73 \pm 0.57 *$ | $81.06 \pm 0.91$ | $81.48 \pm 1.17$ | $82.50 \pm 0.59 \dagger$ | $82.85 \pm 0.52 \dagger$ | $82.50 \pm 0.59 \dagger$ | $82.98 \pm 0.43 \dagger$ | $82.68 \pm 0.49 \dagger$ | $82.10 \pm 1.31$ | $79.84 \pm 1.60$ | $80.64 \pm 1.63$ |
|  |  |  |  |  |  |  | $63.64 \pm 1.03$ | $62.56 \pm 1.33$ | $62.65 \pm 0.40$ | $63.25 \pm 0.77$ | $62.68 \pm 0.53$ | $61.79 \pm 0.92$ | $61.23 \pm 1.93$ |
| pppr <br> pppr | $81.17 \pm 1.03$ <br> $0.05 \pm 0.05$ | $80.90 \pm 1.09$ | $80.90 \pm 1.09$ | $77.73 \pm 1.50$ | $80.72 \pm 2.09$ | $80.90 \pm 1.09$ | $81.00 \pm 1.11$ | $80.55 \pm 1.03$ | $80.90 \pm 1.09$ | $79.48 \pm 2.55$ | $79.13 \pm 2.14 \neq$ | $77.58 \pm 1.51$ | $80.22 \pm 1.96$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=28&width=62&top_left_y=965&top_left_x=450) | 0 | $0 / 4 / 1$ | $0 / 4 / 1$ | 0/5/0 | 0/5/5/0 | $0 / 4 / 1$ | $0 / 4 / 1$ | $0 / 4 / 1$ | 0/4/1 | $0 / 4 / 1$ | 0/5/0 | $1 / 4 / 0$ | $\overline{1}, 0$ |

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=312&width=374&top_left_y=1134&top_left_x=320)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=298&width=371&top_left_y=1485&top_left_x=324)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=303&width=377&top_left_y=1144&top_left_x=690)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=301&width=377&top_left_y=1484&top_left_x=690)

(f)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=301&width=393&top_left_y=1145&top_left_x=1061)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=308&width=383&top_left_y=1480&top_left_x=1058)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=301&width=371&top_left_y=1145&top_left_x=1446)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-09.jpg?height=296&width=385&top_left_y=1489&top_left_x=1426)

(h)

Fig. 5. Comparison of the state-of-the-art methods with $P O A F$, using AdaBoostM1 to conduct homogeneous ensembles. (a-d) Friedman test chart on three group fairness measures (i.e., DP, EO, and $\mathrm{PQP}$ ) and the discrepancy of test accuracy between the privileged group and marginalised groups, respectively, rejecting the null hypothesis at $5 \%$ significance level. (e-h) The aggregated rank.

is reliable. Similar results for one single individual classifier are presented in Figure $2 \mid \mathrm{e})$ to demonstrate that inequality 10 . is reliable.

## C. Comparison between POAF and fairness-aware ensemblebased methods

In this subsection, we validate the effectiveness of $P O A F$ presented in Section III-D, compared with existing fairnessaware ensemble-based methods. The baseline ensemble methods used in this subsection include bagging [49], lightGBM [52], FairGBM [42], and AdaFair [40]. For baselines, an ensemble will be trained on the training set and then evaluated on the test set; For the proposed pruning method, an ensemble will be trained and pruned on the training set and then evaluated on the test set. The experimental results are reported in Figure 3 and Tables II and III

As we can see from Figure $31(\mathrm{a}), P O A F$ is significantly better than AdaFair, although it shows comparable performance yet is not better than FairGBM. Figures $31(\mathrm{c})+3(\mathrm{~d})$ and $31(\mathrm{f})-3(\mathrm{~g})$ indicate that the fairness level of $P O A F$ is surely better than the corresponding unpruned ensembles (aka. bagging); besides, the fairness level of $P O A F$ is also nearly the best, compared with FairGBM and AdaFair. Therefore, $P O A F$ could be viewed as a method that improves fairness with acceptable accuracy damage. The same results are also reported in Tables II and III To be specific, Table I|(a) contains the average test accuracy

TABLE VI

COMPARISON OF THE STATE-OF-THE-ART PRUNING METHODS WITH POAF ON THREE GROUP FAIRNESS MEASURES, USING DT AS INDIVIDUAL MEMBERS

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-10.jpg?height=25&width=1787&top_left_y=237&top_left_x=169)
ATTRIBUTES ON THE DATASET. THE BEST FAIRNESS WITH A LOWER STANDARD DEVIATION IS INDICATED WITH BOLD FONTS, WHERE THE BEST FAIRNESS IS REPRESENTED BY THE SMALLEST NUMERICAL DISCREPANCY BETWEEN THE PRIVILEGED GROUP AND MARGINALISED GROUPS. (A-C) COMPARISON ON THE TEST SET USING DP, EO, AND PQP, RESPECTIVELY

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-10.jpg?height=288&width=1434&top_left_y=363&top_left_x=343)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-10.jpg?height=285&width=1431&top_left_y=703&top_left_x=347)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-10.jpg?height=288&width=1431&top_left_y=1035&top_left_x=347)

(C)

of each method and the corresponding standard deviation under 5 -fold cross-validation on each dataset concerning each sensitive attribute; Table II|(b) and Tables III)(a) to III|(d) contain the average values of $\mathrm{f}_{1}$ score, DR, DP, EO, and PQP alongside the corresponding standard deviation on the test set accordingly as well. For instance, each row (one sensitive attribute of one of the datasets) in Table II||a) compares the test discriminative risk with the same type of individual classifiers (that is, decision trees in this case), indicating the best results by bold fonts. Although $P O A F$ is not the best among itself, AdaFair, and FairGBM, the average ranks of $P O A F$ are still relatively high among all opponents from all criteria aspects except for the $\mathrm{f}_{1}$ score. Furthermore, $P O A F$ is better than the corresponding unpruned ensembles (aka. bagging) in most of the cases, which means that POAF can indeed improve fairness and accuracy concurrently. Therefore, we can view the proposed DR measure does its work and functions well in $P O A F$.

## D. Comparison between POAF and the state-of-the-art (SOTA) ensemble pruning methods

In this subsection, we validate the effectiveness of $P O A F$ as an ensemble pruning method, in comparison with existing various pruning methods. An ensemble will be trained and pruned on the training set and then evaluated on the test set. The experimental results are reported in Figure 4 and Tables IVV. Furthermore, we also validate the ensemble fairness after using $P O A F$ to check if our primary goal of boosting fairness is fulfilled. Including DR defined in Section III-A three group fairness measures are taken into account as baselines to measure the level of bias in algorithms, that is, DP, EO, and PQP. The experimental results are reported in Figures 4 and 9 as well as Tables VI VIII.

Baseline pruning methods To evaluate our proposed pruning method $P O A F$, the baselines that we considered are a variety of ranking-based methods, namely KL-divergence pruning (KL), Kappa pruning (KP) [53], reduce error pruning (RE) [54], complementarity measure pruning (CM) [55], [56], orientation ordering pruning (OO) [57], diversity regularised ensemble pruning (DREP) [58], ordering-based ensemble pruning (OEP) [44], accuracy-based ensemble pruning (TSP.AP), and diversitybased ensemble pruning (TSP.DP) [59], as well as optimisationbased methods, namely single-objective ensemble pruning (SEP), Pareto ensemble pruning (PEP) [44], the maximal relevant minimal redundant (MRMR) method, the maximum relevancy maximum complementary (MRMC) method [60], and the discriminative ensemble pruning (DiscEP) [56]. Moreover, we adopt two methods from diversity maximisation via composable core-sets and change them slightly to make them suitable for pruning problems, namely Gonzalez's algorithm (GMA) and local search algorithm (LCS) [61], [62].

a) POAF achieves the best fairness with acceptable accuracy performance: Table $\mathrm{V}$ contains the average test accuracy of each method and the corresponding standard deviation under 5 -fold cross-validation on each dataset; Table IV

TABLE VII

COMPARISON OF THE SOTA PRUNING METHODS WITH POAF ON THREE GROUP FAIRNESS MEASURES, USING DT AS INDIVIDUAL MEMBERS AND ADABOOSTM1 TO PRODUCE HOMOGENEOUS ENSEMBLES. (A-C) COMPARISON ON THE TEST SET USING DP, EO, AND PQP, RESPECTIVELY. NOTE THAT THE NOTATIONS USED HERE ARE THE SAME AS TABLEVI

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-11.jpg?height=288&width=1395&top_left_y=325&top_left_x=365)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-11.jpg?height=279&width=1418&top_left_y=668&top_left_x=359)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-11.jpg?height=287&width=1431&top_left_y=995&top_left_x=347)

(c)

contains the average fairness quality of each method and the corresponding standard deviation on the test set accordingly as well. For instance, each row (dataset) in Table IV compares the test fairness quality with the same type of individual classifiers, indicating the best results by bold fonts. Besides, the fairness quality performance between two pruning methods is examined by two-tailed paired $t$-tests at $5 \%$ significance level to tell if they have significantly different results. Two methods end in a tie if there is no significant statistical difference; otherwise, the one with better values of the proposed fairness quality will win. The performance of each method is reported in the last two rows of Table IV compared with $P O A F$ in terms of the average rank and the number of datasets that $P O A F$ has won, tied, or lost, respectively. As we can see in each sub-table of Table IV. the average ranks of $P O A F$ outperform all baseline pruning methods including the original ensemble on the test fairness quality, which means that $P O A F$ substantially achieves the best fairness performance. We may also notice that $P O A F$ does not rank so high in Table V, yet it still settles with ties in most of the cases, which means that $P O A F$ could at least achieve acceptable accuracy performance, compared with other baseline methods. Figure 4)(a) shows that $P O A F$ could achieve at least the same level of test accuracy compared to baselines, and the same observation is also presented in Figures $4(\mathrm{~b}), 4(\mathrm{e})$ and $4 \mid \mathrm{f})$ Besides, Figures $4(\mathrm{c})$ and $41(\mathrm{~d})$ indicate that $P O A F$ achieves the best fairness quality among all opponents, while Figure $4 \mid \mathrm{h}$ ) shows that $P O A F$ achieves the best performance on $\mathcal{L}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$. It is worth mentioning that the test fairness quality on the Ricci dataset in Table IV shows extraordinarily good results, of which the reason is probably that the size of Ricci is rather small.

## b) POAF achieves good fairness performance on three

 group fairness measures: We have reported Figure 4 to demonstrate that $P O A F$ can improve fairness in the ensembles without much accuracy degradation. Here we provide extra results in Tables VI-VIII as well as Figures $5-6$ and Figure 9 to demonstrate that the aforementioned observation is not transitory. As we may see from Table VI, it contains the average fairness values of each method on the test set and the corresponding standard deviation under 5 -fold cross-validation on each sensitive attribute. For instance, each row (attribute of one dataset) in Table VI compares the discrepancy between the privileged group and marginalised groups of each method on the test set, using three group fairness measures in three sub-tables, respectively. Besides, two-tailed paired $t$-tests at $5 \%$ significance level are used to examine if two pruning methods have significantly different results on the group fairness measures. Two methods end in a tie if there is no significant statistical difference; otherwise, the one with the numerical result that is closer to zero will win, as it means that the treatment between the privileged group and marginalised groups is closer to equality. The performance of each method is reported in the last two rows of Table VI , compared with $P O A F$ in terms of the average rank and the number of datasets that POAF has won, tied, or lost, respectively. As shown in Tables VI, VII|(a) VII|(b) and VII||(a) VIII|(b), POAF achievesTABLE VIII

COMPARISON OF THE SOTA PRUNING METHODS WITH POAF ON THREE GROUP FAIRNESS MEASURES, USING DT AS INDIVIDUAL MEMBERS AND SAMME TO PRODUCE HOMOGENEOUS ENSEMBLES. (A-C) COMPARISON ON THE TEST SET USING DP, EO, AND PQP, RESPECTIVELY. NOTE THAT THE NOTATIONS USED HERE ARE THE SAME AS TABLEVI

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-12.jpg?height=288&width=1401&top_left_y=325&top_left_x=362)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-12.jpg?height=288&width=1395&top_left_y=661&top_left_x=365)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-12.jpg?height=287&width=1418&top_left_y=995&top_left_x=359)

(c)

the best or nearly the best average ranking from the perspective of three group fairness measures, which indicates that $P O A F$ could obtain fairer ensemble classifiers than most of the baseline pruning methods, not only in terms of our proposed DR, but also in terms of DP, EO, and PQP. As for Tables VIl(c) and VII|(c), $E P A F-C$ shows the potential to achieve fairer subensembles as well in terms of PQP. Besides, Figure 5 shows that $P O A F$ achieves competitive accuracy and high rankings on group fairness (especially, DP and EO) compared with baselines. Similar observations are also presented in Figure 6 demonstrating that $P O A F$ achieves the best fairness quality from the DP and EO perspectives. Furthermore, Figure 9 depicts the discrepancy between the privileged group and marginalised groups on the Income dataset, indicating that $P O A F$ works generally with the target of improving fairness despite the type of ensemble classifiers. To be specific, Figure 9 indicates that $P O A F$ achieves the best fairness quality $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ and objective $\mathcal{L}\left(\mathbf{w v}_{\rho}\right)$ compared with baselines; Moreover, even from the perspective of other group fairness measures, $P O A F$ still achieves the best results of fairness on each sensitive attribute in most cases.

Additionally, the space cost between $P O A F$ and baselines are reported in Figure 7, to depict the size of pruned subensembles after using different pruning methods. It shows that part of the pruning methods cannot fix the size of the pruned sub-ensemble in advance, including OO, DREP, SEP, OEP, PEP, TSP.AP, TSP.DP, and POAF . Extra analysis to reveal the effect of $\lambda$ value is provided in Section IV-F.

## E. EPAF-D vs. EPAF-C over time cost

To verify whether $E P A F-D$ could achieve competitive performance quicker than $E P A F-C$ or not, we employ different numbers of machines and compare their speedup and efficiency $6^{6}$ Empirical results are reported in Figure 8, wherein speedup is calculated as the ratio of the execution time of $E P A F-C$ to that of $E P A F-D$. The efficiency in Figure 8 can achieve 1, indicating that $E P A F-D$ scales linearly and even super-linearly in a few cases.$^{7}$

## F. Effect of $\lambda$ value in POAF

In this subsection, we investigate the effect of the regularisation factor $\lambda$ that may cause on two sub-objectives in the defined objective function of Eq. (13). The experimental results are reported in Figure 10

As we may see from Figure 1 (c) $P O A F$ beats both $E P A F-C$ and $E P A F$-D along with the original ensemble in comparison of the objective function Eq. 13, no matter which value is assigned to $\lambda$. Nevertheless, different values of $\lambda$ may directly affect the results of (sub-)ensembles in terms of sub-objectives and the discrepancy treatment between the privileged group[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=301&width=374&top_left_y=215&top_left_x=320)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=303&width=374&top_left_y=564&top_left_x=320)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=301&width=377&top_left_y=218&top_left_x=690)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=307&width=377&top_left_y=562&top_left_x=690)

(f)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=301&width=376&top_left_y=218&top_left_x=1075)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=304&width=382&top_left_y=561&top_left_x=1056)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=303&width=369&top_left_y=217&top_left_x=1450)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=314&width=398&top_left_y=561&top_left_x=1422)

(h)

Fig. 6. Comparison of the state-of-the-art methods with $P O A F$, using SAMME to conduct homogeneous ensembles. (a-d) Friedman test chart on three group fairness measures (i.e., DP, EO, and PQP) and the discrepancy of test accuracy between the privileged group and marginalised groups, respectively, rejecting the null hypothesis at $5 \%$ significance level. (e-h) The aggregated rank.

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=344&width=436&top_left_y=1129&top_left_x=172)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=347&width=425&top_left_y=1125&top_left_x=617)

(b)
Fig. 7. Comparison of the state-of-the-art pruning methods with $P O A F$, where the dashed and solid lines indicate the size of the original ensemble and the expected size of the pruned sub-ensemble, respectively. (a-b) Comparison over the space and time cost, respectively.

and marginalised groups. Figures $1 प \mid(\mathrm{a})$ and $10 \mid(\mathrm{d})$ indicate that $P O A F$ usually achieves the best accuracy performance when fairness and accuracy are considered equally important in the objective function; Figures 10)(b), 1Q|(e), and 10.(f) depict that $P O A F$ reaches the best fairness performance in the same situation, in terms of the proposed fairness quality in Section III-A and two group fairness measures (namely DP and $\mathrm{EO}$ ).

## G. Discussion and limitations

Discrimination mitigation techniques are meaningful given the wide applications of ML models nowadays, therefore, bringing such a technique with learning guarantees matters as it could provide theoretical foundations to boost fairness without potentially vain and repetitive practical attempts. In this view, our work throws away a brick in order to get a gem, showing that fairness can indeed be boosted with learning guarantees instead of being dependent on specific (hyper)parameters. The proposed discriminative risk measure and the proposed oracle bounds are suitable for both binary and

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=794&width=864&top_left_y=1121&top_left_x=1094)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=338&width=414&top_left_y=1132&top_left_x=1105)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=338&width=420&top_left_y=1517&top_left_x=1102)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=341&width=420&top_left_y=1128&top_left_x=1530)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-13.jpg?height=344&width=423&top_left_y=1514&top_left_x=1529)

(d)
Fig. 8. Comparison of speedup and efficiency between $E P A F-C$ and $E P A F-D$. (a-b) Speedup with two and three machines, respectively; (c-d) Efficiency with two and three machines, respectively.

multi-class classification, enlarging the applicable fields, which is advantageous. However, there are also limitations in the proposed measure and the pruning method $P O A F$. For instance, the computation of discriminative risk is relevant to disturbed sensitive attributes, which means a randomness factor exists and may affect somehow computational results. Besides, although POAF could achieve acceptable performance, its time cost is relatively high compared with the baselines, which may restrict its applicable scenarios if the response time is one of the major criteria. Therefore, the effect of randomness on discriminative risk and acceleration of $P O A F$ are worth exploring in the future.

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=1030&width=1479&top_left_y=203&top_left_x=323)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=293&width=355&top_left_y=217&top_left_x=343)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=290&width=369&top_left_y=554&top_left_x=347)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=287&width=371&top_left_y=884&top_left_x=346)

(i)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=293&width=352&top_left_y=217&top_left_x=713)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=293&width=358&top_left_y=550&top_left_x=710)

(f)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=287&width=356&top_left_y=884&top_left_x=711)

(j)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=287&width=355&top_left_y=220&top_left_x=1075)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=293&width=358&top_left_y=550&top_left_x=1073)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=293&width=377&top_left_y=881&top_left_x=1058)

(k)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=296&width=361&top_left_y=215&top_left_x=1424)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=298&width=361&top_left_y=545&top_left_x=1424)

(h)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=287&width=361&top_left_y=884&top_left_x=1424)

(I)

Fig. 9. Comparison of objective functions (i.e., $\mathcal{L}_{\text {err }}\left(\mathbf{w} \mathbf{v}_{\rho}\right), \mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$, and $\left.\mathcal{L}\left(\mathbf{w} \mathbf{v}_{\rho}\right)\right)$ and three group fairness measures (i.e., DP, EO, and PQP) on the Income dataset. (a-d), (e-h), and (i-1) Using bagging, AdaBoostM1, and SAMME to conduct homogeneous ensembles, respectively. Note that (b), (f), and (j) indicate the fairness discrepancy on the Race attribute, (c), (g), and (k) indicate that on the Sex attribute, and (d), (h), and (l) indicate that on the joint sensitive attribute, respectively, wherein the closer to zero, the better. Note that the smaller the better in (a), (e), and (i).

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=732&width=1548&top_left_y=1420&top_left_x=294)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=301&width=385&top_left_y=1432&top_left_x=312)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=303&width=390&top_left_y=1781&top_left_x=304)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=301&width=377&top_left_y=1435&top_left_x=690)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=307&width=380&top_left_y=1779&top_left_x=688)

(f)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=301&width=393&top_left_y=1432&top_left_x=1058)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=309&width=393&top_left_y=1775&top_left_x=1061)

( $\mathrm{g}$ )

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=303&width=379&top_left_y=1434&top_left_x=1445)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_ebfbbce8a56664040d90g-14.jpg?height=309&width=382&top_left_y=1775&top_left_x=1446)

(h)

Fig. 10. Effect of $\lambda$ value on the Ricci dataset using bagging with MLPs as individual classifiers. (a-c) Two sub-objectives used in the domination relationship (that is, $\mathcal{L}_{\text {err }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ and $\mathcal{L}_{\text {fair }}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ ) and the objective function $\mathcal{L}\left(\mathbf{w} \mathbf{v}_{\rho}\right)$ in Eq. 113, respectively; the smaller the better. (d) Test accuracy (\%); the larger the better. (e-h) The test discrepancy between the privileged group and marginalised groups using three group fairness measures (i.e., DP, EO, and PQP) and the test accuracy, respectively, wherein the closer the numerical value is to zero, the better result it shows.

## V. CONCLUSION

We have presented a novel analysis of the expected fairness quality via weighted vote and demonstrated that fairness can be boosted due to the cancellation-of-biases effect of the combination. We have also shown that POAF based on domination and Pareto optimality can fulfill the expectations of achieving fairer ensembles without much accuracy decline, confirmed by extensive empirical results. Our work shows that combination could boost fairness with theoretical learning guarantees, which is helpful to save some fruitless efforts on (hyper-)parameter tuning.

## ACKNOWLEDGMENTS

This research is supported by the National Science Foundation (NSF:2010778), the National Research Foundation, Singapore, and the Agency for Science Technology and Research (A*STAR), Singapore, under its Prenatal/Early Childhood Grant (Grant No. H22P0M0007), and by the Singapore Ministry of Education (MOE2019-T2-2-094, T2EP201230007, FRC Tier 1). Additional support is provided by the A*STAR Computational Resource Centre through the use of its high-performance computing facilities, the Hong Kong global STEM scholar scheme, the internal fund of the Hong Kong Polytechnic University, and STI 2030-Major Projects (No.2022ZD0209000).

The authors would like to personally thank Prof. Yevgeny Seldin at the University of Copenhagen for discussing with them and thank Prof. Huanhuan Chen at the University of Science and Technology of China for their help and support.

## REFERENCES

[1] S. Verma and J. Rubin, "Fairness definitions explained," in FairWare. IEEE, 2018, pp. 1-7.

[2] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamilton, and D. Roth, "A comparative study of fairness-enhancing interventions in machine learning," in FAT. New York, NY, USA: Association for Computing Machinery, 2019, pp. 329-338.

[3] C. Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson, "Decoupled classifiers for group-fair and efficient machine learning," in FAT, vol. 81 PMLR, 2018, pp. 119-133.

[4] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, "Certifying and removing disparate impact," in SIGKDD New York, NY, USA: Association for Computing Machinery, 2015, pp. 259-268.

[5] P. Gajane and M. Pechenizkiy, "On formalizing fairness in prediction with machine learning," in FAT/ML, 2018.

[6] M. Hardt, E. Price, and N. Srebro, "Equality of opportunity in supervised learning," in NIPS, vol. 29. Red Hook, NY, USA: Curran Associates Inc., 2016, pp. 3323-3331.

[7] A. Chouldechova, "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments," Big Data, vol. 5, no. 2, pp. 153-163, 2017.

[8] M. Joseph, M. Kearns, J. H. Morgenstern, and A. Roth, "Fairness in learning: Classic and contextual bandits," in NIPS, vol. 29. Curran Associates, Inc., 2016.

[9] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, "Fairness through awareness," in ITCS. New York, NY, USA: Association for Computing Machinery, 2012, pp. 214-226.

[10] S. Barocas, M. Hardt, and A. Narayanan, Fairness and machine learning. fairmlbook.org, 2019.

[11] R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth, "Fairness in criminal justice risk assessments: The state of the art," Sociol Methods Res, vol. 50, no. 1, pp. 3-44, 2021.

[12] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger, "On fairness and calibration," in NIPS, vol. 30. Curran Associates, Inc. 2017.

[13] A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner, "Scalable fair clustering," in ICML, vol. 97. PMLR, 2019, pp. 405-413.

[14] S. Samadi, U. Tantipongpipat, J. H. Morgenstern, M. Singh, and S. Vempala, "The price of fair pca: One extra dimension," in NeurIPS, vol. 31. Curran Associates, Inc., 2018.

[15] F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii, "Fair clustering through fairlets," in NIPS, vol. 30. Curran Associates, Inc., 2017.

[16] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R Varshney, "Optimized pre-processing for discrimination prevention," in NIPS, vol. 30. Curran Associates, Inc., 2017.

[17] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. S. Zemel, "The variational fair autoencoder," in ICLR, 2016.

[18] F. Kamiran and T. Calders, "Data preprocessing techniques for classification without discrimination," Knowl Inf Syst, vol. 33, no. 1, pp. 1-33, 2012.
[19] A. K. Menon and R. C. Williamson, "The cost of fairness in binary classification," in FAT, vol. 81. PMLR, 2018, pp. 107-118.

[20] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, "Algorithmic decision making and the cost of fairness," in SIGKDD. New York, NY, USA: Association for Computing Machinery, 2017, pp. 797-806.

[21] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, "Fairness beyond disparate treatment \& disparate impact: Learning classification without disparate mistreatment," in $W W W$, Republic and Canton of Geneva, CHE, 2017, pp. 1171-1180.

[22] M. B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi, "Fairness constraints: Mechanisms for fair classification," in AISTATS, vol. 54 PMLR, 2017, pp. 962-970.

[23] B. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro, "Learning non-discriminatory predictors," in COLT, vol. 65. PMLR, 2017, pp. 1920-1953

[24] N. Quadrianto and V. Sharmanska, "Recycling privileged learning and distribution matching for fairness," in NIPS, vol. 30. Curran Associates, Inc., 2017.

[25] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, "Fairness-aware classifier with prejudice remover regularizer," in ECML-PKDD. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 35-50.

[26] F. Kamiran, T. Calders, and M. Pechenizkiy, "Discrimination aware decision tree learning," in ICDM. IEEE, 2010, pp. 869-874.

[27] A. Agarwal, M. Dudík, and Z. S. Wu, "Fair regression: Quantitative definitions and reduction-based algorithms," in ICML, vol. 97. PMLR, 2019, pp. 120-129

[28] A. Agarwal, A. Beygelzimer, M. Dudík, J. Langford, and H. Wallach, "A reductions approach to fair classification," in ICML, vol. 80. PMLR, 2018, pp. 60-69.

[29] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, "Learning fair representations," in ICML, Atlanta, Georgia, USA, 2013, pp. 325-333.

[30] N. Grgić-Hlača, M. B. Zafar, K. P. Gummadi, and A. Weller, "Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning," in AAAI, vol. 32, no. 1. AAAI Press, 2018, pp. 51-60.

[31] I. Žliobaitė, "Measuring discrimination in algorithmic decision making," Data Min Knowl Discov, vol. 31, no. 4, pp. 1060-1089, 2017.

[32] J. Kleinberg, S. Mullainathan, and M. Raghavan, "Inherent trade-offs in the fair determination of risk scores," in ITCS, vol. 67. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017, p. 43.

[33] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, "Enhancement of the neutrality in recommendation," in RecSys Workshop on Decisions, 2012, pp. 8-14.

[34] T. Calders and S. Verwer, "Three naive bayes approaches for discrimination-free classification," Data Min Knowl Discov, vol. 21, no. 2, pp. 277-292, 2010

[35] H. Nilforoshan, J. D. Gaebler, R. Shroff, and S. Goel, "Causal conceptions of fairness and their consequences," in ICML, vol. 162. PMLR, 2022, pp. 16848-16887.

[36] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, "Counterfactual fairness," in NIPS, vol. 30. Curran Associates, Inc., 2017, pp. 4069-4079.

[37] Y. Bechavod and K. Ligett, "Penalizing unfairness in binary classification," arXiv preprint arXiv:1707.00044, 2017.

[38] D. Pessach and E. Shmueli, "Improving fairness of artificial intelligence algorithms in privileged-group selection bias data settings," Expert Syst Appl, vol. 185, p. 115667, 2021.

[39] M. Wick, S. Panda, and J.-B. Tristan, "Unlocking fairness: a trade-off revisited," in NeurIPS, vol. 32. Curran Associates, Inc., 2019, pp 8783-8792

[40] V. Iosifidis and E. Ntoutsi, "Adafair: Cumulative fairness adaptive boosting," in CIKM. New York, NY, USA: ACM, 2019, pp. 781-790

[41] W. Zhang, A. Bifet, X. Zhang, J. C. Weiss, and W. Nejdl, "Farf: A fair and adaptive random forests classifier," in PAKDD. Springer, 2021, pp. $245-256$.

[42] A. F. Cruz, C. Belém, J. Bravo, P. Saleiro, and P. Bizarro, "Fairgbm: Gradient boosting with fairness constraints," in ICLR, 2023

[43] A. R. Masegosa, S. S. Lorenzen, C. Igel, and Y. Seldin, "Second order pac-bayesian bounds for the weighted majority vote," in NeurIPS, vol. 33 Curran Associates, Inc., 2020, pp. 5263-5273.

[44] C. Qian, Y. Yu, and Z.-H. Zhou, "Pareto ensemble pruning," in AAAI, vol. 29, no. 1, 2015, pp. 2935-2941.

[45] J. Demšar, "Statistical comparisons of classifiers over multiple data sets," J Mach Learn Res, vol. 7, pp. 1-30, 2006.

[46] S. A. Zadeh, M. Ghadiri, V. Mirrokni, and M. Zadimoghaddam, "Scalable feature selection via distributed diversity maximization," in AAAI, vol. 31, no. 1, 2017, pp. 2876-2883.

[47] Y. Bian, Y. Wang, Y. Yao, and H. Chen, "Ensemble pruning based on objection maximization with a general distributed framework," IEEE Trans Neural Netw Learn Syst, vol. 31, no. 9, pp. 3766-3774, 2020.

[48] Z.-H. Zhou, Machine learning. Springer Nature, 2021.

[49] L. Breiman, "Bagging predictors," Mach Learn, vol. 24, no. 2, pp. 123140,1996 .

[50] Y. Freund, R. E. Schapire et al., "Experiments with a new boosting algorithm," in ICML, vol. 96. Citeseer, 1996, pp. 148-156.

[51] Y. Freund, "Boosting a weak learning algorithm by majority," Inf Comput, vol. 121, no. 2, pp. 256-285, 1995.

[52] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu, "Lightgbm: A highly efficient gradient boosting decision tree," in NIPS, vol. 30, 2017, pp. 3146-3154.

[53] D. Margineantu and T. Dietterich, "Pruning adaptive boosting," in ICML, vol. 97, 1997, pp. 211-218.

[54] G. Martínez-Muñoz, D. Hernández-Lobato, and A. Suárez, "An analysis of ensemble pruning techniques based on ordered aggregation," IEEE Trans Pattern Anal Mach Intell, vol. 31, no. 2, pp. 245-259, 2009.

[55] G. Martınez-Munoz and A. Suárez, "Aggregation ordering in bagging," in AIA. Citeseer, 2004, pp. 258-263.

[56] J. Cao, W. Li, C. Ma, and Z. Tao, "Optimizing multi-sensor deployment via ensemble pruning for wearable activity recognition," Inf Fusion, vol. 41, pp. 68-79, 2018

[57] Martínez-Muñoz and Suárez, "Pruning in ordered bagging ensembles," in ICML, 2006, pp. 609-616.

[58] N. Li, Y. Yu, and Z.-H. Zhou, "Diversity regularized ensemble pruning," in ECML-PKDD, 2012, pp. 330-345.

[59] H. Zhang, Y. Song, B. Jiang, B. Chen, and G. Shan, "Two-stage bagging pruning for reducing the ensemble size and improving the classification performance," Math Probl Eng, vol. 2019, 2019.

[60] X. Xia, T. Lin, and Z. Chen, "Maximum relevancy maximum complementary based ordered aggregation for ensemble pruning," Appl Intell, vol. 48, no. 9, pp. 2568-2579, 2018.

[61] S. Aghamolaei, M. Farhadi, and H. Zarrabi-Zadeh, "Diversity maximization via composable coresets," in CCCG, 2015, p. 43.

[62] P. Indyk, S. Mahabadi, M. Mahdian, and V. Mirrokni, "Composable core-sets for diversity and coverage maximization," in $P O D S, 2014$, pp $100-108$.

[63] B. Guedj, "A primer on pac-bayesian learning," arXiv preprint arXiv:1901.05353, 2019.

[64] B. Guedj and J. Shawe-Taylor, "A primer on pac-bayesian learning," Tech. Rep., 2019.
