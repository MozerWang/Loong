# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search 

Yuchen Zhuang ${ }^{1 *}$, Xiang Chen ${ }^{2}$, Tong Yu ${ }^{2}$, Saayan Mitra ${ }^{2}$<br>Victor Bursztyn ${ }^{2}$, Ryan A. Rossi ${ }^{2}$, Somdeb Sarkhel ${ }^{2}$, Chao Zhang ${ }^{1}$<br>Georgia Institute of Technology ${ }^{1}$ Adobe Research ${ }^{2}$<br>yczhuang@gatech.edu, \{xiangche, tyu, smitra\}@adobe.com<br>\{soaresbu, ryrossi, sarkhel\}@adobe.com, chaozhang@gatech.edu


#### Abstract

Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the $\mathrm{A}^{*}$ search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by $3.1 \%$ and $3.5 \%$ on average while requiring $7.35 \mathrm{x}$ and $2.31 \mathrm{x}$ less time, respectively.


## 1 INTRODUCTION

Large language models (LLMs), such as GPT (Radford et al., 2018; 2019; Brown et al., 2020; OpenAI, 2023) and PaLM (Chowdhery et al., 2022; Anil et al., 2023), have exhibited remarkable capabilities of reasoning and instruction-following across a wide range of tasks (Huang \& Chang, 2023). Recently, instructing LLMs to utilize external tools for complex real-world problems has emerged as a topic of growing importance (Hao et al., 2023b; Zhang et al., 2023; Zhuang et al., 2023; Yang et al., 2023b; Schick et al., 2023; Lu et al., 2023). For complicated tasks, LLM-based autonomous agents integrate LLMs with various external tools (APIs), generating solutions that involve intermediate reasoning steps (Schick et al., 2023; Lu et al., 2023; Patil et al., 2023; Qin et al., 2023b). Given a problem description, the goal of an agent is to determine a chain of API function calls that can be executed sequentially toward a valid solution. However, given an action space of hundreds of candidate API functions, each comprised of various function names and parameters available at every planning step, searching for a globally optimal solution becomes highly challenging.

Existing methods that leverage LLMs as autonomous agents for decision-making and reasoning can be broadly classified into four categories (Figure 1): (1) open-loop methods (Wei et al., 2022; Zhou et al., 2022; Huang et al., 2022a; Shen et al., 2023; Lu et al., 2023) generate a complete plan for problemsolving without any adaptation during the execution; (2) greedy closed-loop methods (Yao et al., 2023b; Jang, 2023; Huang et al., 2022b; Kim et al., 2023; Liang et al., 2022) leverage environmental feedback to greedily determine the next step in the plan; and (3) closed-loop methods (Wang et al., 2023; Sun et al., 2023) incorporate environment feedback to continuously monitor system behaviors[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-02.jpg?height=92&width=268&top_left_y=363&top_left_x=389)

Open-Loop System

(e.g., Chain-of-Thoughts)

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-02.jpg?height=260&width=317&top_left_y=279&top_left_x=725)

Greedy Closed-Loop System (e.g., ReAct)

Potential Actions

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-02.jpg?height=284&width=336&top_left_y=281&top_left_x=1060)

(e.g., AdaPlanner)

Explored Actions $\bigotimes$ Infeasible Options

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-02.jpg?height=262&width=334&top_left_y=281&top_left_x=1405)

Tree Search-based System

(e.g., Tree of Thoughts)

Figure 1: A comparison of existing methods that leverage LLMs for decision-making from a searching space perspective. Most existing methods of (1) open-loop systems (e.g., Chain-of-Thought (Wei et al., 2022)), (2) greedy closed-loop systems (e.g., ReAct (Yao et al., 2023b)), and (3) closed-loop systems (e.g., Adaplanner (Sun et al., 2023)) only explore one possible direction. This often leads to limited exploration of the entire action space. In contrast, (4) tree search-based methods (e.g., Tree-of-Thoughts (Yao et al., 2023a)) identify a valid solution path by extensively examining multiple decision space branches, covering almost every conceivable node. Our proposed ToolChain* belongs to the tree search-based category and improves by developing an efficient search algorithm.

and modify subsequent plans accordingly. However, such unidirectional navigation systems have two major limitations: error propagation, originating from a mistaken action and leading to a faulty loop; limited exploration, despite being equipped with plan refinement strategies, most existing methods only explore a small portion of the large action space, falling into locally optimal solutions. To this end, few studies initiate exploring (4) tree search-based methods (Yao et al., 2023a; Hao et al., 2023a) for leveraging multiple reasoning paths simultaneously and evaluating branches to decide the next course of action. However, existing tree search-based algorithms, such as depth-first search (DFS) (Yao et al., 2023a) and Monte Carlo Tree Search (MCTS) (Hao et al., 2023a), require exhaustive exploration of nearly all potential actions within the entire decision space, resulting in inefficient searches for globally optimal solutions.

To address these limitations, we propose ToolChain*, an efficient $\mathrm{A}^{*}$ tree search-based planning method for LLM-based agents. We formulate the tool-use planning process as a decision tree, where each node represents a potential API call for a given step. Aligned with the traditional A* search algorithm, the proposed ToolChain* determines which paths to extend based on both the cost of the current path and an estimated future cost required for completing the current plan. With task-specific cost functions, erroneous actions will be penalized and mitigated, as these actions cause additional costs when propagated along the path, leading the path to be progressively de-prioritized and left unexpanded over iterations. In addition, unlike the simulation stage in MCTS, which requires multiple steps to simulate until a terminal state during rollout, the future cost estimation in ToolChain* enables expansion of only the next step. With efficient node expansion, ToolChain* effectively searches for globally optimal solutions within a manageable number of steps.

Our main contributions are as follows: (1) We propose ToolChain*, a novel A*-like tree search algorithm, to develop autonomous LLM-based agents for complex planning and reasoning tasks; (2) ToolChain* formulates the action space as a decision tree, effectively mitigating error propagation and expanding search space; and (3) ToolChain* significantly accelerates LLM-based agents in navigating expansive action tree spaces, striking a balance between exploring unvisited actions and exploiting global optimal solutions.

## 2 PRELIMINARIES

Problem Formulation. Leveraging LLMs as agents for problem solving can be conceptualized as a planning process. For initialization, the LLM agent is augmented with access to a pool of $m$ candidate API functions, denoted as $\mathcal{A}=\left\{\mathrm{API}_{0}, \mathrm{API}_{1}, \cdots, \mathrm{API}_{m}\right\}$, along with a natural language task description $g \in \mathcal{G}$ from the task space $\mathcal{G}$. The objective of the LLM agent is to translate the task description $g$ into an ordered sequence of $T_{g}$ API function calls $p_{g}=\left\{a_{0}, a_{1}, \cdots, a_{T_{g}}\right\}$. Specifically, considering the task description $g$ as the initial state $s_{0}$, we sample the plan $p_{g}$ by prompting the LLM
agent with the API definitions $\mathcal{I}$ and demonstration samples $\mathcal{D}$ as: $p_{g} \sim \rho\left(a_{0}, a_{1}, \cdots, a_{T_{g}} \mid s_{0} ; \mathcal{I}, \mathcal{D}\right)$ : $\mathcal{G} \times \mathcal{I} \times \mathcal{D} \rightarrow \Delta\left(\mathcal{A}^{T_{g}}\right)$, where $\Delta(\cdot)$ is a probability simplex function. The final output is derived after executing the entire plan $y \sim \pi\left(y \mid s_{0}, a_{1}, a_{2}, \cdots, a_{T_{g}}\right)$, where $\pi(\cdot)$ indicates a plan executor.

Tree Search-Based Systems. Tree search methods frame a planning problem as a search over a decision tree, where each node $n$ represents an action $a_{n}$, accompanied by a state $s_{n} \in \mathcal{S}$ indicating a valid path from the initial state to the current action. When exploring the tree space, tree search approaches expand $k$ potential child nodes $c h(n)$ of the current node $n$ via sampling from the potential action set generated by LLMs $a_{c h(n)}^{(j)} \sim \rho\left(a_{c h(n)} \mid s_{n} ; \mathcal{I}, \mathcal{D}\right),(j=1, \cdots, k)$ and add the new nodes to the tree state space $\mathcal{S}=\mathcal{S} \cup\left\{\left(s_{n}, a_{c h(n)}^{(j)}\right)\right\}_{j=1}^{k}$. With value functions for state evaluation, tree search-based methods aim to identify a path from the root node $s_{0}$ to the leaf nodes with the highest value or lowest cost. Our proposed ToolChain* is a tree search-based method.

Monte Carlo Tree Search. MCTS, which employs heuristic exploration to construct its search tree, has achieved great success in decision-making tasks, such as GO (Silver et al., 2016). Its variant, UCT (Kocsis \& Szepesvári, 2006), has been adopted in Hao et al. (2023a) for the development of LLM-based agents. Specifically, it initiates from the root node of the task description $g$ and moves down the tree by selecting optimal actions (child nodes) until the leaf node. Then, MCTS introduces one or multiple child nodes based on available actions provided by LLMs and identifies the most promising node $n$. From the newly expanded node $n$, MCTS requires LLM agents to execute a simulated rollout until a terminal state is reached. Upon completing the simulation, a result is returned from $n$ all the way back to the root node, accompanied by the value function $Q(n)$ to update all the scores on the selected path.

MCTS vs. A* Search. Despite the performance gains attained by MCTS in planning and reasoning tasks, its direct application to LLM agents comes with significant execution costs. The rollout mechanism within MCTS requires multiple LLM calls to prompt the next actions until a terminal state. Furthermore, unlike two-player zero-sum games, the planning tasks essentially operate as one-player games, where value functions estimated by random rollouts might exhibit significant inaccuracies. To mitigate the issue, ToolChain* is proposed based on a more efficient $\mathrm{A}^{*}$ search algorithm. A comparison between MCTS and our proposed ToolChain* is illustrated in Figure 2. Unlike MCTS, A* search

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-03.jpg?height=332&width=696&top_left_y=1230&top_left_x=1061)

Figure 2: A comparison between MCTS and A* search in ToolChain*. Unlike MCTS, A* search only requires one-step expansion guided by cost functions. necessitates only a single LLM call for determining the next actions during expansion according to two cost functions, $g(n)$, quantifying the cost of the path from the root node to $n$, and $h(n)$, a heuristic function estimating the cost of the most promising path from $n$ to the goal.

## 3 ToolChAIn*: A Tree SeARch Perspective on ExternAl Tool Use

In this section, we introduce the ToolChain* that enables LLM-based agents to efficiently navigate the action space to identify a valid solution path for problem-solving (Figure 3). First, we outline the framework of ToolChain* (Section 3.1), consisting of three iterative stages: selecting the most promising path in the explored decision tree, expanding the potential following actions along the selected path, and subsequently updating the cost functions. Within ToolChain*, the cost function is composed of two components: cumulative cost $g(n)$ (Section 3.2) and future score $h(n)$ (Section 3.3).

### 3.1 OVERVIEW

ToolChain* is a best-first search algorithm, efficiently guiding LLM agents in generating a sequence of API function calls as a solution plan. We formulate the action space as a search tree $\mathcal{T}$, where each node $n$ represents an action $a_{n}$, accompanied by a state composed of the initial task description $s_{0}$

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-04.jpg?height=452&width=1396&top_left_y=270&top_left_x=362)

Figure 3: ToolChain* framework of three phases: (a) selection, (b) expansion, and (c) update. The dark and grey circles indicate the explored actions and the potential but unexplored ones, respectively. The blue circles represent the selected next step.

and previous actions. This facilitates the translation of action sequence planning into a navigation task originating from the root node of the decision tree. ToolChain* starts the search tree $\mathcal{T}$ with a single root node, corresponding to the input input problem description $s_{0}$. At each step, it selects a node $n$ from the frontiers of $\mathcal{T}$ (denoted as $\mathcal{F}(\mathcal{T})$ ) according to the cost function. Then, it expands $n$ with the LLM to generate a set of $k$ potential i.i.d. actions $\left\{a_{c h(n)}^{(j)}\right\}_{j=1}^{k}$ for the next step and grows $\mathcal{T}$ with the generated actions. Finally, we update the actions into new nodes $s_{c h(n)}^{(j)}=\left(s_{n}, a_{c h(n)}^{(j)}\right)$ and update their cost functions accordingly. Algorithm 1 describes the procedure in detail.

Selection. Given a search tree $\mathcal{T}$, we denote its nodes as $\mathcal{V}(\mathcal{T})$. The frontier $\mathcal{F}(\mathcal{T}) \subseteq \mathcal{V}(\mathcal{T})$ contains all the leaf nodes in $\mathcal{T}$ that have yet to be explored. Given our objective to minimize the total cost of the final solution, the optimal next node to expand would be the most promising plan as part of the best solution. Assume we possess a cost function oracle $f(n)$, which provides the cost of the best plan incorporating $n$ to address the problem $s_{0}$ under $\mathcal{T}$. Then, we can select the next node with the lowest cost: $n_{\text {next }}=\arg \min _{n \in \mathcal{F}(\mathcal{T})} f(n)$. A proper design of the value function $f(n)$ not only augments search efficiency but also aids in identifying globally optimal solutions.

Expansion. Once the node $n$ with the minimum cost estimation $f(n)$ has been selected, we expand the search tree with $k$ potential actions for the next

```
Algorithm 1: ToolChain*.
Input: $x$ : input; $\rho$ : large language model; $T$ :
    the maximum exploring steps; $\mathcal{T}$ : the
    decision tree; $\mathcal{F}(\mathcal{T})$ : the set of frontier
    nodes in $\mathcal{T} ; f(n)$ : the cost function of
    node $n$.
Initialize $\mathcal{T}=\{\mathcal{V}, \mathcal{E}\}, \mathcal{V} \leftarrow x, \mathcal{E} \leftarrow \varnothing$
for $t=1,2, \cdots, T$ do
    $n_{\text {next }} \leftarrow \arg \min _{n \in \mathcal{F}(\mathcal{T})} f(n) / /$ Selection
    $\left\{a^{(i)}\right\}_{i=1}^{k} \leftarrow \rho\left(n_{\text {next }}\right) / /$ Expansion
    for $i=1,2, \cdots, k$ do
        $\left\lfloor\right.$ Add $\left[n_{\text {next }}, a^{(i)}\right]$ to $\mathcal{T}$ under $n_{\text {next }}$
    Update $f(n)$ for $n$ in $\mathcal{F}(\mathcal{T})$. // Update
```

Output: The valid path to solve the problem

$\arg \max _{n \in \mathcal{F}(\mathcal{T})} f(n)$.

step. These actions are sampled from the potential

action set generated by $\operatorname{LLMs} a_{c h(n)}^{(j)} \sim \rho\left(a_{c h(n)} \mid s_{n} ; \mathcal{I}, \mathcal{D}\right),(j=1, \cdots, k)$, given the API definitions

$\mathcal{I}$ and demonstration examples $\mathcal{D}$. For the generated actions or reasoning steps $\left\{a_{c h(n)}^{(j)}\right\}_{j=1}^{k}$, we establish their corresponding nodes under node $n$. Contrasting with the approach in MCTS (Нао et al., 2023a), which requires multiple calls to $\rho$ until a terminal state during rollout, our expansion only requires a single call to generate the possible actions at the next step.

Update. Denote the search tree $\mathcal{T}$ after expansion of node $n$ as $\mathcal{T}^{\prime}$. Given that new nodes have been incorporated and the original tree structure has changed, we need to update the frontier nodes as $\mathcal{F}\left(\mathcal{T}^{\prime}\right)$. With the new frontier nodes $n \in \mathcal{F}\left(\mathcal{T}^{\prime}\right)$, we can compute their corresponding cost functions for the next selection-expansion-update iteration.

Cost Function. We draw inspiration from A* algorithm to design and update the cost function $f(n)$. Specifically, A* selects the path that minimizes $f(n)=g(n)+h(n)$, where $n$ is the current
node, $g(n)$ represents the cost of the path from the start node to $n$, and $h(n)$ is a heuristic function estimating the cost of the cheapest path from $n$ to the goal.

### 3.2 DESIGN OF CUMULATIVE Cost $g(n)$

During the planning process, we assess the cumulative cost of actions in the current plan and guide the planning based on the assessment. For each node $n$ in the searching tree, we design a single-step value function $g_{t}(n)$ ranging from 0 to 1 and formulate the cost as its complement $1-g_{t}(n)$. Thus, the cumulative cost of $n$ can be computed by summing up all the single-step costs of its ancestor nodes $a n(n): g(n)=\sum_{i \in a n(n)} 1-g_{t}(i)$. More specifically, we combine two different value functions, the task-specific heuristic function from reference data (long-term memory) $g_{t, 1}(n)$ and the self-consistency frequency by LLM $g_{t, 2}(n)$, to compute cumulative cost $g(n)$ :

$$
\begin{equation*}
g(n)=\sum_{i \in\{a n(n), n\}}\left(1-g_{t, 1}(i)\right)^{\alpha} \cdot\left(1-g_{t, 2}(i)\right)^{1-\alpha} \tag{1}
\end{equation*}
$$

where $\alpha$ is a weight parameter for the geometric mean.

Task-Specific Heuristic Function $g_{t, 1}(n)$. We can also maintain a long-term memory with successful experiences and compute a heuristic score accordingly. The long-term memory starts from a seed set of demonstration examples provided in a specific dataset and is iteratively extended with successful plans during evaluation. Each example within the long-term memory is represented as a plan $m_{j}=\left(s_{j, 0}, a_{j, 1}, a_{j, 2}, \cdots, a_{j, T_{j}}\right) \in \mathcal{M}$. The number of actions $T_{j}$ in the plan varies case-bycase. To leverage the successful experiences for evaluating the current plan, we compute the longest common sub-sequence (LCS) score between the current generated plan $s_{n}$ and each plan $m_{j}$ in the long-term memory LCS_score $\left(s_{n}, m_{j}\right)=\frac{\operatorname{LCS}\left(s_{n}, m_{j}\right)}{\min \left(L\left(s_{n}\right), L\left(m_{j}\right)\right)}$, where $L(\cdot)$ indicates the length of the plan. Following this, we compute the cumulative functions as the highest LCS score $g_{t, 1}(n)=\max _{m_{j} \in \mathcal{M}}$ LCS_score $\left(s_{n}, m_{j}\right)$, measuring the proportion of success in the plan relative to the experiences accumulated in the long-term memory.

Self-consistency Frequency $g_{t, 2}(n)$. Self-consistency (Wang et al., 2022b) is an ensemble approach that samples $k$ i.i.d. actions at the next step $\left\{a_{t+1}^{(j)}\right\}_{j=1}^{k} \sim p\left(a_{t+1} \mid x, a_{0}, a_{1}, \cdots, a_{t}\right)$. We then select the semantically different actions from the $k$ generated samples as the set of potential next steps. For tool-use scenarios, as the actions are strict in format of API functions and parameters, we directly construct the set with non-repeating actions. For reasoning scenarios, however, actions represent intermediate thought processes articulated in natural language. Inspired by Kuhn et al. (2022), we apply a DeBERTa-large model (He et al., 2020) fine-tuned on natural language inference (NLI) dataset MNLI (Williams et al., 2018) to determine whether the two generated actions entail each other semantically. This allows us to discard actions that are semantically equivalent, only retaining those that offer distinct reasoning as potential next steps. Lastly, we consider the frequencies of different actions in the set as their corresponding cumulative score, given by $g_{t, 2}(n)=\#\left\{j \mid a_{t+1}^{(j)}=n\right\} / k$.

### 3.3 DESIGN OF FUTURE Cost $h(n)$

Similar to the formulation of cumulative cost $g(n)$, we integrate two distinct reward functions, the task-specific heuristic function $h_{t, 1}(n)$ and the Imagination Score by LLM $h_{t, 2}(n)$, to compute $h(n)$ :

$$
\begin{equation*}
h(n)=\left(1-h_{t, 1}(n)\right)^{\beta} \cdot\left(1-h_{t, 2}(n)\right)^{1-\beta} \tag{2}
\end{equation*}
$$

where $\beta$ is the geometric mean weight for future cost.

Task-Specific Heuristic Function. Similar to the heuristic function in the cumulative cost (Section 3.2), we continue to leverage the long-term memory to compute the future score. From the long-term memory, we can derive the average relative position score of the action $a$ appearing in the plans $m_{j}: h_{t, 1}(a)=\sum_{m_{j} \in \mathcal{M}} \mathbb{1}_{\left\{a \in m_{j}\right\}} \frac{\operatorname{pos}\left(a, m_{j}\right)}{T_{j}}$, where $\operatorname{pos}\left(a, m_{j}\right)$ indicates the relative position of action $a$ in the plan $m_{j}$. Note that the action space can be infinite, and the long-term memory may not cover all potential actions relevant to unseen tasks. Thus, given an action node $n$, we compute its future score as the heuristic score of the lexically closest action covered in the long-term memory: $h_{t, 1}(n)=h_{t, 1}\left(\arg \max _{a \in \mathcal{M}}\right.$ LCS_score $\left.(n, a)\right)$.

Table 1: Main experiment results (success rate) on ToolBench, including tool use scenarios of (1) Home Search, (2) Trip Booking, (3) Google Sheets, and (4) Virtual Home.

| Models | GPT-3.5-turbo |  |  |  |  | GPT-4 |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Home <br> Search | Trip <br> Booking | Google <br> Sheets | Virtual <br> Home | Average | Home <br> Search | Trip <br> Booking | Google <br> Sheets | Virtual <br> Home | Average |
| GPT (OpenAI, 2023) | 80.0 | 85.8 | 51.4 | 18.9 | 59.2 | 97.0 | 96.7 | 62.9 | 23.5 | 70.0 |
| ReAct (Yao et al., 2023b) | 83.0 | 86.7 | 47.1 | 20.5 | 59.3 | 94.0 | 97.5 | 64.3 | 22.7 | 69.6 |
| AdaPlanner (Sun et al., 2023) | 90.0 | 87.5 | 55.7 | 20.7 | 63.5 | 97.0 | 97.5 | 66.7 | 27.1 | 72.1 |
| ToT-DFS (Yao et al., 2023a) | 82.0 | 81.7 | $53.4 \quad$ | 21.0 | 59.5 | 95.0 | 96.7 | 62.9 | 24.8 | 69.9 |
| ToT-BFS (T=5) (Yao et al., 2023a) | 83.0 | 83.3 | 48.6 | 21.8 | 59.9 | 92.0 | 94.2 | 64.3 | 26.6 | 69.3 |
| MCTS (Hao et al., 2023a) | 85.0 | 86.7 | 62.9 | 24.4 | 64.8 | 96.0 | 94.2 | 66.7 | 31.3 | 72.1 |
| ToolChain* | 93.0 | 90.8 | 61.4 | 28.6 | 68.5 | 98.0 | 97.5 | 68.6 | 34.5 | 74.7 |

Imagination Score by LLM. Directly querying LLMs for self-evaluation of the future cost at the current step often yields over-confident scores (Lin et al., 2022). To address this, we enable LLMs to imagine more concrete future steps until the target $n_{T}$. However, it is worth noting that the imagined actions may not align with the real executed actions in future plans. To this end, we compute the future score as the proportion of current steps present in the imagined plan, i.e., the ratio of the number between the current node $n$ ancestors to the target node $n_{T}: h_{t, 2}(n)=\frac{|\{a n(n)\}|}{\left|\left\{a n\left(n_{T}\right)\right\}\right|}$. A higher score suggests that the imagined plan closely captures the path to the current step, indicating that fewer remaining steps are needed to accomplish the task in the imagination of LLMs.

## 4 EXPERIMENTS

In this section, we demonstrate the effectiveness and efficiency of ToolChain* through comprehensive experiments across a wide range of tool-use scenarios from ToolBench (Xu et al., 2023) (Section 4.2). In addition, we conduct extensive experiments on GSM8K (Cobbe et al., 2021) (Section 4.3) to showcase the generalization of ToolChain* on pure reasoning tasks without tool interaction.

### 4.1 EXPERIMENTAL SETUP

Datasets. We evaluate ToolChain* on four tool-use environments in ToolBench (Xu et al., 2023) and one reasoning task in GSM8K (Cobbe et al., 2021). For tool-use scenarios, we select environments with both a vast action space comprising a large number of function tools, and a requirement of a deep solution path with multiple API functions (i.e., complicated tasks), including Home Search, Trip Booking, Google Sheets, and Virtual Home. Given that numerical reasoning requires multi-step computations to calculate answers, we choose GSM8K (Cobbe et al., 2021) for evaluation on math reasoning. Dataset details are available in Appendix D.1.

Baselines. For environments from ToolBench, we compare ToolChain* with the state-of-the-art LLM planning algorithms from three main categories, including open-loop systems (GPT (OpenAI, 2023)), closed-loop systems (ReAct (Yao et al., 2023b) and Adaplanner (Sun et al., 2023)), and tree search-based systems (Tree-of-Thoughts (Yao et al., 2023a) and MCTS (Hao et al., 2023a)). For mathematical reasoning problems, we employ a similar set of baselines as in the tool-use tasks. However, we exclude ReAct and AdaPlanner from mathematical reasoning evaluations. This is because they heavily depend on high-quality environment feedback to adjust action plans, which is unavailable in the GSM8K dataset. Additionally, since the action steps in the tool-use scenarios inherently form coherent sequences, we limit our comparison of ToolChain* to Chain-ofThought (Wei et al., 2022) and Self-Consistency (Wang et al., 2022b) only for the math reasoning task, and exclude it from the ToolBench evaluations. Baseline details can be found in Appendix D.2.

### 4.2 Tool UsE: ToOLBENCH

We conduct experiments across four distinct planning tasks to assess the effectiveness and efficiency of ToolChain* in tool usage. The objective is to generate a sequence of API function calls to formulate
a solution plan for each given task. For instance, these tasks include questions or requirements from users, e.g., "Could you help me find train tickets to Cape Coral?". We present the main results, visualize the case study, analyze time-wise efficiency, and discuss ablation studies within the tool-use scenarios as follows. We report the success rate as the evaluation metric. Detailed task setup for ToolBench is available in Appendix B.3.

Results. Table 1 presents the main experiment results on ToolBench. Our proposed ToolChain* consistently outperforms across nearly all datasets, surpassing state-of-the-art baselines by margins of $3.7 \%$ and $2.5 \%$ with the base LLMs GPT-3.5-turbo and GPT-4, respectively. In comparison with the strongest closed-loop baseline AdaPlanner, ToolChain* improves the average success rate by $3.8 \%$. This improvement is because AdaPlanner relies heavily on environmental feedback, which may not always be available in the tool-use scenarios. Without such highquality feedback, closed-loop methods tend to explore a restricted trajectory within the action space, making them more susceptible to propagating errors from previous actions to future plans. Moreover, ToolChain* not only surpasses the strongest tree search-based method, MCTS, but also shows the ability to exploit a better solution plan within the same exploration budgets. This is because our proposed task-specific cost function allows ToolChain* to prioritize the expansion of the most promising branches. Additional analysis is available in Appendix D.3.

Case Study. Figure 4 depicts an example of ToolChain* (GPT-4) and ReAct (Yao et al., 2023b) on a "take shower" task in Virtual Home dataset. According to the ground truth (green, "shower"), ToolChain* generates the correct action plan (blue, "shower") with an expanded search space, whereas the baseline searching method gets trapped in a locally optimal solution (red, "soap"). This suggests that by formulating and expanding upon a tree-based action space, ToolChain* is capable of effectively searching for the globally optimal solution in complex multi-step planning tasks.

Efficiency Evaluation. In terms of efficiency, we evaluate the running time of ToolChain* against all the baselines based on GPT-3.5turbo, as depicted in Figure 5(a). Remarkably, ToolChain* is $37.2 \%$ faster than the most efficient tree search-based method, Tree-ofThoughts (BFS). This efficiency gain may stem from the proposed superior cost function, which efficiently navigates the most promising paths. Additionally, ToolChain* outpaces the bestperforming tree search-based method, MCTS,

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-07.jpg?height=461&width=702&top_left_y=840&top_left_x=1058)

Figure 4: Case study of ToolChain* and $R e$ Act (Yao et al., 2023b) on Virtual Home dataset. Compared to ReAct with a unidirectional search (red), ToolChain* effectively enlarges search space (blue) with tree structures.

Table 2: Ablation studies on ToolBench.

|  | Home <br> Search | Trip <br> Booking | Google <br> Sheets | Virtual <br> Home | Average |
| :---: | :---: | :---: | :---: | :---: | :---: |
| ToolChain* | 93.0 | 90.8 | 61.4 | 28.6 | 68.5 |
| $-g_{1, t}(n)$ | 91.0 | 88.3 | 60.0 | 22.6 | 65.5 |
| $-g_{2, t}(n)$ | 84.0 | 83.3 | 54.3 | 25.3 | 61.7 |
| $-h_{1, t}(n)$ | 88.0 | 87.5 | 61.4 | 23.0 | 65.0 |
| $-h_{2, t}(n)$ | 85.0 | 85.8 | 51.4 | 24.9 | 61.8 |
| $-g(n)$ | 61.0 | 34.9 | 44.2 | 21.0 | 40.3 |
| $-h(n)$ | 84.0 | 85.8 | 53.4 | 26.1 | 62.3 |

by an impressive $415.84 \%$. This discrepancy arises because ToolChain* focuses on expanding only the immediate next action during exploration. In contrast, MCTS goes through a more exhaustive process, simulating the entire future plan step by step using a rollout mechanism. Efficiency results based on GPT-4 are available in Appendix D.5.

Ablation Studies. We conduct ablation studies to evaluate the effectiveness (success rate) of both the cumulative and future cost functions (Table 2). The results suggest that each component of the cumulative and future cost functions contributes to the performance of ToolChain*. This verifies the efficacy of our proposed cost functions in guiding the search through the decision tree. In addition, eliminating either the entire cumulative or future cost results in a marked decline in the success rate. Relying exclusively on the future cost results in a sharp performance drop of $28.2 \%$, deteriorating ToolChain* to a greedy strategy that favors the shortest solution plans with the least number of actions. Conversely, if the search is guided only by the cumulative cost, ToolChain* essentially mirrors the
behavior of the BFS algorithm, yielding similar performance. Further ablation study analysis can be found in Appendix D.6.

### 4.3 MATH REASONING: GSM8K

Beyond tool-use scenarios, we demonstrate the flexibility of ToolChain* by generalizing its application to mathematical reasoning for solving math word problems. We conduct experiments on the entire set of GSM8K and also a subset of hard questions from GSM8K collected in ToolQA (Zhuang et al., 2023). Detailed task setup for GSM8K is available in Appendix B.4.

Results. Table 3 presents the main experimental results (accuracy) for GSM8K and its challenging subset from ToolQA. Similar to tool-use studies (Table 1), ToolChain* consistently outperforms all baselines in both the original set and the challenging subset. These results demonstrate the flexibility and generalization capabilities of ToolChain* in mathematical reasoning tasks. Notably, ToolChain* demonstrates greater advantages over other baselines on ToolQA (hard questions) than on GSM8K, indicating its superior capability in solving complicated tasks. This is because simpler questions are composed of simple and static reasoning,

Table 3: Main results on math reasoning task in GSM8K and its hard subset collected in ToolQA.

| Models | GPT-3.5-turbo |  |  | GPT-4 |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | GSM8K | ToolQA | GSM8K | ToolQA |  |
| GPT | 67.3 | 26.0 |  | 86.6 | 66.0 |
| CoT | 70.1 | 30.0 |  | 87.5 | 75.0 |
| Self-Consistency | 76.1 | 47.0 |  | 92.4 | 78.0 |
| ToT-DFS | 69.9 | 32.0 |  | 89.2 | 76.0 |
| ToT-BFS | 72.3 | 39.0 |  | 91.3 | 77.0 |
| MCTS | 74.7 | 27.0 |  | 91.0 | 74.0 |
| ToolChain* | $\mathbf{7 7 . 0}$ | $\mathbf{5 2 . 0}$ |  | $\mathbf{9 3 . 5}$ | $\mathbf{8 4 . 0}$ |

eliminating the need for multiple branches. In contrast, challenging questions often involve complex reasoning, numerous intermediate steps, and multiple solution paths. The superior performance on hard subsets emphasizes the capability of ToolChain* in solving complicated reasoning problems. Furthermore, the efficiency analysis presented in Figure 5(b) indicates that ToolChain* ranks among the most efficient tree-based search baselines and has a time efficiency comparable to closed-loop systems without a tree structure. Detailed case studies of action space exploration and efficiency analysis with the number of valid actions are available in Appendix D. 4 and D.5, respectively.

### 4.4 DISCUSSION: EMPIRICAL ANALYSIS

From the comprehensive evaluations in planning and reasoning tasks presented in Sections 4.2 and 4.3 , we validate that ToolChain* addresses the two core limitations of open-/closedloop LLM-based agents, error propagation in multi-step solutions and constrained exploration in expansive action spaces. Meanwhile, we demonstrate ToolChain* a more efficient searching strategy compared to existing tree search-based agents. From the scaling-up analysis in Figure 10 in Appendix D.5, alongside experimental results in Table 1 and efficiency metrics in Figure 5, we identify a crucial trade-off between effectiveness and efficiency in the direct application of tree search-based reasoning methods to complex tool use scenarios.

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-08.jpg?height=317&width=635&top_left_y=2037&top_left_x=409)

(a) ToolBench.

![](https://cdn.mathpix.com/cropped/2024_06_04_62280b3249cc09cf0c81g-08.jpg?height=319&width=634&top_left_y=2036&top_left_x=1060)

(b) GSM8K.

Figure 5: Time efficiency evaluation on (a) ToolBench and (b) GSM8K. We report the average running time in seconds over all instances in the dataset. ToolChain* achieves competitive efficiency to closed-loop systems without a tree structure and outpaces other tree search-based algorithms.

To validate ToolChain* in solving these issues, we summarize key findings from experiments as follows: (1) From the main experimental results shown in Tables 1 and 3, ToolChain* surpasses open-/closed-loop and tree search baselines in complex multi-step planning and reasoning tasks, effectively mitigating error propagation. A visualization example of how ToolChain* gradually abandons the faulty path and mitigates error propagation is available in Figure 7 in Appendix D.4. (2) From case studies in Figures 4, 7, and 8, ToolChain* navigates the path toward an optimal solution by formulating the action space as a decision tree, thereby extensively broadening the exploration space. (3) From Figures 5 and 9, ToolChain* significantly accelerates the search process compared to other tree search-based methods, achieving time efficiency even comparable to closed-loop systems without a tree structure. (4) From tool-use in ToolBench to math problems in GSM8K, we show that ToolChain* is a plug-and-play generalizable framework applicable to a wide range of planning and reasoning problems. Notably, it exhibits exceptional proficiency in solving more challenging tasks, like ToolQA, compared to baselines. Additional results in Appendix E and F show that ToolChain* can generalize to a wide range of complex reasoning tasks and open-source LLMs (e.g., LLaMA 2 (Touvron et al., 2023)). (5) There is a trade-off between search depth (i.e., limitations on the number of steps) and the quality of the solution path (Figure 6). ToolChain* efficiently searches optimal solutions within limited steps, striking a balance between exploration and exploitation.

## 5 RELATED WORKS

LLMs for Tool Use. Recent advances have leveraged LLMs as autonomous agents to master tools and generate solution plans for complicated problems (Qin et al., 2023a;b; Mialon et al., 2023). Interacting with various tools, LLM agents can augment themselves with real-time factual knowledge (Nakano et al., 2022; Yang et al., 2023a), multi-modality understanding (Shen et al., 2023; Lu et al., 2023; Yang et al., 2023c), computational abilities (Schick et al., 2023; Parisi et al., 2022), code interpretabilities (Gao et al., 2022; Paranjape et al., 2023), and domain-specific functionalities (Zhang, 2023; Jin et al., 2023). However, many existing methods either concentrate on individual tool-use scenarios (Schick et al., 2023; Parisi et al., 2022) or simply inject human-made heuristic ordering rules for multi-tool utilization (Shen et al., 2023; Lu et al., 2023). With the increasing number of potential API functions at each step and the escalating sequence of actions for complex problem solutions, the action space expands exponentially, thereby diminishing their effectiveness. ToolChain* frames the planning challenge across various tools as navigation through the action space to efficiently identify a valid solution path.

LLMs with Search Algorithms. The majority of LLM-based agents with open- or closed-loop systems rely on linear reasoning or planning structure. To explore multiple branches in the action space, self-consistency (Wang et al., 2022b) samples multiple chains of thoughts, which can be considered as multiple i.i.d. solution paths in the decision space, selecting the best answer through majority voting. Maieutic prompting (Jung et al., 2022) generates a tree of explanations, enforcing logical consistency. Xie et al. (2023) adopts beam search to decode and improve Chain-of-Thoughts reasoning chain. CoRe (Zhu et al., 2023) proposes to fine-tune both the reasoning step generator and verifier to solve math word problems, incorporating MCTS for reasoning decoding. Tree-ofThoughts (Yao et al., 2023a) utilizes heuristic approaches, including depth- and breadth-first search to identify better reasoning pathways. Additionally, RAP (Hao et al., 2023a) combines a world model with rewards within an advanced MCTS search approach. However, many search-guided planning approaches face the trade-off between efficient exploration of an expansive action space against the effective exploitation of global optimal solutions. To avoid exhaustive exploration like MCTS, we propose ToolChain* to combine efficient $\mathrm{A}^{*}$ search with the effective reasoning ability of LLMs.

## 6 CONCLUSION

In this paper, we propose ToolChain*, an $\mathrm{A}^{*}$ tree search-based planning algorithm to augment LLMs with external tools for complicated real-world planning and reasoning tasks. Compared to existing open- or closed-loop LLM agents, ToolChain* formulates the action space as a decision tree, thereby effectively mitigating error propagation and extensively expanding the search space. Furthermore, ToolChain* significantly accelerates the search process compared to other tree search-based methods, enabling tree search in complicated action space and striking a dynamic balance between exploration and exploitation. We demonstrate ToolChain* as a generalizable framework in a wide range of planning and reasoning tasks with both closed- and open-source LLMs. By achieving significant improvements over state-of-the-art baselines, ToolChain* showcases its potential as an efficient planning algorithm, navigating LLM-based agents in addressing complex real-world challenges.

## REFERENCES

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv, pp. 2211.10435v2, 2022.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023a.

Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings, 2023b.

Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2020.

Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey, 2023.

Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv, pp. 2201.07207v2, 2022a.

Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. arXiv, pp. 2207.05608v1, 2022b.

Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.

Eric Jang. Can llms critique and iterate on their own outputs? evjang.com, 2023.

Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. ArXiv, 2023.

Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1266-1279, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.82. URL https: / / aclanthology .org/ 2022.emnlp-main. 82 .

Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv, pp. 2303.17491v1, 2023.

Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282-293. Springer, 2006.

Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2022.

Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv, pp. $2209.07753 v 3,2022$.

Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words, 2022.

Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023.

Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey, 2023.

Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517, 2022.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022.

OpenAI. Gpt-4 technical report. arXiv, pp. 2303.08774v3, 2023.

Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.

Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.

Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.

Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8494-8502, 2018.

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023a.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023b.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI Blog, 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.

Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.

Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Xingyao Wang, Sha Li, and Heng Ji. Code4struct: Code generation for few-shot structured prediction from natural language. arXiv preprint arXiv:2210.12810, 2022a.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022b.

Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv, pp. $2302.01560 v 1,2023$.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv, pp. 2201.11903v6, 2022.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112-1122, 2018.

Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding, 2023.

Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504, 2023.

Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling, 2023a.

Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction, 2023b.

Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023c.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023a.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https : / / openreview . net / forum? id=WE_vluYUL-X.

Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning, 2023.

Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv, pp. 2205.10625v2, 2022.

Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4471-4485, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.245. URL https://aclanthology.org/ 2023.acl-long. 245 .

Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for llm question answering with external tools, 2023.
