# Fast Rates for General Unbounded Loss Functions: From ERM to Generalized Bayes 

Peter D. Grünwald<br>PDG @ CWI.NL<br>Centrum Wiskunde \& Informatica, Amsterdam, The Netherlands<br>Leiden University, Mathematical Institute, Leiden, The Netherlands

Nishant A. Mehta<br>NMEHTA @UVIC.CA<br>Department of Computer Science, University of Victoria<br>Victoria, Canada


#### Abstract

We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to $\eta$-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate $\eta$ is set correctly. For general loss functions, our bounds rely on two separate conditions: the $v$-GRIP (generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter $v$ in the $v$-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in combination with small model complexity leads to $\tilde{O}(1 / n)$ rates. The witness condition allows us to connect the excess risk to an "annealed" version thereof, by which we generalize several previous results connecting Hellinger and Rényi divergence to KL divergence.


Keywords: statistical learning theory, fast rates, PAC-Bayes, misspecification, generalized Bayes.

## 1. Introduction

Much of statistical learning theory has operated under the restrictive assumption that the loss suffered for any prediction falls into some finite interval, which to say that the losses are bounded. In addition, much of this theory for deterministic estimators and even more so for randomized estimators only yields "slow" convergence rates of the risk of the predictor to the minimum risk achievable via the model in use; these are the best rates possible in the face of a worst case distribution. Faster rates of convergence are often possible under various, practically-applicable conditions on the learning problem, and showing such improvements is important as they can translate to drastic reductions on the number of examples needed to achieve a fixed level of error. We provide a novel theory of excess risk bounds for deterministic and randomized estimators in settings with general unbounded loss functions which may have heavy-tailed distributions - important applications include regression in situations with heavy-tailed noise and density estimation with log loss without assuming boundedness of likelihood ratios. These bounds have implications for two different areas: in statistical learning, they establish that with unbounded losses, under weak conditions, one can obtain
estimators with fast convergence rates of their risk - such conditions previously were only well understood in the bounded case (earlier work on generalization bounds for unbounded loss functions such as (Meir and Zhang, 2003; Cortes et al., 2019) typically needs much stronger conditions to obtain fast rates). In density estimation under misspecification, the new bounds imply convergence rates for $\eta$-generalized Bayesian posteriors, in which the likelihood is raised to a power $\eta$ not necessarily equal to 1 , under surprisingly weak conditions. Finally, the bounds highlight the close similarity between PAC-Bayesian and $\eta$-generalized Bayesian learning methods under misspecification; these methods usually are studied within different communities. We now consider these applications in turn:

1. Statistical Learning In Statistical Learning Theory (Vapnik, 1995) the goal is to learn an action or predictor $\hat{f}$ from some set of actions, or model, $\mathcal{F}$ based on i.i.d. data $Z^{n} \equiv Z_{1}, Z_{2}, \ldots, Z_{n} \sim P$, where $P$ is an unknown probability distribution over a sample space $\mathcal{Z}$. One hopes to learn an $\hat{f}$ with small risk, i.e., expected loss $\mathbf{E}\left[\ell_{\hat{f}}(Z)\right]$, for some given loss function $\ell$. Here, $\mathbf{E}$ denotes expectation under $P$, and $\hat{f} \equiv \hat{f}\left(Z^{n}\right)$ is a function from $\mathcal{Z}^{n}$ to $\mathcal{F}$ that represents a learning algorithm; a prototypical example is empirical risk minimization (ERM). Thus, as is common, with some abuse of notation a learning algorithm is really a function, i.e., we do not insist it to be computable; and, in statistical contexts, we sometimes refer to learning algorithms as estimators, simply because this is common usage. A learning problem can thus be summarized as a tuple $(P, \ell, \mathcal{F})$. Wellknown special cases include classification (with $\ell$ the $0-1$ loss or some convex surrogate thereof) and regression (with $\ell$ the squared loss). As is customary (see e.g. (Bartlett et al., 2005) and (Mendelson, 2014)), in most of our results we assume existence of an optimal $f^{*} \in \mathcal{F}$ achieving $\mathbf{E}\left[\ell_{f}(Z)\right]=$ $\inf _{f \in \mathcal{F}} \mathcal{E}\left[\ell_{f}(Z)\right]$, and we define the excess loss of $f$ as $L_{f}=\ell_{f}-\ell_{f *}$.

When the losses are almost surely bounded under $P$, there exists a well-established theory that gives optimal convergence rates of the excess risk $\mathbf{E}\left[L_{\hat{f}}\right]$ of estimator $\hat{f}$ in terms of sample size $n$. Broadly speaking, in the bounded case the optimal rate is usually of order

$$
\begin{equation*}
O\left(\left(\frac{\operatorname{CoMP}_{n}}{n}\right)^{\gamma}\right) \tag{1}
\end{equation*}
$$

where $\operatorname{COMP}_{n}$ is a measure of model complexity such as the Vapnik-Chervonenkis (VC) dimension or the log-cardinality of an optimally chosen $\epsilon$-net over $\mathcal{F}$, among others. For the models usually studied in statistics, such complexity measures are sublinear in $n$, and for "simple" models (often called parametric models, like those of finite $\mathrm{VC}$ dimension in classification) are finite or logarithmic in $n$. The exponent $\gamma$, which is in the range [1/2,1] in practically all cases of interest, reflects the easiness of a learning problem by depending on both geometric and statistical properties of $(P, \ell, \mathcal{F})$. This exponent is equal to $1 / 2$ in the worst case but can be larger, allowing for faster rates, if the loss $\ell$ has sufficient curvature, e.g., if it is exponentially concave (exp-concave) or mixable (Cesa-Bianchi and Lugosi, 2006), or if ( $P, \ell, \mathcal{F}$ ) satisfies "easiness" conditions such as the Tsybakov margin condition (Tsybakov, 2004), a Bernstein condition (Audibert, 2004; Bartlett and Mendelson, 2006), or (stochastic) exp-concavity (Juditsky et al., 2008). Because these conditions and the others on which this paper centers can allow for learning at faster rates, when any of the conditions hold a learning problem is intuitively easier. We thus call all such conditions easiness conditions throughout this work. In this literature, one often calls (1) with $\gamma=1 / 2$ the slow rate and (1) with $\gamma=1$ the fast rate. We note, however, that the terminology "fast rate" is somewhat imprecise, as there are special cases for which rates even faster than $n^{-1}$ are possible (Audibert
and Tsybakov, 2007). A more precise term may be "optimistic rate" (see (Mendelson, 2017a) for a lucid discussion), as this is the rate obtainable in the optimistic situation where an easiness condition holds. We opt for "fast" primarily for historical reasons.

Van Erven et al. (2015) showed that, in the case when the excess losses are bounded ${ }^{1}$, all the "easiness" conditions above are subsumed by what they term the $v$-central condition, where $v$ is a function that effectively modulates $\gamma$. While Van Erven et al. (2015) do show connections between such conditions for unbounded excess losses as well, they left open the question of whether the conditions still imply fast rates in that case. Thus, the first main target of the present paper is to extend this "fast rate theory" to the unbounded and heavy-tailed excess loss case. A main consequence of our bounds is that under $v$-GRIP conditions ("GRIP" stands for generalized reversed information projection), which consist of the $v$-central condition and a weakening thereof, and an additional witness condition, the obtainable rates remain the same as in the bounded case.

2. Density Estimation under Misspecification Letting $\mathcal{F}$ index a set of probability densities $\left\{p_{f}: f \in \mathcal{F}\right\}$ and setting the loss $\ell$ to the $\log \operatorname{loss}, \ell_{f}(z)=-\log p_{f}(z)$, we find that the statistical learning problem becomes equivalent to density estimation, the excess risk becomes equal to the generalized Kullback-Leibler (KL) divergence

$$
D\left(f^{*} \| \hat{f}\right)=\mathbf{E}_{Z \sim P}\left[\log \left(p_{f^{*}}(Z) / p_{\hat{f}}(Z)\right)\right]
$$

and ERM becomes maximum likelihood estimation. We call a model $\mathcal{F}$ well-specified if it is correct, i.e., if $p_{f^{*}}$ is the density of the true distribution $P$; in that case $D\left(f^{*} \| \hat{f}\right)$ becomes the standard KL divergence. In this setting, our results thus automatically become convergence bounds of estimators $\hat{f}$ to the KL-optimal density within $\mathcal{F}$, where the convergence itself is in terms of KL divergence rather than more usual, weaker metrics such as Hellinger distance. Here, our results vastly generalize earlier results on KL bounds which typically rely on strong conditions such as boundedness of likelihood ratios or exponential tail conditions (Birgé and Massart, 1998; Yang and Barron, 1998; Wong and Shen, 1995; Sason and Verdú, 2016); in this work, the much weaker witness condition suffices.

We also provide bounds that are more similar to the standard Hellinger-type bounds and that hold without the witness condition, having a generalization of squared Hellinger distance (suitable for misspecification) rather than KL divergence on the left. Our bounds also allow for estimators that output a distribution $\Pi$ on $\mathcal{F}$ rather than a single $\hat{f}$ and are particularly well-suited for $\eta$ generalized Bayesian posteriors, in which the likelihood in the prior-posterior update is raised to a power $\eta$; standard Bayes corresponds to $\eta=1$. We thus can compare our rates to classical results on Bayesian rates of convergence in the well-specified case, such as in the influential paper (Ghosal, Ghosh, and van der Vaart, 2000) (GGV from now on). In this case, we generally obtain rates comparable to those of GGV, but under weaker conditions, as long as we take $\eta$ (arbitrarily close to but) smaller than 1, a fact already noted for $\eta$-generalized Bayes by Zhang (2006a); Martin et al. (2017); Walker and Hjort (2002). In contrast to earlier work, however, our results remain valid in the misspecified case, although $\eta$ has to be adjusted there to get convergence at all; moreover, the rates obtained are with respect to a new "misspecification metric" and hence are not always comparable to those obtained in the well-specified case. The optimal $\eta$ depends on the "best" parameter $v$ for which a $v$-GRIP condition holds. Grünwald and Van Ommen (2017) give a simple example which

1. Van Erven et al. (2015) actually assume that the losses are bounded, but inspection of the results therein reveals that all that is needed is in fact bounded excess losses.
shows that taking $\eta=1$ (standard Bayes) in regression under misspecification can lead to results that are dramatically worse than taking the right $\eta$, thus showing that our results do have practical implications.
2. $\boldsymbol{\eta}$-generalized Bayes and PAC-Bayes The $\eta$-generalized Bayesian posterior can be further generalized: for general loss functions $\ell$, we can define "posteriors" $\Pi_{n}^{B}$ with densities given by

$$
\begin{equation*}
\frac{d \Pi_{n}^{B}}{d \Pi_{0}}(f) \equiv \pi_{n}^{B}(f) \equiv \pi^{B}\left(f \mid z_{1}, \ldots, z_{n}\right):=\frac{\exp \left(-\eta \sum_{i=1}^{n} \ell_{f}\left(z_{i}\right)\right)}{\int_{\mathcal{F}} \exp \left(-\eta \sum_{i=1}^{n} \ell_{h}\left(z_{i}\right)\right) \cdot d \Pi_{0}(h)} \tag{2}
\end{equation*}
$$

for some "prior" distribution $\Pi_{0}$ on $\mathcal{F}$. This idea goes back at least to Vovk (1990) and is central in the PAC-Bayesian approach to statistical learning (McAllester, 2003). Recently, it has also been embraced within the Bayesian community (Bissiri et al., 2016; Miller and Dunson, 2018). Nevertheless, the communities studying frequentist convergence of Bayesian methods under misspecification and PAC-Bayesian analysis are still largely separate; yet, the present paper shows that the approaches can be analyzed using the very same machinery and that it is fruitful to do so. To wit, all our results are based on an existing lemma due to T. Zhang (2006b; 2006a) which provides convergence bounds in terms of an "annealed" pseudo-excess risk for general estimators; these bounds are optimized if one plugs in $\eta$-generalized Bayesian estimators of the general form above. Zhang's bound is itself based on earlier works in the information theory literature (in particular, the Minimum Description Length (MDL) literature) (Barron and Cover, 1991; Li, 1999)) and the PAC-Bayesian literature (Catoni, 2003; Audibert, 2004). Of course, the technique also has some disadvantages, to which we return in the Discussion (Section 7).

### 1.1. Overview and Main Insights of the Paper

Section 2 formalizes the setting; Section 7 discusses additional related work and potential future work and provides discussion. The paper ends with appendices containing all long proofs, technical details concerning infinities, and some additional examples. The main results are in Sections 3-6:

Section 3: Zhang's Bound; Information Complexity In Section 3, for which we do not claim any novelty, we present Lemma 5; this lemma is T. Zhang's (2006b; 2006a) result that bounds a pseudo-excess risk of estimator $\hat{f}: \mathcal{Z}^{n} \rightarrow \mathcal{F}$ in terms of the information complexity $\mathrm{IC}_{n, \eta}$. A very simplified form of this lemma is

$$
\begin{equation*}
\mathbf{E}_{Z \sim P}^{\operatorname{ANN}(\eta)}\left[L_{\hat{f}}\right] \unlhd_{\eta \cdot n} \mathrm{IC}_{n, \eta} \tag{3}
\end{equation*}
$$

where the pseudo-excess risk $\mathbf{E}_{Z \sim P}^{\operatorname{ANN}(\eta)}$ is formally defined in (11) and $\unlhd$ indicates exponential stochastic inequality (ESI), a useful notational tool which we define. ESI implies both inequality in expectation and with high probability over the sample $Z^{n}$ that determines $\hat{f} \equiv \hat{f}\left(Z^{n}\right)$; the subscript $\eta \cdot n$ is only relevant for the in-probability version (see Proposition 3) and can be ignored for now. The actual bound (14) we provide in Lemma 5 generalizes (3), also allowing for estimators that output a distribution such as generalized Bayesian posteriors as given by (2). $\mathrm{IC}_{n, \eta}$ is a notion of model complexity which, apart from $n$ and $\eta$, also depends (for now suppressed in the notation) on the data $Z^{n}$, the choice of estimator $\hat{f}$ or $\Pi_{n}$, and on a distribution $\Pi_{0}$ on $\mathcal{F}$ which we may think of as "something like" a prior: while the bound holds for any fixed $\Pi_{0}$, the estimator that minimizes
$\mathrm{IC}_{n, \eta}$ for given prior $\Pi_{0}$ and data $Z^{n}$ is the corresponding $\eta$-generalized Bayesian posterior $\Pi_{n}^{B}$ given by (2).

For this choice of estimator, one can often design priors such that, with high probability and in expectation, $\mathrm{IC}_{n, \eta}$ for the $\eta$-generalized Bayesian estimator can be upper bounded as

$$
\begin{equation*}
\mathrm{IC}_{n, \eta}=\tilde{O}\left(\frac{\operatorname{COMP}_{n}}{\eta n}\right) \tag{4}
\end{equation*}
$$

for functions $\operatorname{COMP}_{n}$ that rely on the model $\mathcal{F}$ 's complexity as indicated above (the $\tilde{O}$-notation suppresses logarithmic factors). In Section 3 we show that in the application to well-specified density estimation, priors can always be chosen such that the classical posterior contraction rates of GGV are (essentially) recovered for any fixed $\eta>0$, in the sense that (3) would imply the same rates if the left-hand side were replaced by a squared Hellinger distance. For example, for standard finite and parametric statistical models, we obtain for Bayesian estimators that $\operatorname{COMP}_{n}=\tilde{O}(1)$; for the nonparametric statistical models considered by GGV, we obtain $\operatorname{COMP}_{n}=\tilde{O}\left(n^{\alpha}\right)$ for an $\alpha$ such that (4) becomes the minimax optimal rate. Similar bounds on $\mathrm{IC}_{n, \eta}$ with general loss functions are given in Section 6 . Henceforth, we use the term parametric to refer to $\mathcal{F}$ for which generalized Bayes estimators give $\operatorname{COMP}_{n}=O(\log n)=\tilde{O}(1)$.

We would thus get good convergence bounds if the left-hand side of (3) were the actual excess risk, but instead it is an "annealed" version thereof, always smaller than the actual excess risk and sometimes even negative. All of our own results can be viewed as establishing conditions under which the annealed excess risk can either be related to the actual excess risk or otherwise to a (generalized Hellinger) metric measuring "distance" between $f^{*}$ and $f$ in some manner; this is done by modifying $\eta$. Both the information complexity and its upper bound (4) can only increase as we decrease $\eta$ (Proposition 6); yet, for small enough $\eta$, annealed convergence implies convergence in the sense in which we are interested (either excess risk or generalized Hellinger distance) up to some constant factor (Sections 4 and 5) and sometimes with an additional slack term (Sections 5 and 6). Thus, the optimal $\eta$ is given by a tradeoff between information complexity and these additional factors and terms.

Sections 4-6 each contain (a) a condition enabling a link between annealed excess risk and the divergence of interest in that section; (b) a new theoretical concept underlying the condition, (c) convergence result(s) relating information complexity to an actual metric or excess risk, and (d) example(s) that illustrate it.

Section 4: The Strong Central Condition and a New Metric; First Convergence Result The strong central condition (Van Erven et al., 2015) expresses that the lower tail of the excess loss $L_{f}:=\ell_{f}-\ell_{f^{*}}$ is exponential, i.e., $P\left(\ell_{f^{*}}-\ell_{f}>A\right)$ is exponentially small in $A$. It has a parameter $\bar{\eta}>0$ that determines the precise bound that can be obtained. While this may sound like a very strong condition, due to the nature of the log loss it automatically holds for density estimation with $\bar{\eta}=1$ if the model is well-specified or convex. We show (Theorem 10) that the $\bar{\eta}$-strong central condition is sufficient for convergence in a new "misspecification" metric $d_{\bar{\eta}}$ (Definition 8) that generalizes the Hellinger distance: there exist estimators such that for every $0<\eta<\bar{\eta}$,

$$
d_{\bar{\eta}}^{2}\left(f^{*}, \hat{f}\right) \unlhd_{\eta \cdot n} C_{\eta} \cdot \mathrm{IC}_{n, \eta}
$$

where $C_{\eta}$ is a constant that tends to $\infty$ as $\eta \uparrow \bar{\eta}$ and is bounded by 1 if $\eta \leq \bar{\eta} / 2$. For misspecified models, $\bar{\eta}$ can in principle be either smaller or larger than 1 . This metric is mainly of interest in the
density estimation application of our work, and we thus compare our results to those of GGV for well-specified density estimation and illustrate them for the case of misspecified generalized linear models (GLMs). Plugging in any fixed $\eta<\bar{\eta}$ in (4) and comparing to (1), we see that under the strong central condition, we can always achieve the fast rate, i.e., (1) with $\gamma=1$.

Section 5: The Witness Condition and a First Excess Risk Convergence Result Here we consider when, under the strong central condition, we can get bounds on the actual excess risk (or, in density estimation, on the generalized KL divergence). We provide a new concept, the empirical witness of badness condition, or witness condition for short, which provides control over the upper tail of the excess loss $L_{f}=\ell_{f}-\ell_{f^{*}}$ (whereas the central condition concerns the lower tail). Essentially, the witness condition says that whenever $f \in \mathcal{F}$ is worse than $f^{*}$ in expectation, the probability that we witness this in our training example should not be negligibly small. We thus rule out the case that $f$ has extremely large loss with extremely small probability. This condition turns out to be quite weak - it can still hold if, for example, the excess loss $\ell_{f}-\ell_{f} *$ is heavy-tailed (it suffices for the conditional second moment of the target to be uniformly bounded almost surely; see Example 7). Thus we establish our first excess risk convergence result, Theorem 14, which, in its simplest form, says that if both the central condition holds with parameter $\bar{\eta}$ and the witness condition holds, then for all $0<\eta<\bar{\eta}$,

$$
\begin{equation*}
\mathbf{E}\left[L_{\hat{f}}\right] \unlhd_{\eta \cdot n / a_{\eta}} a_{\eta} \cdot \mathrm{IC}_{n, \eta} \tag{5}
\end{equation*}
$$

where $a_{\eta}$ is a constant that again tends to $\infty$ as $\eta \uparrow \bar{\eta}$. Once again, by combining (5) and (4), we see that under a witness and $\bar{\eta}$-central condition, we can achieve the fast rate by taking $\gamma=1$ in (1).

The witness condition vastly generalizes earlier conditions such as boundedness of likelihood ratios in density estimation (Birgé and Massart, 1998; Yang and Barron, 1998) and the exponential tail condition of Wong and Shen (1995). Moreover, (5) (Theorem 14) is based on Lemma 13, which generalizes earlier results relating KL divergence to Hellinger and Rényi-type divergences such as those of Yang and Barron (1999), Haussler and Opper (1997), Birgé and Massart (1998), Wong and Shen (1995), and Sason and Verdú (2016). We also discuss the similarity between the witness condition and the recently introduced small-ball assumption of Mendelson (2014).

Section 6: Weaker Fast Rate Conditions; the GRIP The $\bar{\eta}$-central condition of Section 4 can be generalized to the $v$-central condition, where $v: \mathbb{R}^{+} \rightarrow \mathbb{R}^{+}$is a non-decreasing function; nonconstant $v(x)$ gives weaker conditions that still allow for fast rates. Van Erven et al. (2015) showed that for the bounded excess loss case, most existing easiness conditions can be shown to be equivalent to either a $v$-central condition or to what they call a $v$-PPC (pseudo-probability-convexity) condition. In one of their central results, they show these two seemingly different conditions to be equivalent to one another, and also, if $v$ is of the form $v(x) \asymp x^{1-\beta}$, (essentially) equivalent to a ( $\left.B, \beta\right)$ Bernstein condition (Audibert, 2004; Bartlett and Mendelson, 2006). In this section we show that for unbounded excess losses, the $v$-central and $v$-PPC conditions become quite different from each other (and also from the Bernstein condition): the $v$-PPC condition allows for heavy- (polynomial) tailed loss distributions, whereas the $v$-central condition does not.

We first present Theorem 22, an excess risk bound under the $v$-central condition that is a relatively straightforward consequence of Theorem 14, our risk bound under the $\bar{\eta}$-central condition. We then move to Theorem 29, a similar excess risk bound under the $v$-PPC condition. This theorem involves the GRIP, the novel, fundamental concept of this section (Definition 23). GRIP stands for generalized reversed information projection and generalizes the concept of reversed information
projection introduced by $\mathrm{Li}$ (1999). The GRIP $m_{\mathcal{F}}^{\eta}$ is an $\eta$-dependent pseudo-predictor (it might achieve smaller risk than any $f$ for which $\ell_{f}$ is defined). We show that, for each $\eta$, if $f^{*}$ is replaced by the GRIP $m_{\mathcal{F}}^{\eta}$, then the convergence result (5) above holds. We can interpret the $v$-PPC condition as controlling the excess risk of $f^{*}$ over the GRIP $m_{\mathcal{F}}^{\eta}$ as a function of $\eta$ : the smaller $\eta$, the smaller this excess risk. This determines, for each sample size, an optimal $\eta$ at which the bound (5) and the excess risk of $f^{*}$ relative to $m_{\mathcal{F}}^{\eta}$ balance. Theorem 22 establishes that whenever the witness condition holds and a $v$-central condition holds, we have, for every $\epsilon>0$, for $\eta<v(\epsilon)$,

$$
\begin{equation*}
\mathbf{E}\left[L_{\hat{f}}\right] \unlhd_{\eta \cdot n / a_{\eta}^{\prime}} a_{\eta}^{\prime} \cdot \mathrm{IC}_{n, \eta}+\epsilon \tag{6}
\end{equation*}
$$

where again $a_{\eta}^{\prime}$ is a constant. Theorem 29 shows that if a $v$-PPC condition holds, the same result holds whenever $\eta<v(\epsilon) / 2$, but now only in expectation, for yet another $a_{\eta}^{\prime}$. Thus, the optimal rate now depends on $v$; in particular, if $v(\epsilon) \propto \epsilon^{1-\beta}$, then we can optimize over $\epsilon$ using upper bound (4) and find that, as long as $\operatorname{CoMP}_{n}$ is logarithmic in $n$ (as in parametric settings), by setting $\eta$ at sample size $n$ equal to $\eta \asymp n^{-(1-\beta) /(2-\beta)}$ we obtain the rate

$$
\begin{equation*}
\mathbf{E}\left[L_{\hat{f}}\right]=\tilde{O}\left(n^{-\frac{1}{2-\beta}}\right) \tag{7}
\end{equation*}
$$

which interpolates between the fast rate ((1) with $\gamma=1)$ and the slow rate $(\gamma=1 / 2)$, where $\gamma=$ $1 /(2-\beta)$ depends on $\beta$. Such calculations are well-known for the bounded loss case, and our results establish that the same story continues to hold for the unbounded excess loss case, as long as a witness condition holds - even for heavy-tailed losses. While Theorems 22 and 29 are applicable to the unbounded-loss-yet-bounded risk case (for which $\sup _{f \in \mathcal{F}} \mathbf{E}\left[\ell_{f}\right]<\infty$ ), Theorem 31 extends this result to the unbounded risk case, requiring a slight generalization of the witness condition. Examples 11 and 12 illustrate our results by considering regression with heavy-tailed losses, the latter example further linking the aforementioned small-ball assumption to our generalized witness condition.

The Picture that Emerges Our results point to three separate factors that determine achievable convergence rates for generalized Bayesian, two-part MDL, and empirical risk minimization (ERM) estimators, which often, but not always (see below) coincide with minimax rates:

1. The information complexity $\mathrm{IC}_{n, \eta}$, which determines the "richness" of the model. It is dataand algorithm- dependent, but we can often bound it with high probability or even independently of the underlying $P$. In addition, to see what rates can be achieved, we can plug in the ( $\eta$-generalized Bayesian) learning algorithms that minimize it.
2. The interaction between $P, \ell$, and $\mathcal{F}$ that determines, for each $f \in \mathcal{F}$, the distribution of the lower tail of the excess loss $L_{f}$. This interaction is sometimes called the easiness of the problem (Koolen et al., 2016); it determines the optimal $\eta$ at which a bound on $\eta$-information complexity implies a bound on the generalized Hellinger-type metric. This is captured by our $v$-GRIP conditions, which generalize several existing easiness conditions.
3. The interaction between $P, \ell$, and $\mathcal{F}$ that determines the distribution of the upper tail of the excess loss. This interaction plays no role for bounded excess losses and no role for density estimation if one only cares about convergence in the weak misspecification metric. Yet for unbounded excess losses with the excess risk target (or density estimation with KL-type target), this interaction becomes crucial to take into account and is done so via the witness condition.

In the Discussion (Section 7), Figure 1 summarizes how the various conditions hang together and are in some special cases (e.g. squared loss) implied by existing, better-known easiness conditions imposed in other works.

What we do not cover We stress at the outset that we do not cover everything there is to know about the type of convergence bounds we prove. First of all, our bounds are most useful for ERM, $\eta$-generalized Bayesian, and MDL estimators, for a specific $\eta$ that depends on the learning problem $(P, \ell, \mathcal{F})$ and often also on $n$. Thus to apply generalized Bayes/MDL in practice, $\eta$ needs to be determined in some data-driven way; we discuss various ways to do this in Section 7. Note though that our bounds can be directly used for ERM, which can be implemented without knowledge of $\eta$.

We also leave untouched the fact that for parametric models, Zhang's bounds lead to an unnecessary $\log n$-factor in the convergence rates. Zhang (2006b; 2006a), following Catoni (2003), addresses this issue by a relatively straightforward "localized" modification of his bound; since it distracts from our main points (the witness and GRIP conditions, which lead to polynomial gains in rate), we will simply ignore all logarithmic factors in this paper.

Third, the new convergence rates for $\eta$-generalized Bayesian, MDL, and ERM estimators that we establish are in some cases, but not always, minimax optimal. We do explicitly discuss for each example below whether the obtained rates are optimal and discuss exceptions, unknowns, and potential remedies in Section 7.

Finally, we only discuss proper and randomized proper learning algorithms and estimators here. This means that our estimators either output an $\hat{f} \in \mathcal{F}$ or, if they output a distribution $\Pi \mid Z^{n}$, it is always a distribution on $\mathcal{F}$, and the quality of this distribution is evaluated by the expected loss incurred if one draws an $f$ randomly from $\Pi \mid Z^{n}$. The terminology "proper" is from learning theory (Lee et al., 1996); in statistics such estimators are sometimes called "in-model" (Grünwald, 2007). In learning theory, one often considers more general "improper" set-ups in which one can play an element of (say) $\operatorname{conv}(\mathcal{F})$, the convex hull of $\mathcal{F}$, which sometimes improves the obtainable rates. We briefly return to this issue in Example 11 and Section 7.

| Notation <br> General notation | Description | Page |
| :---: | :---: | :---: |
|  |  |  |
| $Z^{n}$ | i.i.d. sample; $\quad Z^{n}=\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right) \sim P^{n}$ | 2 |
| $P$ | Probability distribution over $\mathcal{Z}$ | 2 |
| $\hat{f}$ | Deterministic estimator or learning algorithm; $\hat{f} \equiv \hat{f}\left(Z^{n}\right)$ | 2 |
| $(P, \ell, \mathcal{F})$ | Learning problem for distribution $P$, loss function $\ell$, and model $\mathcal{F}$ | 2 |
| $\ell_{f}$ | Loss of hypothesis $f ; \quad \ell_{f}(z) \equiv \ell(f, z)$ and $\ell_{f} \equiv \ell_{f}(Z)$ | 10 |
| $f^{*}$ | Risk minimizer within $\mathcal{F}$ | 2 |
| $L_{f} \quad$ r | Excess loss (w.r.t. $f^{*}$ ) of $f ; \quad L_{f}(z) \equiv \ell_{f}(z)-\ell_{f^{*}}(z)$ and $L_{f} \equiv L_{f}(Z)$ | 2 |
| $\Pi_{n}^{B}\left(\right.$ and $\left.\pi_{n}^{B}\right)$ | $\eta$-generalized Bayesian posterior (and its density relative to $\Pi_{0}$ ) | 4 |
| $\unlhd_{\eta} \quad$ r | Exponential stochastic inequality (E.S.I.) | 13 |
| $\Pi_{1}$ | Randomized estimator or learning algorithm; $\Pi_{\mid}: \cup_{n=0}^{\infty} \mathcal{Z}^{n} \rightarrow \Delta(\mathcal{F})$ | 10 |
| $\Pi_{n}$ | Output of algorithm $\Pi_{\mid}$based on sample $Z^{n} ; \quad \Pi_{n} \equiv \Pi \mid Z^{n}$ | 10 |
| $\Pi_{0}$ | Prior; $\Pi_{0} \equiv \Pi \mid\{\}$ | 12 |
| $\mu$ | Common dominating measure for $\left\{p_{f}\right\}_{f \in \mathcal{F}}$ in the case of log loss | 11 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_7036aa2c38f32dab95e4g-09.jpg?height=57&width=277&top_left_y=1122&top_left_x=323) | $\eta$-generalized two-part MDL estimator for prior $\Pi_{0}$ at sample size $n$ | 12 |
| $\left(\hat{f}, \Pi_{0}\right)$ | Deterministic estimator $\hat{f}$ viewed as randomized estimator | 12 |
| $\mathbf{E}^{\mathrm{HE}(\eta)}[U] \quad$ | Hellinger-transformed expectation; $\quad \mathbf{E}^{\mathrm{HE}(\eta)}[U]=\frac{1}{n}\left(1-\mathbf{E}\left[e^{-\eta U}\right]\right)$ | 13 |
| $\mathbf{E}^{\operatorname{ANN}(\eta)}[U]$ | Annealed expectation; $\quad \mathbf{E}^{\operatorname{ANN}(\eta)}[U]=-\frac{1}{n} \log \mathbf{E}\left[e^{-\eta U}\right]$ | 13 |
| $\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)$ | Information complexity | 14 |
| {fef71c93e-1576-4a93-941d-fef3677e3507} | ![](https://cdn.mathpix.com/cropped/2024_06_04_7036aa2c38f32dab95e4g-09.jpg?height=53&width=1029&top_left_y=1377&top_left_x=660) | 18 |
| $d_{\bar{\eta}}(\cdot, \cdot)$ | misspecification metric | 18 |
| $\mathcal{N}(\mathcal{A},\\|\cdot\\|, \epsilon)$ | $\varepsilon$-covering number of $(\mathcal{A},\\|\cdot\\|)$ | 30 |
| Divergences |  |  |
| $\mathrm{KL}(\cdot \\| \cdot)$ | Standard Kullback-Leibler divergence | 12 |
| $\mathrm{H}_{1 / 2}(\cdot \\| \cdot)$ | standard (squared) Hellinger distance | 18 |
| $\mathrm{H}_{\eta}(\cdot \\| \cdot)$ | $\eta$-generalized Hellinger divergence | 18 |
| $D_{\alpha}(p \\| q)$ | Rényi divergence of order $\alpha ; \quad D_{\alpha}(p \\| q)=\frac{1}{\alpha-1} \log \int p^{\alpha} q^{1-\alpha} d \mu$ | 48 |
| Pseudo-predictors |  |  |
| $\overline{\mathcal{F}}$ | enlarged action space $\overline{\mathcal{F}} \supseteq \mathcal{F}$ that also contains pseudo-predictors | 28 |
| $f_{\epsilon}^{*}$ | pseudo-predictor, defined via its loss by $\ell_{f_{\varepsilon}^{*}}(z)=\ell_{f^{*}}(z)-\epsilon$ for all $z \in \mathcal{Z}$ | 28 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_7036aa2c38f32dab95e4g-09.jpg?height=53&width=277&top_left_y=1930&top_left_x=323) | set of pseudoprobability densities; $\mathcal{E}_{\mathcal{F}, \eta}=\left\{e^{-\eta \ell_{f}}: f \in \mathcal{F}\right\}$ | 31 |
| $\xi_{Q}$ | mixture of pseudoprobability densities; $\quad \xi_{Q}=\mathbf{E}_{f \sim Q}\left[e^{-\eta \ell_{f}^{f}}\right]$ | 31 |
| $m_{\mathcal{F}}^{\eta}$ or $\ell_{g_{n}}$ | $\operatorname{GRIP;} \mathbf{E}\left[m_{\mathcal{F}}^{\eta}\right]=\inf _{Q \in \Delta(\mathcal{F})} \mathbf{E}\left[-\frac{1}{\eta} \log \mathbf{E}_{\underline{f} \sim Q}\left[e^{-\eta \ell_{f}}\right]\right]$ | 31 |
| $m_{Q}^{\eta}$ | mix loss for $Q \in \Delta(\mathcal{F}) ; \quad m_{Q}^{\eta}=-\frac{1}{\eta} \log \mathbf{E}_{\underline{f} \sim Q}\left[e^{-\eta \ell_{f}}\right]$ | 31 |
| $m_{A}^{\eta}$ | generalized GRIP w.r.t. $A \subseteq \overline{\mathcal{F}} ; \quad \mathbf{E}\left[m_{A}^{\eta}\right]=\inf _{Q \in \Delta\left(A \cup\left\{f^{*}\right\}\right)} \mathbf{E}\left[m_{Q}^{\eta}\right]$ | 31 |
| $m_{f}^{\eta}$ | mini-grip w.r.t. $f ; \mathbf{E}\left[m_{f}^{\eta}\right]=\inf _{\alpha \in[0,1]} \mathbf{E}\left[-\frac{1}{\eta} \log \left((1-\alpha) e^{-\eta \ell_{f^{*}}}+\alpha e^{-\eta \ell_{f}}\right)\right.$ | 59 |
| $g_{\mathcal{F}}^{\eta}$ and $g_{f}^{\eta} \quad$ | pseudo-actions for GRIP losses $m_{\mathcal{F}}^{\eta}$ and $m_{f}^{\eta}$ respectively | 59 |


| Notation | Description | Page |
| :---: | :---: | :---: |
| Conditions |  |  |
| $(\beta, B)$-Bernstein | $\mathbf{E}\left[L_{f}^{2}\right] \leq B\left(\mathbf{E}\left[L_{f}\right]\right)^{\beta} \quad$ for all $f \in \mathcal{F}$ | 26 |
| strong $\bar{\eta}$-central | $\exists f^{*} \in \mathcal{F}$ s.t. $\ell_{f^{*}}-\ell_{f} \unlhd_{\bar{\eta}} 0$ for all $f \in \mathcal{F}$ | 17 |
| $\eta$-central up to $\varepsilon$ | $\exists f^{*} \in \mathcal{F}$ s.t. $\ell_{f^{*}}-\ell_{f} \unlhd_{\eta} \epsilon$ for all $f \in \mathcal{F}$ | 28 |
| $v$-central | for all $\varepsilon \geq 0, \exists f^{*} \in \mathcal{F}$ s.t. $\ell_{f^{*}}-\ell_{f} \unlhd_{v(\varepsilon)} \epsilon$ for all $f \in \mathcal{F}$ | 28 |
| $\eta$-PPC up to $\varepsilon$ | $\exists f^{*} \in \mathcal{F}$ s.t. $\mathbf{E}_{Z \sim P}\left[\ell_{f^{*}}-m_{\mathcal{F}}^{\eta}\right] \leq \epsilon$ | 31 |
| $v$-PPC | for all $\varepsilon \geq 0, \exists f^{*} \in \mathcal{F}$ s.t. $\mathbf{E}_{Z \sim P}\left[\ell_{f^{*}}-m_{\mathcal{F}}^{v(\varepsilon)}\right] \leq \epsilon$ | 31 |
| $(u, c)$-witness | $\mathbf{E}\left[\left(\ell_{f}-\ell_{f^{*}}\right) \cdot \mathbf{1}_{\left\{\ell_{f}-\ell_{f} \leq \leq u\right\}}\right] \geq c \mathbf{E}\left[\ell_{f}-\ell_{f^{*}}\right]$ for all $f \in \mathcal{F}$ | 22 |
| $(\tau, c)$-witness | generalized version of $(u, c)$-witness condition (see Definition 12) | 23 |
| witness w.r.t. $\phi$ | $(u, c)$-witness condition with dynamic comparator (see Assumption 1) | 59 |
| weak witness w.r.t. $\phi$ | weakened version of the previous condition (see Assumption 1) | 59 |
| unif. exp. upper tail | $U_{f}$ (for $\left.f \in \mathcal{F}\right)$ has condition if $\exists \kappa \in(0, \infty)$ s.t. $\sup _{f \in \mathcal{F}} \mathbf{E}\left[e^{\kappa U_{f}}\right]<\infty$ | 25 |
| small-ball assumption | $\exists \kappa>0$ and $\epsilon \in(0,1)$ s.t. $\forall f, h \in \mathcal{F}, \quad \operatorname{Pr}\left(\|f-h\| \geq \kappa\\|f-h\\|_{L_{2}(P)}\right) \geq \varepsilon$ | 27 |
| convex luckiness | (for squared loss); $\quad \arg \min _{f \in \mathcal{F}} \mathbf{E}\left[\ell_{f}\right]=\arg \min _{f \in \operatorname{conv}(\mathcal{F})} \mathbf{E}\left[\ell_{f}\right]$ | 26 |

## 2. Setting, Technical Preliminaries, Global Assumptions

We now formally introduce the problem setting, cover some preliminaries, and state the assumptions used throughout this work. A glossary appearing on this page and the last one describes all frequently used symbols and conditions.

Let $\ell_{f}(z):=\ell(f, z) \in \mathbb{R} \cup\{\infty\}$ denote the loss of action $f \in \mathcal{F}$ under outcome $z \in \mathcal{Z}$. In the classical statistical learning problems of classification and regression with i.i.d. samples, we have $\mathcal{Z}=\mathcal{X} \times \mathcal{Y}$. Classification (0-1 loss) is recovered by taking $\mathcal{Y}=\{0,1\}$ and $\ell_{f}(x, y)=|y-f(x)|$, and we obtain regression with squared loss by taking $\mathcal{Y}=\mathbb{R}$ and $\ell_{f}(x, y)=(y-f(x))^{2}$. In either case, the class $\mathcal{F}$ is some subset of the set of all functions $f: \mathcal{X} \rightarrow \mathcal{Y}$, such as the set of decision trees of depth at most 5 for classification. Our setting also includes conditional density estimation (see Example 1). Unless we explicitly state otherwise, whenever we introduce a random variable we assume it is a function of $Z, Z_{1}, \ldots, Z_{n}$ which are i.i.d. $\sim P$. If we write $\ell_{f}$ we mean $\ell_{f}(Z)$.

While in frequentist statistics one mostly considers learning algorithms (often called "estimators") that always output a single $f \in \mathcal{F}$, we also will consider algorithms that output distributions on $\mathcal{F}$. Such distributions can, but need not, be Bayesian or generalized Bayesian posteriors as described below. Formally, a learning algorithm based on a set of predictors $\mathcal{F}$ is a function $\Pi_{\mid}: \bigcup_{n=0}^{\infty} \mathcal{Z}^{n} \rightarrow \Delta(\mathcal{F})$, where $\Delta$ is the set of distributions on $\mathcal{F}$. The output of algorithm $\Pi_{\mid}$based on sample $Z^{n}$ is written as $\Pi \mid Z^{n}$ and abbreviated to $\Pi_{n}$. $\Pi_{n}$ is a function of $Z^{n}$ and hence a random variable under $P$. For fixed given $z^{n}, \Pi \mid z^{n}$ is a measure on $\mathcal{F}$. Importantly, our learning algorithms are always defined such that they can also output a distribution $\Pi_{0}$ based on an empty data sequence; we may think of this as a "prior" guess of $f$. We explain below how to recast standard estimators such as ERM, for which $\Pi_{0}$ is undefined, in this framework. Whenever we consider a distribution $\Pi$ on $\mathcal{F}$ for a problem $(P, \ell, \mathcal{F})$, we denote its outcome, a random variable, as $\underline{f}$. Whenever we compare the performance of a learning algorithm $\Pi_{\mid}$to a fixed $\tilde{f} \in \mathcal{F}$, we call $\tilde{f}$ a comparator. $\tilde{f}$ is called optimal or risk-minimizing if $\mathbf{E}\left[\ell_{f}(Z)-\ell_{\tilde{f}}(Z)\right] \geq 0$ for all $f \in \mathcal{F}$; under the assumptions below, this expectation is always well-defined. We usually (but not in Section 6
and the proofs) take as our comparator $\tilde{f}=f^{*}$, where $f^{*}$ is a risk minimizer. Whenever this cannot cause confusion, we write $L_{f}=\ell_{f}-\ell_{f} *$ for the excess loss relative to $f^{*}$.

Assumptions on Learning Algorithms $\Pi_{\mid}$Whenever in the sequel we mention a learning algorithm $\Pi_{\mid}$, we make the following (very mild) assumptions: (1) for all $n, z^{n} \in \mathcal{Z}^{n}, \Pi_{n}$ has a density $\pi_{n} \equiv \pi \mid z^{n}$ relative to the prior distribution $\Pi_{0}$; (2) $\Pi_{0}$ satisfies the natural requirement that for all $z \in \mathcal{Z}, \Pi_{0}\left(f \in \mathcal{F}: \ell_{f}(z)<\infty\right)>0$.

Assumptions on and Conventions for Learning Problems $(P, \ell, \mathcal{F}) \quad$ All of our mathematical results concern learning problems $(P, \ell, \mathcal{F})$ for which we invariably make the following assumptions:

1. Unless the loss function $\ell$ is log-loss or conditional log-loss (see the example below), it is is uniformly bounded from below in the sense that $\inf _{f \in \mathcal{F}} \inf _{z \in \mathcal{Z}} \ell_{f}(Z)>-\infty$.
2. For (conditional) log-loss, we assume for all $f \in \mathcal{F}$ that $p_{f}$ is a probability density relative to some fixed common dominating measure $\mu$, so that $P_{f}$, the distribution with density $p_{f}$, is absolutely continuous with respect to $\mu$; we also assume that $P$ itself is absolutely continuous with respect to $\mu$. Moreover, we additionally assume that

$$
\begin{equation*}
\mathrm{KL}\left(P \| P_{f^{*}}\right)<\infty \tag{8}
\end{equation*}
$$

and, with $H(P)$ the differential entropy of $P$ relative to $\mu$,

$$
\begin{equation*}
H(P)>-\infty \tag{9}
\end{equation*}
$$

3. The learning problem is nontrivial in the sense that for some $f \in \mathcal{F}, \mathbf{E}_{Z \sim P}\left[\ell_{f}(Z)\right]<\infty$ (we require this irrespective of whether $\ell$ is log-loss).
4. There exists an optimal $f \in \mathcal{F}$. We fix any one among these (our results hold no matter which we take) and denote it by $f^{*}$.

Some of our results continue to hold without the final assumption; we shall in all cases say so explicitly. Since we invariably want to impose these assumptions, from now on learning problems $(P, \ell, \mathcal{F})$ are defined to be such that they satisfy these four assumptions, and we will not explicitly mention them any more. The assumptions, and all other issues concerning unboundedness and infinities, are discussed in detail in Appendix H. The requirement that the loss is bounded from below ensures that there are no issues involving undefined expectations or problems with interchanging order of expectations, as we show in Appendix H.1. It holds for just about all loss functions encountered in the literature, except for log-loss defined on continuous outcome spaces, where the log-loss can be unbounded both from above and below; in Appendix H. 2 we motivate the requirements we impose on log-loss and show that, while very mild, they are still sufficient to make all expectations well-defined.

Example 1 (Conditional Density Estimation) Let $\mathcal{Z}=\mathcal{X} \times \mathcal{Y}$ and let $\left\{p_{f} \mid f \in \mathcal{F}\right\}$ be a statistical model of conditional densities for $Y \mid X$, i.e., for each $x \in \mathcal{X}, p_{f}(\cdot \mid x)$ is a probability density on $\mathcal{Y}$ relative to a fixed underlying measure $\mu$. Take (conditional) $\log$ loss, defined on outcome $z=(x, y)$ as $\ell_{f}(x, y)=-\log p_{f}(y \mid x)$. The excess risk, now $\mathbf{E}\left[L_{f}\right]=\mathbf{E}_{Z \sim P}\left[\log \frac{p_{f}(Y \mid X)}{p_{f}(Y \mid X)}\right]$, is formally equivalent to the generalized $K L$ divergence, as already defined in the original paper by Kullback
and Leibler (1951) that also introduced what is now the "standard" KL divergence. Assuming that $P$ has a density $p$ relative to the underlying measure, and denoting standard KL divergence by KL, we have $\operatorname{KL}\left(p \| p_{f}\right)=\mathbf{E}_{Z \sim P}\left[\log \frac{p(Y \mid X)}{p_{f}(Y \mid X)}\right]$, so that $\mathbf{E}\left[L_{f}\right]=\operatorname{KL}\left(p \| p_{f}\right)-\operatorname{KL}\left(p \| p_{f^{*}}\right)$. Thus, minimizing the excess risk under log loss is equivalent to learning a distribution minimizing the KL divergence from $P$ over $\left\{p_{f}: f \in \mathcal{F}\right\}$. We have $\inf _{f \in \mathcal{F}} \mathrm{KL}\left(p \| p_{f}\right)=\mathrm{KL}\left(p \| p_{f^{*}}\right)=\epsilon \geq 0$. If $\epsilon=0$, we must have $p_{f^{*}}=p$, so we deal with a standard well-specified density estimation problem, i.e., the model $\left\{p_{f} \mid f \in \mathcal{F}\right\}$ is "correct" and $f^{*} \in \mathcal{F}$ represents the true $P$. If $\epsilon>0$, we still have $\inf _{f \in \mathcal{F}} \mathbf{E}\left[L_{f}\right]=0$ and may view our problem as learning an $f$ that is closest to $f^{*}$ in generalized KL divergence.

Generalized (PAC-) Bayesian, Two-Part, and ERM Estimators Although our main results hold for general estimators, Proposition 6 below indicates that they are especially suited for generalized Bayesian, two-part MDL, or ERM estimators, since these minimize the bounds provided by our theorems under various constraints. To define these estimators, fix a distribution $\Pi_{0}$ on $\mathcal{F}$, henceforth called prior, and a learning rate $\eta>0$. The $\eta$-generalized Bayesian posterior based on prior $\Pi_{0}, \mathcal{F}$ and sample $z_{1}, \ldots, z_{n}$ is the distribution $\Pi_{n}^{B}$ on $f \in \mathcal{F}$, defined by (2). By our requirement that for all $z \in \mathcal{Z}, \Pi_{0}\left(f \in \mathcal{F}: \ell_{f}(z)<\infty\right)>0$, (2) is guaranteed to be well-defined.

Now, given a learning problem as defined above, fix a countable subset $\ddot{\mathcal{F}}$ of $\mathcal{F}$, a distribution $\Pi_{0}$ concentrated on $\ddot{\mathcal{F}}$ and define the $\eta$-generalized two-part MDL estimator for prior $\Pi_{0}$ at sample size $n$ as

$$
\begin{equation*}
\ddot{f}_{2-\mathrm{P}}:=\underset{f \in \mathcal{F}}{\arg \min } \sum_{i=1}^{n} \ell_{f}\left(Z_{i}\right)+\frac{1}{\eta} \cdot\left(-\log \Pi_{0}(\{f\})\right) \tag{10}
\end{equation*}
$$

where, if the minimum is achieved by more than one $f \in \ddot{\mathcal{F}}$, we take the smallest in the countable list, and if the minimum is not achieved, we take the smallest $f$ in the list that is within $1 / n$ of the minimum. Note that the $\eta$-two part estimator is deterministic: it concentrates on a single function. ERM is recovered for finite $\mathcal{F}$ by setting the prior $\Pi_{0}$ to be uniform over $\ddot{\mathcal{F}}$. We may view the $\eta$-two part estimator as a learning algorithm $\Pi_{\mid}$in our sense by defining $\Pi_{0}$ to be the prior on $\ddot{\mathcal{F}}$ as above and, for each $n, \Pi_{n}$ as the distribution that puts all of its mass at $\ddot{f}_{2 \text {-p }}$ at sample size $n$. While we could denote this estimator as $\Pi_{22-\mathrm{p}}$, it will be convenient to write $\left(\ddot{f}_{2-\mathrm{P}}, \Pi_{0}\right)$ so as to also specify the prior. In the same way, general priors $\Pi_{0}$ combined with general deterministic estimators $\hat{f}$ defined for samples of length $\geq 1$ may be viewed as learning algorithms $\Pi_{\mid}$which we will denote as $\left(\hat{f}, \Pi_{0}\right)$.

Finally, we formally define the ERM estimator as the $f \in \mathcal{F}$ that minimizes $\sum_{j=1}^{n} \ell_{f}\left(Z_{j}\right)$; whenever we refer to ERM we will make sure that at least one such $f$ exists; ties can then be broken in any way desired. It is important to note that ERM can be applied without knowledge of $\eta$; however, for general two-part and Bayesian estimators we need to know $\eta$ - we return to this issue in Section 7 .

## 3. Annealed Risk, ESI, and Complexity

In this section we present Lemma 5, a PAC-Bayesian style bound that underlies all our results to follow. Remarkably, it holds without any regularity conditions. However, on the left hand side it has an "annealed" version of the risk rather than the actual risk. In Sections 4, 5, and 6 we give conditions under which the annealed risk can be replaced by either a Hellinger-type distance or the
standard risk, which is what we are really interested in. Lemma 5 relates the annealed risk to an information complexity via exponential stochastic inequality (ESI). We now introduce the technical notions of annealed expectation and ESI. We then present Lemma 5 and discuss its right-hand side, the information complexity. We do not claim any novelty for the technical results in this section the lemma below can be found in (Zhang, 2006b,a), for example. Still, we need to treat these results in some detail to prepare the new results in subsequent sections.

### 3.1. Main Concepts: Annealed and Hellinger Risk, ESI

For $\eta>0$ and general random variables $U$, we define, respectively, the Hellinger-transformed expectation and the annealed expectation (terminology from statistical mechanics; see e.g. (Haussler et al., 1996)), also known as Rényi-transformed expectation (terminology from information theory, see e.g. (Van Erven and Harremoës, 2014)) as

$$
\begin{equation*}
\mathbf{E}^{\mathrm{HE}(\eta)}[U]:=\frac{1}{\eta}\left(1-\mathbf{E}\left[e^{-\eta U}\right]\right) ; \mathbf{E}^{\operatorname{ANN}(\eta)}[U]:=-\frac{1}{\eta} \log \mathbf{E}\left[e^{-\eta U}\right] \tag{11}
\end{equation*}
$$

with log the natural logarithm. We will frequently use that for $\eta>0$,

$$
\begin{equation*}
\mathbf{E}^{\mathrm{HE}(\eta)}[U] \leq \mathbf{E}^{\operatorname{ANN}(\eta)}[U] \leq \mathbf{E}[U] \tag{12}
\end{equation*}
$$

where the first inequality follows from $-\log x \geq 1-x$ and the second from Jensen. We also note that if, for example, $U$ is bounded, then the inequalities become equalities in the limit:

Proposition 1 If $\mathbf{E}\left[e^{-\eta X}\right]<\infty$, we have $\lim _{\eta \downarrow 0} \mathbf{E}^{\mathrm{HE}(\eta)}[X]=\mathbf{E}[X]$ and we also have that $\eta \mapsto$ $\mathbf{E}^{\operatorname{ANN}(\eta)}[X]$ is non-increasing.

All our results below may be expressed succinctly via the notion of exponential stochastic inequality.

Definition 2 (Exponential Stochastic Inequality (ESI)) Let $\eta>0$ and let $U, U^{\prime}$ be random variables on some probability space with probability measure $P$. We define

$$
\begin{equation*}
U \unlhd_{\eta} \quad U^{\prime} \Leftrightarrow \mathbf{E}_{U, U^{\prime} \sim P}\left[e^{\eta\left(U-U^{\prime}\right)}\right] \leq 1 \tag{13}
\end{equation*}
$$

In all our applications of this notation, $P$ is the distribution appearing in a given learning problem $(P, \ell, \mathcal{F})$ that will be clear from the context; hence, we omit it in the ESI notation. An ESI simultaneously captures "with (very) high probability" and "in expectation" results.

Proposition 3 (ESI Implications) For all $\eta>0$, if $U \unlhd_{\eta} U^{\prime}$ then, (i), $\mathbf{E}[U] \leq \mathbf{E}\left[U^{\prime}\right]$; and, (ii), for all $K>0$, with $P$-probability at least $1-e^{-K}, U \leq U^{\prime}+K / \eta$ (or equivalently, for all $\delta \geq 0$, with probability at least $1-\delta, U \leq U^{\prime}+\eta^{-1} \cdot \log (1 / \delta)$ ).

Proof Jensen's inequality yields (i). Apply Markov's inequality to $e^{-\eta\left(U-U^{\prime}\right)}$ for (ii).

The following proposition will be extremely convenient for our proofs:

Proposition 4 (Weak Transitivity) Let $(U, V)$ be a pair of random variables with joint distribution $P$. For all $\eta>0$ and $a, b \in \mathbb{R}$, if $U \unlhd_{\eta} a$ and $V \unlhd_{\eta} b$, then $U+V \unlhd_{\eta / 2} a+b$.

Proof From Jensen's inequality: $\mathbf{E}\left[e^{\frac{\eta}{2}((U-a)+(V-b))}\right] \leq \frac{1}{2} \mathbf{E}\left[e^{\eta(U-a)}\right]+\frac{1}{2} \mathbf{E}\left[e^{\eta(V-b)}\right]$.

### 3.2. PAC-Bayesian Style Inequality

All our results are based on the following lemma due to Zhang (2006b):

Lemma 5 Let $(P, \ell, \mathcal{F})$ represent a learning problem with $L_{f}$ the excess loss relative to an optimal $f^{*}$. Let $\Pi_{\mid}$be a learning algorithm (defining a "prior" $\Pi_{0}$ ) for this learning problem that outputs distributions on $\mathcal{F}$. For all $\eta>0, n \in \mathbb{N}$, we have:

$$
\begin{equation*}
\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}_{Z \sim P}^{\mathrm{ANN}(\eta)}\left[L_{\underline{f}}\right]\right] \unlhd_{\eta \cdot n} \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right) \tag{14}
\end{equation*}
$$

where $\mathrm{IC}_{n, \eta}$ is the information complexity, defined as:

$$
\begin{equation*}
\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right):=\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\frac{1}{n} \sum_{i=1}^{n} L_{\underline{f}}\left(Z_{i}\right)\right]+\frac{\mathrm{KL}\left(\Pi_{n} \| \Pi_{0}\right)}{\eta \cdot n} \tag{15}
\end{equation*}
$$

By the finiteness considerations of Appendix $\mathrm{H}, \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)$is always well-defined but may in some cases be equal to $-\infty$ or $\infty$. We prove a generalized form of this result, which does not require existence of an optimal $f^{*}$, in Appendix A. 1 The proof is essentially taken from the proof of Theorem 2.1 of Zhang (2006b) and is presented only for completeness.

This result is similar to various results that have been called PAC-Bayesian inequalities, although this name is sometimes reserved for a different type of inequality involving an empirical (observable) quantity on the right that does not involve $f^{*}$ (McAllester, 2003). Lemma 5 generalizes earlier in-expectation results by Barron and $\mathrm{Li}$ (1999) for deterministic estimators rather than (randomized) learning algorithms; these in-expectation results further refine in-probability results of Barron and Cover (1991), arguably the starting point of this research.

To explain the potential usefulness of Lemma 5, let us weaken (14) to an in-expectation statement via Proposition 3, so that it reduces to:

$$
\begin{equation*}
\mathbf{E}_{Z^{n} \sim P}\left[\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}^{\operatorname{ANN}(\eta)}\left[L_{\underline{f}}\right]\right]\right] \leq \mathbf{E}_{Z^{n} \sim P}\left[\operatorname{IC}_{n, \eta}\left(\Pi_{\mid}\right)\right] \tag{16}
\end{equation*}
$$

If the annealed expectation were a standard expectation, the left-hand side would be an expected excess risk. Then we would have a great theorem: by (16), the lemma bounds the expected excess risk of estimator $\Pi_{\mid}$by a complexity term, which, as we will see below, generalizes a large number of previous complexity terms (and allows us to get the same rates), both for well-specified density estimation and for general loss functions. The nonstandard inequality $\triangle$ implies that we get such bounds not only in expectation but also in probability. The only problem is that the left-hand side in Lemma 5 is not the standard risk but the annealed risk, which is always smaller and can even be negative. It turns out however that - as already suggested, but not proved by Proposition 1 by making $\eta$ small enough, the left-hand side can in many cases be related to the standard excess risk or another divergence-like measure after all. The conditions which allow this are the subject of Sections 4-6; but first, in the remainder of the this section we study the complexity term in detail.

### 3.3. Information Complexity

The present form of the information complexity is due to Zhang (2006b), with precursors from Rissanen (1989); Barron and Cover (1991); Yamanishi (1998). For generalized Bayesian, two-part MDL and standard ERM, a first further bound is given via the following proposition, the first part of which is also from Zhang (2006b); we note that this result can be extended to the generalized definition of $\mathrm{IC}_{n, \eta}$ given in Section A.1; the extended result does not rely on the existence of $f^{*}$.

Proposition 6 Consider a learning problem $(P, \ell, \mathcal{F})$ and let $Z^{n} \equiv Z_{1}, \ldots, Z_{n}$ be any sample with $\sum_{i=1}^{n} \ell_{f^{*}}\left(Z_{i}\right)<\infty$ (this will hold a.s. if $Z^{n} \sim P$ ). Let $\Pi_{0}$ be a distribution on $\mathcal{F}$. and let $\Pi_{1}^{B}$ be the corresponding $\eta$-generalized Bayesian posterior, with, for each $n$, $\pi_{n}^{B}$ given by (2). We have for all $\eta>0$ that $\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}^{B}\right)$ is non-increasing in $\eta$, and that

$$
\begin{align*}
n \cdot \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}^{B}\right) & =n \cdot \inf _{\Pi_{\mid} \in \mathrm{RAND}} \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)=-\frac{1}{\eta} \log \mathbf{E}_{\underline{f} \sim \Pi_{0}} \exp \left(-\eta \sum_{i=1}^{n} L_{\underline{f}}\left(Z_{i}\right)\right)  \tag{17}\\
& \leq \inf _{A}\left\{-\frac{1}{\eta} \log \Pi_{0}(A)+n \cdot \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}^{B} \mid f \in A\right)\right\}  \tag{18}\\
& \leq \inf _{A}\left\{-\frac{1}{\eta} \log \Pi_{0}(A)+\mathbf{E}_{\underline{f} \sim \Pi_{0} \mid A}\left[\sum_{i=1}^{n} L_{\underline{f}}\left(Z_{i}\right)\right]\right\} \tag{19}
\end{align*}
$$

where RAND is the set of all learning algorithms $\Pi_{j}^{\prime}$ that can be defined relative to $(P, \ell, \mathcal{F})$ with $\Pi_{0}^{\prime}=\Pi_{0}$ and the second infimum is over all measurable subsets $A \subseteq \mathcal{F}$. In the special case that $\Pi_{0}$ has countable support $\ddot{\mathcal{F}}$ so that the $\eta$-two part estimator (10) is defined, we further have

$$
\begin{align*}
n \cdot \operatorname{IC}_{n, \eta}\left(\Pi_{\mid}^{B}\right) & \leq n \cdot \inf _{f \in \mathrm{DET}} \operatorname{IC}_{n, \eta}\left(\left(\dot{f}, \Pi_{0}\right)\right)  \tag{20}\\
& =n \cdot \operatorname{IC}_{n, \eta}\left(f^{*} \|\left(\ddot{f}_{2-\mathrm{P}}, \Pi_{0}\right)\right) \leq \inf _{f \in \mathscr{F}}\left\{-\frac{1}{\eta} \log \Pi_{0}(\{f\})+\sum_{i=1}^{n} L_{f}\left(Z_{i}\right)\right\}
\end{align*}
$$

where DET is the set of all deterministic estimators with range $\ddot{\mathcal{F}}$.

From Lemma 5 and this result, we see that we have three equivalent characterizations of information complexity for $\eta$-generalized Bayesian estimators. First, there is just the basic definition (15) with $\Pi_{n}$ instantiated to the $\eta$-generalized Bayesian posterior. Second, there is the characterization as the minimizer of (15) for the given data, over all distributions $\Pi_{n}$ on $\mathcal{F}$. And third, there is the characterization in terms of a generalized Bayesian marginal likelihood: (19) shows that for $\eta=1$ and $\ell$ the $\log$ loss, the information complexity $\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}^{\mathrm{B}}\right)$ is the $\log$ Bayes marginal likelihood of the data relative to $f^{*}$, divided by $n$. If furthermore $\mathcal{F}$ is a sufficiently regular $k$-dimensional parametric probability model equipped with a prior $\Pi_{0}$ with full support on $\mathcal{F}$, and the model is correct, i.e., $Z_{1}, Z_{2}, \ldots$ are sampled i.i.d. from a distribution with density in $\mathcal{F}$, then, as is wellknown, the information complexity will almost surely coincide, up to $O(1 / n)$, with the BIC penalty: $n \cdot \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}^{\mathrm{B}}\right)=(k / 2) \log n+O(1)$; see Grünwald (2007) for precise results.

### 3.3.1. BOUNDS ON INFORMATION COMPLEXITY FOR $\eta$-GENERALIZED BAYES

Ghosal et al. (2000) (GGV from now on) presented several theorems implying concentration of the (standard) Bayesian posterior around the true distribution in the well-specified i.i.d. case; their results were employed in many subsequent papers such as, for example, (Ghosal and Van Der Vaart, 2007; Ghosal et al., 2008; Bickel and Kleijn, 2012). We compare our results to theirs in Example 2 in Section 4. One of the conditions they impose is the existence of a sequence $\left(\epsilon_{n}\right)_{n \geq 1}$ such that $n \epsilon_{n}^{2} \rightarrow \infty$, and, for some constant $C>0$, for all $n$, a certain $\epsilon_{n}^{2}$-ball around the true distribution has prior mass at least $\exp \left(-n C \epsilon_{n}^{2}\right)$. Generalizing from log loss to arbitrary loss functions, their condition reads

$$
\begin{equation*}
\Pi_{0}\left(f: \mathbf{E}\left[L_{f}\right] \leq \epsilon_{n}^{2} ; \mathbf{E}\left(L_{f}\right)^{2} \leq \epsilon_{n}^{2}\right) \geq e^{-n C \epsilon_{n}^{2}} \tag{21}
\end{equation*}
$$

They then show that, under this and further conditions, the posterior concentrates with Hellinger rate $\epsilon_{n}$ (see Example 2 of Section 4 for the precise meaning). Now note that (21) implies the weaker

$$
\begin{equation*}
\Pi_{0}\left(f: \mathbf{E}\left[L_{f}\right] \leq \epsilon_{n}^{2}\right) \geq e^{-n C \epsilon_{n}^{2}} \tag{22}
\end{equation*}
$$

which in turn implies, via (19), for any $0<\eta \leq 1$, the following bound on IC for the $\eta$-generalized Bayesian estimator:

$$
\begin{equation*}
\mathbf{E}_{Z^{n} \sim P}\left[\operatorname{IC}_{n, \eta}\left(\Pi_{1}\right)\right] \leq \epsilon_{n}^{2} \cdot(1+(C / \eta)) \tag{23}
\end{equation*}
$$

To see this, note that (19) and (22) imply

$$
\begin{align*}
& \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}^{\mathrm{B}}\right) \\
& \leq-\frac{1}{n} \sum_{i=1}^{n} \ell_{f}^{*}\left(Z_{i}\right)-\frac{1}{n \eta} \log \Pi_{0}\left\{f: \mathbf{E}\left[L_{f}\right] \leq \epsilon_{n}^{2}\right\}+\frac{1}{n} \mathbf{E}_{\underline{f} \sim \Pi_{0} \mid\left\{f: \mathbf{E}\left[L_{f}\right] \leq \epsilon_{n}^{2}\right\}}\left[\sum_{i=1}^{n}\left(\ell_{\underline{f}}\left(Z_{i}\right)\right)\right] \\
& \leq C \frac{\epsilon_{n}^{2}}{\eta}+\frac{1}{n} \mathbf{E}_{\underline{f} \sim \Pi_{0} \mid\left\{f: \mathbf{E}\left[L_{f}\right] \leq \epsilon_{n}^{2}\right\}}\left[\sum_{i=1}^{n}\left(L_{\underline{f}}\left(Z_{i}\right)\right)\right] \tag{24}
\end{align*}
$$

This implies (23).

All the examples of nonparametric families provided by GGV (including priors on sieves, logspline models and Dirichlet processes) rely on showing that condition (21) above holds for specific priors, and hence in all these cases we get bounds on the expected-information complexity which, by (16) allows us to establish comparable rates in expectation for the $\eta$-generalized Bayesian estimator in the well-specified case, for any $\eta$ such that the left-hand side can be linked to an actual distance measure - see Example 2 in Section 4.

We also would like to bound the excess risk in probability in terms of the expected information complexity. For this, we can proceed in either of two ways: we either start with an expectation bound such as (16) and then use Markov's inequality (since the excess risk of any estimator is a.s. nonnegative) to go back from expectation to in-probability. However, under GGV's condition (21) (the weaker (22) is not sufficient here), we can also use the in-probability version of Lemma 5 directly. In combination with Lemma 8.1 of GGV (which straightforwardly extends to our setting with general loss and $\eta$ ) this implies that for all $\delta>0$ :

$$
\begin{equation*}
P\left(\operatorname{IC}_{n, \eta}\left(\Pi_{\mid}\right) \geq\left(1+\delta^{-1 / 2}\right) \epsilon_{n}^{2}\right) \leq \frac{\delta}{n \epsilon_{n}^{2}} \tag{25}
\end{equation*}
$$

It follows that under (21), since $n \epsilon_{n}^{2} \rightarrow \infty, \epsilon_{n}^{2}$ is, up to constant factors depending on $\delta$, an upper bound both on $\mathbf{E}\left[\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)\right]$, and, for every $\delta$, with probability at least $1-\delta$, on $\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)-$see the discussion below Theorem 31 in Section 6.

Finally, there often exist nontrivial worst-case (sup norm) or almost-sure bounds on the information complexity; such bounds - mostly developed for parametric models but also, e.g., for Gaussian processes (Seeger et al., 2008) have historically mostly been established within the MDL literature; see (Grünwald, 2007) for an extensive overview. While we will not go into such bounds in detail here, below we provide a very simple such bound for countably infinite classes, which shows the ease by which IC allows for model aggregation.

Suppose that we have a countably infinite collection of classes $\mathcal{F}_{1}, \mathcal{F}_{2}, \ldots$ and a corresponding set of priors $\Pi_{0}^{(1)}, \Pi_{0}^{(2)}, \ldots$ Let us select a new prior $q: \mathbb{N} \rightarrow \mathbb{R}^{+}$over the collection $\mathcal{F}:=\bigcup_{j \in \mathbb{N}} \mathcal{F}_{j}$.

Then we may define a new prior $\Pi_{0}=\sum_{j \in \mathbb{N}} q(j) \Pi_{0}^{(j)}$ over $\mathcal{F}$. We will assume that the risk minimizer in the full class, $f^{*}$, is equal to $f_{j^{*}}^{*}$ for some $j^{*} \in \mathbb{N}$. By Proposition 6, Eq. (18), we must now have, for all data $Z_{1}, \ldots, Z_{n}$, that

$$
\begin{equation*}
n \cdot \operatorname{IC}_{n, \eta}\left(\Pi_{\mid}\right) \leq-\frac{1}{\eta} \log q\left(j^{*}\right)+n \cdot \mathrm{IC}_{n, \eta}\left(\Pi \mid f \in \mathcal{F}_{j^{*}}\right) \tag{26}
\end{equation*}
$$

where $\Pi \mid f \in \mathcal{F}_{j^{*}}$ is the $\eta$-generalized Bayesian estimator based on the prior $\Pi_{0}^{\left(j^{*}\right)}$ within $\mathcal{F}_{j^{*}}$.

If we now further assume that, for each $j$, the GGV-type condition (22) is satisfied (with prior $\Pi_{0}^{(j)}$ and with $f^{*}$ replaced by $f_{j}^{*}$, the risk minimizer over $\mathcal{F}_{j}$ ), then taking expectations in (26) implies that (22) holds for $\Pi_{0}$, with $f^{*}=f_{j^{*}}^{*}$, with the RHS scaled by a factor $q\left(j^{*}\right)$. A simple adaptation of (23) then gives

$$
\begin{equation*}
\mathbf{E}_{Z^{n} \sim P}\left[\operatorname{IC}_{n, \eta}\left(\Pi_{\mid}\right)\right] \leq \epsilon_{n}^{2} \cdot(1+(C / \eta))+\frac{-\log q\left(j^{*}\right)}{n \eta} \tag{27}
\end{equation*}
$$

Thus, the overhead in information complexity for combining the classes is simply $\frac{-\log q\left(j^{*}\right)}{n \eta}$. Moreover, in the case of a finite collection of $M$ classes, we may take $q$ uniform and the overhead becomes $\frac{\log M}{n \eta}$.

## 4. The Strong Central Condition

As we explained below Lemma 5, our strategy in proving our theorems will be to determine conditions under which the $\eta$-annealed excess risk is similar enough to either the standard risk or a meaningful weakening thereof for Lemma 5 to be useful. In this section we present the simplest such condition, which is still quite strong - it requires an exponentially small upper tail of the distribution of $\ell_{f^{*}}-\ell_{f}$. This strong central condition has a parameter $\bar{\eta}>0$, and whenever we want to make this explicit we refer to it as "the $\bar{\eta}$-central condition". Intuitively, its usefulness for learning is obvious: it ensures that the probability that a "bad" $f$ outperforms $f^{*}$ by more than $L$ is exponentially small in $L$. Technically, its use is that it ensures that the annealed risk is positive for all $\eta<\bar{\eta}$. This allows us to turn Lemma 5 into a useful result by replacing its left-hand side by a metric which (for log loss) generalizes the squared Hellinger distance.

### 4.1. Definitions and Main Results

We now turn to the strong central condition, which, along with its weakened versions discussed in Section 6 was introduced by Van Erven et al. (2015).

Definition 7 (Central Condition) Let $\bar{\eta}>0$. We say that $(P, \ell, \mathcal{F})$ satisfies the strong $\bar{\eta}$-central condition if there exists some $\tilde{f} \in \mathcal{F}$ such that

$$
\begin{equation*}
\mathbf{E}\left[e^{-\bar{\eta}\left(\ell_{f}-\ell_{\tilde{f}}\right)}\right] \leq 1 \text {, i.e., } \ell_{\tilde{f}}-\ell_{f} \unlhd_{\bar{\eta}} 0 \quad \text { for all } f \in \mathcal{F} \tag{28}
\end{equation*}
$$

Jensen's inequality implies that if a $\tilde{f}$ exists satisfying (28), it must be optimal; hence we can take $\tilde{f}=f^{*}$. The special case of this condition with $\bar{\eta}=1$ under log loss has appeared previously, often implicitly, in works studying rates of convergence in density estimation (Barron and Cover, 1991; Li, 1999; Zhang, 2006a; Kleijn and van der Vaart, 2006; Grünwald, 2011). For details about the
myriad of implications of the central condition and its equivalences to other conditions we refer to Van Erven et al. (2015). Here we merely highlight the most important facts. First, trivially, the strong central condition automatically holds for density estimation with log loss in the well-specified setting since then $p_{f^{*}}$ is the density of $P$ (see Example 1), as we then have

$$
\begin{equation*}
\mathbf{E}_{Z \sim P}\left[e^{-\bar{\eta}\left(\ell_{f}-\ell_{f^{*}}\right)}\right]=\mathbf{E}_{Z \sim P}\left[\frac{p_{f}(Z)}{p_{f^{*}}(Z)}\right]=1 \tag{29}
\end{equation*}
$$

Second, less trivially, it also automatically holds under a convex model in the misspecified setting (see Li (1999) and Example 2.2 of Van Erven et al. (2015)). Third, for classification and other bounded excess loss cases, it can be related to the Massart condition, a special case of the Bernstein condition (Audibert, 2004; Bartlett and Mendelson, 2006) (as discussed immediately before Definition 17 in Section 5).

We now introduce a new metric which is derived from the Hellinger metric, introduced below (as is common) in terms of its square.

Definition 8 (Misspecification Metric) For a given learning problem $(P, \ell, \mathcal{F})$, associate each $f \in \mathcal{F}$ and $\eta>0$ with a probability density

$$
\begin{equation*}
p_{f, \eta}(z):=p(z) \frac{\exp \left(-\eta L_{f}(z)\right)}{\mathbf{E}\left[\exp \left(-\eta L_{f}(Z)\right)\right]} \tag{30}
\end{equation*}
$$

where $p$ is the density of $P$. Now define $d_{\bar{\eta}}\left(f, f^{\prime}\right)$ as the Hellinger distance between $p_{f, \bar{\eta}}$ and $p_{f^{\prime}, \bar{\eta}}$ :

$$
\begin{align*}
d_{\bar{\eta}}^{2}\left(f, f^{\prime}\right) & :=\frac{2}{\bar{\eta}}\left(1-\int \sqrt{p_{f, \bar{\eta}}(z) p_{f^{\prime}, \bar{\eta}}(z)} d \mu(z)\right) \\
& =\mathbf{E}^{\mathrm{HE}(\bar{\eta} / 2)}\left[L_{f}-\mathbf{E}^{\mathrm{ANN}(\bar{\eta})}\left[L_{f}\right]+L_{f^{\prime}}-\mathbf{E}^{\mathrm{ANN}(\bar{\eta})}\left[L_{f^{\prime}}\right]\right] \tag{31}
\end{align*}
$$

The following result is obvious:

Proposition 9 If $\ell$ is log loss and $\mathcal{F}$ is well-specified relative to $P$ we can take $\bar{\eta}=1$ and then for every $f \in \mathcal{F}, d_{\bar{\eta}}^{2}\left(f^{*}, f\right)$ coincides with the standard squared Hellinger distance $\mathrm{H}_{1 / 2}\left(P_{f^{*}} \| P_{f}\right)$ defined by $\mathrm{H}_{1 / 2}\left(P_{f} \| P_{f^{\prime}}\right):=2\left(1-\int \sqrt{p_{f}(z) p_{f^{\prime}}(z)} d \mu(z)\right)$.

Since $d_{\bar{\eta}}$ is always interpretable as a Hellinger distance, it is clearly a metric. This is different from an existing, more well-known generalization of the Hellinger distance for the well-specified case (Sason and Verdú, 2016), $\mathbf{H}_{\eta}(P \| Q):=\eta^{-1}\left(1-\mathbf{E}_{Z \sim P}(q(z) / p(z))^{\eta}\right)$ which does not define a metric except for $\eta=1 / 2$ (and then coincides with $d_{1}$ ). The $d_{\bar{\eta}}$ metric is of interest in the misspecified density estimation setting - with density estimation, we may not necessarily be interested in $\log$ loss prediction and a metric weaker than excess risk (i.e. generalized KL divergence) may be sufficient for our purposes. With other loss functions, the main interest will usually be learning an $\hat{f}$ with small prediction error. Then the metric above, while still well-defined, may not be appropriate, and one is interested in the excess risk bounds of the next section instead.

Theorem 10 Suppose that the $\bar{\eta}$-strong central condition holds. Then for any $0<\eta<\bar{\eta}$, the metric $d_{\bar{\eta}}$ satisfies

$$
\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[d_{\bar{\eta}}^{2}\left(f^{*}, \underline{f}\right)\right] \unlhd_{\eta \cdot n} C_{\eta} \cdot \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)
$$

with $C_{\eta}=\eta /(\bar{\eta}-\eta)$. In particular, $C_{\eta}<\infty$ for $0<\eta<\bar{\eta}$, and $C_{\eta}=1$ for $\eta=\bar{\eta} / 2$.

Example 2 (Comparison to results by GGV) Following (Zhang, 2006a) we illustrate the considerable leverage provided in the well-specified density estimation case by allowing $\eta$-generalized Bayesian estimators for $\eta<1$. GGV show that for the standard Bayesian estimator, under condition (21) (which only refers to local properties of the prior in neighborhoods of the true density $p_{f^{*}}$ ), in combination with a rather stringent global entropy condition, the following holds: there exists a constant $C^{\prime}$ such that $\Pi_{n}\left(f \in \mathcal{F}: d_{1}^{2}\left(f^{*}, f\right)>C^{\prime} \epsilon_{n}^{2}\right) \rightarrow 0$ in $P$-probability, i.e., for every $B>0$,

$$
P\left(\Pi_{n}\left(f \in \mathcal{F}: d_{1}^{2}\left(f^{*}, f\right)>C^{\prime} \epsilon_{n}^{2}\right)>B\right) \rightarrow 0
$$

Now, suppose the model is correct so that the $\bar{\eta}$-central condition holds for $\bar{\eta}=1$. Then we get from Theorem 10 that for any $\eta<\bar{\eta}$, using only condition (22), the following holds: for any $\gamma_{1}, \gamma_{2}, \ldots$ such that $\gamma_{n} / \epsilon_{n} \rightarrow \infty$, the generalized Bayesian estimator satisfies $\Pi_{n}\left(f \in \mathcal{F}: d_{1}^{2}\left(f^{*}, f\right)>C^{\prime} \gamma_{n}^{2}\right) \rightarrow$ 0 in $P$-probability, i.e., for every $B>0$,

$$
\begin{equation*}
P\left(\Pi_{n}\left(f \in \mathcal{F}: d_{1}^{2}\left(f^{*}, f\right)>C^{\prime} \gamma_{n}^{2}\right)>B\right) \rightarrow 0 \tag{32}
\end{equation*}
$$

as immediately follows from applying Markov's inequality twice as done below. Thus, by taking $\eta<1$ we need neither the stronger condition (21) nor the much stronger GGV global entropy condition; for this we pay only a slight price since our bound is not in terms of $\epsilon_{n}^{2}$ but is instead in terms of $\gamma_{n}^{2}$, which we have to take slightly larger (a factor $\log \log n$ is of course sufficient). Under well-specification, we thus obtain the same rates as GGV for all the statistical models they consider, up to a $\log \log n$ factor; as GGV show, these rates are usually minimax optimal. Interestingly, other works on Bayesian and MDL nonparametric consistency for the well-specified case also consider $\eta<1$ (Barron and Cover, 1991; Zhang, 2006a; Walker and Hjort, 2002; Martin et al., 2017) or invoke an alternative stringent condition to deal with $\eta=1$ ((Zhang, 2006a, Section 5.2), Barron et al. (1999)); see Zhang (2006a) for a very detailed discussion. While it may be argued that one should be able to deal with standard Bayes $(\eta=1)$, in this paper we also aim to deal with misspecification where we need to take $\eta<1$ (and cannot take it arbitrarily close to 1 ) even for simple problems (Grünwald and Van Ommen, 2017), and then there is no special reason to handle $\eta=1$ via additional conditions.

To show (32), note that, if the $\bar{\eta}$-central condition holds, then for general $A, B>0$, we have

$$
\begin{aligned}
& P\left(\Pi_{n}\left(f \in \mathcal{F}: d_{\bar{\eta}}^{2}\left(f^{*}, f\right)>A\right)>B\right) \leq B^{-1} \mathbf{E}_{Z^{n}}\left[\Pi_{n}\left(f \in \mathcal{F}: d_{\bar{\eta}}^{2}\left(f^{*}, f\right)>A\right)\right] \\
& \quad \leq(A B)^{-1} \mathbf{E}_{Z^{n}} \mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[d_{\bar{\eta}}^{2}\left(f^{*}, \underline{f}\right)\right] \leq(A B)^{-1} \mathbf{E}_{Z^{n}}\left[\operatorname{IC}_{n, \bar{\eta} / 2}\left(\Pi_{\mid}\right)\right]
\end{aligned}
$$

where we applied Markov's inequality twice, and the final inequality is from Theorem 10. Plugging in $A=C^{\prime} \gamma_{n}^{2}$ and $\epsilon_{n}^{2} \geq \mathbf{E}\left[\operatorname{IC}_{n, \bar{\eta} / 2}\left(\Pi_{\mid}\right)\right]$(using (23)), this can be further bounded as $B^{-1} \epsilon_{n}^{2} / \gamma_{n}^{2} \rightarrow 0$.

### 4.2. Applying Theorem 10 in Misspecified Density Estimation

From the above it is clear that Theorem 10 has plenty of applications whenever the model under consideration is correct. We now consider applications of Theorem 10 to misspecified models of probability densities $\mathcal{F}$ with generalized Bayesian estimators $\Pi^{B}$. For this we must establish (a) that the central condition holds for $\mathcal{F}$, and (b) suitable bounds on the information complexity relative to $\Pi_{1}^{B}$. As to (a), we know that the $\bar{\eta}$-central condition holds for $\bar{\eta}=1$ whenever the set of distributions $\left\{p_{f}: f \in \mathcal{F}\right\}$ is correct or convex; as shown elsewhere and illustrated in Example 3 below, it also
holds for 1-dimensional (nonconvex) exponential families and high-dimensional generalized linear models (GLMs) under potentially severe misspecification of the noise, as long as the regression function is well-specified and $P$ has exponentially small tails. As to (b), we may consider priors such that in the well-specified case, the GGV condition holds for some sequence $\epsilon_{1}^{2}, \epsilon_{2}^{2}, \ldots$ as in Example 2. As explained in the example, the GGV condition then automatically holds for GLMs under misspecification as well, so that the same bounds on information complexity can be given as in the well-specified case. It appears that this is a special property of GLMs though - for general $\mathcal{F}$, we only have the following proposition which shows that, if the GGV condition holds for some specific prior in the well-specified case with some bounds $\epsilon_{1}, \epsilon_{2}, \ldots$, then, as long as $p_{f^{*}}$ dominates $p$, it must still hold in the misspecified case for the same prior for a strictly larger sequence $\epsilon_{1}^{\prime}, \epsilon_{2}^{\prime}, \ldots$, leading to a potential deterioration of the bound given by Theorem 10 .

Proposition 11 Consider a learning problem $(P, \ell, \mathcal{F})$ where $\mathcal{F}$ indexes a set of probability distributions $\left\{P_{f}: f \in \mathcal{F}\right\}$ with densities $p_{f}$, and suppose that $\sup _{z \in \mathcal{Z}} \frac{d P(z)}{d P_{f^{*}}(z)}=C<\infty$. Then for all $f \in \mathcal{F}$,

$$
\begin{equation*}
\mathbf{E}_{Z \sim P}\left[L_{f}\right] \leq C \cdot\left(\mathbf{E}_{Z \sim P_{f^{*}}}\left[L_{f}\right]+\sqrt{2 \mathbf{E}_{Z \sim P_{f^{*}}}\left[L_{f}\right]}\right) \tag{33}
\end{equation*}
$$

Proof Observe that

$$
\mathbf{E}_{Z \sim P}\left[L_{f}\right] \leq \mathbf{E}_{Z \sim P}\left[0 \vee L_{f}\right] \leq C \mathbf{E}_{Z \sim P_{f^{*}}}\left[0 \vee L_{f}\right] \leq C \cdot\left(D\left(f^{*} \| f\right)+\sqrt{2 D\left(f^{*} \| f\right)}\right)
$$

where $\mathbf{E}_{Z \sim P_{f^{*}}}\left[L_{f}\right]=D\left(f^{*} \| f\right)$ is the $\mathrm{KL}$ divergence between $f^{*}$ and $f$ and the last inequality is from Yang and Barron (1998) (see the remark under their Lemma 3); for completeness we provide a proof in the appendix.

As a trivial consequence, whenever the weakened GGV condition (22) holds for all $P_{f}$ with $f \in \mathcal{F}$ for a sequence $\epsilon_{1}, \epsilon_{2}, \ldots$, it will still hold for a sequence $\epsilon_{1}^{\prime}, \epsilon_{2}^{\prime}, \ldots$ with $\epsilon_{j}^{\prime} \asymp \sqrt{\epsilon_{j}}$. It follows from (23) that we now automatically have a bound of order $\epsilon_{n}^{\prime} / n$ on the misspecified expected information complexity. Theorem 10 now establishes that whenever the GGV condition holds in the well-specified case, under the further (weak) condition that $\sup _{z \in \mathcal{Z}} d P(z) / d P_{f^{*}}(z)=C<\infty$, we automatically get a form of consistency for $\eta$-generalized Bayes, for $\eta<\bar{\eta}$. The question whether we get the same rates of convergence is obfuscated in two ways: first, the misspecification metric is in general incomparable to the Hellinger metric; second, even in cases in which the misspecification metric dominates the standard Hellinger, for nonparametric $\mathcal{F}$ with $\mathbf{E}\left[\mathrm{IC}_{n, \eta}\right] \asymp n^{-\gamma}$, the conversion $\epsilon_{j}^{\prime} \leftrightharpoons \sqrt{\epsilon_{j}}$ worsens the rates obtained by Theorem 10 to $n^{-\gamma / 2}$. To deal with the first problem, one could establish a condition under which the misspecification metric dominates standard Hellinger; but this is tricky and will be left for future work. The second problem is still of interest in the next section, in which the misspecification metric is replaced by the excess risk, which has the same meaning irrespective of whether $\mathcal{F}$ is well-specified. As indicated below, for generalized linear models we can get rid of the square root in (33), but whether this can be done more generally also remains an important open problem for future work. An alternative, also to be considered for future work, is to refrain from using the priors constructed for the well-specified case altogether and instead directly design priors for the misspecified case, with hopefully better bounds on information complexity.

Example 3 (Exponential Families and Generalized Linear Models) Consider a learning problem $(P, \ell, \mathcal{F})$ in the conditional density estimation setting of Example 1, so that $\ell$ is the conditional
log-loss; $Z=(X, Y)$ with $X$ taking values in $\mathcal{X} \subset \mathbb{R}^{k} ;$ and $\left\{p_{f}: f \in \mathcal{F}\right\}$ for some $\mathcal{F} \subset \mathbb{R}^{k}$ represents a $k$-dimensional generalized linear model (GLM), given in its standard parameterization (so that $\langle x, f\rangle$ is the linear predictor fed into the link function) (McCullagh and Nelder, 1989). Heide et al. (2019, Theorem 2) show ${ }^{2}$ that, under three further conditions on $(P, \ell, \mathcal{F})$, the central condition holds for some $\bar{\eta}>0$, even under misspecification. In essence, the conditions require (1) that $Y$ has exponential tails, in the sense that $\sup _{x \in \mathcal{X}} \mathbf{E}[\exp (\eta|Y|) \mid X=x]<\infty$ for some $\eta>0$ (a requirement that is automatically satisfied for, e.g., logistic regression, for which $\mathcal{Y}$ is finite); (b) that $\mathcal{F}$ is restricted to a compact (though possibly very high dimensional) set, and (c), that the misspecification is of a certain type: the noise may be misspecified in arbitrary ways, but the GLM should contain the distribution with the correct generalized regression function. That is, there should be an $f \in \mathcal{F}$ indexing distribution $P_{f}$ with the correct conditional mean, so that $\mathbf{E}_{P_{f}}[Y \mid X]=\mathbf{E}_{P}[Y \mid X]$. This $f$ will then in fact be equal to the risk-optimal $f^{*}$. By taking $\mathcal{X}$ to be a singleton, a GLM becomes a 1-dimensional natural exponential family, and the result thus also applies to such families. For this simplified case, Heide et al. (2019) show that the smallest $\bar{\eta}$ for which the $\bar{\eta}$-central condition holds is upper bounded by, and in some cases not much smaller than, the ratio of variances $\mathbf{E}_{P_{f^{*}}}\left[\left(Y-\mathbf{E}_{P_{f^{*}}}[Y]\right)^{2}\right] / \mathbf{E}_{P}\left[\left(Y-\mathbf{E}_{P}[Y]\right)^{2}\right]$.

Heide et al. (2019, Proposition 2) shows that, if $\mathcal{F}$ represents a GLM, then under the same three conditions, we have $\mathbf{E}_{Z \sim P}\left[L_{f}\right]=\mathbf{E}_{Z \sim P_{f}}\left[L_{f}\right]$, so that there is no need to resort to Proposition 11. This implies that for any prior satisfying the GGV condition in the well-specified case, the same prior can be used in the misspecified case and, using Theorem 10, we can prove the same risk bounds, up to a constant factor, as in the well-specified case for generalized Bayes with any fixed $\eta<\bar{\eta}$. In particular, $k$-dimensional GLMs being sufficently general parametric models, we can use any continuous prior on $\mathcal{F}$ that is bounded away from 0 and obtain that, for any fixed $\eta$, $n \cdot \mathrm{IC}_{n, \eta}\left(\Pi_{\|}^{\mathrm{B}}\right) \leq(k / 2 \eta) \log n+O(1)$, cf. the remark after Proposition 6. Theorem 10 then gives a bound of $\tilde{O}(k / n)$, which is within a log factor of the minimax optimal parametric rate $O(k / n)$ for squared Hellinger distance in the well-specified case.

Example 4 (Comparison to Bhattacharya et al. (2019)) After submitting the present paper, we became aware of (Bhattacharya et al., 2019). The analysis and results of that paper (first submitted to arXiv in 2016, around the same time as the present paper) overlap with our Theorem 10, and some of their examples have implications for our work as well. Bhattacharya et al. (2019) focus exclusively on generalized Bayesian estimators $\Pi_{\|}^{B}$. Their Theorem 3.6 is a variation of Zhang's Lemma 5, extended to handle non-i.i.d. $P$. Their $\alpha$-Rényi divergence is just our $\eta$-annealed excess risk, with $\eta=1-\alpha$. For $\mathcal{F}$ satisfying the $\bar{\eta}$-central condition, they provide Theorem 3.1, which has some similarity to Theorem 10: their result extends ours in that it allows non-i.i.d. $P$; it rephrases ours so that the result is directly stated in terms of GGV-style conditions on $\Pi_{0}$ rather than on bounds on $\operatorname{IC}_{n, \eta}\left(\Pi_{\mid}^{B}\right)$, similar to our (32); and it stays closer to Lemma 5 in that it keeps the annealed excess risk on the left (a nonsymmetric divergence) where Theorem 10 has a (symmetric) metric. In their Lemma 2.1. they re-prove the result of $\mathrm{Li}$ (1999) and Van Erven et al. (2015) that 1 -strong central holds for convex probability models. Also, they provide (Section 5.1) an interesting novel example in which the strong 1-central condition holds: Gaussian regression, with probability densities $p_{f}(y \mid x) \propto \exp \left(-(y-f(x))^{2} / 2 \sigma^{2}\right)$ with fixed variance $\sigma^{2}$, where the true noise is[^0]

Gaussian and the set of regression functions $\mathcal{F}$ is convex (but the corresponding density functions $\left\{p_{f}: f \in \mathcal{F}\right\}$ are not, so Li's result does not apply). The model is misspecified in that $\mathcal{F}$ does not contain the true regression function; in contrast, in Example 3 above we considered the reverse case in which the noise is misspecified yet the regression function is not. They show that in their setting, bounds on the annealed excess risk imply bounds on the $L_{2}(P)$-parameter estimation error that we consider in Example 9. They do not consider the non-annealed excess risk bounds and weaker forms of the central condition that we will turn to in the following sections.

## 5. The Witness Condition

We have seen via Theorem 10 that under the $\bar{\eta}$-central condition, Lemma 5 provides a bound on a weak Hellinger-type metric. For problems different from density estimation, i.e., loss functions different from log loss, we often mainly are interested in a bound on the excess risk. To get such bounds, we need a second condition on top of the $\bar{\eta}$-central condition. To see why, consider again the density estimation example (Example 1). If we assume a correct model, $p=p_{f^{*}}$, then from (29) the $\bar{\eta}$-central condition holds automatically for all $\bar{\eta} \leq 1$, and so Theorem 10 gives a bound on the Hellinger distance. Yet, while the Hellinger distance is bounded, in general we can have $\operatorname{KL}\left(p \| p_{f}\right)=\infty$. If, for example, $\mathcal{F}$ is the set of densities for the Bernoulli model, $P$ is Bernoulli(1/2), and we use ERM for log loss (so that $\hat{f}$ is the maximum likelihood estimator for the Bernoulli model), we observe with positive probability only 0 's. In this case, we will infer $\hat{f}$ with $p_{\hat{f}}(Y=0)=1$, and thus with positive probability the excess risk between $\hat{f}$ and $f^{*}$ is $\infty$ even though the expected Hellinger distance is of order $O(1 / n)$. We thus need an extra condition.

For log loss, the simplest such condition is that the likelihood ratio of $p_{f^{*}}$ to $p_{f}$ is uniformly bounded for all $f \in \mathcal{F}$. For that case, Birgé and Massart (1998) proved a tight bound on the ratio between the standard KL divergence and the standard $(\eta=1 / 2)$ Hellinger distance. Lemma 13 below represents a generalization of their result to arbitrary $\eta$, misspecified $\mathcal{F}$, and general loss functions under the witness condition which we introduce below, and which is a significant weakening of the bounded likelihood ratio condition. It is the cornerstone for proving our subsequent results: Theorems $14,22,29$, and 31 . Whereas the strong central condition imposes exponential decay of the lower tail of the excess loss $\ell_{f}-\ell_{f^{*}}$, the witness condition imposes a much weaker type of control on the upper tail of $\ell_{f}-\ell_{f}$.

Below, we show that the witness condition generalizes not only conditions of Birgé and Massart (1998) but also of Sason and Verdú (2016) and Wong and Shen (1995) (Example 6). We also show that it holds in a variety of settings, e.g., with exponential families with suitably restricted parameter spaces in the well-specified setting and when the log likelihood has exponentially small tails (Example 5), but also with bounded regression under heavy-tailed distributions (Example 7). Moreover, although the conditions are not equivalent, there is an intriguing similarity to the recent small-ball assumption of Mendelson (2014) (Example 9).

### 5.1. Definition and Main Result

Definition 12 (Empirical Witness of Badness) We say that $(P, \ell, \mathcal{F})$ satisfies the $(u, c)$-empirical witness of badness condition (or witness condition) for constants $u>0$ and $c \in(0,1]$ if for all $f \in \mathcal{F}$

$$
\begin{equation*}
\mathbf{E}\left[\left(\ell_{f}-\ell_{f^{*}}\right) \cdot \mathbf{1}_{\left\{\ell_{f}-\ell_{f^{*}} \leq u\right\}}\right] \geq c \mathbf{E}\left[\ell_{f}-\ell_{f^{*}}\right] \tag{34}
\end{equation*}
$$

More generally, for a function $\tau: \mathbb{R}^{+} \rightarrow[1, \infty)$ and constant $c \in(0,1)$ we say that $(P, \ell, \mathcal{F})$ satisfies the $(\tau, c)$-witness condition if for all $f \in \mathcal{F}, \mathbf{E}\left[\ell_{f}-\ell_{f^{*}}\right]<\infty$ and

$$
\begin{equation*}
\left.\left.\left.\mathbf{E}\left[\left(\ell_{f}-\ell_{f^{*}}\right) \cdot \mathbf{1}_{\left\{\ell_{f}-\ell_{f} \leq \tau\left(\mathbf{E}\left[\ell_{f}-\ell_{f}\right]\right.\right.}\right]\right)\right\}\right] \geq \mathbf{E}\left[\ell_{f}-\ell_{f^{*}}\right] \tag{35}
\end{equation*}
$$

The $(u, c)$-witness condition (34) is just the $(\tau, c)$-witness condition for the constant function $\tau$ identically equal to $u$. In our results we frequently use that, by adding $\mathbf{E}\left[\left(\ell_{f}-\ell_{f^{*}}\right) \cdot \mathbf{1}_{\left\{\ell_{f}-\ell_{f}>u\right\}}\right]$ to both sides of (34) and rearranging, the $(u, c)$-witness condition holds if and only if for $c^{\prime}=1-c$ (and hence $c^{\prime} \in(0,1)$ ),

$$
\begin{equation*}
\mathbf{E}\left[\left(\ell_{f}-\ell_{f^{*}}\right) \cdot \mathbf{1}_{\left\{\ell_{f}-\ell_{f}>u\right\}}\right] \leq c^{\prime} \mathbf{E}\left[\ell_{f}-\ell_{f^{*}}\right] \tag{36}
\end{equation*}
$$

and similarly for the $\tau$-version.

The intuitive reason for imposing this condition is to rule out situations in which learnability simply cannot hold. For instance, consider a setting with $\mathcal{F}=\left\{f^{*}, f_{1}, f_{2}, \ldots\right\}$ where $\ell_{f^{*}}=1$ with probability 1 and, for each $j \geq 1, \ell_{f_{j}}$ is equal to 0 with probability $1-\frac{1}{j}$ and equal to $2 j$ with probability $\frac{1}{j}$. Then for all $j, \mathbf{E}\left[\ell_{f_{j}}-\ell_{f^{*}}\right]=1$, but as $j \rightarrow \infty$, empirically we will never witness the badness of $f_{j}$ as it almost surely achieves lower loss than $f^{*}$. On the other hand, if the excess loss is upper bounded by some constant $b$, we may always take $u=b$ and $c=1$ so that a witness condition is trivially satisfied. Below we provide several nontrivial examples besides bounded excess losses and finite $\mathcal{F}$ in which the witness condition holds.

The following result shows how the witness condition, combined with the strong central condition, leads to fast-rate excess risk bounds:

Lemma 13 Let $\bar{\eta}>0$. Assume that the $\bar{\eta}$-strong central condition (28) holds and let, for arbitrary $0<\eta<\bar{\eta}, c_{u}:=\frac{1}{c} \frac{\eta u+1}{1-\frac{\eta}{\eta}}$. Suppose further that the $(u, c)$-witness condition holds for $u>0$ and $c \in(0,1]$. Then for all $f \in \mathcal{F}$, all $\eta \in(0, \bar{\eta})$ :

$$
\begin{equation*}
\mathbf{E}\left[L_{f}\right] \leq c_{u} \cdot \mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right] \leq c_{u} \cdot \mathbf{E}^{\mathrm{ANN}(\eta)}\left[L_{f}\right] \tag{37}
\end{equation*}
$$

More generally, suppose that the $\bar{\eta}$-central condition and the $(\tau, c)$-witness condition hold for $c \in$ $(0,1]$ and a non-increasing function $\tau$. Then for all $\lambda>0$, all $f \in \mathcal{F}$,

$$
\begin{equation*}
\mathbf{E}\left[L_{f}\right] \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right]\right) \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbf{E}^{\mathrm{ANN}(\eta)}\left[L_{f}\right]\right) \tag{38}
\end{equation*}
$$

Note that for large $u, c_{u}$ is approximately linear in $u / c$.

The following theorem is now an almost immediate corollary of Lemma 5 and Lemma 13:

Theorem 14 Consider a learning problem $(P, \ell, \mathcal{F})$ and a learning algorithm $\Pi_{\mid}$. Suppose that the $\bar{\eta}$-strong central condition holds. If the $(u, c)$-witness condition holds, then for any $\eta \in(0, \bar{\eta})$,

$$
\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}\left[L_{f}\right]\right] \unlhd_{\frac{\eta \cdot n}{c_{u}}} c_{u} \cdot \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)
$$

with $c_{u}$ as in Lemma 13. If instead the $(\tau, c)$-witness condition holds for some non-increasing function $\tau$ as above, then for any $\lambda>0$

![](https://cdn.mathpix.com/cropped/2024_06_04_7036aa2c38f32dab95e4g-23.jpg?height=97&width=745&top_left_y=2375&top_left_x=690)

Proof The first and second inequalities are from chaining Lemma 5 with Lemma 13 ((37) and (38) respectively). The first inequality is immediate using that for general random variables $U, V$, we have $U \unlhd_{a} V \Leftrightarrow c U \unlhd_{a / c} c V$. For the second inequality, we first upper bound the max on the RHS of (38) by the sum of the terms.

This theorem is applicable if the $(\tau, c)$-witness condition holds for a non-increasing $\tau$. If the risk $\sup _{f \in \mathcal{F}} \mathbf{E}\left[L_{f}\right]$ is unbounded, we can only expect the witness condition to hold for $\tau$ such that for large $x, \tau(x)$ is increasing; such $\tau$ are considered in Section 6.3. Non-increasing $\tau$ are often appropriate for scenarios with bounded risk (even though the loss may be unbounded and even heavy-tailed); we encounter one instance thereof in the exponential family example below. There, $\lim _{x \downarrow 0} \tau(x)=\infty$, but the increase as $x \downarrow 0$ is so slow that the optimal $\lambda$ at sample size $n$ is of order $O(1 / n)$ and $c_{\tau(\delta)}=O(\log n)$, leading only to an additional log factor in the bound compared to the case where the $(u, c)$-witness condition holds for constant $u$.

Some Existing Bounds Generalized by Lemma 13 Lemma 13 generalizes a result of Birgé and Massart (1998, Lemma 5) (also stated and proved in Yang and Barron (1998, Lemma 4)) that bounds the ratio between the standard $\mathrm{KL}$ divergence $\mathrm{KL}(P \| Q)$ and the (standard) $1 / 2$-squared Hellinger distance $\mathrm{H}_{1 / 2}(P \| Q)$ for distributions $P$ and $Q$. To see this, take density estimation under log loss in the well-specified setting with $\eta<\bar{\eta}=1$, so that $f^{*}=p$ and $f=q$; then the left-hand side becomes $\mathrm{KL}(P \| Q)$ and the right-hand side $\frac{1}{\eta} \mathbf{E}\left[1-e^{-\eta L_{f}}\right]=\frac{1}{\eta}\left(1-\mathbf{E}\left[(q / p)^{\eta}\right]\right)=\mathrm{H}_{\eta}(P \| Q)$ (this notation was introduced below Proposition 9). Under a bounded density ratio $p / q \leq V$, we can take $u=\log V$ and $c=1$ (the $(u, c)$-witness condition is then trivially satisfied), so that $c_{u}=\frac{\eta \log V+1}{1-\eta}$, which for $\eta=1 / 2$ coincides with the Birgé-Massart bound. The case of general $\eta \in(0,1)$ first was handled by Haussler and Opper (1997) (see Lemma 4 therein), but their bound stops short of providing an explicit upper bound for the ratio.

Sason and Verdú (2016) independently obtained an upper bound (see Theorem 9 therein) on the ratio of the standard $\mathrm{KL}$ divergence $\mathrm{KL}(P \| Q)$ to the $\eta$-generalized Hellinger divergence in the case of bounded density ratio ess sup $\frac{d P}{d Q}$, for general $\eta$. Theorem 13 generalizes Theorem 9 of Sason and Verdú (2016) by allowing for misspecification in the case of density estimation with log loss, allowing for general losses, and, critically for our applications, allowing for unbounded density ratios under a witness condition. We note that in the case of bounded density ratio $\frac{d P}{d Q}$ and the regime $\eta \in(0,1)$ (corresponding to $\alpha=1-\eta \in(0,1)$ in Theorem 9 of Sason and Verdú (2016)), their bound and the unsimplified form of our bound (see $C_{0 \leftarrow \eta}(V)$ in Lemma 36 in Appendix C) are identical, as they should be since both bounds are tight. The additional, slightly looser simplified bound that we provide greatly helps to simplify the treatment for unbounded excess losses under the witness condition. We stress though that Sason and Verdú (2016) treat general $F$-divergences under well-specification, including a wide array of divergences beyond $\eta$-generalized Hellinger for $\eta \in(0,1)$, so in that respect, their bounds are far more general. In the next section we establish that Lemma 13 also generalizes a bound by Wong and Shen (1995).

### 5.2. Example Situations in which the Witness Condition Holds

We now present some examples of common learning problems in which the $(\tau, c)$-witness condition holds for a suitable $\tau$. We first consider a case where the distribution of the excess loss has exponentially decaying tails in both directions. The $(u, c)$-witness condition (34) does not always hold for such excess losses, but we now show that the $\tau$-witness condition is always guaranteed to
hold in such cases for a non-increasing function $\tau$, which leads to a bound on excess risk that is only a $\log$ factor worse than the direct bound on the annealed risk of Lemma 5 .

Definition 15 Suppose that for given $(P, \ell, \mathcal{F})$ and a collection of random variables $\left\{U_{f}: f \in \mathcal{F}\right\}$, there is $a<\kappa<\infty$ such that $\sup _{f \in \mathcal{F}} \mathbf{E}\left[e^{\kappa U_{f}}\right]<\infty$. Then we say that $U_{f}$ has a uniformly exponential upper tail.

The name reflects that $U_{f}$ has uniformly exponential upper tails if and only if there are constants $c_{1}, c_{2}>0$ such that for all $u>0, f \in \mathcal{F}, P\left(U_{f} \geq u\right) \leq c_{1} e^{-c_{2} u}$, as is easily shown (we omit the details).

Lemma 16 Define $M_{\kappa}:=\sup _{f \in \mathcal{F}} \mathbf{E}\left[e^{\kappa L_{f}}\right]$ and assume that $L_{f}$ has a uniformly exponential upper tail, so that $M_{\kappa}<\infty$. Then, for the map $\tau: x \mapsto 1 \vee \kappa^{-1} \log \frac{2 M_{\kappa}}{\kappa x}=O(1 \vee \log (1 / x))$, the $(\tau, c)$ witness condition holds with $c=1 / 2$.

Now let $\bar{\eta}>0$. Assume both the $\bar{\eta}$-strong central condition, i.e., $\mathbf{E}\left[e^{-\bar{\eta} L_{f}}\right] \leq 1$, and that $L_{f}$ has a uniformly exponential upper tail. As an immediate consequence of the lemma above, Theorem 14 now gives that for any learning algorithm $\Pi_{\mid}$for any $\eta \in(0, \bar{\eta})$, (using $\lambda=1 / n$ ), there is $C_{\eta}<\infty$ such that

$$
\begin{equation*}
\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}\left[L_{\underline{f}}\right]\right] \Delta_{\frac{\eta \cdot n}{C_{\eta} \log n}} \frac{1}{n}+C_{\eta} \cdot(\log n) \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right) \tag{40}
\end{equation*}
$$

so we obtain an excess risk bound that is only a log factor worse than the bound that can be obtained for the generalized Hellinger metric in Theorem 14.

Example 5 (Generalized Linear Models and Witness) Consider again Example 3, about GLMs. Heide et al. (2019, Appendix B) show that, under the three assumptions that we informally listed in Example 3, the conditions of Lemma 16 are satisfied. We can thus use (40) to give us that, up to log-factors, for misspecified GLMs satisfying the three conditions mentioned in Example 3 and generalized Bayesian estimators based on priors that are continuous and bounded away from 0 on $\mathcal{F}$, we can prove a rate of order $\tilde{O}(d / n)$, which, up to log factors, is equal to the minimax parametric rate.

As a second consequence of Lemma 16, this time combined with (38) from Lemma 13 with $\lambda=\mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right]$, we find that under the conditions of Lemma 16, there is $C_{\eta}<\infty$ such that

$$
\begin{equation*}
\mathbf{E}\left[L_{f}\right] \leq \max \left\{\mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right], C_{\eta} \cdot \mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right] \cdot \log \frac{1}{\mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right]}\right\} \tag{41}
\end{equation*}
$$

The above result generalizes a bound due to Wong and Shen (1995), as we now show.

Example 6 The bound (41) generalizes a bound of Wong and Shen (1995). Their result, the first part of their Theorem 5, allows one to bound KL divergence in terms of Hellinger distance, i.e., it holds in the special case of well-specified density estimation under log loss with the choice $\bar{\eta}=1$, $\eta=1 / 2$. Formally, consider probability model $\left\{P_{f} \mid f \in \mathcal{F}\right\}$ where each $P_{f}$ has density $p_{f}$, and assume the model is well-specified in that $Z \sim P=P_{f^{*}}$ with $f^{*} \in \mathcal{F}$. Wong and Shen (1995) consider the condition that for some $0<\kappa<1$, it holds that $M_{\kappa}^{\prime}:=\sup _{f \in \mathcal{F}} \int_{\left(p_{f} / p_{f^{*}}\right) \geq e^{1 / \kappa}} p_{f^{*}}\left(p_{f^{*}} / p_{f}\right)^{\kappa}<\infty$.

They show that, under this condition, the following holds for all $f \in \mathcal{F}$ in the regime $\mathrm{H}_{1 / 2}\left(P_{f^{*}} \| P_{f}\right)=$ $\mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right] \leq \frac{1}{2}\left(1-e^{-1}\right)^{2}:$

$$
\begin{equation*}
\mathbf{E}\left[L_{f}\right] \leq\left(6+\frac{2 \log 2}{\left(1-e^{-1}\right)^{2}}+\frac{4}{\kappa} \max \left\{2, \log \frac{M_{\kappa}^{\prime}}{\mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right]}\right\}\right) \mathbf{E}^{\mathrm{HE}(\eta)}\left[L_{f}\right] \tag{42}
\end{equation*}
$$

where $\ell_{f}=-\log p_{f}$ is $\log$ loss. Now, note that for this loss function and in the case $\bar{\eta}=1$ (where their result applies too), $M_{\kappa}$ in Lemma 16 and $M_{\kappa}^{\prime}$ in (42) satisfy $M_{\kappa}^{\prime} \leq M_{\kappa} \leq M_{\kappa}^{\prime}+e$. Comparing (42) to (41), we see that up to values of the constants, our result generalizes Wong and Shen's.

We just showed that a $\tau$-witness condition always holds under exponential tails of the loss. The following example shows that even if the loss random variables $\ell_{f}$ have fat (polynomial) tails, the witness condition often holds, even for constant $\tau$. Before providing the example, we first recall the Bernstein condition (Audibert, 2004; Bartlett and Mendelson, 2006) and a useful proposition that will be leveraged in the example.

Definition 17 (Bernstein Condition) For some $B>0$ and $\beta \in(0,1]$, we say $(P, \ell, \mathcal{F})$ satisfies the $(\beta, B)$-Bernstein condition if, for all $f \in \mathcal{F}, \mathbf{E}\left[L_{f}^{2}\right] \leq B\left(\mathbf{E}\left[L_{f}\right]\right)^{\beta}$.

The best case of the Bernstein condition is when the exponent $\beta$ is equal to 1 . In past works, the Bernstein condition has mostly been used to characterize fast rates in the bounded excess loss regime, where the $(u, c)$-witness condition holds automatically. In that regime, the Bernstein condition for $\beta=1$ and the central condition become equivalent (i.e. for each $(\beta, C)$ pair there is some $\bar{\eta}$ and vice versa, where the relationship depends only on the upper bound on the loss; see Theorem 5.4 of Van Erven et al. (2015)). The following proposition shows that with unbounded excess losses, the Bernstein condition can also be related to the witness condition:

Proposition 18 (Bernstein implies Witness) If $(P, \ell, \mathcal{F})$ satisfies the $(\beta, B)$-Bernstein condition, then, for any $u>B,(P, \ell, \mathcal{F})$ satisfies the $(\tau, c)$-witness condition with $\tau(x)=u \cdot(1 / x)^{1-\beta}$ and $c=1-\frac{B}{u}$. In particular, if $\beta=1$ then $(P, \ell, \mathcal{F})$ satisfies the $(u, c)$-witness condition with constant $u$.

The special case of this result for $\beta=1$ will be put to use in Example 11 in Section 6.

Example 7 (Heavy-tailed regression with convex luckiness and bounded predictions) Consider a regression problem with squared loss, so that $\mathcal{Z}=\mathcal{X} \times \mathcal{Y}$. Further assume that the risk minimizer $f^{*}$ over $\mathcal{F}$ continues to be a minimizer when taking the minimum risk over the convex hull of $\mathcal{F}$. We call this assumption convex luckiness for squared loss. It is implied, for example, when $\mathcal{F}$ is convex or when the model is well-specified in the sense that $Y=f^{*}(X)+\xi$ for $\xi$ a zero-mean random variable that is independent of $X$. Thus, when $\mathcal{F}$ is convex, we can enforce it; if we are not willing to work with a convex $\mathcal{F}$ (for example, because this would blow up the COMP $_{n}$ in (4)), then we are "lucky" if it holds - since it allows, in general, for better rates (see Section 7 for additional discussion).

Now assume further that $\mathbf{E}\left[Y^{2} \mid X\right] \leq C$ a.s. and the function class $\mathcal{F}$ consists of functions $f$ for which the predictions $f(X)$ are bounded as $|f(X)| \leq r$ almost surely. Proposition 19 shows that in this setup, the Bernstein condition holds with exponent 1 and multiplicative constant $8(\sqrt{C}+r)^{2}$. Proposition 18 then implies that the $(u, c)$-witness condition holds with $u=16(\sqrt{C}+r)^{2}$ and $c=\frac{1}{2}$.

Proposition 19 Under the assumptions of the example above, the $\left(1,8(\sqrt{C}+r)^{2}\right)$-Bernstein condition holds.

We note that Theorem 14 cannot be used with squared loss when $Y$ is heavy-tailed as then the strong central condition cannot hold. Thus, while Example 7 might imply in this case that a $(u, c)$-witness condition holds, we do not yet have the machinery to put this fact to use. However, in Example 11, we show that weaker easiness conditions can still hold and fast rates can still be obtained.

Example 8 (Example 7 and Lemma 13 in light of Birgé (2004)) Proposition 1 of Birgé (2004) shows that, in the case of well-specified bounded regression with Gaussian noise $\xi$, the excess risk is bounded by the $1 / 2$-annealed excess risk times a constant proportional to $r^{2}$, where $r$ is the bound on $|f(X)|$ as in Example 7. This result thus gives an analogue of Lemma 13 for bounded regression with Gaussian noise and also allows us to apply one of our main results, Theorem 29 below (excess risk bounds with heavy-tailed losses), for this model. Our earlier Example 7 extends Birgé's result, since it shows that the excess risk can be bounded by a constant times the annealed excess risk if the target $Y$ has an almost surely uniformly bounded conditional second moment, which, in the well-specified setting in particular, specializes to $\xi \mid X$ almost surely having (uniformly) bounded second moment (and thus potentially having quite heavy tails) rather than Gaussian tails. On the other hand, (Birgé, 2004, Section 2.2) also gives a negative result for sets $\mathcal{F}$ that are not bounded (i.e. $\sup _{x \in \mathcal{X}, f \in \mathcal{F}}|f(x)|=\infty$ ): even in the "nice" case of Gaussian regression, there exist such sets for which the ratio between excess risk and annealed excess risk can be arbitrarily large, i.e., there exists no finite constant $c_{u}$ for which (37) holds for all $f \in \mathcal{F}$. From this we infer, by using Lemma 13 in the contrapositive direction, that for such $\mathcal{F}$ the witness condition also does not hold.

Example 9 (witness vs. the small-ball assumption) Intriguingly, on an intuitive level the witness condition bears some similarity to the small-ball assumption of Mendelson (2014). This assumption states that there exist constants $\kappa>0$ and $\epsilon \in(0,1)$ such that, for all $f, h \in \mathcal{F}$, we have

$$
\begin{equation*}
\operatorname{Pr}\left(|f-h| \geq \kappa\|f-h\|_{L_{2}(P)}\right) \geq \varepsilon \tag{43}
\end{equation*}
$$

Under this assumption, Mendelson (2014) established bounds on the $L_{2}(P)$-parameter estimation error $\left\|\hat{f}-f^{*}\right\|_{L_{2}(P)}$ in function learning. For the special case that $h=f^{*}$, one can read the small-ball assumption as saying that "no $f$ behaving very similarly to $f^{*}$ with high probability is very different from $f^{*}$ only with very small probability so that it is still quite different on average." The witness condition reads as "there should be no $f$ that is no worse than $f^{*}$ with high probability and yet with very small probability is much worse than $f^{*}$, so that on average it is still substantially worse". Despite this similarity, the details are quite different. In order to compare the approaches, we may consider regression with squared loss in the well-specified setting as in the example above. Then the $L_{2}(P)$-estimation error becomes equivalent to the excess risk, so both Mendelson's and our results below bound the same quantity. But in that setting one can easily construct an example where the witness and strong central conditions hold (so Theorem 14 applies) yet the small-ball assumption does not (Example 16 in Appendix I); but it is also straightforward to construct examples of the opposite by noting that small-ball assumption does not refer to $Y$ whereas the witness condition does. In Section 6.3 we will see that, nevertheless, the small-ball assumption can be related to the $\tau$-witness condition for a particular $\tau$ that is needed in the unbounded risk scenario (Theorem 31).

## 6. Bounds under Weaker Easiness Conditions

In many learning problems, there is no $\eta>0$ such that the strong $\eta$-central condition is satisfied. Yet, it turns out that in many cases of interest there still exist weaker conditions under which fast convergence rates are possible. We consider two types of conditions. Both are best understood by generalizing the notion of excess risk: whereas hitherto, this was invariably defined as the risk (expected loss of some learner $\Pi_{\text {_ }}$ ) relative to the comparator $f^{*}$ that was optimal within $\mathcal{F}$, we will now also allow more general comparators that lie outside $\mathcal{F}$. In particular we will consider as comparator a pseudo-predictor $g$ with risk $\mathbf{E}\left[\ell_{g}\right]=\mathbf{E}\left[\ell_{f}\right]-\epsilon$ for some small $\epsilon>0$. Being better than $f^{*}, g$ does not correspond to an action that can be actually played, but one can often find a $g$ such that, with $f^{*}$ replaced by $g$, the $\eta$-central condition does hold for some $\eta>0$ while, simultaneously, $\epsilon$ is so small that an excess risk bound relative to $g$ implies also a good excess risk bound relative to the original comparator $f^{*}$. We will soon introduce a function $v$ that modulates how large one can take $\eta$ for a desired $\epsilon$ (the larger $\eta$, the better the bounds that ensue).

In order to work with comparators that are pseudo-predictors, we now introduce $\overline{\mathcal{F}}$, an enlarged action space that is a superset of $\mathcal{F}$ and that also contains the pseudo-predictors we use in the remainder of this work. These pseudo-predictors always will be deterministic and typically will be constant-shifted versions of $\ell_{f}$ (for some $f \in \mathcal{F}$ ) or versions of a GRIP (introduced in Definition 23). Although a given pseudo-predictor $f \in \overline{\mathcal{F}}$ can fail to be well-defined as a playable action, the loss $\ell_{f}$ of any pseudo-action we employ will always be well-defined. We thus extend our loss notation $\ell_{f}(z)$ to all $f \in \overline{\mathcal{F}}$.

We first consider the $v$-central condition, a strict weakening of the strong central condition which applies if the excess loss is bounded or has exponential tails; here the comparator can be taken to be a trivial modification of $f^{*}$. We next consider the $v$-PPC condition, a strict weakening of the $v$-central condition, which applies if the losses have polynomial tails. It is based on using a new type of comparator, the generalized reversed information projection (GRIP), which generalizes a concept from Barron and $\mathrm{Li}$ (1999). In Section 6.1 we present the $v$-central condition and a corresponding excess risk bound for bounded excess risks. Section 6.2 presents the $v$-PPC condition, the GRIP, and the corresponding excess risk bound for bounded excess risks. Finally, Section 6.3 shows risk bounds under the $v$-PPC and $v$-central conditions for unbounded excess risks.

### 6.1. The $v$-Central Condition

Definition 20 ( $\boldsymbol{v}$-Central Condition (Van Erven et al., 2015)) Let $\eta>0$ and $\epsilon \geq 0$. We say that $(P, \ell, \mathcal{F})$ satisfies the $\eta$-central condition up to $\epsilon$ if there exists some $\tilde{f} \in \mathcal{F}$ such that

$$
\begin{equation*}
\ell_{\tilde{f}}-\ell_{f} \unlhd_{\eta} \epsilon \quad \text { for all } f \in \mathcal{F} \tag{44}
\end{equation*}
$$

Let $v:[0, \infty) \rightarrow[0, \infty)$ be a bounded, non-decreasing function satisfying $v(\epsilon)>0$ for all $\epsilon>0$. We say that $(P, \ell, \mathcal{F})$ satisfies the $v$-central condition if, for all $\epsilon \geq 0$, there exists a function $\tilde{f} \in \mathcal{F}$ such that (44) is satisfied with $\eta=v(\epsilon)$.

The special case with constant $v(\epsilon) \equiv \bar{\eta}$ reduces to the earlier strong $\bar{\eta}$-central condition (and then $\tilde{f}$ must be optimal so we can take $\tilde{f}=f^{*}$ ); for nonconstant $v$, the condition is weaker in that it allows a little slack $\epsilon$, and to make $\epsilon$ small, we need to take $\eta$ small. For each $\epsilon \geq 0$, we now define $f_{\epsilon}^{*}$ in terms of its loss by $\forall z \in \mathcal{Z}: \ell_{f_{\epsilon}^{*}}(z):=\ell_{f^{*}}(z)-\epsilon$. This $f_{\epsilon}^{*}$ plays the role of alternative comparator
referred to above. We can now apply Lemma 5 with $f_{\epsilon}^{*}$ instead of $f^{*}$ to get a bound on the annealed excess risk:

$$
\begin{equation*}
\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}^{\mathrm{ANN}(\eta)}\left[\ell_{f}-\ell_{f_{\epsilon}^{*}}\right]\right] \unlhd_{\eta \cdot n} \quad \mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)+\epsilon \tag{45}
\end{equation*}
$$

Analogous to the story in Section 5.1, we want to turn this bound into an actual excess risk bound. This is done by the following lemma, which is a straightforward consequence from the first part of Lemma 13 and only differs from it in that it has $\ell_{f *}$ on the right-hand side replaced by $\ell_{f_{\epsilon}^{*}}$ and a slightly larger constant factor.

Lemma 21 Let $(P, \ell, \mathcal{F})$ be a learning problem that satisfies the $v$-central condition for some $v$. Let $f \in \mathcal{F}$. Suppose that (34) holds for some $u>0$ and $c \in(0,1]$, i.e., $\left(P, \ell,\left\{f, f^{*}\right\}\right)$ satisfies the $(u, c)$-witness condition. Fix $\epsilon \geq 0$ and let $\bar{\eta}=v(\epsilon)$. As in Lemma 13, let $c_{u}=\frac{1}{c} \frac{\eta u+1}{1-\frac{1}{\eta}}$. Then for all $\eta \in(0, \bar{\eta})$,

$$
\begin{equation*}
\mathbf{E}\left[L_{f}\right] \leq c_{u+\epsilon} \cdot \mathbf{E}^{\operatorname{ANN}(\eta)}\left[\ell_{f}-\ell_{f_{\epsilon}^{*}}\right] \tag{46}
\end{equation*}
$$

In particular, if $(P, \ell, \mathcal{F})$ satisfies the $(u, c)$-witness condition then (46) holds for all $f \in \mathcal{F}$.

The key to the proof is that, if $(P, \ell, \mathcal{F})$ satisfies the $v$-central condition, then we have that

$$
\begin{equation*}
\left(P, \ell, \mathcal{F} \cup\left\{f_{\epsilon}^{*}\right\}\right) \text { satisfies the } \eta \text {-central condition with } \eta=v(\epsilon) \text {. } \tag{47}
\end{equation*}
$$

We now show how Lemma 21 straightforwardly implies a strict strengthening of Theorem 14, one which holds under the $v$-central condition rather than just the $\bar{\eta}$-central condition: since (46) holds for all $f \in \mathcal{F}$, it also holds in expectation over $f$, under any arbitrary distribution $\Pi$ over $f$. We can thus take expectations over $\Pi_{n}$ on both sides of (46) and chain the resulting inequality with ESI (45). Using that for general random variables $U, V$ and $c>0, U \unlhd_{a} V \Leftrightarrow c U \unlhd_{u / c} c V$, this gives:

Theorem 22 ( $v$-Central Excess Risk Bound - Bounded Excess Risk Case) Let $\Pi_{\mid}$be an arbitrary learning algorithm based on $\mathcal{F}$. Assume that $(P, \ell, \mathcal{F})$ satisfies the $(u, c)$-witness condition (34) and let $c_{u}$ be defined as in Lemma 21. Then under the $v$-central condition, for any $\epsilon \geq 0$, any $0<\eta<v(\epsilon)$ :

$$
\begin{equation*}
\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}\left[L_{\underline{f}}\right]\right] \unlhd \frac{\eta \cdot n}{c_{u+\epsilon}} c_{u+\epsilon} \cdot\left(\operatorname{IC}_{n, \eta}\left(\Pi_{\mid}\right)+\epsilon\right) . \tag{48}
\end{equation*}
$$

Analogously to the second part of Lemma 13 and Theorem 14, one can give versions of this result for the $\tau$-witness condition as well, but for simplicity we will not do so. This theorem allows unbounded losses but is only useful when the excess risk is bounded, i.e., $\sup _{f \in \mathcal{F}} \mathbf{E}\left[L_{f}\right]<\infty$, because for unbounded risk, the required $(u, c)$-witness condition is excessively strong; see Section 6.3.

The factor $c_{u+\epsilon}$ explodes if $\eta \uparrow v(\epsilon)$. If the $v$-central condition holds for some $v$, it clearly also holds for any smaller $v$, in particular for $\underline{v}(\epsilon):=v(\epsilon) \wedge 1$. Applying the theorem with $\underline{v}$ (which will not affect the rates obtained), we may thus take $\eta=\underline{v}(\epsilon) / 2$, so that $c_{u+\epsilon}$ is bounded by $\frac{1}{c}(u+\epsilon+2)$. The ESI in (48) then implies that with probability at least $1-e^{-K}$ the left-hand side exceeds the right-hand side by at most $\frac{(u+\epsilon+2) K}{c \eta n}$. For the case of bounded excess loss, we can further take $u$ to be $\sup _{f \in \mathcal{F}}\left\|L_{f}\right\|_{\infty}$ and $c=1$. Finally, in the special case when the strong $\bar{\eta}$-central condition holds, we can take $\epsilon=0$ and $v(0)=\bar{\eta}$ and Theorem 22 specializes to Theorem 14.

In Section 6.2 below we introduce the $v$-PPC condition. One of the main results of Van Erven et al. (2015) (in their Section 5) is that, for bounded excess losses, the $v$-central condition holds
for some $v$ with $v(\epsilon) \asymp \epsilon^{1-\beta}$ if and only if the $v$-PPC condition hold for some $v$ with $v(\epsilon) \asymp \epsilon^{1-\beta}$ if and only if the Bernstein condition holds for exponent $\beta$ and some $B>0$; the three conditions are thus equivalent up to constant factors in the bounded excess loss case. The best case of the Bernstein condition of $\beta=1$ corresponds to a $v$ with $v(0)>0$, i.e., to the strong central condition. The Bernstein condition is known to characterize the rates that can be obtained in bounded excess loss problems for proper learners, and the same thus holds for the $v$-central and $v$-PPC conditions. It is also implied by the well-known Tsybakov margin condition as long as $\mathcal{F}$ contains the Bayes optimal classifier (see (Lecué, 2011) and (Van Erven et al., 2015) for discussion).

We now illustrate Theorem 22 for the case of ERM over certain parametric classes when the $v$-central condition holds for $v$ of the form $v(\epsilon) \asymp \epsilon^{1-\beta}$, so that a Bernstein condition holds with exponent $\beta$. We will see that for bounded losses our result recovers, up to log factors, rates that are known to be minimax optimal. We first need some notation. For a pseudo-metric space $(\mathcal{A},\|\cdot\|)$ and any $\epsilon>0$, let $\mathcal{N}(\mathcal{A},\|\cdot\|, \epsilon)$ be the $\epsilon$-covering number of $(\mathcal{A}, \epsilon)$, defined as the minimum number of radius- $\epsilon$ balls whose union contains $\mathcal{A}$.

Example 10 (Lipschitz (and Bounded) Loss) Suppose that (i) for each $z \in \mathcal{Z}$, the loss $\ell$ is $G$ Lipschitz as a function of $f \in \mathcal{F}$; (ii) $\mathcal{F}$ has bounded metric entropy in some pseudometric $\|\cdot\|$; and (iii) the loss is uniformly bounded over $\mathcal{F}$ (so that a witness condition holds). Let $\mathcal{F}_{\epsilon}$ be an optimal $\epsilon$-net with respect to $\|\cdot\|$. Take a uniform prior over $\mathcal{F}$, and (purely for the analysis) consider the randomized predictor $\Pi_{\mid}$that predicts by drawing an $f$ uniformly from a radius- $\epsilon$ ball around $\hat{f}$, the ERM predictor. If the $v$-central condition holds, it follows that the information complexity of $\Pi_{\mid}$is bounded as $G \epsilon+\frac{\log \mathcal{N}(\mathcal{F},\|\cdot\|, \epsilon)}{v(\varepsilon) n}$. To see this, for any $A \subset \mathcal{F}$ let $A^{\epsilon}$ be the $\epsilon$-extension of $A$, defined as $\left\{f \in \mathcal{F}: \inf _{f^{\prime} \in A}\left\|f-f^{\prime}\right\| \leq \epsilon\right\}$. Then observe that

$$
e^{\operatorname{KL}\left(\Pi_{n} \| \Pi_{0}\right)}=\frac{\operatorname{vol}(\mathcal{F})}{\operatorname{vol}\left(\{\hat{f}\}^{\epsilon}\right)} \leq \frac{\operatorname{vol}\left(\cup_{f \in \mathcal{F}_{\epsilon}}\{f\}^{\epsilon}\right)}{\operatorname{vol}\left(\{\hat{f}\}^{\epsilon}\right)} \leq \frac{\left.\sum_{f \in \mathcal{F}_{\epsilon}} \operatorname{vol}\left(\{f\}^{\epsilon}\right)\right)}{\operatorname{vol}\left(\{\hat{f}\}^{\epsilon}\right)}=\mathcal{N}(\mathcal{F},\|\cdot\|, \epsilon)
$$

Moreover, it is easy to see that the risk of standard ERM (rather than its randomized version) over the entire class $\mathcal{F}$ is at most the risk of $\Pi_{n}$ plus an additional $G \epsilon$. Hence, if $v$ satisfies $v(\epsilon)=C \epsilon^{1-\beta}$ for some $\beta \in[0,1]$ and if the metric entropy is logarithmic in $\epsilon$, then by tuning $\epsilon$ and $\eta$ as in (7) we see from (48) that ERM obtains a rate of $\tilde{O}\left(n^{-1 /(2-\beta)}\right)$ (suppressing log-factors) with high probability - which is the minimax optimal rate in this setting (Van Erven et al., 2015). Note that the Bernstein condition is automatically satisfied for $\beta=0$, yielding the slow rate of $\tilde{O}(1 / \sqrt{n})$, and the other extreme of $\beta=1$ yields the fast rate of $\tilde{O}(1 / n)$.

### 6.2. The $v$-PPC Condition and the GRIP

Trivially, if the $v$-central condition holds for some function $v$, then there exists $\epsilon>0$ such that, with $c=e^{\epsilon v(\epsilon)}$, for all $f \in \mathcal{F}, \mathbf{E}\left[e^{-v(\epsilon) L_{f}}\right] \leq c$, so that $-L_{f}$ must have a uniformly exponential upper tail as in Definition 15. Thus, if $-L_{f}$ has a polynomial upper tail, the $v$-central condition cannot hold. The $v$-PPC condition is a further weakening of the $v$-central condition which can still hold in the latter case. We achieve this by replacing the comparator $f_{\epsilon}^{*}$ by a more sophisticated pseudopredictor $m_{\mathcal{F}}^{\eta}$, the generalized reversed information projection (GRIP). The original projection (Li, 1999) was used in the context of density estimation under log loss. We now extend it to general learning problems:

Definition 23 (GRIP) Let $(P, \ell, \mathcal{F})$ be a learning problem. Define ${ }^{3}$ the set of pseudoprobability densities $\mathcal{E}_{\mathcal{F}, \eta}:=\left\{e^{-\eta \ell_{f}}: f \in \mathcal{F}\right\}$. For $Q \in \Delta(\mathcal{F})$, define $\xi_{Q}:=\mathbf{E}_{f \sim Q}\left[e^{-\eta \ell_{f}}\right]$. The generalized reversed information projection of $P$ onto $\operatorname{conv}(\mathcal{E})$ is defined as the pseudo-loss $\ell_{g_{\eta}}$ satisfying

$$
\mathbf{E}\left[\ell_{g_{\eta}}\right]=\inf _{Q \in \Delta(\mathcal{F})} \mathbf{E}\left[-\frac{1}{\eta} \log \mathbf{E}_{\underline{f} \sim Q}\left[e^{-\eta \ell_{\underline{f}}}\right]\right]=\inf _{\xi_{Q} \in \operatorname{conv}(\mathcal{E})} \mathbf{E}\left[-\frac{1}{\eta} \log \xi_{Q}\right]
$$

Following terminology from the individual-sequence prediction literature, we call the quantity appearing in the center expectation above a "mix loss" (De Rooij et al., 2014) defined for a distribution $Q \in \Delta(\mathcal{F})$ as $m_{Q}^{\eta}:=-\frac{1}{\eta} \log \mathbf{E}_{f \sim Q}\left[e^{-\eta \ell_{f}}\right]$. The notion of mix loss can be extended from distributions to sets by defining, for any $A \subseteq \overline{\mathcal{F}}$, the object $m_{A}^{\eta}$ as the pseudo-loss satisfying $\mathbf{E}\left[m_{A}^{\eta}\right]=\inf _{Q \in \Delta\left(A \cup\left\{f^{*}\right\}\right)} \mathbf{E}\left[m_{Q}^{\eta}\right] .{ }^{4}$ We thus have that $\ell_{g_{\eta}}=m_{\mathcal{F}}^{\eta}$, and we use the latter notation from here on out.

Even though the GRIP is only a pseudo-predictor, meaning that it may fail to correspond to any actual prediction function, the corresponding loss for a GRIP is well-defined, as shown in Appendix G. The main use of the GRIP lies in the fact that the probability that its loss exceeds that of any $f \in \mathcal{F}$ is exponentially small:

Proposition 24 For all $f \in \mathcal{F}$, for every $\eta>0$, we have $m_{\mathcal{F}}^{\eta}-\ell_{f} \unlhd_{\eta} 0$.

The proposition implies that $m_{\mathcal{F}}^{\eta} \unlhd_{\eta} \ell_{f^{*}}$ and hence $\mathbf{E}\left[m_{\mathcal{F}}^{\eta}\right] \leq \mathbf{E}\left[\ell_{f^{*}}\right]$ and, for any $\eta>0, \mathcal{F} \cup\left\{m_{\mathcal{F}}^{\eta}\right\}$ satisfies the $\eta$-central condition, with $m_{\mathcal{F}}^{\eta}$ in the role of $f^{*}$. We can now define the $v$-PPC condition:

Definition 25 (Pseudoprobability convexity (PPC) condition) Let $\eta>0$ and $\varepsilon \geq 0$. We say that $(P, \ell, \mathcal{F})$ satisfies the $\eta$-PPC condition up to $\varepsilon$ if there exists some $\tilde{f} \in \mathcal{F}$ such that

$$
\begin{equation*}
\mathbf{E}_{Z \sim P}\left[\ell_{\tilde{f}}\right]-\inf _{Q \in \Delta(\mathcal{F})} \mathbf{E}\left[-\frac{1}{\eta} \log \mathbf{E}_{\underline{f} \sim Q}\left[e^{-\eta \ell_{\underline{f}}}\right]\right] \leq \epsilon \text {, i.e., } \mathbf{E}_{Z \sim P}\left[\ell_{\tilde{f}}-m_{\mathcal{F}}^{\eta}\right] \leq \epsilon \tag{49}
\end{equation*}
$$

Let $v:[0, \infty) \rightarrow[0, \infty)$ be a bounded, non-decreasing function satisfying $v(\epsilon)>0$ for all $\epsilon>0$. We say that $(P, \ell, \mathcal{F})$ satisfies the $v$-PPC condition if, for all $\epsilon \geq 0$, there exists a function $\tilde{f} \in \mathcal{F}$ such that (49) is satisfied with $\eta=v(\epsilon)$.

In both the $v$-central and $v$-PPC conditions, we look at pairs $(\eta, \epsilon)$ such that there exists a comparator $g$ which has risk no better than $\mathbf{E}\left[\ell_{f^{*}}\right]-\epsilon$, and for which $(P, \ell, \mathcal{F} \cup\{g\})$ satisfies the $\eta$-central condition. We achieve this for any $(\eta, \epsilon)$ with $0<\eta \leq v(\epsilon)$, where for the $v$-central condition, the comparator was $g=f_{\epsilon}^{*}$ (see (47)) and for the $v$-PPC condition, it is $g=m_{\mathcal{F}}^{\eta}$.

The name "PPC" stems from the fact that the condition expresses a pseudo-convexity property of the set of pseudoprobability densities mentioned in Definition 23; see Van Erven et al. (2015) for a graphical illustration and for the proof that the $v$-central condition implies the $v$-PPC condition for the same $v$. We already mentioned that Van Erven et al. (2015) (in their Section 5) proved the reverse implication, hence equivalence of the $v$-central and $v$-PPC conditions, up to constant[^1]factors, for bounded excess losses. To give some initial intuition for the unbounded case, we note that the $v$-PPC condition is satisfied for $v(\epsilon)=C \cdot \epsilon$ for a suitable constant $C$ whenever the witness condition holds. While this was known for bounded excess losses (where linear $v$ corresponds to the weakest Bernstein condition, which automatically holds), by Proposition 26 below it turns out to hold even if the excess losses are heavy-tailed (so the $v$-central condition can never hold) and the risk can be unbounded, as long as the second moment of the risk of $f^{*}$ is finite. This will imply, for example, (Theorem 31 below and discussion) that the "slow" $\tilde{O}(1 / \sqrt{n})$ excess risk rate for parametric models can be obtained in-probability by $\eta_{n}$-generalized Bayes (with the optimal $\eta_{n}$ depending on the sample size as $\eta_{n} \asymp 1 / \sqrt{n}$ ) under hardly any conditions.

Proposition 26 Let $(P, \ell, \mathcal{F})$ be such that for all $f \in \mathcal{F}$, all $z \in \mathcal{Z}, \ell_{f}(z) \geq 0$ and such that for some fixed $u>0$, for all $f \in \mathcal{F}$ with $\mathbf{E}\left[L_{f}\right]>0$,

$$
\begin{equation*}
\mathbf{E}\left[\left(\ell_{f}-\ell_{f^{*}}\right) \cdot 1_{\left\{\ell_{f}-\ell_{f^{*}} \leq u\right\}}\right] \geq 0 \tag{50}
\end{equation*}
$$

(in particular this is implied by the ( $u, c$ )-witness condition (34)). Then for all $\eta \leq 1 / \mathbf{E}\left[\ell_{f^{*}}\right]$,

$$
\mathbf{E}_{Z \sim P}\left[\ell_{f^{*}}-m_{\mathcal{F}}^{\eta}\right] \leq \eta \cdot e \cdot\left(u^{2}+\frac{3}{2} \mathbf{E}\left[\ell_{f^{*}}^{2}\right]\right)
$$

As a consequence, if $\mathbf{E}_{Z \sim P}\left[\ell_{f^{*}}^{2}\right]<\infty$ then the $v$-PPC condition holds with $v(\epsilon)=(C \epsilon) \wedge\left(1 / \mathbf{E}\left[\ell_{f^{*}}\right]\right)$ with $C=e^{-1} \cdot\left(u^{2}+\frac{3}{2} \mathbf{E}\left[\ell_{f^{*}}^{2}\right]\right)^{-1}$.

The proof of this proposition is based on the following fact, interesting in its own right and also used in the proof of later results:

Proposition 27 For given learning problem $(P, \ell, \mathcal{F})$, let $\ell^{\prime}$ be such that (a) for all $f \in \mathcal{F}$, all $z \in \mathcal{Z}, \ell_{f}^{\prime}(z) \leq \ell_{f}(z)$, and $(b), \ell_{f^{*}}^{\prime}(z)=\ell_{f}(z)$. If the "smaller-loss" learning problem $\left(P, \ell^{\prime}, \mathcal{F}\right)$ satisfies the $v$-PPC condition for some function $v$, then so does $(P, \ell, \mathcal{F})$.

We now work towards a first risk bound under the $v$-PPC condition, using the GRIP. The development is entirely analogous to that leading up to Theorem 22, our risk bound under the $v$-central condition. We start with the following result, which essentially only differs from Lemma 13 and the corresponding lemma for the $v$-central condition and $f_{\epsilon}^{*}$-comparator, Lemma 21, in that it has $\ell_{f^{*}}$ (as in Lemma 13) and $\ell_{f_{\epsilon}^{*}}$ (as in Lemma 21) on the right-hand side replaced by the GRIP loss $m_{\mathcal{F}}^{\bar{\eta}}$ and requires $\eta<\bar{\eta} / 2$. The proof is much more involved though since the comparators on the left and the right are not connected in a straightforward manner.

Lemma 28 Let $(P, \ell, \mathcal{F})$ be a learning problem and let $f \in \mathcal{F}$. Let $\bar{\eta}>0$. Suppose that (34) holds for some $u>0$ and $c \in(0,1]$, i.e., $\left(P, \ell,\left\{f, f^{*}\right\}\right)$ satisfies the $(u, c)$-witness condition. Let $c_{u}^{\prime}:=\frac{1}{c} \frac{\eta \cdot u+1}{1-\frac{2 \eta}{\bar{\eta}}}$. Then for all $\eta \in(0, \bar{\eta} / 2)$,

$$
\begin{equation*}
\mathbf{E}\left[L_{f}\right] \leq c_{2 u}^{\prime} \cdot \mathbf{E}^{\operatorname{ANN}(\eta)}\left[\ell_{f}-m_{\mathcal{F}}^{\bar{\eta}}\right] \tag{51}
\end{equation*}
$$

In particular, if $(P, \ell, \mathcal{F})$ satisfies the $(u, c)$-witness condition then (51) holds for all $f \in \mathcal{F}$.

Based on this lemma it is now easy to prove analogues of Theorem 14. Below we first present our second main result, an excess risk bound that holds under the basic witness condition. The result allows unbounded and heavy-tailed losses but is only useful when the excess risk is bounded; see Section 6.3.

Theorem 29 (Excess Risk Bound - Bounded Excess Risk Case) Let $\Pi_{\mid}$be an arbitrary learning algorithm based on $\mathcal{F}$. Assume that $(P, \ell, \mathcal{F})$ satisfies the $(u, c)$-witness condition (34). Let $c_{u}^{\prime}$ be as in Lemma 28. Then under the $v$-PPC condition, for any $\eta<\frac{v(\epsilon)}{2}$,

$$
\begin{equation*}
\mathbf{E}_{Z_{1}^{n}}\left[\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\mathbf{E}\left[L_{\underline{f}}\right]\right]\right] \leq c_{2 u}^{\prime}\left(\mathbf{E}_{Z_{1}^{n}}\left[\operatorname{IC}_{n, \eta}\left(\Pi_{\mid}\right)\right]+\epsilon\right) \tag{52}
\end{equation*}
$$

The result is entirely analogous to Theorem 22 (and the remarks made there apply here as well), with two differences: first, $v$ is replaced by $v / 2$, which will worsen the obtainable bounds by a factor of 2 and hence will not affect the rates. Second, the ESI in (48) is replaced by an expectation. Thus, we have an exponential in-probability bound (holding with probability $1-\delta$ up to an $O(\log (1 / \delta))$-term) under the $v$-central condition but not under the $v$-PPC condition. That such a deviation bound does not hold under the $v$-PPC condition is inevitable since all of our bounds are valid for ERM estimators, which, under heavy-tailed loss distributions, are known to behave poorly in probability (Catoni, 2012, Proposition 6.2). There exist specialized $M$-estimators for mean estimation problems (Catoni, 2012) or more generally (for regression problems) that achieve better high-probability bounds by employing a variation of the median-of-means idea (Nemirovskii and Yudin, 1983; Hsu and Sabato, 2016; Lugosi and Mendelson, 2019).

To illustrate Theorem 29, we now provide an example where the $v$-central condition cannot hold because the excess risk has polynomially decaying tails; yet, the $v$-PPC condition may still hold for $v$ that allow for faster rates than the "slow" $\tilde{O}(1 / \sqrt{n})$.

Example 11 (Heavy-tailed regression with bounded predictions) We continue with the setting of Example 7. In addition to assuming that $\mathbf{E}\left[Y^{2} \mid X\right] \leq C$ a.s. for a constant $C$, we also assume that $\mathbf{E}\left[|Y|^{s}\right]<\infty$ for some $s \geq 2$; note that the first assumption already implies the second for $s=2$. We further assume that $\mathcal{F}$ has bounded metric entropy in sup-norm, with covering numbers $\mathcal{N}\left(\mathcal{F},\|\cdot\|_{\infty}, \epsilon\right)$ growing polynomially in $\epsilon$. Without subexponential tail decay, the $v$-central condition fails to hold for any non-trivial $v$; however, as shown by Van Erven et al. (2015, Example 5.10) (based on a result of Juditsky et al. (2008)), if $\mathbf{E}\left[|Y|^{s}\right]<\infty$ for some $s \geq 2$, then the $v$-PPC condition holds for $v(\epsilon)=O\left(\epsilon^{2 / s}\right) \cdot{ }^{5}$ Moreover, as we showed in Example 7, the witness condition holds if $\mathbf{E}\left[Y^{2} \mid X\right]<\infty$ a.s.; there, we also established that the Bernstein condition holds with $\beta=1$.

Now, take a uniform prior over $\mathcal{F}$, and take the randomized predictor $\Pi_{\mid}$as in Example 10 which randomizes over an $\epsilon$-ball around the ERM predictor $\hat{f}$. Then, for $s \geq 2$, Theorem 29 implies that the expected excess risk of $\Pi_{n}$ is at most

$$
\mathbf{E}_{Z_{1}^{n}}\left[\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\frac{1}{n} \sum_{j=1}^{n} L_{\underline{f}}\left(Z_{j}\right)\right]\right]+\frac{\log \mathcal{N}(\mathcal{F},\|\cdot\|, \epsilon)}{v(\epsilon) n}+\epsilon
$$

5. What is actually shown there is that a property called $v$-stochastic exp-concavity holds, but, the results of that paper imply then that $v$-stochastic mixability holds which in turn implies that the $v$-PPC condition holds.

The first term can be bounded as

$$
\begin{aligned}
& \mathbf{E}_{Z_{1}^{n}}\left[\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\frac{1}{n} \sum_{j=1}^{n}\left(L_{\hat{f}}\left(Z_{j}\right)+\ell_{\underline{f}}\left(Z_{j}\right)-\ell_{\hat{f}}\left(Z_{j}\right)\right)\right]\right] \\
& \leq \mathbf{E}_{Z_{1}^{n}}\left[\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\frac{1}{n} \sum_{j=1}^{n}\left(\ell_{\underline{f}}\left(Z_{j}\right)-\ell_{\hat{f}}\left(Z_{j}\right)\right)\right]\right] \\
& =\mathbf{E}_{Z_{1}^{n}}\left[\mathbf{E}_{\underline{f} \sim \Pi_{n}}\left[\frac{1}{n} \sum_{j=1}^{n}\left(\underline{f}^{2}\left(X_{j}\right)-\hat{f}^{2}\left(X_{j}\right)+2 Y_{j}\left(\hat{f}\left(X_{j}\right)-\underline{f}\left(X_{j}\right)\right)\right)\right]\right] \\
& \leq \mathbf{E}_{Z_{1}^{n}}\left[2 \epsilon\left(\|\mathcal{F}\|_{\infty}+\left(\frac{1}{n} \sum_{j=1}^{n} Y_{j}^{2}\right)^{1 / 2}\right)\right]
\end{aligned}
$$

which is at most $2 \epsilon\left(\|\mathcal{F}\|_{\infty}+\|Y\|_{L_{2}(P)}\right)=O(\epsilon)$, and it is simple to verify that the ERM predictor $\hat{f}$ satisfies the same bound. Tuning $\epsilon$ in $O\left(\epsilon+\frac{\log \mathcal{N}(\mathcal{F},\|\| \|, \epsilon)}{\epsilon^{2 / s} n}\right)$ yields a rate of $\tilde{O}\left(n^{-s /(s+2)}\right)$ in expectation, where the notation hides log factors.

Two remarks are in order about the rate obtained in the above example.

First, Juditsky et al. (2008) previously obtained this rate for finite classes $\mathcal{F}$ without the assumption that $\mathbf{E}\left[Y^{2} \mid X\right]$ is almost surely uniformly bounded; their result is achieved by an online-tobatch conversion of a sequential algorithm which, after the conversion, plays actions in the convex hull of $\mathcal{F}$. It is unclear if we truly need the assumption on the conditional second moment of $Y$ or if the need for this assumption is just an artifact of our analysis. In the regime where our stronger assumption holds, in the case of convex luckiness (see Example 7) the rates obtained in the present paper match those of Juditsky et al. (2008). However, if convex luckiness does not hold, then the results of Juditsky et al. (2008) still enjoy the rate of $\tilde{O}\left(n^{-s /(s+2)}\right)$ whereas we cannot guarantee this rate. This is not surprising: without convex luckiness, "improper learners" that play in the convex hull of $\mathcal{F}$ are inherently more powerful than (randomized) proper learners.

Second, even when convex luckiness does hold, the rate obtained in Example 11 above is not optimal. The reason is that in the setting of this example, a Bernstein condition with $\beta=1$ does hold, as was established earlier in Example 7. Thus, via Corollary 6.2 of Audibert (2009) it is possible to obtain the better rate of $\tilde{O}(1 / n)$ in expectation using Audibert's SeqRand algorithm. Notably, the SeqRand algorithm for statistical learning involves using a sequential learning algorithm which incorporates a second-order loss-difference term. For new predictions, SeqRand employs an online-to-batch conversion based on drawing functions uniformly at random from the set of previously played functions. It is thus a randomized proper learning algorithm. There are now two possibilities. The first is that there exist $\mathcal{F}$ satisfying the condition of Example 7 for which ERM and $\eta$-generalized Bayes simply do not achieve the rate of $\tilde{O}(1 / n)$; in that case either SeqRand's second-order nature or its online-to-batch step may be needed to get the fast rate. The other possibility is that ERM and $\eta$-generalized Bayes do generally attain the fast rate under the Bernstein condition and a.s. bounded $\mathbf{E}\left[Y^{2} \mid X\right]$-condition, in which case Theorem 29 is suboptimal for this situation - we return to this issue in the Discussion (Section 7). In any case, SeqRand is computationally intractable for most infinite classes, and we are not aware of any polynomial-time learning algorithms that match the rate of SeqRand.

### 6.3. Bounds for Unbounded Excess Risk

We now present a result for a learning problem $(P, \ell, \mathcal{F})$ with unbounded excess risk. Once again, the result follows (now with some work) from Lemma 28, but now we need to be careful because the $(u, c)$-witness condition with fixed $u$ and $c$ cannot be expected to hold: it would become an exceedingly strong condition for $\mathbf{E}\left[L_{f}\right] \rightarrow \infty$. We will thus require the $\tau$-witness condition for a particular, easier $\tau$, namely $\tau(x)=u(1 \vee x)$ for some $u \geq 1$, so that for large $x, \tau(x) \asymp x$. We first show, in Proposition 30 below, that at least for the squared loss this condition can be expected to hold in a variety of situations. The price to pay for using this $\tau$ is that we only get in-probability results - we show those in Theorem 31 (we do not know whether in-expectation results hold as well). Note that one could obtain better constants in that theorem if one employed $\tau(x)=a \vee(b x)$ for the best possible $a$ and $b$, but for simplicity we did not do this.

Proposition 30 (Bernstein plus small-ball implies unbounded witness) Consider the setting of Example 7, i.e., regression with $\ell$ the squared loss and convex luckiness. We still assume convex luckiness and make the weaker assumption $\mathrm{E}\left[Y^{2}\right]<\infty$, but now we do not not assume that the risk is bounded; i.e., we can have $\sup _{f \in \mathcal{F}} \mathrm{E}\left[\ell_{f}\right]=\infty$. Fix some $b>0$ and suppose that there exists constants $\kappa>0, \epsilon \in(0,1)$ such that

1. for all $f \in \mathcal{F}$ with $\mathbf{E}\left[L_{f}\right]>b$, Mendelson's (2014) small-ball assumption (43) holds with constants $\epsilon, \kappa$ for $f, f^{*}$ (i.e. with $f^{*}$ in the role of $h$ ),
2. For all $c_{0}>b$, all $f \in \mathcal{F}$ with $\mathbf{E}\left[L_{f}\right] \leq c_{0}$, there is a $B$ such that the $(1, B)$-Bernstein condition holds, i.e., $\mathbf{E}\left[L_{f}^{2}\right] \leq B \mathbf{E}\left[L_{f}\right]$.

Then $(P, \ell, \mathcal{F})$ satisfies the $(\tau, c)$-witness condition, with $\tau(x)=u(1 \vee x)$ for some $u \geq 1$ and with $c \in(0,1]$ which depends only on $\kappa, \epsilon, b$, and $\mathbf{E}\left[\ell_{f^{*}}\right]$.

Example 12 (Heavy-Tailed Regression, Continued) Mendelson provides several examples of convex $\mathcal{F}$ for which the small-ball assumption holds; the proposition above shows that for all these examples, the $\tau$-witness condition holds as well as soon as, for $f$ with small excess risk, the Bernstein condition holds. For example, under the following "meta"-condition the small-ball assumption holds (see (Mendelson, 2014, Lemma 4.1)) and, as we show in Appendix C.3, the Bernstein condition holds as well for $\mathcal{F}_{c_{0}}:=\left\{f \in \mathcal{F}: \mathbf{E}\left[L_{f}\right]<c_{0}\right\}$, for all $c_{0} \geq b$, as long as we assume convex luckiness (see Example 7).

$$
\begin{aligned}
& \mathbf{E}\left[\ell_{f^{*}}^{2}\right]<\infty \quad \text { and } \quad \text { for some } A>0, \text { for all } f \in \mathcal{F}_{c_{0}} \\
& \mathbf{E}\left[\left(f(X)-f^{*}(X)\right)^{4}\right]^{1 / 2} \leq A \cdot \mathbf{E}\left[\left(f(X)-f^{*}(X)\right)^{2}\right]
\end{aligned}
$$

We stress however that our theorem below does not recover Mendelson's rates for $L_{2}(P)$-estimation error (Section 7), which rely on further highly sophisticated analysis of the squared loss situation; our goal here is merely to show that our $\tau$-witness condition for the unbounded risk case is not a very strong one.

Theorem 31 (Excess Risk Bound - Unbounded Excess Risk Case) Assume that $(P, \ell, \mathcal{F})$ satisfies the $(\tau, c)$-witness condition (35) with $\tau: x \mapsto u(1 \vee x)$ for some $u \geq 1$ and constant c. Let $\epsilon_{1}, \epsilon_{2}, \ldots$ and $\eta_{1}, \eta_{2}, \ldots$ be sequences such that

$$
\epsilon_{n} \rightarrow 0, \quad n \eta_{n} \rightarrow \infty
$$

Let $c_{u}:=\frac{u}{c} \frac{\eta_{n}+1}{1-\frac{\eta_{n}}{v\left(\epsilon_{n}\right)}}$ and $c_{u}^{\prime}:=\frac{u}{c} \frac{\eta_{n}+1}{1-\frac{2 \eta_{n}}{v\left(\epsilon_{n}\right)}}$. Suppose that $\mathrm{IC}_{n, \eta}:=\mathrm{IC}_{n, \eta}\left(\Pi_{\mid}\right)$is nontrivial in the sense that $\mathbf{E}\left[\mathrm{IC}_{n, \eta_{n}}\right] \rightarrow 0$.

1. Let $\Pi_{\mid} \equiv\left(\hat{f}, \Pi_{0}\right)$ represent a deterministic estimator. Suppose that, for given function $v$, the $v$-PPC condition holds and that for all $n, 0<\eta_{n}<v\left(\epsilon_{n}\right) / 2$. Then for all $n$ larger than some $n_{0}$, the right-hand side of the following equation is bounded by 1 , and for all such $n$, for all $\delta>0$, with probability at least $1-\delta$,

$$
\begin{equation*}
\mathbf{E}\left[L_{\hat{f}}\right] \leq\left(c_{2 u}^{\prime} \cdot \frac{1}{\delta}\right) \cdot \text { BOUND, with BOUND }=\left(\mathbf{E}\left[\mathrm{IC}_{n, \eta_{n}}\right]+\epsilon_{n}\right) \tag{53}
\end{equation*}
$$

Now suppose that, more strongly, the v-central condition holds as well. Let $\overline{\mathrm{IC}}_{n, \eta}$ be any upper bound on $\mathrm{IC}_{n, \eta}\left(f^{*} \| \Pi_{\mid}\right)$that is nontrivial in that $\mathbf{E}\left[\overline{\mathrm{IC}}_{n, \eta_{n}}\right] \rightarrow 0$. Let $C_{n, \delta}$ be a function of $\delta \in(0,1)$ such that for all $\delta \in(0,1), C_{n, \delta}>2 \log (2 / \delta)$ and

$$
\begin{equation*}
P\left(\overline{\mathrm{IC}}_{n, \eta_{n}} \geq C_{n, \delta} \cdot \mathbf{E}\left[\overline{\mathrm{IC}}_{n, \eta_{n}}\right]\right) \leq \delta \tag{54}
\end{equation*}
$$

Then for all $n$ larger than some $n_{0}$, the right-hand side of the following equation is bounded by 1, and for all such $n$, for all $0<\delta<1$, with probability at least $1-\delta$,

$$
\begin{equation*}
\mathbf{E}\left[L_{\hat{f}}\right] \leq\left(c_{u+\epsilon_{n}}^{\prime} \cdot C_{n, \delta}\right) \cdot \text { BOUND, with BOUND }=\left(\mathbf{E}\left[\overline{\mathrm{IC}}_{n, \eta_{n}}\right]+\epsilon_{n}+\frac{2}{n \eta_{n}}\right) \tag{55}
\end{equation*}
$$

2. Now let $\Pi_{\mid}$be a general, potentially nondeterministic estimator, suppose that the $v-P P C$ condition holds and let $\overline{\mathrm{IC}}_{n, \eta_{n}}$ be any bound on $\mathrm{IC}\left(\Pi_{\mid}\right)$that is slightly larger than $\mathrm{IC}_{n, \eta_{n}}$, i.e., there exist a sequence $a_{1}, a_{2}, \ldots \rightarrow \infty$ such that, for all $n$, all $z^{n}, \overline{\mathrm{IC}}_{n, \eta_{n}} \geq a_{n} \mathrm{IC}_{n, \eta_{n}}$. Then

$$
\begin{equation*}
\Pi_{n}\left(\left\{f \in \mathcal{F}: \mathbf{E}\left[L_{f}\right]>c_{2 u}^{\prime} \cdot \mathrm{BOUND}\right\}\right) \rightarrow 0 \text { in P-probability } \tag{56}
\end{equation*}
$$

with $\mathrm{BOUND}=\mathbf{E}\left[\overline{\mathrm{IC}}_{n, \eta_{n}}\right]+\epsilon_{n}$.

When $\Pi_{\mid}$represents a deterministic estimator $\hat{f}$ such as an $\eta$-two part MDL estimator, the result is just a standard convergence-in-probability result. For learning algorithms that output a distribution such as generalized Bayes, the result seems fairly weak as nothing is said about the rate at which the deviation probability goes to 0 . Note, however, that the same holds for most standard results about posterior convergence in Bayesian statistics; for example, the results of GGV (see Example 2) are stated in exactly the same manner.

Note that the factor for the PPC-results increases quickly with $\delta$; depending on how strong a bound (54) can be given, the $v$-central results can thus become substantially stronger asymptotically. This is the case even though their bound has an additional $1 /\left(n \eta_{n}\right)$ term. Indeed, this extra term is of the right order, comparable to the upper bound on $\mathrm{IC}_{n, \eta_{n}}$ given by (4). Therefore, for $v(x) \asymp x^{1-\beta}$, optimization of $\epsilon_{n}$ and $\eta_{n}$ can be done in the same way as for the bounded risk case, leading to a rate of $\tilde{O}\left(n^{-1 /(2-\beta)}\right)$ as in (7). To give an example in which the bound for the $v$-central condition gets a better dependence on $\delta$ than $v$-PPC consider generalized Bayesian posteriors under the GGV condition (21) discussed in Section 3.3; in that case, we get the bound (25) which implies (54) for a $C_{n, \delta}=o\left(\delta^{-1 / 2}\right)$ (rather than the $O\left(\delta^{-1}\right)$ in the PPC-result) and with $\epsilon_{n}$, as defined there used as an upper bound on $\mathrm{IC}_{n, \eta}$. Still, in this example $C_{n, \delta}$ is polynomial in $\delta$ whereas Theorem 22 had only
a logarithmic dependence on $\delta$. As mentioned earlier, this stronger dependence on $\delta$ is unavoidable as the results under the $v$-PPC condition apply to methods like ERM, which have poor deviation properties.

To derive further corollaries from this theorem, we mention the following extension of Proposition 26:

Proposition 32 (when $(\tau, c)$-witness implies $v$-PPC) Suppose that the $(\tau, c)$-witness condition holds for given learning problem $(P, \ell, \mathcal{F})$ with $\tau: x \mapsto u(1 \vee x)$ for some $u \geq 1$ and constant $c \in(0,1]$ as in Theorem 31. Further suppose that that $\ell_{f}(z) \geq 0$ for all $f \in \mathcal{F}$ and all $z \in \mathcal{Z}$. Then the $v-P P C$ condition holds with $v(\epsilon)=(C \epsilon) \wedge\left(1 / \mathbf{E}\left[\ell_{f^{*}}\right]\right)$, where $C=e^{-1} \cdot\left(u^{2}\left(1 \vee\left(\mathbf{E}\left[\ell_{f^{*}}\right] / c\right)^{2}\right)+\right.$ $\left.\frac{3}{2} \mathbf{E}\left[\ell_{f^{*}}^{2}\right]\right)^{-1}$.

The above proposition implies that if the $\tau$-witness condition holds with $\tau$ as in Theorem 31 above, then the results (53) and (56) automatically hold with choice $2 \eta_{n}<\left(C \epsilon_{n}\right) \wedge\left(1 / \mathbf{E}\left[\ell_{f^{*}}\right]\right)$, which for large $n$ is equivalent to $\eta_{n}<C \epsilon_{n} / 2$. For parametric $\mathcal{F}$ we can take $\epsilon_{n} \asymp 1 / \sqrt{n}$, so that the $v$-PPC condition is satisfied with $\eta_{n} \asymp 1 / \sqrt{n}$. Thus, under quite weak conditions (for all $f, z, \ell_{f}(z) \geq 0$, $\mathbf{E}\left[\ell_{f^{*}}^{2}\right]<\infty$, and the $\tau$-witness condition holds as above), but with unbounded, heavy tailed losses and without explicitly imposing any GRIP conditions, we get in all three cases of Theorem 31, by choosing $\eta_{n} \asymp 1 / \sqrt{n}$, that BOUND $=\tilde{O}(1 / \sqrt{n})$. Consequently, even under very weak assumptions, we still get convergence for generalized $\eta_{n}$-Bayesian estimators, albeit at the "slow" rate.

## 7. Discussion \& Open Questions

In this paper we presented several theorems that gave convergence rates for general estimators, including pseudo-Bayesian and ERM estimators, under general "easiness conditions". We end by putting these conditions in context and discussing some of the limitations of our approach, thereby pointing to avenues for future work.

Easiness Conditions We proved our convergence rates under the GRIP conditions (the $v$-central and $v$-PPC conditions) and the $\tau$-witness condition, and we provided some relations to other conditions such as convex luckiness for squared loss (defined in Example 7), Bernstein conditions (Definition 17), and uniformly exponential tails (Definition 15). As promised in the beginning of this paper, our conditions and results complement those of Van Erven et al. (2015) which are mostly for the bounded case. The most important conditions of that paper that did not show up here are (a) the extension of convex luckiness beyond the squared loss (it is formally defined for general losses by Van Erven et al. (2015) under the name "Assumption B") and (b) the $v$-stochastic mixability condition (see Definition 5.9 of Van Erven et al. (2015)). We will restrict discussion of the $v$-stochastic mixability condition to the case where the decision set $\mathcal{F}_{d}$ from Van Erven et al. (2015) is equal to $\operatorname{conv}(\mathcal{F})$. In the present paper, where the set $\mathcal{P}$ from Van Erven et al. (2015) is always equal to the singleton $\{P\}$, it is easy to see that $v$-stochastic mixability is equivalent to the $v$-PPC condition but with the minimizer $f^{*}$ over $\mathcal{F}$ replaced by the minimizer $f_{\text {conv }}^{*}$ over $\operatorname{conv}(\mathcal{F})$. Van Erven et al. (2015) show that for bounded excess losses, $v$-stochastic mixability characterizes obtainable rates for improper learners that are allowed to play in the convex hull of $\mathcal{F}$. $v$-stochastic mixability is in turn implied by the easiness conditions of Juditsky et al. (2008), (for constant $v$ ) by conditions on the loss function such as mixability and exp-concavity (Cesa-Bianchi and Lugosi, 2006), and by strong convexity. For clarity we give an overview of the relevant implications between our conditions and those of Van Erven et al. (2015) in Figure 1.

| excess loss <br> is... | condition <br> type | loss function | result |
| :--- | :--- | :--- | :--- |
| bounded | GRIP | general | $v$-PPC $\Leftrightarrow v$-central (vE) <br> $x^{1-\beta}$-PPC $\Leftrightarrow x^{\beta}$-Bernstein $(\mathrm{vE})$ |
|  | witness | general | $(u, c)$-witness always holds (trivial) |
| unbounded | GRIP | general | convex luckiness $+v$-stochastic mixability $\Rightarrow v$ - <br> PPC $(\mathrm{vE})$ <br> $v$-central $\Rightarrow v$-PPC (vE) <br> $v$-central $\Rightarrow L_{f}$ has uniformly exponential lower tail <br> $(\mathrm{vE})$ <br> convex luckiness $\Rightarrow 1$-central (vE) <br> convex luckiness + bounded predictions $+Y \mid X$ <br> has a.s. uniformly bounded 2 <br> Bernstein $(\mathrm{GM}$, Example 7) 7 |
| unbounded $\Rightarrow(1, B)-$ |  |  |  |
|  |  | general <br> general | $(\beta, B)$-Bernstein $\Rightarrow(\tau, c)$-witness, $\tau(x) \leftrightharpoons x^{\beta-1}$ <br> $(\mathrm{GM}$, Proposition 18) <br> $L_{f}$ has uniformly exponential upper tail $\Rightarrow(\tau, c)-$ <br> witness, $\tau(x) \leftrightharpoons 1 \vee \log (1 / x)($ GM, Lemma 16) <br> Wong-Shen $\Leftrightarrow L_{f}$ has uniformly exponential tails <br> $(\mathrm{GM}$, Example 6) |

Figure 1: GM stands for "established in the present paper", vE refers to Van Erven et al. (2015). All implications hold up to constant factors. Note that boundedness always refers to excess loss. For example, for Lipschitz losses on a bounded domain, the losses themselves may have heavy tails but the excess loss will be bounded.

Misspecification We showed that our methods are particularly well-suited for proving a form of consistency for (generalized Bayesian) density estimation under misspecification; under only the $\bar{\eta}$ central condition, a weak condition on the support of $p_{f^{*}}$, and using a prior such that the weakened GGV condition (22) holds, we can show that for any $\eta<\bar{\eta}$, the $\eta$-generalized Bayesian posterior is consistent in the sense of our misspecification metric (see Proposition 11 and discussion below it). As stated there, an interesting open question is under which conditions the metric entropy for the misspecified case is of the same order as the metric entropy for the well-specified case, as then the misspecification metric dominates the standard Hellinger metric.

Proper vs. Improper There exist learning problems $(P, \ell, \mathcal{F})$ on which no proper learner - one which always predicts inside $\mathcal{F}$ - can achieve a rate as good as that of an improper learner, which can select $\hat{f}_{n} \notin \mathcal{F}$ (Audibert, 2007; Van Erven et al., 2015). In this paper we considered randomized proper estimators, to which the same lower bounds apply; hence, they cannot in general compete with improper methods such as exponentially weighted average forecasters and other aggregation methods. Such methods achieve fast rates under conditions such as stochastic exp-concavity (Juditsky et al., 2008), which imply the "stochastic mixability" condition that, as explained by Van Erven et al. (2015), is sufficient for fast rates for aggregation methods. To get rates comparable to those of improper learners, we invariably need to make a "convex luckiness" assumption under which, as again shown by Van Erven et al. (2015), $v$-stochastic mixability implies the $v$-PPC condition (see also Figure 1); the latter allows for fast rates for randomized proper learners. An interesting question for future work is whether our proof techniques can be extended to incorporate, and get the right rates for, improper methods such as the empirical star estimator (Audibert, 2007) and Q-aggregation (Lecué and Rigollet, 2014). Since the original analysis of these methods bears some similarity to our techniques, this might very well be possible.

While superior rates for improper learners are inevitable, it is more worrying that the rate we showed for ERM in heavy-tailed bounded regression is worse than the rate for the SeqRand algorithm, which is also randomized proper (see Example 11 and text below it). We do not know whether the rate we obtain is the actual worst-case rate that ERM achieves under our conditions, or whether ERM achieves the same rate as SeqRand, or something in between. In the latter two cases, it would mean that our bounds are suboptimal. Sorting this out is a major goal for future work.

Empirical process vs Information-theoretic Broadly speaking, one can distinguish approaches to proving excess risk bounds into two main groups: on the one hand are approaches based on empirical process theory (EPT) such as (Bartlett et al., 2005; Bartlett and Mendelson, 2006; Koltchinskii, 2006; Mendelson, 2014; Liang et al., 2015; Dinh et al., 2016) and most work involving VC dimension in classification. On the other hand are information-theoretic approaches based on prior measures, change-of-measure arguments, and KL penalties such as PAC-Bayesian and MDL approaches (Barron and Cover, 1991; Li, 1999; Catoni, 2003; Audibert, 2004; Grünwald, 2007; Audibert, 2009). A significant advantage of EPT approaches is that they often can achieve optimal rates of convergence for "large" models $\mathcal{F}$ with metric entropy $\log \mathcal{N}(\mathcal{F},\|\cdot\|, \epsilon)$ that increases polynomially in $1 / \epsilon$, where $\|\cdot\|$ is the $L_{1}(P)$ or $L_{2}(P)$-metric. Prior-based approaches (including the one in this paper) may yield suboptimal rates in such cases (see Audibert (2009) for discussion). A closely related advantage of EPT approaches is that they can handle empirical covers of $\mathcal{F}$, thus allowing one to prove bounds for VC classes, among others.

An advantage of prior-based approaches is that they inherently penalize, so that whenever one has a countably infinite union of classes $\mathcal{F}=\cup_{j \in \mathbb{N}} \mathcal{F}_{j}$, the approaches automatically adapt to the
rate that can be obtained as if the best $\mathcal{F}_{j}$ containing $f^{*}$ were known in advance; this adaptation was illustrated at various places in this paper (see final display in Proposition 6, equation (27)). This happens even if for every $n$, there is a $j$ and $f \in \mathcal{F}_{j}$ with empirical error 0 ; in such a case unpenalized methods as often used in EPT methods would overfit. In the paper (Grünwald and Mehta, 2019), a companion paper to the present one, we show for bounded excess losses that the two approaches may be combined. In fact one can provide a single excess risk bound in which the information complexity is replaced by a strictly smaller quantity and instead of a prior one uses a more general "luckiness function" (Grünwald, 2007) that is better suited for dealing with penalized estimators. For some choices of luckiness function, one gets a slight strengthening of the excess risk bounds given in this paper; for other choices, one gets bounds in terms of Rademacher complexity, $L_{2}(P)$ and empirical $L_{2}\left(P_{n}\right)$ covering numbers. Thus, the best of both worlds is achievable, but for the time being only for bounded excess losses.

Another major goal for future work is thus to provide such a combined EPT-information theoretic bound for unbounded excess losses that allows for heavy-tailed excess loss. Within the EPT literature, some work has been done: Mendelson $\left(2014,2017\right.$ b) provides bounds on the $L_{2}(P)$ estimation error $\left\|\hat{f}-f^{*}\right\|_{L_{2}(P)}^{2}$ and Liang et al. (2015) on the related squared loss risk. For other loss functions not much seems to be known: Mendelson (2017b) shows that improved $L_{2}(P)$-estimation error rates may be obtained by using other, proxy loss functions during training; however, the target remains $L_{2}(P)$-estimation. In contrast, our approach allows for general loss functions $\ell$ including density estimation, but we do not specially study proxy training losses.

These last three EPT-based works can deal with $(P, \ell, \mathcal{F})$ with unbounded excess (squared loss) risk. This is in contrast to earlier papers in the information-theoretic/PAC-Bayes tradition; as far as we know, our work is the first one that allows one to prove excess risk convergence rates in the unbounded risk case (Theorem 31) for general models including countable infinite unions of models as in Proposition 6. Previous works dealing with unbounded excess loss all rely on a Bernstein condition — we are aware of (Zhang, 2006a), requiring $\beta=1$; (Audibert, 2004), for the transductive setting rather than our inductive setting; and, the most general, (Audibert, 2009). However, for convex or linear losses, a Bernstein condition can never hold if $\sup _{f \in \mathcal{F}} \mathbf{E}\left[L_{f}\right]$ is unbounded, as follows trivially from inspecting Definition 17 , whereas the $v$-central and PPC-conditions can hold. See for instance Example 15 in Appendix I, where $\mathcal{F}$ is just the densities of the normal location family without any bounds on the mean: here the Bernstein condition must fail, yet the strong central condition and the witness condition both hold and thus Theorem 31 applies (for some moderate $M$ ).

In the unbounded-excess-loss-yet-bounded-risk case, the difference between these works and ours opaques: there may well be cases (though we have not produced one) where the Bernstein condition holds for some $\beta$ but the $v$-PPC condition does not hold for $v(\epsilon) \asymp \epsilon^{1-\beta}$; the opposite certainly can happen (note however that in the bounded excess loss case these two conditions are equivalent; see Figure 1). Indeed, Example 14 in Appendix I exhibits an $\mathcal{F}$ for which the excess risk is bounded but its second moment is not, whence the Bernstein condition fails to hold for any positive exponent, while both the strong central condition and the witness condition hold. Theorem 29 therefore applies whereas the results of Audibert (2009) and Zhang (2006b) do not. Finally we note that Audibert (2009) proves his bounds for his ingenious SeqRand learning algorithm, whereas Zhang's and our bounds hold for general estimators.

Yet another major goal for current work is thus to disentangle the role of the PPC condition and the Bernstein condition for unbounded excess losses; ideally we would extend our bounds to cover faster rates under a weaker condition implied by either of the Bernstein or PPC conditions.

Additional future work: learning $\eta \quad$ A general issue with generalized Bayesian and MDL methods, but one that is avoided by ERM, is the fact that they depend on the learning rate parameter $\eta$. While this is often pragmatically resolved by cross-validation (see e.g. Audibert (2009) and many others), Grünwald $(2011,2012)$ give a method for learning $\eta$ that provably finds the "right" $\eta$ (i.e. optimal for the best Bernstein condition that holds for the given learning problem) for bounded excess loss functions and likelihood ratios; experiments (Grünwald and Van Ommen, 2017) indicate that this "safe Bayesian" method works excellently in the unbounded case as well. While it seems that the proof technique to handle learning $\eta$ carries over to the present unbounded setting, actually proving that the SafeBayes method still works remains a task for future work.

## 8. Acknowledgments

We would like to thank Bob Williamson (who saw the use of bringing structure in the various existing 'easiness conditions'), Tim van Erven (who brought our attention to the fact that GGV's entropy condition is not needed whenever $\eta<1$, as earlier noted by Tong Zhang) and Bas Kleijn (who brought Wong and Shen (1995) to our attention), Andrew Barron (for various discussions), and Alice Kirichenko (for pointing out numerous small mistakes). An anonymous referee made some highly useful suggestions. This research was supported by the Netherlands Organization for Scientific Research (NWO) VICI Project Nr. 639.073.04.

## References

Jean-Yves Audibert. PAC-Bayesian statistical learning theory. Thèse de doctorat de l'Université Paris, 6:29, 2004.

Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In NIPS, 2007.

Jean-Yves Audibert. Fast learning rates in statistical inference through aggregation. The Annals of Statistics, 37(4):1591-1646, 2009.

Jean-Yves Audibert and Alexandre B. Tsybakov. Fast learning rates for plug-in classifiers. The Annals of statistics, 35(2):608-633, 2007.

Andrew Barron, Mark J. Schervish, and Larry Wasserman. The consistency of posterior distributions in nonparametric problems. The Annals of Statistics, 27(2):536-561, 1999.

Andrew R. Barron and Thomas M. Cover. Minimum complexity density estimation. IEEE Transactions on Information Theory, 37(4):1034-1054, 1991.

Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory and Related Fields, 135(3):311-334, 2006.

Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexities. The Annals of Statistics, 33(4):1497-1537, 2005.

Anirban Bhattacharya, Debdeep Pati, Yun Yang, et al. Bayesian fractional posteriors. The Annals of Statistics, 47(1):39-66, 2019.

Peter J. Bickel and Bas J.K. Kleijn. The semiparametric Bernstein-von Mises theorem. The Annals of Statistics, 40(1):206-237, 2012.

Lucien Birgé. Model selection for Gaussian regression with random design. Bernoulli, 10(6): $1039-1051,2004$.

Lucien Birgé and Pascal Massart. Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli, 4(3):329-375, 1998.

Pier Giovanni Bissiri, Chris C. Holmes, and Stephen G. Walker. A general framework for updating belief distributions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.

Olivier Catoni. A PAC-Bayesian approach to adaptive classification. Technical report, Laboratoire de Probabilités et Modèles Aléatoires, Universités Paris 6 and Paris 7, 2003.

Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. Annales de l'Institut Henri Poincaré, Probabilités et Statistiques, 48(4):1148-1185, 2012.

Nicòlo Cesa-Bianchi and Gábor Lugosi. Prediction, Learning and Games. Cambridge University Press, Cambridge, UK, 2006.

Corinna Cortes, Spencer Greenberg, and Mehryar Mohri. Relative deviation learning bounds and generalization with unbounded loss functions. Ann. Math. Artif. Intell., 85(1):45-70, 2019.

Steven de Rooij, Tim van Erven, Peter D. Grünwald, and Wouter M. Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research, 15:1281-1316, 2014.

Vu C. Dinh, Lam S. Ho, Binh Nguyen, and Duy Nguyen. Fast learning rates with heavy-tailed losses. In Advances in Neural Information Processing Systems 29, pages 505-513. Curran Associates, Inc., 2016.

Richard M. Dudley. Real analysis and probability, volume 74. Cambridge University Press, 2002.

Subhashis Ghosal and Aad W. Van Der Vaart. Convergence rates of posterior distributions for noniid observations. The Annals of Statistics, 35(1):192-223, 2007.

Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior distributions. The Annals of Statistics, 28(2):500-531, 2000.

Subhashis Ghosal, Jüri Lember, and Aad W. van der Vaart. Nonparametric Bayesian model selection and averaging. Electronic Journal of Statistics, 2:63-89, 2008.

Peter D. Grünwald. Viewing all models as probabilistic. In Proceedings of the twelfth annual conference on Computational learning theory, pages 171-182. ACM, 1999.

Peter D. Grünwald. The Minimum Description Length Principle. MIT Press, Cambridge, MA, 2007.

Peter D. Grünwald. Safe learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity. In COLT, pages 397-420, 2011.

Peter D. Grünwald. The safe Bayesian: learning the learning rate via the mixability gap. In Proceedings 23rd International Conference on Algorithmic Learning Theory (ALT '12). Springer, 2012.

Peter D. Grünwald and A. Philip Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. The Annals of Statistics, 32(4):1367-1433, 2004.

Peter D. Grünwald and Nishant A. Mehta. A tight excess risk bound via a unified PAC-BayesianRademacher-Shtarkov-MDL complexity. In Proceedings 30th Conference on Algorithmic Learning Theory (ALT'19), 2019.

Peter D. Grünwald and Thijs Van Ommen. Inconsistency of Bayesian inference for misspecified linear models, and a proposal for repairing it. Bayesian Analysis, 2017.

David Haussler and Manfred Opper. Mutual information, metric entropy and cumulative relative entropy risk. The Annals of Statistics, 25(6):2451-2492, 1997.

David Haussler, Michael Kearns, H. Sebastian Seung, and Naftali Tishby. Rigorous learning curve bounds from statistical mechanics. Machine Learning, 25(2-3):195-236, 1996.

R. De Heide, A. Kirichenko, P. Grünwald, and N. Mehta. Safe-Bayesian generalized linear regression. arXiv preprint arXiv:1910....., 2019.

Daniel J. Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. The Journal of Machine Learning Research, 17(1):543-582, 2016.

Anatoli Juditsky, Philippe Rigollet, and Alexandre B. Tsybakov. Learning by mirror averaging. The Annals of Statistics, 36(5):2183-2206, 2008.

Bas J.K. Kleijn and Aad W. van der Vaart. Misspecification in infinite-dimensional Bayesian statistics. The Annals of Statistics, 34(2):837-877, 2006.

Vladimir Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34(6):2593-2656, 2006.

Wouter M. Koolen, Peter Grünwald, and Tim van Erven. Combining adversarial guarantees and stochastic fast rates in online learning. In Advances in Neural Information Processing Systems, pages $4457-4465,2016$.

Solomon Kullback and Richard A. Leibler. On information and sufficiency. The Annals of Mathematical Statistics, 22(1):79-86, 1951.

Guillaume Lecué. Interplay between concentration, complexity and geometry in learning theory with applications to high dimensional data analysis. Habilitation à diriger des recherches, Université Paris-Est, 2011.

Guillaume Lecué and Philippe Rigollet. Optimal learning with $Q$-aggregation. The Annals of Statistics, 42(1):211-224, 2014.

Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. Efficient agnostic learning of neural networks with bounded fan-in. IEEE Transactions on Information Theory, 42(6):2118-2132, 1996.

Qiang (Jonathan) Li. Estimation of mixture models. PhD thesis, Yale University, 1999.

Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: localization through offset Rademacher complexity. In Proceedings of The 27th Conference on Learning Theory (COLT 2015), pages 1260-1285, 2015.

Gábor Lugosi and Shahar Mendelson. Regularization, sparse recovery, and median-of-means tournaments. Bernoulli, 25(3):2075-2106, 2019.

Ryan Martin, Raymond Mess, and Stephen G. Walker. Empirical Bayes posterior concentration in sparse high-dimensional linear models. Bernoulli, 23(3):1822-1847, 2017.

David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51(1):5-21, 2003.

Peter McCullagh and John Nelder. Generalized Linear Models. Chapman and Hall/CRC, Boca Raton, second edition, 1989.

R. Meir and T. Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine Learning Research, 4:839-860, 2003.

Shahar Mendelson. Learning without concentration. In Proceedings of The 27th Conference on Learning Theory, pages 25-39, 2014.

Shahar Mendelson. On aggregation for heavy-tailed classes. Probability Theory and Related Fields, $168(3-4): 641-674,2017 \mathrm{a}$.

Shahar Mendelson. Learning without concentration for general loss functions. Probability Theory and Related Fields, Jun 2017b.

Jeffrey W Miller and David B Dunson. Robust Bayesian inference via coarsening. Journal of the American Statistical Association, pages 1-13, 2018.

Arkadii Nemirovskii and David Borisovich Yudin. Problem complexity and method efficiency in optimization. Wiley-Interscience, 1983.

Jorma Rissanen. Stochastic Complexity in Statistical Inquiry. World Scientific, Hackensack, NJ, 1989 .

R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, 1970.

Igal Sason and Sergio Verdú. $f$-divergence inequalities. IEEE Transactions on Information Theory, 62(11):5973-6006, 2016.

Matthias W. Seeger, Sham M. Kakade, and Dean P. Foster. Information consistency of nonparametric Gaussian process methods. IEEE Transactions on Information Theory, 54(5):2376-2382, 2008.

Alexander B. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135-166, 2004.

Tim van Erven and Peter Harremoës. Rényi divergence and Kullback-Leibler divergence. IEEE Transactions on Information Theory, 60(7):3797-3820, 2014.

Tim van Erven, Peter D. Grünwald, Nishant A. Mehta, Mark D. Reid, and Robert C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793$1861,2015$.

Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., 1995.

Vladimir Vovk. Aggregating strategies. In Proceedings of the third annual workshop on Computational learning theory, pages 371-383. Morgan Kaufmann Publishers Inc., 1990.

Stephen Walker and Nils Lid Hjort. On Bayesian consistency. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(4):811-821, 2002.

Wing Hung Wong and Xiaotong Shen. Probability inequalities for likelihood ratios and convergence rates of sieve MLEs. The Annals of Statistics, 23(2):339-362, 1995.

Kenji Yamanishi. A decision-theoretic extension of stochastic complexity and its applications to learning. IEEE Transactions on Information Theory, 44(4):1424-1439, 1998.

Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of convergence. The Annals of Statistics, 27(5):1564-1599, 1999.

Yuhong Yang and Andrew R Barron. An asymptotic property of model selection criteria. IEEE Transactions on Information Theory, 44(1):95-116, 1998.

Tong Zhang. From $\varepsilon$-entropy to KL-entropy: Analysis of minimum information complexity density estimation. The Annals of Statistics, 34(5):2180-2210, $2006 \mathrm{a}$.

Tong Zhang. Information-theoretic upper and lower bounds for statistical estimation. IEEE Transactions on Information Theory, 52(4):1307-1321, 2006b.
