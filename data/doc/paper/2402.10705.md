# AutoSAT: Automatically Optimize SAT Solvers via Large Language Models 

Yiwen Sun<br>School of Data Science<br>Fudan University<br>Shanghai, China<br>ywsun22@m.fudan.edu.cn

Shiyu Huang

4paradigm Inc.

Beijing, China

huangsy1314@163.com

Xianyin Zhang<br>School of Data Science<br>Fudan University<br>Shanghai, China<br>xianyinzhang22@m.fudan.edu.cn

Shaowei Cai

State Key Laboratory of Computer Science Institute of Software

Chinese Academy of Sciences

Beijing, China

caisw@ios.ac.cn

Bingzhen Zhang<br>Department of Industrial Engineering<br>Tsinghua University<br>Beijing, China<br>zhangbz23@mails.tsinghua.edu.cn

## Ke Wei *

School of Data Science

Fudan University

Shanghai, China

kewei@fudan.edu.cn


#### Abstract

Heuristics are crucial in SAT solvers, but no heuristic rules are suitable for all SAT problems. Therefore, it is helpful to refine specific heuristics for specific problems. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on Large Language Models (LLMs) which is able to autonomously generate codes, conduct evaluation, and then utilize feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive enterprise and model training, and fosters a Multi-Agent-based collaborative process with fault tolerance to ensure robust heuristic optimization. We implement AutoSAT on a lightweight Conflict-Driven Clause Learning (CDCL) solver EasySAT (the volume of EasySAT is about onefiftieth of the State-of-the-Art hybrid solver Kissat) and extensive experiments on seven datasets demonstrate its superior performance. Out of the seven testing datasets, AutoSAT shows a superior performance to Kissat in two datasets and displays an overall similar performance in three datasets. Some heuristics generated by AutoSAT are even counter-intuitive but are very effective.


## 1 Introduction

The Boolean satisfiability problem (SAT, or B-SAT), which is the first proven NP-complete problem [12], has received a lot of attention due to its broad applications. A variety of algorithms have been developed for the SAT problem, including complete algorithms [29, 34, 27, 36], local search[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_f439562fb80ba2c06093g-02.jpg?height=742&width=1374&top_left_y=274&top_left_x=381)

Figure 1: Architecture of AutoSAT. AutoSAT employs the original solver code to instantaneously create an understanding and recommendations for heuristic functions, and subsequently engages in an iterative process of code generation and performance assessment by 3 agents named Advisor, Coder, and Evaluator. Upon completion, AutoSAT returns the optimal code along with the corresponding interpretations to users.

algorithms [18, 3], and hybrid algorithms [9, 32, 14]. Due to the inherent combinatorial nature of SAT, solving it usually requires massive computing resources, and heuristics often play an essential role in boosting performance. Moreover, due to the heterogeneous nature of various SAT problems, it is difficult to design heuristics that yield effective results for different problems from first principles.

Researchers usually design heuristics based on their intuition and expertise through a trial-anderror approach. This process of manually designing heuristics is not only laborious but sometimes inefficient. To address these challenges, the concept of Hyper Heuristics (HHs) [30] has received a lot of attention recently [44, 21]. The core of traditional HHs is to select the best-performing heuristics from a predefined set or to generate new heuristics through the combination of simpler heuristic components. HHs offer a higher level of generality in solving various optimization problems but are limited by the heuristic space predefined by human experts. Even though many efforts have been made to improve the performance of $\mathrm{HHs}$ using machine learning [45, 21], these methods are still limited by the heuristic space, and cannot be directly used in current SAT solvers.

The rise of Large Language Models (LLMs) [28, 1, 23] opens up new possibilities for heuristics design [40]. The domain of code generation and improvement through LLMs is rapidly evolving [22]. AutoGPT and Flows [41, 19] make an early effort in employing LLMs to automatically write codes. AutoGen, Camel-AI, Agentverse and XAgent [20, 39, 10, 37] also leverage autonomous agents to perform coding tasks. EUREKA [25] harnesses LLMs to construct reward functions, demonstrating the feasibility of integrating LLM-driven insights into more sophisticated algorithmic structures. FunSearch [33] tackles the knapsack problem by assembling a function pool through the standardization of algorithms from various programming languages. ReEvo and $\mathrm{EoH}$ [42, 24] revise heuristics through LLMs for several problems with small-scale solvers, including online bin packing problems, traveling salesman problems, and flow shop scheduling problems. Even though these works have laid an important ground in code optimization, due to the required accuracy and complexity of the SAT problems, none of them can be directly applied to optimize the SAT heuristics.

Since existing methods cannot be directly implemented to improve heuristics in SAT solvers, we attempt to create a useful multi-agent-based heuristics generation framework for this task due to the fundamental role of SAT in many industrial applications. In particular, AutoSAT is presented to
automatically optimize heuristics in SAT solvers through the integration of metric-aware pretrained LLMs. The pictorial illustration of AutoSAT is provided in Figure 1. The main contributions of this paper are as follows:

Superior empirical performance of AutoSAT: AutoSAT has been tested on optimizing heuristics of a representative Conflict-Driven Clause Learning (CDCL) solver known as EasySAT ${ }^{2}$. Extensive empirical comparisons with original EasySAT, as well as another two representative SAT solvers (MiniSAT [36] and Kissat[14]) over seven complex datasets clearly show superior performance of AutoSAT in improving heuristics in SAT solver. Out of the seven testing datasets, AutoSAT (with only about one-fiftieth of the Kissat solver code) surpasses the State-of-the-Art (SOTA) hybrid solver Kissat in two datasets and displays an overall similar performance in three datasets. Some heuristics generated by AutoSAT are even counter-intuitive but are very effective.

Multi-Agent-Based mechanism with zero-shot learning: AutoSAT introduces a multi-agent-based mechanism to realize Chain of Thought (CoT) [38] to enhance heuristic optimization. Here all agents are LLMs driven with different prompts and they do not require training. Within this framework, exploration and fault tolerance are implemented for agents. The CoT is achieved by an automated feedback loop that splits the process of coding. In this loop, the original heuristics are analyzed and the potential direction for modifications is pointed out. Subsequently, new codes are generated, with a balance between diverse modification directions and codes from previous iterations. Once the new codes are evaluated, further opportunity for correction is provided for any unsuccessful attempts, reinforcing the fault tolerance aspect. Finally, the revised codes, along with their comprehensive analyses and assessments, are passed to the next feedback loop. This workflow requires no human intervention and allows for a more efficient and dynamic manner to enhance the capability of the SAT solver.

## 2 Preliminaries on SAT

In a nutshell, SAT is about finding a satisfying assignment for a given formula in the conjunctive normal form (CNF). An example of CNF is:

$$
(\neg A \vee \neg C) \wedge(B \vee C)
$$

where $A, B, C$ are Boolean variables, $\neg A, \neg C, B$, and $C$ are literals (True or False of a variable), and $(\neg A \vee \neg C)$ and $(B \vee C)$ are clauses. It is evident that satisfying assignments for this formula include

$$
A=\text { False, } B=\text { True or False, } C=\text { True }
$$

or

$$
A=\text { True or False, } B=\text { True, } C=\text { False. }
$$

If a clause has all but one of its literals or variables evaluated as False, then the free literal must be True in order for the clause to be True. For example, if letting $A=$ False and $B=$ False in $(A \vee B \vee C)$, one must have $C=$ True in order for it to be true. This is well-known as the unit clause rule. The process of applying the unit clause rule iteratively is referred to as Boolean constraint propagation (BCP).

Conflict-Driven Clause Learning (CDCL) [35, 27, 6] is a complete algorithmic framework which are widely adopted in modern SAT solvers. The framework consists of the following ingredients:

1. Select a variable and assign True or False, then apply BCP;
2. Build the implication graph;
3. If there is any conflict:
- Find the cut in the implication graph that led to the conflict;
- Derive a new clause which is the negation of the assignments that led to the conflict;
- Non-chronologically backtrack (back jump) to the appropriate decision level, where the first-assigned variable involved in the conflict was assigned;[^1]

Otherwise, repeat step 1 until all variable values are assigned.

Variable State Independent Decaying Sum (VSIDS) [7] refers to a family of branching heuristics that rank all variables of a Boolean formula during the run of a solver when utilizing the CDCL framework [34]. In this paper, we focus on three important heuristics: bump variables function, restart mechanism, and rephase function, which are detailed below.

Bump variables function. The bump variables function is used to increase the activity score of a particular variable within the SAT solver. The solver assigns a score, called activity, to each variable in the Boolean formula. The activity scores of all variables are typically initialized to 0 . In each iteration, the solver selects the variable with the highest activity to branch on. When the solver learns a clause, a set of variables is chosen, and their activities will be added by a positive value (typically 1). This is usually referred to as the (additive) bump. The activities of all variables are usually shrunk by multiplying a small constant after a fixed number of iterations.

Restart mechanism. Restart is an important component in SAT solvers. It is required especially when there are heavy conflicts, which imply current variable assignments are potentially not on the correct path, and thus need to be fully reassigned. When restart occurs, solver parameters and variable value structures are reinitialized in order to force the search into somewhere else in the search space without discarding learned clauses.

Rephase function. The purpose of the rephase function is to adjust the phasing strategy of an SAT solver in order to diversify the solver's strategy for choosing variable phases. When the solver gets stuck in a phase, or the performance becomes poor, reversing the phase enables the solver to explore new parts of the solution space.

## 3 AutoSAT

A multi-agent-based LLM framework, called AutoSAT, is proposed in this paper for automatic heuristic optimization in SAT solvers. The overall goal of AutoSAT is to search for effective heuristics automatically in an SAT solver. This can be roughly formalized as a Heuristics Generation Process (HGP), represented as a tuple $M=\langle S, H, \pi, F\rangle$, where:

- $S$ represents the state space including solver code and prompt;
- $H$ denotes the space of heuristic functions;
- $\pi: S \rightarrow H$ is a policy mapping state to heuristic functions;
- $F$ is the evaluation metric assessing policies, accessible only through evaluation. It can be regarded as a reward function in the classic Markov Decision Process (MDP).

The goal in HGP is to find a policy $\pi$ to achieve a high $F(\pi)$ (small PAR2 and more solved numbers of a solver). In our setting, all HGP components are represented as texts (including codes). The overall description of AutoSAT is presented in Algorithm 1 , with details given below.

### 3.1 Architecture

AutoSAT implements a CoT mechanism with exploration based on three different collaborative agents: Advisor, Coder, and Evaluator. The functions of these three agents are depicted as Fig1

In addition, to mitigate the "hallucination" issues of LLMs [43], an automated feedback loop is employed, in which the code provided by LLM agents is measured in each loop, providing automatic feedback to help the agents analyze and regenerate heuristics in the next loop based on the experimental results. The performance of these heuristic codes determines the feedback for optimization. Furthermore, to accurately provide instructions and write codes for the heuristic function design, it is essential to ensure that all agents share the same context. Therefore, the raw code of the SAT solver is provided to the LLM agents if the token limit of the LLM is not exceeded. In this paper, we feed the key part of our baseline SAT solver code as context, ensuring all agents know what the solver semantically entails and which variables can and should be used to compose a heuristic function for the specified task. The complete description can be found in Algorithm 1. We set the iteration number as 4 and the batch size as 6 in our experiments.

### 3.2 Component Details

Next, we provide more details on each component to clarify our design motivation.

Advisor. The Advisor reviews all solver code and then provides a concise description for a specific part that requires improvement (e.g. explanation for its intuition and application as well as comments for key variables), as well as advice for further modification to guide Coder.

Coder. Both the advice and feedback are passed to Coder, which is responsible for modifying the actual heuristic code within the SAT solver. Codes are generated concurrently with different modification directions from Advisor and previous results in order to explore various optimization avenues.

Evaluator. There are two issues that may arise here: (1) writing incorrect codes; and (2) writing identical codes. Here we use a fault tolerance mechanism to detect erroneous code to ensure that the main program runs smoothly. After the codes are generated, the Evaluator will debug them and categorize each as "Substantial Improvement", "Parameter Tuning" and "No Modification". Invalid codes such as those containing only variable name changes or errors, will be sent back to Coder for revision. Subsequently, valid codes, with a detailed description from Evaluator, will be executed in the running environment and their evaluation results will be collected together for the next iteration.

### 3.3 Prompt Engineering

The success of AutoSAT relies heavily on the prompt engineering for LLM agents. The templates for the three different agents of AutoSAT are presented in Appendix C. Brief interpretations are provided below.

1. Define the Role of an agent as a solver expert who needs to assess and improve the heuristics in an SAT solver.
2. Clearly state the Goal, such as providing optimization suggestions, writing code, or feedback.
3. Enhance the agents' capabilities by inserting optional Tips that guide them to avoid common mistakes during code generation. Additionally, through this flexible interface, agents can effectively utilize external codes and results and can be instructed to specify the types of modification directions such as changing parameters, modifying heuristics, or adding new heuristics.
4. Total SAT solver code is appended at the end of each prompt to ensure all agents are in the same context.

Maintaining consistency in the prompt design ensures that agents operate in the same context, thereby enabling their outputs to remain coherent. This consistency provides the possibility for the iterative refinement of heuristics in SAT solvers.

It is worth mentioning that, as an automatic optimization framework that can potentially be applied to heuristic improvement in any solver, AutoSAT is not very sensitive to the exact wording of prompts. Indeed, many prompts are automatically produced from the previous responses of the agents. More details on the prompt configurations in AutoSAT can be found in Appendix C.1.

## 4 Experiments and Results

In the experiments, AutoSAT will automatically optimize the three heuristics mentioned in Section 2 as well as their combinations. As an illustration, we show in Section 4.3 a revised heuristics. It can be observed that AutoSAT can change the parameters of original heuristics and add new heuristic rules too. Moreover, generated heuristics by AutoSAT can transcend the boundaries of human thought, providing heuristics of significant efficacy even if they are counter-intuitive. LLMs agents in AutoSAT also provide the reason for the modifications, more details can be found in Appendix C.2.

In the evaluation of AutoSAT within the framework of a CDCL solver, a rigorously defined evaluation benchmark is devised to validate the efficacy of the heuristics. The AutoSAT framework is also tested

![](https://cdn.mathpix.com/cropped/2024_06_04_f439562fb80ba2c06093g-06.jpg?height=735&width=1268&top_left_y=267&top_left_x=450)

Figure 2: Iteration Results during searching: PAR-2 (sum over 32 Zamkeller instances). We have highlighted the modification directions in one iteration of the bump variable searching stage, as well as the diverse heuristic codes generated in one iteration during the search for restart conditions. The 'Star' indicates the direction or code that achieved the best results in that iteration.

across different LLMs including GPT-4 [1], Qwen-72B-Chat [2], and Deepseek-Coder-33B [15] 33 Details can be found in Section 4.4 .

Our experiments are conducted on twelve servers equipped with Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz processors. Another server with two AMD EPYC 7763 64-core Processors and eight NVIDIA H100 GPUs is used for inferring LLMs. In all our experiments, we consistently utilized a random seed of 42. The experiments were conducted on CPUs of the same models to ensure uniformity in execution environments. Results were averaged to mitigate any minor variations.

### 4.1 Experiment Settings

Backbone Solver. To maintain a balance between lightweight design and test efficiency, we focus on the optimization of a SAT solver EasySAT written in C++. It is a CDCL solver with a clean code style that implements VSIDS [7] and is capable of reaching medium-standing performance levels in SAT competitions, which are presented in Table 5 More details about EasySAT are provided in Appendix A.1. We utilized EasySAT over the more well-known CDCL solver MiniSAT[36] and Kissat[14], due to the token restrictions imposed by LLMs. Modifying MiniSAT or Kissat would result in an excessive expenditure of tokens, as well as increasing the difficulty for LLMs to understand and modify SAT solvers. By utilizing EasySAT, we maintain approximately 6,000 tokens per prompt, which greatly alleviates these concerns. We explore adaptations of heuristics suitable for the characteristics of each dataset based on the foundational heuristic component of our backbone. Combinations of different heuristic components are also considered.

Compared Methods. We first compare AutoSAT with the original EasySAT, MiniSAT and Kissat. Kissat is a "keep it simple and clean bare metal SAT solver" written in C [13]. It is a port of $\mathrm{CaDiCaL}$ [32] with improved data structures, better scheduling of CDCL solver, and a local search algorithm. We have compared it with the newest version of Kissat after SAT Competition 2023 [14] which incorporates more powerful heuristics such as Kissat-MAB[11], initial local search through propagation [8] and actual watch replacement of true literals [26].[^2]

Table 1: Testing results of PAR-2 over different dataset (lower is better).

|  | CNP | SCPC | PRP | CoinsGrid | LangFord | KnightTour | Zamkeller |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| EasySAT | 882.96 | 3000 | 2935.95 | 2415.92 | 2756.88 | 2258.58 | 1093.08 |
| AutoSAT | 646.78 | 2221.24 | 1704.10 | 2119.65 | 2511.33 | 1254.58 | 761.12 |
| random_search | 711.84 | 3000 | 2779.29 | 2264.63 | 2691.13 | 1624.38 | 969.92 |
| bayes_search | 789.63 | 3000 | 2762.79 | 2394.29 | 2639.17 | 1652.25 | 961.10 |
| MiniSAT | 1495.34 | 1935.73 | 2770.47 | 2696.88 | 2955.33 | 1973.21 | 2566.31 |
| Kissat | 521.20 | 93.45 | 1677.13 | 2058.62 | 1799.33 | 2011.00 | 806.31 |

Table 2: Testing results of solved number over different dataset (more is better).

|  | CNP | SCPC | PRP | CoinsGrid | LangFord | KnightTour | Zamkeller |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| EasySAT | 39 | 0 | 4 | 12 | 5 | 6 | 33 |
| AutoSAT | 41 | 11 | 74 | 16 | 9 | 14 | 45 |
| random_search | 41 | 0 | 12 | 14 | 7 | 12 | 34 |
| bayes_search | 39 | 0 | 13 | 12 | 8 | 12 | 34 |
| MiniSAT | 26 | 13 | 12 | 6 | 1 | 9 | 7 |
| Kissat | 42 | 33 | 78 | 17 | 22 | 8 | 36 |

We also compare AutoSAT with parameter tuning methods such as random search and Bayesian optimization("Bayesian search" in our experiments). 4 In the Bayesian search, we use Upper Confidence Bound (UCB) as an acquisition function and the Gaussian Process as a surrogate function. All the experiments follow the same setting (e.g. iterations, batch size,...) with AutoSAT, even the Bayesian search method requires 3 more initial points.

Dataset. We selected seven datasets to explore the capabilities of AutoSAT. Three of them were chosen from SAT Competition [17, 16, 4], including the Chromatic Number of the Plane (CNP), the Setting Covering problem with Conflict (SCPC), and the Profitable Robust Production (PRP). We then extended CNP and PRP by constructing additional 30 CNP problems from the open-source dataset and generating extra 160 instances by Picar5, the same way as mentioned in [5]. More details are shown in Appendix B.

The other datasets were also efficiently generated by Picat, a CSP (Constrained Satisfied Problem) solver that accepts CSP problems and converts them into CNF formulas. We consult Zhou's work[46], specifically, all scalable CSP mentioned in Chapter 2,3, to construct our dataset such as CoinsGrid, LangFord, KnightTour, Zamkeller (N-Queens is excluded as it is a subset of CNP). Most of the instances are more than $50 \mathrm{k}$ variables and $420 \mathrm{k}$ clauses. Details of construction can be found in Appendix B.

Evaluation metric. During the heuristics generating process, we utilize total solving time as the main evaluation metric, which facilitates LLMs in identifying directions for problem-solving. With a focus on tackling problems of low to moderate complexities and also being aware of the computational resource constraints, timeout thresholds of 1500 seconds have been used for each round of searching.

Given the iterative nature of our searching process, the total runtime of the codes produced by AutoSAT in each iteration across different LLMs is reported in Table 2. It can be observed that AutoSAT can find heuristics that outperform EasySAT during the searching process.[^3]

During testing, we utilize two specific metrics, aligned with the standards of the SAT competition, to assess the effectiveness and resilience of our heuristic codes. The metrics comprise: (1) The number of SAT instances solved within the timeout; (2) The Penalized Average Runtime with a factor of 2 score (PAR-2).

PAR-2 determines the average resolution time and imposes penalties for timeout by assigning them a value twice the timeout threshold, and a detailed explanation of the PAR-2 metric is provided in Appendix A.2. The timeout threshold in the testing stage is set to 1500 seconds. In the SAT Competition, the timeout threshold is set at 5000 seconds. Our comparative analysis Table 5 revealed consistent performance disparities among EasySAT, MiniSAT, and KisSAT at both 1500-second and 5000 -second timeouts. Considering the computational efficiency, we decided to set the timeout threshold as 1500 seconds.

### 4.2 Results

In this section, the testing results are presented and analyzed to illustrate the performance of AutoSAT. Owing to the superior performance of AutoSAT on GPT-46, we only report results from it. The numerical results are reported in Table 1 and Table 2 (with a 1500-second timeout threshold), from which the following observations can be made.

AutoSAT outperforms EasySAT, MiniSAT, and parameter tuning methods in all datasets. we present a comprehensive evaluation of the heuristic codes generated by AutoSAT on seven datasets. It is evident that AutoSAT is superior to EasySAT, MiniSAT, and other parameter tuning methods in both solved number and PAR-2.

AutoSAT achieves close performance with SOTA Kissat solver. Kissat has achieved SOTA in the latest SAT Competition 2023, which also suggests the limits of current solvers. Thus, expectations for AutoSAT to optimize CDCL solvers and outperform the current SOTA solvers may be deemed unrealistic. However, AutoSAT has demonstrated the ability to enhance a CDCL solver with minimal codebase, to achieve results that are nearly on par with or even surpasses the SOTA on various categorized datasets.

### 4.3 Generated Heuristics Analysis

The key to AutoSAT's capacity to boost solver performance lies in its occasional generation of counter-intuitive code via LLMs. For instance, in the PRP dataset, AutoSAT wrote the 'bump_var' function in a non-standard manner, as shown below.

```
# Original bump_var function:
void Solver::bump_var(int var, double coeff) {
    if ((activity[var] += var_inc * coeff) > 1e100) {
        for (int i = 1; i <= vars; i++) { activity[i] *= 1e-100;
        var_inc *= 1e-100;}
}
    if (vsids.inHeap(var)) vsids.update(var);
# Improved bump_var function:
void Solver::bump_var(int var, double coeff) {
    activity[var] += var_inc * coeff;
    if (activity[var] > 1e100) {
        double rescale_factor = 1e-100;
        int active_vars = 0;
        for (int i = 1; i <= vars; i++) {
            if (activity[i] >= 1e90) {
                activity[i] *= rescale_factor;
            }
                active_vars++;
            if (active_vars > 10) var_inc *= rescale_factor;
        }
        if (var_inc < 1e-10) var_inc = 1e-10;
        else if (var_inc > 1e10) var_inc = 1e10;
        if (vsids.inHeap(var)) vsids.update(var);
}
```

[^4]Typically, the approach would be to assign higher activity to variables with more conflicts, thus increasing their likelihood of being reassigned. In contrast, AutoSAT opted to reset the variable with the highest conflict, and instead increase the activity of the variable with the second-highest conflict. This unconventional modification is counter-intuitive, yet it produced astonishing results, underscoring the significant potential of AutoSAT. Perhaps counter-intuitive strategies might be the key to surpassing even the most proficient experts.

Although the "more conflict, more activity" principle serves as the core mechanism of the VSIDS[34], it remains a heuristic without theoretically proven. Due to its empirical effectiveness across a variety of datasets, researchers often utilize such heuristics in all areas. In contrast, LLMs have the capability to "Transcend the Boundaries of Human Thought". By leveraging the feedback from experimental results, AutoSAT generate tailored heuristics for diverse instance structures, including those that might initially seem counter-intuitive. Such an approach often yields surprisingly effective results across different datasets.

### 4.4 Ablation Study

An ablation study is carried out to provide more evidence of the contribution of each part in AutoSAT. Unless otherwise specified, all experiments are conducted with a maximum of 4 iterations, a batch size of 4 (i.e., 4 coders with different directions run in parallel each iteration), timeout of 1500 seconds. The reported results are collected after three independent simulations. Dataset PRP is selected due to its large number of instances.

Firstly, we evaluate various combinations of agents, including configurations such as Coder-Only, Coder-Evaluator, Advisor-Coder, and Advisor-Coder-Evaluator. All experiments in Table 3 are conducted on GPT-4o, a powerful and responsive LLMs. More importantly, these new independent experiments are not

Table 3: Comparison of different agents combination.

| Agents-Type | Solved | PAR-2 Score |
| :--- | :---: | ---: |
| AutoSAT-3-stage | 64 | 1909.48 |
| Coder-only | 9 | 2855.35 |
| Coder-Evaluator | 16 | 2616.91 |
| Advisor-Coder | 61 | 1937.48 |

influenced by previous results. In EasySAT, the most flexible heuristic component-the bump variable function-has been selected for modification.

We observed that in the absence of the Advisor, it tends to either make no changes or only minor adjustments, such as parameter tuning, due to a lack of guidance. Besides, the Evaluator enhances fault tolerance and analyzes the code generated in the current iteration, providing additional insights across the iterations for the Coder.

We also evaluate various LLMs and different heuristic components within EasySAT. The outcomes that surpass the baseline are presented in Table 4. AutoSAT can generate better heuristics with diverse LLMs and GPT-4 can generate heuristics with greater stability. However, small models like Deepseek-33B, which excels in coding, also possess the capability to discover effective heuristics.

Table 4: Comparison of different LLMs and Heuristic Components.

|  | Solved | PAR-2 Score |
| :--- | :---: | ---: |
| EasySAT(baseline) | 4 | 2916.44 |
| AutoSAT_gpt4_restart | 16 | 2703.90 |
| AutoSAT_gpt4_rephase | 14 | 2749.16 |
| AutoSAT_gpt4_bump | 66 | 1848.85 |
| AutoSAT_Qwen-72B_bump | 21 | 2616.91 |
| AutoSAT_deepseek-33B_restart | 11 | 2789.88 |
| AutoSAT_deepseek-33B_bump | 67 | 1817.46 |

## 5 Conclusion and Discussion

In this work, we introduce AutoSAT, an innovative framework that utilizes LLMs for the automatic heuristic optimization in SAT solvers. AutoSAT realizes the chain of thought processes for autonomous feedback generation. This procedure not only reduces human efforts in the search for effective heuristics but also improves the efficiency and adaptability of the heuristics. The incorporation of an automated feedback loop successfully mitigates the hallucination issue associated with LLMs. The philosophy behind AutoSAT is indeed simple: When it is not clear how to improve the performance of a solver using handcrafted rules, generation through LLMs might be able to provide useful results and hints.

There are several lines of research for future work. Currently, the token size of LLMs and their capacity to comprehend extended texts restrict the possibilities to optimize solvers such as MiniSAT or Kissat. It is interesting to explore the scaling of AutoSAT to improve other SAT solvers, with the continuous evolution of LLMs. In addition, the success of AutoSAT suggests that similar mechanisms might be developed to improve the performance of other logical reasoning algorithms for different computational problems. Lastly, we would like to investigate the fundamental ideas on how and why LLMs modify the heuristics and lay mathematical foundations for this process.

Lastly, we do not believe our work has broader negative societal impacts, as we focus on developing new algorithms for enhancing heuristics in SAT solvers through LLMs.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

[3] Adrian Balint and Andreas Fröhlich. Improving stochastic local search for sat with a new probability distribution. In Theory and Applications of Satisfiability Testing-SAT 2010: 13th International Conference, SAT 2010, Edinburgh, UK, July 11-14, 2010. Proceedings 13, pages 10-15. Springer, 2010.

[4] Tomás Balyo, Marijn JH Heule, Markus Iser, Matti Järvisalo, and Martin Suda. Proceedings of sat competition 2022: Solver and benchmark descriptions. 2022.

[5] Tomás Balyo, Marijn JH Heule, Markus Iser, Matti Järvisalo, and Martin Suda. Proceedings of sat competition 2023: Solver and benchmark descriptions. 2023.

[6] Roberto J Bayardo Jr and Robert Schrag. Using csp look-back techniques to solve real-world sat instances. In Aaai/iaai, pages 203-208. Citeseer, 1997.

[7] Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh. Conflict-driven clause learning sat solvers. Handbook of Satisfiability, Frontiers in Artificial Intelligence and Applications, pages 131-153, 2009.

[8] Shaowei Cai, Chuan Luo, Xindi Zhang, and Jian Zhang. Improving local search for structured sat formulas via unit propagation based construct and cut initialization (short paper). In 27th International Conference on Principles and Practice of Constraint Programming (CP 2021). Schloss-Dagstuhl-Leibniz Zentrum für Informatik, 2021.

[9] Shaowei Cai, Xindi Zhang, Mathias Fleury, and Armin Biere. Better decision heuristics in cdcl through local search and target phases. Journal of Artificial Intelligence Research, 74:1515-1563, 2022.

[10] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2023.

[11] Mohamed Sami Cherif, Djamal Habet, and Cyril Terrioux. Kissat mab: Upper confidence bound strategies to combine vsids and chb. SAT COMPETITION 2022, page 14, 2022.

[12] Stephen A Cook. The complexity of theorem-proving procedures. Proceedings of the Third Annual ACM Symposium on Theory of Computing, pages 151-158, 1971.

[13] ABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. SAT COMPETITION, 2020:50, 2020.

[14] Armin Biere Mathias Fleury. Gimsatul, isasat, kissat. SAT COMPETITION 2022, page 10, 2022.

[15] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.

[16] Marijn JH Heule, Matti Järvisalo, and Martin Suda. Benchmark selection of sat race 2019. SAT RACE 2019, page 46 .

[17] Marijn JH Heule, Matti Järvisalo, and Martin Suda. Sat competition 2018. Journal on Satisfiability, Boolean Modeling and Computation, 11(1):133-154, 2019.

[18] Holger H Hoos and Thomas Stützle. Local search algorithms for sat: An empirical evaluation. Journal of Automated Reasoning, 24(4):421-481, 2000.

[19] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, and Robert West. Flows: Building blocks of reasoning and collaborating ai. arXiv preprint arXiv:2308.01285, 2023.

[20] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[21] Nan Li, Lianbo Ma, Guo Yu, Bing Xue, Mengjie Zhang, and Yaochu Jin. Survey on evolutionary deep learning: Principles, algorithms, applications, and open issues. ACM Computing Surveys, 56(2):1-34, 2023.

[22] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.

[23] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. arXiv preprint arXiv:2305.17390, 2023.

[24] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model.

[25] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023.

[26] Norbert Manthey. Cadical modification-watch sat. SAT COMPETITION 2021, page 28, 2021.

[27] Joao P Marques-Silva and Karem A Sakallah. Grasp: A search algorithm for propositional satisfiability. IEEE Transactions on Computers, 48(5):506-521, 1999.

[28] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023.

[29] Robert Nieuwenhuis, Albert Oliveras, and Cesare Tinelli. Solving sat and sat modulo theories: From an abstract davis-putnam-logemann-loveland procedure to dpll (t). Journal of the ACM (JACM), 53(6):937-977, 2006.

[30] Nelishia Pillay and Rong Qu. Hyper-heuristics: theory and applications. Springer, 2018.

[31] CJ Priday and Roy O Davies. On langford's problem. The Mathematical Gazette, pages 250-255, 1959 .

[32] SEPARATE DECISION QUEUE. Cadical at the sat race 2019. SAT RACE 2019, page 8, 2019.

[33] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, pages 1-3, 2023.

[34] Lawrence Ryan. Efficient algorithms for clause-learning sat solvers. 2004.

[35] JP Marques Silva and Karem A Sakallah. Grasp-a new search algorithm for satisfiability. In Proceedings of International Conference on Computer Aided Design, pages 220-227. IEEE, 1996.

[36] Niklas Sörensson. Minisat 2.2 and minisat++ 1.1. A short description in SAT Race, 2010, 2010.

[37] XAgent Team. Xagent: An autonomous agent for complex task solving, 2023.

[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

[39] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.

[40] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.

[41] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.

[42] Haoran Ye, Jiarui Wang, Zhiguang Cao, and Guojie Song. Reevo: Large language models as hyper-heuristics with reflective evolution. arXiv preprint arXiv:2402.01145, 2024.

[43] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive mirage: A review of hallucinations in large language models. arXiv preprint arXiv:2309.06794, 2023.

[44] Fangfang Zhang, Yi Mei, Su Nguyen, and Mengjie Zhang. Survey on genetic programming and machine learning techniques for heuristic design in job shop scheduling. IEEE Transactions on Evolutionary Computation, 2023.

[45] Zi-Qi Zhang, Fang-Chun Wu, Bin Qian, Rong Hu, Ling Wang, and Huai-Ping Jin. A q-learningbased hyper-heuristic evolutionary algorithm for the distributed flexible job-shop scheduling problem with crane transportation. Expert Systems with Applications, 234:121050, 2023.

[46] Neng-Fa Zhou, Håkan Kjellerstrand, and Jonathan Fruhman. Constraint solving and planning with Picat, volume 11. Springer, 2015.
