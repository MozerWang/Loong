# RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness 

Tianyu Yu $^{1} \quad$ Haoye Zhang ${ }^{1} \quad$ Yuan Yao $^{2 *} \quad$ Yunkai Dang $^{1} \quad$ Da Chen $^{1} \quad$ Xiaoman Lu $^{1}$<br>Ganqu Cui $^{1} \quad$ Taiwen He $^{1} \quad$ Zhiyuan Liu $^{1 *} \quad$ Tat-Seng Chua ${ }^{2} \quad$ Maosong Sun $^{1}$<br>${ }^{1}$ Department of Computer Science and Technology, Tsinghua University<br>${ }^{2} \mathrm{NExT}++$ Lab, School of Computing, National University of Singapore<br>yiranytianyu@gmail.com yaoyuanthu@gmail.com<br>https://github.com/RLHF-V/RLAIF-V


#### Abstract

Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences. While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention. However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues. Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap. As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness. RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm. In order to better expose the difference in trustworthiness, we propose a novel deconfounded candidate response generation strategy. To enhance the accuracy of pairwise feedback data from open-source MLLMs, we employ a divide-and-conquer approach that breaks down the response into atomic claims, simplifying the task to obtain more reliable results. For the feedback learning algorithm, RLAIF-V mitigates the distribution shift problem of vanilla direct preference optimization in an online learning fashion, improving both the learning performance and efficiency. Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks. Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by $82.9 \%$ and overall hallucination by $42.1 \%$, outperforming the labeler model. Remarkably, RLAIF-V also reveals the selfalignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than $29.5 \%$ overall hallucination rate, surpassing GPT-4V (45.9\%) by a large margin. The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs.


[^0]![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-02.jpg?height=601&width=485&top_left_y=285&top_left_x=427)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-02.jpg?height=591&width=762&top_left_y=282&top_left_x=931)

(b)

Figure 1: (a) This work aims to shift the current paradigm of aligning MLLMs with feedback from superior teachers, to align with feedback from peers exhibiting comparable or equal capabilities. (b) Trustworthiness of RLAIF-V compared to other methods. We assess the generative trustworthiness with human evaluation benchmark MHumanEval [58], and evaluate the discriminative trustworthiness with automatic evaluation benchmark AMBER [51].

## 1 Introduction

Recent advances in multimodal large language models (MLLMs) mark a significant milestone in AI research [4, 10, 31-33, 57]. These models are trained on large-scale multimodal corpora and possess profound world knowledge, showing remarkable capabilities in tackling diverse multimodal tasks [25, 36, 42]. However, it has been commonly noticed that MLLMs are prone to confidently generating incorrect content that deviates from human preferences [49, 70, 58, 18]. In order to align MLLMs with human preferences, reinforcement learning from human feedback (RLHF) has been widely used and demonstrates substantial results [49, 58]. However, RLHF depends heavily on labor-intensive human annotations, and consequently is hard to cover the widespread misalignment between model and human preferences. Recently, reinforcement learning from AI feedback (RLAIF), which uses the preference collected from labeler models as a proxy of human preference, has shown promising potential as an alternative to RLHF [22].

Existing RLAIF methods, demonstrated at the top of Figure 1(a), rely on ultra-powerful proprietary models to distill feedback from [26, 67, 71, 69]. Such approaches present scalability issues due to dependence on expensive API access. More critically, this paradigm essentially distills the capability of proprietary models to provide a temporary solution to bridge the performance gap between opensource models and proprietary models. As this gap continues to shrink, the community is soon facing the challenge of aligning MLLMs using labeler models of comparable capability, demonstrated at the bottom of Figure 1 (a). However, simply changing the labeler model from a proprietary model to an open-source model leads to unsatisfactory feedback regarding both data efficiency and pairwise data accuracy [6]. Moreover, even supplied with high-quality feedback, existing training methods are prone to saturate and cannot fully utilize the data [49, 1] due to the distribution shift problem, where the preference data is static during training while model output distribution is constantly shifting.

To address the challenges, we propose the RLAIF-V framework, which aligns MLLMs using opensource feedback, achieving more trustworthy behavior compared with the labeler and even GPT4V [42] (Figure 1](b)). As shown in Figure 2, RLAIF-V leverages two key innovations to achieve this objective: (1) At the feedback quality level, we propose a novel deconfounded candidate response generation strategy for better data efficiency and a divide-and-conquer approach for higher pairwise data accuracy. The deconfounded strategy accurately exposes the genuine trustworthiness difference within response pairs by generating candidate responses from multiple sampling decoding trials under the same condition (i.e., same inputs and decoding parameters). Consequently, confounding factors such as the text style are eliminated, and the feedback is ensured to focus on the substantial content of responses. The divide-and-conquer approach decomposes the difficult response-evaluation task

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-03.jpg?height=528&width=1377&top_left_y=243&top_left_x=379)

Figure 2: Overview of the RLAIF-V framework. (1) Given the input image and prompt, multiple candidate responses are generated with the deconfounded strategy. (2) Each response is split into atomic claims and assigned with trustworthiness scores separately by an open-source MLLM. (3) During preference learning, the model is aligned under an iterative alignment framework which periodically updates the feedback.

into simpler claim-evaluation. We divide the full response into claims to acquire scores, and then combine them to get the final assessment. It substantially simplifies the task and thus obtains more accurate feedback from open-source MLLMs. (2) For the training method, we devise an iterative alignment framework to approximate online training and mitigate the distribution shift problem of widely used direct preference optimization (DPO) [44]. Specifically, the feedback is periodically refreshed based on the latest model weights to reduce the distribution deviation. At each iteration, we update the model using the fresh feedback with direct preference optimization.

Comprehensive experiments on seven benchmarks show that RLAIF-V can substantially enhance the trustworthiness of models without any human or proprietary model intervention. Using feedback from LLaVA-NeXT 34B [32] to train a LLaVA 1.5 7B [31], we significantly reduce the object hallucination rate of the base model on Object HalBench [58] by 81.0\%, surpassing the labeler model by a large margin. Pushing the limit to an extreme scenario where no stronger models are available, we align OmniLMM [43] with the model itself as the labeler. Experimental results show that RLAIF-V can reduce object hallucination by $71.0 \%$ relative points in Object HalBench and reduce overall hallucination by $41.8 \%$ relative points in MHumanEval, surpassing GPT-4V by a large margin and revealing the self-alignment potential of open-source MLLMs.

The contribution of this work can be summarized as fourfold: (1) We present RLAIF-V, a novel framework that aligns MLLMs with open-source feedback. (2) We propose a novel deconfounded response generation strategy and a divide-and-conquer response evaluation approach to improve the feedback data quality. (3) We employ an iterative alignment method to train the model, improving both the learning efficiency and performance. (4) We conduct comprehensive experiments to demonstrate the effectiveness of the proposed framework, achieving state-of-the-art performance in trustworthiness among both open-source and proprietary MLLMs. All codes, data, and model weights are released.

## 2 RLAIF-V

In this section, we first elaborate on how to collect high-quality AI feedback from open-source MLLMs by introducing the response collection and evaluation process. Then we introduce the iterative alignment approach to mitigate the distribution shift problem and thus improve the scalability. See Figure 2 for an overview of the RLAIF-V framework.

### 2.1 Response Generation

The feedback collected for preference learning is in the form of comparison pairs, where each pair includes a preferred response $y_{w}$ and an inferior response $y_{l}$ to the same input $x$ (including the image and prompt). During training, the model learns preferences by distinguishing the differences between $y_{w}$ and $y_{l}$. However, these differences can be complex and consist of many factors including not only
the meaning of content but also textual styles such as the use of specific words or structure of the text, making the learning more difficult.

To expose the genuine differences in trustworthiness between responses, we propose a novel deconfounded strategy to generate candidate responses. Specifically, we ask the model to generate $n$ candidate responses $\left\{y_{1}, y_{2}, \cdots, y_{n}\right\}$ through sampling decoding with different random seeds, where input $x$ and decoding parameters are invariant. In this way, $y_{w}$ and $y_{l}$ are sampled from the same distribution and consequently share similar textual styles and linguistic patterns. During training, the model can effectively concentrate on the differences in trustworthiness. In our experiments, we find the deconfounded strategy can significantly improve the learning efficiency (see Section 3.3).

### 2.2 Response Evaluation

Evaluating the quality of model responses is a challenging task even for human annotators due to the complexity of full responses. Existing methods using models as labelers rely on costly API of proprietary models with extraordinary instruction-following and task-solving capabilities [61], resulting in scalability issues. In contrast, we employ a divide-and-conquer approach to simplify the task to achieve more reliable results from open-source MLLMs. The detail of collecting high-quality feedback with this approach is described as follows:

Divide. The complexity of full responses makes the holistic assessment of response quality hard to acquire based on existing open-source MLLMs [6]. One of the important complexity causes is that a full response might contain multiple statements and specific textual structure which interferes with the recognition of incorrect spans. To make such a complicated task solvable, we decompose the response evaluation into atomic claim evaluation, as shown in Figure 2. Specifically, we prompt a large language model to split a response $y$ into atomic claims $\left\{c_{1}, c_{2}, \cdots, c_{m}\right\}$, which can be evaluated separately, by extracting facts excluding opinions and subjective statements.

Conquer. To access the trustworthiness quality of a claim $c$ (e.g., "The clock reads around 11:20."), we first convert it into a polar question like "Does the clock read around 11:20?", which can be answered with simply yes or no, without introducing any extra content. For each atomic polar question, we ask an open-source MLLM to generate the confidence of agreement and disagreement as the claim score $s_{c}=\left(p_{\text {yes }}, p_{n o}\right)$, where $p_{y e s}$ is the probability of answering with "Yes" or "yes" and $p_{n o}$ is the probability of answering with "No" or "no". A higher $p_{y e s}$ score suggests the corresponding claim is considered more trustworthy by the labeler model. The scores collected in this way are generally more accurate compared with directly querying the evaluation result of the full response since the claims are simpler in both structure and content.

Combine. After obtaining the quality assessment of each claim, we finally combine them into the score of the whole response. For each response, we denote the number of claims having $p_{\text {no }}>p_{\text {yes }}$ as $n_{r e j}$, measuring how many incorrect claims are recognized by the labeler model. We use $-n_{r e j}$ as the final score $S$ of the response, where a higher score indicates less incorrectness of the content. Given the score of each response, we can now construct a preference dataset for training. For each instruction $x$, we keep all response pairs $\left(y, y^{\prime}\right)$ such that $S>S^{\prime}$ and choose the higher score response $y$ as the preferred response. To save the training cost, we randomly sample at most 2 pairs for each instruction and we find such filtering process only causes minor performance drop. We also investigate the effectiveness of more combining methods (see Appendix C.1).

### 2.3 Iterative Alignment

DPO is widely used to align MLLMs with human preference. However, naive DPO faces the distribution shift problem, i.e., the preference data is static during training process while model output distribution is constantly shifting [14]. As a result, the data distribution might deviate from the expected feedback distribution and cause sub-optimal alignment results.

Mitigating the distribution shift problem requires updating the feedback data constantly during the training process. However, collecting new feedback data at each optimization step is costly and makes the training unstable. We tackle this problem by conducting the alignment process including both data collection and training in an iterative manner. The iterative alignment is presented in Algorithm 1 .

Specifically, we select $N$ multimodal instructions at the beginning of each iteration and leverage the deconfounded strategy to generate $n$ candidate responses for each instruction with the latest

```
Algorithm 1 Iterative alignment of RLAIF-V
Input: Initial instruction model $M_{1}$, labeler model $L$, instruction sets $X=\left\{X_{1}, X_{2}, \ldots, X_{k}\right\}$, each
    $X_{i}$ contains $N$ different multimodal instructions
Output: $M_{k}$
    for $i=1$ to $k$ do
        Select the instruction set $X_{i} \in X$ of current iteration
        $Y_{i} \leftarrow$ DeconfoundedGeneration $\left(M_{i}, X_{i}, n\right)$
        $\left\{S_{1,1}, \ldots, S_{N, n}\right\} \leftarrow$ EvaluateResponse $\left(L, Y_{i}\right)$
        $D_{i} \leftarrow$ ConstructPairs $\left(\left\{S_{1,1}, \ldots, S_{N, n}\right\}, X_{i}, Y_{i}\right)$
        $M_{i+1} \leftarrow \operatorname{TrainDPO}\left(M_{i}, D_{i}\right)$
    end for
    return $M_{k}$
```

instruction model $M_{i}$. We assign each response with a trustworthiness score through the divide-andconquer approach using the labeler model $L$ and construct comparison pairs $D_{i}$ for training. Then we train the $M_{i}$ with direct preference optimization on $D_{i}$ to get $M_{i+1}$, which is used as the instruction model of next iteration. In this way, the feedback distribution can be updated in an iterative manner, resulting in better learning efficiency.

## 3 Experiments

In this section, we empirically investigate the effectiveness of RLAIF-V in aligning MLLMs through open-source feedback. In addition to evaluating model performance regarding trustworthiness and helpfulness, we also analyze the efficacy of different components, the compatibility with other methods, and the generalizability of feedback data collected with RLAIF-V.

### 3.1 Experimental Setup

We introduce models, training data, evaluation benchmarks, baselines, and other implementation details. All experiments are conducted based on LLaVA 1.5 7B [31] unless otherwise specified.

Models. We present two settings to align MLLMs with the RLAIF-V framework. First, we use LLaVA 1.5 [31] as the instruction model and LLaVA-NeXT [32] as the labeler model, demonstrating the effectiveness of open-source feedback. Second, we use OmniLMM [43] as both the instruction model and labeler model, representing the extreme scenario where no stronger models are available.

Training Data. The diversity of instructions can be critical for models to learn generalizable preferences. In practice, we use instructions collected from a diverse range of datasets, including MSCOCO [30], ShareGPT-4V [7], MovieNet [19], Google Landmark v2 [54], VQA v2 [15], OKVQA [38], and TextVQA [48]. In addition, we adopt image description prompts introduced in [58] to construct long-form image describing instructions.

Evaluation. We evaluate the models from two perspectives, including trustworthiness reflecting the hallucination degree, and helpfulness reflecting the general interaction capability. For trustworthiness, we perform evaluation on five benchmarks:

(1) Object HalBench [46] is a widely adopted benchmark for assessing common object hallucination in detailed image descriptions. We follow [58] to use 8 diverse prompts to improve the evaluation stability. We report the response-level hallucination rate (i.e., the percentage of hallucinated responses) and the mention-level hallucination rate (i.e., the percentage of hallucinated objects).

(2) MMHal-Bench [49] evaluates response-level hallucination rate and informativeness. It asks GPT-4 [41] to compare model outputs with human responses and object labels for evaluation.

(3) MHumanEval [58] comprises 146 samples collected from both Object HalBench (50) and MMHal-Bench (96) to provide a more comprehensive evaluation over both long-form description and short-form questions. We only label the overall response-level hallucination rate to control the evaluation cost.

(4) AMBER [51] is a multi-dimensional hallucination benchmark comprising more than $15 \mathrm{k}$ samples. We use the discriminative part of AMBER and report the accuracy and F1 metric.

The above trustworthiness evaluations are either limited to common object hallucination, which is mostly eliminated, constrained format (e.g., yes-no choices) or manual labeling. To reliably and automatically assess the trustworthiness of MLLMs under any format, we construct a novel Reliable Free-format Multimodal Benchmark (RefoMB) containing 120 images and 360 instructions covering 8 critical tasks such as mechanical reasoning [35] and image perception [2]. We assess the performance of MLLMs by asking GPT-4 to compare the model response with GPT-4V regarding both trustworthiness and helpfulness. We calculate the trustworthiness win rate and overall win rate based on the GPT-4 evaluation review. Each instruction is paired with a thoroughly written image description as the reference for GPT-4, achieving a notable $96 \%$ human agreement. Results on the dev split ( 99 instructions) are reported in this section to save evaluation costs, we present the test split (261 instructions) results of popular models in the Appendix.

For helpfulness, we adopt two benchmarks: (1) LLaVA Bench [33] is a widely adopted benchmark for assessing multimodal conversation, detailed description and complex reasoning capabilities. We use the in-the-wild version to better capture the capabilities of MLLMs in real-world scenarios. It scores model output against a human annotated image description and reference answer via GPT-4. (2) MMStar [8] is a comprehensive benchmark containing 1500 challenge samples collected from 6 popular multimodal benchmarks [62, 34, 37, 24, 35, 21], covering 6 core capabilities and 18 detailed axes. We report the overall score on this benchmark.

Baselines. We compare our model with state-of-the-art baselines of different types, including general baselines with strong performance, baselines trained with feedback data, baselines reduce hallucination without feedback data and proprietary baselines.

(1) General baselines. We adopt LLaVA 1.5 [31], Qwen-VL-Chat [4], OmniLMM [43], LLaVANeXT [32], MiniGemini [28] as representative general baselines. These models are mostly pre-trained on large-scale multimodal data and fine-tuned on high-quality instruction data, achieving strong performance across various multimodal tasks.

(2) Baselines tailored for feedback learning. LLaVA-RLHF [49] trains the reward model on $10 \mathrm{k}$ human-labeled preference data and performs proximal policy optimization to train the model on $72 \mathrm{k}$ factually augmented data. RLHF-V [58] collects $1.4 \mathrm{k}$ fine-grained correctional human feedback and trains the model with the proposed dense direction preference optimization method to reduce hallucination. Silkie [26] utilizes GPT-4V to assess the helpfulness, visual faithfulness and ethical considerations of responses generated by 12 MLLMs on more than $80 \mathrm{k}$ multimodal instruction.

(3) Baselines tailored for hallucination reduction without feedback. LURE [70] trains a revisor model to rectify object hallucination in model response. VCD [23] contrasts model logits derived from original and distorted visual input to reduce the over-reliance on statistical bias and unimodal priors. OPERA [18] introduces a penalty term on the model logits. Less-is-more [63] propose a selective end-of-sentence (EOS) special token supervision loss and data filtering strategy.

(4) Proprietary Baseline. We also include GPT-4V [42] as strong reference to evaluate the gap between the open-source models and state-of-the-art proprietary model.

Implementation Details. We use the Nous-Hermes-2-Yi-34B [3] version of LLaVA-NeXT and the no-RLHF version of OmniLMM [43] as labeler models. For each iteration, we train the model with DPO for 4 epochs, with learning rate $5 \mathrm{e}-7$, beta 0.1 , and batch size of 8 . We train both RLAIF-V 7B and RLAIF-V 12B for 4 iterations, where we use $4 \mathrm{k}$ instructions to collect feedback at each iteration. In summary, it costs $48 \mathrm{~h}$ and $50 \mathrm{~h}$ for data collection of 7B and 12B models, and costs $6 \mathrm{~h}$ and $8 \mathrm{~h}$ for training separately, using an $8 \mathrm{xA} 100$ 80G machine.

### 3.2 Main Results

The main experimental results are reported in Table 1 , from which we observe that: (1) RLAIF$\mathrm{V}$ achieves state-of-the-art performance in trustworthiness among open-source models and even surpasses proprietary models such as GPT-4V. The framework significantly reduces the object hallucination rate of LLaVA 1.5 and OmniLMM by $82.9 \%$ and $71.0 \%$ relative points on Object HalBench. For the overall hallucination rate, RLAIF-V 12B achieves $29.5 \%$ on MHumanEval, surpassing GPT-4V by a large margin. The reduction of hallucination is consistent among multiple

Table 1: Main experimental results. We report hallucination rates in different granularities including response-level (Resp.) and mention-level (Ment.). MHuman.: MHumanEval, Hall.: Hallucination Rate, Trust: trustworthiness win rate, Win.: overall win-rate. The best and second best results are shown in bold and underlined respectively.

| Model | Size | Feedback | Object <br> HalBench |  | $\frac{\text { MHuman. }}{\text { Resp. } \downarrow}$ | MMHal- <br> Bench |  | AMBER |  | LLaVA <br> Bench <br> Overall | $\frac{\text { MMStar }}{\text { Overall }}$ | RefoMB |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $\overline{\text { Resp. } \downarrow}$ | $\overline{\text { Ment. } \downarrow}$ |  | Score | Hall. $\downarrow$ | $\overline{\text { Acc. }}$ | F1 |  |  | $\overline{\text { Trust. }}$ | Win. |
| VCD 23 | $7 \mathrm{~B}$ | $x$ | 48.8 | 24.3 | 67.1 | 2.12 | 54.2 | 71.8 | 74.9 | 65.8 | 33.8 | 39.9 | 16.7 |
| Less-is-more 63 | 7B | $x$ | 40.3 | 17.8 | 63.7 | 2.33 | 50.0 | 72.4 | 75.8 | 60.9 | 32.9 | 51.1 | 16.2 |
| OPERA [18] | 7B | $x$ | 45.1 | 22.3 | 63.0 | 2.15 | 54.2 | 75.2 | 78.3 | 60.3 | 32.9 | 33.8 | 13.1 |
| LURE 70 | 7B | $x$ | 27.7 | 17.3 | 78.1 | 1.64 | 60.4 | 73.5 | 77.7 | 36.9 | 33.3 | 29.8 | 3.0 |
| QWEN-VL 4 | $10 \mathrm{~B}$ | $x$ | 40.4 | 20.7 | 61.0 | 2.76 | 38.5 | 81.9 | 86.4 | 71.9 | 34.5 | 40.9 | 17.7 |
| LLaVA-NeXT 32 | $34 B$ | $x$ | 12.6 | 6.4 | 53.4 | 3.31 | 34.4 | 81.4 | 85.4 | 77.7 | 51.6 | 44.4 | 35.4 |
| MiniGemini 28] | $34 \mathrm{~B}$ | $x$ | 14.5 | 8.0 | 59.6 | 3.08 | 38.5 | 82.6 | 87.6 | 79.2 | 45.5 | $\underline{50.0}$ | $\underline{36.9}$ |
| HA-DPO 67 | $7 \mathrm{~B}$ | Rule | 39.9 | 19.9 | 53.4 | 1.98 | 60.4 | 75.2 | 79.9 | 67.2 | 32.9 | 39.9 | 17.2 |
| POVID 69] | 7B | Rule | 48.1 | 24.4 | 67.8 | 2.08 | 56.2 | 82.9 | 87.4 | 62.2 | 34.3 | 44.4 | 13.6 |
| LLaVA-RLHF 49] | $13 \mathrm{~B}$ | Human | 38.1 | 18.9 | 72.6 | 2.02 | 62.5 | 79.7 | 83.9 | 61.5 | 34.2 | 26.3 | 17.2 |
| Silkie 26 | $10 \mathrm{~B}$ | GPT-4V | 27.1 | 13.4 | 54.1 | 3.19 | 32.3 | 82.2 | 87.6 | 73.2 | 33.6 | 38.9 | 21.2 |
| RLHF-V 58$]$ | $13 \mathrm{~B}$ | Human | 12.2 | 7.5 | 55.5 | 2.45 | 51.0 | 72.6 | 75.0 | 51.4 | 33.2 | 41.4 | 17.7 |
| LLaVA 1.5 | $7 \mathrm{~B}$ | $x$ | 53.6 | 25.2 | 65.1 | 2.36 | 51.0 | 73.5 | 77.7 | 59.7 | 33.3 | 30.8 | 12.1 |
| + RLAIF-V | 7B | LLaVA-NeXT | $\underline{8.5}$ | $\underline{4.3}$ | 37.7 | 3.06 | 29.2 | 76.8 | 84.5 | 64.9 | 35.4 | 47.5 | 20.7 |
| OmniLMM 43] | $12 \mathrm{~B}$ | $x$ | $\overline{12.2}$ | $\overline{6.2}$ | $\overline{50.7}$ | 3.14 | 36.5 | 86.5 | 89.5 | 72.7 | 39.7 | 48.0 | 23.7 |
| + RLAIF-V | $12 \mathrm{~B}$ | OmniLMM | 3.3 | 1.8 | 29.5 | $\underline{3.36}$ | $\underline{29.2}$ | 88.0 | 90.9 | 74.3 | 40.9 | 62.1 | 32.8 |
| GPT-4V $\sqrt{42}$ | - | Unknown | 13.6 | 7.3 | 45.9 | 3.49 | 28.1 | 83.4 | 87.4 | 93.1 | 50.4 | 50.0 | $\overline{50.0}$ |

benchmarks including MMHal-Bench, AMBER, and RefoMB. (2) RLAIF-V achieves promising performance in response helpfulness, where the results on LLaVA Bench and MMStar are improved compared to the base model. This shows that RLAIF-V can enhance the trustworthiness of MLLMs without sacrificing the performance of other tasks. (3) Using OmniLMM as both the instruction model and the labeler model, RLAIF-V 12B achieves significant hallucination reduction on multiple benchmarks and comparable helpfulness. Remarkably, RLAIF-V 12B outperforms GPT-4V in trustworthiness on Object HalBench, MHumanEval, AMBER, and RefoMB, by substantial margins. The results demonstrate a promising path to achieve self-alignment of leading-edge MLLMs.

### 3.3 Analysis

We conduct analysis on the framework considering the following research questions: (1) How effective is the deconfounded candidate response generation strategy for improving feedback quality? (2) Can we directly acquire satisfactory feedback from open-source MLLMs without the divide-and-conquer approach? (3) What is the advantage of learning from feedback in an iterative manner? (4) Can RLAIF-V be used with other sources of feedback together? (5) Can feedback data collected for one model with RLAIF-V be adopted to enhance the trustworthiness of other MLLMs?

Deconfounded strategy enables better learning efficiency. To quantify the advantage of the deconfounded candidate response generation strategy, we conduct an experiment based on the RLHF$\mathrm{V}$ dataset [58]. We compare the performance of model trained under three different settings: (1) $R L H F-V$, the model is directly aligned with fine-grained correctional human feedback data; (2) Ours, we collect high-quality feedback from LLaVA-NeXT based on original multimodal instructions using the RLAIF-V framework; (3) Adapted, we replace the preferred responses generated under the deconfounded strategy with original human annotations.

From experimental results in Table 2, we observe that model trained with our deconfounded responses achieves the best performance on both long-form description and discriminative tasks. Changing the preferred response with high quality human annotated response, though improving the feedback precision and response quality, exhibits significant performance loss. We hypothesize this action introduces more non-robust shallow patterns into the training data and thus harms the learning efficiency. Moreover, performance of our method even surpasses training on fine-grained humanannotated correctional feedback by a large margin.
Table 2: Experimental results of different response generation methods. ObjHal.: Object HalBench.

| Data | ObjHal. |  |  | AMBER |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Resp. $\downarrow$ | Ment. $\downarrow$ |  | Acc. | F1 |
| RLHF-V [58] | 28.5 | 12.3 |  | 76.4 | 84.6 |
| Adapted | 25.7 | 11.8 |  | 73.3 | 83.0 |
| Ours | $\mathbf{1 0 . 1}$ | $\mathbf{4 . 7}$ |  | $\mathbf{8 0 . 1}$ | $\mathbf{8 6 . 1}$ |

Table 3: Performance comparison of different feedback collection methods. We conduct the experiment on various labeler models. ObjHal.: Object HalBench.

| Method | Labeler | Labeler Size | ObjHal. |  |  | AMBER |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | Resp. $\downarrow$ | Ment. $\downarrow$ |  | Acc. | F1 |
| Self Rewarding [61] | LLaVA 1.5 | 7B | 48.4 | 24.6 |  | 54.5 | 48.3 |
|  | OmniLMM | 12B | 46.7 | 23.9 |  | 57.5 | 53.2 |
|  | LLaVA-NeXT | 34B | 58.4 | 28.3 |  | 74.5 | 78.5 |
|  | LLaVA 1.5 | 7B | 39.5 | 19.6 |  | 77.4 | 81.3 |
|  | OmniLMM | 12B | 33.0 | 17.5 | 75.2 | 78.2 |  |
|  | LLaVA-NeXT | 34B | $\mathbf{2 0 . 6}$ | $\mathbf{1 0 . 4}$ | $\mathbf{8 0 . 5}$ | $\mathbf{8 6 . 0}$ |  |

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-08.jpg?height=344&width=1390&top_left_y=755&top_left_x=365)

Figure 3: Comparison of iterative and non-iterative alignment. We report overall win rate compared with GPT-4V on RefoMB and response-level non-hallucination rate on Object HalBench.

Divide-and-conquer enhances feedback quality. We compare our divide-and-conquer strategy with direct self-rewarding [61] by replacing only the implementation of response evaluation process. Specifically, self-rewarding asks the labeler model to generate an overall quality score of each candidate response with a long prompt introducing multiple criteria. To bolster the reliability, we adopt three labeler models from weak to strong and present the results in Table 3. We observe that simply asking open-source models to generate an overall assessment of responses yields unsatisfactory results and can even harm the performance. In contrast, our method not only improves performance but also demonstrates increasing efficacy as the size of the labeler model grows, showing promising scalability and compatibility.

Iterative alignment mitigates distribution shift problem. To quantify the effectiveness of iterative alignment, we compared the results of our iterative alignment and non-iterative naive direct preference optimization in Figure 3. Considering the scale of data, we conduct this experiment under the RLAIFV 12B setting to save cost with a smaller labeler model. We observe that the performance of the non-iterative method quickly saturates and the effectiveness of data cannot be fully utilized, while iterative alignment continues to reduce object hallucination and improve overall win rate compared with GPT-4V on RefoMB.

RLAIF-V is complementary with existing feedback learning methods. Besides collecting feedback using models as labelers, multiple existing works generate feedback based on heuristic rules or human annotation. We explore the possibility of combining RLAIF-V with other sources of feedback. Results in Figure 4 show that heuristically collected feedback from HA-DPO [67] and human annotated feedback from RLHF-V can further improve the trustworthiness, indicating RLAIF-V is complementary with other types of feedback.

RLAIF-V produces generalizable high-quality feedback. We train different models with the feedback collected during the first iteration of training RLAIF-V 12B. Specifically, we train LLaVA 1.5 7B [31], LLaVA 1.5 13B [31], MiniCPM-V [43] and MiniCPM-V 2 [43] with direct preference optimization and report the trustworthiness improvement in Figure 4 (b). We observe that data collected from OmniLMM (as both instruction model and labeler model) with RLAIF-V can effectively reduce the hallucination of other MLLMs on different benchmarks. Notably, the improvement can be even more significant compared with the OmniLMM which generates the candidate responses. The results demonstrate that feedback data collected with RLAIF-V are generalizable to improve trustworthiness of different MLLMs.

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-09.jpg?height=458&width=1393&top_left_y=275&top_left_x=366)

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-09.jpg?height=390&width=420&top_left_y=282&top_left_x=365)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-09.jpg?height=388&width=534&top_left_y=283&top_left_x=793)

(b)

Figure 4: (a) Results of combining RLAIF-V with other sources of feedback. (b) Hallucination reduction of other MLLMs with data from the first training iteration of RLAIF-V 12B (i.e., using OmniLMM as instruction model). We report the response-level hallucination rate reduction on Object HalBench for generative hallucination and error rate reduction on AMBER for discriminative hallucination.

![](https://cdn.mathpix.com/cropped/2024_05_29_acca344dff27a5e315ceg-09.jpg?height=713&width=1391&top_left_y=972&top_left_x=367)

Figure 5: Qualitative results of RLAIF-V 7B and RLAIF-V 12B compared with GPT-4V. Correct answers and hallucinations are highlighted in color respectively.

### 3.4 Case Study

To provide an intuitive understanding and comparison of different models, we provide qualitative results in Figure 5 We show cases of two RLAIF-V models: (1) In the first example, both RLAIF-V 7B and GPT-4V correctly identify the relationships between characters in the image as colleagues or members with a common purpose. However, GPT-4V incorrectly claims that the person talking to the rest of the group is standing. (2) In the second example, RLAIF-V 12B exhibits superior trustworthiness level compared with GPT-4V, without introducing any hallucination. On the other hand, GPT-4V incorrectly mentions the blue outline and darker blue URLs which do not exist in the image. We refer readers to the Appendix for more qualitative results.

## 4 Conclusion

Aligning models with human preference is a critical target. In this work, we present RLAIF-V, a novel framework that enhances the trustworthiness of MLLMs through open-source AI feedback. Comprehensive experimental results show that our models achieve state-of-the-art performance
in both generative and discriminative trustworthiness. In this work, we propose a deconfounded sampling and divide-and-conquer strategy to improve the efficiency and quality of feedback. By iteratively aligning the model with such feedback, the trustworthiness can be substantially improved without sacrificing performance on other tasks. In the future, we will explore collecting more complex feedback from models to improve logical reasoning and complex task solving capabilities.

## References

[1] Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. The CRINGE loss: Learning what language not to model. In Anna Rogers, Jordan L. BoydGraber, and Naoaki Okazaki, editors, Proceedings of ACL, pages 8854-8874. Association for Computational Linguistics, 2023.

[2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of ICCV, pages 8948-8957, 2019.

[3] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.

[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023.

[5] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: A survey, 2024.

[6] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-ajudge with vision-language benchmark. CoRR, abs/2402.04788, 2024.

[7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. CoRR, $\mathrm{abs} / 2311.12793,2023$.

[8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024.

[9] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.

[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. InstructBLIP: Towards general-purpose visionlanguage models with instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Proceedings of NeurIPS, 2023.

[11] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large vision-language models via clip-guided decoding. CoRR, abs/2402.15300, 2024.

[12] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. CoRR, abs/2403.14003, 2024.

[13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. MME: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.

[14] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of ICML, volume 202 of Proceedings of Machine Learning Research, pages 10835-10866. PMLR, 2023.

[15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the $\mathrm{V}$ in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of CVPR, pages 6325-6334. IEEE Computer Society, 2017.

[16] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 18135-18143. AAAI Press, 2024

[17] Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, and Mike Zheng Shou. Skip $\ln$ : A simple method to reduce hallucination in large vision-language models. CoRR, abs/2402.01345, 2024.

[18] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. OPERA: alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Processing of CVPR, 2024.

[19] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic dataset for movie understanding. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Proceedings of ECCV, volume 12349 of Lecture Notes in Computer Science, pages 709-727. Springer, 2020.

[20] Liqiang Jing and Xinya Du. Fgaif: Aligning large vision-language models with fine-grained ai feedback, 2024.

[21] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. ArXiv, abs/1603.07396, 2016.

[22] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: scaling reinforcement learning from human feedback with AI feedback. CoRR, abs/2309.00267, 2023.

[23] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. CoRR, abs/2311.16922, 2023.

[24] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.

[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of ICML, volume 202 of Proceedings of Machine Learning Research, pages 19730-19742. PMLR, 2023.

[26] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. CoRR, abs/2312.10665, 2023.

[27] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. 2023.

[28] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. CoRR, abs/2403.18814, 2024.

[29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 292-305. Association for Computational Linguistics, 2023.

[30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tomás Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Proceedings of ECCV, volume 8693 of Lecture Notes in Computer Science, pages 740-755. Springer, 2014.

[31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. CoRR, abs/2310.03744, 2023.

[32] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Proceedings of NeurIPS, 2023.

[34] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024.

[35] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. In Proceedings of ICLR, 2024.

[36] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. In Processing of ICLR, 2024.

[37] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Proceedings of NeurIPS, 2022.

[38] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In Proceedings of CVPR, pages 3195-3204. Computer Vision Foundation / IEEE, 2019.

[39] Meta. Introducing meta Llama 3: The most capable openly available LLM to date. https: //ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-09.

[40] openai. Introducing chatgpt. https://openai.com/index/chatgpt/, 2022. Accessed: 2022-12-05.

[41] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.

[42] OpenAI. GPT-4V(ision) system card, 2023.

[43] OpenBMB. Large multi-modal models for strong performance and efficient deployment. https://github.com/OpenBMB/OmniLMM, 2024. Accessed: 2024-03-05.

[44] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Proceedings of NeurIPS, 2023.

[45] Anku Rani, Vipula Rawte, Harshad Sharma, Neeraj Anand, Krishnav Rajbangshi, Amit P. Sheth, and Amitava Das. Visual hallucination: Definition, quantification, and prescriptive remediations. CoRR, abs/2403.17306, 2024.

[46] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of EMNLP, pages 4035-4045. Association for Computational Linguistics, 2018

[47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.

[48] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In Proceedings of CVPR, pages 8317-8326. Computer Vision Foundation / IEEE, 2019.

[49] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. CoRR, abs/2309.14525, 2023.

[50] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Finetuning language models for factuality. CoRR, abs/2311.08401, 2023.

[51] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. CoRR, abs/2311.07397, 2023.

[52] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites. In Stevan Rudinac, Alan Hanjalic, Cynthia C. S. Liem, Marcel Worring, Björn Pór Jónsson, Bei Liu, and Yoko Yamakata, editors, MultiMedia Modeling - 30th International Conference, MMM 2024, Amsterdam, The Netherlands, January 29 - February 2, 2024, Proceedings, Part IV, volume 14557 of Lecture Notes in Computer Science, pages 32-45. Springer, 2024.

[53] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding, 2024.

[54] Tobias Weyand, André Araújo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2 - A large-scale benchmark for instance-level recognition and retrieval. In Proceedings of CVPR, pages 2572-2581. Computer Vision Foundation / IEEE, 2020.

[55] Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, and Tieniu Tan. Logical closed loop: Uncovering object hallucinations in large vision-language models, 2024.

[56] Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Hao Jiang, Fei Wu, and Linchao Zhu. Detecting and mitigating hallucination in large vision language models via fine-grained ai feedback, 2024.

[57] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. Reformulating vision-language foundation models and datasets towards universal multimodal assistants. CoRR, $\mathrm{abs} / 2310.00653,2023$.

[58] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. RLHF-V: towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of CVPR, 2024.

[59] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023.

[60] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024.

[61] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. CoRR, abs/2401.10020, 2024.

[62] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR, abs/2311.16502, 2023.

[63] Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an EOS decision perspective. CoRR, abs/2402.14545, 2024.

[64] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of CVPR, June 2019.

[65] Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, and Xiangjun Fan. Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. CoRR, $\mathrm{abs} / 2310.01779,2023$.

[66] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. Mitigating object hallucination in large vision-language models via classifier-free guidance. CoRR, abs/2402.08680, 2024.

[67] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. CoRR, abs/2311.16839, 2023.

[68] Ziqiang Zheng, Yiwei Chen, Jipeng Zhang, Tuan-Anh Vu, Huimin Zeng, Yue Him Wong Tim, and Sai-Kit Yeung. Exploring boundary of gpt-4v on marine analysis: A preliminary case study, 2024 .

[69] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024 .

[70] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In Proceedings of ICLR, 2024.

[71] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. IBD: alleviating hallucinations in large vision-language models via image-biased decoding. CoRR, abs/2402.18476, 2024 .
