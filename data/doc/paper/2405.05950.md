# Federated Combinatorial Multi-Agent Multi-Armed Bandits 

Fares Fourati ${ }^{1}$ Mohamed-Slim Alouini ${ }^{1}$ Vaneet Aggarwal ${ }^{2}$


#### Abstract

This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback. In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals. Our framework transforms any offline resilient single-agent $(\alpha-\epsilon)$-approximation algorithm-having a complexity of $\tilde{\mathcal{O}}\left(\frac{\psi}{\epsilon^{\beta}}\right)$, where the logarithm is omitted, for some function $\psi$ and constant $\beta$-into an online multi-agent algorithm with $m$ communicating agents and an $\alpha$-regret of no more than $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3+\beta}} \psi^{\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right)$. Our approach not only eliminates the $\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents. Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\tilde{\mathcal{O}}\left(\psi T^{\frac{\beta}{\beta+1}}\right)$. Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both singleagent and multi-agent settings and recovering specialized single-agent theoretical guarantees. We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios.


[^0]
## 1. Introduction

The Multi-Armed Bandits (MAB) (Slivkins et al., 2019; Lattimore \& Szepesvári, 2020) model online decision-making, where at every time step, an agent plays an arm and observes its associated reward. In combinatorial MAB, the agent can play a set of arms, instead of one arm, at each time step and receive a reward for that selection. When the agent only learns about the reward linked to the played set, it is known as full-bandit feedback or bandit feedback. If the agent gains additional insights into how each arm contributes to the overall reward, it is called semi-bandit feedback. Dealing with bandit feedback is more challenging since agents have less knowledge than in the semi-bandit feedback setting. In this work, we consider bandit feedback (Fourati et al., 2023a; Nie et al., 2023; Fourati et al., 2024), which has several applications, such as recommender systems, revenue maximization (Fourati et al., 2023a), influence maximization (Nie et al., 2023; Fourati et al., 2024), and data summarization, as shown in this work.

Federated learning (FL), an emerging machine learning paradigm, involves collaborative learning among multiple agents. In this process, selected agents share their local updates with a central server, which then aggregates these updates and sends the consolidated output back to each participating agent (Konečnỳ et al., 2016; McMahan et al., 2017; Li et al., 2020; Hosseinalipour et al., 2020; Elgabli et al., 2022; Fourati et al., 2023b). While the problem has been studied in the context of continuous optimization, we address combinatorial optimization (Korte et al., 2011) in a federated online stochastic setting with bandit feedback. For example, the multi-agent setting under consideration can be applied to recommender systems, where each agent aims to recommend a set of products and then shares its findings with a server. We provide a general FL framework to adapt combinatorial offline single-agent approximation algorithms to online multi-agent settings, presenting the first results for the regret bounds in this setup.

We consider a setting with $m^{\prime} \geq 1$ agents connected through a network. Among them, only a randomly selected subset of $m \leq m^{\prime}$ agents can cooperate and share information in communication rounds, possibly through a server. This setup accommodates scenarios of partial participation, which are more practical in some real-world settings due to inherent
availability and communication constraints. When $m=m^{\prime}$, the scenario reverts to a full-participation setting. All agents aim to solve the same online stochastic combinatorial problem within a time horizon of $T$. They conduct local exploration and then, if selected, share local estimations. Each agent $i$ can play any subset of arms in parallel with others and receives noisy rewards for that set. In a combinatorial setting, the number of possible actions becomes exponentially large with the number of base arms, making sharing estimations for all possible actions prohibitive. Therefore, we consider a more practical approach wherein, in each communication round, selected agents share only a single action (subset) estimation.

Our work is the first to provide a general multi-agent framework for adapting combinatorial offline single-agent approximation algorithms to a FL setting to solve stochastic combinatorial multi-agent MAB (C-MA-MAB) problems with only bandit feedback. The proposed approach provably achieves a regret of at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3+\beta}} \psi^{\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right)$, which is sub-linear in the time horizon $T$ and decreases with an increasing number of agents $m$, for some constant $\beta$ and some function $\psi$ that govern the complexity of the considered offline approximation algorithm. The framework does not require the agents to communicate every step; instead, it only needs $\tilde{\mathcal{O}}\left(\psi T^{\frac{\beta}{\beta+1}}\right)$ communication times. Our proposed framework can serve to solve online combinatorial problems for both single-agent $\left(m^{\prime}=m=1\right)$ and multi-agent $\left(m^{\prime}>1\right)$ scenarios. Notably, our framework enjoys a linear speedup with an increasing number of agents, translating to decreased regrets.

We note that for a single agent, under bandit feedback, various offline-to-online transformations have been proposed for submodular maximization (Nie et al., 2022; Fourati et al., 2023a; 2024). Additionally, Nie et al. (2023) studied a framework for general combinatorial optimization in which any resilient offline algorithm with an $\alpha$-approximation guarantee can be adapted to an online algorithm with sublinear $\alpha$-regret guarantees. Similarly, an offline resilient $(\alpha-\epsilon)$ approximation algorithm can provide sublinear $(\alpha-\epsilon)$ regret guarantees for an online algorithm for any $\epsilon \geq 0$. This approach leads to a linear $\alpha$-regret online algorithm when $\epsilon>0$. Recently, Fourati et al. (2024) addressed this issue of linear regret in the case of monotone submodular optimization with a cardinality constraint. They adapted a sub-optimal offline $(1-1 / e-\epsilon)$-approximation algorithm-whose complexity grows with $\log \left(\frac{1}{\epsilon}\right)$-to a sublinear $(1-1 / e)$-regret online algorithm with bandit feedback, successfully eliminating the $\epsilon$ approximation error while ensuring sub-linearity with respect to the horizon $T$.

In this work, we generalize and extend the results previously established by Nie et al. (2023) and Fourati et al. (2024). We demonstrate that any sub-optimal single-agent offline approximation algorithm, with an approximation factor of $(\alpha-\epsilon)$ where $\epsilon \geq 0$, and ensuring resilience, can be adapted to a multi-agent setting with bandit feedback. This adaptation provides sub-linear $\alpha$-regret guarantees, in contrast to the sub-linear $(\alpha-\epsilon)$-regret guarantees provided by Nie et al. (2023). This approach eliminates the $\epsilon$ error while maintaining sub-linearity with respect to $T$ across any combinatorial problem, any reward, and under any constraints.

We note that in contrast to previous works that dealt with specific assumptions about reward functions and constraints on actions-such as assuming stochastic submodular rewards (Fourati et al., 2023a) or monotone rewards with cardinality constraints (Nie et al., 2022; Fourati et al., 2024)—our work considers general reward functions without making any assumptions about the reward type or constraints. Furthermore, our work explores the use of any offline algorithm $\mathcal{A}(\epsilon)$ as a subroutine with complexity in the general form of $\mathcal{O}\left(\frac{\psi}{\epsilon^{\beta}}\right)$ or $\mathcal{O}\left(\frac{\psi}{\epsilon^{\beta}} \log \left(\frac{1}{\epsilon}\right)\right)$, where $\psi$ is a function of the problem characteristics, $\beta \geq 0$ some constant, and $\epsilon$ is an approximation error factor, ensuring that our results apply to any algorithm with these characteristics.

In addition, unlike previous works that focused on combinatorial single-agent scenarios (Fourati et al., 2023a; 2024; Nie et al., 2022; 2023), we address a combinatorial multi-agent setting where collaboration is permitted, possibly with partial participation, and optimize the worst-case regret for any agent, thereby recovering and generalizing the single-agent setting. Additionally, although our proposed algorithm has parameters, such as $\epsilon^{\star}$ and $r^{\star}$, none are considered hyperparameters. We derive closed-form values for these parameters as functions of the problem parameters, such as the range $T$, the number of available agents $m$, and the subroutine-related value of the constant $\beta$, and the function $\psi$, to minimize the expected cumulative $\alpha$-regret.

Contributions: We introduce a novel FL framework for online combinatorial optimization, adapting single-agent offline algorithms to tackle online multi-agent problems with bandit feedback. The paper demonstrates the adaptability of any single-agent sub-optimal offline $(\alpha-\epsilon)$-approximation algorithm, ensuring resilience, for a C-MA-MAB with $\alpha$ regret guarantees of $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right)$, lifting the $\epsilon$ error, providing sub-linearity in the horizon $T$, and ensuring linear speedup with an increasing number of agents $m$. The framework only requires sub-linear communication rounds of $\tilde{\mathcal{O}}\left(T^{\frac{\beta}{\beta+1}}\right)$, which becomes at most logarithmic with respect to $T$ when $\beta=0$. Furthermore, we leverage our theoretical results to address online submodular maximization, present the first specialized regret for the non-monotone cardinality constraint case, and recover bounds for different offline algorithms, demonstrating tighter regrets than previous specialized works. Finally, we showcase the applicability of
our framework to both single-agent and multi-agent stochastic data summarization problems against MAB baselines, highlighting its effectiveness in practical scenarios.

## 2. Problem Statement

We formally present the problem as follows. We denote $\Omega$ as the ground set of $n$ base arms. We consider a set $\mathbb{A}^{\prime}$ with $m^{\prime} \geq 1$ agents, where only a randomly selected subset $\mathbb{A}$ of $m \geq 1$ agents communicates in the communication rounds. We examine decision-making problems within a fixed period $T$, where, at every time step $t$, agent $i$ chooses a subset $S_{i, t} \subseteq \Omega$. Let $\mathbb{S} \subseteq 2^{\Omega}$ represent the set of all permitted subsets, depending on the problem constraints.

In each time step $t$, agent $i$ plays an action $S_{i, t} \in \mathbb{S}$ and acquires a noisy reward $f_{t}\left(S_{i, t}\right)$. We assume that the reward $f_{t}$ is a realization of a stochastic function with a mean of $f$, bounded in $[0,1]^{1}$, and i.i.d. conditioned on a given action. Thus, over a horizon $T$, agent $i$ achieves a cumulative reward of $\sum_{t=1}^{T} f_{t}\left(S_{i, t}\right)$. We define the expected reward function for a given action $S$ as $f(S)=\mathbb{E}\left[f_{t}(S)\right]$, hence $S^{\star}=$ $\arg \max _{S \subseteq \mathbb{S}} f(S)$ denote the optimal set in expectation.

In offline settings, attention is on the algorithm's complexity and worst-case expected output approximation guarantees. Conversely, in the online setting, attention is on cumulative rewards, where agents seek to minimize their expected cumulative regrets over time. One standard metric to assess the algorithm's online performance is to contrast the agent to an ideal learner that knows and consistently plays the best choice in expectation $S^{\star}$. However, the significance of such a comparison becomes questionable if the optimization of $f$ over $\mathbb{S}$ is NP-hard, if the horizon is not exponentially large in the problem parameters (Fourati et al., 2023a; 2024; Nie et al., 2023). Hence, if a polynomial time offline algorithm $\mathcal{A}(\epsilon)$, happen to be an $(\alpha-\varepsilon)$-approximation ${ }^{2}$ algorithm, for a given error $\epsilon \geq 0$, and an approximation ratio of $(\alpha-\epsilon) \leq 1$, for specific combinatorial objectives, a common approach involves comparing the agent's cumulative reward to $\sum_{t=1}^{T}(\alpha-\epsilon) f\left(S^{\star}\right)$ and denoting the difference as the $(\alpha-\epsilon)$-regret (Nie et al., 2023). In this work, we compare the learner's cumulative reward to a (tighter) agent that achieves $\sum_{t=1}^{T} \alpha f\left(S^{\star}\right)$, and we denote the difference as the $\alpha$-regret, which is defined for every agent $i$ as follows:

$$
\begin{equation*}
\mathcal{R}_{i}(T)=\sum_{t=1}^{T}\left(\alpha f_{t}\left(S^{\star}\right)-f_{t}\left(S_{i, t}\right)\right) \tag{1}
\end{equation*}
$$[^1]

The cumulative $\alpha$-regret $\mathcal{R}_{i}(T)$ is random, given that it is a sum of $\alpha$-regrets, which are functions of stochastic rewards and depend on the chosen subsets. In this work, we aim to minimize the expected cumulative $\alpha$-regret, where the expectation encompasses the oracle noise and the randomness of the series of actions.

## 3. Related Work

In Table 1, we provide a comprehensive overview comparing our $\alpha$-regret guarantees with existing ones across various adaptations of offline approximations for different combinatorial problems. Each row corresponds to a case involving an offline algorithm or a general class of algorithms with a specified complexity form, offline approximation factor, and target online regret factor $\alpha$. The last two columns present previously established $\alpha$-regret bounds and our own for adapting the proposed offline algorithm or class of algorithms as a subroutine to the online setting. The third row presents the GENERAL transformation, which generalizes the previous two cases, with a complexity of the form $\mathcal{O}(\psi)$ for any function $\psi$. The fifth row presents the More General case, with a complexity of the form $\mathcal{O}\left(\psi \log ^{\gamma}\left(\frac{1}{\epsilon}\right)\right)$, where $\gamma \in\{0,1\}$, further generalizing the previous cases. The last row, presenting the Most GENERAL, encompasses all previous rows, with a complexity of the form $\mathcal{O}\left(\frac{\psi}{\epsilon^{\beta}} \log ^{\gamma}\left(\frac{1}{\epsilon}\right)\right)$, where $\beta \geq 0$, including the General when $\beta=\gamma=0$, the More General when $\beta=0$, and the example in the sixth row.

Combinatorial Single-Agent Examples: We note that online submodular optimization with bandit feedback has been considered in (Nie et al., 2022; Fourati et al., 2023a; 2024). These results are differentiated from our work in Table 1. For single-agent non-monotone submodular rewards (See Section 6) under bandit feedback, Fourati et al. (2023a) adapts the RANDOMIZEDUSM in (Buchbinder et al., 2015), achieving sub-linear $\frac{1}{2}$-regret. Additionally, for single-agent monotone submodular rewards under bandit feedback with a cardinality constraint $k$, Nie et al. (2022) adapts the GREEDY in (Nemhauser et al., 1978), and Fourati et al. (2024) adapts the Stochastic-GreEdY in (Mirzasoleiman et al., 2015), with both achieving sub-linear $\left(1-\frac{1}{e}\right)$-regret. Our work not only recovers all the results above in the single-agent setting but also generalizes them to the multi-agent setting, showing a decreasing regret with a factor of $m^{-\frac{1}{3}}$.

Multi-Agent: Previous works have proposed solutions for solving offline distributed submodular maximization. For example, partitioning the arms among the agents and running a greedy algorithm on each agent was proposed in previous works (Barbosa et al., 2015; Mirzasoleiman et al., 2013). While this is practical in some settings, it is designed for deterministic objectives and leads to lower approximation guarantees (half the ratio for monotone submodular

| Offline Algorithm | Offline <br> Complexity | Offline <br> Factor | Online <br> Factor $\alpha$ | Prior $\alpha$-regret | Our $\alpha$-regret Bound |
| :---: | :---: | :---: | :---: | :---: | :---: |
| RANDOMIZEDUSM <br> (Buchbinder et al., 2015) | $\mathcal{O}(n)$ | $1 / 2$ | $1 / 2$ | $\tilde{\mathcal{O}}\left(n T^{\frac{2}{3}}\right)^{*}$ | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} n T^{\frac{2}{3}}\right)$ |
| GREEDY <br> (Nemhauser et al., 1978) | $\mathcal{O}(n k)$ | $1-\frac{1}{e}$ | $1-\frac{1}{e}$ | $\tilde{\mathcal{O}}\left(k n^{\frac{1}{3}} T^{\frac{2}{3}}\right)^{* *}$ | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} k n^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ |
| GENERAL <br> Excluding the next rows | $\mathcal{O}(\psi)$ | $\alpha$ | $\alpha$ | $\tilde{\mathcal{O}}\left(\delta^{\frac{2}{3}} \psi^{\frac{1}{3}} T^{\frac{2}{3}}\right)^{* *}$ | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} \delta^{\frac{2}{3}} \psi^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ |
| STOCHASTIC-GREEDY <br> Mirzasoleiman et al., 2015) | $\mathcal{O}\left(n \log \left(\frac{1}{\epsilon}\right)\right)$ | $1-\frac{1}{e}-\epsilon$ | $1-\frac{1}{e}$ | $\tilde{\mathcal{O}}\left(k^{\frac{2}{3}} n^{\frac{1}{3}} T^{\frac{2}{3}}\right)^{\dagger}$ | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} k^{\frac{2}{3}} n^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ |
| MORE GENERAL <br> Excluding the next rows | $\mathcal{O}\left(\psi \log ^{\gamma}\left(\frac{1}{\epsilon}\right)\right)$ | $\alpha-\epsilon$ | $\alpha$ | None | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} \delta^{\frac{2}{3}} \psi^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ |
| RANDOMSAMPLING <br> (Buchbinder et al., 2017) | $\mathcal{O}\left(\frac{n}{\epsilon^{2}} \log \left(\frac{1}{\epsilon}\right)\right)$ | $\frac{1}{e}-\epsilon$ | $\frac{1}{e}$ | None | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{5}} k^{\frac{2}{5}} n^{\frac{1}{5}} T^{\frac{4}{5}}\right)$ |
| MoST GENERAL <br> Including the previous rows | $\mathcal{O}\left(\frac{\psi}{\epsilon^{\beta}} \log ^{\gamma}\left(\frac{1}{\epsilon}\right)\right)$ | $\alpha-\epsilon$ | $\alpha$ | None | $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3+\beta}} \delta^{\frac{2}{3+\beta}} \psi^{\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right)$ |

Table 1: The table summarizes the results of combinatorial optimization under bandit feedback. We use $\tilde{\mathcal{O}}$ to simplify expressions. Key parameters include horizon $T$, number of communicating agents $m$, base arm count $n$, and cardinality constraint $k$. Each row presents a specific offline algorithm or a class transformation with a given complexity, offline approximation factor, and a target online factor $\alpha$. For the general rows, we consider classes of offline algorithms, with an approximation error factor $\epsilon$, with general complexity forms, with general constants, $\psi \geq 0, \beta \geq 0, \gamma \in\{0,1\}$, and $\delta \geq 0 .{ }^{*}$ (Fourati et al., 2023a), ${ }^{* *}$ (Nie et al., 2023), ${ }^{\dagger}$ (Fourati et al., 2024).

maximization). Moreover, regret analysis for multi-agent (non-combinatorial) MAB problems has been investigated (Chawla et al., 2020; Wang et al., 2020; Agarwal et al., 2022). In (Chawla et al., 2020), gossip style communication approach is used between agents to achieve $\tilde{\mathcal{O}}\left(\left(\frac{n}{m}+2\right)^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ regret. Wang et al. (2020) present an algorithm for a distributed bandit setting where all the agents communicate with a central node, which is shown to achieve a regret of $\tilde{\mathcal{O}}(\sqrt{n T / m})$. Agarwal et al. (2022) propose another algorithm, which splits the arms among the different agents, such that each learner plays arms only within a subset of arms and the best-communicated arm indices from other agents in the previous round. This achieves a regret of $\tilde{O}\left(\sqrt{\left(\frac{n}{m}+m\right) T}\right)$ while reducing the communication significantly. We note that these works do not directly extend to combinatorial bandits since the confidence-bound based approaches here cannot work for combinatorial bandits since $\mathcal{O}\left(2^{n}\right)$ sets cannot be explored. Recent works have considered FL for contextual bandits (Li \& Wang, 2022; He et al., 2022; Li et al., 2023); however, these works do not apply to our setting. Our work is the first to present an FL framework for general combinatorial multi-agent MAB with bandit feedback.

Combinatorial Single-Agent Frameworks: Previous frameworks have been proposed for combinatorial singleagent MAB problems (Niazadeh et al., 2021; Nie et al., 2023). Niazadeh et al. (2021) proposes a framework aiming to adjust an iterative greedy offline algorithm into an online version within an adversarial bandit setting. However, their approach requisites the offline algorithm to possess an iterative greedy structure. In contrast, our framework treats the offline algorithm as a black-box algorithm. Furthermore, unlike our work, Niazadeh et al. (2021) imposes a condition known as Blackwell reproducibility on the offline algorithm, in addition to the resilience property. A closely related work is the single-agent framework by Nie et al. (2023) called C-ETC, which adapts offline algorithms with robustness guarantees to stochastic combinatorial single-agent MAB, generalizing some previous works for submodular maximization (Nie et al., 2022; Fourati et al., 2023a). However, C-ETC fails to generalize the more recent work of Fourati et al. (2024). Our framework generalizes and outperforms the results of Nie et al. (2023) in many ways. First, it extends to the multi-agent scenario involving $m \geq 1$ communicating agents, including the single-agent scenario. Additionally, while the C-ETC framework cannot replicate the results of Fourati et al. (2024) for submodular maximization, ours not only recovers all these previous works, including the framework, but also achieves even tighter regret guarantees that decrease with an increasing number of selected agents.

## 4. Combinatorial MA-MAB Framework

We first define resilient approximation algorithms and then present our proposed offline-to-online framework.

### 4.1. Offline Resilient-Approximation

We introduce the concept of resilient approximation, a metric that allows us to evaluate how an offline approximation algorithm reacts to controlled variations in function evaluations. We demonstrate that this specific characteristic alone can ensure that the offline algorithm can be modified to tackle stochastic C-MA-MAB settings, with only bandit feedback and achieving a sub-linear regret. Moreover, this adaptation does not rely on the algorithm's structure but treats it as a black-box algorithm.

We define the $\xi$-controlled-estimation $\bar{f}$ of a reward function $f$ to deal with controlled variations.

Definition 4.1 ( $\xi$-controlled-estimation). For a set function $f: \mathbb{S} \rightarrow \mathbb{R}$ defined over a finite domain $\mathbb{S} \subseteq 2^{\Omega}$, a set function $\bar{f}: \mathbb{S} \rightarrow \mathbb{R}$, for a $\xi \geq 0$, is a $\xi$-controlled estimation of $f$ if $|f(S)-\bar{f}(S)| \leq \xi$ for all $S \in \mathbb{S}$.

An estimation $\bar{f}$ is a $\xi$-controlled-estimation for a set function $f$ if it consistently stays within a small positive range $\xi$ of the actual values across all possible sets in $\mathbb{S}$. Given such a $\xi$-controlled-estimation for a set function $f$, we define $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation as follows:

Definition 4.2 (( $\alpha, \beta, \gamma, \psi, \delta)$-resilient approximation). For any $\epsilon \geq 0$, an algorithm $\mathcal{A}(\epsilon)$ is an $(\alpha, \beta, \gamma, \psi, \delta)$-resilient approximation for maximizing a function $f: \mathbb{S} \subseteq 2^{\Omega} \rightarrow$ $\mathbb{R}$ if, after making a number of calls $N(\beta, \gamma, \psi, \epsilon)$ to a $\xi$ controlled estimator $\bar{f}$, its output $\Theta$ satisfies the following condition: $\mathbb{E}[f(\Theta)] \geq(\alpha-\epsilon) f\left(S^{\star}\right)-\delta \xi$, where $S^{\star}$ is the optimal set under $f$, and the expectation is over the randomness of $\mathcal{A}(\epsilon)$. Here, $N(\beta, \gamma, \psi, \epsilon)$ is defined as $\psi$ when $\epsilon=0$, and as $\psi \frac{1}{\epsilon^{\beta}} \log ^{\gamma}\left(\frac{1}{\epsilon}\right)$ otherwise.

Remark 4.3. Several offline approximation algorithms are designed to solve specific combinatorial problems under particular reward and constraint assumptions. In Appendix E, we study various ones and demonstrate their resilience, characterizing each by their corresponding parameters: $\alpha, \beta, \gamma, \psi$, and $\delta$. Given the resilience of an offline algorithm, one can directly apply Theorem 5.3 to ascertain the corresponding worst-case online guarantees when extending that offline algorithm to online settings.

Remark 4.4. An equivalent definition to resilient approximation was proposed by Nie et al. (2023) as a robust approximation. However, the resilient approximation provides more information about the algorithm's complexity. Specifically, an $(\alpha-\epsilon, \delta)$-robust-approximation algorithm that requires a number of oracle calls equal to $\frac{\psi}{\epsilon^{\beta}} \log ^{\gamma}\left(\frac{1}{\epsilon}\right)$, then it is an $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm. Furthermore, an $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm is an $(\alpha-\epsilon, \delta)$-robust-approximation algorithm, with $\epsilon \geq 0$.

Generally, the offline algorithms are designed for deterministic (noiseless) functions. However, in real-world applications, access to a noiseless reward function is not always possible, often due to the inherently stochastic nature of the problem. For example, recommending the same set of products to different people, or even to the same person at different times, may not yield consistent outcomes and rewards. This noise could also stem from using a stochastic approximation of the oracle function for computational efficiency, as seen in the case of stochastic data summarization discussed in Section 7. Therefore, in such cases, resilience against noisy rewards is necessary.

We demonstrate in Theorem 5.3 that resilience is, in fact, sufficient to ensure that the offline algorithm can be adjusted to

```
Algorithm 1 C-MA-MAB
    Input: Horizon $T$, actions $\mathbb{S}$, agents $\mathbb{A}^{\prime}$, number $m$,
    $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm $\mathcal{A}(\epsilon)$
    Initialize $r_{i}^{\star} \leftarrow\left\lceil m^{-1}\left(\delta \sqrt{\log (T)}\left(\frac{T m}{\psi}\right)^{\frac{1}{\beta+1}}\right)^{\frac{2+2 \beta}{3+\beta}}\right]$
    Initialize $\epsilon^{\star} \leftarrow\left(\frac{\psi r_{i}^{\star}}{T}\right)^{\frac{1}{\beta+1}} \mathbf{1}_{\{\beta>0 \text { OR } \gamma>0\}}$.
    \# Multi-Agent Exploration Time
    Server starts running $\mathcal{A}\left(\epsilon^{\star}\right), j \leftarrow 0$
    while $\mathcal{A}\left(\epsilon^{\star}\right)$ queries the value of some $A \subseteq \mathbb{S}$ do
        Server broadcasts action $A$ to the agents, $j \leftarrow j+1$
        Server randomly selects a set $\mathbb{A} \subseteq \mathbb{A}^{\prime}$ of $m$ agents
        for agent $i$ in $\mathbb{A}^{\prime}$ in parallel do
            For $r_{i}^{\star}$ times, play action $A$
            If $i \in \mathbb{A}$, agent tracks $\&$ upload local mean $\bar{f}_{i}$
        end for
        Server calculates the mean $\bar{f}$ and feeds it to $\mathcal{A}\left(\epsilon^{\star}\right)$
    end while
    \# Multi-Agent Exploitation Time
    Server broadcasts $\Theta$, the final output of $\mathcal{A}\left(\epsilon^{\star}\right)$
    for agent $i$ in $\mathbb{A}^{\prime}$ in parallel do
        for remaining time of the agent $i$ do
            Play action $\Theta$
        end for
    end for
```

achieve online sub-linear regret guarantees. In what follows, we explain how we use a resilient approximation algorithm $\mathcal{A}(\epsilon)$ as a subroutine in our proposed framework.

### 4.2. Offline-to-Online Multi-Agent Framework

We present our proposed C-MA-MAB Framework; see Algorithm 1. This framework is applicable for both singleagent and multi-agent settings, wherein the design of our algorithm ensures that the single-agent setting is simply a special case of the multi-agent scenario. For a set $\mathbb{S}$ of possible actions, a set of $m^{\prime} \geq 1$ agents $\mathbb{A}^{\prime}$, a number $m \geq 1$ of communicating agents, and a time horizon $T$, our algorithm can adapt any off-the-shelf, offline single-agent combinatorial $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm $\mathcal{A}(\epsilon)$ to the C-MA-MAB setting. This adaptation comes with theoretical guarantees, as detailed in Theorem 5.3, applicable under any reward type or action constraint.

While the offline algorithm may be a function of some variable $\epsilon \geq 0$, which trades off its complexity and approximation guarantees, our proposed algorithm finds $\epsilon^{\star}$ which optimizes this trade-off based on the problem parameters and its complexity to minimize regret and uses $\mathcal{A}\left(\epsilon^{\star}\right)$ for exploration instead. Furthermore, in the exploration time,
whenever the offline algorithm $\mathcal{A}\left(\epsilon^{\star}\right)$ requests the value oracle for action $A$, each agent of the $m^{\prime} \geq 1$ agents plays the action $A$ for $r_{i}^{\star}=r^{\star} / m$ times, where $r^{\star}$ is another parameter chosen based on the setting and the subroutine complexity to minimize regret, with

$$
\begin{equation*}
r^{\star}=m\left\lceil m^{-1}\left(\delta \sqrt{\log (T)}\left(\frac{T m}{\psi}\right)^{\frac{1}{\beta+1}}\right)^{\frac{2+2 \beta}{3+\beta}}\right\rceil \tag{2}
\end{equation*}
$$

Furthermore, we choose $\epsilon^{\star}$ as follows:

$$
\begin{equation*}
\epsilon^{\star}=\left(\frac{\psi r^{\star}}{T m}\right)^{\frac{1}{\beta+1}} \mathbf{1}_{\{\beta>0 \text { OR } \gamma>0\}} \tag{3}
\end{equation*}
$$

Only the randomly selected $m$ agents track and broadcast their local estimations to the server, i.e., each agent $i \in \mathbb{A}$ sends its local estimation $\bar{f}_{i}$, then the server aggregates these estimations in one global estimation $\bar{f}$ of rewards for $A$ and then returns $\bar{f}$ to $\mathcal{A}\left(\epsilon^{\star}\right)$. Finally, in the exploitation phase, all the agents in $\mathbb{A}^{\prime}$ play $\Theta$, the output from algorithm $\mathcal{A}\left(\epsilon^{\star}\right)$, for the remaining time.

Remark 4.5. C-MA-MAB has low storage complexity. In every step, an agent needs to store, at most, $n$ indices and a real value representing the empirical mean of one action. Only the action $\Theta$ is stored during exploitation time, and no additional computation is required. During exploration time, each agent needs to store only the proposed action $A$ and update its associated empirical mean; everything is deleted once the server proposes another action. Furthermore, the proposed framework does not require the combinatorial offline algorithm $\mathcal{A}(\epsilon)$ to have any particular structure and employs $\mathcal{A}\left(\epsilon^{\star}\right)$ as a black-box algorithm. Consequently, it shares the same complexity as the subroutine $\mathcal{A}\left(\epsilon^{\star}\right)$. Moreover, $\mathcal{A}\left(\epsilon^{\star}\right)$ is executed on the server, alleviating the computational overhead for the agents.

Remark 4.6. In the multi-agent setting, we assume that the server can either be a separate entity or one of the agents playing the role of the server. In the single-agent setting, without loss of generality, the sole agent can be considered as the server. The server orchestrates communication by selecting clients and recommending which actions to explore, based on the offline subroutine, in a synchronized manner. Recent studies have explored federated linear and kernelized contextual bandits with asynchronous communication (Li \& Wang, 2022; He et al., 2022; Li et al., 2023). Future research might investigate general combinatorial optimization with asynchronous communication.

Remark 4.7. The proposed C-MA-MAB uses the time horizon $T$ to compute $r^{\star}$ and $\epsilon^{\star}$. When the exact time horizon is unknown, the results can be enhanced by employing the concept of an anytime algorithm through the use of the geometric doubling trick by establishing a geometric sequence of time intervals, denoted as $T_{i}$, where $T_{i}=T_{0} 2^{i}$ for $i \in \mathbb{N}$, where $T_{0}$ is a sufficiently large value to ensure proper initialization. From Theorem 4 in (Besson \& Kaufmann, 2018), it follows that the regret bound preserves the $T^{\frac{2+\beta}{3+\beta}}$ dependence with only changes in constant factors.

## 5. Theoretical Analysis

We upper-bound the required communication rounds, we lower-bound the probability of the empirical mean being $\xi$-controlled-estimation of the expectation (good event), and upper-bound the expected cumulative $\alpha$-regret.

### 5.1. Communication Analysis

By the design of the algorithm, the $m$ randomly selected agents, after locally estimating the quality of a suggested action $A$, communicate only one value, representing the local estimation $\bar{f}_{i}$. The agents exploit the decided set $\Theta$ during the exploitation phase without further communication. During exploration, the selected agents must communicate their local estimation for every requested action $A$. Therefore, the number of communication rounds is upper-bounded by the number of requested actions, i.e., the required oracle calls $N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)$, which we upper-bound in the following lemma, which we prove in Appendix B.

Lemma 5.1. The number of communication times, i.e., the number of oracle queries $N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)$ of the subroutine $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm $\mathcal{A}\left(\epsilon^{\star}\right)$ satisfies: $N\left(\beta, \gamma, \psi, \epsilon^{\star}\right) \leq \mathcal{O}\left(\psi T^{\frac{\beta}{\beta+1}} \log ^{\gamma}(T)\right)$.

From Lemma 5.1 it follows that for cases where $\beta=0$, which applies to several offline algorithms (Nemhauser et al., 1978; Khuller et al., 1999; Sviridenko, 2004; Buchbinder et al., 2015; Mirzasoleiman et al., 2015; Yaroslavtsev et al., 2020), the communication rounds are at most $\widetilde{\mathcal{O}}(\psi)$, scaling at most logarithmically with $T$. For example, as shown in Corollary 6.1, with $n$ arms and using RandomizedUSM (Buchbinder et al., 2015) as a subroutine (where $\psi$ is $n, \beta$ is zero, and $\gamma$ is zero), our framework guarantees a communication complexity of $\mathcal{O}(n)$, not scaling with $T$.

By design of the C-MA-MAB algorithm, after every action queried by the subroutine, the agents have to explore and estimate the values of the proposed action. To do that, each agent has to play the proposed action for $r_{i}^{\star}$ times. Therefore, using the result from Lemma 5.1 on the number of required communication rounds and by the definition of $r_{i}^{\star}$, we can derive that the required exploration steps for every agent, i.e., $\mathcal{O}\left(r_{i}^{\star} \psi T^{\frac{\beta}{\beta+1}} \log ^{\gamma}(T)\right)$, which is decreasing with an increasing number of agents $m$.

### 5.2. Estimation Analysis

In C-MA-MAB, every agent plays each action queried by the subroutine $\mathcal{A}\left(\epsilon^{\star}\right)$ the same number of times. These repetitions provide an estimation of the action values. We
define a good event $\mathcal{E}$ when the empirical mean estimation $\bar{f}$ is a $\xi$-controlled-estimation of the reward expectation $f$, on the played actions during exploration time, with $\xi:=\sqrt{\log (T) / r^{\star}}$. For every communication round $j$, each action $A_{j}$ queried by the $(\alpha, \beta, \gamma, \psi, \delta)$-resilientapproximation $\mathcal{A}\left(\epsilon^{\star}\right)$, where $j \in\left\{1, \cdots, N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)\right\}$, we define the event $\mathcal{E}_{j}$ as:

$$
\begin{equation*}
\mathcal{E}_{j} \triangleq\left\{\left|\bar{f}\left(A_{j}\right)-f\left(A_{j}\right)\right| \leq \xi\right\} \tag{4}
\end{equation*}
$$

Therefore, the good event $\mathcal{E}$, which considers the empirical mean estimation $\bar{f}$ is a $\xi$-controlled estimate of the reward expectation $f$ for every communication round $j$, i.e., considers the realization of $\mathcal{E}_{j}$ for every $j \in$ $\left\{1, \cdots, N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)\right\}$, which is expressed as follows:

$$
\begin{equation*}
\mathcal{E}=\mathcal{E}_{1} \cap \cdots \cap \mathcal{E}_{N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)} \tag{5}
\end{equation*}
$$

Each action $A$ queried by the offline algorithm have been explored for $r^{\star}$ number of times among the agents. These $r^{\star}$ rewards are i.i.d. with expectation $f(A)$ and confined within the $[0,1]$ range. Consequently, we can bound the deviation of the empirical mean $\bar{f}\left(A_{j}\right)$ from the expected value $f\left(A_{j}\right)$ for every action undertaken. Thus, we upper bound the probability of the good event in the following lemma, which we prove in Appendix C.

Lemma 5.2. The probability of the good event $\mathcal{E}$, (5), when using an $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm $\mathcal{A}\left(\epsilon^{\star}\right)$ as a subroutine satisfies:

$$
\mathbb{P}(\mathcal{E}) \geq 1-2 N\left(\beta, \gamma, \psi, \epsilon^{\star}\right) T^{-2}
$$

Combining both results of Lemma 5.1 and Lemma 5.2 it follows that the bad event happens with a probability of at most $\tilde{\mathcal{O}}\left(\psi T^{\frac{\beta}{\beta+1}-2}\right)$, decreasing as $T$ increases.

### 5.3. Regret Analysis

We analyze the expected cumulative $\alpha$-regret for the C-MAMAB (Algorithm 1), with $m$ communicating agents.

Theorem 5.3. For the sequential combinatorial decisionmaking problem defined in Section 2, with $T \geq$ $\max \left\{\psi m, \psi m^{\frac{1+\beta}{2}} / \delta^{\beta+1}\right\}$, the expected cumulative $\alpha$ regret of the $C$-MA-MAB presented in Algorithm 1 using an $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithm $\mathcal{A}\left(\epsilon^{\star}\right)$ as subroutine is at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3+\beta}} \delta^{\frac{2}{3+\beta}} \psi^{\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right)$.

The above theorem implies that an offline algorithm does not need to have an $\alpha$-approximation guarantee to be adapted to achieve sublinear $\alpha$-regret guarantees. In fact, an $(\alpha-\epsilon)$ approximation algorithm, denoted as $\mathcal{A}(\epsilon)$ with $\epsilon>0$, if it is a $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation, can be extended to a sub-linear $\alpha$-regret algorithm. Later, in Section 6, we apply the above theorem to special combinatorial cases.
Remark 5.4. Linear speedup is evident in our approach, as the collective regret across $m$ agents is $\tilde{\mathcal{O}}\left(\delta^{\frac{2}{3+\beta}} \psi^{\frac{1}{3+\beta}}(T m)^{\frac{2+\beta}{3+\beta}}\right)$. This mirrors the regret one would observe if all $\mathrm{m}$ agents collaborated, sharing a total of $T m$ time for the central agent. Consequently, the distributed setup incurs no loss, with each agent interacting with the environment $T$ times, and the combined regret reflects that of a single agent allocated $T m$ time for interaction.

Remark 5.5. The $\delta$ function depends on the offline algorithm and refers to a general function of the combinatorial problem parameters, such as the number of main arms, $n$, or the cardinality constraint, $k$. This function serves as the scaling factor for the radius $\xi$ in the lower bound on the expected reward, as given by $\left(\mathbb{E}[f(\Theta)] \geq(\alpha-\epsilon) f\left(S^{\star}\right)-\delta \xi\right)$, as defined in Definition 4.2 for $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation algorithms. In some offline algorithms, we have demonstrated that this scaling function depends on the cardinality constraint $k$. For example, Lemma E. 4 shows that $\delta$ equals $4 k$ for RANDOMSAMPLING. In other cases, the function may depend solely on the number of main arms $n$. For instance, Lemma E. 1 establishes that $\delta$ is $\frac{5}{2} n$ for the RandomizeDUSM. Our analysis accommodates any $\delta$ function defined in terms of the problem parameters.

Remark 5.6. When algorithm approximations do not depend on $\epsilon$, it implies that their complexity is of the form $\tilde{\mathcal{O}}(\psi)$, hence $\beta=\gamma=0$. It follows that using such an $(\alpha, 0,0, \psi, \delta)$-resilient-approximation algorithm as a subroutine achieves a regret of at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} \delta^{\frac{2}{3}} \psi^{\frac{1}{3}} T^{\frac{2}{3}}\right)$.

Remark 5.7. A lower bound remains an open question for general combinatorial stochastic rewards under bandit feedback. A lower bound is missing even for the special cases of stochastic submodular rewards under bandit feedback. Some lower bounds have been proposed in restrictive special settings. For example, Niazadeh et al. (2021) showed a $\tilde{\Omega}\left(T^{\frac{2}{3}}\right)$ lower bound for adversarial submodular rewards, where the reward could only be observed in user-specified exploration rounds. Moreover, Tajdini et al. (2023) demonstrated that for monotone stochastic submodular bandits with a cardinality constraint, for small time horizon $T$, a regret scaling like $T^{\frac{2}{3}}$ is inevitable when compared to the greedy algorithm in (Nemhauser et al., 1978). However, it does not provide a lower bound on $(1-1 / e)$-regret.

In the following, we provide a sketch of the proof and leave a detailed one in Appendix D. We separate the proof into two cases. One case when the good event $\mathcal{E}$ happens, which we show in Lemma 5.2 happens with high probability and then we generalize the result under any event.

### 5.3.1. REGRet oF AN AGENT UNDER THE Good EVENT

We upper-bound the expected $\alpha$-regret conditioned on the good event $\mathcal{E}$. However, for simplicity in notation, we employ $\mathbb{E}[\cdot]$ rather than $\mathbb{E}[\cdot \mid \mathcal{E}]$ in certain instances. We de-
compose and then bound the expected $\alpha$-regret into two components: one addressing the regret stemming from exploration $\left(P_{1}\right)$ and the other from exploitation $\left(P_{2}\right)$.

$$
\begin{align*}
& \mathbb{E}\left[\mathcal{R}_{i}(T) \mid \mathcal{E}\right]=\sum_{t=1}^{T}\left(\alpha f\left(\mathrm{~S}^{\star}\right)-\mathbb{E}\left[f\left(S_{i, t}\right)\right]\right)  \tag{6}\\
& =\underbrace{\sum_{j=1}^{N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)} r_{i}^{\star}\left(\alpha f\left(\mathrm{~S}^{\star}\right)-\mathbb{E}\left[f\left(A_{j}\right)\right]\right)}_{\alpha \text {-regret from exploration }\left(P_{1}\right)} \\
& +\underbrace{\sum_{t=T_{N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)}^{T}}^{T}\left(\alpha f\left(\mathrm{~S}^{\star}\right)-\mathbb{E}[f(\Theta)]\right)}_{\alpha \text {-regret from exploitation }\left(P_{2}\right)} \tag{7}
\end{align*}
$$

We begin with bounding regret from exploration and using that the rewards are within the interval $[0,1]$,

$$
\begin{equation*}
P_{1} \leq \sum_{j=1}^{N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)} \frac{r^{\star}}{m} \alpha \leq N\left(\beta, \gamma, \psi, \epsilon^{\star}\right) \frac{r^{\star}}{m} \tag{8}
\end{equation*}
$$

When the good event $\mathcal{E}$ occurs, we know that $\mid \bar{f}(A)-$ $f(A) \mid \leq \xi$ for all considered action $A$. Using an $(\alpha, \beta, \gamma, \psi, \delta)$-resilient-approximation $\mathcal{A}\left(\epsilon^{\star}\right)$, with output $\Theta$, we have

$$
\begin{equation*}
\alpha f\left(S^{\star}\right)-\mathbb{E}[f(\Theta)] \leq \delta \xi+\epsilon^{\star} f\left(S^{\star}\right) \tag{9}
\end{equation*}
$$

Therefore, with $f\left(\mathrm{~S}^{\star}\right)<1$, we have:

$$
\begin{equation*}
P_{2} \leq \sum_{t=T_{N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)}+1}^{T}\left(\delta \xi+\epsilon^{\star}\right) \leq T\left(\delta \xi+\epsilon^{\star}\right) \tag{10}
\end{equation*}
$$

Therefore, using Eq. (8) and Eq. (10), the total expected cumulative regret in Eq. (7) can be bounded as:

$$
\begin{equation*}
\mathbb{E}\left[\mathcal{R}_{i}(T) \mid \mathcal{E}\right] \leq N\left(\beta, \gamma, \psi, \epsilon^{\star}\right) \frac{r^{\star}}{m}+T\left(\delta \xi+\epsilon^{\star}\right) \tag{11}
\end{equation*}
$$

Using the confidence radius $\xi=\sqrt{\log (T) / r^{\star}}$ and $N\left(\beta, \gamma, \psi, \epsilon^{\star}\right)=\psi \frac{1}{\epsilon^{\star \beta}} \log ^{\gamma}\left(\frac{1}{\epsilon^{\star}}\right)$, we have

$$
\mathbb{E}\left[\mathcal{R}_{i}(T) \mid \mathcal{E}\right] \leq \frac{\psi r^{\star} \log ^{\gamma}\left(\frac{1}{\epsilon^{\star}}\right)}{\epsilon^{\star \beta} m}+T\left(\sqrt{\frac{\delta^{2} \log (T)}{r^{\star}}}+\epsilon^{\star}\right)
$$

We note that the above inequality is correct for all values of $r^{\star} \geq m$ and $\epsilon^{\star} \geq 0$ with the convention that $0^{0}=1$. In our algorithm, we choose the values of $r^{\star}$ and $\epsilon^{\star}$ as functions of the problem parameters and on the subroutine complexity parameters. We choose $r^{\star}$ as defined in Eq. (2) and $\epsilon^{\star}$ as defined in Eq. (3).

Recall that $\beta \geq 0$ and $\gamma \in\{0,1\}$. Therefore, we consider all the possible cases, the first when $\beta=\gamma=0$, the second when $\beta=0$ and $\gamma=1$, and the third when $\beta>0$. For all the cases, when the good event $\mathcal{E}$ happens, the expected $\alpha$ regret of our C-MA-MAB with an $(\alpha, \beta, \gamma, \psi, \delta)$-resilientapproximation as subroutine we have

$$
\begin{equation*}
\mathbb{E}\left[\mathcal{R}_{i}(T) \mid \mathcal{E}\right] \leq \tilde{\mathcal{O}}\left(\delta^{\frac{2}{3+\beta}} \psi^{\frac{1}{3+\beta}} m^{-\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right) \tag{12}
\end{equation*}
$$

### 5.3.2. Regret of an AgEnt Under Any EVENT

Given that the reward $f_{t}(\cdot)$ is upper bound by 1 , the expected cumulative $\alpha$-regret when the event $\overline{\mathcal{E}}$ happens over a range $T$ is upper-bounded as follows: $\mathbb{E}\left[\mathcal{R}_{i}(T) \mid \overline{\mathcal{E}}\right] \leq T$. Combining the results when good event happens and does not happen, using the law of total expectation, and using Eq. (12), Lemma 5.1, Lemma 5.2, and $T \geq m \psi$ :

$$
\begin{aligned}
\mathbb{E}\left[\mathcal{R}_{i}(T)\right] & =\mathbb{E}\left[\mathcal{R}_{i}(T) \mid \mathcal{E}\right] \cdot \mathbb{P}(\mathcal{E})+\mathbb{E}\left[\mathcal{R}_{i}(T) \mid \overline{\mathcal{E}}\right] \cdot \mathbb{P}(\overline{\mathcal{E}}) \\
& \leq \tilde{\mathcal{O}}\left(\delta^{\frac{2}{3+\beta}} \psi^{\frac{1}{3+\beta}} m^{-\frac{1}{3+\beta}} T^{\frac{2+\beta}{3+\beta}}\right)
\end{aligned}
$$

This establishes the result in Theorem 5.3.

## 6. Application to Submodular Maximization

We use our C-MA-MAB framework to address scenarios involving stochastic submodular ${ }^{3}$ rewards, with bandit feedback. Submodular maximization (SM) is an NP-hard problem (Nemhauser et al., 1978; Feige et al., 2011), which recently has shown growing interest in studying combinatorial MAB (Chen et al., 2018; Niazadeh et al., 2021; Nie et al., 2022; Fourati et al., 2023a; 2024). In the following, we present our results for SM for monotone ${ }^{4}$ and non-monotone rewards with and without a cardinality constraint and leave the knapsack constraint in Appendix E.4.

For unconstrained SM (USM), Buchbinder et al. (2015) proposed RANDOMIZEDUSM, achieving a $\frac{1}{2}$-approximation. In Lemma E. 1 in Appendix E, we generalize Corollary 2 in (Fourati et al., 2023a) to show its resilience and present the following corollary, which recovers the guarantees of the online algorithm in (Fourati et al., 2023a) for a single agent and generalizes it for multi-agent setting.

Corollary 6.1. $C$-MA-MAB, using the RANDOMIZEDUSM as a subroutine, needs at most $\mathcal{O}(n)$ communication times and its $\frac{1}{2}$-regret is at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} n T^{\frac{2}{3}}\right)$ for USM.

For SM under a cardinality constraint $k$ (SMC) with monotone rewards, the GreEDY in (Nemhauser \& Wolsey, 1978) achieves $1-1$ / $e$. In contrast, the StochaStic-GREEDY[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_d9ec934efbf1a36aa39bg-09.jpg?height=485&width=661&top_left_y=232&top_left_x=271)

Figure 1: Cumulative regrets of summarizing images from CIFAR10 for different horizons $T$ using our C-MA-MAB framework with different number of agents $m$, against C-ETC-N and UCB1.

in (Mirzasoleiman et al., 2015) achieves $1-1 / e-\epsilon$, where $\epsilon$ is a parameter balancing accuracy and complexity. We provide the resilience of these two algorithms in Appendix $\mathrm{E}$ and present the following results.

Corollary 6.2. C-MA-MAB, using the GREEDY as a subroutine, needs $\mathcal{O}(n k)$ communication times and its $(1-1 / e)$ regret is at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} k n^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ for monotone SMC.

Corollary 6.3. $C$-MA-MAB, using the StochAsticGREEDY, needs $\tilde{\mathcal{O}}(n)$ communication times and its $\left(1-\frac{1}{e}\right)$ regret is at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{3}} k^{\frac{2}{3}} n^{\frac{1}{3}} T^{\frac{2}{3}}\right)$ for monotone SMC.

The StochASTIC-GREEDY algorithm has sub-optimal approximation guarantees of $(1-1 / e-\epsilon)$; thus, using the $\mathrm{C}$ ETC framework from (Nie et al., 2023) can only guarantee sub-linear $(1-1 / e-\epsilon)$-regret. Consequently, $(1-1 / e)$ regret will be linear in $T$. However, our C-MA-MAB guarantees sublinear $(1-1 / e)$-regret, recovering the single agent's results in (Fourati et al., 2024). Furthermore, the two corollaries above demonstrate that employing a sub-optimal approximation algorithm in terms of the approximation factor, rather than one that achieves optimal approximation guarantees, does not necessarily imply lower regret guarantees, as shown in (Fourati et al., 2024).

For non-monotone SMC, the RANDOMSAMPLING algorithm in (Buchbinder et al., 2017) achieves $1 / e-\epsilon$, where $\epsilon$ is a parameter balancing accuracy and complexity. We provide the resilience of this algorithm in Appendix E and derive the first result for single-agent and multi-agent online stochastic non-monotone SMC with sublinear regrets.

Corollary 6.4. $C-M A-M A B$, using the RANDOM SAMPLING in (Buchbinder et al., 2017) as a subroutine, needs $\tilde{\mathcal{O}}\left(n T^{\frac{2}{3}}\right)$ communication times and its $\left(\frac{1}{e}\right)$-regret is at most $\tilde{\mathcal{O}}\left(m^{-\frac{1}{5}} k^{\frac{2}{5}} n^{\frac{1}{5}} T^{\frac{4}{5}}\right)$ for non-monotone SMC.

## 7. Experiments with Data Summarization

We employ our C-MA-MAB on data summarization, a primary challenge in machine learning (Mirzasoleiman et al., 2013), mainly when dealing with a large dataset. While this problem has been widely studied with access to a deterministic oracle (Lin \& Bilmes, 2011; Mirzasoleiman et al., 2013; 2015; 2020; Sivasubramanian et al., 2024), this work is the first to address online data summarization under a stochastic objective function. We run experiments on FMNIST (Xiao et al., 2017) and CIFAR10 (Krizhevsky et al., 2009), present the latter in the main paper, and relegate more details and results to Appendix F.

In data summarization, an action $A$ consists of a set of at most $k$ images to summarize a large dataset $\mathcal{D}$. Adding more images achieves better summarization but follows a diminishing return property. Thus, it falls in the monotone SMC (Mirzasoleiman et al., 2015). Evaluating a given action $A$ against a dataset $\mathcal{D}$ may become expensive with a large dataset. Thus, we consider a stochastic objective where the chosen subset is compared only to a random subset $\mathcal{R} \subseteq \mathcal{D}$ drawn uniformly at random from $\mathcal{D}$, using a similarity metric $C$, resulting in noisier but lower complexity evaluations. We do not solve the problem for a given realization $\mathcal{R}$, but we solve it in expectation: $\arg \max \mathbb{E}_{\mathcal{R}}\left[\sum_{i \in \mathcal{R}} \max _{v \in \mathcal{A}} C(i, v)\right]$. $\mathcal{A} \subseteq \mathcal{D}:|\mathcal{A}| \leq k$

We test our method when using the StochAStic-GREEDY algorithm as a subroutine (Mirzasoleiman et al., 2015) for one agent and multiple agents and compare it to the proposed algorithm in C-ETC framework for SMC (C-ETC-N) (Nie et al., 2023), and the upper confidence bound (UCB1) algorithm (Auer et al., 2002).

The C-MA-MAB demonstrates sub-linear regret guarantees as depicted in Fig. 1. Additionally, it is apparent that, for varying values of $m$, the C-MA-MAB consistently outperforms both C-ETC-N and UCB1, even with a single agent, exhibiting lower regrets over diverse time horizons. Notably, an increase in the number of agents correlates with a reduction in regret for these agents. These observations reinforce the same conclusions drawn from the theoretical analysis.

## Conclusion

We introduce C-MA-MAB, a framework for single-agent and multi-agent online stochastic combinatorial problems, which adapts resilient offline $(\alpha-\epsilon)$-approximation algorithms to online algorithms under bandit feedback, achieving sublinear $\alpha$-regret bounds with respect to the time horizon $T$, eliminating the $\epsilon$ error, and ensuring a linear speedup. We also present specialized bounds for SM with and without constraints and apply C-MA-MAB to online stochastic data summarization.

## Impact Statement

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## References

Agarwal, M., Aggarwal, V., and Azizzadenesheli, K. Multiagent multi-armed bandits with limited communication. The Journal of Machine Learning Research, 23(1):95299552,2022 .

Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235-256, 2002.

Balakrishnan, R., Li, T., Zhou, T., Himayat, N., Smith, V., and Bilmes, J. Diverse client selection for federated learning via submodular maximization. In International Conference on Learning Representations, 2022.

Barbosa, R., Ene, A., Nguyen, H., and Ward, J. The power of randomization: Distributed submodular maximization on massive datasets. In International Conference on Machine Learning, pp. 1236-1244. PMLR, 2015.

Besson, L. and Kaufmann, E. What doubling tricks can and can't do for multi-armed bandits. arXiv preprint arXiv:1803.06971, 2018.

Buchbinder, N., Feldman, M., Seffi, J., and Schwartz, R. A tight linear time (1/2)-approximation for unconstrained submodular maximization. SIAM Journal on Computing, 44(5):1384-1402, 2015.

Buchbinder, N., Feldman, M., and Schwartz, R. Comparing apples and oranges: Query trade-off in submodular maximization. Mathematics of Operations Research, 42(2): 308-329, 2017.

Chawla, R., Sankararaman, A., Ganesh, A., and Shakkottai, S. The gossiping insert-eliminate algorithm for multiagent bandits. In International conference on artificial intelligence and statistics, pp. 3471-3481. PMLR, 2020.

Chen, L., Xu, J., and Lu, Z. Contextual combinatorial multiarmed bandits with volatile arms and submodular reward. Advances in Neural Information Processing Systems, 31: 3247-3256, 2018.

Edmonds, J. Submodular functions, matroids, and certain polyhedra. In Combinatorial Optimization-Eureka, You Shrink!, pp. 11-26. Springer, 2003.

Elgabli, A., Issaid, C. B., Bedi, A. S., Rajawat, K., Bennis, M., and Aggarwal, V. Fednew: A communicationefficient and privacy-preserving newton-type method for federated learning. In International Conference on Machine Learning, pp. 5861-5877. PMLR, 2022.

Feige, U. A threshold of $\ln n$ for approximating set cover. Journal of the ACM (JACM), 45(4):634-652, 1998.

Feige, U., Mirrokni, V. S., and Vondrák, J. Maximizing non-monotone submodular functions. SIAM Journal on Computing, 40(4):1133-1153, 2011.

Fourati, F., Aggarwal, V., Quinn, C., and Alouini, M.-S. Randomized greedy learning for non-monotone stochastic submodular maximization under full-bandit feedback. In International Conference on Artificial Intelligence and Statistics, pp. 7455-7471. PMLR, 2023a.

Fourati, F., Kharrat, S., Aggarwal, V., Alouini, M.-S., and Canini, M. FilFL: Client filtering for optimized client participation in federated learning. arXiv preprint arXiv:2302.06599, 2023b.

Fourati, F., Quinn, C. J., Alouini, M.-S., and Aggarwal, V. Combinatorial stochastic-greedy bandit. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 12052-12060, 2024.

Goemans, M. X. and Williamson, D. P. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):1115-1145, 1995.

He, J., Wang, T., Min, Y., and Gu, Q. A simple and provably efficient algorithm for asynchronous federated contextual linear bandits. Advances in neural information processing systems, 35:4762-4775, 2022.

Hoeffding, W. Probability inequalities for sums of bounded random variables. In The collected works of Wassily Hoeffding, pp. 409-426. Springer, 1994.

Hosseinalipour, S., Brinton, C. G., Aggarwal, V., Dai, H., and Chiang, M. From federated to fog learning: Distributed machine learning over heterogeneous wireless networks. IEEE Communications Magazine, 58(12):4147,2020 .

Iwata, S., Fleischer, L., and Fujishige, S. A combinatorial strongly polynomial algorithm for minimizing submodular functions. Journal of the ACM (JACM), 48(4): $761-777,2001$.

Khuller, S., Moss, A., and Naor, J. S. The budgeted maximum coverage problem. Information processing letters, $70(1): 39-45,1999$.

Konečnỳ, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.

Korte, B. H., Vygen, J., Korte, B., and Vygen, J. Combinatorial optimization, volume 1. Springer, 2011.

Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Canadian Institute for Advanced Research, 2009. URL http://www.cs.toronto.edu/ kriz/cifar.html, 2009.

Lattimore, T. and Szepesvári, C. Bandit algorithms. Cambridge University Press, 2020.

Li, C. and Wang, H. Asynchronous upper confidence bound algorithms for federated linear bandits. In International Conference on Artificial Intelligence and Statistics, pp. 6529-6553. PMLR, 2022.

Li, C., Wang, H., Wang, M., and Wang, H. Learning kernelized contextual bandits in a distributed and asynchronous environment. In International Conference on Learning Representation, 2023.

Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429-450, 2020.

Lin, H. and Bilmes, J. A class of submodular functions for document summarization. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pp. 510-520, 2011.

McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273-1282. PMLR, 2017.

Mirzasoleiman, B., Karbasi, A., Sarkar, R., and Krause, A. Distributed submodular maximization: Identifying representative elements in massive data. Advances in Neural Information Processing Systems, 26, 2013.

Mirzasoleiman, B., Badanidiyuru, A., Karbasi, A., Vondrák, J., and Krause, A. Lazier than lazy greedy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.

Mirzasoleiman, B., Bilmes, J., and Leskovec, J. Coresets for data-efficient training of machine learning models. In International Conference on Machine Learning, pp. 6950-6960. PMLR, 2020.

Nemhauser, G. L. and Wolsey, L. A. Best algorithms for approximating the maximum of a submodular set function. Mathematics of operations research, 3(3):177-188, 1978.

Nemhauser, G. L., Wolsey, L. A., and Fisher, M. L. An analysis of approximations for maximizing submodular set functions-i. Mathematical programming, 14(1):265294, 1978 .

Niazadeh, R., Golrezaei, N., Wang, J. R., Susan, F., and Badanidiyuru, A. Online learning via offline greedy algorithms: Applications in market design and optimization. In Proceedings of the 22nd ACM Conference on Economics and Computation, pp. 737-738, 2021.

Nie, G., Agarwal, M., Umrawal, A. K., Aggarwal, V., and Quinn, C. J. An explore-then-commit algorithm for submodular maximization under full-bandit feedback. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.

Nie, G., Nadew, Y. Y., Zhu, Y., Aggarwal, V., and Quinn, C. J. A framework for adapting offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback. International Conference on Machine Learning, 2023.

Sivasubramanian, D., Nagalapatti, L., Iyer, R., and Ramakrishnan, G. Gradient coreset for federated learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2648-2657, 2024.

Slivkins, A. et al. Introduction to multi-armed bandits. Foundations and Trends $®$ in Machine Learning, 12(12):1-286, 2019 .

Sviridenko, M. A note on maximizing a submodular set function subject to a knapsack constraint. Operations Research Letters, 32(1):41-43, 2004.

Tajdini, A., Jain, L., and Jamieson, K. Minimax optimal submodular optimization with bandit feedback. arXiv preprint arXiv:2310.18465, 2023.

Takemori, S., Sato, M., Sonoda, T., Singh, J., and Ohkuma, T. Submodular bandit problem under multiple constraints. In Conference on Uncertainty in Artificial Intelligence, pp. 191-200. PMLR, 2020.

Wang, P.-A., Proutiere, A., Ariu, K., Jedra, Y., and Russo, A. Optimal algorithms for multiplayer multi-armed bandits. In International Conference on Artificial Intelligence and Statistics, pp. 4120-4129. PMLR, 2020.

Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

Yaroslavtsev, G., Zhou, S., and Avdiukhin, D. "bring your own greedy"+ max: Near-optimal 1/2-approximations for submodular knapsack. In International Conference on Artificial Intelligence and Statistics, pp. 3263-3274. PMLR, 2020.
