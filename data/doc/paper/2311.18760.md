# TASKBENCH: BENCHMARKING LARGE 

 LANGUAGE MODELS FOR TASK AUTOMATIONYongliang Shen ${ }^{1 *}$, Kaitao Song $^{2 *}$, Xu Tan ${ }^{2}$, Wenqi Zhang ${ }^{1}$,<br>Kan Ren ${ }^{2}$, Siyu Yuan ${ }^{3}$, Weiming $\mathbf{L u}^{1}$, Dongsheng $\mathbf{L i}^{2}$, Yueting Zhuang ${ }^{1}$<br>Zhejiang University ${ }^{1}$, Microsoft Research Asia ${ }^{2}$, Fudan University ${ }^{3}$<br>\{syl,luwm,zhangwenqi\}@zju.edu.cn, syyuan21@m.fudan.edu.cn<br>\{kaitaosong, xuta, dongsli\}@microsoft.com

https://github.com/microsoft/JARVIS


#### Abstract

Recently, the incredible progress of large language models (LLMs) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. However, there lacks a systematic and standardized benchmark to foster the development of LLMs in task automation. To this end, we introduce TASKBENCH to evaluate the capability of LLMs in task automation. Specifically, task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction to fulfill user intent. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To generate high-quality evaluation datasets, we introduce the concept of Tool Graph to represent the decomposed tasks in user intent, and adopt a back-instruct method to simulate user instruction and annotations. Furthermore, we propose TASKEvAL to evaluate the capability of LLMs from different aspects, including task decomposition, tool invocation, and parameter prediction. Experimental results demonstrate that TASKBENCH can effectively reflects the capability of LLMs in task automation. Benefiting from the mixture of automated data construction and human verification, TASKBENCH achieves a high consistency compared to the human evaluation, which can be utilized as a comprehensive and faithful benchmark for LLM-based autonomous agents ${ }^{1}$.


## 1 INTRODUCTION

Due to the recent advances of large language models (LLMs) Brown et al., 2020; Ouyang et al. 2022; OpenAI, 2023; Touvron et al. 2023a, Anil et al., 2023), LLM-empowered autonomous agents (e.g., AutoGPT (Gravitas, 2023), HuggingGPT (Shen et al., 2023), BabyAGI (Nakajima, 2023), TaskMatrix.AI (Liang et al. 2023)) have unveiled remarkable potential towards artificial general intelligence and become a new rising trend in the realm of AI research. Generally, within the realm of LLM-empowered autonomous agents, task automation is considered as the most important component, which aims to leverage LLMs to autonomously analyze user instructions and accomplish their objectives. Consequently, many researchers attempt to delve deeper into LLM to enable more intelligent task automation. However, it is worth noting that a critical challenge in advancing this area is the lack of a systematic and standardized benchmark to thoroughly evaluate the capability of LLMs in automating tasks. Therefore, creating such a benchmark to facilitate research in this area has become an urgent need.

Nevertheless, it is non-trivial to build such a benchmark for task automation since its setting is closer to real-world scenarios that makes the collection of evaluation data and the design of evaluation[^0]("script.txt") for a voiceover. I need to download the video, extract the audio, add the voiceover to the video using the provided script, and finally combine the extracted audio with an additional audio file ("new.wav").
![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-02.jpg?height=750&width=1376&top_left_y=276&top_left_x=382)

Figure 1: LLM-based task automation and our TASKBENCH evaluation. Task automation implies that LLM-based agents use task decomposition, tool invocation, and parameter prediction to autonomously complete tasks. The evaluation process unfolds as follows: (1) Given a user request, the large language model carries out task decomposition and predicts the tool invocation graph; (2) TaskBench assesses the capability of LLMs in task decomposition based on the decomposed subtasks; 3) For the predicted tool invocation graph, TaskBench evaluates the accuracy of the tool nodes, edges, and parameters.

metrics more challenging than conventional NLP tasks. Figure 1 illustrates a simple example to outline the pipeline of task automation, and we can have these observations:

- In contrast to conventional NLP tasks, the procedure to fulfill task automation usually requires multiple stages (e.g., task decomposition, tool invocation, and parameter prediction of tools). This indicates that we need to take all of these elements into consideration when building benchmark datasets and evaluation metrics;
- In particular, the implementation of task automation necessitates a broader consideration of realworld scenarios. As a result, the user instruction could be complex. For example, it could be composed of multiple sub-tasks with complex task dependencies. And sometimes, its task scope could demand advanced functions beyond language and thus require the utilization of external tools. These issues also verify the difficulty in constructing the benchmark;
- Furthermore, to better fulfill task automation, it requires LLMs to derive multiple task elements (e.g., decomposed tasks, invoked tools, and tool parameters). These also compel us to devise more advanced evaluation metrics tailored to assess the performance of LLMs in these dimensions.

Therefore, in light of these observations, we found that the majority of existing benchmarks fall short of adequately showcasing the full potential of LLMs in autonomous task completion. For example, conventional benchmarks like GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a), encompass a lot of specific tasks to evaluate the capability of LLMs in a single scenario, while cannot well reflect the versatility of task automation. Some other researchers attempted to advocate for more rigorous benchmarks (e.g., MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AGIEval (Zhong et al. 2023)) by involving more general scenarios (e.g., exams). But all of them can only reflect the capability of language skills, and are not able to manifest the capability of LLMs in task automation. Besides, how to conduct the evaluation for task automation is still a troublesome problem. Therefore, in this paper, we expect to develop a benchmark with appropriate evaluation metrics to better evaluate LLMs in task automation.

To this end, we present TASKBENCH to benchmark the capability of LLMs in the realm of task automation. Specifically, as aforementioned, the data collection for task automation requires us to consider different sophisticated settings, which makes it more challenging. However, just as shown
in Figure 1. compared with directly simulating user requests, LLMs usually need to parse tasks for automation, and these parsed tasks (i.e., decomposed tasks, invoked tools, and parameters) are easier to collect and construct. Therefore, we raise a simple idea: is it possible to synthesize user instruction based on the expected parsed tasks?

To fulfill this, we first present the concept of Tool Graph (TG), which gathers diverse tools to address specific tasks. Specifically, every two tools in TG can have a connection if they have a dependency. The structure of the whole tool connections can considered as a graph. Therefore, to simulate user instruction, we can randomly sample a sub-graph from TG to represent the expected task list in user instruction and then apply a back-instruct strategy to generate the final user instruction. During the generation, we provide three different architectures for sampling to enable better controllability, which are node, chain, and directed acyclic graph (DAG). Moreover, a self-critic mechanism is appended to further refine the data quality of our dataset by reviewing their consistency. To guarantee diversity, our data generation is applied to three domains (e.g., Hugging Face (Wolf et al., 2019), multimedia, and daily life) to formulate our TASKBENCH for evaluating LLM in task automation.

After building the dataset, another challenge is how to effectively and quantitatively evaluate the capability of LLMs in task automation. We note that the primary steps of LLMs in automating tasks include task decomposition, tool invocation, and parameter prediction. Therefore, we further propose an evaluation system, called TASKEVAL, which encompasses a series of metrics to provide objective evaluations to measure the capability of LLMs in task decomposition, tool invocation, and predicting the parameters of tools. Moreover, we also conduct human evaluation to prove the positive correlation of our evaluation with human assessment.

Overall, the contributions of our paper can be summarized as:

- We introduce TaskBench, a new benchmark to support the development of LLM in task automation, which comprises a novel data generation to address the data deficiency in this area;
- We further present TASKEVAL to effectively and quantitatively evaluate the capability of LLMs in automating tasks from different aspects, including task decomposition, tool invocation, and parameter predictions;
- The experimental results on different LLMs and additional dataset analysis demonstrate that our proposed TASKBENCH can effectively reflect the capability of LLMs in multiple dimensions with the support of TASKEVAL and show high correlations with human evaluation.


## 2 TASKBENCH DATASET

In this section, we introduce the construction of TASKBENCH, the benchmark meticulously designed to facilitate the development of LLMs in task automation. Specifically, unlike previous methods which use collection or instruction methods, TASKBENCH can consider the complex relationships among multiple tasks to simulate more practical and complex user instruction. Figure 2 illustrates the entire process of our method to build the datasets. More details will be introduced in the following subsections.

### 2.1 PRELIMINARY

Task automation aims to fulfill complex user instructions in real-world scenarios. In this setting, the user instructions could encompass multiple sub-tasks, and the execution of each sub-task can be completed by invoking a tool (Schick et al. 2023). Besides, there could also remain some temporal or resource dependencies among these sub-tasks. Therefore, we think that each user instruction can be represented as a combination of tools with connections like graph structure, just as shown in Figure 1 Consequently, we introduce the concept of the Tool Graph (TG), which will be used in our benchmark construction. The tool graph can be viewed as a structured representation that centers on tools with their dependency. Here, we assume a tool as $t$ and denote a TG as $\mathcal{G}=\{T, D\}$, where $T=\left\{t_{1}, t_{2}, \ldots, t_{n}\right\}$ represents the collection of tools, and $D$ is a collection of $\left\{\left(t_{a}, t_{b}\right)\right\}$ that means tool $t_{a}$ exhibits a dependency on tool $t_{b}$. To some extent, the tool graph offers a novel approach to organizing tools, capturing the relationships between different tools more effectively than traditional taxonomy trees. In the next subsection, we will introduce how to build a tool graph and utilize it to formulate our benchmark.

### 2.2 DATASET CONSTRUCTION

To accomplish user intent, LLMs usually adopt a stepwise process (e.g., task decomposition $\rightarrow$ tool invocation $\rightarrow$ parameter prediction) to analyze the user request and convert it into multiple executable tasks. Therefore, it is essential to construct the dataset and allow LLMs to evaluate their automation capability in the above process.

To guarantee that the generated user instructions could cover the expected tasks and dependencies, we adopt a back-instruct strategy to simulate data. More specifically, it can summarized as three steps: 1) we first collect a tool repository and build a tool graph $\mathcal{G}$ with a collection of tools and their dependencies; 2) then we sample a sub-graph from $\mathcal{G}$, to obtain a specified structure; 3) based on the sampled tool sub-graph, we use LLMs to generate user instruction via back-instruct. More details are introduced as below.

### 2.2.1 TOOL GRAPH CONSTRUCTION

Building a tool graph requires us to collect many standalone tools from different sources. When combining different tools together, the dependencies among tools could be diverse, encompassing resource dependencies, temporal dependencies, environment dependencies, and so on. In our research, we mainly investigate two of them: resource and temporal dependencies. For the former one, it means the two tools can have a connection if the input type of tool $t_{a}$ can match the output type of tool $t_{b}$. For the latter one, we devise tool graphs that highlight temporal dependencies, allowing any two tools to be linked to illustrate their order. In this work, we choose three scenarios to build the datasets for our benchmark:

Hugging Face Hugging Face Wolf et al. (2019) provides a wide variety of AI models to cover massive tasks across language, vision, audio, video, and so on. Each task defined by Hugging Face can be viewed as a tool to address a specific task. Specifically, each tool in Hugging Face has determined the type of its input and output. Hence, if tool $t_{a}$ and $t_{b}$ have a connection, the input type of $t_{a}$ should match the output type of $t_{b}$. Guided by this principle, we constructed Hugging Face's tool graph, comprising 23 tools and 225 edges.

Multimedia In contrast to the Hugging Face tools, which are tailored for AI tasks, the multimedia tools is broader in scope. It provides more user-centric tools like file downloader, video editor, and so on. The policy for tool connections is the same as the Hugging Face domain. Finally, we could construct a tool graph over multimedia tools with 40 nodes and 449 edges.

Daily Life APIs Sometimes, we also need some daily life services, including web search, shopping, and etc. Hence, these daily life APIs can also be considered as tools for specific tasks. However, it is worth noting that the type of dependencies among these APIs is predominantly temporal. Therefore, two daily life APIs have a successive order if they are connected. In this scenario, we can build a tool graph with 40 nodes and 1,560 edges.

Please note that our method's applicability extends beyond the scenarios mentioned above. We present more details about these tool graphs on different domains in Appendix A. 10

### 2.2.2 Sampling ON TOOL GRaPH

Based on the above steps, we can sample a sub-graph from the constructed TG and keep the connections of sampled tools from the TG to capture the dependencies between tools. Following the setting of HuggingGPT, we categorize the sub-structure of a TG into three types: node, chain, and directed acyclic graph (DAG). Each type embodies a specific pattern for tool invocation:

- Node represents standalone tool invocations, suitable for addressing simple tasks necessitating only a single tool.
- Chain corresponds to sequential tool invocations, where tools need to be stepwise executed to complete a task.
- DAG depicts more intricate tool invocations. A tool might rely on multiple preceding tools or influence several subsequent tools.

![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-05.jpg?height=556&width=1399&top_left_y=275&top_left_x=360)

Figure 2: Construction of the TASKBENCH: Initially, we transform the toolbox into a tool graph by creating connections between tools based on their dependencies (either resource or temporal dependencies). Subsequently, we sample diverse subgraphs from the tool graph, which may be single nodes, chains, or directed acyclic graphs. Utilizing the sampled tool subgraphs (which encompass the tools and their interrelations), we "back-instruct" the large language model to inversely craft user instructions, task steps, and tool invocation graphs. Furthermore, we implement critics to evaluate the consistency of the generated tool invocation graphs with the sampled tool subgraphs.

By sampling sub-graphs from these three substructures, we can emulate a variety of valid tool invocation patterns for user instruction. We represent the tool subgraph in $\mathcal{G}$ as $\mathcal{G}_{s}=\left\{T_{s}, D_{s}\right\}$, where $T_{s}=\left\{t_{s 1}, t_{s 2}, \ldots, t_{s k}\right\}$ with $k<n$ and $D_{s}=\left\{\left(t_{s a}, t_{s b}\right)\right\}$, such that $t_{s a}$ and $t_{s b}$ belong to $T_{s}$. The sampling of the tool graph can be described as:

$$
\begin{equation*}
\text { Sample }(\mathcal{G}, \text { mode, size }) \rightarrow \mathcal{G}_{s} \tag{1}
\end{equation*}
$$

where the mode specifies the sampling mode (e.g., Nodes, Chains, DAGs), and the size indicates the number of tools (Here we set its range as $\{1,2, \ldots, 10\}$ ). These factors determine the topological nature and magnitude of the tool sub-graph in user instructions, respectively.

### 2.2.3 BACK-INSTRUCT

Next, based on the sampled sub-graph $\mathcal{G}_{s}$, we use LLMs to synthesize user instructions. We term this process BACK-INSTRUCT, which can considered as a data engine to convert the sampled tools into user instruction. Specifically, given a sampled subgraph $\mathcal{G}_{s}$, we formulate the following BACKINSTRUCT procedure, empowering LLMs to produce the corresponding instructions:

$$
\begin{equation*}
\operatorname{BackInstruct}_{1}\left(\mathcal{G}_{s}=\left(T_{s}, D_{s}\right)\right) \rightarrow \text { Instruction. } \tag{2}
\end{equation*}
$$

Here, the sampled sub-graph $\mathcal{G}_{s}$ can instruct LLMs to generate user requests covering these related sub-tasks, and further with their dependencies. Such a strategy ensures the complexity and quality of the generated data.

Specifically, we note that sampled sub-graphs can only provide information on tool invocation skeletons, lacking the critical parameters for tool execution. Therefore, based on the generated instruction in Eqn. 2. we encourage the LLM to populate the parameters for the tool subgraphs and generate the final tool invocation graph along with the corresponding task decomposition steps:

$$
\begin{equation*}
\operatorname{BackInstruct}_{2}\left(\mathcal{G}_{s}=\left(T_{s}, D_{s}\right), \text { Instruction }\right) \rightarrow\{\text { Task Steps, Tool Invocation Graph }\} \tag{3}
\end{equation*}
$$

After that, we introduce a self-critic mechanism to check and filter out the generated instruction to guarantee quality. Here, we offer two variants: LLM-based and rule-based. The former aims to use LLM to check the alignments between the generated data and the sampled tool sub-graph. While the latter uses straightforward rules to determine the alignment between the tool graphs in created data and the sampled tool graphs. Here, we use the nodes and edges of the sampled graph to determine the consistency. Figure 2illustrates each step of our data engine to simulate user instructions. More detailed designs about our data engine are provided in the Appendix A. 8

### 2.3 StATISTICS OF THE DATASETS

Based on the above steps, we build TASKBENCH across three domains, which use GPT-4 as the data engine. The ratio of different modes (i.e., Node, Chain, DAG) is set as $3: 7: 8$ for sampling and the ratio for the number of different tools is set as $\{0.1,0.2,0.3,0.2,0.1,0.05,0.025,0.025,0.025\}$. Table 1 reports the statistical information of the tool graph and the datasets across three domains. Notably, it is evident that the two critics we introduced play a crucial role in improving data quality. The rule-based and LLM-based critics respectively filter out an average of $15.13 \%$ and $22.73 \%$ of the samples. In addition, we invited human experts to revise and filter the data. And finally, we obtained $61.76 \%, 62.71 \%$, and $60.42 \%$ of the aligned samples for the three datasets, respectively.

Table 1: Statistics for the TASKBENCH. We report the number of nodes and links of the tool graphs. "\# Avg. Nodes" and "\# Avg. Links" stands for the average number of nodes and links involved in one sample. We also report the sample number and average request length for the datasets.

| Statistic |  | Hugging Face Tools | Multimedia Tools | Daily Life APIs |
| :---: | :---: | :---: | :---: | :---: |
| {\# Nodes of Tool Graph <br> \# Links of Tool Graph} |  | 23 | 40 | 40 |
|  |  | 225 | 449 | 1,560 |
| \# Avg. Nodes |  | 3.47 | 3.68 | 3.82 |
| \# Avg. Links |  | 2.46 | 2.68 | 2.8 |
| \# Samples |  | 12,217 | 8,904 | 7,150 |
| - Node / Chain / DAG |  | $3,270 / 4,302 / 4,645$ | $2,117 / 3,145 / 3,642$ | $1,277 / 2,716 / 3,157$ |
| Avg. Request Length |  | 41.21 | 39.15 | 38.64 |
| - Node / Chain / DAG |  | $28.42 / 45.72 / 46.04$ | $24.71 / 43.55 / 43.73$ | $12.36 / 44.49 / 44.23$ |
| 灾: | Both | $8,456(69.22 \%)$ | $6,281(70.54 \%)$ | $5,432(75.97 \%)$ |
|  | LLM-based critic | $9,042(74.01 \%)$ | $6,959(78.16 \%)$ | $5,694(79.63 \%)$ |
|  | Rule-based critic | $10,289(84.22 \%)$ | $7,363(82.69 \%)$ | $6,271(87.70 \%)$ |
| Human Verification |  | 7,546 (61.76\%) | $5,584(62.71 \%)$ | $4.320(60.42 \%)$ |

### 2.4 EVALUATION OF THE DATASET QUALITY

To demonstrate the quality of TASKBENCH datasets, we conducted in-depth human evaluations based on generated samples. We also conduct a case study and error analysis on the constructed datasets. Please refer to Appendix A. 3 and Appendix A. 4 for more details.

Evaluation Metrics To assess the quality of datasets constructed by Back-Instruct, we designed three metrics in our evaluation criteria. Two measure the quality of instructions, and one evaluates tool invocation graphs:

- Metrics for Instruction:
- Naturalness: This metric measures the reasonableness of the instructions, including the commonality of dependencies between tools and their alignment with real-world needs.
- Complexity: This metric assesses the complexity of the instructions, considering factors such as task depth, the number of involved tools, and the relationships between these tools.
- Metric for Tool Invocation Graphs:
- Alignment: Building upon the Feasibility metric, this measures how well the tool invocation graphs align with the instructions, i.e., whether the tool invocation graphs can effectively address the user's commands.

Each metric is scored from 1 to 5 , and we design these metrics to assess the effectiveness and faithfulness of our TASKBENCH in task automation.

Comparison with Baselines To make a fair comparison, we choose two additional baselines to compare our Back-Instruct:

- Back-Instruct (Ours): we sample tool subgraphs and then backtranslate to instructions and further refine the tool invocation graph.
- Back-Instruct w/o edges: compared with our Back-Instruct, we eliminated edges from our sampled tool subgraphs, preserving only the tool node information in the prompt.
- Self-Instruct: (Wang et al. 2023) based on manually labeled demonstrations and all tools with descriptions, we directly employed GPT-4 to autonomously select tools and then generate the instructions with tool invocation graphs.

Evaluation Results During the human evaluation, we randomly selected 50 samples from our TASKBENCH and invited three domain experts to assess the quality of these samples. To ensure a fair and unbiased evaluation, all samples will be anonymized. We provide canonical samples for these experts to calibrate their criteria during the annotations, and calculate an average score of all experts' ratings as the final results. All results can be found in Table 2 .

Table 2: Human evaluation (rating from 1 to 5) on samples constructed by different methods. Average score rating from three human experts.

| Methods | Naturalness $\uparrow$ | Complexity $\uparrow$ | Alignment $\uparrow$ | Overall $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: |
| Back-Instruct | 3.89 | 4.01 | 3.66 | 3.85 |
| Back-Instruct w/o edges | 3.44 | 3.27 | 3.62 | 3.44 |
| Self-Instruct | 2.18 | 2.01 | 3.64 | 2.61 |

We observed that all methods (Self-Instruct or Back-Instruct) can guarantee the alignment. However, our method, Back-Instruct, scored highest in Naturalness and Complexity. We attribute these superiorities to the realistic resource or temporal dependencies in the sampled tool subgraphs, which allow us to generate more natural instructions in complex scenarios (e.g., multi-tools utilization).

## 3 TASKEVAL

We could collect ample samples (i.e., synthetic user instructions) with annotations (i.e., sampled tool sub-graph) to evaluate the capability of LLMs in automating tasks. Here, we introduce TASKEvaL, which encompasses a series of evaluation metrics to measure LLMs in multiple dimensions, including task decomposition, tool invocation, and parameter prediction. To simulate the process of LLMs in automating tasks, we adopt a standard prompt for each LLM, which enables it to first disassemble user requests into multiple sub-tasks (i.e., task decomposition), and then predict tool invocations with their parameters and task dependencies to generate a tool invocation graph. Based on the built datasets and the standard inference process, we design pertinent metrics to evaluate three stages (i.e., task decomposition, tool invocation, and parameter predictions) in task automation. Here, we choose the GPT family (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023) and open-source LLMs (Touvron et al., 2023a; Chiang et al., 2023; Rozière et al., 2023; Xu et al., 2023; Yang et al. 2023) as our main evaluation. Please see the Appendix A. 7 for full evaluations of other open-source LLMs (Team, 2023a, Li et al., 2023a; Team, 2023b).

### 3.1 TASK DECOMPOSITION

Task decomposition is a pivotal component of task automation. By decomposing user instruction into a sequence of executable sub-tasks, the autonomous agent can more effectively fulfill user intent. During the task decomposition, each step will generate textual descriptions. Here, we use three subjective metrics to measure the quality in analyzing sub-tasks: Rouge-1 (R1), Rouge-2 (R2), and BertScore F1 (BsF) (Zhang et al. 2019). The results are reported in Table 3 We observe that the GPT-4 model significantly outperforms the open-source LLM model in task decomposition, achieving approximately $20 \%+$ higher than others in Rouge-1/2. Moreover, we find that codellama$13 \mathrm{~b}$ achieves the closest performance to the GPT family. We attribute the substantial code pretraining endowing it with a higher-level task decomposition capability compared to other LLMs.

### 3.2 TOOL INVOCATION

The graph of tool invocation can be viewed as a concrete representation of task steps in user instruction, specifying the appropriate tool for each step. To orchestrate external tools effectively, the

Table 3: Evaluation for task decomposition. We compare the text descriptions between the generated and real task steps in terms of Rouge-1 (R1), Rouge-2 (R2), and BertScore F1 (BsF).

| LLM | TASK DECOMPOSITION - Step-by-step task decomposition |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Hugging Face Tools |  |  | Multimedia Tools |  |  | Daily Life APIs |  |  |
|  | $R 1 \uparrow$ | $R 2 \uparrow$ | $B s F \uparrow$ | $R 1 \uparrow$ | $R 2 \uparrow$ | $B s F \uparrow$ | $R 1 \uparrow$ | $R 2 \uparrow$ | $B s F \uparrow$ |
| gpt-4 | 52.42 | 30.38 | 90.12 | 60.84 | 40.08 | 91.19 | 85.07 | 72.36 | 96.91 |
| claude-2 | 44.21 | 21.12 | 88.71 | 48.85 | 23.59 | 89.22 | 82.26 | 69.88 | 96.64 |
| text-davinci-003 | 36.68 | 17.61 | 87.03 | 49.23 | 27.97 | 89.21 | 68.27 | 50.30 | 93.59 |
| gpt-3.5-turbo | 42.99 | 21.58 | 88.47 | 49.66 | 28.51 | 89.54 | 58.53 | 39.90 | 91.29 |
| codellama-13b | 38.75 | 18.37 | 88.32 | 44.46 | 23.30 | 88.66 | 89.86 | 83.27 | 97.90 |
| wizardlm-13b | 34.47 | 15.38 | 87.38 | 35.87 | 17.55 | 87.29 | 82.02 | 72.43 | 96.36 |
| vicuna-13b-v1.5 | 37.12 | 17.03 | 87.90 | 44.75 | 23.75 | 88.94 | 81.76 | 71.76 | 96.31 |
| nous-hermes-13b | 37.36 | 16.91 | 88.18 | 35.73 | 16.11 | 87.53 | 78.49 | 68.04 | 95.61 |
| baichuan-13b-chat | 19.93 | 5.97 | 83.85 | 20.41 | 3.77 | 83.31 | 49.43 | 27.25 | 88.32 |
| llama-2-13b-chat | 39.37 | 18.64 | 88.67 | 26.16 | 7.88 | 84.82 | 45.39 | 22.42 | 87.74 |

tool invocation graph should provide these pieces of information: 1) the dependency between tools to guarantee the order of executable sub-tasks; 2) the parameters that tools require. Therefore, it is necessary for us to measure the capability of LLMs in these aspects. Here, we first evaluate the predicted graph structure to measure the capability of LLMs in this subsection.

In a tool invocation graph, we denote the nodes as tools and the edge as the dependency between two tools. Therefore, we can evaluate the nodes and edges to measure the capability of LLMs in tool invocation. Here, we crafted two distinct metrics: Node F1 ( $\boldsymbol{n}-\mathbf{F 1}$ ) for node prediction and Edge F1 (e-F1) for edge prediction. We also introduced the Normalized Edit Distance (NED) metric for chain structure, quantifying the adjustments needed to correct predictions. The results are detailed in Table 4 revealing that predicting edges in the tool invocation graph is more challenging than predicting nodes, with an F1 score difference of about $30 \%$ across all LLMs. Furthermore, we also observe that tasks with varying structures pose different challenges for LLMs. For instance, on simple node structures, open-source LLMs match the performance of gpt-3.5-turbo and text-davinci003, but lag behind on more complex tasks. Overall, these designed metrics can effectively help us to better measure the capability of LLMs in task automation.

### 3.3 PARAMETER PREDICTION

We further divide the evaluation of the tool parameter prediction as twofold. The first metric is Parameter Name F1 (t-F1) to evaluate the capability of LLMs in predicting the parameter of the tools. Another one is the Parameter Name \& Value F1 ( $\boldsymbol{v}-\mathbf{F 1}$ ) to measure both the parameter and its value. The results can be found in Table 5 . We found that the precise prediction of parameters can determine the successful execution of tools, and the precise prediction of both parameters and values can guarantee the correctness of the tool invocation. Besides, we found that GPT-4 can significantly outperform open-source LLMs, which achieve $v$ - $F 1$ scores of $60.86 \%, 72.31 \%$, and $71.14 \%$ across the three domains. These results also highlight the limitations of current open-source LLMs in task automation, and which parts should be enhanced.

### 3.4 HUMAN EVALUATION

To ensure the reliability of TASKBENCH evaluations, we further investigate their consistency with human evaluations. We randomly select 50 samples from the constructed instructions and then enforce the evaluated LLMs to generate responses. We then compares pairwise LLMs and summarizes their ranking based on the executability and problem-solving efficacy of the tool invocation graphs. To illustrate the alignment between TASKBENCH and human evaluations, we used two metrics: Kendall's $\tau$ and Spearman's $\rho$. The results are shown in Table 6 We find that the average values for Kendall's $\tau$ and Spearman's $\rho$ are 0.89 and 0.78 , respectively. This indicates a very positive correlation between human evaluation and our TASKBENCH, which further validates the effectiveness of our proposed framework for dataset construction.

Table 4: Evaluation for tool invocation. Node F1 ( $n-F 1$ ) for node prediction and Edge F1 (e-F1) for edge prediction. For nodes, a prediction is deemed positive if the predicted node's ID aligns with any of the ground-truth node labels. For edges, both the source and target nodes of a predicted edge must correspond exactly. Normalized Edit Distance (NED) measures the normalized number of operations required to correct the prediction for chain structure.

| TOOL INVOCATION - Tool invocation graph prediction. |  |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | LLM | $\frac{\text { Node }}{n-F 1 \uparrow}$ | Chain |  |  | DAG |  | Overall |  |
|  |  |  | $n-F 1 \uparrow$ | $e-F 1 \uparrow$ | $N E D \downarrow$ | $\overline{n-F 1 \uparrow}$ | $e-F 1 \uparrow$ | $\overline{n-F 1 \uparrow}$ | $e-F 1 \uparrow$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-09.jpg?height=381&width=57&top_left_y=649&top_left_x=377) | gpt-4 | 84.34 | 80.79 | 55.73 | 39.70 | 82.86 | 56.39 | 81.54 | 54.70 |
|  | claude-2 | 69.83 | 80.67 | 48.11 | 40.03 | 84.52 | 53.40 | 79.00 | 43.51 |
|  | gpt-3.5-turbo | 56.91 | 72.63 | 39.92 | 46.52 | 73.79 | 38.55 | 69.49 | 33.36 |
|  | text-davinci-003 | 40.71 | 66.05 | 36.04 | 48.57 | 64.64 | 34.19 | 59.38 | 29.37 |
|  | codellama-13b | 43.68 | 55.65 | 17.80 | 62.23 | 52.87 | 13.19 | 53.16 | 14.64 |
|  | nous-hermes-13b | 58.66 | 52.39 | 9.01 | 62.48 | 51.99 | 6.33 | 53.62 | 8.29 |
|  | baichuan-13b-chat | 58.29 | 52.82 | 8.07 | 61.52 | 53.29 | 7.82 | 53.85 | 7.65 |
|  | vicuna-13b-v1.5 | 51.74 | 50.37 | 8.40 | 66.83 | 52.46 | 9.06 | 50.82 | 7.28 |
|  | llama-2-13b-chat | 43.59 | 49.87 | 8.22 | 64.99 | 49.60 | 9.11 | 48.47 | 7.30 |
|  | wizardlm-13b | 54.69 | 54.50 | 2.22 | 60.55 | 52.93 | 0.92 | 54.40 | 2.05 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-09.jpg?height=377&width=57&top_left_y=1030&top_left_x=377) | gpt-4 | 97.13 | 89.70 | 69.29 | 28.93 | 92.32 | 71.64 | 90.90 | 69.27 |
|  | claude-2 | 66.16 | 83.95 | 59.22 | 33.41 | 82.98 | 54.28 | 80.94 | 53.01 |
|  | text-davinci-003 | 59.15 | 76.87 | 50.79 | 38.54 | 79.00 | 50.69 | 73.97 | 45.81 |
|  | gpt-3.5-turbo | 53.55 | 76.81 | 50.30 | 39.05 | 78.65 | 49.52 | 72.83 | 44.02 |
|  | codellama-13b | 43.70 | 66.89 | 28.77 | 46.35 | 68.68 | 28.79 | 62.78 | 24.61 |
|  | vicuna-13b-v1.5 | 66.64 | 59.18 | 16.49 | 54.17 | 61.40 | 13.95 | 60.61 | 14.78 |
|  | nous-hermes-13b | 60.58 | 58.53 | 9.47 | 56.02 | 59.39 | 9.57 | 58.97 | 8.90 |
|  | baichuan-13b-chat | 45.59 | 41.96 | 4.95 | 64.28 | 42.05 | 8.46 | 42.51 | 5.19 |
|  | wizardlm-13b | 55.13 | 50.57 | 4.92 | 58.46 | 49.38 | 5.52 | 51.24 | 4.82 |
|  | llama-2-13b-chat | 38.02 | 45.14 | 1.62 | 65.29 | 45.95 | 2.11 | 43.87 | 1.63 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-09.jpg?height=370&width=60&top_left_y=1410&top_left_x=375) | gpt-4 | 95.97 | 97.06 | 83.47 | 38.69 | 96.41 | 42.01 | 96.91 | 80.53 |
|  | claude-2 | 79.57 | 95.36 | 80.68 | 39.93 | 93.85 | 41.04 | 93.52 | 75.31 |
|  | gpt-3.5-turbo | 52.18 | 90.80 | 70.66 | 43.50 | 86.94 | 30.85 | 85.37 | 60.67 |
|  | text-davinci-003 | 68.49 | 82.15 | 60.12 | 47.14 | 76.81 | 24.54 | 80.42 | 54.90 |
|  | codellama-13b | 89.75 | 87.80 | 65.92 | 44.42 | 83.61 | 27.47 | 87.73 | 63.16 |
|  | llama-2-13b-chat | 34.11 | 57.61 | 20.13 | 67.06 | 56.18 | 8.42 | 55.77 | 17.02 |
|  | wizardlm-13b | 92.27 | 65.74 | 14.51 | 55.80 | 63.80 | 9.20 | 69.34 | 14.18 |
|  | vicuna-13b-v1.5 | 90.59 | 73.74 | 13.24 | 51.43 | 67.92 | 5.62 | 75.67 | 12.48 |
|  | baichuan-13b-chat | 52.50 | 52.60 | 11.59 | 69.27 | 52.08 | 6.53 | 52.55 | 10.61 |
|  | nous-hermes-13b | 92.50 | 71.17 | 3.55 | 53.47 | 70.65 | 2.86 | 73.45 | 3.50 |

### 3.5 ANALYSIS

Factors Contributing to Task Automation Performance Our analysis identifies two primary factors impacting the performance of LLMs in task automation: 1) Reasoning Capabilities: LLMs vary significantly in their abilities to solve complex problems and reason effectively, which are crucial for task automation. For instance, GPT series demonstrate superior reasoning abilities in math and coding tasks, which indicates enhanced skills in task planning and tool utilization. 2) Instruction Following: Our analysis reveals that models fine-tuned with instruction-following settings, such as Vicuna-13b, WizardLLM-13b, and Nous-Hermes-13b, outperform the baseline Llama-2-13b model in task automation. Furthermore, WizardLLM-13b can perform better than Vicuna-13b due to more complex instruction fine-tuning, which demonstrate the necessity of instruction following.

Intrinsic Differences in LLMs in Performing Task Automation Our findings suggest that: 1) Code Pre-training: We note that the models (Code-Llama) with more code-pretraining outperform other open-source LLMs in task automation. Experimental results show an average improvement of $4.45 \%$ in tool prediction ( $n-f l$ ) and $12.76 \%$ in parameter prediction ( $v-f l$ ) across various domain datasets. We conclude that task automation usually involves multiple stages which makes it need structural text as the interface to connect different stages; 2) Alignment Techniques: Besides, the

Table 5: Evaluation for parameter prediction of tools. $t$-F1 evaluate the pair of (task, parameter name), $v-F 1$ evaluate the triple of (task, parameter name, parameter value).

| TOol ParameTER Prediction - Predicts parameters for the tool execution. |  |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | LLM | Node |  | Chain |  | $\overline{\text { DAG }}$ |  | Overall |  |
|  |  | $\overline{t-F 1 \uparrow}$ | $v-F 1 \uparrow$ | $\overline{t-F 1 \uparrow}$ | $v-F 1 \uparrow$ | $\overline{t-F 1 \uparrow}$ | $v-F 1 \uparrow$ | $\overline{t-F 1 \uparrow}$ | $v-F 1 \uparrow$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-10.jpg?height=373&width=43&top_left_y=551&top_left_x=388) | gpt-4 | 80.05 | 74.10 | 76.66 | 58.15 | 78.24 | 60.03 | 77.31 | 60.86 |
|  | claude-2 | 48.07 | 32.14 | 66.35 | 45.57 | 68.59 | 48.19 | 63.00 | 43.08 |
|  | text-davinci-003 | 38.51 | 27.43 | 56.90 | 38.76 | 57.03 | 38.90 | 52.53 | 36.04 |
|  | gpt-3.5-turbo | 37.70 | 19.81 | 60.96 | 41.15 | 61.33 | 42.89 | 55.88 | 36.32 |
|  | codellama-13b | 20.09 | 12.58 | 36.40 | 21.31 | 33.43 | 20.48 | 32.06 | 18.87 |
|  | nous-hermes-13b | 46.38 | 31.06 | 35.55 | 13.81 | 33.06 | 13.69 | 37.51 | 17.66 |
|  | wizardlm-13b | 43.97 | 25.90 | 37.34 | 12.48 | 38.43 | 13.79 | 38.76 | 15.35 |
|  | llama-2-13b-chat | 29.80 | 20.54 | 32.14 | 13.57 | 32.16 | 15.23 | 31.61 | 15.38 |
|  | baichuan-13b-chat | 46.18 | 29.46 | 30.29 | 9.55 | 30.10 | 10.37 | 33.17 | 13.53 |
|  | vicuna-13b-v1.5 | 25.71 | 13.11 | 28.99 | 11.14 | 30.04 | 13.60 | 28.34 | 11.85 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-10.jpg?height=381&width=47&top_left_y=928&top_left_x=384) | gpt-4 | 95.64 | 87.12 | 85.60 | 69.83 | 87.57 | 72.79 | 87.06 | 72.31 |
|  | claude-2 | 53.81 | 24.02 | 75.60 | 58.12 | 72.41 | 52.43 | 71.63 | 51.58 |
|  | gpt-3.5-turbo | 44.94 | 11.96 | 70.53 | 47.76 | 71.82 | 47.95 | 65.91 | 40.80 |
|  | text-davinci-003 | 60.30 | 20.78 | 69.91 | 44.76 | 71.91 | 45.76 | 68.48 | 40.70 |
|  | codellama-13b | 32.01 | 16.10 | 52.30 | 32.51 | 53.08 | 33.79 | 48.19 | 29.13 |
|  | vicuna-13b-v1.5 | 52.72 | 35.55 | 39.31 | 21.00 | 40.05 | 21.40 | 41.62 | 23.62 |
|  | nous-hermes-13b | 50.11 | 37.80 | 41.98 | 17.89 | 43.99 | 20.04 | 43.60 | 21.69 |
|  | wizardlm-13b | 49.79 | 33.59 | 36.88 | 14.87 | 36.61 | 18.68 | 39.10 | 18.74 |
|  | baichuan-13b-chat | 40.41 | 27.87 | 25.80 | 8.50 | 25.87 | 10.13 | 28.04 | 11.77 |
|  | llama-2-13b-chat | 28.49 | 17.01 | 30.26 | 9.66 | 31.00 | 11.35 | 29.99 | 11.32 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6846cd93f27563c0cc61g-10.jpg?height=370&width=57&top_left_y=1311&top_left_x=384) | gpt-4 | 95.83 | 76.21 | 97.23 | 70.67 | 95.95 | 69.65 | 97.02 | 71.14 |
|  | claude-2 | 78.12 | 59.43 | 94.72 | 65.30 | 91.83 | 66.39 | 92.71 | 64.72 |
|  | gpt-3.5-turbo | 43.81 | 28.77 | 89.21 | 61.11 | 83.88 | 56.13 | 81.97 | 55.66 |
|  | text-davinci-003 | 61.68 | 45.53 | 80.68 | 54.54 | 76.51 | 51.91 | 78.37 | 53.40 |
|  | codellama-13b | 86.34 | 71.20 | 84.31 | 61.51 | 80.42 | 60.1 | 84.26 | 62.38 |
|  | vicuna-13b-v1.5 | 83.63 | 67.71 | 61.80 | 44.54 | 57.14 | 41.72 | 64.27 | 47.31 |
|  | nous-hermes-13b | 79.69 | 63.29 | 62.64 | 45.32 | 63.26 | 45.74 | 64.47 | 47.22 |
|  | wizardlm-13b | 89.27 | 72.96 | 50.68 | 36.48 | 49.03 | 35.75 | 55.00 | 40.53 |
|  | baichuan-13b-chat | 32.47 | 21.72 | 38.31 | 24.24 | 36.84 | 21.84 | 37.48 | 23.77 |
|  | llama-2-13b-chat | 10.39 | 7.32 | 38.89 | 25.37 | 36.43 | 23.40 | 35.11 | 22.94 |

Table 6: Alignment of TASKBENCH with human evaluation. Kendall's $\tau$ alculates the proportion of aligned pairs, while Spearman's $\rho$ measures the correlation between the ranks of elements.

| Correlation Metric | Hugging Face Tools | Multimedia Tools | Daily Life APIs | Average |
| :--- | :---: | :---: | :---: | :---: |
| Kendall's $\tau$ | 0.89 | 0.83 | 0.94 | 0.89 |
| Spearman's $\rho$ | 0.78 | 0.62 | 0.93 | 0.78 |

models (e.g., GPT series models) with human alignments (e.g., RLHF) demonstrate stronger task automation capabilities than open-source LLMs. These results also indicate that RLHF allows large language models to develop more generalized reasoning abilities, reducing overfitting to specific instructions.

## 4 CONCLUSION

In this paper, we introduce TaskBench, a benchmark to evaluate LLMs for task automation. More in detail, we first summarize three critical stages for LLMs in automating tasks, which include task decomposition, tool invocation, and parameter prediction for tools. The performance of these three stages reflects the task automation capabilities of LLMs, and thus we expect to construct evaluation datasets for them. To fulfill this, we introduce the concept of ToolGraph, which collects different
tools with their connections, and then adopt a back-instruct method to simulate user instruction based on the sampled sub-graph from ToolGraph. Based on our curated datasets, we further introduce TASKEVAL to systematically evaluate the capability of LLMs in automating tasks, including task decomposition, tool invocation, and parameter prediction. Experimental results demonstrate our TaskBench can be effectively utilized to evaluation LLMs in task automation. In the future, we will extend our benchmark to various domains and design more advanced metrics to further explore the potential of LLMs in task automation and build powerful autonomous agents.

## REFERENCES

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, pp. $1-25,2020$.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https : //lmsys.org/blog/2023-03-30-vicuna/

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.

Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/ Significant-Gravitas/Auto-GPT, 2023.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, , and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023a. URLhttps://lmsys.org/blog/2023-06-29-longchat.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023b.

Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis. CoRR, abs/2303.16434, 2023.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. CoRR, abs/2308.03688, 2023.

Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.

OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, volume 35, pp. 27730-27744, 2022.

Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334, 2023.

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. CoRR, abs/2304.08354, 2023 .

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761, 2023.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023.

Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. CoRR, abs/2306.05301, 2023.

InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM, 2023a.

MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, ly usable llms, 2023b. URLwww.mosaicml.com/blog/mpt-7b. Accessed: 2023-03-28.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In NeurIPS, pp. 3261-3275, $2019 \mathrm{a}$.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2019b.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/ 2023.acl-long. 754.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.

Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for LLM question answering with external tools. CoRR, abs/2306.13304, 2023.
