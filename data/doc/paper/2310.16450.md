# CLEX: CONTINUOUS LENGTH EXTRAPOLATION FOR LARGE LANGUAGE MODELS 

Guanzheng Chen ${ }^{1,2,3, *}$ Xin Li $^{2,3, \dagger}$ Zaiqiao Meng ${ }^{4}$ Shangsong Liang ${ }^{1,5, \dagger}$ Lidong Bing $^{2,3}$<br>${ }^{1}$ Sun Yat-sen University ${ }^{2}$ DAMO Academy, Alibaba Group<br>${ }^{3}$ Hupan Lab, 310023, Hangzhou, China ${ }^{4}$ University of Glasgow<br>${ }^{5}$ Mohamed bin Zayed University of Artificial Intelligence<br>guanzzh.chen@gmail.com, \{xinting.lx,l.bing\}@alibaba-inc.com<br>zaiqiao.meng@glasgow.ac.uk, liangshangsong@gmail.com


#### Abstract

Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical longcontext applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over $4 \times$ or almost $8 \times$ training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a $4 \mathrm{k}$ length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to $32 \mathrm{k}$. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.


## 1 INTRODUCTION

Transformer-based large language models (LLMs), such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023a b), have now emerged as the state-of-the-art models in various natural language processing (NLP) tasks. However, these models grapple with the limitations inherent to the Transformer architecture - mainly, a preset context window, beyond which performance plummets catastrophically (Press et al. 2022). The quadratic complexity of the attention mechanism renders training LLMs with a larger context window extraordinarily resource-intensive. Prior works (Dai et al., 2019, Beltagy et al., 2020; Bulatov et al., 2022) have proposed circumventing full context length access via hierarchical architecture or sparse attention, albeit at the expense of forfeiting partial context information.

Recently, there have been two lines of methods aimed at efficiently extending the pre-trained context length of LLMs, both centred on position embedding (PE). The first line of methods, known as PE scaling, are proposed to effectively extend the context window of LLMs integrated with Rotary[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_75d5b8a7ecf4e8d50504g-02.jpg?height=409&width=1390&top_left_y=281&top_left_x=365)

Figure 1: The PPLs of our CLEX and various baselines tested on $64 \mathrm{k}$ context length.

Position Embedding (RoPE) (Su et al. 2022). They allow LLMs to access longer context by scaling either position indices (Chen et al. 2023) or frequency basis (Rozi√®re et al., 2023, Peng et al., 2023) of RoPE, demonstrating remarkable performance in long-context applications. However, such methods are designed for extending the context length corresponding to a fixed scaling factor, which either restricts their ability to extrapolate to longer sequences (when using small factors) or impairs the performance even within the native context window (when using large factors) as shown in Figure 1. On the other hand, length extrapolation methods (Press et al., 2022; Sun et al., 2023, Chi et al. 2022; 2023), typified by ALiBi (Press et al., 2022), strive to achieve test-time context length extension (i.e., "training on short, testing on long") by substituting position embeddings with additional biases, where the biases encode positional information to the attention scores. Despite their impressive capability in language modelling, ALiBi-like methods usually struggle in the practical tasks requiring long-context dependency (Pal et al., 2023) (also see $\$ 4.3$ ).

In this work, we present Continuous Length EXtrapolation (CLEX), a novel approach that efficiently extrapolates the context window of LLMs through continuous PE scaling. Concretely, we propose a unified view of PE scaling via generalising the PE scaling methods to the transition of frequency basis. Upon it, we formulate the PE scaling as a continuous dynamical system, which models the transition of frequency basis through the continuous dynamics over the length scaling factor. We argue that previous PE scaling methods, training models using fixed (discrete) scaling factors, overlook the progressively continuous dynamics over the gradually length-extending process. This ensnares themselves in the aforementioned dilemma between extrapolating the length and preserving the performance within shorter lengths. In contrast, our CLEX exploits a neural ordinary differential equation (ODE) (Chen et al. 2018), parameterised by an up-and-down projection layer with lightweight parameters to learn these continuous dynamics, enabling fine-grained extending to long context. More essentially, by extending the dynamics beyond training length, CLEX empowers models to progressively extrapolate to longer contexts even when trained with short sequences.

CLEX can serve as a drop-in component for RoPE-based LLMs, such as LLaMA (Touvron et al., 2023a b) and GPT-NeoX (Black et al., 2022), with negligible overhead in computation and parameters size. We evaluate the performance of CLEX on two datasets: (1) a subset of RedPajamaBook (Computer, 2023) for long-context language modelling, and (2) LongBench (Bai et al. 2023) for long-context practical tasks. Empirically, CLEX demonstrates remarkable length extrapolation ability in language modelling, which can extend the context window to more than $4 \times$ training length without any performance deterioration. For example, LLaMA-2-7B trained with CLEX on 16k context length achieves comparable perplexities when testing on $16 \mathrm{k}$ and $64 \mathrm{k}$ tokens, respectively. By scaling the base model scale from 7B to 13B, CLEX exhibits an expanded extrapolation scope from $4 \times$ to almost $8 \times$ training length. To be complementary, we also conduct instruction tuning (Wei et al. 2022) with the proposed CLEX on the sequences of $4 \mathrm{k}$ length. The resulting model, when evaluated on the LongBench benchmark, is on par with current state-of-the-art open-source models trained on context lengths up to $32 \mathrm{k}$. These findings underscore the effectiveness of CLEX in extrapolating context length, signifying its efficiency for developing long-context LLMs.

## 2 PRELIMINARIES

### 2.1 Rotary Position EMBEdDing (RoPE)

Rotary Position Embedding (RoPE) (Su et al., 2022) has recently emerged as the most prevailing positional encoding method in open-source LLMs like LLaMA. It integrates both absolute and relative positional information for Transformer models. Given a position index $m \in[1, L]$, RoPE injects the absolute positional information into $\boldsymbol{x} \in \mathbb{R}^{d}$ via the transformation $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ as:

$$
\begin{equation*}
f(\boldsymbol{x}, m, \boldsymbol{\theta})=\boldsymbol{R}_{\boldsymbol{\theta}, m} \boldsymbol{x} \tag{1}
\end{equation*}
$$

where $\boldsymbol{\theta} \in \mathbb{R}^{\lfloor d / 2\rfloor}$ is the rotation frequency basis and $\theta_{i}=10,000^{-2 i / d} ; \boldsymbol{R}_{\boldsymbol{\theta}, m} \in \mathbb{R}^{d \times d}$ is a block diagonal matrix formed by the elements

$$
\left(\boldsymbol{R}_{\boldsymbol{\theta}, m}\right)_{i}=\left[\begin{array}{cc}
\cos m \theta_{i} & -\sin m \theta_{i}  \tag{2}\\
\sin m \theta_{i} & \cos m \theta_{i}
\end{array}\right], \text { for } i=1,2, \ldots,\lfloor d / 2\rfloor
$$

The transformation in Eq. 11 is applied to the query and key vectors during self-attention. When calculating the attention score for the query vector $\boldsymbol{q}_{m} \in \mathbb{R}^{d}$ at position $m$ and the key vector $\boldsymbol{k}_{n} \in \mathbb{R}^{d}$ at position $n$, we have

$$
\begin{equation*}
\left(\boldsymbol{R}_{\boldsymbol{\theta}, m} \boldsymbol{q}_{m}\right)^{\top}\left(\boldsymbol{R}_{\boldsymbol{\theta}, n} \boldsymbol{k}_{n}\right)=\boldsymbol{q}_{m} \boldsymbol{R}_{\boldsymbol{\theta}, n-m} \boldsymbol{k}_{n} \tag{3}
\end{equation*}
$$

Hence, the relative positional information $\boldsymbol{R}_{\boldsymbol{\theta}, n-m}$ is implicitly incorporated into the attention scores. However, even given the relative information, LLMs trained with RoPE, e.g., LLaMA, still cannot achieve reasonable performance beyond the pre-trained context length.

### 2.2 PE SCALING METHODS

To extend the context length $L$, several strategies are proposed to adjust the position embedding by scaling either the position index $m$ or frequency basis $\boldsymbol{\theta}$ in Eq. 1). Formally, we define $t=L^{\prime} / L$ as the length scaling factor where $L^{\prime}$ denotes the desired extended length. While Chen et al. (2023) introduces scaling the index $m$ by Position Interpolation (PI) as

$$
\begin{equation*}
f_{t}^{\mathrm{PI}}(\boldsymbol{x}, m, \boldsymbol{\theta})=f\left(\boldsymbol{x}, \frac{m}{t}, \boldsymbol{\theta}\right) \tag{4}
\end{equation*}
$$

This strategy maintains the position indices within the range $[1, L]$, while effectively extending the processed range to $[1, t \cdot L]$ by minimal fine-tuning steps on $t \cdot L$ sequences. On the other hand, Peng et al. (2023) proposes Yarn, a.k.a. NTK-Aware Scaled RoPE, extends the context window by frequency basis scaling (FBS). This strategy is similarly utilised by CodeLLaMA (Rozi√®re et al. 2023). Formally, the FBS methods are denoted as

$$
\begin{equation*}
f_{t}^{\mathrm{FBS}}(\boldsymbol{x}, m, \boldsymbol{\theta})=f\left(\boldsymbol{x}, m, \boldsymbol{\theta}_{t}\right) \tag{5}
\end{equation*}
$$

where $\boldsymbol{\theta}_{t}$ is the scaled frequency basis. Specifically, $\theta_{t, i}=\theta_{i} \cdot(t)^{-2 i /(d-2)}$ in Yarn and $\theta_{t, i}=$ $\theta_{i} \cdot 100^{-2 i / d}$ in CodeLLaMA.

## 3 METHODOLOGY

This section demonstrates the details of CLEX. We first generalise the PE scaling to a continuous dynamical system in a unified manner (see \$3.1). On top of the continuous dynamical system, CLEX employs the neural ODE, parameterised by an up-and-down projection layer, to adaptively learn the continuous dynamics during $\mathrm{PE}$ scaling (see $\$ 3.2$. In $\$ 3.3$, we introduce the training strategy of CLEX that distributes the continuous dynamics beyond the training sequence length, thereby enabling the generalisation of continuous PE scaling to achieve the length extrapolation.

### 3.1 Position EMBEdDing ScALING: A UNIFIEd VIEW

Given the various methods that extend models' context length through position indices scaling and frequency basis scaling, we first show that the transformations applied to position indices are essentially casting the frequency basis, which is formalised in Theorem 1.
![](https://cdn.mathpix.com/cropped/2024_06_04_75d5b8a7ecf4e8d50504g-04.jpg?height=386&width=1330&top_left_y=300&top_left_x=386)

Figure 2: The graphical model of discrete PE scaling (left) and our continuous PE scaling (right).

Theorem 1. For the transformation $\mathcal{T}$ to position index $m$, there exists an equivalent transformation $\mathcal{T}$ to frequency basis $\boldsymbol{\theta}$ in Eq. (1), namely

$$
\begin{equation*}
f(\boldsymbol{x}, \mathcal{T} \cdot m, \boldsymbol{\theta})=f(\boldsymbol{x}, m, \mathcal{T} \odot \boldsymbol{\theta}) \tag{6}
\end{equation*}
$$

where $\mathcal{T}=[\mathcal{T}]_{i=1}^{d / 2}$ and $\odot$ denotes the element-wise transformation.

Proof. From Eq. (I), we have $f(\boldsymbol{x}, \mathcal{T} \cdot m, \boldsymbol{\theta})=\boldsymbol{R}_{\boldsymbol{\theta}, \mathcal{T}} \boldsymbol{x}$ and $f(\boldsymbol{x}, m, \mathcal{T} \odot \boldsymbol{\theta})=\boldsymbol{R}_{\mathcal{T} \odot \boldsymbol{\theta}, m} \boldsymbol{x}$. For any $\mathcal{T}=[\mathcal{T}]_{i=1}^{d / 2}$,

$$
\left(\boldsymbol{R}_{\boldsymbol{\theta}, \mathcal{T} m}\right)_{i}=\left[\begin{array}{cc}
\cos \mathcal{T} m \theta_{i} & -\sin \mathcal{T} m \theta_{i}  \tag{7}\\
\sin \mathcal{T} m \theta_{i} & \cos \mathcal{T} m \theta_{i}
\end{array}\right]=\left[\begin{array}{cc}
\cos m\left(\mathcal{T} \odot \theta_{i}\right) & -\sin m\left(\mathcal{T} \odot \theta_{i}\right) \\
\sin m\left(\mathcal{T} \odot \theta_{i}\right) & \cos m\left(\mathcal{T} \odot \theta_{i}\right)
\end{array}\right]=\left(\boldsymbol{R}_{\mathcal{T} \odot \boldsymbol{\theta}, m}\right)_{i}
$$

Hence, there is a unified form for PE scaling that consistently projects the frequency basis by $\boldsymbol{\alpha}(t)$ :

$$
\begin{equation*}
f_{t}(\boldsymbol{x}, m, \boldsymbol{\theta})=f(\boldsymbol{x}, m, \boldsymbol{\alpha}(t) \odot \boldsymbol{\theta}) \tag{8}
\end{equation*}
$$

where $\boldsymbol{\alpha}(t)$ is a single-variable transformation defined over the length scaling factor $t$. Through this unified formulation, PI (Chen et al. 2023) and Yarn (Peng et al. 2023) can be viewed as the special cases when plugging $\boldsymbol{\alpha}^{\mathrm{PI}}(t)=[1 / t]_{i=1}^{d / 2}$ and $\boldsymbol{\alpha}^{\text {Yarn }}(t)=\left[t^{-2 i /(d-2)}\right]_{i=1}^{d / 2}$ into Eq. 8, respectively.

Note that $\boldsymbol{\theta}_{t}=\boldsymbol{\alpha}(t) \odot \boldsymbol{\theta}$ denotes the scaled frequency basis at context length of $t \cdot L$ and $\boldsymbol{\theta}_{1}=\boldsymbol{\theta}$ (namely $\boldsymbol{\alpha}(1)=1$ ). As illustrated in Figure 2 this indicates a progressive chain across discrete $t$ values that

$$
\begin{equation*}
\boldsymbol{z}(t)=\boldsymbol{z}(1)+\log \boldsymbol{\alpha}(t)=\boldsymbol{z}(t-1)+\log \frac{\boldsymbol{\alpha}(t)}{\boldsymbol{\alpha}(t-1)} \tag{9}
\end{equation*}
$$

where $\boldsymbol{z}(t)=\log \boldsymbol{\theta}_{t}$.

By continuizing the progressive chain, we can formulate the PE scaling as a continuous dynamical system, with the continuous dynamics of frequency basis $d \boldsymbol{z}(t) / d t$ as

$$
\begin{equation*}
\frac{d \boldsymbol{z}(t)}{d t}=\frac{d \log \boldsymbol{\alpha}(t)}{d t} \tag{10}
\end{equation*}
$$

In essence, recent PE scaling methods, concentrating on manually formulating the $\boldsymbol{\alpha}(t)$, are equivalent to applying various dynamics for frequency basis that enable models to adapt to longer contexts.

### 3.2 ConTinUOUS PE SCALING VIA NEURAL ODE

Even given the continuous dynamics of frequency basis, previous methods are inherently designed for extending the context length at discrete $t$ values. For example, PI (Chen et al. 2023) fine-tunes the model on a specific scaling factor $t$ to extend the context window length to $t \cdot L$. One potential issue of these methods, as depicted in Figure 1, is that they are susceptible to overfitting to the specified frequency basis, leading to either poor extrapolation ability to longer lengths beyond training or performance drops within short lengths, or both in some cases. Therefore, our CLEX aims to build a continuous PE scaling, which induces the model to adapt the frequency basis corresponding to a continuous scope of $t$ as illustrated in Figure 2 (right).

Recall that previous PE scaling, corresponding to a manually defined $\boldsymbol{\alpha}(t)$, implies the constant dynamics in Eq. 10. In our method, we utilise a variable function $g: \mathbb{R}^{d / 2} \rightarrow \mathbb{R}^{d / 2}$ to model the dynamics, hence towards a more general and flexible view as:

$$
\begin{equation*}
\frac{d \boldsymbol{z}(t)}{d t}=g(\boldsymbol{z}(t), t) \tag{11}
\end{equation*}
$$

By restricting the function to be associated with the latent states $\boldsymbol{z}(t), g$ is capable of capturing the fine-grained changes of frequency basis during the length-extending process. However, it is nontrivial to manually define the $\boldsymbol{z}(t)$-aware function $g$. Here, we directly parameterise the function using the neural network $\phi$. Therefore, for any $t^{\prime} \in[1, t]$, there is a neural ODE modelling the scaling of frequency basis as

$$
\begin{equation*}
\boldsymbol{z}\left(t^{\prime}\right)=\boldsymbol{z}(1)+\int_{1}^{t^{\prime}} g_{\boldsymbol{\phi}}(\boldsymbol{z}(t), t) d t \tag{12}
\end{equation*}
$$

where the frequency basis at the length $t^{\prime} \cdot L$ can be derived by $\boldsymbol{\theta}_{t^{\prime}}=\exp \left(\boldsymbol{z}\left(t^{\prime}\right)\right)$.

More specifically, we adopt an up-and-down projection as the neural network, expressed as:

$$
\begin{equation*}
g_{\boldsymbol{\phi}}(\boldsymbol{z}(t), t)=\boldsymbol{W}_{\text {down }} \cdot \sigma\left(\boldsymbol{W}_{\text {up }} \cdot \boldsymbol{z}(t)\right)+\xi_{t} \tag{13}
\end{equation*}
$$

where $\boldsymbol{W}_{\text {up }} \in \mathbb{R}^{\frac{d}{2} \times \lambda d}$ and $\boldsymbol{W}_{\text {down }} \in \mathbb{R}^{\lambda d \times \frac{d}{2}}$ are the transformation matrices, of which the parameters are determined by the amplification factor $\lambda ; \sigma$ is the SiLU activation function and $\xi_{t}$ is the scalar embedding typifying the scaling procedure at factor of $t$. Here, we adopt the constant dynamics of Yarn as the $\xi_{t}$ for speeding up convergence, namely

$$
\begin{equation*}
\xi_{t}=\frac{d \log \boldsymbol{\alpha}^{\mathrm{Yarn}}(t)}{d t}=-\left[\frac{2 i}{(d-2) \cdot t}\right]_{i=1}^{d / 2} \tag{14}
\end{equation*}
$$

### 3.3 Continuous Length Extrapolation: Train on Short, Test on LonG

Continuous PE scaling can serve as a more adaptive and flexible PE scaling method to extend the context length to a given training length $L^{\text {Train }}$. Unlike the previous PE scaling methods built on a larger scaling factor, which would lead to inferior performance on the lengths corresponding to smaller counterparts, the continuous $\mathrm{PE}$ scaling would enable non-destructively generalisation to larger scaling factors via adaptive continuous dynamics. Therefore, by simply extending the continuous dynamics beyond the factor $t=L^{\text {Train }} / L$ during training (where we denote the desired scaling factor as $t^{\text {Train }}$ ), we can access the continuous length extrapolation (CLEX) method, which achieves the capability of "training on short, testing on long".

Moreover, to learn the neural ODE in Eq. 12) for continuous $t$, we randomly sample $t^{\prime} \in\left[1, t^{\text {Train }}\right]$ for each training step, enabling the model to adapt to the broad scope frequency basis without overfitting a specific one. Note that the frequency basis is bound with the position index in Eq. (1). This reveals the aforementioned training involves inconsistency between the frequency basis and position indices: the frequency basis is varied corresponding to the $t^{\prime} \in\left[1, t^{\text {Train }}\right]$, while the position indices are fixed as $\left\{1,2, \ldots, L^{\text {Train }}\right\}$. Here, we propose the position extrapolation strategy to address this consistency. Contrary to PI, which shrinks the position indices into the context length, we enlarge the position indices $\left\{1,2, \ldots, L^{\text {Train }}\right\}$ of the trained sequences up to the range $\left[1, t^{\prime} \cdot L\right]$ for each training step. The position indices can be acquired by uniformly scaling to $\left\{1 \cdot s, 2 \cdot s, \ldots, L^{\text {Train }} \cdot s\right\}$ where $s=t^{\prime} \cdot L / L^{\text {Train }}$, or alternatively, by randomly sampling $L^{\text {Train }}$ of indices from $\left[1, t^{\prime} \cdot L\right]$. Empirically, we found that random sampling generally performs better. More discussions can be found in $\$ 4.2$.

During inference, the ideal scenario is to acquire the frequency basis corresponding to each sequence length. However, this approach is computationally demanding. To improve efficiency, we first cache some frequency basis derived from $g_{\phi}$ for $K$ discrete $t$ values as $\left\{t_{k} \mid k \in[1, K]\right\}$. For each sequence with a length of $L^{\text {Infer }}$ during inference, we employ the frequency basis corresponding to the nearest upper bound within $t_{k} \cdot L$ for $k=1, \ldots, K$. Through this, our method introduces negligible time cost compared to naive inference of LLMs.

## 4 EXPERIMENTS

In this section, we conduct a thorough evaluation of CLEX's performance in terms of handling long contexts and its extrapolation capabilities. We compare our approach against other methods covering

| Methods | Train <br> Length | Evaluation Length |  |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $4096(4 \mathrm{k})$ |  | 8192 (8k) |  | 16,384 (16k) |  | 32,768 (32k) |  | $65,536(64 \mathrm{k})$ |  |
|  |  | PPL | ACC. | PPL | ACC. | PPL | ACC. | PPL | ACC. | PPL | $\mathrm{ACC}$ |
| LLaMA-2 | $4 \mathrm{k}$ | 6.04 | 58.18 | 20.54 | 44.50 | $>100$ | 22.43 | $>1000$ | 12.70 | $>1000$ | 10.64 |
| CodeLLaMA | $16 \mathrm{k}$ | 7.60 | 54.88 | 7.40 | 55.19 | 7.33 | 55.30 | 15.12 | 44.70 | 52.02 | 31.16 |
| Naive FT | $16 \mathrm{k}$ | 5.98 | 58.83 | 5.93 | 58.91 | 5.91 | 58.58 | 18.31 | 43.04 | $>100$ | 26.05 |
| PI | $16 \mathrm{k}$ | 5.90 | 59.05 | 5.71 | 59.44 | 5.72 | 59.87 | 6.05 | 58.5 | 8.75 | 52.02 |
| $\operatorname{Yarn}(t=16)$ | $16 \mathrm{k}$ | 6.50 | 57.28 | 5.71 | 59.57 | 5.73 | 59.87 | 5.99 | 58.13 | 8.51 | 52.62 |
| Yarn $(t=32)$ | $16 \mathrm{k}$ | 6.61 | 57.12 | 5.94 | 58.27 | 5.96 | 58.04 | 6.08 | 57.73 | 6.22 | 57.98 |
| CL-Scaling | $16 \mathrm{k}$ | 24.99 | 37.84 | 5.86 | 59.08 | 5.87 | 59.05 | 10.56 | 50.47 | 41.09 | 34.16 |
| ALiBi | $4 \mathrm{k}$ | 6.34 | 58.01 | 6.39 | 57.8 | 6.41 | 57.78 | 6.50 | 57.47 | 6.51 | 56.44 |
| RandomPos | $4 \mathrm{k}$ | 5.88 | 58.49 | $>100$ | 34.23 | $>1000$ | 18.27 | $>1000$ | 9.31 | $>1000$ | 7.44 |
| CLEX | $4 \mathrm{k}$ | 5.86 | 59.21 | 5.70 | 59.62 | 5.87 | 58.93 | 14.53 | 47.55 | 30.51 | 35.33 |
|  | $8 \mathrm{k}$ | 5.98 | 58.75 | 5.78 | 59.44 | 5.71 | 59.64 | 5.99 | 58.66 | 11.74 | 47.50 |
|  | $16 \mathrm{k}$ | 5.88 | 59.21 | 5.68 | $\mathbf{5 9 . 7 3}$ | 5.52 | 60.28 | $\mathbf{5 . 5 5}$ | $\mathbf{6 0 . 0 6}$ | 5.64 | 59.94 |

Table 1: Perplexity (PPL) and next-token-prediction accuracy (ACC.) on language modeling with evaluation lengths from $4 \mathrm{k}$ to $64 \mathrm{k}$. We train the LLaMA-2-7B using length extrapolation methods on $4 \mathrm{k}$ length and PE scaling methods on 16k length, while reporting the results of CLEX trained across $4 \mathrm{k}, 8 \mathrm{k}$ and 16k. CL-Scaling denotes training LLaMA-2-7B with the scaling method of CodeLLaMA but using our training data. The training loss curves are depicted in Figure 9 .

both length extrapolation (i.e., ALiBi (Press et al. 2022) and random positions (denoted as RandomPos) (Ruoss et al. 2023) and PE scaling methods (i.e., PI (Chen et al. 2023) and Yarn (Peng et al. 2023)). We primarily conduct experiments on the LLaMA-2-7B model. For the language modelling, we train our model and the baselines on 2B tokens extracted from Redpajama-Book (Computer, 2023), which is collected from Pile-Books3 (Gao et al. 2020) and PG-19 (Rae et al. 2019) datasets. The performance of the models is assessed based on perplexity and next-token-prediction accuracy, with evaluation sequence lengths up to $64 \mathrm{k}$. Furthermore, we conduct instruction tuning for LLaMA-2-7B using CLEX on the UltraChat dataset (Ding et al., 2023b). The evaluation is performed on the LongBench benchmark (Bai et al., 2023), where we compare our model with GPT-3.5-turbo and other LLaMA-2-based open-source models designed for handling long context. Further details about baselines and training configuration will be discussed in Appx. $\$$ A, as well as more experimental results and ablations in Appx. $B$

### 4.1 LONG-CONTEXT LANGUAGE MODELLING

CLEX achieves length extrapolation. We first report the experimental results of baselines and CLEX on language modelling, with the evaluation length from $4 \mathrm{k}$ to $64 \mathrm{k}$. As shown in Table 1 . our CLEX consistently demonstrates remarkable performance in length extrapolation, being able to extrapolate the context length to more than $4 \times$ training length without any performance drops. Taking CLEX- $4 \mathrm{k}$ as an example, its PPL on $4 \mathrm{k}$ sequence (training length) is comparable to that on $16 \mathrm{k}$ sequence ( 5.86 vs. 5.87). When evaluated on the sequences within 16k, CLEX-4k is on par with or even better than all of the compared methods trained on lengths up to $16 \mathrm{k}$. Moreover, with the increase in training length, our CLEX not only exhibits promising generalisation capability to very long contexts (up to $64 \mathrm{k}$ ) but also guarantees performance on short sequences.

We also found that discrete PE scaling methods (i.e., PI and Yarn) have self-extending property: training with scaled frequency basis equips the model with the ability to extrapolate to furtherscaled counterparts (see Appx. \$B. 2 for more discussions.). As depicted in Figure 1 however, the extrapolation capability of these methods is limited, accompanied by a significant performance decline even within the naive context length. This indicates the inherent challenge of achieving a delicate balance between extrapolation to longer lengths and performance maintenance within short lengths when using the discrete scaling factor. In contrast, CLEX tackles this issue via learnable continuous dynamics, providing a more fine-grained extrapolation while preserving the performance for the internal context.

Note that ALiBi may extend further than CLEX trained on $4 \mathrm{k}$ sequences (though typically producing inferior results), our experiments reveal that these gains may come at the cost of long-term information, leading to underperformance in long-context practical tasks (see $\$ 4.3$ for more details).

![](https://cdn.mathpix.com/cropped/2024_06_04_75d5b8a7ecf4e8d50504g-07.jpg?height=412&width=1244&top_left_y=282&top_left_x=430)

Figure 3: Left: The PPLs of CLEX on different evaluation sequence lengths with 7B and 13B parameter sizes. Right: The PPLs of CLEX cross variable training data size with different parameter sizes and evaluation lengths.
![](https://cdn.mathpix.com/cropped/2024_06_04_75d5b8a7ecf4e8d50504g-07.jpg?height=324&width=1306&top_left_y=900&top_left_x=407)

Figure 4: The ablation studies for continuous dynamics, sampling strategies and $\lambda$ factor in Eq. 13.)

The scaling law for the extrapolation ability of CLEX. To investigate the effectiveness of CLEX over the scale of the base model and training data size, we further port our method to LLaMA-213B. As depicted in Figure 3, when trivially extending the base model scale from 7B to 13B, our CLEX demonstrates an increased capacity to extrapolate to longer context lengths. Specifically, the extrapolation ability of CLEX-13B trained on $4 \mathrm{k}$ length approaches that of CLEX-7B trained on $8 \mathrm{k}$. While the training data scale, more surprisingly, does not significantly impact the extrapolation capability of CLEX. Models trained with $0.25 \mathrm{~B}$ or $2 \mathrm{~B}$ tokens with $4 \mathrm{k}$ sequence length achieve comparable PPLs when evaluating on 16k or 32k lengths in Figure 3, indicating the negligible margins from the larger training data size. This also implies that CLEX can efficiently extend the context length of LLMs through minimal training steps resembling PI and Yarn.

Based on these findings, we propose a scaling law for CLEX: to scale the context length of LLMs to moderately desired lengths (e.g., $16 \mathrm{k} \rightarrow 64 \mathrm{k}$ ), one should proportionally enlarge the training sequence lengths (e.g., $4 \mathrm{k} \rightarrow 16 \mathrm{k}$ ). For scaling the context length up to considerably long lengths (e.g., $>200 \mathrm{k}$ ), the parameter size of the base model should be correspondingly increased while maintaining the training length, since the contexts may take more footprints than model parameters. Note that scaling the training data does not directly affect the extrapolation ability of CLEX, but may be implicitly incorporated when scaling the base pre-trained LLMs.

### 4.2 ABLATION StUdY

We now conduct three types of ablations to investigate the efficacy of the components in CLEX:

Continuous dynamics. To learn the continuous dynamics using neural ODE, we adopt a distinct training approach that involves sampling the scaling factor $t$ for each data batch. Here we seek to explore if the exceptional extrapolation ability of CLEX is solely derived from the variable $t$ rather than the continuous dynamics. We employ the discrete Yarn method with $t=16$, that undergoes the same training procedure of CLEX but removes the ODE parameters, serving as a discrete baseline. In Figure 4 (left), we discover that the discrete approach equipped with the random-sampled $t$ significantly underperforms our CLEX, indicating the essentiality of the learnable continuous dynamics in CLEX for accessing the extrapolation ability.
![](https://cdn.mathpix.com/cropped/2024_06_04_75d5b8a7ecf4e8d50504g-08.jpg?height=524&width=1370&top_left_y=280&top_left_x=367)

Figure 5: Left: the average scores for each domain of tasks in LongBench. Right: the average scores of all tasks corresponding to the training length of each model. Note that CLEX is trained on $4 \mathrm{k}$ sequence length but directly tested on $16 \mathrm{k}$ context length without truncation.

Position extrapolation. We adopt the position extrapolation strategy, which extends the scope of position indices in training sequences by sampling from a broader range, to reconcile the inconsistency between frequency basis and position indices during the training process. In this study, we examine the impact of various sampling strategies (uniform or random) and contrast them with the naive position indices. The results in Figure 4 underscore the efficacy of position extrapolation in CLEX, without which the extrapolation ability of models declines significantly. Furthermore, random sampling slightly performs better than uniform sampling, so we adopt it across all experiments.

The parameter scale of ODE. We also study the impact of parameter size of the neural ODE in CLEX. The parameter size is determined by the $\lambda$, namely the amplification factor in Eq. (13). In Figure 4 , we plot the results of CLEX with $\lambda=1,2,4$, where they achieve similar performance. Note that the parameter size of neural ODE in CLEX is quite small even when $\lambda=4$, as the dimension $d$ in Eq. 133 is usually equal to 128. Although it is possible to enhance CLEX with larger $\lambda$ (e.g., 32), we set the $\lambda=1$ in all experiments for the minimal effect on inference latency.

### 4.3 EVALUATION ON LONG-CONTEXT BENCHMARK

To ascertain the comprehensive performance of CLEX in real-world scenarios, we further conduct an evaluation on the zero-shot LongBench benchmark. This benchmark encompasses a broad range of tasks, such as question-answering, summarization, and code completion, where the evaluation length ranges from $5 \mathrm{k}$ to $15 \mathrm{k}$. We perform a pilot instruction tuning for LLaMA-2-7B by employing CLEX on the UltraChat dataset, with a sequence length of $4 \mathrm{k}$. During inference, we harness all models to tackle the context length of $16 \mathrm{k}$, thereby ensuring the comprehensive exploitation of contextual information in the tasks. As depicted in Figure 5, we present the average scores of each domain in LongBench for CLEX, in comparison to the GPT-3.5-Turbo-16k model and strong opensource LLMs like LongChat-v1.5-7B-32k and CodeLLaMA-7B-16k.

Generally, when trained with sequences of $4 \mathrm{k}$ length, CLEX holds its own against any open-source LLMs that are trained on lengths up to $32 \mathrm{k}$. In the specific domains of Summarization, Few-shot Learning, and Code Completion, CLEX on LLaMA-2-7B remains competitive with or even surpasses the GPT-3.5-Turbo-16k. We note that the Baichuan-13B-4k, pre-trained with ALiBi (Press et al., 2022), demonstrates marked underperformance on the LongBench although with a larger parameter size. Additionally, similar poor results are achieved by ALiBi when applying it upon LLaMA-2-7B using the same training procedure as CLEX (see Appx. $\$$ B.5). This could likely be attributed to ALiBi's overemphasis on local context through the attention bias, which, while advantageous for language modelling, restricts access to long-context information in practical tasks. In contrast, CLEX directly extends the context length of LLMs without imposing any constraints on context, which consistently achieves superior extrapolation ability on both language modelling and the LongBench. This substantiates the considerable potential of CLEX to serve as the state-of-the-art approach for extrapolating the context length of LLMs to excel in long-context applications.

In addition, we highlight that our CLEX merely introduces minuscule inference latency. Given a context length of $16 \mathrm{k}$ in LongBench with a generation length of 512, the generation throughput between our CLEX-7B and LLaMA-2-7B is comparable ( 27.8 tokens/s vs 28.3 tokens/s, in a single A100), when using the cache mechanism introduced in $\$ 3.3$.

## 5 RELATED WORK

Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al. 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al. 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information. Additionally, modifications to the model architecture make these methods challenging to apply to existing pre-trained LLMs. Conversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity of models to tack the entire long sequences without explicit drops of context information.

Length Extrapolation. Building on the foundation laid by ALiBi (Press et al. 2022), a series of works (Sun et al., 2023, Chi et al. 2022, 2023) seek to train the Transformer-based models on a short length, while directly testing on longer counterparts. These methods substitute the position embedding with bias introduced into attention scores, thereby incorporating positional information. Notably, the bias typically gives higher profits to closer tokens. This mechanism intuitively amplifies the local context for each token at the expense of distant information. Consequently, these length-extrapolation methods encounter challenges in effectively handling long contexts in practical applications (Pal et al. 2023). However, our CLEX demonstrates remarkable effectiveness in practical tasks such as summarization, indicating the de facto extrapolation ability for applications.

Position Embedding (PE) Scaling. Recent research has sought to extend the context length of Transformers through the scaling of the extensively employed RoPE. Specifically, Chen et al. (2023) proposed position interpolation, a method that efficiently extends the context window by scaling the position index within RoPE. In a similar vein, Peng et al. (2023); Rozi√®re et al. (2023) opted to scale the frequency basis, achieving superior performance. However, these methods necessitate training (or fine-tuning) on the desired extended length. As a result, they exhibit a limited ability to extrapolate beyond the trained length and even suffer from performance drops within the shorter lengths. In CLEX, we generalise the discrete PE scaling to a continuous counterpart, hence uniformly extrapolating the context length of LLMs while preserving the performance within short lengths.

## 6 CONCLUSION

We have presented the Continuous Length EXtrapolation (CLEX), a novel approach that efficiently extrapolates the context length of Large Language Models (LLMs) to over $4 \mathrm{x}$ the training (finetuning) length without any decline in performance. CLEX utilises the neural ODE to learn the continuous dynamics over the length scaling factor during PE scaling, hence enabling fine-grained extension for the frequency basis in the RoPE. We conduct thorough experiments to investigate the effectiveness of CLEX compared to a variety of strong LLMs, covering the language modelling task and LongBench benchmark. The experimental results have demonstrated the exceptional extrapolation ability of CLEX, where our CLEX trained with a sequence length of $4 \mathrm{k}$ holds the potential to remain competitive to any open-source long-context LLMs (e.g., CodeLLaMA) trained on lengths up to $32 \mathrm{k}$. These results highlight the potential of CLEX as a state-of-the-art approach for efficiently extrapolating the context length of LLMs, paving the way for advancements in long-context applications. By scaling the base model size up, we found CLEX can be correspondingly enhanced and subsequently is capable of extrapolating the model to a longer context length. This property indicates the tempting effectiveness of CLEX in the era of LLMs.

## ACKNOWLEDGEMENTS

This work was substantially supported by DAMO Academy through DAMO Academy Research Intern Program. Shangsong Liang was supported by the National Natural Science Foundation of China (Grant No. 61906219) and the Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates.

## REFERENCES

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. 2023. URL https://arxiv.org/ $\mathrm{abs} / 2308.14508$.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. URL/https://arxiv.org/abs/2004.05150.

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX20B: An open-source autoregressive language model. In Proceedings of BigScience Episode \#5 - Workshop on Challenges \& Perspectives in Creating Large Language Models. Association for Computational Linguistics, May 2022. URL https://aclanthology.org/2022. bigscience-1.9.

Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URLhttps://openreview.net/forum?id= Uynr3iPhksa.

Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to $1 \mathrm{~m}$ tokens and beyond with rmt. 2023. URLhttps://arxiv.org/abs/2304.11062.

Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URLhttps://proceedings.neurips.cc/paper_files/ paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. 2023. URL https://arxiv.org/ $\mathrm{abs} / 2306.15595$.

Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 37a413841a614b5414b333585e7613b8-Paper-Conference.pdf.

Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, July 2023. URLhttps://aclanthology .org/2023. acl-long. 756

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. 2019. URLhttps://arxiv.org/abs/1904.10509.

Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URLhttps://github.com/togethercomputer/RedPajama-Data.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, July 2019. URL https://aclanthology .org/P19-1285.

Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. URLhttps://arxiv.org/abs/2307.08691

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. URL/https://arxiv.org/abs/2205.14135.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to $1,000,000,000$ tokens. 2023a. URL https://arxiv.org/abs/2307.02486.

Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023b. URL https://arxiv.org/abs/ 2305.14233 .

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https: //arxiv.org/abs/2101.00027.

Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. Mixtral of experts. 2024. URL/https://arxiv.org/abs/2401.04088.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980

OpenAI. GPT-4 Technical Report. 2023. URL https://arxiv.org/abs/2303.08774.

Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. Giraffe: Adventures in expanding context lengths in llms. 2023. URL https:// arxiv.org/abs/2308.10882

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. 2023. URLhttps://arxiv.org/abs/2309.00071.

Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.

Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https:// arxiv.org/abs/2308.12950.

Anian Ruoss, Gr√©goire Del√©tang, Tim Genewein, Jordi Grau-Moya, R√≥bert Csord√°s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, July 2023. URL https://aclanthology.org/2023.acl-short.161.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. 2022. URLhttps://arxiv.org/abs/ 2104.09864 .

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, July 2023. URL https://aclanthology .org/ 2023.acl-long.816.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023a. URL/https://arxiv.org/abs/2302.13971

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. 2023b. URLhttps://arxiv.org/abs/2307.09288.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gEZrGCozdqR
