# Sparse Expansion and Neuronal Disentanglement 

Shashata Sawmya*<br>MIT<br>shashata@mit.edu

Linghao Kong*<br>MIT<br>linghao@mit.edu

Ilia Markov<br>IST Austria<br>ilia.markov@ist.ac.at

Dan Alistarh<br>IST Austria \& Neural Magic<br>dan.alistarh@ist.ac.at

Nir Shavit<br>MIT \& Neural Magic<br>shanir@mit.edu


#### Abstract

We show how to improve the inference efficiency of an LLM by expanding it into a mixture of sparse experts, where each expert is a copy of the original weights, one-shot pruned for a specific cluster of input values. We call this approach Sparse Expansion. We show that, for models such as Llama 2 70B, as we increase the number of sparse experts, Sparse Expansion outperforms all other one-shot sparsification approaches for the same inference FLOP budget per token, and that this gap grows as sparsity increases, leading to inference speedups.

But why? To answer this, we provide strong evidence that the mixture of sparse experts is effectively disentangling the input-output relationship of every individual neuron across clusters of inputs. Specifically, sparse experts approximate the dense neuron's output distribution with fewer weights by decomposing the distribution into a collection of simpler ones, each with a separate sparse dot product covering it. Interestingly, we show that the Wasserstein distance between a neuron's output distribution and a Gaussian distribution is an indicator of its entanglement level and contribution to the accuracy of the model. Every layer of an LLM has a fraction of highly entangled "Wasserstein" neurons, and model performance suffers more when these are sparsified as opposed to others.


## 1 Introduction

The past few years have seen an explosion in machine learning model capabilities, specifically those of transformers and large language models (LLMs). However, such an increase in capability has demanded an exponential increase in model size, incurring drastic monetary and energy costs.

The performance of LLMs is dominated by the number of parameters executed per input token during computation, both due to the number of multiply-add (FMA) operations required per parameter, and data movement to set up these operations. Neural network sparsity [1] attempts to reduce the executed number of parameters and thus improve performance. The most common form of sparsity is static sparsification, wherein unnecessary model parameters are pruned away, and the model performs the same computation for every input. Many methods, such as gradual magnitude pruning [2], Wanda [3], and SparseGPT [4], utilize this approach. SparseGPT, in particular, is the state-of-the-art one-shot pruning technique, allowing models to be cheaply compressed without expensive retraining.

In contrast, input-dynamic sparsity involves performing different computations on different inputs and is generally achieved through converting a dense model into a mixture of experts (MoE) like framework. Existing techniques, including DejaVu [5], MoEification [6], and Scaling Transformers[^0]

[7], route inputs only to computations predicted to be necessary. These methods are based on the sparsity induced by the ReLU activation function and cannot be easily extended to other popular activation functions such as GeLU (as in GPT models [8]) and SwiGLU (as in Llama [9]).

In this paper, we improve the one-shot sparsification of a model by expanding it into a mixture of experts, where each expert is a copy of the original, statically pruned to perform well on a subset of the inputs. Our technique, which we dub Sparse Expansion, begins by clustering input embeddings layerwise; based on this clustering, Sparse Expansion leverages the input-aware nature of the SparseGPT pruner to specialize different sparse experts to different sets of inputs, starting from the same base weights. At run time Sparse Expansion routes the inputs to the sparse experts at each layer based on the clustering (one could of course use more a sophisticated MoE router [6] in place of clustering). Our approach increases the overall size of the model, but greatly reduces the number of executed run time parameters. Through Sparse Expansion, we achieve state-of-the-art one-shot sparsification performance in terms of parameters activated per token, across LLMs from the Llama and Pythia families, at low accuracy loss. Moreover, we show that our approach can be coupled with efficient execution kernels, leading to non-trivial speedups for generative inference; for the largest linear layers in Llama 2 7B and 70B, a variant of our approach provides speedups of $3 \mathrm{x}$ and 4.8x, respectively, even taking the expert routing overhead into account.

Our main technical contribution is a detailed study of how this expansion method obtains performance. Every neuron in a network has a certain level of "entanglement," which we view as the degree to which it must deal with different combinations of features in its inputs. We observe for the first time the existence of "highly entangled" neurons in the dense model. These neurons, which appear in all model layers, have striking, irregular output distributions (Figure 1d), and as we show later, seem to be responsible for the difficult task of differentiating similar input vectors through their dot product computation. Furthermore, these neurons have an outsize effect on model performance: removing them increases error much more than a similar collection of random neurons (Figure 4). We found that one can quantify the degree to which a neuron is entangled via the Wasserstein distance [10] between its output distribution and a properly-normalized Gaussian.

Sparsity tends to push the outputs of a dot-product of a neuron towards the Gaussian distribution as it loses edges (see Figure 5), even with an adjustment of edge weights. It thus reduces the expressive power of each neuron, and leads to increased error. In accordance, high Wasserstein distance neurons are particularly difficult to sparsify, especially at higher sparsities.

![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-02.jpg?height=288&width=1387&top_left_y=1512&top_left_x=369)

Figure 1: The output distributions of neurons in Llama 2 7B. One can see that the red Sparse Expansion output distribution covers a random neuron slightly better than than the green SparseGPT distribution, and this improvement becomes much more pronounced for an entangled neuron. The reason is the ability of the separate dot product operations of the multiple experts, depicted each in a separate color (in c and f), to better cover the dense distribution. Sparsity is set to $90 \%$ per expert and the neurons are from the gate projection of the second FFN block. WD is the Wasserstein distance between the Sparse Expansion sparse output distribution and the dense distribution. RI is the relative improvement of Sparse Expansion over SparseGPT, defined later.

We provide evidence that the key to the performance of Sparse Expansion is that it effectively "disentangles" the neuronal output distributions, in particular those with high Wasserstein distances, into much simpler ones. For example, SparseGPT mostly succeeds in capturing the dense output distribution of a non-entangled neuron with a sparse one (Figure 1a). However, it fails to capture the full dense output distribution of a highly entangled neuron (Figure1d). Sparse Expansion overcomes this by disentangling the neuron. Each sparse expert is responsible only for a clustered subset of the inputs (i.e. can be viewed as capturing a subset of the input features) allowing it to model these with the more Gaussian shaped distribution imposed by sparsity. Each neuron's output is then modeled
by the collection of outputs of all the experts together. While sparse disentanglement helps cover the normal neuron's distribution slightly better than SparseGPT, it greatly helps it in covering the output distribution of the entangled neuron where SparseGPT fails (Figure 1f). The Sparse Expansion technique allows one to readily see this because, unlike in traditional MoEs, each neuron retains the exact same position and role in each of the experts.

Because Sparse Expansion effectively disentangles neurons throughout the model, it improves its overall sparse performance, and not surprisingly, as the number of experts applied to a model grows, and with it its ability to disentangle, the overall accuracy of Sparse Expansion improves (Figure A.1).

Finally, we note that in the literature, disentanglement is generally used in the context of mechanistic interpretability to generate human-understandable features [11, 12]. Here, we use disentanglement as a performance enhancing tool, and link it to improved performance via induced sparsity. We believe this phenomena is at the base of general MoE performance, but is harder to analyze because of the retraining of the experts.

In the next section we explain the implementation of Sparse Expansion in detail. We then study how it supports disentanglement in LLMs, and finally present results showing its performance relative to other state of the art one-shot compression techniques.

## 2 Sparse Expansion

![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-03.jpg?height=314&width=656&top_left_y=1072&top_left_x=363)

Sparse Expansion FFN Inference

![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-03.jpg?height=279&width=748&top_left_y=1102&top_left_x=1008)

Figure 2: The Sparse Expansion process. One-shot expert creation process of Sparse Expansion (left). Inference process in an FFN block of Sparse Expansion (right).

At a high level, Sparse Expansion clusters the incoming inputs to each layer into separate groups. Then, each expert is sparsified via the SparseGPT pruning algorithm [4]. Briefly, SparseGPT approximates the optimal sparse version w.r.t. L2 distance relative to the dense outputs of a given layer by leveraging the Hessian matrix of the second order derivatives of the error relative to the parameters within each linear layer $\mathbf{y}=\mathbf{W X}+\mathbf{b}$. Here, $\mathbf{y}$ is the set of outputs of the layer, $\mathbf{W}$ is the weight matrix for the layer, $\mathbf{b}$ is the bias for the layer, and $\mathbf{X}$ is a set of inputs arranged as a matrix, where each column is a separate input vector. Doing so yields $\mathbf{H}=\mathbf{X X}^{\mathrm{T}}$, where $\mathbf{H}$ is the Hessian matrix. Furthermore, SparseGPT is natively capable of performing one-shot quantization as well, due to the sharing of the error updates between sparsification and quantization. In our experiments, we thus test Sparse Expansion in both sparse and joint sparse-quantized settings.

In Sparse Expansion, we leverage the observation that SparseGPT is input-aware, and cluster the inputs to each layer via K-means and designating a separate, initially identical, expert for every cluster of inputs. (In order to facilitate better and faster clustering, we first utilize principal component analysis (PCA) to reduce the dimensionality of the inputs.) Following the clustering of the inputs, each sparse expert can now be tailored to a subset of inputs rather than trying to adapt to all of them. To allow the experts to specialize, a separate Hessian matrix is calculated for every input cluster, which is then used to prune the expert weight matrix through the SparseGPT pruning algorithm. Specialized experts have now been generated that are adapted to the subset of inputs routed to the corresponding cluster (Figure 2, Algorithm 1). Our Sparse Expansion approach therefore yields a mixture of sparse experts through one-shot sparsification, removing the need for expensive retraining.

During inference, each input will first have its dimensionality reduced by the PCA model, then separately evaluated via the K-means model to decide its expert, and finally be routed to the corresponding expert for the matrix multiply (Algorithm 2). As the routing is done via $\mathrm{K}$-means on a lower dimension, and the PCA is a very low dimension matrix multiply, both are inexpensive to add
on to normal LLM inference. Furthermore, routing in this manner prevents the need to train and run a more expensive router.

Our design departs from traditional MoEs in terms of their routing for the benefit of sparsity. In traditional MoEs, inputs are generally routed at the start of each FFN block once, such that the same up, gate, and down projection experts are associated with a particular routing. Furthermore, inputs are generally routed to multiple experts per inference pass, thus increasing the effective number of experts through the combination of different ones. However, routing to multiple experts in our case would incur a penalty in the sparsity of the model. Thus, we instead chose to do routing for the up and gate projection experts, and then again for the down projection expert (2). In this way, we square the effective number of experts combinations per input to a FFN block without decreasing sparsity.

## 3 Neuronal disentanglement

### 3.1 Characterizing non-Gaussian neuronal output distributions

We investigate the output distributions of individual neurons as the model processes its inputs during inference across all linear layers in each transformer block feed forward network (FFN). We focus our analysis in Pythia 1.4B [13] and Llama 2 7B [9]. Most neurons exhibit a reasonably Gaussian output distribution after their dot product operation on the input vector (Figure $3 \mathrm{k}, 1 \mathrm{k}$ ). However, we find the existence of a small group neurons with highly non-Gaussian outputs (Figure 3 , 15). We observe the existence of such neurons in all FFN blocks of transformers.

To characterize the degree of difference between the non-Gaussian output distributions of these neurons from the Gaussian-like output distribution of an average neuron, we considered several metrics. However, we ultimately chose the Wasserstein distance (WD) [10, 14] as a means of quantifying the degree of difference. In optimal transport theory, the WD measures the minimal transportation cost between two distributions, taking their geometry in real space into account.

To find the WD of every neuron to the Gaussian $\mathcal{N}$, we first normalize the output distributions of each neuron $n$ to have zero mean and unit variance, and compare this normalized distribution $n^{\prime}$ to $\mathcal{N}(0,1)$. This is because the range of neurons is quite variable, and we wanted to prioritize finding the differences in the shape of the output distribution, not other properties of the distribution. We use the 1-Wasserstein distance in one dimension, as shown in Equation 1 .

$$
\begin{equation*}
W_{1}\left(n^{\prime}, \mathcal{N}\right)=\int_{0}^{1}\left|F^{-1}(z)-G^{-1}(z)\right| d z \tag{1}
\end{equation*}
$$

$F^{-1}$ and $G^{-1}$ represent the inverse cumulative distribution function of $n^{\prime}$ and $\mathcal{N}(0,1)$, respectively, which can be approximated with our empirical data. To compute the WD of every neuron efficiently, we use the implementation provided by SciPy [15]. When computing the difference metric in this way, we find that our originally observed neurons have been designated correctly with high WD to $\mathcal{N}$.

### 3.2 Wasserstein distances and entanglement

All neurons in the models we examined show some non-zero WD, and yet we find neurons with a high WD in all up and gate projection layers, some down projection layers (Figure A.4) in Llama $27 \mathrm{~B}$. Henceforth we focus on the high WD neurons because they offer a clearer example of the correlation of the WD with the computational roles of neurons. The high WD neurons have the odd shapes that we have been observing (Figures A.10 and A.9). We consider the difficulty of the dot product computation neurons perform in terms of the input-output relationships they are expected to handle in four broad groups. Namely, the dot product may map similar inputs to similar outputs, dissimilar inputs to dissimilar outputs, dissimilar inputs to similar outputs, and similar inputs to dissimilar outputs. Of the four relationships, the last seems most difficult to model, as small changes in the inputs must be mapped far away in output space. This relationship would be even more difficult to maintain under sparsity. This is because, as the number of nonzero entries in the weight vector decrease, the means and variance both tend toward zero (Figure A.6. Such a setting makes it all the more difficult to keep similar inputs far apart in the output space.

We use cosine similarity to measure the similarity between inputs and $\mathrm{L} 1$ distance to measure the similarity between outputs following computation by a particular neuron's dot product. Larger cosine similarities indicate more similar inputs, and smaller L1 distances indicate more similar outputs. In Pythia 1.4B, for a neuron with a normal output distribution, the relationship between its input-output pairs is fairly reasonable. Very similar inputs with a cosine similarity of close to one have very similar outputs with L1 distance close to zero. Unrelated inputs with cosine similarity of close to zero have very far outputs with a large L1 distance between them. As input similarity decreases, output distance increases. Finally, there are pairs of dissimilar inputs that are mapped to close values. However, there are no input pairs that are similar with highly dissimilar outputs (Figure 3p).

However, for a high WD neuron, this is not the case-in addition to the previously mentioned relationships, it now has to contend with discerning between two similar inputs and map them far away in output space (Figure 3d). Note that even for two inputs that share a cosine similarity of 0.9 , this neuron sometimes has to map them across a majority of its entire effective range.
![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-05.jpg?height=308&width=1242&top_left_y=798&top_left_x=432)

Figure 3: Neuronal entanglement. The high WD neuron must map fairly similar inputs to outputs that are very far apart (as indicated by the dense upper right hand corner of (d) vs the empty one in (b)) through its dot product operation. The neurons are from the up projection of the second FFN block in Pythia 1.4B.

Because inputs that must be mapped far away in output space are close by and entangled in input space, we therefore term these high WD neurons as "entangled."

### 3.3 Effect of high Wasserstein neurons on sparsification

We show that entangled neurons have a substantially outsized effect on model performance through the lens of sparsity. In Llama 2 7B, if just the $3 \%$ of the highest WD neurons are sparsified in every FFN block, model performance significantly degrades, far more than if random neurons were sparsified. The same is true if the neurons are sparsified $2: 4$ and then quantized to fewer bits, with high WD neurons drastically inhibiting model performance under compression. As compression increases, this effect becomes more obvious (Figure 4). Therefore, entangled neurons are crucial for maintaining model accuracy and are severely limited in their ability to be compressed.
![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-05.jpg?height=394&width=1248&top_left_y=1814&top_left_x=430)

Figure 4: Entangled neurons are sensitive to compression. In Llama 2 7B, 3\% of neurons from every layer were sparsified in an unstructured manner (left) or sparsified to $2: 4$ and quantized (right). Compressing high WD neurons impairs the model significantly more than compressing random ones.

Additionally, as a neuron is increasingly sparsified, the output distribution becomes more Gaussian (See Figure 5). This in turn places even more stress upon the neuron - not only is it contending with the decreasing mean and variance, but also with the less expressive distribution shape. Thus, it seems
that, especially at the higher sparsities that we are investigating, the irregular shape of the entangled neurons is much more challenging to model with fewer weights than a Gaussian-like distribution. To address the difficulty of sparsifying entangled neurons, we introduce Sparse Expansion.

![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-06.jpg?height=309&width=1244&top_left_y=434&top_left_x=430)

Figure 5: Increasing sparsity induces normality. A highly entangled neuron's dense distribution (blue) and sparse distributiuon (red). As sparsity increases, the output distribution of sparse neurons becomes progressively more Gaussian. WD represents the Wasserstein distance. This is the same neuron from Figure 3 .

## 4 Sparse Expansion disentangles neurons

We revisit the output distributions of neurons to determine the effect that clustering has in a sparse setting. We measure the relative improvement (RI) that Sparse Expansion provides, where relative improvement is defined as the ratio of the L2 error between the dense output and the SparseGPT sparse output to the L2 error between the dense output and the Sparse Expansion sparse output.

For Llama 2 7B (Figure 1) and Pythia 1.4B (Figure A.3), both models and both neuron types-random and entangled-improve through Sparse Expansion, with the entangled neuron showing greater improvement. Furthermore, for the random neuron and especially for the entangled neuron, the geometry of the sparse output distribution much more closely matches that of the dense distribution in Sparse Expansion.

We also provide a visualization for the specialization of each cluster. Figure 1 and Figure A. 3 each show the sparse output distributions of each individual cluster, with a different color per expert. For the randomly selected neurons, there is an improvement though each expert by and large is still responsible for approximately the same range and shape. For the entangled neurons, there is significant specialization for different parts of the distribution further away from the mean.

However, Sparse Expansion is not limited to improving just neurons with a high WD to normal. Across different sparsities and across different models, all but a tiny fraction of neurons improve through Sparse Expansion (Figure A.7). Thus, we believe that, in fact, every neuron is to some extent entangled. However, neurons with a high WD are the most obviously entangled neurons, and they benefit more from sparse disentanglement, especially at higher sparsities.

Finally, we note that other metrics of the dense neuronal output distribution, such as their means and variances, fail to act as a predictor of neuronal improvement to the degree that the Wasserstein distance to a Gaussian does (Figure A.8). Thus, we believe the Wasserstein distance to be the most suitable metric of entanglement within a neuron.

### 4.1 More sparse experts better fit the output distribution

The complex dense output distribution of highly entangled neurons is difficult to model with a single sparse expert, as in the case of SparseGPT. In figure 6, we show the output distribution of a high WD neuron in both dense and sparse computation. As the number of sparse experts increases, the output distribution of the sparse computation more closely matches that of the dense computation, as measured in both WD between the two distributions and relative improvement.
![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-07.jpg?height=302&width=1226&top_left_y=244&top_left_x=442)

Figure 6: Modeling recovery with more experts. The sparse computation output distribution (red) matches the dense one (blue) better with more clusters. Sparsity is set to $90 \%$ for each expert. WD represents the Wasserstein distance between the Sparse Expansion sparse output distribution and the dense distribution. RI represents relative improvement of Sparse Expansion ( $n \geq 1$ clusters) over SparseGPT ( $n=1$ cluster). This is the same neuron from Figure 3 .

## 5 Sparse Expansion performance

### 5.1 Models, datasets, and setup

We use the Pythia [13] series of pre-trained LLMs to evaluate how Sparse Expansion performs across model sizes, from Pythia 70M to Pythia 12B. We further evaluate Sparse Expansion across the entire Llama 2 family [16]. We use the Wikitext-2 dataset [17] as calibration data for input-aware pruning and evaluation. For our performance benchmarks, we use 16 clusters at each level of routing in Sparse Expansion. For all experiments, we use two NVIDIA A100 Tensor Core GPUs, except for Llama 2-70B where we use three NVIDIA A100s. We rely upon the RAPIDS library [18] to accelerate the fitting and running of the PCA and K-means models by orders of magnitude. To run all models densely and through the MP, Wanda, and SparseGPT pruning approaches, we use a single AMD Instinct MI100 Accelerator GPU. Finally, we utilize and build upon the SparseGPT repositories.

### 5.2 Compression across scales

We evaluate the performance of Sparse Expansion against other one-shot pruning techniques across a range of model sizes in Pythia (Figure 7). To test both sparsification as well as compression, we compare the pruning techniques when sparsifying each model to $50 \%$ unstructured sparsity, as well as to $2: 4$ structured sparsity and 3 and 4 bit quantization. Across all model sizes, Sparse Expansion outperforms all other pruning techniques, approaching dense performance as model size increases. In fact, for compression, Sparse Expansion performs better at 3 bits than SparseGPT does at 4 bits across all model sizes but one.
![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-07.jpg?height=386&width=1244&top_left_y=1846&top_left_x=432)

Figure 7: Sparse Expansion across model sizes. Comparisons were made between Magnitude Pruning (MP), Wanda, SparseGPT, and Sparse Expansion on the Pythia series of models from sizes of 70M parameters to 12B parameters. Every MLP in each model was sparsified to $50 \%$ sparsity (left) or sparsified 2:4 and quantized to 3 or 4 bits (right). Performance was measured in cross-entropy loss to better show scaling laws.

### 5.3 Performance across sparsification and quantization

To assess the performance of Sparse Expansion at different levels of sparsity or compression for the same model, we compared it to SparseGPT in Llama 2 7B (Figure 8). Every linear layer in each FFN block was sparsified in an unstructured manner across different degrees of sparsity, or sparsified in a structural manner to 2:4 sparsity and then quantized to a different number of bits. Across all levels of sparsity and compression, Sparse Expansion performs better than SparseGPT. Furthermore, at higher levels of sparsity and compression, the gap in performance between the two techniques grows. Here as well, Sparse Expansion performs better at 3 bits than SparseGPT does at 4 bits.
![](https://cdn.mathpix.com/cropped/2024_06_04_07c6ac63fe8f301ae206g-08.jpg?height=400&width=1266&top_left_y=610&top_left_x=424)

Figure 8: Sparse Expansion across levels of compression. Comparisons were made between SparseGPT and Sparse Expansion on Llama 2 7B. Every layer in each transformer block was sparsified in an unstructured manner (left) or sparsified $2: 4$ sparsity and quantized to the number of bits (right). Sparse Expansion performs the best across all settings as compared to SparseGPT.

### 5.4 Performance across the Llama family

Finally, we analyze the performance of Sparse Expansion against other sparsification algorithms across all members of the Llama 2 family - Llama 2 7B, Llama 2 13B, and Llama 2 70B - both under sparsity and joint sparsity-quantization compression.

Table 1: Sparse Expansion across the Llama family. Sparse Expansion outperforms all other sparsification techniques for every member of the Llama family.

|  | Sparsity | Bits | Llama 2 7B | Llama 2 13B | Llama 2 70B |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Dense | $0 \%$ | 16-bit | 5.1168 | 4.5736 | 3.3192 |
| MP | $50 \%$ | 16-bit | 16.029 | 6.8270 | 4.9846 |
| Wanda | $50 \%$ | 16-bit | 6.7757 | 5.8527 | 4.0219 |
| SparseGPT | $50 \%$ | 16-bit | 5.7082 | 5.0521 | 3.9013 |
| Sparse Expansion | $50 \%$ | 16-bit | $\mathbf{5 . 5 8 3 9}$ | $\mathbf{4 . 9 7 2 8}$ | $\mathbf{3 . 8 7 9 1}$ |
| SparseGPT | $2: 4$ | 16-bit | 6.9767 | 5.9934 | 4.8002 |
| Sparse Expansion | $2: 4$ | 16-bit | $\mathbf{6 . 4 4 5 6}$ | $\mathbf{5 . 6 2 5 5}$ | $\mathbf{4 . 6 6 7 1}$ |
| SparseGPT | $2: 4$ | 4-bit | 7.2759 | 6.1101 | 4.9036 |
| Sparse Expansion | $2: 4$ | 4-bit | $\mathbf{6 . 5 7 4 5}$ | $\mathbf{5 . 7 1 5 1}$ | $\mathbf{4 . 7 5 8 6}$ |
| SparseGPT | $2: 4$ | 3-bit | 13.076 | 6.5055 | 5.2552 |
| Sparse Expansion | $2: 4$ | 3-bit | $\mathbf{7 . 0 7 5 7}$ | $\mathbf{5 . 9 8 7 2}$ | $\mathbf{5 . 0 5 8 8}$ |

Sparse Expansion outperforms all other pruning techniques for both $50 \%$ unstructured sparsity as well 2:4 sparsity in all Llama models (Figure 1). In addition to non-quantized sparsity, we consider how Sparse Expansion performs in the context of compression with 2:4 structured sparsity and quantization. We first sparsify each linear layer in each FFN block to $2: 4$ sparsity, then quantized to 3 and 4 bits. Our method outperforms SparseGPT across all models and across both conditions and in all models (Figure 1).

Across multiple model sizes, sparsity and compression levels, and advanced models, Sparse Expansion attains state-of-the-art performance for post-training one-shot sparsification when compared to other highly competitive pruning techniques. We do so by leveraging the powerful pruning algorithm of SparseGPT and combining it with input specialization to utilize the insights we gain from how entangled neurons behave under sparsity.

### 5.5 Inference speed

To evaluate the inference latency of Sparse Expansion we implemented a Sparse Expansion layer based on PyTorch and optimized sparse-quantized GPU kernels called Sparse Marlin [19, 20], which supports the INT4 $+2: 4$ sparse format. We use a linear layer of appropriate size as an upper bound approximation for our router cost, which is followed by 4 bits $2: 4$ sparse matrix multiplication. We have run the layer-wise benchmarks for the typical layers sizes from Llama models on a single RTX3090 GPU. We can see in Table 2 that Sparse Expansion allows us to get up to $4.8 \times$ speedup over the dense baseline. The speedup comes from the highly-compressed linear layer representation, while the overhead over the original kernels can be explained by the effect of interleaved calls of dense router and sparse experts during Sparse Expansion inference.

Table 2: Sparse Expansion inference speedup. Layer-wise single batch inference latency (in $\mu \mathrm{s}$ ). The layer sizes are chosen specifically to match the layers of Llama 2 7b and Llama 2 70B.

| Layer Size | $4 \mathrm{k} \times 12 \mathrm{k}$ | $4 \mathrm{k} \times 22 \mathrm{k}$ | $11 \mathrm{k} \times 4 \mathrm{k}$ | $8 \mathrm{k} \times 10 \mathrm{k}$ | $8 \mathrm{k} \times 57 \mathrm{k}$ | $28 \mathrm{k} \times 8 \mathrm{k}$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Dense | 132 | 227 | 114 | 220 | 1168 | 556 |
| Sparse Expansion | 76 | 76 | 75 | 76 | 241 | 138 |
| Speedup | $1.7 \times$ | $3.0 \times$ | $1.5 \times$ | $2.9 \times$ | $4.8 \times$ | $4.0 \times$ |
| Sparse | 26.8 | 44.7 | 24.4 | 42.3 | 216 | 109 |
| Expansion Overhead | $2.9 \times$ | $1.7 \times$ | $3.1 \times$ | $1.8 \times$ | $1.1 \times$ | $1.3 \times$ |

## 6 Additional related work

We cover additional related work not cited above.

Sparsification Recently, several papers have attempted to impart activation sparsity to non-ReLU models by converting their activation functions to ReLU, as is the case with Relufication [21], or to directly threshold post-activation function values based on magnitude, as is the case with CATS [22]. However, the former approach requires expensive retraining, and the latter requires one linear layer in each feedforward network (FFN) block to be run densely.

Mixture of experts MoE models involve specializing components of a neural network to different sets of inputs during training and and inference [23]. MoEs have been developed for LLMs [24], and recent MoE models have achieved state of the art performance over standard LLM architectures [25]. An arising area of $\mathrm{MoE}$ research is converting a model with standard architecture into an MoE, either for performance or for sparsity. One approach is to do so through first fully duplicating the weight matrix of linear layers in FFN blocks as separate experts, then fully training a router for and fine-tuning the experts [26]. Other approaches center around models that utilize the ReLU activation function [6, 5, 7], and very recently non-ReLU activation functions [22]. As different inputs sparsely activate different sets of neurons, neurons that activate together can be grouped together into a single expert. Different sets of experts can then be run for different inputs in a dynamic fashion, essentially creating an MoE like architecture.

## 7 Discussion and conclusion

In this work, we have shown for the first time the role of entanglement, and in particular of highly entangled neurons, in the accuracy of computing the FFN blocks of LLMs. We have shown that the Wasserstein distance between the output distributions of neurons and the Gaussian distribution is a very suitable metric to identify the degree of entanglement in different neurons. Furthermore, we demonstrated the relevance of highly entangled neurons in the performance of models, both in terms of differentiating between similar inputs and in their significant sensitivity to sparsification and compression. As the focus of our study is analytical, we have only shown per-layer inference speeds. In future work, we plan to integrate with an inference framework such as HuggingFace Transformers to show end-to-end inference numbers. We also plan to integrate with MoE-specific inference optimizations such as [27], which should be compatible with our approach as well.

Looking forward, perhaps, just as outlier features and weights are well understood to be one of the most significant challenges when quantizing to fewer bits [28, 29, 30, 31, 32, 33], neuronal entanglement can provide a path to understanding the challenge of pruning to higher sparsities.

## 8 Acknowledgements

The authors would like to extend their gratitude to Lori Leu for her insightful comments on the application of the Wasserstein distance metric. We also wish to thank Elias Frantner for his help in working with the SparseGPT implementation and his advice for the project. Additionally, we would like to thank Tony Tong Wang for his valuable feedback and constructive discussions.

This work was supported by an NIH Brains CONNECTS U01 grant and AMD's AI \& HPC Fund.

## References

[1] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research, 22(241):1-124, 2021.

[2] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.

[3] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.

[4] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323-10337. PMLR, 2023.

[5] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023.

[6] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Transformer feed-forward layers are mixtures of experts. arXiv preprint arXiv:2110.01786, 2021.

[7] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers. Advances in Neural Information Processing Systems, 34:9895-9907, 2021.

[8] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.

[9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[10] Leonid V Kantorovich. On the translocation of masses. Journal of mathematical sciences, 133 (4):1381-1382, 2006.

[11] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.

[12] Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR (Poster), 3, 2017.

[13] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR, 2023.

[14] Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.

[15] Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 17(3):261-272, 2020.

[16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[17] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[18] Sebastian Raschka, Joshua Patterson, and Corey Nolet. Machine learning in python: Main developments and technology trends in data science, machine learning, and artificial intelligence. arXiv preprint arXiv:2002.04803, 2020.

[19] Elias Frantar and Dan Alistarh. Marlin: a fast 4-bit inference kernel for medium batchsizes. https://github.com/IST-DASLab/marlin, 2024.

[20] Roberto Lopez Castro and Dan Alistarh. Sparse marlin: a fast sparse plus 4-bit kernel for generative inference. https://github.com/IST-DASLab/Sparse-Marlin, 2024.

[21] Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar. Relu strikes back: Exploiting activation sparsity in large language models. arXiv preprint arXiv:2310.04564, 2023.

[22] Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. Cats: Contextually-aware thresholding for sparsity in large language models. arXiv preprint arXiv:2404.08763, 2024.

[23] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79-87, 1991.

[24] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[25] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

[26] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055, 2022.

[27] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.

[28] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.

[29] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, $35: 30318-30332,2022$.

[30] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024.

[31] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024.

[32] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless $11 m$ weight compression. arXiv preprint arXiv:2306.03078, 2023.

[33] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023.
