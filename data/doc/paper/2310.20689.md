# Learning From Mistakes Makes LLM Better Reasoner 

\author{
Shengnan An ${ }^{* \diamond, ゅ}$, Zexiong Ma* ${ }^{*}, \uparrow$, Zeqi Lin ${ }^{+\infty}$, Nanning Zheng ${ }^{\dagger \diamond}$, <br> Jian-Guang Lous, Weizhu Chen ${ }^{\circ}$ <br> $\diamond$ IAIR, Xi'an Jiaotong University, "Microsoft Corporation, ${ }^{\ominus}$ Peking University <br> $\diamond\{$ an1006634493@stu, nnzheng@mail \}.xjtu.edu.cn, <br> $\odot_{\text {mazexiong@stu.pku.edu.cn, }}^{\text {\$Zeqi.Lin, jlou, wzchen\}@microsoft.com }}$

}


#### Abstract

Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at Github Link.


![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-01.jpg?height=648&width=1212&top_left_y=1625&top_left_x=454)

Question: Tina makes $\$ 18.00$ an hour. If she works more than 8 hours per shift, she is
eligible for overtime, which is paid by your hourly wage $+1 / 2$ your hourly wage

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-01.jpg?height=336&width=450&top_left_y=1629&top_left_x=1203)

Correction

Incorrect Step: Step 3

Explanation: Step 3 only calculates the earnings for one day, but not for the entire five days.

‥ Step 3: For one day, she makes $\$ 144.00+\$ 54.00=\$ 198.00$.

Step 4: For 5 days, she makes $\$ 198.00 * 5=\$ 990.00$.

Step 5: The answer is 990 .

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-01.jpg?height=284&width=447&top_left_y=1983&top_left_x=1208)

Figure 1: Left: Process of LEarning from MistAkes (LEMA). Right: Performance of LEMA on GSM8K and MATH.[^0]

Preprint.

## 1 Introduction

Mistakes are the portals of discovery.

—James Joyce

With exponential growth in data size and model scale, contemporary large language models (Brown et al., 2020: Zhang et al., 2022; Hoffmann et al., 2022: Smith et al. 2022; OpenAI, 2023b; Anil et al. 2023) have demonstrated significant advancements on various NLP tasks, particularly in mathematical problem solving that necessitates complex chain-of-thought (CoT) reasoning (Wei et al., 2022; Wang et al., 2022; Li et al., 2023b, Shi et al. 2023, Qin et al., 2023: Lightman et al. 2023). In terms of performance on challenging mathematical tasks like GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al., 2021), proprietary large language models, including GPI-4 (OpenAI, 2023b) and PaLM-2 (Anil et al., 2023), have attained notable results. However, open-source LLMs such as LLaMA-2 (Touvron et al., 2023b) still have much room for improvement.

To further improve the CoT reasoning capabilities of open-source LLMs for tackling mathematical tasks, a common approach is to fine-tune these models using annotated/generated question-rationale data pairs (referred to as CoT data), which directly teach the model how to perform CoT reasoning on these tasks (Magister et al., 2022. Huang et al., 2022, Ho et al., 2022; Li et al., 2022: Yuan et al., 2023; Luo et al., 2023; Yu et al., 2023; Li et al., 2023a; Liang et al., 2023, Ranaldi \& Freitas, 2024). While this straightforward learning process has exhibited its effectiveness, this study investigates whether the reasoning capabilities of LLMs can be further improved through a backward learning process, i.e., learning from the mistakes that LLMs have made. The insight of learning from mistakes comes from the learning process of human students. Consider a student who is just beginning to learn math. Beyond learning from golden knowledge and examples in books, he also does exercises. After failing to solve a problem, he will learn what mistakes he made and how to correct them. By learning from the mistakes he has made, his reasoning capability will be further improved. Inspired by this error-driven learning process, this work explores whether the reasoning capabilities of LLMs can also benefit from understanding and correcting mistakes.

To this end, we first generate mistake-correction data pairs (referred to as correction data) and then inject these correction data into the CoT fine-tuning process (Figure 1). For generating correction data, we employ multiple LLMs, including the LLaMA and GPT series models, to collect inaccurate reasoning paths (i.e., with incorrect final answers). We then use GPT-4 as the "corrector" to generate corrections for these inaccurate reasoning paths. The generated corrections contain three pieces of information: (1) the incorrect step in the original solution, (2) an explanation of why this step is incorrect, and (3) how to correct the original solution to arrive at the correct final answer. After filtering out corrections with incorrect final answers, our human evaluation reveals that our correction data exhibits adequate quality for the subsequent fine-tuning stage. In addition to using the original training questions to generate correction data, we also consider extending the question sets to scale up our correction data. Inspired by the evolution techniques for CoT data $\sqrt{\mathrm{Xu}}$ et al., 2023; Yu et al. 2023; Li et al., 2023a), we apply a correction-centric evolution strategy: compared to randomly selecting seed questions for evolution, our correction-centered evolution focuses more on moderately difficult questions for expanding the correction data. We blend the generated correction data with the CoT data and then fine-tune LLMs to perform LEarning from MistAkes (LEMA).

Our experiments on five open-source LLMs and five challenging reasoning tasks demonstrate the effectiveness of LEMA. Compared to fine-tuning on CoT data alone, LEMA consistently improves the performance across various LLMs and tasks. For instance, LEMA with LLaMA-2-70B (Touvron et al. 2023b) achieves $83.5 \%$ on GSM8K and $25.0 \%$ on MATH, while fine-tuning on CoT data alone yields $81.4 \%$ and $23.6 \%$, respectively. By incorporating our correction-centric evolution strategy on MATH, LEMA with LLaMA-2-70B can be further improved from $25.0 \%$ to $29.3 \%$. Moreover, LEMA can also enhance specialized LLMs such as WizardMath (Luo et al., 2023) and MetaMath(Yu et al. 2023). In addition to math tasks, LEMA also benefits commonsense reasoning, improving the performance of LLaMA-2-70B on CSQA (Talmor et al. 2019) from $84.2 \%$ to $85.3 \%$.

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-03.jpg?height=404&width=1358&top_left_y=278&top_left_x=381)

Figure 2: Process of generating and expanding correction data.

Beyond these impressive results, our ablation studies on correction data shed further light. In controlling the training data sizes and training tokens to be the same, our experimental results reveal that mixing CoT and correction data outperforms a single data source. These results indicate the non-homogeneous effectiveness of CoT data and correction data. Moreover, compared with randomly selecting seed questions, our correction-centric evolution better improves the performance of LEMA. It demonstrates that moderately difficult questions are more suitable for expanding the correction data.

## 2 Methodology

LEMA consists of three primary stages: generating correction data, correction-centric evolution, and fine-tuning.

### 2.1 Correction Data Generation

Figure 2 briefly illustrates the process of generating correction data. Given a question-answer example $\left(q_{i}, a_{i}\right) \in \mathcal{Q}$, a corrector model $\mathcal{M}_{c}$, and a reasoning model $\mathcal{M}_{r}$, we generate the mistake-correction data pair $\left(q_{i} \oplus \widetilde{r_{i}}, c_{i}\right) \in \mathcal{C}$, where $\widetilde{r_{i}}$ represents an inaccurate reasoning path to the question $q_{i}$, and $c_{i}$ denotes the correction for $\widetilde{r}_{i}$.

Collecting Inaccurate Reasoning Paths. We first sample multiple reasoning paths for each question $q_{i}$ using the reasoning model $\mathcal{M}_{r}$ and retain paths not achieving the correct final answer $a_{i}$,

$$
\begin{equation*}
\widetilde{r}_{i} \sim \mathcal{M}_{r}\left(\mathcal{P}_{r} \oplus q_{i}\right), \quad \operatorname{Ans}\left(\widetilde{r}_{i}\right) \neq a_{i} \tag{1}
\end{equation*}
$$

where $\mathcal{P}_{r}$ is the few-shot prompt instructing the model to perform CoT reasoning, and Ans $(\cdot)$ extracts the final answer from the reasoning path.

Generating Corrections for Mistakes. For question $q_{i}$ and the inaccurate reasoning path $\widetilde{r}_{i}$, we employ the corrector model $\mathcal{M}_{c}$ to generate a correction and check the final answer in the correction,

$$
\begin{equation*}
c_{i} \sim \mathcal{M}_{c}\left(\mathcal{P}_{c} \oplus q_{i} \oplus \widetilde{r}_{i}\right), \quad \operatorname{Ans}\left(c_{i}\right)=a_{i} \tag{2}
\end{equation*}
$$

where $\mathcal{P}_{c}$ contains 4 annotated mistake-correction examples to guide the corrector model what kind of information should be contained in the generated corrections. Figure 3 briefly illustrates $\mathcal{P}_{c}$. Specifically, the annotated corrections comprises three pieces of information:

- Incorrect Step: which step in the original reasoning path has made a mistake.
- Explanation: explain what kind of mistake has been made in this step.
- Correct Solution: how to revise the original reasoning path to achieve the correct answer.

Human Evaluation for Generated Corrections. Before generating data on a large scale, we first manually assess the quality of the generated corrections. We take LLaMA-2-70B as $\mathcal{M}_{r}$, utilize GPT-4 as $\mathcal{M}_{c}$, and generate 50 mistake-correction data pairs based on the GSM8K training set. We classify the corrections into three quality levels.

## Prompt For Generating Corrections

```
For the following math word problems, the original solutions may contain errors. Please identify the incorrect step in each solution, explain why it is incorrect,
and provide the correct solution starting from that step
Question: James creates a media empire. He creates a movie for $2000. Each DVD cost $6 to make. He sells it for 2.5 times that much. He sells 500 movies a
day for 5 days a week. How much profit does he make in 20 weeks?
Original Solution: Step 1, 500 movies a day, 5 days a week, for 20 weeks, he sells 500 * 5*20=50000 movies. Step 2, Each movie he sells for $6 * 2.5 =
$15. Step 3, Thus, he makes a profit of $15-$6 = $9 per movie. Step 4, Therefore, he makes a profit of $9*50000=$450000. Step 5, The answer is 450000
Incorrect Step: Step 4
Explanation: The error in Step 4 is that it does not take into account the initial cost of creating the movie ($2000). The correct solution should subtract this
amount from the total profit calculated in Step 4.

```

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-04.jpg?height=27&width=1033&top_left_y=583&top_left_x=393)

```
(Another 3 annotated examples)
Question: }\mp@subsup{\boldsymbol{q}}{i}{}\mathrm{ , Original Solution: 
```

Figure 3: A brief illustration of our prompt for generating corrections, containing the incorrect step in the original solution, the reason of mistake, and the corrected step .

- Excellent: the corrector successfully identifies the incorrect step in $\widetilde{r}_{i}$, provides a reasonable explanation, and the corrected reasoning path exhibits high continuity with the pre-steps in the original reasoning path ${ }^{1}$.
- Good: the corrector successfully identifies the incorrect step in $\widetilde{r}_{i}$, provides a reasonable explanation, while the corrected reasoning path has minor issues in continuity.
- Poor: the corrector fails to identify the incorrect step in $\widetilde{r}_{i}$ or provides unreasonable explanations.

Appendix B. 1 lists several examples under each quality level. Our evaluation finds that 35 out of 50 generated corrections are of excellent quality, 11 are good, and 4 are poor. Based on this human evaluation, we suppose the overall quality of corrections generated with GPT-4 is sufficient for the further fine-tuning stage. We generate corrections on a large scale and take all corrections that have correct final answers for fine-tuning LLMs. We provide further analysis on the choice and behavior of corrector model in Section D. 6

### 2.2 Correction-Centric Evolution

After building up the data generation pipeline, we explore how to scale up our correction data. We consider that expanding the question-answer set $\mathcal{Q}$ is a promising direction, as it primarily determines the correction data diversity.

Inspired by the recent success of evolution techniques on CoT augmentation $\mathrm{Xu}$ et al. 2023; Yu et al. 2023; Li et al. 2023a), we explore how to effectively apply the evolution method to expand our correction data. The "evolution" means to generate a set of new question-answer pairs from the given seed questions by prompting powerful LLMs.

The general evolution method for CoT augmentation randomly selects seed questions to evolve. However, this strategy does not well suit the nature of our correction data, as too simple or too challenging questions are less valuable for evolving and collecting correction information.

- For too simple questions, the reasoning models such as LLaMA can already solve them. Evolving these questions may not be effective for collecting mistakes.
- For too challenging questions, the most powerful LLMs still cannot handle them. Evolving these questions may lead to much inaccurate information in corrections.

Therefore, we apply a correction-centric evolution strategy which more focuses on moderately difficult questions: we only sample seed questions that occur in our correction data $\mathcal{C}$, rather than randomly sampling from the entire set $\mathcal{Q}$,

$$
\begin{equation*}
\hat{q}_{i} \sim \mathcal{M}_{e}\left(\mathcal{P}_{e} \oplus q_{i}\right), \quad q_{i} \in \mathcal{C} \tag{3}
\end{equation*}
$$[^1]

Table 1: Our main experimental results (\%) on four mathematical reasoning tasks (GSM8K, MATH, SVAMP and ASDiv) and one commonsense reasoning task (CSQA). Appendix D. 1 and D.2 illustrate the performance variances during training.

| Model | Training | Tasks |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | GSM8K | MATH | SVAMP | ASDiv | CSQA |
| LLaMA-2-70B Touvron et al., 2023b) | CoT Fine-Tuning | 81.4 | 23.6 | 80.3 | 80.7 | 84.2 |
|  | + Learning From Mistakes | $83.5(+2.1)$ | $25.0(+1.4)$ | $81.6(+1.3)$ | $82.2(+1.5)$ | $85.3(+1.1)$ |
| LLaMA-65B Touvron et al., 2023a) | CoT Fine-Tuning | 76.2 | 19.7 | 71.9 | 77.4  | 83.1 |
|  | + Learning From Mistakes | $77.9(+1.7)$ | $20.8(+1.1)$ | $72.8(+0.9)$ | $77.7(+0.3)$ | $84.0(+0.9)$ |
| CodeLLaMA-34B Rozière et al., 2023) | CoT Fine-Tuning | 68.8 | 19.1 | 67.4 | 73.9 | 78.1 |
|  | + Learning From Mistakes | $71.7(+2.9)$ | $20.4(+1.3)$ | $72.0(+4.6)$ | $74.4(+0.5)$ | $80.8(+2.7)$ |
| LLaMA-2-13B Touvron et al., 2023b) | CoT Fine-Tuning | 62.9 | 12.2 | 58.0 | 67.8 | 80.4 |
|  | + Learning From Mistakes | $65.7(+2.8)$ | $12.6(+0.4)$ | $62.0(+4.0)$ | $71.1(+3.3)$ | $81.9(+1.5)$ |
| LLaMA-2-7B Touvron et al., 2023b) | CoT Fine-Tuning | 52.6 | 8.7 | 53.0 | 63.8 | 76.9 |
|  | + Learning From Mistakes | $54.1(+1.5)$ | $9.4(+0.7)$ | $54.1(+1.1)$ | $65.5(+1.7)$ | $78.8(+1.9)$ |

where $q_{i}$ is the seed question, and $\mathcal{M}_{e}$ and $\mathcal{P}_{e}$ are the LLM and prompt for evolving questions, respectively. Appendix B.3 illustrates our $\mathcal{P}_{e}$.

The underlying principle of this strategy is straightforward. If one question frequently appears in correction data, it means that this question is not well solved by many reasoning models, but its inaccurate reasoning paths can be well handled by the corrector model.

### 2.3 Fine-Tuning LLMs

After generating the correction data, we fine-tune LLMs to examine whether these correction data can facilitate CoT reasoning. We compare the results under two settings:

- Fine-Tuning on CoT Data Alone. In addition to the annotated data in each task, we additionally take CoT data augmentation following existing methods (Yuan et al. 2023. Li et al., 2023a; Yu et al. 2023). We generate more reasoning paths for each question in the training sets with GPT-4 and filter out paths with wrong final answers. We apply this CoT data augmentation to set up strong fine-tuning baselines that only utilize CoT data.
- Fine-Tuning on CoT Data + Correction Data. We fine-tune LLMs on both CoT data and generated mistake-correction data. This setting is referred to as LEMA.

Appendix B.2 shows the input-output formats of CoT data and correction data used for fine-tuning and evaluation.

## 3 Experimental Setup

### 3.1 Tasks

We undertake experiments on five challenging reasoning tasks, including four mathematical reasoning tasks (GSM8K, MATH, SVAMP and ASDiv) and one commonsense reasoning task (CSQA) ${ }^{2}$. For GSM8K, MATH and CSQA, we generate correction data based on their training sets. For SVAMP and ASDiv, we take the same training data for GSM8K.

GSM8K (Cobbe et al. 2021) contains high quality linguistically diverse grade school math word problems. It has 7,473 training examples with CoT and 1,319 test cases.

MATH (Hendrycks et al. 2021) examines math reasoning on solving challenging competition mathematics problems. It contains 7,500 training CoT data and 5,000 test cases.

SVAMP (Patel et al. 2021) consists of questions with short NL narratives as state descriptions. For evaluation on SVAMP, we use the same training data as for GSM8K and take all 1,000 examples in SVAMP as test cases.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-06.jpg?height=404&width=1349&top_left_y=270&top_left_x=388)

Figure 4: Performances of LEMA and CoT-alone fine-tuning with controlled data sizes ( $32 \mathrm{~K}$ and $45 \mathrm{~K}$ ) on GSM8K. See Table 2 for results with controlled number of training tokens.

ASDiv (Miao et al., 2020) is a diverse math dataset in terms of both language patterns and problem types for evaluating. For evaluation on ASDiv, we use the same training data as for GSM8K and test on 2,084 examples in ASDiv ${ }^{3}$

CSQA (Talmor et al., 2019) is a question answering dataset for commonsense reasoning. It has 9,741 examples in the training set and 1,221 examples in the dev set. As it does not contain any CoT annotation, we first annotate 4 CoT examples (detailed in Appendix C.3), then take its training set to augment CoT data and generate correction data.

### 3.2 Data Construction

CoT Data. For GSM8K (also SVAMP and ASDiv), the CoT data contains all training examples of GSM8K and 24,948 augmented reasoning paths. We first generate 30,000 reasoning paths with GPT-4 and filter out 5,052 paths with wrong final answers or unexpected format ${ }^{4}$ For MATH, the CoT data contains all training examples and 12,509 augmented reasoning paths. We sample 30,000 reasoning paths with GPT-4 and filter out 17,491 paths. For CSQA, we generate 15,000 reasoning paths with GPT-4 and then filter out 4,464 paths.

Correction Data. We utilize multiple LLMs to collect inaccurate reasoning paths, including LLaMA-2 (Touvron et al., 2023b), WizardLM (Xu et al., 2023), WizardMath (Luo et al., 2023), Text-Davinci-003 (OpenAI, 2023c), GPT-3.5-Turbo (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b). We take GPT-4 as the corrector model. Finally, we collect 12,523, 6,306, 7,241 mistakecorrection pairs based on the training sets of GSM8K, MATH and CSQA, respectively.

Correction-Centric Evolution. We take $10 \mathrm{~K}$ bootstrap samples from the questions in our correction data. We utilize GPT-4 to evolve the questions. To generate "ground-truth" answers for the evolved questions, we utilize GPT-4 to sample three answers for each question and conduct a majority voting. The question that leads to three different answers will be filtered. Note that the evolved data will only be used in Section 4.2 .

### 3.3 Fine-Tuning and Evaluation

We fine-tune multiple open-source LLMs in the LLaMA (Touvron et al., 2023a), LLaMA2 (Touvron et al., 2023b), CodeLLaMA (Rozière et al. 2023), WizardMath (Luo et al., 2023) and MetaMath (Yu et al. 2023) families. We utilize QLoRA ${ }^{5}$ (Hu et al., 2022; Dettmers et al., 2023) by default to conduct parameter-efficient fine-tuning (PEFT) for these models. We set low-rank dimension as 64 and dropout rate as 0.05 . We set learning rate as 0.0001 for LLMs[^3]

| Model | Data | Acc (\%) |
| :---: | :---: | :---: |
| LLaMA-2-70B | CoT-5.8M | 82.1 |
|  | LEMA-5.8M | $\mathbf{8 3 . 5 ( + 1 . 4 )}$ |
| LLaMA-2-13B | CoT-5.8M | 64.2 |
|  | LeMA-5.8M | $\mathbf{6 5 . 7 ( + 1 . 5 )}$ |

Table 2: Performances with the same size of training tokens (5.8M) on GSM8K.

| Model | $\operatorname{Acc}(\%)$ |
| :---: | :---: |
| WizardMath-70B (Luo et al., 2023) <br> WizardMath-70B + LEMA | 81.6 <br> $84.2(+2.6)$ |
| MetaMath-70B (Yu et al., 2023) <br> MetaMath-70B + LEMA | 82.3 <br> $85.4(+3.1)$ |

Table 3: Performances of LEMA with specialized LLMs on GSM8K.

larger than (or equal to) 34B and 0.0002 for LLMs smaller than 34B. We set batch size as 96, train for 2,000 steps, and save checkpoints for every 100 training steps.

For evaluation, we evaluate the performance of all saved checkpoints based on vLLM library ${ }^{6}$ (Kwon et al. 2023) and report the accuracy of the best checkpoint. During inference, we set temperature as 0 (i.e., greedy decoding) and max sample length as 2,048 . To clarify the influence from random disturbances during training, we provide the performances of the best three checkpoints in Appendix D.1 and the performance curves during the whole training processes in Appendix D. 2 We do not add demonstration examples into the prompt for both fine-tuning and evaluation by default. All evaluations are conducted under the same CoT instruction. For models trained with LEMA, we do not generate corrections during evaluations. All our experiments can be conducted on $4 \times$ A100 GPU stations.

## 4 Results and Analysis

We focus on two main research questions in this section. More results and analysis are contained in Appendix D.

### 4.1 Can LLMs Learn From Mistakes?

LEMA effectively improves CoT-alone fine-tuning. Table 1 shows the main experimental results on five challenging reasoning tasks. Compared to fine-tuning on CoT data alone, incorporating correction data during fine-tuning brings improvements across all five backbone LLMs and five tasks. It demonstrates that LEMA can effectively facilicate CoT fine-tuning. Note that SVAMP and ASDiv can be regarded as two out-of-distribution tasks as the training data is constructed based on GSM8K. The gains on these two tasks reflect that LEMA has a certain extent of generalizability in the out-of-distribution scenarios.

The effectiveness of CoT data and correction data are non-homogeneous. If the effectiveness of the two data sources are homogeneous, the gains in Table 1will be diminished if the data sizes of two fine-tuning settings are controlled as the same. To further validate the effectiveness of correction data, we conduct two ablation studies with controlled data sizes. In default settings, we have about $32 \mathrm{~K}$ examples for CoT-alone fine-tuning and $45 \mathrm{~K}$ examples for LEMA. Here are another two controlled settings:

- LEMA-32K. We keep the $13 \mathrm{~K}$ correction data and randomly remove $13 \mathrm{~K}$ CoT data.
- CoT-45K. To expand CoT data, we extract the corrected CoT from each correction example.

Figure 4 shows that LEMA can still bring gains for four out of five backbone LLMs under the same data size. It means that these LLMs do learn extra information from our correction data that is not provided by the CoT data. The only exception is for LLaMA-2-7B. It indicates that a stronger backbone model can more effectively learn from mistakes.

Despite controlling the training data sizes to be the same, we also investigate the trainingtoken efficiency of LEMA compared with CoT-alone fine-tuning. Notice that the target-side length of correction data is generally longer than CoT data, so LEMA will have slightly more training tokens than CoT-alone fine-tuning under the same data size. Specifically,[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-08.jpg?height=309&width=629&top_left_y=279&top_left_x=412)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_f6fcf81b642dc26f574dg-08.jpg?height=301&width=659&top_left_y=283&top_left_x=1080)

(b)

Figure 5: Performance of LEMA on MATH with evolution strategies. (a) Compare general and correction-centric evolution strategies (full fine-tuning). (b) The performance trend of LEMA with QLoRA or full fine-tuning. X-axis is the number of sampled questions.

CoT-45K has 5.4M training tokens and LEMA-45K has 5.8M ( $\mathrm{a} \sim 7 \%$ relative increment). To conduct the comparison under the same size of training tokens, we construct CoT-5.8M by sampling more reasoning paths (following Section 2.3) to add into CoT-45K.

Table 2shows that LEMA still outperforms CoT-alone fine-tuning with the same number of training tokens. Note that this comparison is under an unfavorable setup for LEMA as it increases the training samples for CoT-alone fine-tuning. The improvements in Table 2 further support the non-homogeneous effectiveness of CoT data and correction data. Moreover, we notice that augmenting more reasoning paths for LLaMA-2-70B does not continuously boost the model performance on GSM8K. To validate this, we further expand CoT-5.8M to CoT-6.8M and have a $82.2 \%$ accuracy. Such an observation is in line with the Yu et al. (2023). We suppose that this is because sampling too many reasoning paths for the same question will only bring redundant information to the training.

A stronger backbone model can be more effective at learning from mistakes. As evidenced in Table 1. LLaMA-2-70B has the highest baseline performances in CoT alone fine-tuning, while maintaining significant improvements in all five tasks (an accuracy gain of over $1 \%$ ) with the help of LEMA. In contrast, for other four less powerful models in Table 1 . the improvements from LEMA are occasionally less significant. This comparison, along with the performance of LLaMA-2-7B in Figure 4, suggests that the inherent strength of backbone LLMs can influence how well the models can learn from mistakes.

LEMA can also facilitate specialized LLMs. To adapt generally pre-trained LLMs into the math domain, there have been several specialized LLMs such as WizardMath (Luo et al. 2023) and MetaMath (Yu et al. 2023). We also apply LEMA on these specialized LLMs to further examine its effectiveness. As these models have been already trained on a large amount of CoT data designed for math tasks, we directly compare LEMA with the results reported in the original papers for these specialized models. Table 3 shows that LEMA can further improve these specialized LLMs. Appendix D.3 contains detailed comparisons.

### 4.2 How Beneficial Is Correction-Centric Evolution?

Figure $5 \mathrm{a}$ and Figure $5 \mathrm{~b}$ demonstrate further improvements on the performance of LEMA with incorporating the correction-centric evolution strategy to expand the correction data.

Correction-centric evolution can more effectively improve LEMA. Figure 5 shows the performance of LEMA with incorporating different evolution strategies. Besides the correction-centric evolution introduced in Section 2.2, we also compare with the general evolution strategy applied in previous work (Xu et al. 2023; Yu et al. 2023; Li et al. 2023a). For a fair comparison, the number of seed questions is kept the same for both evolution strategies (i.e., 10K). We also tried the Llemma (Azerbayev et al. 2023) model which has
been pre-trained on a math-related corpus (such as arXiv papers). We fully fine-tune LLMs as the correction data scale has been much increased7

There are two primary conclusions. First, LEMA can effectively benefit from evolution techniques. It indicates that the performance of LEMA can be further improved by incorporating existing data augmentation techniques. Second, the correction-centric evolution outperforms the general evolution. It demonstrates that moderately difficult questions are more suitable for expanding the correction data.

Evolution techniques can better facilitate LEMA under full fine-tuning. To explore the scaling trend of LEMA, we apply the correction-centric evolution on another $10 \mathrm{~K}$ sampled seed questions (detailed in Appendix C.5). Figure 5b shows the performance trends of LEMA as the question set expands. It shows that if only the original question-answer pairs in MATH are used (i.e., the initial points in each line), there is no significant difference in the performances of LEMA between QLoRA and full fine-tuning. However, as the question set expands, the performance with full fine-tuning improves significantly, while QLoRA fine-tuning increases only slightly. It indicates that the parameter-efficient fine-tuning can only "digest" a limited scale of correction data. Appendix D.5 provides further analysis.

## 5 Related Work

LLMs with CoT reasoning. Wei et al. (2022) uncovered the emergence of CoT reasoning capability for extremely large language models, and this reasoning capability was then examined in various reasoning-related domains including logical reasoning (Creswell et al. 2022: Pan et al. 2023; Lei et al., 2023), commonsense reasoning (Talmor et al. 2019: Geva et al., 2021: Ahn et al. 2022), and math reasoning (Miao et al. 2020, Koncel-Kedziorski et al. 2016, Patel et al., 2021 Cobbe et al. 2021. Hendrycks et al., 2021). The impressive performance of LLMs in these domains has spurred the research community to further investigate methods for effectively harnessing and enhancing CoT reasoning for LLMs (Wang et al., 2022: Zhou et al., 2022, Creswell \& Shanahan, 2022; Li et al. 2023b; Lightman et al., 2023).

Enhancing CoT reasoning for solving mathematical problems. There has been much work dedicated to enhancing the performance of LLMs in solving mathematical problems from various perspectives. Some studies explored the voting or verification methods based on sampling multiple reasoning paths (Wang et al. 2022; Li et al. 2023b; Lightman et al. 2023). Some methods considered to generate executable programs to obtain the final answer or to integrate plug-in tools that facilitate the execution of external APIs during intermediate steps (Jie \& Lu, 2023; Wang et al., 2023a; Yue et al., 2023; Azerbayev et al., 2023: Gou et al. 2023). Some work collected math-related corpus such as arXiv papers tor pre-training better base models for math (Azerbayev et al., 2023. Wang et al., 2023d). Some work focused on augmenting existing datasets, which expanded training sets or provided external annotations (Magister et al., 2022, Huang et al., 2022; Ho et al., 2022; Li et al., 2022; Luo et al., 2023: Yu et al. 2023; Li et al., 2023a; Liang et al. 2023: Liu et al., 2023a). From the perspective of the techniques used, this work follows the data augmentation approach.

Data augmentation for mathematical tasks. With the help of advanced LLMs (e.g., GPT-4 and GPT-3.5-Turbo), various methods have been proposed to generate more CoT data for mathematical tasks: Yuan et al. (2023) proposed rejection sampling for augmenting CoT data; Xu et al. (2023) evolved the math questions in the training sets; Li et al. (2023a) applied both query augmentation and response augmentation; Yu et al. (2023) used selt-verification and FOBAR to generate CoT with high diversity. While the effectiveness of CoT data has been well studied, how to improve math reasoning with other auxiliary data is still underexplored. To this end, there are some preliminary explorations: Azerbayev et al. (2023) and Yue et al. (2023) found that code data can facilitate math reasoning; Liu et al. (2023b) and Wang et al. (2023e) constructed re-ranking data or verification data to make the model judge the quality of reasoning paths. This work takes a further step toward leveraging auxiliary[^5]data: we propose and examine the effectiveness of mistake-correction data, which informs the model what kind of mistakes could be made in CoT reasoning and how to correct them.

## 6 Conclusion

This work explores whether the reasoning capabilities of LLMs can be further improved by learning from mistakes. Experimental results and in-depth analysis demonstrate the effectiveness and potential of learning from mistakes.

## Ethics Statement

Due to the utilization of pre-trained language models, this work could be exposed to some potential risks of ethical issues on general deep learning models (such as social bias and privacy breaches). We hope that the idea of learning from mistakes would facilitate the development of responsible AI models, for instance, on training LLMs to recognize and modify risky generated contents.

## Reproducibility Statement

We open source our training code, evaluation scripts and fine-tuned checkpoints to facilitate further explorations on learning from mistakes. For generating the training data, we provide all our prompts used for data generation.

## Acknowledgments

Shengnan An and Nanning Zheng were supported in part by NSFC under grant No. 62088102. Thank Chen Li at IAIR, Xi'an Jiaotong University for his valuable comments on this work.

## References

Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022.

Alibaba. Alibaba open sources qwen, a $7 \mathrm{~b}$ parameter ai model, 2023. URL https://www.maginative.com/article/alibaba-open-sources-qwen-a-7bparameter-ai-model/

Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models, 2022.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha

Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.

Anthropic. Model card and evaluations for claude models, 2023. URL https://www-files anthropic.com/production/images/Model-Card-Claude-2.pdf.

Kourosh Hakhamaneshi Artur Niederfahrenhorst and Rehaan Ahmad. Finetuning llms: Lora or full-parameter? an in-depth analysis with llama 2, 2023. URL https://www.anyscale.com/blog/fine-tuning-llms-lora-or-fullparameter-an-in-depth-analysis-\with-llama-2.

Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022.

Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, 2022.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 04 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_00370. URLhttps://doi.org/10.1162/tacl_ a_00370

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2023.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.

Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071, 2022.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9

Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.

Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet, 2023.

Zhanming Jie and Wei Lu. Leveraging training data in few-shot prompting for numerical reasoning. arXiv preprint arXiv:2305.18170, 2023.

Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology. org/N16-1136

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, $202 \overline{3}$.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.

Bin Lei, Chunhua Liao, Caiwen Ding, et al. Boosting logical reasoning in large language models through a new framework: The graph of thought. arXiv preprint arXiv:2308.08614, 2023.

Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.

Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv preprint arXiv:2310.05506, 2023a.

Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726, 2022.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, 2023b.

Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, and Ashwin Kaylan. Let gpt be a math tutor: Teaching math word problem solvers with customized exercise generation. arXiv preprint arXiv:2305.14386, 2023.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023.

Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving ¿80

Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, and Peter J. Liu. Improving large language model fine-tuning for solving math problems, 2023b.

Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. In Advances in Neural Information Processing Systems, 2022.

Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.

Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022.

Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.92. URL https://aclanthology.org/2020.acl-main. 92 .

OpenAI. Gpt-3.5 turbo fine-tuning and api updates, 2023a. URL https://openai.com/ blog/gpt-3-5-turbo-fine-tuning-and-\api-updates

OpenAI. Gpt-4 technical report, 2023b.

OpenAI. Openai documentation: Models, 2023c. URL https://platform.openai.com/ docs/models/gpt-3-5.

Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295, 2023.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology .org/2021 naacl-main. 168

Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.

Leonardo Ranaldi and Andre Freitas. Aligning large and small language models via chain-ofthought reasoning. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1812-1827, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long. 109.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2023.

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=fR3wGCk-IXp.

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.

Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems, 2023.

Yusheng Su, Chi-Min Chan, Jiali Cheng, Yujia Qin, Yankai Lin, Shengding Hu, Zonghan Yang, Ning Ding, Xingzhi Sun, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Exploring the impact of model scaling on parameter-efficient tuning, 2023.

Xianghui Sun, Yunjie Ji, Baochang Ma, and Xiangang Li. A comparative study between full-parameter and lora-based fine-tuning on chinese instruction data for instruction following large language model, 2023.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149-4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans?, 2023.

Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning, 2023a.

Ruida Wang, Wangchunshu Zhou, and Mrinmaya Sachan. Let's synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models, 2023b.

Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023c.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.

Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for math: Part i - mathpile: A billion-token-scale pretraining corpus for math, 2023d.

Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. Democratizing reasoning ability: Tailored learning from large language model, 2023e.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.

Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.

Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022.

This is the Appendix of the paper: Learning From Mistakes Makes LLM Better Reasoner.
