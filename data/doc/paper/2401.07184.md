# Scaling Advantage in Approximate Optimization with Quantum Annealing 

Humberto Munoz Bauza ${ }^{1,2}$ and Daniel Lidar ${ }^{1,2,3,4}$<br>${ }^{1}$ Department of Physics and Astronomy, University of Southern California, Los Angeles, California 90089, USA<br>${ }^{2}$ Center for Quantum Information Science 8 Technology,<br>University of Southern California, Los Angeles, California 90089, USA<br>${ }^{3}$ Department of Electrical Engineering, University of Southern California, Los Angeles, California 90089, USA<br>${ }^{4}$ Department of Chemistry, University of Southern California, Los Angeles, California 90089, USA


#### Abstract

Quantum annealing is a heuristic optimization algorithm that exploits quantum evolution to approximately find lowest energy states [1,2]. Quantum annealers have scaled up in recent years to tackle increasingly larger and more highly connected discrete optimization and quantum simulation problems [3-7]. Nevertheless, despite numerous attempts, a computational quantum advantage in exact optimization using quantum annealing hardware has so far remained elusive [8-16]. Here, we present evidence for a quantum annealing scaling advantage in approximate optimization. The advantage is relative to the top classical heuristic algorithm: parallel tempering with isoenergetic cluster moves (PT-ICM) [17]. The setting is a family of 2D spin-glass problems with high-precision spin-spin interactions. To achieve this advantage, we implement quantum annealing correction (QAC) [18]: an embedding of a bit-flip error-correcting code with energy penalties that leverages the properties of the D-Wave Advantage quantum annealer to yield over 1, 300 error-suppressed logical qubits on a degree-5 interaction graph. We generate random spin-glass instances on this graph and benchmark their time-to-epsilon, a generalization of the time-to-solution metric [8] for low-energy states. We demonstrate that with QAC, quantum annealing exhibits a scaling advantage over PTICM at sampling low energy states with an optimality gap [19] of at least $1.0 \%$. This amounts to the first demonstration of an algorithmic quantum speedup in approximate optimization.


The pursuit of a quantum speedup using quantum processors is a major theme in modern physics and computer science. Two of the leading application areas are discrete optimization and quantum simulation. In the latter context, impressive recent quantum annealing (QA) advances have been made for fast, coherent anneals lasting on the order of the single superconducting flux qubit coherence time $[20,21]$. While this diabatic approach is considered promIsing [22], it cannot be expected to scale up without the introduction of error correction or suppression, as decoherence and control errors pose insurmountable challenges to Hamiltonian quantum computation models [23-28], just as they do for gate-model quantum computers. In the absence of a fault-tolerance threshold theorem [29] for QA, a variety of Hamiltonian error suppression techniques have been proposed and analyzed as ways to reduce the error rates of this computational model and the closely related model of adiabatic quantum computation [30-36], providing tools towards scalability.

However, despite these theoretical advances, there are currently practical limitations to the types and locality of programmable interactions in the Hamiltonians of quantum annealing hardware, even as new devices have started to emerge [16, 37-39]. To address these limitations, quantum annealing correction (QAC) [18] was developed as a realizable error suppression method for quantum annealing, targeting the available and restricted set of control operations in quantum annealers. QAC has been demonstrated to enhance the success probability of quantum annealing and mitigate the analog control problem $[18,40,41]$. The QAC method is based on a repetition-code encoding of the Hamiltonian and does not fully realize a Hamiltonian stabilizer code. Despite this, it has been shown theoretically to increase the energy gap of the encoded Hamiltonian and reduce tunneling barriers, thus softening the onset of the associated critical dynamics as well as lowering the effective temperature [42].

Here, departing from the traditional paradigm of using QA for exact optimization, we demonstrate-by incorporating QAC-the first genuine scaling advantage in approximate optimization (low-energy sampling) using a quantum annealer. Even approximate optimization can be computationally hard unless $\mathrm{P}=\mathrm{NP}[43,44]$, so we do not expect the advantage we exhibit to amount to more than a polynomial speedup. However, whereas the scaling advantages reported in previous work were relative to simulated annealing $[13,16]$, the advantage we find here is over the best currently available general heuristic classical optimization method: parallel tempering with isoenergetic cluster moves (PT-ICM) [17]. This result is enabled by implementing QAC on the D-Wave Advantage quantum annealer for the Sidon-set spin glass instance class [9], embedded on the logical graph formed after the encoding step. The advantage of quantum annealing over PT-ICM is diminished without QAC, thus highlighting the crucial role of quantum error suppression.

## Quantum annealing

The D-Wave quantum processing unit (QPU) uses superconducting flux qubits to implement the transverse field
(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-02.jpg?height=517&width=545&top_left_y=262&top_left_x=259)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-02.jpg?height=529&width=531&top_left_y=248&top_left_x=865)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-02.jpg?height=501&width=507&top_left_y=270&top_left_x=1427)

FIG. 1. Embedding of QAC on the Pegasus hardware graph. (a) Close-up of the embedding of three clusters of two coupled logical qubits each. Physical qubits are denoted by circles and couplings by lines. Penalty qubits are marked by a $\mathrm{P}$ and the three penalty couplings (thin lines) to their corresponding physical data qubits. Thick lines indicate the physical couplings between the corresponding physical qubits of different logical qubits. Only a subset of all possible couplings are shown. (b) A zoomed-out view from (a) showing the logical qubits (circles) and the logical couplings induced by the QAC embedding. Thick lines indicate the logical couplings shown in (a), whose type is colored by direction: horizontal/vertical/diagonal (long or short). (c) A zoomed-out view from (b) showing a $4 \times 4$ induced logical graph of QAC. The logical graph is equivalent to a honeycomb graph with additional non-planar bonds. The induced logical graph of the D-Wave Advantage 4.1 QPU has a maximum of 1322 logical qubits; the largest available side length is $L=15$.

Ising Hamiltonian

$$
\begin{equation*}
H(s)=-A(s) \sum_{i \in \mathcal{V}} \sigma_{i}^{x}+B(s) H_{z} \tag{1}
\end{equation*}
$$

where $\mathcal{V}$ is the vertex set of the hardware graph of the $\mathrm{QPU}, i$ is the qubit index, $\sigma_{i}^{x}$ are Pauli matrices, and $A(s)$ and $B(s)$ are the annealing schedules, respectively decreasing to 0 and increasing from 0 with $s: 0 \rightarrow 1$. $H_{z}$ is the Ising problem Hamiltonian:

$$
\begin{equation*}
H_{z}=\sum_{i \in \mathcal{V}} h_{i} \sigma_{i}^{z}+\sum_{\{i, j\} \in \mathcal{E}} J_{i j} \sigma_{i}^{z} \sigma_{j}^{z} \tag{2}
\end{equation*}
$$

where $h_{i}$ and $J_{i j}$ are programmable local fields and couplings, respectively, and $\mathcal{E}$ is the edge set of the hardware graph. Many NP-complete and NP-hard problems can be mapped to $H_{z}$ [45] by minor-embedding onto the hardware graph. We performed QA experiments on the D-Wave Advantage 4.1 QPU accessed through the D-Wave Leap cloud interface, featuring the Pegasus hardware graph [46].

## Quantum annealing correction

We implement the $[[3,1,3]]_{1}$ QAC encoding introduced in Ref. [18], which encodes a logical qubit into three physical "data qubits," each of which is coupled to the same additional "energy penalty qubit" with a fixed coupling strength $J_{p}$; the logical qubit is decoded via a majority vote on the data qubits. The logical subgraph induced by the QAC encoding on the Pegasus graph has a bulk degree of 5 and admits native loops of length 5. Fig. 1 illustrates the encoding and the induced logical graph. All previous QAC results were obtained using the "Chimera" hardware graph of the previous generation of D-Wave QPUs, which has degree 3 and no odd-length loops. The features of the induced Pegasus logical graph permit the benchmarking of significantly harder spin-glass instances than was possible on Chimera. The induced logical graphs we examine have side length $L \in[5,15]$, corresponding to a problem size range of $N \in[142,1322]$ logical, error-corrected qubits.

Problems on the logical QAC graph can also be encoded directly by setting $J_{p}=0$, resulting in three uncoupled and unprotected, parallel classical copies of the problem instance. We then extract the energies of all three copies as independent annealing samples and denote this "unprotected" QA method by U3. We use the U3 method below to test whether QAC has a genuine advantage over simple classical repetition coding.

## Spin-glass instances

We generate random native spin-glass instances on the induced logical graph. These types of instances have been widely used to benchmark the previous D-Wave QPUs (with the Chimera hardware graph) against classical algorithms $[4,8,9]$. We tested three types of spin-glass disorder: binomial, where $J_{i j}$ was randomly selected as $\pm 1$ with equal probability, Sidon- 28 (S28) [9], where $J_{i j}$ was randomly sampled from the set $\pm\{8 / 28,13 / 28,19 / 28,1\}$, and finally range 6 (R6)
disorder, where $J_{i j} \in \pm\{1 / 6, \ldots, 6 / 6\}$. In a Sidon set, the sum of any two set members gives a number that is not part of the set. Moreover, no five numbers from the S28 set sum to zero, which prevents the occurrence of "floppy" qubits [47] given the bulk degree- 5 of the Pegasus graph. Binomial disorder generally admits a degenerate ground state, simplifying the optimization problem. In contrast, the S28 disorder can yield instances with a unique ground state [26]. The ground states are robust to small errors in the implementation of the $J_{i j}$ values in the binomial disorder case, but this is not the case when high precision in implementing the $J_{i j}$ values is required (as for Sidon disorder). The latter case is expected to benefit more from QAC than the former [41]. From here on, we focus on the S28 case; see Methods.

## Time-to-epsilon metric

The standard performance metric for exact optimization using heuristic solvers is the time-to-solution (TTS), which quantifies the time to reach the ground state at least once with $99 \%$ probability, given that the success probability per run is $p$ : TTS $=t_{f} R$, where each annealing run lasts time $t_{f}$ and $R=\frac{\log (1-0.99)}{\log (1-p)}$ is the expected number of runs [8]. To address approximate optimization, we instead consider the time required to reach an energy within a fraction $\varepsilon$ of the ground state energy $E_{0}$, and define the time-to-epsilon for an instance as

$$
\begin{equation*}
\mathrm{TT} \varepsilon=t_{f} \frac{\log (1-0.99)}{\log \left(1-p_{E \leq E_{0}+\varepsilon\left|E_{0}\right|}\right)} \tag{3}
\end{equation*}
$$

where $p_{E \leq E_{0}+\varepsilon\left|E_{0}\right|}$ is the probability that the energy $E$ of a sample is no more than $\varepsilon\left|E_{0}\right|$ above $E_{0}$. The TTS is the special case $\varepsilon=0$. In a mixed integer programming optimization context, $\varepsilon$ is known as the optimality gap [19], which is how we refer to it here. The ground state energies are known for our instances (see Methods ), so $\varepsilon$ is exactly calculated for each sample rather than bounded. An alternative time metric is the residual energy density from the ground state [21]; we focus on the optimality gap due to its relevance to benchmarking approximate optimization algorithms.

We define $[\mathrm{TT} \varepsilon]_{q}$ of an instance class as the $q$-th quantile of $\mathrm{TT} \varepsilon$ over the entire instance class. Here, we focus only on the median quantile, $q=0.5$, denoted $[\mathrm{TT} \varepsilon]_{\mathrm{Med}}$. For a given disorder, instance size, and $\varepsilon$-target, we find the annealing time $t_{f}$ (and penalty strength for QAC) that minimizes $[\mathrm{TT} \varepsilon]_{\text {Med. }}$. We restrict the penalty coupling strengths to the set $J_{p} \in\{0.1,0.2,0.3\}$ to reduce resource requirements for parameter optimization, as $J_{p}=0.2$ is the penalty strength that most frequently optimizes the success probabilities of individual instances, and the dependence on $J_{p}$ above 0.2 becomes weak.

## Fitting the TT $\varepsilon$

Below, we fit the $\mathrm{TT} \varepsilon$ to a power law: $\mathrm{TT} \varepsilon(N)=c N^{\alpha}$, where $\alpha$ is the scaling exponent, the quantity we use to quantify the scaling of the different algorithms we compare. The choice of a power law is motivated by the existence of an $O(N)$ classical algorithm for the residual energy density; we describe such an algorithm in Methods (though this algorithm is utterly impractical due to its huge prefactor). Due to the power law fit, we should account for factors that can modify the scaling exponent. Indeed, we could use all $N_{\max }$ qubits of the QPU and embed $N_{\max } / N$ parallel copies of each problem of size $N$, then select the best of these copies. Since, in reality, we work with only one copy due to a small fraction of the qubits and couplers being absent, we multiply the $\mathrm{TT} \varepsilon$ by a factor of $N / N_{\max }$ [8]. The U3 TT $\varepsilon$ is similarly multiplied by a constant factor of $3 / 4$ since, due to a lack of needed couplings, each instance is repeated only over the data qubits, thus leaving $1 / 4$ of the available (penalty) qubits unused.

## Parallel tempering algorithm

Our baseline classical algorithm is parallel tempering with isoenergetic cluster moves (PT-ICM) [17]. The runtime of this algorithm has the best scaling with problem size known in the task of finding the ground state of various benchmark problems on D-Wave QPUs [12, 14], with the only known exception being certain XORSAT instances for which highly specialized solvers have been developed [15, 48]. Our optimization of the algorithmic parameters of PT-ICM is described in Methods.

## Results

It is well known that the TTS metric generates unreliable results unless the annealing time is optimized for each size $N[8,13]$. This is because an artificially high TTS at small $N$ results in an overly flat TTS scaling. The same considerations apply to the $\mathrm{TT} \varepsilon$, so here we find the annealing time $t_{f}$ that minimizes $[\mathrm{TT} \varepsilon]_{\text {Med }}$ for each $N$-denoted $t_{f}^{\text {opt }}(N)$-and report the resulting median $\mathrm{TT} \varepsilon$ and its scaling estimate for QAC and U3 in Fig. 2, along with the analogously optimized PT-ICM results.

The shortest available annealing time on the D-Wave Advantage QPU accessed via Leap is $t_{f}^{\min }=0.5 \mu \mathrm{s}$, and the bottom panels of Fig. 2 show that as the target residual energy density is increased, progressively larger problem sizes are needed to ensure that $t_{f}^{\mathrm{opt}}(N)>t_{f}^{\min }$. We cannot rule out that with access to lower annealing times, one would find $t_{f}^{\mathrm{opt}}(N)<t_{f}^{\min }$ for all $N$ values where we empirically find $t_{f}^{\text {opt }}(N)=t_{f}^{\mathrm{min}}$. We thus formulate a null hypothesis for each $N$ that $t_{f}^{\mathrm{opt}}(N) \leq t_{f}^{\min }$ and compute a $P$ value as the empirical number of bootstrap samples whose $t_{f}^{\mathrm{opt}}(N)=t_{f}^{\mathrm{min}}$, out of a total of 200 samples (see Methods for details of our statistical analysis). To compute the $\mathrm{TT} \varepsilon$ scaling, i.e., the slope $\alpha$ in a fit to $\mathrm{TT} \varepsilon=c N^{\alpha}$, for each $\varepsilon$ we use only those $t_{f}^{\mathrm{opt}}(N)$ values whose $P<0.05$ (filled circles in Fig. 2). We can thus be
![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-04.jpg?height=592&width=1768&top_left_y=172&top_left_x=186)

FIG. 2. Time-to-epsilon scaling for QAC, U3, and PT-ICM for Sidon-28 (S28) spin-glass disorder. The bottom panels show the optimal annealing times of U3 and QAC. The top panels show the TT $\varepsilon$ results for the corresponding optimal annealing times, along with optimized PT-ICM results. The straight lines are best fits assuming power law scaling TT $\varepsilon=c N^{\alpha}$ and the accompanying numbers are the corresponding slopes $\alpha$. As indicated in the legends, the target $\varepsilon$ increases from left to right, with a corresponding improvement in quantum annealing's performance: PT-ICM is better at low $\varepsilon$, but for $\varepsilon=1 \%$, while U3 is still worse than PT-ICM, QAC already outperforms PT-ICM. At $\varepsilon=1.25 \%$, both QAC and U3 have better scaling than PT-ICM. For higher residual energy targets, the scaling of QAC becomes unreliable since we can no longer guarantee that the optimal $t_{f}$ has been identified. We used $t_{f} \in[0.5,27] \mu \mathrm{s}$ and $N \in[142,1322]$. Error bars for $\mathrm{TT} \varepsilon$ data points are twice the standard error of the parameter estimate calculated using bootstrapping. Filled (open) circles correspond to a $P$ value of 0.05 (0.20) that $t_{f}^{\text {opt }}(N)>t_{f}^{\min }$ for the corresponding problem size $N$ (see Methods for details). We use only those $t_{f}$ values for which $P \leq 0.05$ to compute the slopes.

confident that the reported slopes reflect the true scaling of U3 and QAC.

Our first observation is that the $Q A C$ scaling is always better than the U3 scaling, which is consistent with previous studies concerning the effect of analog coupling errors (" $J$-chaos") on the TTS for spin-glass instances [41]. Such errors are expected for the S28 instances due to the relatively high precision their specification requires.

Second, we observe that U3 and QAC reduce the absolute algorithmic runtime by four orders of magnitude compared to PT-ICM. However, this is not a scaling advantage, and since our PT-ICM calculations could be sped up by employing faster classical processors, we do not consider this a robust finding. Similarly, we exclude the programming and readout time used by the D-Wave QPU. The primary source of overhead is the readout time per sample, which scales with problem size and can reach $200 \mu$ s per sample for this QPU. This timing varies by hardware generation, and its inclusion obfuscates the dominant scaling source.

Third, and most significantly, we observe that as the target optimality gap increases $Q A$ 's scaling overtakes PT-ICM. Notably, at $\varepsilon=1 \%$, QAC exhibits a scaling exponent of $1.69 \pm 0.12$, compared to PT-ICM's $1.93 \pm 0.03$, and at $\varepsilon=1.25 \%, \mathrm{QAC}$ and $\mathrm{U} 3$ exhibit scaling exponents of $1.15 \pm 0.22$ and $1.76 \pm 0.06$, respectively, compared to PT-ICM's $1.87 \pm 0.02$. The scaling of U3 at optimal annealing times continues to decrease as $\varepsilon$ increases; at $\varepsilon=1.5 \%$ (not shown), the scaling exponents for U3 and
PT-ICM become $1.60 \pm 0.07$ and $1.86 \pm 0.04$, respectively. This is robust evidence of a quantum annealing scaling advantage over the best available classical heuristic optimization algorithm.

We are unable to determine the scaling of QAC for $\varepsilon>$ $1.25 \%$, as we cannot confirm that $t_{f}^{\mathrm{opt}}(N)>t_{f}^{\min }$ for any $N$; as can be seen in Fig. 2, already for $\varepsilon=1.25 \%$ only the largest three $N$ values satisfy the $P<0.05$ criterion. However, given the consistently better scaling of QAC for lower values of $\varepsilon$, where $t_{f}^{\mathrm{opt}}(N)>t_{f}^{\min }$ for QAC over a range of problem sizes, it is reasonable to conclude that the QAC scaling would be a further improvement over $\mathrm{U} 3$ if its true $t_{f}^{\mathrm{opt}}(N)$ could be established for $\varepsilon>1.25 \%$; this would require access to shorter annealing times or larger system sizes.

We note that it is unsurprIsing that, given a sufficiently large target optimality gap, the D-Wave QPU returns sample energies within that gap. Similarly, we can expect that for large enough $\varepsilon$, even simulated annealing or greedy descent will be nearly guaranteed success in polynomial time. The significance of our result is that $Q A$ reaches near-linear scaling at a smaller optimality gap target than PT-ICM. Thus, we refer to this result as an approximate optimization advantage for quantum annealing. We also note that Ref. [21] similarly reported a QA optimization advantage for the residual energy density (for 3D spin glasses), but this was done at a fixed problem size (of $N=5374$ physical qubits), and was instead concerned with the convergence of the
residual energy density with the annealing time. We reemphasize that, in contrast, we are reporting a scaling advantage as a function of problem size, the proper context for quantum speedup claims. We explain in Methods that our results stand regardless of whether an optimality gap or residual energy density target is chosen for benchmarking.

## Dynamical critical scaling

As an additional perspective on the different quantum annealing dynamics resulting from error suppression, we study the difference in the dynamical critical scaling under the Kibble-Zurek (KZ) scaling ansatz [49, 50], where an annealing time of $t_{f} \sim L^{\mu}$ is required to suppress diabatic excitations with a correlation length of $L$, and where $\mu$ is the dynamical critical exponent or $\mathrm{KZ}$ exponent. We examined signatures of dynamical critical scaling by calculating the Binder cumulant $U=\frac{1}{2}\left(3-\left\langle q^{4}\right\rangle /\left\langle q^{2}\right\rangle^{2}\right)$, where $\langle\cdot\rangle$ denotes the sample average, either from U3 or after decoding with QAC, and $U$ is averaged over all instances for each system size $N$ and annealing time $t_{f}$. Here $q=\frac{1}{N} \sum_{i=1}^{N} \sigma_{i}^{z} \sigma_{i}^{z \prime}$ is the overlap between two replicas, i.e., independently annealed $N$-spin states $\left\{\sigma_{i}^{z}\right\}$ and $\left\{\sigma_{i}^{z \prime}\right\}$ of a given disorder realization (set of couplings $J_{i j}$ ).

We performed a finite-size scaling analysis and data collapse, and the collapsed Binder cumulant for the S28 instances is shown in Fig. 3. We find that $\mu_{\mathrm{QAC}}=4.81 \pm$ 0.22 (at a penalty strength of 0.1 ) compared to $\mu_{\mathrm{U} 3}=$ $7.53 \pm 0.47$. The reduction of $\mu$ is lost when $\lambda=0.2$ (not shown), suggesting that $\lambda=0.1$ is optimal in the sense of diabatic error suppression.

This effect indicates that QAC is much more effective at suppressing diabatic excitations. I.e., at equal annealing times, the dynamics are more adiabatic under QAC, in agreement with theory [42]. The significant reduction in $\mu$ suggests that in addition to $J$-chaos suppression, diabatic error suppression by QAC is responsible for the improved $\mathrm{TT} \varepsilon$ and shorter optimal annealing times. Additional context is provided in Methods.

## Discussion

Using the largest available quantum annealer to date we have demonstrated an approximate optimization timescaling advantage for quantum annealing on a family of spin-glass problems with low ground-state degeneracy and high-precision couplings. Our demonstration involves up to 1322 logical qubits, the largest number to date in an error-corrected setting. Our key result is the demonstration of an algorithmic quantum scaling advantage. The advantage is relative to PT-ICM, the best classical heuristic algorithm currently known for such spin glass problems, and appears at optimality gaps $\gtrsim 1 \%$.

There are a few limitations to the scope of our conclusion. First, our results do not imply that finding states

![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-05.jpg?height=794&width=805&top_left_y=178&top_left_x=1126)

FIG. 3. Collapsed Binder cumulant for samples collected for U3 and QAC with a penalty strength of $\lambda=0.1$. Here $t_{f} \in[0.5,5.2] \mu \mathrm{s}$ and $L \in[5,15]$.

within small, constant gaps (and indeed finding the ground state itself) is easy for quantum annealing, nor do they imply that all spin glass problems are amenable to an approximate optimization scaling advantage via quantum annealing. Second, being finite-range and two-dimensional limits the range of applications of the problem family we have studied here. To achieve an algorithmic quantum advantage in an application setting, the next challenge for quantum optimization is demonstrating a hardware-scalable advantage in densely connected problems at sufficiently small optimality gaps.

## Acknowledgements

We thank Dr. Evgeny Mozgunov for suggesting the use of the time-to-epsilon metric and Dr. Victor Kasatkin for discussions about the theoretical lower bound of finiterange approximate optimization. We also thank Dr. Mohammad Amin, Dr. Carleton Coffrin, Dr. Itay Hen, and Dr. Tameem Albash for various helpful discussions and suggestions. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112190071. This research was also supported by the ARO MURI grant W911NF-22-S-0007. The authors acknowledge the Center for Advanced Research Computing (CARC) at the University of Southern California for computing resources.

[1] T. Kadowaki and H. Nishimori, Quantum annealing in the transverse Ising model, Physical Review E 58, 5355 (1998).

[2] P. Hauke, H. G. Katzgraber, W. Lechner, H. Nishimori, and W. D. Oliver, Perspectives of quantum annealing: Methods and implementations, Reports on Progress in Physics (2020).

[3] M. W. Johnson, M. H. S. Amin, S. Gildert, T. Lanting, F. Hamze, N. Dickson, R. Harris, A. J. Berkley, J. Johansson, P. Bunyk, E. M. Chapple, C. Enderud, J. P. Hilton, K. Karimi, E. Ladizinsky, N. Ladizinsky, T. Oh, I. Perminov, C. Rich, M. C. Thom, E. Tolkacheva, C. J. S. Truncik, S. Uchaikin, J. Wang, B. Wilson, and G. Rose, Quantum annealing with manufactured spins, Nature 473, 194 (2011).

[4] S. Boixo, T. F. Rønnow, S. V. Isakov, Z. Wang, D. Wecker, D. A. Lidar, J. M. Martinis, and M. Troyer, Evidence for quantum annealing with more than one hundred qubits, Nature Physics 10, 218 (2014).

[5] A. D. King, J. Carrasquilla, J. Raymond, I. Ozfidan, E. Andriyash, A. Berkley, M. Reis, T. Lanting, R. Harris, F. Altomare, K. Boothby, P. I. Bunyk, C. Enderud, A. Fréchette, E. Hoskinson, N. Ladizinsky, T. Oh, G. Poulin-Lamarre, C. Rich, Y. Sato, A. Y. Smirnov, L. J. Swenson, M. H. Volkmann, J. Whittaker, J. Yao, E. Ladizinsky, M. W. Johnson, J. Hilton, and M. H. Amin, Observation of topological phenomena in a programmable lattice of 1,800 qubits, Nature 560, 456 (2018).

[6] R. Harris, Y. Sato, A. J. Berkley, M. Reis, F. Altomare, M. H. Amin, K. Boothby, P. Bunyk, C. Deng, C. Enderud, S. Huang, E. Hoskinson, M. W. Johnson, E. Ladizinsky, N. Ladizinsky, T. Lanting, R. Li, T. Medina, R. Molavi, R. Neufeld, T. Oh, I. Pavlov, I. Perminov, G. PoulinLamarre, C. Rich, A. Smirnov, L. Swenson, N. Tsai, M. Volkmann, J. Whittaker, and J. Yao, Phase transitions in a programmable quantum spin glass simulator, Science 361,162 (2018)

[7] P. Weinberg, M. Tylutki, J. M. Rönkkö, J. Westerholm, J. A. Åström, P. Manninen, P. Törmä, and A. W. Sandvik, Scaling and diabatic effects in quantum annealing with a d-wave device, Physical Review Letters 124, 090502 (2020).

[8] T. F. Ronnow, Z. Wang, J. Job, S. Boixo, S. V. Isakov, D. Wecker, J. M. Martinis, D. A. Lidar, and M. Troyer, Defining and detecting quantum speedup, Science 345, 420 (2014).

[9] H. G. Katzgraber, F. Hamze, Z. Zhu, A. J. Ochoa, and H. Munoz-Bauza, Seeking Quantum Speedup Through Spin Glasses: The Good, the Bad, and the Ugly, Physical Review X 5, 031026 (2015).

[10] D. Venturelli, S. Mandrà, S. Knysh, B. O'Gorman, R. Biswas, and V. Smelyanskiy, Quantum optimization of fully connected spin glasses, Phys. Rev. X 5, 031040 (2015).

[11] V. S. Denchev, S. Boixo, S. V. Isakov, N. Ding, R. Babbush, V. Smelyanskiy, J. Martinis, and H. Neven, What is the computational value of finite-range tunneling?, Phys. Rev. X 6, 031015 (2016).

[12] S. Mandrà, H. G. Katzgraber, and C. Thomas, The pitfalls of planar spin-glass benchmarks: RaIsing the bar for quantum annealers (again), Quantum Science and Tech- nology 2, 038501 (2017).

[13] T. Albash and D. A. Lidar, Demonstration of a Scaling Advantage for a Quantum Annealer over Simulated Annealing, Physical Review X 8, 031016 (2018).

[14] S. Mandrà and H. G. Katzgraber, A deceptive step towards quantum speedup detection, Quantum Science and Technology 3, 04LT01 (2018).

[15] M. Kowalsky, T. Albash, I. Hen, and D. A. Lidar, 3regular three-xorsat planted solutions benchmark of classical and quantum heuristic optimizers, Quantum Science and Technology 7, 025008 (2022).

[16] S. Ebadi, A. Keesling, M. Cain, T. T. Wang, H. Levine, D. Bluvstein, G. Semeghini, A. Omran, J. Liu, R. Samajdar, X.-Z. Luo, B. Nash, X. Gao, B. Barak, E. Farhi, S. Sachdev, N. Gemelke, L. Zhou, S. Choi, H. Pichler, S. Wang, M. Greiner, V. Vuletic, and M. D. Lukin, Quantum Optimization of Maximum Independent Set using Rydberg Atom Arrays, Science 376, 1209 (2022).

[17] Z. Zhu, A. J. Ochoa, and H. G. Katzgraber, Efficient Cluster Algorithm for Spin Glasses in Any Space Dimension, Physical Review Letters 115, 077201 (2015).

[18] K. L. Pudenz, T. Albash, and D. A. Lidar, Errorcorrected quantum annealing with hundreds of qubits, $\mathrm{Na}-$ ture Communications 5, 3243 (2014).

[19] Gurobi Optimization: MIPGap.

[20] A. D. King, S. Suzuki, J. Raymond, A. Zucca, T. Lanting, F. Altomare, A. J. Berkley, S. Ejtemaee, E. Hoskinson, S. Huang, E. Ladizinsky, A. J. R. MacDonald, G. Marsden, T. Oh, G. Poulin-Lamarre, M. Reis, C. Rich, Y. Sato, J. D. Whittaker, J. Yao, R. Harris, D. A. Lidar, H. Nishimori, and M. H. Amin, Coherent quantum annealing in a programmable 2,000 qubit Ising chain, Nature Physics 18,1324 (2022).

[21] A. D. King, J. Raymond, T. Lanting, R. Harris, A. Zucca, F. Altomare, A. J. Berkley, K. Boothby, S. Ejtemaee, C. Enderud, E. Hoskinson, S. Huang, E. Ladizinsky, A. J. R. MacDonald, G. Marsden, R. Molavi, T. Oh, G. Poulin-Lamarre, M. Reis, C. Rich, Y. Sato, N. Tsai, M. Volkmann, J. D. Whittaker, J. Yao, A. W. Sandvik, and M. H. Amin, Quantum critical dynamics in a 5,000qubit programmable spin glass, Nature 617, 61 (2023).

[22] E. J. Crosson and D. A. Lidar, Prospects for quantum enhancement with diabatic quantum annealing, Nature Reviews Physics 3, 466 (2021).

[23] A. M. Childs, E. Farhi, and J. Preskill, Robustness of adiabatic quantum computation, Phys. Rev. A 65, 012322 (2001).

[24] M. H. S. Amin, D. V. Averin, and J. A. Nesteroff, Decoherence in adiabatic quantum computation, Phys. Rev. A 79,022107 (2009).

[25] T. Albash and D. A. Lidar, Decoherence in adiabatic quantum computation, Physical Review A 91, 062320 (2015).

[26] Z. Zhu, A. J. Ochoa, S. Schnabel, F. Hamze, and H. G. Katzgraber, Best-case performance of quantum annealers on native spin-glass benchmarks: How chaos can affect success probabilities, Physical Review A 93, 012317 (2016).

[27] T. Albash, V. Martin-Mayor, and I. Hen, Temperature scaling law for quantum annealing optimizers, Physical Review Letters 119, 110502 (2017).

[28] T. Albash, V. Martin-Mayor, and I. Hen, Analog errors in Ising machines, Quantum Sci. Technol. 4, 02LT03 (2019).

[29] E. T. Campbell, B. M. Terhal, and C. Vuillot, Roads to-
wards fault-tolerant universal quantum computation, Nature 549, 172 EP (2017).

[30] S. P. Jordan, E. Farhi, and P. W. Shor, Error-correcting codes for adiabatic quantum computation, Physical Review A 74, 052322 (2006).

[31] D. A. Lidar, Towards fault tolerant adiabatic quantum computation, Phys. Rev. Lett. 100, 160506 (2008).

[32] K. C. Young, M. Sarovar, and R. Blume-Kohout, Error suppression and error correction in adiabatic quantum computation: Techniques and challenges, Phys. Rev. X 3, 041013 (2013).

[33] A. Ganti, U. Onunkwo, and K. Young, Family of [[6k,2k,2]] codes for practical, scalable adiabatic quantum computation, Phys. Rev. A 89, 042313 (2014).

[34] A. D. Bookatz, E. Farhi, and L. Zhou, Error suppression in Hamiltonian-based quantum computation using energy penalties, Physical Review A 92, 022317 (2015).

[35] Z. Jiang and E. G. Rieffel, Non-commuting two-local hamiltonians for quantum error suppression, Quant. Inf. Proc. 16, 89 (2017).

[36] M. Marvian and D. A. Lidar, Error Suppression for Hamiltonian-Based Quantum Computation Using Subsystem Codes, Physical Review Letters 118, 030504 (2017).

[37] A. W. Glaetzle, R. M. W. van Bijnen, P. Zoller, and W. Lechner, A coherent quantum annealer with Rydberg atoms, Nature Communications 8, 15813 EP (2017).

[38] R. Hamerly, T. Inagaki, P. L. McMahon, D. Venturelli, A. Marandi, T. Onodera, E. Ng, C. Langrock, K. Inaba, T. Honjo, K. Enbutsu, T. Umeki, R. Kasahara, S. Utsunomiya, S. Kako, K.-i. Kawarabayashi, R. L. Byer, M. M. Fejer, H. Mabuchi, D. Englund, E. Rieffel, H. Takesue, and Y. Yamamoto, Experimental investigation of performance differences between coherent Ising machines and a quantum annealer, Science Advances 5, eaau0823 (2019).

[39] D. M. Tennant, X. Dai, A. J. Martinez, R. Trappen, D. Melanson, M. A. Yurtalan, Y. Tang, S. Bedkihal, R. Yang, S. Novikov, J. A. Grover, S. M. Disseler, J. I. Basham, R. Das, D. K. Kim, A. J. Melville, B. M. Niedzielski, S. J. Weber, J. L. Yoder, A. J. Kerman, E. Mozgunov, D. A. Lidar, and A. Lupascu, Demonstration of long-range correlations via susceptibility measurements in a one-dimensional superconducting Josephson spin chain, npj Quantum Information 8, 85 (2022).

[40] W. Vinci, T. Albash, and D. A. Lidar, Nested quantum annealing correction, npj Quantum Information 2, 16017 (2016).

[41] A. Pearson, A. Mishra, I. Hen, and D. A. Lidar, Analog errors in quantum annealing: Doom and hope, npj Quantum Information 5, 1 (2019).

[42] S. Matsuura, H. Nishimori, W. Vinci, T. Albash, and D. A. Lidar, Quantum-annealing correction at finite temperature: Ferromagnetic $p$-spin models, Physical Review A 95, 022308 (2017).

[43] L. Trevisan, Inapproximability of Combinatorial Optimization Problems, arXiv e-prints (2004), arxiv:cs/0409043.

[44] S. Arora and B. Barak, Computational Complexity: A Modern Approach (Cambridge University Press, 2009).

[45] A. Lucas, Ising formulations of many NP problems, Front. Phys. 2, 5 (2014).

[46] K. Boothby, P. Bunyk, J. Raymond, and A. Roy, Nextgeneration topology of d-wave quantum processors (2020), arXiv:2003.00133 [quant-ph].

[47] S. Boixo, T. Albash, F. M. Spedalieri, N. Chancellor, and D. A. Lidar, Experimental signature of programmable quantum annealing, Nat. Commun. 4, 2067 (2013).

[48] M. Bernaschi, M. Bisson, M. Fatica, E. Marinari, V. Martin-Mayor, G. Parisi, and F. Ricci-Tersenghi, How we are leading a 3-XORSAT challenge: From the energy landscape to the algorithm and its efficient implementation on GPUs, Europhysics Letters 133, 60005 (2021).

[49] W. H. Zurek, U. Dorner, and P. Zoller, Dynamics of a quantum phase transition, Physical Review Letters 95, 105701 (2005).

[50] A. del Campo, Universal statistics of topological defects formed in a quantum phase transition, Physical Review Letters 121, 200601 (2018).

## Methods

## Ground state energies

We solved all S28 instances to optimality using Gurobi 10 within feasible runtime, except for 7 instances of size $L=15$. For these remaining instances, Gurobi proved an optimality gap of at most $1.2 \%$, meaning the lowest energy found is guaranteed to be no greater than $1.2 \%$ above the true ground state. Furthermore, the lowest energies found by PT-ICM were no higher than those found by Gurobi for all instances. Thus, we assigned the ground state energies of the remaining instances to the values found by PT-ICM with high confidence. As we use a median over instances as our summary statistic, our conclusions are unaffected by the possibility that the ground state energy was never reached for such few instances.

## PT-ICM parameter optimization

We first ran every replica once for $N_{\mathrm{sw}}^{\max }=500,000$ sweeps to ensure the ground state energy was reached. For the largest size $L=15$, we determined $N_{\mathrm{sw}}^{(90 \%)}$, the number of sweeps that were required for $90 \%$ of the instances to reach their lowest recorded energy, where we found $N_{\mathrm{sw}}^{(90 \%)} \approx 31,000$ for the S28 instances. As this was significantly less than $N_{\mathrm{sw}}^{\max }$, we considered the ground states for these instances as "validated" by PT-ICM to calculate median quantities over the instances. We finally ran PT-ICM 100 times for each instance, setting $N_{\mathrm{sw}}=N_{\mathrm{sw}}^{(90 \%)}$. This yields an empirical cumulative density function for $p_{E \leq E_{0}+\varepsilon}$ as a function of the runtime of PT-ICM. The TT $\varepsilon$ is then evaluated for each instance by optimizing over the runtime of the PT-ICM repetition (where $t_{f}$ is now the time needed to reach the target rather than the annealing time).

The scaling of PT-ICM is ideally evaluated using the parameters that best optimize the $\mathrm{TT} \varepsilon$ for each disorder realization, instance size, and target $\varepsilon$. However, a rigorous optimization of the number and choice of replica temperatures for all target $\varepsilon$ 's and system sizes is computationally infeasible. To ensure our results hold for any choice of reasonably optimized temperatures, we repeated the $\mathrm{TT} \varepsilon$ evaluation with four temperature sets summarized in Table I, which includes both logarithmically-spaced and feedback-optimized [1] temperatures. The TT $\varepsilon$ for a given disorder, instance size, and energy target was chosen from the best $\mathrm{TT} \varepsilon$ out of the four temperature sets, illustrated in Fig. 4. At the optimality gap targets of interest, most temperature sets' differences are not appreciable and are unlikely to affect our conclusions. A more comprehensive but computationally expensive optimization of PT-ICM would involve a grid search over a range of the parameters $\beta_{\min }, \beta_{\max }$, and $N_{\mathrm{icm}}$.

|  |  | $\mathbf{S 2 8}$ |  |  |
| :---: | :---: | :---: | :---: | :---: |
| Set | $N_{T}$ | $\beta_{\min }$ | $\beta_{\max }$ | $N_{\mathrm{icm}}$ |
| 1 | 32 | 0.1 | 5.0 | 8 |
| 2 | 24 | 0.2 | 10.0 | 6 |
| 3 | 32 | 0.2 | 10.0 | 8 |
| 4 | 32 | 0.1 | 20.0 | 8 |

TABLE I. Temperature sets used for PT-ICM for S28 instances, where $\beta_{\min }$ is the hottest temperature, $\beta_{\max }$ is the coldest temperature, and $N_{\mathrm{icm}}$ is the number of low temperature (largest $\beta$ ) subject to ICM moves. The temperatures are logarithmically spaced in sets 1 and 4. The temperatures in sets 2 and 3 were feedback-optimized with initially logarithmically spaced temperatures.
![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-08.jpg?height=704&width=834&top_left_y=764&top_left_x=1100)

FIG. 4. Time-to-epsilon of the different temperature sets used for PT-ICM and their individual scaling for $\varepsilon=0.75 \%$ (left) and $\varepsilon=1.1 \%$ (right). The four sets correspond, in order, to the sets shown in Table I. B5: Logarithmically-spaced with $\beta_{\max }=5$. F24: Feedback-optimized with 24 temperatures. F32: Feedback-optimized with 32 temperatures. B20: Logarithmically-spaced with $\beta_{\max }=20$.

## Software Implementation

Our PT-ICM implementation is available as the TAMC software package [2]. Our implementation of QAC for the D-Wave Advantage graph is available as part of the PegasusTools Python package [3].

We also implemented and considered the performance of simulated annealing but do not present its results here as this algorithm was not competitive at large problem sizes. Our PT-ICM implementation is a general-purpose solver written in the Rust programming language, targeting CPU execution, and accepts instances with any connectivity. This is in contrast to previous studies that have used simulated annealing or the Hamze-de Freitas-Selby algorithm solvers that are specialized for problems defined on the Chimera graph $[4,5]$.

## Statistical methods

For a given problem size and target optimality gap, we calculate the median $\mathrm{TT} \varepsilon$ from the quantum annealing data using a three-step Bayesian bootstrap procedure over the levels of readout samples, gauges (spinreversal transformations at the hardware level), and instances: (1) the success probability for each gauge is resampled from a beta distribution for $N_{\text {samp }}$ samples per gauge, (2) the statistical weight of each gauge is sampled from a Dirichlet distribution of length $N_{\text {gauge }}$ to take the weighted average success probability for each instance, and (3) the statistical weight of each instance is sampled from a Dirichlet distribution of length $N_{\text {inst }}$ to take the weighted median. We performed our quantum annealing experiments with $N_{\text {samp }}=1000(\mathrm{QAC}) / 3000(\mathrm{U} 3)$, $N_{\text {gauge }}=10$, and $N_{\text {inst }}=125$. We performed $N_{\text {boots }}=$ 200 bootstrap samples per size and energy density target pair and found the annealing time and penalty strength that resulted in an optimal $\mathrm{TT} \varepsilon$ for each bootstrap sample. The distribution of the optimal median TT $\varepsilon$ values and the distribution of optimal annealing parameters are the two final products of this sampling procedure shown in Fig. 2, with $2 \sigma$ error intervals for the optimal TT $\varepsilon$.

Next, we formulate a null hypothesis and compute $P$ values as follows. The null hypothesis is that the optimal annealing time $t_{f}$ is the minimum accessible $t_{f}^{\min }=0.5 \mu \mathrm{s}$, i.e., that the true optimal annealing time is not above $t_{f}^{\min }$. The $P$ value is the empirical number of bootstrap samples whose optimal $t_{f}$ was $0.5 \mu \mathrm{s}$, out of 200 samples. Filled circles in Fig. 2 mean $P<0.05$ for the probability that $t_{f}=0.5 \mu \mathrm{s}$ in the bootstrap sample, while open circles mean $P<0.20$. The filled circles show which points have the highest confidence that the optimal annealing time is not below $0.5 \mu \mathrm{s}$.

## Non-universality of the observed $\mathrm{KZ}$ critical exponent

The Binder cumulant $U$ is well-known to provide a statistical signature of phase transitions. Under the dynamic finite-size scaling ansatz, $U\left(L, t_{f}\right)$ is expected to collapse onto a common curve for all system sizes $N$ when $t_{f}$ is rescaled by $L^{-z-1 / \nu}$, where $\nu$ and $z$ are the correlation length and dynamic critical exponents, respectively. This reflects the $\mathrm{KZ}$ ansatz: the annealing time required for the system to remain adiabatic up to a correlation length of $L$ scales as

$$
\begin{equation*}
t_{f}(L) \sim L^{\mu}, \quad \mu=z+\frac{1}{\nu} \tag{4}
\end{equation*}
$$

While the collapse seen in Fig. 3 is visually convincing, the error estimates for $U$ are, unfortunately, too large to determine the $\mathrm{KZ}$ exponent or to extract $\nu$ and $z$. In addition, we do not observe that the extracted KZ exponent is universal: the estimate for $\mu$ that best collapses the Binder cumulant for binomial (as opposed to S28) spin glass instances is $\mu \approx 9 \pm 1$ for $\mathrm{U} 3$, and is reduced to $\mu \approx 7.5 \pm 0.6$ with $\mathrm{QAC}$ at $\lambda=0.1$. Thus, while the scaling ansatz yields a useful complementary way to quantify the advantage of QAC in terms of sampled quantities in addition to the $\mathrm{TT} \varepsilon$ metric, the lack of universality and the possibility that the spinglass transition temperature is zero [6] do not clearly support a universal critical description of the annealing dynamics, quite unlike the conclusions of Refs. [7, 8].

## Time-to-residual energy

Ref. [21] performed energy decay measurements of QA dynamics in 3D spin glass instances as captured through the residual energy, a dimensionless quantity,

$$
\begin{equation*}
\rho=\frac{\left\langle H_{z}\right\rangle-E_{0}}{J N} \tag{5}
\end{equation*}
$$

where $N$ is the number of spins and $J$ is the characteristic energy scale of the Ising Hamiltonian (which is simply 1 for all of our instance classes). This motivates an alternative measure for approximate optimization, which we call time-to-residual energy, or time-to-rho,

$$
\begin{equation*}
\operatorname{TTR}(N)=t_{f} \frac{\log (1-0.99)}{\log \left(1-p_{E \leq E_{0}+\rho J N}\right)} \tag{6}
\end{equation*}
$$

where now $\rho$ sets the target energy difference from the ground state in units of $J N$, rather than $E_{0}$ as in the case of TT $\varepsilon$. Nevertheless, both $\varepsilon$ and $\rho$ are targets that grow in proportion to the problem size or number of variables (in finite dimensions). The TTR analog of Fig. 2 at $\rho=1.1 \%$ is shown in Fig. 5. While the precise scaling exponents $\alpha$ of U3 and QAC vary slightly, the optimal annealing times are nearly identical. The trends of the scaling exponents as functions of either $\rho$ or $\varepsilon$ are also qualitatively similar. In finite-dimensional spin glasses, the variance of $E_{0}$ across instances appears to scale with $\sqrt{N}$ in the thermodynamic limit [9]. Hence, $E_{0} / N J$ converges to a constant, and there is effectively no distinction between $\varepsilon$ and $\rho$ in the thermodynamic limit.

## Classical complexity of approximate optimization

Under the TTR metric, it can be shown that the approximate optimization of finite-range spin glasses has, in theory, a linear scaling using a simple divide-and-conquer algorithm. Namely, for a given residual energy target $\rho$, an algorithm exists whose TTR scales linearly in the system size for a sufficiently large size, with a large prefactor depending on the residual energy. For 2D spin glasses on a square lattice, it can be summarized as follows:

1. Partition the size- $N=L^{2}$ instance into $K \times K$ subgraphs $G_{x, y}$, with $x, y \in\{1, \ldots, K\}$, each with constant side length of $L_{0}=L / K$ spins;
2. Find the local ground states for each subgraph using an exact or heuristic solver for each $L_{0} \times L_{0}$

![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-10.jpg?height=900&width=721&top_left_y=190&top_left_x=255)

FIG. 5. Time-to-rho performance and scaling of S28 instances, which are comparable to those using the time-toepsilon metric.

instance. This step has cost $C\left(L_{0}\right)$, potentially exponential in $L_{0}^{2}$ on a non-planar lattice;

3. Patch together all subgraph ground state configurations as the approximate ground state for the global Hamiltonian and return this state's energy as the approximate ground state energy $E^{*}(N)$. This last step requires $O(N)$ time due to the need to sum the energies of $K^{2}=O(N)$ patches.

The overall complexity is, therefore, $C\left(L_{0}\right) O(N)$.

This algorithm optimizes bulk energies throughout the volume of the spin-glass at the expense of large energy violations over regions scaling with the surface area of the spin-glass, i.e., at the patch boundaries. The boundary excitations become negligible for sufficiently large sizes compared to the bulk energies, with the latter eventually reaching the desired residual energy. More precisely, up to $4 L_{0}$ boundary couplings may be violated per volume of $L_{0}^{2}$, so the residual energy density for this algorithm is upper-bounded by $4 / L_{0}$. Thus, we estimate that the regime of system sizes where this algorithm applies for the $\rho$ targets we examine, e.g., $1 \%$, would require a reliable, efficient solver for instances with at least a sub-problem side length of $L_{0} \approx 400$, resulting in a prefactor of $2^{400} \sim 10^{120}$, which is entirely impractical. Nevertheless, this algorithm could be a starting point for parallel and quantum-classical hybrid algorithms for massive, finite-dimensional problems. Furthermore,
![](https://cdn.mathpix.com/cropped/2024_06_04_e9e9bc1d27b2833366ebg-10.jpg?height=754&width=894&top_left_y=173&top_left_x=1079)

FIG. 6. Time-to-epsilon scaling of spin glass instances with R6 disorder for (a) $\varepsilon=0.75 \%$ and (b) $\varepsilon=1.10 \%$. To expedite finding the optimal $\mathrm{TT} \varepsilon$, the annealing times were limited to the range $t_{f} \in[0.5,9] \mu \mathrm{s}$ and $\mathrm{QAC}$ was limited to $J_{p}=0.2$ only.

such an algorithm could reach a similar target $\varepsilon$ in linear time with increasing probability as the system size increases due to the decreasing variance of $E_{0} /(J N)$.

## Alternative spin-glass disorder cases

We performed a similar study of instances with binomial disorder $J= \pm 1$. UnsurprIsingly, there was no substantial difference in scaling between U3 and QAC, and additionally, we were unable to validate the optimal annealing time for QAC at $\varepsilon=1 \%$. Thus, we considered the binomial disorder instances unsuitably easy for our purposes.

In an attempt to estimate the influence of precision and ground state degeneracy in the S28 disorder, we also studied range 6 (R6) instances for which the couplings were randomly drawn from $J \in$ $\{ \pm 1 / 6, \pm 2 / 6, \pm 3 / 6, \pm 4 / 6, \pm 5 / 6, \pm 1\}$. The main contrast between R6 and S28 is that the minimum non-zero local field a qubit may experience is much smaller in S28 disorder. Furthermore, in the absence of the Sidon property, the R6 disorder is more susceptible to ground-state degeneracies due to floppy qubits. However, it is less likely to occur than in a smaller range disorder case such as range 3 .

The $\mathrm{TT} \varepsilon$ scaling for the R6 case is shown in Fig. 6. For a smaller $\varepsilon=0.75 \%$, the scaling of QAC is better with R6 disorder than S28 disorder (Fig. 2), going from 2.26 to 2.02 , then equalizes as $\varepsilon$ increases. Perhaps surprIsingly, the opposite is true of U3, which scales worse on R6 disorder at smaller epsilon (2.92 to 3.10 ) before scaling better than S28 disorder above $\varepsilon \approx 1.1 \%$ (2.02
to 1.82). That is, while S28 is overall more challenging than R6 under QAC, low energy sampling of R6 is more challenging for unprotected QA than S28. This is likely caused by the worse relative precision of small coupling strengths in unprotected QA, which would not affect the S28 disorder.

[1] H. G. Katzgraber, S. Trebst, D. A. Huse, and M. Troyer, Feedback-optimized parallel tempering Monte Carlo, Journal of Statistical Mechanics: Theory and Experiment 2006, P03018 (2006).

[2] H. Munoz Bauza, TAMC software package (2023).

[3] H. Munoz Bauza, PegasusTools Python package (2023).

[4] F. Hamze and N. de Freitas, From fields to trees, in Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI '04 (AUAI Press, Arlington, Virginia, USA, 2004) pp. 243-250.

[5] A. Selby, Efficient subgraph-based sampling of Ising-type models with frustration, arXiv:1409.3934 (2014).
[6] H. G. Katzgraber, F. Hamze, and R. S. Andrist, Glassy Chimeras could be blind to quantum speedup: Designing better benchmarks for quantum annealing machines, Physical Review X 4, 021008 (2014).

[7] A. D. King, S. Suzuki, J. Raymond, A. Zucca, T. Lanting, F. Altomare, A. J. Berkley, S. Ejtemaee, E. Hoskinson, S. Huang, E. Ladizinsky, A. J. R. MacDonald, G. Marsden, T. Oh, G. Poulin-Lamarre, M. Reis, C. Rich, Y. Sato, J. D. Whittaker, J. Yao, R. Harris, D. A. Lidar, H. Nishimori, and M. H. Amin, Coherent quantum annealing in a programmable 2,000 qubit Ising chain, Nature Physics 18, 1324 (2022).

[8] A. D. King, J. Raymond, T. Lanting, R. Harris, A. Zucca, F. Altomare, A. J. Berkley, K. Boothby, S. Ejtemaee, C. Enderud, E. Hoskinson, S. Huang, E. Ladizinsky, A. J. R. MacDonald, G. Marsden, R. Molavi, T. Oh, G. Poulin-Lamarre, M. Reis, C. Rich, Y. Sato, N. Tsai, M. Volkmann, J. D. Whittaker, J. Yao, A. W. Sandvik, and M. H. Amin, Quantum critical dynamics in a 5,000qubit programmable spin glass, Nature 617, 61 (2023).

[9] J.-P. Bouchaud, F. Krzakala, and O. C. Martin, Energy exponents and corrections to scaling in Ising spin glasses, Physical Review B 68, 224404 (2003).

