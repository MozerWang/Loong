# An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention 

Junichiro Niimi ${ }^{1,2}$<br>(jniimi@meijo-u.ac.jp)<br>1. Faculty of Business Management, Meijo University<br>2. RIKEN Center for Advanced Intelligence Project (AIP)


#### Abstract

Today, the acquisition of various behavioral log data has enabled deeper understanding of customer preferences and future behaviors in the marketing field. In particular, multimodal deep learning has achieved highly accurate predictions by combining multiple types of data. Many of these studies utilize with feature fusion to construct multimodal models, which combines extracted representations from each modality. However, since feature fusion treats information from each modality equally, it is difficult to perform flexible analysis such as the attention mechanism that has been used extensively in recent years. Therefore, this study proposes a context-aware multimodal deep learning model that combines Bidirectional Encoder Representations from Transformers (BERT) and cross-attention Transformer, which dynamically changes the attention of deep-contextualized word representations based on background information such as consumer demographic and lifestyle variables. We conduct a comprehensive analysis and demonstrate the effectiveness of our model by comparing it with six reference models in three categories using behavioral logs stored on an online platform. In addition, we present an efficient multimodal learning method by comparing the learning efficiency depending on the optimizers and the prediction accuracy depending on the number of tokens in the text data.


Index Terms-Deep Learning, Multimodal Learning, electronic Word-of-Mouth, BERT, Cross-Attention, LLM, Transformer.

## I. INTRODUCTION

Nowadays, social media and other online platforms play an important role in shaping consumer behaviors and aiding decision-making. However, amidst the burgeoning amount of online information, users often face difficulties in discovering preferred content and suitable services [1]. To address this information overload, recommender systems have recently found application not only in social networking services (SNSs) and electronic commerce (EC) but also in wider domains such as tourism, healthcare, and education [2]. To optimize personalized content for each user, these systems must accurately discern the preferences of consumers with various sets of values to offer tailored recommendations.

With the evolution of machine learning techniques, contemporary models can handle a wide array of data, including text. Notably, Transformer [3] has made substantial contributions to the field of natural language processing (NLP). Bidirectional Encoder Representations from Transformers (BERT)
[4] is known as one of the significant models in this regard. Leveraging large language models (LLMs), BERT enables the prediction and classification of consumers based on the texts they contribute to the platform. Furthermore, many recommender systems leverage review texts posted on platforms, [5], commonly referred to as electronic word-of-mouth (eWOM).

In addition, multimodal learning, which combines multiple types of data to derive joint representations for classification and regression tasks, has gained widespread adoption. In particular, simultaneous analysis of data such as text and images, previously difficult to analyze individually, is now being undertaken in conjunction with other modalities. Nonetheless, despite these advancements, multimodal learning in marketing studies remains relatively limited, primarily due to the specificity of the data and marketing-specific issues, notably consumer heterogeneity [6].

Both the development of an optimal recommender system and the acquisition of review data are crucial on online platforms; however, the use of these data remains limited despite their potential value in understanding customer preferences. Therefore, in this study, we construct a novel multimodal deep learning model to assess user preferences on social platforms. The paper is structured as follows: First, we review prior studies relevant to our research. Next, we formulate hypotheses to address our research question. Subsequently, we outline the model architecture and provide an overview of the dataset. Then, we conduct several analyses to demonstrate the performance of the proposed model. Finally, we summarize the results and discuss the implications and challenges of our study.

## II. RELATEd StUDY

## A. Attention Mechanism

First, it is essential to discuss the attention mechanism (Fig. (1a) [7], which has had a significant impact on the field of machine learning. This mechanism operates by selectively focusing on relevant parts of the input sequence, thereby enabling models to prioritize and process these significant elements with greater emphasis. For example, a scaled-dot

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-02.jpg?height=401&width=217&top_left_y=195&top_left_x=214)

(a) Attention mechanism

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-02.jpg?height=393&width=265&top_left_y=194&top_left_x=453)

(b) Multihead attention

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-02.jpg?height=404&width=249&top_left_y=194&top_left_x=735)

(c) Transformer encoder
Fig. 1. From Attention to Transformer

attention (Att) is computed using query $(Q)$, key $(K)$, value $(V)$, and the softmax function (softmax) as follows:

$$
\begin{equation*}
\operatorname{Att}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{K}}}\right) V \tag{1}
\end{equation*}
$$

It adjusts the focus by computing attention weights within the softmax function, assigning relative importance to each element within the sequence. Particularly advantageous in handling large representations, attention can effectively train the model through layer-wise concatenation of multiple representations [8]. Two widely recognized variations of this mechanism are self-attention (SA) and source-target attention (STA), both obtained through the same calculation. In terms of differences, SA involves query, key, and value for the source and computes relationships between elements within the source sequence. On the other hand, STA uses query and key for the source, and value for the target, computing relationships between the source and the target. For example, in the field of NLP, SA is utilized to identify word-to-word relationships within a sentence, thus providing contextual understanding.

## B. Transformer

Based on the attention mechanism, Transformer distributes multiple attentions with weighted $Q, K$, and $V$ in parallel, a concept known as multihead attention (MHAtt, see Fig. 1b), which is expressed with $m$-th attention head $\left(\mathrm{Head}_{m}\right.$, where $m=1,2, \ldots, M)$ and Att as [3]:

$$
\begin{align*}
\operatorname{Head}_{m}(Q, K, V)= & \operatorname{Att}\left(Q W_{m}^{Q}, K W_{m}^{K}, V W_{m}^{V}\right)  \tag{2}\\
\operatorname{MHAtt}(Q, K, V)= & \operatorname{concat}\left(\operatorname{Head}_{1}, \text { Head }_{2}, \cdots\right.  \tag{3}\\
& \left.\operatorname{Head}_{M}\right) W^{O} \tag{4}
\end{align*}
$$

Transformer consists of an encoder and a decoder. Encoder's output (TransEnc, see Fig. 1c) is obtained using layernormalization $(L N)$ [9], feed-forward layer (FFL), residual network [10], and MHAtt as:

$$
\begin{equation*}
\operatorname{TransEnc}(Q, K, V)=L N(u+F F L(u)) \tag{5}
\end{equation*}
$$

$$
\begin{equation*}
\text { where } u=L N(Q+\operatorname{MHAtt}(Q, K, V)) \tag{6}
\end{equation*}
$$

While prior research has proposed integrating the attention mechanism into recurrent models [7], prior studies have shown that a single Transformer outperforms the combination of attention and recurrent structures [11].

Whether discussing the attention mechanism or Transformer, some studies [12], [13] have highlighted the utility of STA in capturing contextual information (i.e., the background) of the sequential data. Specifically, by setting tabular data (including demographic information) as the target, STA dynamically weighs the attention given to time-series data (including user behavior) as the source, based on demographic and other tabular variables. In addition, other study [14] highlighted the utility of cross-attention of Transformer which integrates both visual and textual post about the same event on social media to evaluate whether the post is informative or not.

## C. BERT

BERT stands out as one of the most significant pre-trained language models, which consists of Transformer encoder [4]. In the NLP field, the problem of ambiguity, where the meaning of a word changes depending on context, has long been recognized when handling textual data [15]. Within the BERT architecture, SA plays an important role in obtaining distributed representations of textual data known as deepcontextualized word representations. This mechanism overcomes the ambiguity problem by adjusting embeddings based on context, i.e., the word's relationship to other words in the sentence, unlike traditional word-embedding methods such as word2vec [16], which assign context-independent unique vectors [4], 17].

BERT utilizes fixed-length tokenization with padding and truncation. The full output shape of BERT is a 3-dimensional tensor with dimensions $\left(b s\right.$, len $\max$, param $_{B E R T}$ ), where len $\max$ represents the aligned length of the tokenized sequence, and param ${ }_{B E R T}$ depends on the scale of the BERT model (e.g., 768 for BERT-Base and 1024 for BERT-Large). In addition, BERT has a pooler-output which is the 2-dimensional tensor with shape ( $b s$, param $_{\text {BERT }}$ ) obtained by applying a tanh activation to a weighted sum of the [CLS] token. Pooler-output has been adopted in many downstream tasks due to its simplicity, effectively addressing ambiguity in natural language. For example, in marketing applications, a study [17] utilized BERT to obtain deep-contextualized word representations from review text about mobile applications on online platforms, enabling the prediction of user loyalty.

In addition, various models based on BERT have been proposed such as a robustly optimized BERT pre-training approach (RoBERTa) [18] and DistilBERT [19]. In particular, RoBERTa [18] is the improved model of BERT, which performance through pre-training on a larger dataset and longer training steps.

## D. Multimodal Learning

Originally, multimodal learning has made significant strides in computer science fields such as machine translation and computer vision [20], [21]. Multimodal learning involves extracting attributes from multiple data streams with different
shapes and dimensions, then learning to fuse these heterogeneous features and project them into a common representation space [22]. Two widely recognized approaches to conducting multimodal learning are early fusion and late fusion [23]. In late fusion, multiple decisions of classifiers are combined, while in early fusion, multiple representations from different inputs share a single hidden layer as a joint representation. In early fusion [21], feature fusion, typically achieved through layer-wise concatenation, forms a single feature map $\mathrm{H}_{3}$ by horizontally combining multiple input features $H_{1}$ and $H_{2}$ as $H_{3}=\left[H_{1}, H_{2}\right]$. In many cases where multimodal learning enhances accuracy, it does so by obtaining additional information beyond a single modality or leveraging information based on relationships between modalities. Prior study [24] shows that models perform optimally when combining representations from feature extraction with human-generated features.

The success of multimodal learning in these domains has spurred its application in wider domains, such as the classification of social media activity [25]-[27], the prediction of stock prices and credit scores in finance [12], [28], forecasting the usage amount of smartphone games [13], and evaluating customer product reviews [6]. Many of these studies emphasize the importance of multimodal learning that considers relationships between multiple modalities. It should be noted that some studies employ multimodal learning using attention mechanisms and developed models such as Memory Fusion Network (MFN) [29], [30], while others combine mechanisms and LSTM [12], [31], STA-Transformer [13], and crossattention between image and text [14]. As mentioned, since STA and cross-attention can model relationships between the source (input) and target (output), enabling the adjustment of attention weights based on features from modalities such as tabular data by placing different modalities at the source and target. It is shown to have the better performance than feature fusion.

## E. Consumer Heterogeneity and UGCs

In marketing literature, there has long been an acknowledgement of consumer heterogeneity, defined as latent differences in behaviors among multiple consumers. These differences, stemming from unobservable attributes such as demographic variables, life stage, and purpose of visit, significantly affect observable behaviors. The problem addressed in this study is that even when multiple users rate a restaurant similarly, understanding their preferences is hindered by the inability to discern latent differences. However, user-generated contents (UGCs), including review texts, offer potential insights into these differences.

Prior studies on electronic word-of-mouth (eWOM) [32], [33] and UGCs [34], [35] have predominantly focused on their impact on other consumers' brand attitudes, purchase intentions, and similar factors. However, UGCs also provide valuable information about consumer's own perceptions and attitudes toward products or services, enabling partial identification of heterogeneity without additional surveys typically required for cross-sectional data such as user profiles. Several studies [5], [6], [17] have enhanced the accuracy of product recommendations by analyzing customer evaluation data.
Nonetheless, a significant challenge for these studies is their reliance on single-modality textual data. As discussed in the Multimodal Learning subsection, extending these studies to multimodal learning models holds promise for further accuracy improvements. Moreover, one study [36] has constructed the crossmodal transfer learning model with considering heterogeneity in image and text.

## F. Research Gap and the Objective

Based on previous studies, we formulate hypotheses. As mentioned above, machine learning has drastically advanced so far; however, research gap exists especially with regard to applications of machine learning in marketing.

For example, while BERT is capable of acquiring deepcontextualized word representations based on literal context, marketing context encompasses broader aspects, including consumer demographics and life stage. Regarding the consumer behaviors such as posting the review on the online platforms, the meaning of the word could depend not only on text context but also on the background information of the consumers. Surprisingly, there are no studies addressing this broader context of textual and tabular data within a single model. Therefore, we construct a context-aware multimodal deep learning model using BERT and cross-attention, considering consumer context to predict behavior. Therefore, our first hypothesis is as follows.

H1 The prediction accuracy improves significantly using the context-aware model compared to reference models.

Additionally, we assess the effectiveness of our model across diverse sample groups: Restaurants, Nightlife, and Café (cf. Data Description subsection). Given the diverse characteristics of the Nightlife category, which may include entertainment factors such as shows, music, and alcohol, predicting ratings in this category is expected to be more challenging. Thus, we propose the following hypothesis.

H2 The prediction accuracy decreases on average in Nightlife category.

Moreover, multimodal learning models often contend with numerous parameters and complex architectures. In such sparse training scenarios, determining which parameters to update can be challenging, leading to vanishing gradient problem. While Adam optimization is a common choice for deep learning, Adamax may prove more effective in training such models since the original paper [37] highlights out the advantage in sparse gradients. Therefore, we establish another hypothesis as follows.

H3 In comparing the performance of multiple optimizers, Adamax achieves the highest average test score.

Although prior studies have compared different forms of LLMs in terms of prediction accuracy, the research landscape in applied domains remains somewhat lacking. In particular, BERT comes in various forms, distinguished by scale (params ${ }_{B E R T}$ : number of parameters in BERT) and advanced models (e.g., RoBERTa and DistilBERT). The question arises regarding BERT's impact on performance: whether it merely
serves as a means of acquiring deep-contextualized word representations, or if prediction accuracy can be further improved by employing more advanced or larger-scale BERT models. Therefore, we establish two hypotheses, respectively.

H4-1 The prediction accuracy improves on average with larger-scale pre-training models.

H4-2 The prediction accuracy improves on average with newer pre-training models.

Lastly, unlike tabular data, textual data differs significantly in the amount of information conveyed in each post (e.g., while one post contains only one-word impressions, another might provide detailed information about the user's situation and background). This variability poses a challenge for prediction accuracy, as illustrated by the differing amounts of information that the post retains. For example, regarding the two review texts shown in Fig. [210, even though they are all text data, the amount of information each holds is completely different. Similar issues have been pointed out in marketing fields,

```
[User A] Rate: 1/5
    Review: Disappointed
[User B] Rate: 1/5
    Review: Disappointed with this place as we were
    treated horribly. Although it was recommended as
    an ideal spot for couples, the staff members seemed
    noticeably unprepared for the sufficient service.
```

Fig. 2. Amount of information in the review text

for example, one study [38] pointed out that the prediction using behavioral logs may vary in accuracy depending on the quantity of services used. This problem is anticipated to arise in multimodal learning with textual data as well. Therefore, in this study, we also assess the change in prediction accuracy based on the number of tokens in the textual input. Thus, we propose the following hypothesis.

H5 Prediction accuracy decreases with fewer tokens in multimodal learning.

## III. MODEL

## A. Architecture

This study addresses both textual and tabular data which needs multiple inputs. The network is divided into three subnets based on their roles: X1-, X2-, and Output-subnet. $\mathrm{X} 1$ - and X2-subnet process each modality with appropriate structures, and Output-subnet concatenates them and predicts the values in the upper layers (Fig. 3, $b s$ : batch size, $J$ : number of tabular variables).

First, in the X1-subnet handling textual data, BERT and a tokenizer are employed to acquire deep-contextualized word representations. As discussed, using the pooler-output in multimodal learning may not always be optimal as it could lead to dimensionality reduction based on the [CLS] token in BERT.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-04.jpg?height=913&width=702&top_left_y=210&top_left_x=1167)

Fig. 3. Context-Aware Model

Despite the possible for selecting necessary features through multihead attentions within cross-attention, in this model, we opt for the state of the final hidden layer in BERT as the BERT

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-04.jpg?height=46&width=876&top_left_y=1408&top_left_x=1080)
in the X2-subnet managing tabular data, while feed-forward layers can be incorporated, the input data should not be overly processed prior to feature fusion. Therefore, we choose to directly feed the input into the Output-subnet.

The Output-subnet receives these two representations, fuses them, and generates outputs. While prior studies [12], [13] have utilized both STA and layer-wise concatenation, if the STA mechanism adequately captures the features of two modalities, it is uncertain whether early fusion is necessary to obtain a joint representation. Hence, our proposed model adopts the cross-attention Transformer encoder with eight attention heads. This mechanism is anticipated to yield high accuracy without feature fusion, as it captures the relationship between the two modalities from multiple perspectives, which can be challenging with a single attention mechanism. In this study, this proposed model is called the context-aware model.

## B. Evaluation

To evaluate the effectiveness of our proposed model, we construct two multimodal learning models and two monomodal models as reference points. First, for the multimodal approach, we introduce a context-fusion model (referred to Fig. 4a), which integrates feature fusion into the contextaware model. Additionally, we implement a typical multimodal model with feature fusion using pooler-output (refer to Fig.

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-05.jpg?height=559&width=423&top_left_y=195&top_left_x=165)

(a) Context-fusion model

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-05.jpg?height=561&width=440&top_left_y=191&top_left_x=604)

(b) Feature-fusion model
Fig. 4. Reference models (multimodal)

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-05.jpg?height=558&width=425&top_left_y=911&top_left_x=167)

(a) Textual model

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-05.jpg?height=556&width=420&top_left_y=909&top_left_x=619)

(b) Tabular model
Fig. 5. Reference models (monomodal)

4b) 2 . Since the feature-fusion model directly receives the highdimensional representation from the BERT output, the number of hidden layers in the output layer post feature fusion is increased to three, with each layer comprising 512,256 , and 128 units.

Second, for the monomodal models, we introduce the textual and tabular models (referred to Fig. 5a, 5b, which process modality-specific layers and transmit them to the output layer without traversing through Transformer or feature-fusion architectures. Moreover, we incorporate two benchmarks: a linear regression model that captures linear relationships and a random model that generates random predictions within the range of $[0,1]$, and. These six reference models allows for a comprehensive comparison of the performance of the proposed models.

In terms of optimization, many existing studies have adopted Adam [37] as an optimizer; however, as described in H2, it has yet to be clarified what optimizer is effective for a complex architecture of multimodal learning. Hence, this study delves into the impact of different optimizers on prediction accuracy,[^1]

including Adam, Nesterov-accelerated Adaptive Moment Estimation (Nadam) [39], and Adamax.

Furthermore, regarding the pre-trained BERT model, we initially employ bert-base-uncased among several pre-trained models to demonstrate the superior prediction accuracy of our proposed model's architecture compared to others (Study 1). Subsequently, we explore changes in accuracy by replacing bert-base-uncased with different pre-trained models (Study 2).

TABLE I

MODEL SETTINGS

| Parameters | Values |
| :--- | :--- |
| Hyper-Parameters | 500 |
| Number of Epochs | 256 |
| Batchsize | Adamax |
| Optimizer | mean squared error (MSE) |
| Loss Function |  |
| X1-subnet | BERT |
| Sert-base-uncased |  |
| Pre-trained Model | 768 |
| params $B E R T$ | 0 |
| X2-subnet | 15 |
| Number of Hidden Layers |  |
| Number of Input Features $(J)$ | used in 'Context-Aware' and 'Context-Fusion' |
| Feature Fusion | 8 |
| Cross-Attention | used in 'Context-Fusion' and 'Feature Fusion' |
| Layer-wise Concatenation |  |
| Output-subnet | tanh |
| Activation (hidden layers) | $2-3$ (cf. Fig. 1 |
| Activation (output) |  |
| Hidden Layers |  |

Note. tanh stands for hyperbolic tangent function.

## C. Data Description

To validate the efficacy of the proposed model, we require behavioral log data containing both textual and tabular information. For this purpose, we utilize the Yelp Open Dataset [40]. Yelp, an online platform, offers a wealth of information about various venues including restaurants, stores, and public facilities, alongside user ratings and reviews. The dataset comprises user review texts, profiles, and venue details. Each location is associated with one or more category tags, facilitating the extraction of target locations by specifying these tags. In our study, we focus on three business categories to demonstrate the robustness of the model: Restaurants (tagged with "Restaurants", but not neither with "Fast Food", "Food Truck", "Nightlife", and "Bar"), Nightlife (tagged with both "Restaurants" and "Nightlife", but not neither with "Fast Food" and "Food Truck"), and Café (tagged with both "Cafes" and "Coffee and Tea", but not neither with "Fast Food" and "Food Truck"). In particular, Nightlife category encompasses various types of establishments such as bars and nightclubs, posing challenges in evaluation solely based on store information.

For the sake of data acquisition convenience, we predict the ratings (i.e., the number of stars) of restaurants using review texts, user profile information, and restaurant information. While the target variable can be readily obtained from the app, its accurate prediction by our proposed model signifies
its suitability in understanding consumer preferences and its potential extension into an effective recommender system [5].

We randomly sample 10,000 posts of ratings and reviews from each category containing one or more English words in year 2018. In cases where a user reviews the same location multiple times, we consider only the latest post. For textual data preprocessing, we replace line breaks, emojis, icons, and other symbols with periods and merge continuous sequences of periods into a single period. A summary of the dataset statistics is provided in Table II Notably, there are no duplications for the location in the Restaurants and Nightlife categories.

TABLE II

StATISTICS OF THE CATEGORIES

| Category | \#Users | \#Spots | \#Stars |  |  | \#Tokens |  |  |  |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
|  |  |  | Mean | Std |  | Mean | Std | Min | Max |
| Restaurants | 9567 | 1387 | 3.9 | 1.4 |  | 110.3 | 91.6 | 9 | 512 |
| Nightlife | 9491 | 2097 | 3.9 | 1.4 |  | 119.9 | 100.1 | 11 | 512 |
| Café | 9189 | 665 | 4.2 | 1.2 |  | 109.9 | 90.5 | 10 | 512 |

Note. \#Users and \#Spots indicate the unique numbers of users and restaurants in each category, respectively.

The dataset of each category consists of $D=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$ with a sample size of $n=10000$, where each input $x_{i}$ comprises one textual variable and $J$ tabular variables, denoted as $x_{i}=\left(x_{i}^{(t e x t)}, x_{i}^{(t a b)}\right)=\left(x_{1 i}^{(t e x t)}, x_{1 i}^{(t a b)}, x_{2 i}^{(t a b)}, \ldots, x_{J i}^{(t a b)}\right) \in$ $\mathbb{X}$. The target variable $y_{i} \in \mathbb{Y}=[0,1]$ represents a normalized value of the ratings, which is scaled between 0 and 1 from a range of 1 to 5 stars. The variables are shown in Table III.

The dataset of 10,000 observations is split into training $(70 \%)$, validation ( $15 \%$ ), and test (15\%) subsets. During training, the loss function employed is mean squared error (MSE), while model performance is evaluated using root mean squared error (RMSE) using actual and predicted values $\left(y_{i}, \hat{y}_{i}\right)$ as follows:

$$
\begin{align*}
\mathrm{MSE} & =\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}  \tag{7}\\
\mathrm{RMSE} & =\sqrt{\mathrm{MSE}} \tag{8}
\end{align*}
$$

Detailed model settings are provided in Table $\square$

## IV. ReSUlts AND DisCUSSION

## A. Study 1: Comparison Across the Model Architectures

The results are presented in Table IV, reveal a similar pattern across all categories. The proposed model consistently achieves the highest prediction accuracy in the test scores across all categories. The context-fusion model follows closely behind, while the performance of the feature-fusion model sometimes lags behind that of the textual and even linear regression models. In particular, despite the context-fusion model having the largest number of parameters in this study. Context-Fusion model fuses the representations twice with STA-Transformer and feature fusion, but the contribution on the performance is actually limited. Conversely, the random model exhibits the lowest accuracy, followed by the linear regression model in most cases.
TABLE III

| Variable Name | Description |
| :---: | :---: |
| $\mathbf{Y}$ : Target Variable $(b s, 1)$ |  |
| - rating | Rating value posted on Yelp ${ }^{\dagger}$ |
| X1: Textual Variable $(b s, 1)$ |  |
| - review | Review text posted on Yelp tokenized <br> with fixed-length of $l e n_{\max }=512$ tokens |
| X2: Tabular Variables shape $(b s, 15)$ |  |
| Location <br> - open_dow <br> - <br> open_hours | ![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-06.jpg?height=152&width=561&top_left_y=644&top_left_x=1346) |
| - open_mon <br> . . <br> - open_sun | A number of opening hours in Monday ${ }^{\dagger}$ <br> A number of opening hours in Sunday ${ }^{\dagger}$ |
| User <br> $-n \_f r i e n d s$ <br> $-n \_f a n s$ <br> $-n \_e l i t e s$ | A number of friends <br> A number of getting fans <br> A number of getting elite |
| Post <br> - n_useful <br> - n_funny <br> - n_cool | A number of getting useful <br> A number of getting funny <br> A number of getting cool |

MODEL VARIABLES

${ }^{\dagger}$ Variables are normalized in $[0,1]$.

Although some reference models stopped training in fewer epochs than the proposed model, this trend does not necessarily indicate early convergence due to the absence of early stopping [41]. Rather, it suggests that these models struggled to escape local convergence in the early stages 3 . These results guarantee the generalized performance of the proposed model, and thus, $\mathbf{H 1}$ is supported.

Second, Table $\mathrm{V}$ provides an overview of the average performance considering various perspectives: such as target categories, modalities, and optimizers. As anticipated, the Nightlife category exhibits slightly lower test performance than the Restaurants category, possibly due to the diverse nature of establishments in the Nightlife category. Nonetheless, the mean score by modality indicates a high level of predictability, underscoring the usefulness of analyzing multiple modalities. This result supports $\mathbf{H 2}$.

Third, the results in Table $\square$ further demonstrate the effectiveness of adamax as an optimizer, particularly in handling the complex structure of neural networks dealing with sparse textual representations. Despite taking longer for training, adamax proves considerably more effective. Notably, even with an enormous number of parameters, adamax demonstrates superior performance in effectively updating the weights. A comparison of the change in losses of the context-aware model among different optimizers in Fig. 6, corroborates these findings. Adamax shows outstanding effectiveness over the training epochs. The progression of learning in the three[^2]

TABLE IV

RESULTS (WITH ADAMAX OPTIMIZER, ASCENDING IN TEST RMSE)

|  | Model | Modality | BERT Model | Optimizer | Train | Validation | Test | Epochs | Training Time | \#Parameters |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-07.jpg?height=242&width=49&top_left_y=349&top_left_x=203) | Multimodal | context-aware | bert-base-uncased | adamax | 0.085 | 0.135 | 0.132 | 316 | 04:01:19 | $119,122,520$ |
|  | Multimodal | context-fusion | bert-base-uncased | adamax | 0.109 | 0.130 | 0.134 | 125 | $01: 50: 58$ | $119,123,080$ |
|  | X1-modal | textual | bert-base-uncased | adamax | 0.151 | 0.149 | 0.143 | 499 | 05:53:48 | $109,712,129$ |
|  | Multimodal | feature-fusion | bert-base-uncased | adamax | 0.152 | 0.152 | 0.155 | 290 | 03:33:04 | $110,048,001$ |
|  | X2-modal | tabular | bert-base-uncased | adamax | 0.258 | 0.260 | 0.261 | 313 | $00: 01: 05$ | 281 |
|  |  |  | Linear regression: |  | 0.259 | 0.261 | 0.262 |  |  |  |
|  |  |  |  | Random: | 0.494 | 0.496 | 0.503 |  |  |  |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-07.jpg?height=242&width=46&top_left_y=596&top_left_x=203) | Multimodal | context-aware | bert-base-uncased | adamax | 0.084 | 0.127 | 0.140 | 401 | 05:22:41 | $119,122,520$ |
|  | Multimodal | context-fusion | bert-base-uncased | adamax | 0.093 | 0.129 | 0.141 | 406 | $05: 25: 48$ | $119,123,080$ |
|  | X1-modal | textual | bert-base-uncased | adamax | 0.150 | 0.141 | 0.150 | 476 | $05: 33: 54$ | $109,712,129$ |
|  | Multimodal | feature-fusion | bert-base-uncased | adamax | 0.141 | 0.144 | 0.161 | 423 | $05: 02: 02$ | $110,048,001$ |
|  | X2-modal | tabular | bert-base-uncased | adamax | 0.255 | 0.254 | 0.257 | 471 | $00: 01: 31$ | 281 |
|  |  |  | Linear regression: |  | 0.262 | 0.259 | 0.260 |  |  |  |
|  |  |  |  | Random: | 0.481 | 0.480 | 0.482 |  |  |  |
| Uّש | Multimodal | context-aware | bert-base-uncased | adamax | 0.076 | 0.127 | 0.125 | 475 | $06: 08: 27$ | $119,122,520$ |
|  | Multimodal | context-fusion | bert-base-uncased | adamax | 0.074 | 0.125 | 0.127 | 480 | 06:02:39 | $119,123,080$ |
|  | Multimodal | feature-fusion | bert-base-uncased | adamax | 0.137 | 0.147 | 0.142 | 213 | $02: 43: 10$ | $110,048,001$ |
|  | X1-modal | textual | bert-base-uncased | adamax | 0.137 | 0.141 | 0.147 | 491 | $06: 10: 34$ | $109,712,129$ |
|  | X2-modal | tabular | bert-base-uncased | adamax | 0.231 | 0.241 | 0.228 | 234 | $00: 01: 03$ | 281 |
|  |  |  | Linear regression: |  | 0.231 | 0.236 | 0.223 |  |  |  |
|  |  |  |  | Random: | 0.509 | 0.500 | 0.516 |  |  |  |

Note. Bold type represents the best model for the indices. Training Time shows the actual duration for the best validation score in $h h: m m$ :ss using the same environment (GPU: NVIDIA A100-SXM4-40GB).

TABLE V

AVERAGE PERFORMANCES BY THE GROUP

| Group | Train | Validation | Test | Epochs | Training Time | \#Parameters |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Category |  |  |  |  |  |  |
| Café | $\mathbf{0 . 1 5 9}$ | $\mathbf{0 . 1 7 8}$ | $\mathbf{0 . 1 7 5}$ | 340.5 | $03: 14: 07$ | $91,601,202.2$ |
| Restaurants | 0.193 | 0.207 | 0.209 | $\mathbf{2 7 8 . 5}$ | $\mathbf{0 2 : 4 8 : 3 3}$ | $91,601,202.2$ |
| Nightlife | 0.208 | 0.218 | 0.226 | 321.4 | $03: 07: 13$ | $91,601,202.2$ |
| Modality |  |  |  |  |  |  |
| context-aware | $\mathbf{0 . 1 0 0}$ | $\mathbf{0 . 1 3 3}$ | $\mathbf{0 . 1 4 0}$ | 394.7 | $05: 01: 51$ | $119,122,520.0$ |
| context-fusion | 0.108 | 0.133 | 0.141 | 400.2 | $05: 02: 31$ | $119,123,080.0$ |
| Multimodal | 0.153 | 0.174 | 0.179 | 320.9 | $04: 02: 31$ | $116,097,867.0$ |
| X1-modal | 0.228 | 0.230 | 0.229 | 312.7 | $03: 08: 03$ | $109,712,129.0$ |
| X2-modal | 0.249 | 0.253 | 0.251 | 292.0 | $\mathbf{0 0 : 0 0 : 5 3}$ | 281.0 |
| feature-fusion | 0.250 | 0.255 | 0.257 | $\mathbf{1 6 7 . 8}$ | $02: 03: 12$ | $110,048,001.0$ |
| Optimizer |  |  |  |  |  |  |
| Adamax | $\mathbf{0 . 1 4 2}$ | $\mathbf{0 . 1 6 0}$ | $\mathbf{0 . 1 6 3}$ | 374.2 | $03: 29: 54$ | $91,601,202.2$ |
| Nadam | 0.202 | 0.214 | 0.217 | 309.1 | $\mathbf{0 2 : 4 8 : 0 2}$ | $91,601,202.2$ |
| Adam | 0.217 | 0.228 | 0.231 | $\mathbf{2 5 7 . 1}$ | $02: 51: 57$ | $91,601,202.2$ |

Note. Bold type represents the best model for the indices. Training Time shows the actual duration for the best validation score in hh:mm:ss using the same environment (GPU: NVIDIA A100-SXM4-40GB)

categories shows that adamax does not always train efficiently from the early stages; however, as it proceeds to the later stages, only adamax continues to reduce the loss while the other optimizers converges locally. Thus, H3 is supported.

## B. Study 2: Impact of Replacing Pre-Trained Models

The results from Study 1 demonstrate the effectiveness of our proposed architecture; however, even with its high accuracy in multimodal learning, the model relies on BERTBase-Uncased component. To further investigate the impact of different pre-trained models, we conducted additional analyses by replacing the BERT component with BERT-Large-Uncased,

![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-07.jpg?height=648&width=876&top_left_y=1237&top_left_x=1080)

Fig. 6. Training process by different optimizers

RoBERTa-Base, and RoBERTa-Large within context-aware model.

The findings, presented in Table VI confirm a significant improvement in test performance on average with BERTLarge-Uncased and RoBERTa-Base compared to Bert-BaseUncased across all three categories. Both RoBERTa-Base and BERT-Large-Uncased contribute to the accuracy, while RoBERTa-Large does not exhibit the same level of improvement. The average test scores suggest that both RoBERTaBase and BERT-Large-Uncased demonstrate comparable generalization capabilities, with RoBERTa-Base outperforming in terms of convergence time. The lower accuracy observed with the RoBERTa-Large component could be attributed to

TABLE VI

IMPACT OF THE PRE-TRAINED MODELS (WITH ADAMAX OPTIMIZER, ASCENDING IN TEST RMSE)

|  | Modality | BERT Model | Optimizer | Train | Validation | Test | Epochs | Training Time | \#Parameters |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-08.jpg?height=151&width=47&top_left_y=358&top_left_x=210) | context-aware | bert-large-uncased | adamax | 0.079 | 0.129 | 0.121 | 496 | 16:16:07 | $347,927,896$ |
|  | context-aware | roberta-base | adamax | 0.091 | 0.133 | 0.131 | 319 | 04:11:54 | $134,285,912$ |
|  | context-aware | bert-base-uncased | adamax | 0.085 | 0.135 | 0.132 | 316 | 04:01:19 | $119,122,520$ |
|  | context-aware | roberta-large | adamax | 0.076 | 0.144 | 0.150 | 490 | 16:00:06 | $368,145,752$ |
| $\frac{9}{7}$ <br> $\bar{z}$ <br> $\bar{z}$ | context-aware | roberta-b: | adamax | 0.095 | 0.120 | 0.130 | 176 | 02:23:58 | $134,285,912$ |
|  | context-aware | bert-large-uncased | adamax | 0.085 | 0.123 | 0.135 | 416 | 16:29:20 | $347,927,896$ |
|  | context-aware | roberta-large | adamax | 0.084 | 0.125 | 0.136 | 401 | 13:14:30 | $368,145,752$ |
|  | context-aware | bert-base-uncased | adamax | 0.084 | 0.127 | 0.140 | 401 | 05:22:41 | $119,122,520$ |
| نֶ | context-aware | roberta-base | adamax | 0.077 | 0.118 | 0.120 | 377 | 04:51:41 | $134,285,912$ |
|  | context-aware | bert-large-uncased | adamax | 0.071 | 0.122 | 0.124 | 494 | 16:16:29 | $347,927,896$ |
|  | context-aware | bert-base-uncased | adamax | 0.076 | 0.127 | 0.125 | 475 | 06:08:27 | $119,122,520$ |
|  | context-aware | roberta-large | adamax | 0.082 | 0.128 | 0.137 | 238 | 07:58:23 | $368,145,752$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-08.jpg?height=137&width=64&top_left_y=831&top_left_x=203) | context-aware | bert-large-uncased | adamax | 0.078 | 0.125 | 0.127 | 468.7 | 16:20:39 | $347,927,896$ |
|  | context-aware | roberta-base | adamax | 0.088 | 0.123 | 0.127 | 290.7 | 03:49:11 | $134,285,912$ |
|  | context-aware | bert-base-uncased | adamax | 0.082 | 0.130 | 0.132 | 397.3 | 05:10:49 | $119,122,520$ |
|  | context-aware | roberta-large | adamax | 0.081 | 0.132 | 0.141 | 376.3 | 12:24:20 | $368,145,752$ |

Note. Bold type represents the best model for the indices. Training Time shows the actual duration for the best validation score in $h h: m m$ :ss using the same environment (GPU: NVIDIA A100-SXM4-40GB).

the insufficient sample size relative to the complexity of the architecture. Previous studies have indicated that largescale models like RoBERTa-Large require a larger sample size for optimal performance. Thus, both $\mathbf{H} 4$-1 and $\mathbf{H} 4-2$ are supported, respectively.

## C. Study 3: Impact of the Number of Tokens

Finally, we examine the impact of the amount of information in the review text on prediction accuracy, as described in H41 and H4-2. We regard the number of tokens in the review as a measure of information and investigate whether accuracy varies with the number of tokens. The best model from Study 1 (context-aware model with bert-base-uncased and the adamax optimizer) is utilized for each category. We set up the token strata by dividing three subsets of training, validation, and test data into $20 \%$ according to the number of tokens. Then, we predict and compute the average RMSE by strata.

The results, categorized by the number of tokens and by stratum are shown in Table VIH. For the test data alone, the prediction accuracy is highest when the number of tokens is lowest in the Restaurants category, and medium in the other two categories. This suggests that while multimodal learning of textual and tabular data is expected to improve prediction accuracy, it does not always require a large amount of textbased information. For two categories other than Restaurants, the prediction accuracy is also best in the lowest tokens strata in training and validation. However, in the Nightlife and Cafécategories, which exhibit wide variation in location attributes, higher numbers of tokens ensure generalizability in test performance, whereas the model for the Restaurants category demonstrates high generalizability with fewer tokens. In addition, regarding the observed decrease in accuracy with[^3]

particularly large numbers of tokens, several possible reasons exist. First, we cut off sentences with more than 512 tokens due to the size of BERT's context window, which may not convey enough information to the model. Second, excessively long texts may contain redundant information unrelated to the user ratings, leading to that the model has not properly discerned the information. Thus, $\mathbf{H 5}$ is not supported.

TABLE VII

IMPACT OF THE NUMBER OF TOKENS (WITH ADAMAX OPTIMIZER, ASCENDING IN THE NUMBER OF TOKENS)

|  | Train |  | Validation |  | Test |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $M$ | $\overline{R M S E}$ | $\bar{M}$ | RMSE | $\bar{M}$ | RMSE |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0a3699d89150659e1cd1g-08.jpg?height=158&width=38&top_left_y=1750&top_left_x=1107) | 31.5 | 0.067 | 30.4 | 0.109 | 31.8 | 0.115 |
|  | 54.1 | 0.068 | 54.3 | 0.130 | 55.5 | 0.125 |
|  | 81.8 | 0.069 | 81.8 | 0.130 | 82.9 | 0.127 |
|  | 127.1 | 0.075 | 125.7 | 0.135 | 131.8 | 0.147 |
|  | 258.2 | 0.086 | 262.7 | 0.165 | 260.3 | 0.154 |
| $\underbrace{0}_{Z}$ | 31.8 | 0.125 | 33.3 | 0.118 | 33.0 | 0.137 |
|  | 56.2 | 0.132 | 56.7 | 0.130 | 59.1 | 0.137 |
|  | 88.4 | 0.139 | 88.8 | 0.141 | 90.3 | 0.102 |
|  | 139.1 | 0.140 | 140.6 | 0.140 | 136.6 | 0.147 |
|  | 284.7 | 0.161 | 288.8 | 0.152 | 282.4 | 0.156 |
| نَّ | 31.1 | 0.112 | 32.4 | 0.131 | 31.0 | 0.114 |
|  | 53.5 | 0.121 | 56.0 | 0.120 | 53.7 | 0.115 |
|  | 80.5 | 0.128 | 86.2 | 0.125 | 84.6 | 0.109 |
|  | 123.7 | 0.128 | 133.8 | 0.130 | 130.7 | 0.123 |
|  | 255.9 | 0.147 | 261.3 | 0.154 | 261.2 | 0.170 |

Note. $M$ represents the mean number of tokens in the strata. Bold type represents the best score in the each dataset.

## V. CONCLUSION

## A. Contribution

In this study, we propose a novel multimodal deep learning model that integrates posted review texts with tabular data,
including user profiles and location information. This model effectively captures consumer heterogeneity to predict user ratings on locations with high accuracy. In addition, we conduct a comprehensive analysis of different pre-trained models and the effect of token count on prediction accuracy.

Our proposed model consistently outperforms reference models on test data across all categories. This result indicates the superiority of contextual understanding facilitated by the cross-attention over mere feature fusion for joint representation. Despite prior studies confirming the efficacy of multimodal learning in the various field, in this study, feature fusion which is a simple form of multimodal learning does not overtake of single-modality models. This limitation may stem from the complexity of features, as even with a substantial number of units in the Output-subnet, the large-scale deepcontextualized word representations may overwhelm the upper hidden layers. This result indicates the limitations of simple feature-fusion methods, and as the complexity of the features to be combined reflects, sophisticated mechanisms are needed to understand them.

In addition, our proposed model exclusively utilizes the cross-attention, unlike previous research that emphasizes the combination of features through both attention and feature fusion [12]. Our results demonstrate that achieving higher accuracy is feasible with the cross-attention alone. By establishing causality between different modalities as the source and target, the model can effectively attend to large and sparse features. Although our study focuses on predicting ratings due to data availability, it highlights the potential to construct models based on an accurate understanding of user preferences.

Extending the proposed model presented in this study opens the door to addressing various advanced tasks, such as a model that recommends the appropriate content based on user's past posts and profile and another model that predicts future repeated purchases based on a consumer's past product reviews and purchase history on the EC platforms.

## B. Challenges

Our model still encounters challenges in improving prediction accuracy, primarily due to computational limitations. All BERT layers in our study remain frozen (i.e., parameters are set to non-trainable) during the training process due to these limitations. In addition, newly developed LLMs are proposed one after another. That is, the model can be further improved through structural refinements, such as selecting different LLMs, fine-tuning BERT layers, incorporating additional dropout, adjusting the number and shape of hidden layers, and optimizing other hyper-parameters.

Finally, despite the use of LLMs, the handling a large number of tokens remains difficult. Our study suggests that an excessive number of tokens may actually decrease prediction accuracy. To address the issue, appropriate measures must be taken, such as pre-summarizing large amounts of text data or using LLMs with larger context windows. It is worth noting that such analyses require additional computational resources and training time, which is a problem to be balanced with prediction accuracy.

## ETHICAL STATEMENT

This study only uses academic open data and does not additionally collect personally identifiable information. We observe the terms of use of the dataset and manage the data in a secure environment.

## ACKNOWLEDGMENT

Our comprehensive analyses were implemented on RAIDEN, a computing infrastructure hosted by RIKEN AIP. We would like to express our gratitude to all the members of the center who maintain the system. Additionally, we extend our gratitude to Yelp which enriched our study by providing the open data.

## REFERENCES

[1] S. Zhang, L. Yao, A. Sun, and Y. Tay, "Deep learning based recommender system: A survey and new perspectives," ACM computing surveys (CSUR), vol. 52, no. 1, pp. 1-38, 2019.

[2] H. Ko, S. Lee, Y. Park, and A. Choi, "A survey of recommendation systems: recommendation models, techniques, and application fields," Electronics, vol. 11, no. 1, p. 141, 2022.

[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[5] Y. Zhuang and J. Kim, "A bert-based multi-criteria recommender system for hotel promotion management," Sustainability, vol. 13, no. 14, p. 8039,2021

[6] J. Niimi, "Multimodal deep learning of word-of-mouth text and demographics to predict customer rating: Handling consumer heterogeneity in marketing," arXiv preprint arXiv:2401.11888, 2024.

[7] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," arXiv preprint arXiv:1409.0473, 2014.

[8] R. A. Baten, Y. Liu, H. Peters, F. Barbieri, N. Shah, L. Neves, and M. W. Bos, "Predicting future location categories of users in a large social platform," in Proceedings of the International AAAI Conference on Web and Social Media, vol. 17, 2023, pp. 47-58.

[9] J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.

[10] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.

[11] D. Soydaner, "Attention mechanism in neural networks: where it comes and where it goes," Neural Computing and Applications, vol. 34, no. 16, pp. $13371-13385,2022$.

[12] M. Ala'raj, M. F. Abbod, and M. Majdalawieh, "Modelling customers credit card behaviour using bidirectional 1stm neural networks," Journal of Big Data, vol. 8, no. 1, pp. 1-27, 2021.

[13] J. Niimi, "Prediction of behavioral loyalty using different dimensionality data: Multimodal deep learning with transformer encoder and serial feature fusion," Japanese Journal of Applied Statistics, vol. 53, no. 1, 2024.

[14] A. Khattar and S. Quadri, "Camm: Cross-attention multimodal classification of disaster-related tweets," IEEE Access, vol. 10, pp. 92 889-92 902, 2022.

[15] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, "Deep contextualized word representations," in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), M. Walker, H. Ji, and A. Stent, Eds. New Orleans, Louisiana: Association for Computational Linguistics, 2018, pp. 2227-2237. [Online]. Available: https://aclanthology.org/N18-1202

[16] T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," arXiv preprint arXiv:1301.3781, 2013.

[17] Z. H. Kilimci, "Prediction of user loyalty in mobile applications using deep contextualized word representations," Journal of Information and Telecommunication, vol. 6, no. 1, pp. 43-62, 2022.

[18] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.

[19] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter," arXiv, 2019.

[20] N. Srivastava and R. R. Salakhutdinov, "Multimodal learning with deep boltzmann machines," Advances in neural information processing systems, vol. 25, 2012.

[21] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, "Multimodal deep learning," in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689-696.

[22] K. Bayoudh, R. Knani, F. Hamdaoui, and A. Mtibaa, "A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets," The Visual Computer, vol. 38, no. 8, pp. 2939-2970, 2022.

[23] L. I. Kuncheva, Combining pattern classifiers: methods and algorithms. John Wiley \& Sons, 2014.

[24] L. Nanni, S. Ghidoni, and S. Brahnam, "Handcrafted vs. non-handcrafted features for computer vision classification," Pattern Recognition, vol. 71, pp. 158-172, 2017.

[25] F. S. Abousaleh, W.-H. Cheng, N.-H. Yu, and Y. Tsao, "Multimodal deep learning framework for image popularity prediction on social media," IEEE Transactions on Cognitive and Developmental Systems, vol. 13, no. 3, pp. 679-692, 2020

[26] F. Ofli, F. Alam, and M. Imran, "Analysis of social media data using multimodal deep learning for disaster response," arXiv preprint arXiv:2004.11838, 2020.

[27] L. Zhang, J. Shen, J. Zhang, J. Xu, Z. Li, Y. Yao, and L. Yu, "Multimodal marketing intent analysis for effective targeted advertising," IEEE Transactions on Multimedia, vol. 24, pp. 1830-1843, 2021.

[28] S. I. Lee and S. J. Yoo, "Multimodal deep learning for finance: integrating and forecasting international stock markets," The Journal of Supercomputing, vol. 76, pp. 8294-8312, 2020.

[29] T. Mittal, U. Bhattacharya, R. Chandra, A. Bera, and D. Manocha, "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues," in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 02, 2020, pp. 1359-1367.

[30] -, "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues," in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 02, 2020, pp. 1359-1367.

[31] M. G. Huddar, S. S. Sannakki, and V. S. Rajpurohit, "Attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional 1stm," Multimedia Tools and Applications, vol. 80, pp. 13 059-13 076, 2021.

[32] P. C. Wu and Y.-C. Wang, "The influences of electronic word-of-mouth message appeal and message source credibility on brand attitude," Asia Pacific Journal of Marketing and Logistics, vol. 23, no. 4, pp. 448-472, 2011.

[33] A. N. Albarq, "Measuring the impacts of online word-of-mouth on tourists' attitude and intentions to visit jordan: An empirical study," International Business Research, vol. 7, no. 1, p. 14, 2014.

[34] J. Mohammad, F. Quoquab, R. Thurasamy, and M. N. Alolayyan, "The effect of user-generated content quality on brand engagement: The mediating role of functional and emotional values," Journal of Electronic Commerce Research, vol. 21, no. 1, pp. 39-55, 2020.

[35] A. J. Kim and K. K. Johnson, "Power of consumers using social media: Examining the influences of brand-related user-generated content on facebook," Computers in human behavior, vol. 58, pp. 98-108, 2016.

[36] L. Zhen, P. Hu, X. Peng, R. S. M. Goh, and J. T. Zhou, "Deep multimodal transfer learning for cross-modal retrieval," IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 2, pp. 798-810, 2020.

[37] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.

[38] P. S. Fader, B. G. Hardie, and K. L. Lee, "Rfm and clv: Using isovalue curves for customer base analysis," Journal of marketing research, vol. 42, no. 4, pp. 415-430, 2005.

[39] T. Dozat, "Incorporating nesterov momentum into adam," 2016.

[40] Yelp, Yelp Open Dataset, An all-purpose dataset for learning (https://www.yelp.com/dataset, accessed Nov. 20th, 2023), 2022.

[41] L. Prechelt, "Early stopping-but when?" in Neural Networks: Tricks of the trade. Springer, 1998, pp. 55-69.


[^0]:    ${ }^{1}$ Since the dataset employed in this study prohibits the disclosure of actual review sentences, the texts shown in the figure are fictitious ones written by the authors.

[^1]:    ${ }^{2}$ We employ pooler-output for the feature-fusion model due to the requirement of two-dimensional representations for layer-wise concatenation

[^2]:    ${ }^{3}$ A similar tendency is confirmed in Fig. 6 Noted that it is about a different analysis.

[^3]:    ${ }^{4}$ Note that the values shown in the table represent the prediction accuracy for trained, validated, and test samples without any further training.

