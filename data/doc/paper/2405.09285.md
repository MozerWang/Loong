# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning 

Junfeng Chen ${ }^{1}$ Kailiang Wu ${ }^{123}$


#### Abstract

Operator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism-a powerful tool originally designed for natural language processing-have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: Is there a more efficient attention mechanism for Transformerbased operator learning? This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Positionattention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.


[^0]
## 1. Introduction

Partial Differential Equations (PDEs) are essential in modeling a vast array of phenomena across various fields including physics, engineering, biology, and finance. They are the foundation for predicting and understanding many complex dynamics in natural and engineered systems. Over the past century, traditional numerical methods, such as finite element, finite difference, and spectral methods, have been well established for solving many PDEs. However, these methods often face challenges for complex nonlinear systems with complicated geometries or in high dimensions.

The advent of machine learning has shifted the paradigm in addressing these challenges in PDEs. Key developments include but are not limited to PINN (Raissi et al., 2019), Deep-Galerkin (Sirignano \& Spiliopoulos, 2018), and DeepRitz (Yu et al., 2018). These methods are typically solutionlearners, i.e., learning a solution of PDEs. They closely resemble traditional approaches such as finite elements, replacing local basis functions with neural networks. While advantageous for high-dimensional problems and complex geometries, solution-learners are usually limited to a single instance, i.e., solving one solution of the PDEs with a fixed initial/boundary condition. To get solution for every new condition, retraining a new neural network is required, which can be very costly.

Solution-learners usually focus on a given PDE. However, for many complex systems the governing equations remain unclear due to uncertain mechanisms, yet identifying underlying PDEs is very challenging without sufficient domain knowledge. Data-driven approaches have emerged as a powerful tool for discovering unknown PDEs or surrogate modeling of known yet complex PDEs. Early strategies include sparse-promoting regression (Rudy et al., 2017; Schaeffer, 2017); however, even after learning the underlying PDEs, numerical solving is still necessary to obtain their solutions. Other techniques, such as PDE-Net (Long et al., 2018; 2019), can learn both the underlying PDE and its solution dynamics.

Another data-driven approach is Flow Map Learning (FML) (Qin et al., 2019; Wu \& Xiu, 2020; Chen et al., 2022b; Churchill \& Xiu, 2023). Unlike solution-learners, FML is an
operator-learner, which approximates the evolution operator of time-dependent systems with varying initial conditions. Once learned, the flow map or evolution operator can be recursively utilized to predict long-term solution behaviors of the equations for any new initial condition without the need for retraining.

Recently, more versatile operator-learners have been systematically developed for learning mappings between infinitedimensional function spaces. Existing frameworks include, but are not limited to, the neural operators (Anandkumar et al., 2020; Li et al., 2020; 2021; Kovachki et al., 2023), DeepONets (Lu et al., 2021b; Jin et al., 2022; Lanthaler et al., 2023), principal component analysis-based methods (Bhattacharya et al., 2021), and attention-based methods (Cao, 2021; Kissas et al., 2022; Hao et al., 2023), etc. These operator-learners are applicable to learn the solution operators of parametric PDEs, including mappings from initial conditions to solutions at specific future times, or from boundary conditions, source terms, and model parameters to steady-state solutions.

Transformers, a powerful tool initially designed for natural language processing (Vaswani et al., 2017), have also been adapted for learning operators in PDEs, e.g., Liu et al. (2022); Hao et al. (2023); Xiao et al. (2023). The core of Transformers is the self-attention mechanism. However, conventional self-attention lacks positional awareness, which is found crucial in natural language processing (Vaswani et al., 2017; Shaw et al., 2018) and graph representation (Dwivedi \& Bresson, 2020), thus sparking significant research interest (Dai et al., 2019; Dufter et al., 2022). In PDE operator learning, there exist few studies on integrating positional knowledge with self-attention. Cao (2021); Lee (2022) concatenate the coordinates of sampling points with input function values, while Li et al. (2022b) adopt the rotary position embedding technique (Su et al., 2024) to enhance self-attention. Self-attention in operator learning is content-based and relies heavily on the values of input functions. This necessitates distinct attention calculations for each training batch instance, resulting in significant memory usage and high computational costs, especially when compared to the neural operators in Kovachki et al. (2023). This raises critical questions: Is self-attention indispensable for Transformer-based operator learning? What key positional information is necessary, and how can it be efficiently encoded into Transformer-based neural operators?

To overcome the challenges of self-attention in operator learning, we propose a novel attention mechanism, termed position-attention, from a numerical mathematics perspective. This mechanism is induced by only spatial relations without relying on input function values, marking a significant difference from classical content-based self-attention. Position-attention greatly enhances computational efficiency and effectively integrates positional knowledge. It also resonates with the principles of numerically solving PDEs, offering an interpretable approach to operator learning. Building upon position-attention and its variants, we develop a novel deep learning architecture, termed Position-induced Transformer (PiT), for operator learning. Compared to current state-of-the-art neural operators, PiT exhibits superior performance across various benchmarks from elliptic to hyperbolic PDEs, even in challenging cases where the solutions contain discontinuities. Like many neural operators (Azizzadenesheli et al., 2024), PiT features a remarkable discretization convergence property (also called discretization/mesh invariance in the literature (Kovachki et al., 2023; Li et al., 2021)), enabling effective generalization to new meshes which are unseen during training.

The main contributions of this work include:

- We find the importance of positional knowledge, specifically the spatial interrelations of the nodal points where the input functions are sampled, in operator learning. We propose the novel position-attention mechanism and its two variants to effectively incorporate such positional knowledge. Compared to self-attention, position-attention is interpretable from a numerical mathematics perspective and is more efficient for operator learning.
- Based on position-attention and its two variants, we construct PiT, a lightweight Transformer whose training time scales only sub-linearly with the sampling mesh resolution of input/output functions. Moreover, PiT is discretizationconvergent, offering consistent and convergent predictions as the testing meshes are refined.
- We conduct numerical experiments on various PDE benchmarks, showcasing the remarkable performance of PiT, and demonstrate its greater robustness in discretization convergence (with $48 \%$ smaller prediction error for the Darcy2D benchmark) compared to the Fourier neural operator (FNO). Our code is accessible at github.com/junfeng-chen/position_ induced_transformer.


## 2. Approach

### 2.1. Preliminaries

Operator Learning. Consider generic parametric PDEs:

$$
\begin{equation*}
\mathcal{L}_{a} u=f \tag{1}
\end{equation*}
$$

where $a \in \mathcal{A}\left(\Omega_{a} ; \mathbb{R}^{d_{a}}\right), u \in \mathcal{U}\left(\Omega_{u} ; \mathbb{R}^{d_{u}}\right)$ are functions defined on the bounded domains $\Omega_{a}$ and $\Omega_{u}$, respectively; $\mathcal{L}_{a}: \mathcal{U} \rightarrow \mathcal{F}$ is a partial differential operator; $f \in \mathcal{F} ; \mathcal{A}$ and $\mathcal{U}$ are Banach spaces of functions over $\Omega_{a}$ and $\Omega_{u}$, respectively. As in Li et al. (2021); Anandkumar et al.

![](https://cdn.mathpix.com/cropped/2024_06_04_2b1a6f543a58f6f59706g-03.jpg?height=271&width=268&top_left_y=222&top_left_x=278)

Training mesh resolution

![](https://cdn.mathpix.com/cropped/2024_06_04_2b1a6f543a58f6f59706g-03.jpg?height=281&width=271&top_left_y=220&top_left_x=645)

Testing mesh resolution
Figure 1. Discretization convergence test for neural operators.

(2020); Li et al. (2020); Kovachki et al. (2023), we assume $\Omega_{a}=\Omega_{u}=\Omega \subset \mathbb{R}^{d}$ in this paper. Denote the operator that maps $a$ to $u$ by $\Phi$, which can be the evolution operator that maps the initial condition to the solution at a specific future time, or the solution operator that maps the source term or model parameters to the steady-state solution. Operator learning aims to construct a neural operator $\Phi_{\theta}$, as surrogate model of $\Phi$, from sampling data pairs $\left\{a^{j}, u^{j}\right\}_{j=1}^{J}$. The data are usually sampled on two (possibly different) meshes $X_{a}=\left\{x_{i}\right\}_{i=1}^{N_{a}} \subset \Omega$ and $X_{u}=\left\{\hat{x}_{i}\right\}_{i=1}^{N_{u}} \subset \Omega$ :

$a^{j}=\left\{a^{j}\left(x_{i}\right)\right\}_{i=1}^{N_{a}}, \quad u^{j}=\left\{u^{j}\left(\hat{x}_{i}\right)\right\}_{i=1}^{N_{u}}, \quad j=1,2, \ldots, J$.

Assume that the input data $\left\{a^{j}\right\}_{j=1}^{J}$ are drawn from a probability measure $\mu_{\mathcal{A}}$ supported on $\mathcal{A}$, and the sampling points $X_{a}$ are i.i.d. drawn from a measure $\mu_{\Omega}$ on $\Omega$, denoted as $X_{a} \sim \mu_{\Omega}$. After training the parameters $\theta$ on the data pairs $\left\{a^{j}, u^{j}\right\}_{j=1}^{J}$, we expect that the trained neural operator $\Phi_{\boldsymbol{\theta}}$ exhibits small generalization error defined as

$$
\begin{equation*}
\mathbb{E}_{a \sim \mu_{\mathcal{A}}}\left(\left\|u-\Phi_{\boldsymbol{\theta}}\left(\left.a\right|_{X}\right)\right\|_{\mathcal{U}}^{2}\right), \quad \forall X \sim \mu_{\Omega} \tag{2}
\end{equation*}
$$

where the norm $\|\cdot\|_{\mathcal{U}}$ is in practice replaced with a vector norm of the output function values queried on a new mesh $X_{\text {new }}$. The formulation (2) indicates that the learned operator $\Phi_{\boldsymbol{\theta}}$ can accept any mesh points in the domain of $a$ and predict $u$ at any queried mesh $X_{\text {new }}$.

It is often desirable to achieve a reliable neural operator that is trained with merely inexpensive data on a coarse mesh and yet generalizes well to finer meshes without the need for retraining, as illustrated in Figure 1. In particular, one expects consistent and convergent predictions as the testing meshes are refined (Azizzadenesheli et al., 2024). A common method for assessing this is the zero-shot superresolution evaluation.

A neural operator typically adopts an Encoder-ProcessorDecoder architecture:

$$
\Phi_{\theta}=\text { Decoder } \circ \mathrm{LAYER}_{L} \circ \cdots \circ \mathrm{LAYER}_{1} \circ \text { Encoder }
$$

where the Encoder lifts the input function from $\mathbb{R}^{d_{a}}$ to a higher-dimensional feature space $\mathbb{R}^{d_{1}}$, and the Decoder projects the hidden features from $\mathbb{R}^{d_{L}}$ to $\mathbb{R}^{d_{u}}$. The Encoder and Decoder are usually implemented by linear layers, and can include nonlinearities if necessary. In the Processor, let the function $v_{\ell}: \Omega \rightarrow \mathbb{R}^{d_{\ell}}$ be the continuum of the feature map $U_{\ell}$ of $\mathrm{LAYER}_{\ell}$. Then, the forward pass $U_{\ell+1}=\operatorname{LAYER}_{\ell+1}\left(U_{\ell}\right)$ approximates the transform

$v_{\ell+1}(x)=\sigma\left(\int_{\Omega} \kappa_{\ell}\left(x, y, v_{\ell}(x), v_{\ell}(y)\right) v_{\ell}(y) d y+v_{\ell}(x) W_{\ell}\right)$, where the integral kernel $\kappa_{\ell}$ needs to be parametrized and trained, $W_{\ell} \in \mathbb{R}^{d_{\ell} \times d_{\ell+1}}$ is a trainable matrix, and $\sigma$ is a nonlinear activation function. Various parametrization methods for $\kappa_{\ell}$ have been explored, including but not limited to message passing on graphs (Anandkumar et al., 2020; Li et al., 2020), Fourier transform (Li et al., 2021), and multiwavelet transform (Gupta et al., 2021).

Transformer and Self-attention. Transformers, proposed by Vaswani et al. (2017), are fundamental in natural language processing and form the basis of major advanced language models including GPT and BERT. Their essence lies in the self-attention mechanism. Recently, Kovachki et al. (2023) observed the connections between attention and neural operators, highlighting the potential of Transformers in operator learning. Various specialized and effective Transformers have been developed for operator learning, using Galerkin-type attention (Cao, 2021), hierarchical attention (Liu et al., 2022), cross-attention (Lee, 2022; Li et al., 2022b), mixture of experts (Hao et al., 2023), and orthogonal regularization (Xiao et al., 2023).

Consider $U \in \mathbb{R}^{N_{v} \times d_{\ell}}$ as the input sequence comprising $N_{v}$ elements, each represented by a $d_{\ell}$-dimensional feature vector. Self-attention can be expressed as

$$
\begin{equation*}
\operatorname{SelfAtt}(U)=\operatorname{Softmax}\left(\frac{U W^{Q}\left(U W^{K}\right)^{T}}{\sqrt{d_{\ell+1}}}\right) U W^{V} \tag{3}
\end{equation*}
$$

where $W^{Q}, W^{K}, W^{V} \in \mathbb{R}^{d_{\ell} \times d_{\ell+1}}$ are trainable matrices. Self-attention is content-based, and it heavily depends on input function values in operator learning. This demands separate attention computations for each training batch instance, leading to intensive memory and computational costs, compared to the neural operators in Kovachki et al. (2023).

### 2.2. Novel Position-attention and Its Variants

We find the positional knowledge, specifically the spatial interrelations of the sampling points, is essential for operator learning. We propose the position-attention mechanism and its two variants, which effectively incorporate such positional knowledge. In contrast to classical self-attention, position-attention does not rely on the input function values themselves, thereby greatly boosting efficiency. Furthermore, position-attention is consistent with the changes of mesh resolution and converges as the meshes are refined.

Position-attention. Let $U \in \mathbb{R}^{N_{v} \times d_{\ell}}$ be the values of a generic function $v$ sampled on a mesh $X_{v}$ of $N_{v}$ nodal
![](https://cdn.mathpix.com/cropped/2024_06_04_2b1a6f543a58f6f59706g-04.jpg?height=472&width=506&top_left_y=234&top_left_x=272)

![](https://cdn.mathpix.com/cropped/2024_06_04_2b1a6f543a58f6f59706g-04.jpg?height=485&width=900&top_left_y=229&top_left_x=840)

Figure 2. Overview of Position-induced Transformer for operator learning. Top left: A trained neural operator can serve as a surrogate model to specific parametric PDEs. Bottom left: Cross position-attention provides learnable downsampling/unsampling between meshes at different resolutions, and local position-attention supports customizable receptive field. Right: The Encoder-Processor-Decoder architecture of PiT.

points. Define the pairwise-distance matrix $D \in \mathbb{R}^{N_{v} \times N_{v}}$ :

$$
\begin{equation*}
D_{i j}=\left\|x_{i}-x_{j}\right\|_{2}^{2} \tag{4}
\end{equation*}
$$

Let $\lambda>0$ and $W^{V} \in \mathbb{R}^{d_{\ell} \times d_{\ell+1}}$ be trainable parameters. The position-attention mechanism is defined by

$$
\begin{equation*}
\operatorname{PosAtt}(U ; D):=\operatorname{Softmax}(-\lambda D) U W^{V} \tag{5}
\end{equation*}
$$

which is linear with respect to $U$. Here, $\operatorname{Softmax}(-\lambda D) U$ can be understood as a global linear convolution, with the kernel weights adjusted according to the relative distances between sampling points. This design is motivated by the concept of domain of dependence in PDEs, reflecting how the solution at a point is influenced by the local neighboring information. Specifically, the $i$ th row of the output is

$$
\begin{equation*}
\operatorname{PosAtt}(U ; D)_{i}=\sum_{k=1}^{N_{v}} \frac{\exp \left(-\lambda D_{i k}\right)}{\sum_{j=1}^{N_{v}} \exp \left(-\lambda D_{i j}\right)}\left(U W^{V}\right)_{k} \tag{6}
\end{equation*}
$$

Theorem 2.1. Let $\left\{X_{n}\right\}_{n=1}^{+\infty}$ be a sequence of refined meshes on $\Omega$ with $X_{n} \sim \mu_{\Omega}$. Denote by $D^{n}$ the pairwisedistance matrix (4) corresponding to $X_{n}$. Assume that $v(x)$ is bounded on $\Omega$, and denote by $U^{n}$ the function values of $v$ on $X_{n}$. As $n \rightarrow+\infty$, the position-attention (5) converges to an integral operator: specifically, for any $\varepsilon>0$,

$$
\lim _{n \rightarrow+\infty} \operatorname{Pr}\left\{\frac{1}{\left|X_{n}\right|}\left\|\operatorname{PosAtt}\left(U^{n} ; D^{n}\right)-\left.\mathcal{F}\right|_{X_{n}}\right\| \leq \varepsilon\right\}=1
$$

where $\left|X_{n}\right|$ denotes the number of nodal points in $X_{n}$,

$$
\begin{equation*}
\mathcal{F}(x):=\int_{\Omega} \kappa^{\lambda}(x-y) v(y) W^{V} d \mu_{\Omega}(y) \tag{7}
\end{equation*}
$$

and

$$
\begin{equation*}
\kappa^{\lambda}(x-y)=\frac{\exp \left(-\lambda\|x-y\|_{2}^{2}\right)}{\int_{\Omega} \exp \left(-\lambda\left\|x-y^{\prime}\right\|_{2}^{2}\right) d \mu_{\Omega}\left(y^{\prime}\right)} \tag{8}
\end{equation*}
$$

is the integral kernel induced by position-attention.
The proof of Theorem 2.1 is put in Appendix B. The fixed measure $\mu_{\Omega}$ and the row-wise Softmax normalization play crucial roles in the convergence, which implies that positionattention is discretization-convergent, eliminating the need for nested mesh refinement as required in Kovachki et al. (2023). If one replaces the Softmax normalization with element-wise exponentiation, then $\kappa^{\lambda}$ becomes a Gaussian kernel, which is, however, sensitive to mesh resolution.

The kernel (8) induced by position-attention is independent of the input functions. This is a notable difference from classical self-attention, which is content-based and heavily relies on the input function values themselves. Indeed, positionattention draws inspiration from numerical schemes solving PDEs. For instance, consider the upwind scheme for the advection equation $v_{t}+s v_{x}=0$ with a constant speed $s$ :

$$
\begin{aligned}
v_{j}^{n+1} & =v_{j}^{n}-\frac{c}{2}\left(v_{j+1}^{n}-v_{j-1}^{n}\right)+\frac{|c|}{2}\left(v_{j+1}^{n}-2 v_{j}^{n}+v_{j-1}^{n}\right) \\
& =: H_{c}\left(v_{j-1}^{n}, v_{j}^{n}, v_{j+1}^{n}\right)
\end{aligned}
$$

where $v_{j}^{n}$ is the numerical solution at the $j$ th grid point and time $t_{n}$. Here, the operator $H_{c}$ is discretization-convergent, depending only on a fixed Courant-Friedrichs-Lewy (CFL) number $c:=s \Delta t / \Delta x$, and is independent of the input function values $\left\{v_{j-1}^{n}, v_{j}^{n}, v_{j+1}^{n}\right\}$. This scheme can be interpreted as a local linear convolution. Position-attention shares a similar concept but employs a global linear convolution, with the kernel reflecting a stronger dependence on local neighboring regions. Indeed, the value of $\lambda$ in positionattention is interpretable, as most attention at a queried point $x$ is directed towards points $y$ with the distance to $x$ smaller than $1 / \sqrt{\lambda}$; see Appendix D for detailed discussions.

Cross Position-attention. We further propose a novel interpretable variant, cross position-attention, for downsampling/upsampling unstructured data. It interpolates $U$ from
a mesh $X_{1}$ onto another mesh $X_{2}$ by

$$
\begin{equation*}
\operatorname{CroPosAtt}(U ; D):=\operatorname{Softmax}\left(-\lambda D_{1 \rightarrow 2}^{T}\right) U W^{V} \tag{9}
\end{equation*}
$$

where $D_{1 \rightarrow 2}$ is the pairwise-distance matrix between $X_{1}$ and $X_{2}$. As $X_{1}$ is refined, cross position-attention also approximates the integral operator defined in equation (7). This property allows us to construct a discretizationconvergent Encoder that downsamples the input function values on any mesh $X_{a}$ to a pre-fixed coarser latent mesh $X_{v}$, on which the Processor is inexpensive. Analogously, a discretization-convergent Decoder can be constructed to upsample the processed features onto any output mesh $X_{u}$. Remark 2.2. While $X_{a}, X_{v} \sim \mu_{\Omega}$ are important for discretization convergence, we do not require any structure in the output mesh $X_{u}$. The output function values can be queried at any point in the domain $\Omega$. The whole model architecture and computational complexity will be detailed in Section 2.3.

## Local Position-attention.

The position-attention mechanism naturally captures global dependencies. However, local patterns are often crucial in the solutions of various PDEs, especially those of hyperbolic nature or dominated by convection. To address this, we introduce a local variant of position-attention:

$\operatorname{LocPosAtt}(U ; D)_{i}=\sum_{D_{i k} \leq r_{i}^{2}} \frac{\exp \left(-\lambda D_{i k}\right)}{\sum_{D_{i j} \leq r_{i}^{2}} \exp \left(-\lambda D_{i j}\right)}\left(U W^{V}\right)_{k}$

Similar to Theorem 2.1, as the mesh is refined, local position-attention approximates the integral operator

$$
\int_{B_{r_{i}}\left(x_{i}\right)} \kappa_{r_{x_{i}}}^{\lambda}\left(x_{i}-y\right) u(y) W^{V} \mu_{\Omega}(d y)
$$

with the induced compact kernel

$$
\begin{equation*}
k_{r_{x}}^{\lambda}(x-y)=\frac{\exp \left(-\lambda\|x-y\|_{2}^{2}\right)}{\int_{B_{r_{x}}(x)} \exp \left(-\lambda\left\|x-y^{\prime}\right\|_{2}^{2}\right) d \mu_{\Omega}\left(y^{\prime}\right)} \tag{11}
\end{equation*}
$$

where $B_{r_{x}}(x)$, usually termed receptive field, is a ball with radius $r_{x}$ and center $x$. We take the radius $r_{x}$ as a quantile of the row values in the pairwise-distance matrix, adapting the receptive field to the local density of nodal points. This design enables local position-attention to effectively handle functions exhibiting multiscale features. The value of quantile is left as a hyperparameter; see Section 4.6.

### 2.3. Position-induced Transformer

We now design our novel Transformer architecture, PiT, which is primarily composed of the proposed global, cross, and local position-attention mechanisms for mixing features over the domain $\Omega$. A sketch of the PiT architecture is depicted in Figure 2.

Encoder. The Encoder comprises lifting and downsampling operations using both local and cross position-attention mechanisms:

$$
\text { Encoder }=\sigma \circ \operatorname{LocPosAtt}_{\text {in }}\left(\cdot ; D_{a \rightarrow v}^{T}\right) \circ \sigma \circ \text { LINEAR, }
$$

where $D_{a \rightarrow v}$ is the pairwise-distance matrix between the input and latent meshes, $X_{a}$ and $X_{v}$; LINEAR refers to a fully connected layer applied row-wisely to the feature matrix. This design allows us to embed the inputs on a coarse mesh into a higher-dimensional feature space, while the local position-attention effectively extracts the local features of the inputs. The dimension $d_{v}$ of the lifted features is termed the encoding dimension, which is an important hyperparameter for the model's expressive capacity.

Processor. The Processor consists of a sequence of global position-attention modules. To address the nonlinearity in general operators, we propose the following module as the building block to construct the Processor:

$$
\begin{aligned}
& h_{\ell}=\sigma\left(\operatorname{PosAtt}_{\ell}\left(U_{\ell-1} ; D_{v}\right)\right) \\
& U_{\ell}=\sigma\left(\operatorname{MLP}_{\ell}(h)+\operatorname{LINEAR}_{\ell}\left(U_{\ell-1}\right)\right)
\end{aligned}
$$

where $D_{v}$ is the pairwise-distance matrix of $X_{v} ; U_{0}$ is the output of Encoder; MLP ${ }_{\ell}$ refers to a multilayer perceptron applied row-wisely to $h_{\ell}$. Throughout our experiments, we stack four global attention modules $(L=4)$ in the Processor with two layers in MLP, and take $d_{\ell}=d_{v}$ for all $1 \leq \ell \leq L$.

Decoder. In the Decoder, we firstly upsample the feature map $U_{L}$ from the Processor to the queried nodal points $X_{u}$, and then apply an MLP row-wisely to project the features back to the range space of the output functions.

$$
\text { Decoder }=\text { MLP } \circ \sigma \circ \operatorname{LocPosAtt} \text { out }\left(\cdot ; D_{u \rightarrow v}^{T}\right)
$$

## High Efficiency: Linear Computational Complexity.

 Due to the kernel matrix multiplication, the forward computation of global position-attention has a quadratic complexity of $O\left(N_{v}^{2}\right)$. To accelerate large-scale operator learning tasks, we adopt a downsampling-processing-upsampling network architecture. Denote the numbers of nodal points in the meshes $X_{a}, X_{v}$, and $X_{u}$ by $N_{a}, N_{v}$, and $N_{u}$, respectively. We take $N_{v}$ relatively small for efficiency. The computational complexities in the Encoder and Decoder are $O\left(N_{a} N_{v} d_{v}+N_{a} d_{v}^{2}\right)$ and $O\left(N_{u} N_{v} d_{v}+N_{u} d_{v}^{2}\right)$, respectively, which are both linear to the numbers of input and output mesh points. This is confirmed by our experiments, where we observe that the training time of PiT scales only sub-linearly with $N_{a}$ and $N_{u}$ (see Appendix F.3).We treat $N_{v}$ as a hyperparameter of PiT for balancing computational efficiency and information retention on the coarse
latent mesh; see Section 4.6. For training data on structured meshes, we obtain a coarser latent mesh via pooling. For unstructured data, if the distribution is known, one can generate an appropriate latent mesh by sampling; if unknown, one can use farthest point sampling (Zhou et al., 2018) to preserve spatial distribution in the latent mesh.

## 3. Related Work

Operator Learning. This area is actively researched with numerous related contributions. Chen \& Chen (1995) established a universal approximation theorem for approximating nonlinear operators using neural networks. Motivated by this theorem, the DeepONet framework (Lu et al., 2021b), comprising a trunk-net and a branch-net, was proposed for operator learning. The branch-net inputs discretized function values, while the trunk-net inputs coordinates in the domain of the output function. They combine to predict the output function values at specified coordinates. This framework has motivated various extensions, e.g., Jin et al. (2022), Seidman et al. (2022), Lanthaler et al. (2023), Lee et al. (2023), and Patel et al. (2024), etc.

Another pioneering framework is the neural operators based on iterative kernel integration (Anandkumar et al., 2020; Li et al., 2021; Kovachki et al., 2023). Unlike the trunkbranch architecture in DeepONet, this framework typically relies on composing linear integral operators with nonlinear activation functions. The graph neural operator (Anandkumar et al., 2020) leverages message passing to approximate linear kernels in the form $\kappa(x, y)$. FNO (Li et al., 2021) is related to a shift-invariant kernel $\kappa(x-y)$, facilitating operator learning in a frequency domain via discrete Fourier transform. This renders FNO efficient for problems with periodic features. As FNO is limited to uniformly distributed data, various new variants have emerged to handle more complex data structures and geometries, e.g, GeoFNO (Li et al., 2022a), the non-equispaced Fourier solver (Lin et al., 2022), and the Vandermonde neural operator (Lingsch et al., 2023). Researchers have also developed other related approaches for learning operators in frequency or modal spaces with generalized Fourier projections ( $\mathrm{Wu}$ \& Xiu, 2020), multi-wavelet basis (Gupta et al., 2021), and Laplacian eigenfunctions (Chen et al., 2023).

Transformer-based Neural Operators. Recently, Transformers have been extended to operator learning, including Galerkin and Fourier Transformers (Cao, 2021), HT-net (Liu et al., 2022), MINO (Lee, 2022), OFormer (Li et al., 2022b), GNOT (Hao et al., 2023), and ONO (Xiao et al., 2023). These Transformers are built on self-attention, which relies on the input function values to compute attention weights. This results in distinct attention calculations for each training batch instance, making the Transformer-based neural operators computationally expensive. In contrast, position- attention only rely on the pre-defined pairwise-distance matrix of the sampling points and does not depend on the input function values. This new mechanism notably reduces memory usage and accelerates training.

Cross position-attention, which downsamples the input functions onto coarse latent meshes, also contributes to the high efficiency of PiT. Related ideas include content-based crossattention used in MINO (Lee, 2022), and bilinear interpolation employed by Galerkin Transformer (Cao, 2021). PiT combines the advantages of both, simultaneously possessing the applicability to irregular point clouds, similar to MINO, and the interpretability, akin to the interpolation in Galerkin Transformer. There are also many other efforts reducing the computational costs of Transformers, such as random feature approximation (Choromanski et al., 2020; Peng et al., 2021), low-rank approximation (Lu et al., 2021a; Xiong et al., 2021), Softmax-free normalization (Cao, 2021), linear cross-attention (Li et al., 2022b; Hao et al., 2023), etc. These techniques may potentially be combined with position-attention to further enhance its efficiency.

Positional/Structural Encoding in Transformers. Transformers, following their success in large language models, have found broad applications in fields such as imaging (Dosovitskiy et al., 2020) and graph modeling (Veličković et al., 2018; Yun et al., 2019). In these models, self-attention is content-based and requires positional encoding. Position information typically falls into two categories: absolute and relative. Vaswani et al. (2017) used sinusoidal functions to encode the absolute positions of words in a sentence. In contrast, Yang et al. (2018) and Guo et al. (2019) focused on the localness of text by adjusting self-attention scores based on word distances. Trainable relative positional encoding was proposed by Shaw et al. (2018); Dai et al. (2019). For graph applications, topological information is as important as position. Structural and positional information in graphs is represented by the graph's Laplacian spectrum (Dwivedi \& Bresson, 2020; Kreuzer et al., 2021), shortest-path distance (Ying et al., 2021), and kernel-based sub-graph (Mialon et al., 2021; Chen et al., 2022a), etc.

## 4. Numerical Experiments

This section presents the experimental results from a variety of PDE benchmarks, demonstrating the superior performance of PiT compared to many other operator learning methods. We also validate the discretization convergence property of PiT in Section 4.3. Section 4.4 presents rigorous comparative studies between self-attention and positionattention. In Section 4.5, we provide some insights on combining self-attention and position-attention. The impacts of hyperparameters are explored in Section 4.6. More experimental results are presented in Appendix F.

### 4.1. Benchmarks and Baselines

Our tests encompass a diverse range of operator learning benchmarks: InviscidBurgers (Lanthaler et al., 2023), ShockTube (Lanthaler et al., 2023), Darcy2D (Li et al., 2021), Vorticity (Li et al., 2021), Elasticity (Li et al., 2022a), and NACA (Li et al., 2022a). These problems cover elliptic, parabolic, and hyperbolic PDEs, including challenging equations whose solutions exhibit discontinuities. Data for these problems are collected on either structured meshes or irregular point clouds. Due to page limitations, we put the detailed setups of these problems in Appendix A.

We compare PiT with various strong baselines in operator learning: DeepONet (Lu et al., 2021b); shift-DeepONet (Lanthaler et al., 2023); FNO (Li et al., 2021) and FNO++, the newest implementation (NeuralOperator, 2023) of FNO using GELU activation and a two-layer MLP after each Fourier layer; Geo-FNO (Li et al., 2022a); Galerkin Transformer (Cao, 2021); OFormer (Li et al., 2022b); GNOT (Hao et al., 2023); ONO (Xiao et al., 2023). The latter four baselines are all Transformer-based neural operators. Besides the results presented in the following sections, we put more comprehensive comparisons between PiT and the baselines in the appendices; see parameter counts in Appendix E.3; see training speed and memory usage in Appendix E.4.

### 4.2. Main Results

Table 1 presents the prediction errors of our method and the nine baselines for the six benchmarks. The results for the baselines are directly cited from those original papers, if applicable, and are marked as "-" otherwise. The results of FNO++ are produced using the network hyper-parameters suggested in the references (Li et al., 2021; Lanthaler et al., 2023). Details about the network architectures and training configurations can be found in Appendix E.

In InviscidBurgers and ShockTube, PiT exhibits excellent performance, comparable to FNO/FNO++, and significant superiority over DeepONet and shift-DeepONet. Both tasks pose notable challenges due to the discontinuous target functions in the solution operators (see Figures 5 and 6), which are inherently difficult for neural networks to learn. As Lanthaler et al. (2023) pointed out, DeepONet fails to effectively address such difficulties, while shift-DeepONet enhances the performance by incorporating shift-net. PiT overcomes the challenges thanks to its nonlinear Transformer architecture with position-attention. In the InviscidBurgers benchmark, PiT's prediction error is remarkably lower, at just $17 \%$ of shift-DeepONet's error and a mere $4.8 \%$ of DeepONet's error. In the ShockTube benchmark, PiT's prediction error is about $45 \%$ of shift-DeepONet's and $29 \%$ of DeepONet's.

Furthermore, in both the Darcy2D and Vorticity benchmarks, PiT achieves the lowest prediction errors, outperforming all tested baselines. In the Darcy2D task, PiT's prediction error is only $38 \%$ to $57 \%$ of those of OFormer, FNO, and Galerkin Transformer. PiT also demonstrates the best performance in the Vorticity benchmark, representing a challenging operator learning task due to data scarcity and the complex patterns of turbulent flow. These results indicate that leveraging spatial interrelations of mesh points is highly beneficial for the attention mechanism in learning these operators.

In the Elasticity and NACA tasks, the PDEs are defined in irregular domains with complex geometries, and the data are sampled on unstructured point clouds for Elasticity and on a deformed mesh for NACA. The complexity of the data and geometry presents significant challenges for operator learning. Again, PiT, with its position-attention mechanism, exhibits superior accuracy over all the baselines, including the other four Transformers (OFormer, Galerkin Transformer, GNOT, and ONO) based on self-attention. This suggests that position-attention is crucial, while self-attention might be unnecessary for Transformer-based operator learning.

In addition to its outstanding accuracy, PiT also exhibits high efficiency in terms of training costs. For example, Table 16 displays the training times of PiT with data on various mesh resolutions. These results validate PiT's sub-linear scaling of training time with mesh resolution, consistent with our computational complexity analysis in Section 2.3.

### 4.3. Discretization Convergence Tests

The Darcy2D dataset, originally collected on a $421^{2}$ Cartesian grid, is downscaled onto a sequence of coarser meshes to serve as reference solutions. This enables the assessment of PiT's discretization convergence via zero-shot superresolution evaluation. To this end, we train neural operators with data on a coarse mesh. After training, the learned operators are tested on a sequence of refined meshes: $43^{2}, 61^{2}$, $71^{2}, 85^{2}, 106^{2}, 141^{2}, 211^{2}, 421^{2}$, respectively. The testing errors on these meshes are illustrated in Figure 3, where PiT and FNO++ trained on the $43^{2}$ mesh (resp. the $85^{2}$ mesh) are denoted as PiT43 and FNO++43 (resp. PiT85 and $\mathrm{FNO}++85$.

![](https://cdn.mathpix.com/cropped/2024_06_04_2b1a6f543a58f6f59706g-07.jpg?height=453&width=574&top_left_y=1950&top_left_x=1163)

Figure 3. Discretization convergence tests on Darcy2D.

Table 1. Relative errors on the test sets of six benchmarks. The results of InviscidBurgers and ShockTube are reported with the relative $l_{1}$ errors. Other benchmarks are evaluated with the relative $l_{2}$ errors. The best result of each task is bolded, and the second best result is underlined. The data of Darcy2D are represented on a $211 \times 211$ uniform grid. The results of Galerkin Transformer and OFormer for Elasticity and NACA are cited from Hao et al. (2023). The results of FNO for Elasticity and NACA are cited from Li et al. (2022a).

|  | InviscidBurgers | ShockTube | Darcy2D | Vorticity | Elasticity | NACA |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| DeepONet | 0.285 | 0.0422 | - | - | - | - |
| shift-DeepONet | 0.0783 | 0.0276 | - | - | - | - |
| FNO++ | $\mathbf{0 . 0 0 9 9 5}$ | 0.0194 | $\underline{0.00509}$ | 0.1315 | - | - |
| FNO | 0.0157 | $\underline{0.0156}$ | 0.0109 | 0.1559 | 0.0508 | 0.0421 |
| OFormer | - | - | 0.0128 | 0.1755 | 0.0183 | 0.0183 |
| Galerkin Transformer | - | - | 0.00844 | 0.1399 | 0.0201 | 0.0161 |
| GNOT | - | - | - | 0.138 | $\underline{0.00865}$ | 0.00757 |
| ONO | - | - | - | $\underline{0.1195}$ | 0.0118 | $\underline{0.0056}$ |
| Geo-FNO | - | - | - | - | 0.0229 | 0.0138 |
| PiT | $\underline{0.0136}$ | $\mathbf{0 . 0 1 2 2}$ | $\mathbf{0 . 0 0 4 8 5}$ | $\mathbf{0 . 1 1 4 0}$ | $\mathbf{0 . 0 0 6 4 9}$ | $\mathbf{0 . 0 0 4 8 0}$ |

As seen from Figure 3, for operator learning on the $43^{2}$ mesh, FNO++'s prediction error surges from $0.95 \%$ to $8.67 \%$ as the testing mesh resolution increases to $421^{2}$, while PiT's prediction error rises from $0.97 \%$ to only $4.50 \%$ (which is $48 \%$ lower than that of FNO++). The greater robustness and accuracy of PiT compared to FNO++ are also observed in Figure 3 for the operators trained with $85^{2}$ mesh data. These results demonstrate PiT's superiority over FNO++ in terms of discretization convergence.

### 4.4. Comparative Ablation Study

We have demonstrated that PiT with position-attention delivers superior performance compared to existing Transformerbased neural operators that utilize self-attention. This suggests that self-attention might not be necessary for operator learning. In this section, we provide a more rigorous ablation study to compare position-attention and vanilla selfattention for operator learning in PDEs. We test two vanilla self-attention models:

SelfAtt A: All PosAtt layers in PiT are replaced with the vanilla self-attention. Three out of six benchmarks encounter an "out of memory" (OOM) issue with a single 24GB RTX-3090 GPU.

SelfAtt B: Only PosAtt layers in Processor are replaced with self-attention, and this avoids the OOM issue.

Table 2 presents the testing errors, parameter counts, and training time. These results validate that PiT is consistently more accurate than Transformers built upon vanilla selfattention, without trade-offs in efficiency.

### 4.5. Can Self-attention Enhance PiT?

In this section, we aim to answer to such a question: Does combining self-attention and position-attention enhance
PiT? To address this, let us consider a Transformer, termed Self-PiT, based on a combined attention mechanism:

$$
\begin{align*}
& \operatorname{SelfPosAtt}(U ; D) \\
& =\operatorname{Softmax}\left(-\lambda D+\frac{U W^{Q}\left(U W^{K}\right)^{T}}{\sqrt{d_{\ell+1}}}\right) U W^{V} \tag{12}
\end{align*}
$$

We have tested Self-PiT on the InviscidBurgers and ShockTube benchmarks, for which the prediction errors are 0.00816 and 0.0179 , respectively. By comparing them with the results of PiT in Table 1, we conclude that Self-PiT does not consistently outperform PiT in terms of accuracy, yet it requires more computational complexities.

### 4.6. Hyperparameter Study

Figure 4 illustrates the impacts of the following three important hyperparameters in PiT.

Quantile in LocPosAtt. A smaller quantile means a more compact receptive field in local position-attention, resulting in a stronger focus on local features. The results in Figure 4 indicate that PiT's performance on ShockTube is sensitive to the quantile in the Encoder but not sensitive to the quantile in the Decoder. Using a small quantile in the Encoder is critical; otherwise, PiT may yield a large prediction error.

Latent Mesh Resolution $N_{v}$. This hyperparameter balances computational efficiency and information retention on coarse latent meshes. Choosing a relatively large $N_{v}$ is crucial to retain essential information in PiT's Processor. Figure 4 shows that the prediction error decreases as $N_{v}$ increases, as expected. However, this benefit diminishes rapidly as $N_{v}$ reaches 64 . On latent mesh of merely 64 points, PiT is efficient and sufficiently accurate for InviscidBurgers and ShockTube, even though the input data for these tasks is sampled on 1,024 and 2,048 grid points, respectively.

Table 2. Comparisons of position-attention and vanilla self-attention on all benchmark problems. The best result of each task is bolded.

|  | Model | InviscidBurgers | ShockTube | Darcy2D | Vorticity | Elasticity | NACA |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Testing errors | SelfAtt A | 0.016 | 0.0259 | OOM | OOM | 0.0295 | OOM |
|  | SelfAtt B | 0.0235 | 0.016 | 0.0072 | 0.156 | 0.169 | 0.0164 |
|  | PiT | $\mathbf{0 . 0 1 3 6}$ | $\mathbf{0 . 0 1 2 2}$ | $\mathbf{0 . 0 0 4 8 5}$ | $\mathbf{0 . 1 1 4}$ | $\mathbf{0 . 0 0 6 4 9}$ | $\mathbf{0 . 0 0 4 8}$ |
| Parameter counts | SelfAtt A | 152,833 | 152,961 | OOM | OOM | $9,732,609$ | OOM |
|  | SelfAtt B | 128,263 | 128,391 | 444,677 | $1,776,387$ | $8,684,049$ | $1,774,341$ |
|  | PiT | $\mathbf{9 5 , 5 0 3}$ | $\mathbf{9 5 , 6 3 1}$ | $\mathbf{3 1 3 , 6 1 3}$ | $\mathbf{1 , 2 5 2 , 1 0 3}$ | $\mathbf{6 , 5 8 6 , 9 2 9}$ | $\mathbf{1 , 2 5 0 , 0 6 1}$ |
| Training time | SelfAtt A | 1.73 | 5.51 | OOM | OOM | $\mathbf{7 . 1 3}$ | OOM |
|  | SelfAtt B | 1.47 | 1.73 | 15.3 | 18.7 | 9.30 | 21.1 |
|  | PiT | $\mathbf{0 . 9 3 8}$ | $\mathbf{1 . 0 4}$ | $\mathbf{1 4 . 7}$ | $\mathbf{1 6 . 3}$ | 7.69 | $\mathbf{1 5 . 3}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_2b1a6f543a58f6f59706g-09.jpg?height=392&width=1564&top_left_y=782&top_left_x=259)

Figure 4. Impacts of the three hyperparameters in PiT.

Encoding Dimension $d_{v}$ (Network Width). $d_{v}$ affects the model's expressive capacity. As expected, Figure 4 shows a consistent decrease in relative errors as $d_{v}$ increases. For Darcy2D, the PiT model with $d_{v}=32$ has only 20,000 trainable parameters, yet its prediction error is merely 0.00808 , which is already lower than the errors of FNO, OFormer, and Galerkin Transformer in Table 1. While the latter three methods all have over 2,000, 000 trainable parameters, PiT achieves superior performance with only 20,000 parameters, demonstrating its parsimonious nature.

## 5. Conclusions

Inspired by numerical mathematics, we propose positionattention (and two variants) and Position-induced Transformer (PiT) for operator learning in PDEs. PiT exhibits outstanding performance across various PDE benchmarks, surpassing many operator learning baselines. Notably, PiT is discretization-convergent, enabling effective generalization to new meshes with different resolutions. We conclude that the position-attention mechanism is highly efficient for learning nonlinear operators, even in challenging hyperbolic PDEs with discontinuous solutions. Unlike classical self-attention, our position-attention is induced solely by the spatial interrelations of sampling points, without relying on input function values. Position-attention greatly enhances computational efficiency and effectively integrates positional knowledge. Our results demonstrate that position- attention is crucial for operator learning, and positional knowledge is all you need.

## Impact Statement

PiT emerges as a versatile operator learning framework, applicable to both the surrogate modeling of known parametric PDEs and the data-driven learning of unknown PDEs. It may broadly influence various PDE-related fields such as physics, engineering, biology, and finance, marking an important advancement in $\mathrm{AI}$ for science.

Incorporating human insights, which encompass physical and numerical knowledge, is recognized as pivotal in datadriven modeling. Our work not only highlights this integration but also facilitates the development of new operator learning frameworks and the enhancement of existing neural operators. This fosters growth in the realm of scientific machine learning.

One possible negative impact relates to the computational cost of PiT for high-dimensional, large-scale PDEs. To retain essential information of the operators, the latent mesh in PiT may require a large number of nodal points, which notably increases the computational cost. This directs future research endeavors toward further enhancing the current position-attention framework through sparse approximations, low-rank approximations, or Softmax-free variants.

## Acknowledgements

This work was partially supported by National Natural Science Foundation of China (Grant No. 92370108) and Shenzhen Science and Technology Program (Grant No. RCJC20221008092757098).

## References

Anandkumar, A., Azizzadenesheli, K., Bhattacharya, K., Kovachki, N., Li, Z., Liu, B., and Stuart, A. Neural operator: Graph kernel network for partial differential equations. In International Conference on Learning Representations, 2020.

Azizzadenesheli, K., Kovachki, N., Li, Z., Liu-Schiaffini, M., Kossaifi, J., and Anandkumar, A. Neural operators for accelerating scientific simulations and design. Nature Reviews Physics, pp. 1-9, 2024.

Bhattacharya, K., Hosseini, B., Kovachki, N. B., and Stuart, A. M. Model reduction and neural networks for parametric pdes. The SMAI Journal of Computational Mathematics, 7:121-157, 2021.

Cao, S. Choose a Transformer: Fourier or Galerkin. Advances in Neural Information Processing Systems, 34: 24924-24940, 2021.

Chen, D., O'Bray, L., and Borgwardt, K. Structure-aware Transformer for graph representation learning. In International Conference on Machine Learning, pp. 3469-3489. PMLR, 2022a.

Chen, G., Liu, X., Li, Y., Meng, Q., and Chen, L. Laplace neural operator for complex geometries. arXiv preprint arXiv:2302.08166, 2023.

Chen, T. and Chen, H. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks, 6(4):911-917, 1995.

Chen, Z., Churchill, V., Wu, K., and Xiu, D. Deep neural network modeling of unknown partial differential equations in nodal space. Journal of Computational Physics, 449:110782, 2022b.

Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with Performers. In International Conference on Learning Representations, 2020.

Churchill, V. and Xiu, D. Flow map learning for unknown dynamical systems: overview, implementation, and benchmarks. Journal of Machine Learning for Modeling and Computing, 4(2), 2023.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q., and Salakhutdinov, R. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, 2019.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020 .

Dufter, P., Schmitt, M., and Schütze, H. Position information in Transformers: An overview. Computational Linguistics, 48(3):733-763, 2022.

Dwivedi, V. P. and Bresson, X. A generalization of Transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020.

Guo, M., Zhang, Y., and Liu, T. Gaussian Transformer: a lightweight approach for natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6489-6496, 2019.

Gupta, G., Xiao, X., and Bogdan, P. Multiwavelet-based operator learning for differential equations. Advances in Neural Information Processing Systems, 34:2404824062, 2021.

Hao, Z., Wang, Z., Su, H., Ying, C., Dong, Y., Liu, S., Cheng, Z., Song, J., and Zhu, J. GNOT: A general neural operator Transformer for operator learning. In International Conference on Machine Learning, pp. 1255612569. PMLR, 2023.

Jin, P., Meng, S., and Lu, L. MIONet: Learning multipleinput operators via tensor product. SIAM Journal on Scientific Computing, 44(6):A3490-A3514, 2022.

Kissas, G., Seidman, J. H., Guilhoto, L. F., Preciado, V. M., Pappas, G. J., and Perdikaris, P. Learning operators with coupled attention. Journal of Machine Learning Research, 23(1):9636-9698, 2022.

Kovachki, N. B., Li, Z., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A. M., and Anandkumar, A. Neural operator: Learning maps between function spaces with applications to PDEs. Journal of Machine Learning Research, 24(89):1-97, 2023.

Kreuzer, D., Beaini, D., Hamilton, W., Létourneau, V., and Tossou, P. Rethinking graph Transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618-21629, 2021.

Lanthaler, S., Molinaro, R., Hadorn, P., and Mishra, S. Nonlinear reconstruction for operator learning of PDEs with discontinuities. In International Conference on Learning Representations, 2023.

Lee, J. Y., CHO, S., and Hwang, H. J. HyperDeepONet: learning operator with complex target function space using the limited resources via hypernetwork. In The Eleventh International Conference on Learning Representations, 2023.

Lee, S. Mesh-independent operator learning for partial differential equations. In ICML 2022 2nd AI for Science Workshop, 2022.

Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Stuart, A., Bhattacharya, K., and Anandkumar, A. Multipole graph neural operator for parametric partial differential equations. Advances in Neural Information Processing Systems, 33:6755-6766, 2020.

Li, Z., Kovachki, N. B., Azizzadenesheli, K., liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021.

Li, Z., Huang, D. Z., Liu, B., and Anandkumar, A. Fourier neural operator with learned deformations for PDEs on general geometries. arXiv preprint arXiv:2207.05209, 2022a.

Li, Z., Meidani, K., and Farimani, A. B. Transformer for partial differential equations' operator learning. Transactions on Machine Learning Research, 2022b.

Lin, H., Wu, L., Xu, Y., Huang, Y., Li, S., Zhao, G., and Li, S. Z. Non-equispaced fourier neural solvers for pdes. arXiv preprint arXiv:2212.04689, 2022.

Lingsch, L., Michelis, M., Perera, S. M., Katzschmann, R. K., and Mishra, S. Vandermonde neural operators. arXiv preprint arXiv:2305.19663, 2023.

Liu, X., Xu, B., and Zhang, L. Ht-net: Hierarchical Transformer based operator learning model for multiscale PDEs. arXiv preprint arXiv:2210.10890, 2022.

Long, Z., Lu, Y., Ma, X., and Dong, B. PDE-Net: Learning PDEs from data. In International Conference on Machine Learning, pp. 3208-3216. PMLR, 2018.

Long, Z., Lu, Y., and Dong, B. PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network. Journal of Computational Physics, 399:108925, 2019 .
Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2016.

Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., and Zhang, L. SOFT: Softmax-free Transformer with linear complexity. Advances in Neural Information Processing Systems, 34:21297-21309, 2021a.

Lu, L., Jin, P., Pang, G., Zhang, Z., and Karniadakis, G. E. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):218-229, $2021 \mathrm{~b}$.

Mialon, G., Chen, D., Selosse, M., and Mairal, J. Graphit: Encoding graph structure in Transformers. arXiv preprint arXiv:2106.05667, 2021.

NeuralOperator. Neuraloperator. https://github. com/neuraloperator/neuraloperator/ tree/master, 2023. Accessed in July, 2023.

Patel, D., Ray, D., Abdelmalik, M. R., Hughes, T. J., and Oberai, A. A. Variationally mimetic operator networks. Computer Methods in Applied Mechanics and Engineering, 419:116536, 2024.

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N., and Kong, L. Random feature attention. In International Conference on Learning Representations, 2021.

Qin, T., Wu, K., and Xiu, D. Data driven governing equations approximation using deep neural networks. Journal of Computational Physics, 395:620-635, 2019.

Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physicsinformed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686-707, 2019.

Rudy, S. H., Brunton, S. L., Proctor, J. L., and Kutz, J. N. Data-driven discovery of partial differential equations. Science advances, 3(4):e1602614, 2017.

Schaeffer, H. Learning partial differential equations via data discovery and sparse optimization. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2197):20160446, 2017.

Seidman, J., Kissas, G., Perdikaris, P., and Pappas, G. J. NOMAD: Nonlinear manifold decoders for operator learning. Advances in Neural Information Processing Systems, 35: $5601-5613,2022$.

Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. In Proceedings of NAACL-HLT, pp. 464-468, 2018.

Sirignano, J. and Spiliopoulos, K. DGM: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339-1364, 2018.

Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.

Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., and Bengio, Y. Graph attention networks. In International Conference on Learning Representations, 2018.

Wu, K. and Xiu, D. Data-driven deep learning of partial differential equations in modal space. Journal of Computational Physics, 408:109307, 2020.

Xiao, Z., Hao, Z., Lin, B., Deng, Z., and Su, H. Improved operator learning by orthogonal attention. arXiv preprint arXiv:2310.12487, 2023.

Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nyströmformer: A Nyström-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14138-14148, 2021.

Yang, B., Tu, Z., Wong, D. F., Meng, F., Chao, L. S., and Zhang, T. Modeling localness for self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4449-4458, 2018.

Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y. Do Transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34:28877-28888, 2021.

Yu, B. et al. The Deep Ritz method: a deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6(1):112,2018

Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. Graph Transformer networks. Advances in neural information processing systems, 32, 2019.

Zhou, Q.-Y., Park, J., and Koltun, V. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018.
