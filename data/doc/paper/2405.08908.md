# THE IMPACT OF 2D AND 3D GAMIFIED VR ON LEARNING AMERICAN SIGN LANGUAGE 

Jindi Wang<br>Durham University<br>jindi.wang@durham.ac.uk

Ioannis Ivrissimtzis<br>Durham University<br>ioannis.ivrissimtzis@durham.ac.uk

Zhaoxing Li<br>University of Southampton<br>zhaoxing.li@soton.ac.uk

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-01.jpg?height=404&width=1646&top_left_y=1088&top_left_x=238)

Figure 1: The implemented 2D versus 3D gamified virtual environment for ASL learning.


#### Abstract

Sign language has been extensively studied as a means of facilitating effective communication between hearing individuals and the deaf community. With the continuous advancements in virtual reality (VR) and gamification technologies, an increasing number of studies have begun to explore the application of these emerging technologies in sign language learning. This paper describes a user study that compares the impact of 2D and 3D games on the user experience in ASL learning. Empirical evidence gathered through questionnaires supports the positive impact of 3D game environments on user engagement and overall experience, particularly in relation to attractiveness, usability, and efficiency. Moreover, initial findings demonstrate a similar behaviour of 2D and 3D games in terms of enhancing user experience. Finally, the study identifies areas where improvements can be made to enhance the dependability and clarity of 3D game environments. These findings contribute to the understanding of how game-based approaches, and specifically the utilisation of 3D environments, can positively influence the learning experience of ASL.


Keywords Human-Computer Interaction $\cdot$ Sign Language Learning $\cdot$ Virtual Reality $\cdot$ Empirical Study

## 1 Introduction

Learning American Sign Language (ASL) has significant advantages that go beyond those experienced by the deaf community. First and foremost, knowing ASL makes it easier to communicate effectively with people who are deaf or hard of hearing, promoting inclusion and lowering obstacles. The pursuit of ASL proficiency shows a dedication to building an inclusive society that values diversity and guarantees equal access for all. Learning ASL also offers unique linguistic and cognitive benefits [1, 2]. According to a large body of research, learning sign language improves

Running Title for Header

cognitive functions [3, 4, 5] and heightens visual-spatial awareness [6, 7]. Being proficient in ASL also provides access to prospective job opportunities given the rising demand for ASL interpreters, educators, and other professionals who can bridge the communication gap between the deaf and hearing communities. As a result, learning ASL has become a significant area for educational study.

Virtual reality (VR) and gamification technologies are rapidly advancing, and their potential for revolutionising sign language learning is gaining significant attention from researchers. The integration of gamified approaches in sign language education offers a range of compelling advantages, including enhanced interactivity, immersive practice experiences, and heightened learning motivation. One notable advantage of gamified sign language learning is the provision of interactive practice opportunities. Traditional sign language learning often relies on static materials, such as textbooks or videos, which limit learners' ability to actively engage with the language. By incorporating gamification elements, learners can participate in dynamic and interactive exercises, allowing them to practice their signing skills in a simulated environment. This interactivity fosters a more engaging and hands-on learning experience, resulting in improved retention and fluency [8, 9, 10, 11]. Additionally, gamified sign language instruction offers a plethora of engaging and practical opportunities that allow students to put their sign language understanding and proficiency to use in a relaxed and pleasant environment. Students can enhance their sign language communication abilities by interacting with the game's characters and completing sign language puzzles, and they can keep becoming better with practice. Students' engagement and learning effectiveness are both improved by this interactive and practical teaching method $12,13,14,15,16,17$.

As a result, extensive research has been conducted to gamify the process of learning sign language, primarily emphasising the creation and advancement of 2D and 3D games. For example, CopyCat, a 2D game proposed by Zafrulla et al. [18] is specifically designed to engage young children in interactive computer-based ASL learning, employing cutting-edge gesture recognition techniques. Its user-friendly interface comprises tutorial videos demonstrating accurate signs, a live video enabling gesture input for the recognition system, and real-time feedback on the child's progress. Additionally, the game features a character named Iris the cat, who carries out the child's instructions. Moreover, an educational game called MatLIBRAS Racing [19], is designed to teach sign language for natural numbers from a cognitive perspective. The game received positive feedback from students in terms of its educational and gaming features, fostering social relationships among players and facilitating sign language learning. It suggests that MatLIBRAS Racing holds promise as an effective educational tool, particularly in academic settings. Besides, 3D games such as Economou et al. [20] presented a work-in-progress study that focuses on evaluating the impact of combining scaffolded instruction with gamification to design a 3D interactive game to support learning the sign language alphabet. Moreover, Adamo et al. [21] presented the second iteration of the SMILE project, which introduces an immersive 3D game tailored for both deaf and hearing children. The game's enhanced design and user interaction have been meticulously crafted to elevate motivation and appeal, fostering an engaging and captivating experience. Although the evaluations demonstrated that the application was enjoyable and user-friendly, the study did not assess the actual learning outcomes achieved by the participants [22, 23]. Collectively, the current proposals for gamified sign language learning have made valuable contributions to the field, advancing our understanding and paving the way for further developments [24, 25]. However, the predominant focus of these approaches has been on either 2D or 3D game formats, overlooking the exploration of potential disparities in user experiences between these two genres.

For the purposes of our study, we leveraged widely used VR technology to create a learning environment that provides users with an immersive environment to learn the ASL numbers 0-9. To improve the user experience, we developed and introduced a whack-a-mole game inspired by the sign language game ASL Sea Battle [26, 27, 28] proposed by Bragget al., a game that makes it easy to collect user data. In our system design, we built two versions of whack-a-mole games, called 2D whack-a-mole and 3D whack-a-mole.

To the best of our knowledge, no prior user studies have specifically examined and compared the user experience of learning ASL through 2D games versus 3D games. Consequently, we conducted a user study using a survey methodology proposed by Schrepp et al. [29], aiming to answer the research question "What is the impact of game type (2D vs 3D) on system usability, user experience, and learning outcome in ASL learning?". Our main contributions are as follows:

- We have developed an immersive virtual environment that facilitates learning ASL numbers $0-9$, featuring two variations of a Whack-a-Mole game. Our system offers a distinct and captivating approach to ASL learning, which we expect will enhance user satisfaction and engagement.
- A user study comparing the potential of 2D and 3D games in enhancing user experience and improving learning outcomes. Empirical suggests a positive impact of 3D game environments on user engagement and experience, specifically in terms of attractiveness, usability, and efficiency. Additionally, we highlight areas for improvement to ensure the dependability and clarity of 3D game environments.

Running Title for Header

## 2 Related Work

### 2.1 Sign Language Recognition

Researchers have conducted several studies on sign language recognition based on deep learning and computer vision techniques. Camgoz et al. [30] introduced an innovative transformer-based architecture that simultaneously learns Continuous Sign Language Recognition and Translation, eliminating the need for ground-truth timing information. This joint approach addresses two interdependent sequence-to-sequence learning problems and yields notable performance improvements. Bragg et al. [31] discuss the challenges and opportunities in developing effective sign language recognition, generation, and translation systems, emphasizing the multidisciplinary expertise required. They presented the comprehensive outcomes of an interdisciplinary workshop, covering essential background information, an overview of the current state-of-the-art, key challenges, and a compelling call to action for the broader research community. $\mathrm{Pu}$ et al. [32] proposed an alignment network with iterative optimisation for weakly supervised continuous sign language recognition. Their framework comprises a 3D convolutional residual network for feature learning and an encoder-decoder network with connectionist temporal classification for sequence modelling. Zhang et al. [33] presented MediaPipe Hands, an on-device hand-tracking pipeline designed for real-time usage in augmented reality and virtual reality applications. Wadhawan et al. [34] proposed a deep learning-based system for recognising static signs in sign language using convolutional neural networks, achieving high training accuracy and surpassing earlier works that focused on a limited number of hand signs. Jiang et al. [35] proposed a Skeleton Aware Multi-modal SLR framework (SAM-SLR) that leverages multi-modal information to achieve higher recognition rates. Kumar et al. [36] proposed a method for recognising 3D sign language using spatiotemporal graph kernels, which is signer invariant, motion invariant, and faster compared to existing graph kernel approaches.

After a thorough review of the available research on sign language recognition, we concluded that Mediapipe aligns best with the objectives of our study. Consequently, we employed Mediapipe as our tool for sign language recognition, capitalising on its exceptional accuracy in identifying hand landmark points in real-time. Moreover, as an open-source hand gesture detection framework developed by Google, Mediapipe benefits from robust support and comprehensive documentation, adding to its appeal and reliability.

### 2.2 Sign Language Dictionary

Several articles delve into various research studies focusing on sign language dictionaries. Schnepp et al. [37] created an animated sign language dictionary to facilitate communication between caregivers and residents who use sign language. Alonzo et al. [38] conducted a study to examine the relationship between the performance of automatic sign recognition technology and users' subjective judgments when searching for unfamiliar words in ASL dictionaries. Their findings revealed that metrics incorporating the precision of the overall results list and the similarity of other words within the list demonstrated a stronger correlation with users' judgments compared to the metrics typically reported in previous research on ASL dictionary. Hassan et al. [39] investigated the impact of a webcam-based ASL dictionary search system on user judgments. Their study found that various factors such as the position of the desired word in the results list, whether the word appeared above or below the fold, and the similarity of other words in the list, significantly influenced users' perceptions of the system. In another study, Hassan et al. [40] discussed the design and evaluation of a hybrid search approach for American Sign Language to English dictionaries. This approach synergistically integrated video-based queries and linguistic properties filtering, resulting in heightened search speed and accuracy. Through interviews with ASL learners and a between-subjects experiment, the hybrid search system exhibited superior performance metrics and user satisfaction compared to a video-based search system. These studies demonstrate the potential of incorporating sign language dictionary queries to enhance learning efficiency and improve the overall learning experience for users. Drawing from these findings, our learning environment design incorporated an ASL dictionary user interface, empowering users to efficiently search for sign language representations.

### 2.3 Sign Language Games

Additionally, there have been studies that specifically focus on the gamification of sign language learning. For example, Gameiro et al. [41] proposed Kinect-Sign, a serious game designed to teach non-deaf individuals sign language. The game offers two modes: School-mode for learning letter signs, and Competition-mode for testing the acquired skills, through engaging challenges. Besides, Uluer et al. [42] presented a robotic platform designed for sign language tutoring for children with communication impairments. The platform utilises an interactive five-fingered robot called Robovie R3, which expresses selected words in Turkish sign language using hand and body movements combined with facial expressions. Moreover, Bantupalli et al. [43] developed a vision-based system that translates sign language into text, aiming to enhance communication between signers and non-signers. Samonte [44] developed an e-tutor system to support instructors in teaching sign language. Economou et al. [45] designed a serious game to assist adults in learning
sign language and bridge the communication gap between hearing-impaired and able-hearing individuals. These studies suggest that integrating gamified components into sign language instruction can enhance learners' motivation to learn. Building upon these insights, we incorporated sign language games into the user interface of our learning environment to improve users' ASL learning experiences.

Hence, our study aimed to investigate the distinct impacts of 2D and 3D games on ASL learning, providing valuable insights for future research in this domain. To achieve this, we created a VR environment utilising advanced VR technology as a platform for users to engage in ASL learning. Within this immersive learning environment, we incorporated various features such as sign language dictionaries, interactive question-and-answer sessions, and captivating sign language games to enhance the overall user experience. Among the sign language games we developed, one notable example was a whack-a-mole game, offered in both a $2 \mathrm{D}$ version and a $3 \mathrm{D}$ version. By comparing the experiences of two distinct user groups, each playing their perspective game versions, we aimed to explore the differential effects of $2 \mathrm{D}$ and $3 \mathrm{D}$ games on the process of ASL learning.

## 3 Learning Environment and Games

The immersive learning environment for learning ASL numerals from 0 to 9 is shown in Fig. 1. Unity (version 2020.3.32f1) was used to create the scene. Utilising the eye-tracking functionality of the HTC Vive Pro, users interact with the device by tracking their eye location. When a user's attention is fixed for three seconds, the system allows them to click or choose an object.

For image acquisition, a built-in camera was utilised, connected to a PC that employed openCV (version 3.4.2) [46]. Gesture recognition was implemented using Mediapipe [33], which detected the user's hand and extracted a sequence of 21 feature points ( $p_{0}, p_{1}, p_{2}, \ldots, p_{20}$ ) representing landmarks on the hand. The coordinate frame's origin was set at $p_{0}$, the point near the user's wrist at the bottom of the palm. The classifier employed was a multilayer perceptron consisting of three fully connected layers, implemented in Python 3.6 [47] and Tensorflow 2.6.0 [48], yielding recognition accuracy rates exceeding $90 \%$. This level of accuracy was considered satisfactory for the study's objectives, ensuring a smooth user experience.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-04.jpg?height=1059&width=1131&top_left_y=1384&top_left_x=497)

Figure 2: Quiz module for learning ASL

To enhance the effectiveness of the learning process, a question-answer module was integrated into the system. This module allows users to evaluate their proficiency level and practice their signing skills by responding to randomly generated questions from a database. In Fig. 2, an example is shown where the system presents the question "Can you sign for 5?". The user has the option to refer to the dictionary or utilise their acquired skills to sign the number ' 5 '. Alternatively, they can choose the "I don't know" option, prompting the system to demonstrate the correct expression. In this case, the user is encouraged to continue practising until they feel confident enough to sign the digit independently by pressing the relevant button.

### 3.1 2D Game and 3D Game

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-05.jpg?height=474&width=1516&top_left_y=670&top_left_x=304)

Figure 3: 2D Whack-a-Mole Game (Left); 3D Whack-a-Mole Game(Right)

To explore potential differences in user experiences during ASL learning, we developed both a 2D and a 3D version of our game, Whack-a-Mole, incorporating ASL elements. The objective of the game remains the same in both versions: players must promptly identify and sign the current location of the gopher within a designated time limit.

In both the $2 \mathrm{D}$ and $3 \mathrm{D}$ versions, the game interface consists of a grid-like layout where each location is uniquely identified. Players earn one point for correctly signing the position of the gopher, indicating their understanding of ASL gestures. Conversely, no points are awarded if the player fails to accurately sign the gopher's location. To maintain an engaging experience, we have implemented a time limit of 3 seconds for each round. If the player does not sign the correct location within the given time, a new gopher automatically appears, providing the player with another opportunity to score points. The game duration is set at a total of 30 seconds, creating a challenging yet manageable time frame for players to showcase their ASL recognition and signing skills.

By offering both 2D and 3D versions of the game, we aimed to investigate potential variations in user experiences and learning outcomes across the two interfaces. This allowed us to explore the potential influence of immersion and visual depth within the 3D environment on players' engagement, motivation, and overall ASL learning, compared to the 2D version. Fig. 3 visually depicts the interfaces of both the 2D and 3D versions of the Whack-a-Mole game, showcasing the grid layout and the gopher's locations.

## 4 Methodology

### 4.1 Participants and Procedure

The sample consisted of 24 participants, aged between 23 and 34 years $(\mathrm{M}=28.13, \mathrm{SD}=2.55)$. All participants played the game twice and were divided into four groups, covering all possible combinations between 2D or 3D environments and first or second attempt. The aim of the design was to detect the learning effect in general and the effect of familiarisation with the virtual environment in particular. None of the participants possessed any prior experience or formal education in ASL, thus ensuring that the focus of the study was on individuals without pre-existing sign language knowledge.

In order to successfully enhance the learning process by combining instructional scaffolding approaches, we divided the learning process into three stages: Learn, Practice, and Assess. It is a suggested strategy for aiding ASL learning that entails the creation of a teaching tool that combines innovative technology with gamification components [49]. The details of the learning process are described below:

- Learn: Participants are encouraged to explore and navigate the virtual learning platform at their own pace, acquainting themselves with the layout and function of each user interface. Following this, they are requested to spend three minutes familiarising themselves with the 0-9 ASL expressions, utilising the 0-9 ASL dictionary interface.
- Practice: To enhance users' comprehension of 0-9 ASL, we offer a Quiz interface where users can engage in a dynamic question-and-answer session. Within the Quiz interface, participants will be presented with randomly generated questions to respond to. During this phase, participants are encouraged to actively engage in the $\mathrm{Q} \& \mathrm{~A}$ session for a period of three minutes.
- Assess: After the users have demonstrated their proficiency in 0-9 ASL expressions and understanding of the subject matter by successfully completing the initial two stages, we present them with an engaging game called Whack a-Mole, specifically designed to assess their learning. This stage aims to compare the effects of the 2D and 3D versions of the game on user experience. First, we asked the users to play the game once and complete our user evaluation questionnaire. In the second attempt on the game, half of the users continued on the same environment, while the other half swapped between 2D and 3D. That is, we had four groups in total, of six participants each: 2D-2D, 2D-3D, 3D-2D, and 3D-3D, depending on the environments of their first and second attempts.


### 4.2 Evaluation Methods

Three dimensions of the VR learning environments were evaluated and compared: usability, user experience, and user performance.

For usability, the well-established SUS (System Usability Scale) questionnaire [50] was employed. The participants were asked to rate the $2 \mathrm{D}$ and $3 \mathrm{D}$ versions of the VR learning environments on a scale of 1 (strongly disagree) to 5 (strongly agree) after using them.

For user experience, we employed the user survey method proposed by Schrepp et al. [29]. This approach utilises six scales, each representing a distinct aspect of the user experience: Attractiveness, Efficiency, Perspicuity, Dependability, Stimulation, Novelty. These scales provide a comprehensive framework for evaluating the users' perceptions and attitudes. Each scale comprises several specific items that capture various dimensions of the user experience, as outlined in Table 1. To gather data on the user experience, we used a 7-point Likert scale for each of the 26 items in the questionnaire. Participants were asked to rate their level of agreement with each statement, ranging from 1 (strongly agree with a negative statement) to 7 (strongly agree with a positive statement). This rating scale allowed a quantitative analysis of the participants' perceptions across the various dimensions of the user experience.

Finally, for user performance, i.e., how well they learned using the VR environments, we collected and analysed the players' game scores.

Table 1: User experience questionnaire.

| Attractiveness | Perspicuity |
| :---: | :---: |
| annoying / enjoyable <br> good / bad <br> unlikable / pleasing <br> unpleasant / pleasant <br> attractive / unattractive <br> friendly / unfriendly | not understandable / understandable <br> easy to learn / difficult to learn <br> complicated / easy <br> clear / confusing |
| Efficiency | Dependability |
| fast / slow <br> inefficient / efficient <br> impractical / practical <br> organized / cluttered | unpredictable / predictable <br> obstructive / supportive <br> secure / not secure <br> meets expectations / does not meet expectations |
| Stimulation | Novelty |
| valuable / inferior <br> boring / exiting <br> not interesting / interesting <br> motivating / demotivating | creative / dull <br> inventive / conventional <br> usual / leading edge <br> conservative / innovative |

Running Title for Header

## 5 Results

### 5.1 Comparing Usability

On a scale of 0 to 100 , with 0 representing low usability and 100 representing high usability, both the 2D and 3D game environments achieved high usability ratings on the SUS scale. The 2D game received an average score of 76.25 (SD = 5.15), while the 3D game scored 80.63 (SD $=3.41$ ). These scores indicate that both game environments received "good" usability ratings [50]. The low standard deviations observed for both games suggest that the majority of participants had consistent and positive experiences with the usability of the interfaces. These findings highlight the success of creating user-friendly interfaces for both 2D and 3D games, with the transition to 3D not significantly impacting usability. This suggests that both 2D and 3D games can be equally effective in terms of usability, and it is important to consider the specific needs and preferences of the target audience when designing game interfaces.

The comparable high usability scores achieved by both the 2D and 3D game environments demonstrate the success of our design in terms of intuitive controls, clear visuals, and engaging gameplay mechanics. The positive SUS ratings indicate that users found both interfaces easy to navigate and efficient.

### 5.2 Comparing User Experience

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-07.jpg?height=715&width=957&top_left_y=1014&top_left_x=584)

Figure 4: Six scales of user experience

Fig. 4 presents a comprehensive comparison of user experience during ASL learning, between the two versions of the game environments. In summary, the user experience analysis reveals that both 2D and 3D games received positive ratings across various scales. However, the 3D game consistently outperformed the 2D game in several aspects, including visual attractiveness, efficiency, novelty, perspicuity, and stimulation. The users found that the visuals of the 3D game were appealing and innovative, providing a fresh and immersive gaming/learning experience, as shown in Figure 4(a) and Figure 4(d). Additionally, the 3D game demonstrated higher efficiency, offering streamlined user and interface interactions, but in terms of "organized", it is similar to those of the 2D game, as shown in Figure 4(c). The interface of the 3D game was also perceived as easier to learn and understand, enhancing the overall user experience, but in terms of "easy", it is lower than that of the 2D game, as shown in Figure 4(e), which means users found that the 2D game is easier to operate. Moreover, the 3D game succeeded in delivering a more stimulating and engaging user experience, eliciting higher levels of excitement and immersion from the users, as shown in Figure 4(f). Overall, the findings suggest that the added dimensionality and immersive elements of the $3 \mathrm{D}$ game contributed to a superior user experience compared to the 2D game. While the 3D game excelled in several areas, Figure 4(b) indicates that both 2D and 3D games were perceived as having low dependability by the users, as they found both games met their basic needs, but there are areas where they would like to improve, such as the complexity of the game, the difficulty of completing the level, and so on. Despite the strengths of the 3D game, it is worth noting that the 2D game still received positive ratings across all scales, indicating that it provided a satisfactory user experience as well. The results emphasise the

## Running Title for Header

importance of considering various factors, including visuals, efficiency, novelty, perspicuity, and stimulation, when designing game environments to enhance user experience and engagement.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-08.jpg?height=488&width=623&top_left_y=453&top_left_x=453)

(a) Attractiveness

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-08.jpg?height=482&width=594&top_left_y=453&top_left_x=1058)

(b) Dependability

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-08.jpg?height=456&width=594&top_left_y=1016&top_left_x=470)

Organized

(c) Efficiency

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-08.jpg?height=434&width=662&top_left_y=1035&top_left_x=970)

Innovative

(d) Novelty

![](https://cdn.mathpix.com/cropped/2024_06_04_4a27b02b2cc26db981deg-08.jpg?height=496&width=1160&top_left_y=1590&top_left_x=472)

(e) Perspicuity

(f) Stimulation

Figure 5: Each item of six scales of user experience.

In the study mentioned in Schrepp et al. [51], the scales of the user experience questionnaire were categorised into two groups: pragmatic quality and hedonic quality. Pragmatic quality includes perspicuity, efficiency, and dependability, which are related to task performance. Hedonic quality includes stimulation and novelty, which are related to non-taskrelated aspects. Attractiveness, on the other hand, is considered a pure valence dimension. Table 2 presents the average scores for these grouped scales for two different games, along with the corresponding $\mathrm{p}$-values. The results indicate that the user preference for the 3D game was statistically significant, especially regarding hedonic quality.

Table 2: Means and p-values for the three groups of scales.

| Pragmatic and Hedonic Quality | 2D | 3D | P-Value |
| :---: | :---: | :---: | :---: |
| Attractiveness | 4.21 | 4.71 | $1.304 \mathrm{E}-03$ |
| Pragmatic Quality | 4.30 | 4.60 | $1.150 \mathrm{E}-02$ |
| Hedonic Quality | 3.93 | 4.68 | $5.056 \mathrm{E}-09$ |

### 5.3 Comparing User Performance

Table 3 summarises the results, for each of the four user groups and for each of their two attempts on the game, separately.

Table 3: Score means and standard deviations, for the four groups, for the first (I) and the second (II) attempts.

| Groups | mean (I) | s.d. (I) | mean (II) | s.d (II) |
| :---: | :---: | :---: | :---: | :---: |
| 2D-2D | 10.50 | 2.22 | 17.33 | 2.92 |
| 2D-3D | 9.83 | 1.34 | 14.67 | 2.49 |
| 3D-2D | 9.33 | 1.70 | 14.83 | 1.95 |
| 3D-3D | 9.67 | 1.49 | 16.00 | 2.58 |

To measure the effect of the choice between $2 \mathrm{D}$ and $3 \mathrm{D}$, we compared scores over the two environments, on the first attempt only. These scores correspond to the first column of the table, and we compare the first two rows against the last two. The t-test returned a p-value of 0.43 , indicating no statistical significance between the two environments, even though the $2 \mathrm{D}$ environment had a slightly higher mean score, 10.16 against 9.50 .

To measure the general learning effect, we compared the scores between the first and the second attempt, which correspond to the first and third columns of the table. The t-test returned a p-value of $2.48 \mathrm{E}-14$, with means of 9.83 and 15.70 for the first and the second attempt, respectively, indicating a very strong learning effect. Notice that the design of the experiment is symmetric, and thus, the result was not affected by the order in which the 2D and 3D games were played. Moreover, by comparing the second column of the table with the fourth, we notice that in the second attempt the standard deviations increased within all groups, indicating that the learning effect was not uniform across all users. This is an observation that requires a larger-scale experiment to assess its significance.

Finally, to specifically detect user familiarisation with the VR environment, as opposed to a general learning effect, we compared the second attempt scores between users who swapped environments between attempts (groups 2D-3D and 3D-2D) and those who did not (groups 2D-2D and 3D-3D). These scores correspond to the third row of the table, and we compare rows 1 and 4 , against rows 2 and 3 . The corresponding means were 14.75 and 16.66 , and a t-test p-value of 0.043 indicates that the detrimental effect of swapping VR environments was small but statistically significant.

## 6 Discussion

When examining the scales related to enjoyment and engagement, the scores show that users had a positive perception of both 2D and 3D games. The 3D games received higher ratings in terms of being enjoyable, good, pleasing, and attractive, indicating that they provided a more captivating and satisfying gaming experience. However, the 2D games also received respectable scores, suggesting that they still managed to offer a satisfactory level of enjoyment and engagement. These results indicate that while 3D games may have an edge in terms of overall enjoyment, 2D games can still provide an enjoyable gaming experience for users.

The scores related to learning and novelty reveal interesting insights about 2D and 3D games. The 3D games received higher ratings in terms of novelty and innovation, suggesting that they provided a fresh and groundbreaking gaming experience. This can be attributed to the immersive nature of 3D environments, which inherently offer a sense of novelty and exploration. Conversely, the 2D games were perceived as easier to use, indicating that they may have a lower learning curve and are more accessible to newcomers. These findings indicate that 3D games excel in terms of offering novel experiences, while 2D games provide a faster learning curve. Overall, the analysis suggests that the 3D game excelled in visual appeal, efficiency, stimulation, enjoyment, and novelty, while, on the other hand, the 2D game demonstrated strengths in interface clarity, perspicuity, and ease of learning.

The quantitative analysis of the game scores did not find a statistically significant difference between 2D and 3D. However, there was a statistically significant learning effect, part at least of which should be explained by the

Running Title for Header

familiarisation with the VR environment, rather than the learning of ASL. Moreover, there was an indication of different learning patterns among users, with the standard deviations increasing in the second attempt within all user groups.

### 6.1 Limitations

It is important to acknowledge that both $2 \mathrm{D}$ and $3 \mathrm{D}$ games have their respective limitations. In the case of 2D games, one area that warrants future work is the enhancement of the visual experience. Advances in graphics technology could potentially narrow the gap between 2D and 3D games by allowing for more detailed and visually appealing 2D environments. Additionally, further exploration of gameplay mechanics and interactivity in the context of 2D games could lead to more engaging and immersive experiences for players. On the other hand, 3D games face a primary limitation related to hardware requirements. Future work and development efforts could focus on optimising game engines and graphics pipelines to improve performance on a wider range of devices. This would ensure that more players have access to and can enjoy 3D gaming experiences, regardless of their shareware capabilities. Additionally, it is important to note several limitations of our evaluation approach. One notable limitation is the relatively small sample size of 24 participants in the user study, which may have resulted in limited data and potential biases. To mitigate this limitation, future studies should consider increasing the sample size to obtain more robust and generalisable results.

## 7 Conclusion and Future Work

In conclusion, our study provides preliminary evidence of the potential of both $2 \mathrm{D}$ and $3 \mathrm{D}$ games in enhancing the user experience of learning ASL. The findings highlight the positive impact of 3D game environments on user engagement and overall experience, as evidenced by their higher ratings in attractiveness, usability, and efficiency, compared to 2D games. However, there is room for improvement in ensuring the dependability and clarity of 3D game environments. These results contribute to our understanding of the benefits of incorporating game-based approaches, particularly 3D environments, into ASL learning.

Future research can build upon these findings by delving deeper into the specific elements and design features that contribute to the positive user experience in both 2D and 3D games. Additionally, exploring strategies to enhance the dependability and clarity of 3D game environments can further optimise learning outcomes in ASL education. It would also be beneficial to increase the number of participants in future user studies to strengthen the generalisability of the findings.

## References

[1] Elissa L Newport and Richard P Meier. The acquisition of american sign language. In The crosslinguistic study of language acquisition, pages 881-938. Psychology Press, 2017.

[2] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, Yunzhan Zhou, and Lei Shi. User-defined hand gesture interface to improve user experience of learning american sign language. In International Conference on Intelligent Tutoring Systems, pages 479-490. Springer, 2023.

[3] Brittany L Freel, M Diane Clark, Melissa L Anderson, Gizelle L Gilbert, Millicent M Musyoka, and Peter C Hauser. Deaf individuals' bilingual abilities: American sign language proficiency, reading skills, and family characteristics. 2011.

[4] Zhaoxing Li, Lei Shi, Yunzhan Zhou, and Jindi Wang. Towards student behaviour simulation: a decision transformer based approach. In International Conference on Intelligent Tutoring Systems, pages 553-562. Springer, 2023.

[5] Zhaoxing Li, Lei Shi, Alexandra I Cristea, and Yunzhan Zhou. A survey of collaborative reinforcement learning: interactive methods and design patterns. In Proceedings of the 2021 ACM Designing Interactive Systems Conference, pages 1579-1590, 2021.

[6] David Quinto-Pozos, Jenny L Singleton, and Peter C Hauser. A case of specific language impairment in a deaf signer of american sign language. The Journal of Deaf Studies and Deaf Education, 22(2):204-218, 2017.

[7] Zhaoxing Li, Lei Shi, Jindi Wang, Alexandra I Cristea, and Yunzhan Zhou. Sim-gail: A generative adversarial imitation learning approach of student modelling for intelligent tutoring systems. Neural Computing and Applications, 35(34):24369-24388, 2023.

[8] Dokun Oluwajana, Adeleye Idowu, Muesser Nat, Vanye Vanduhe, and Samson Fadiya. The adoption of students' hedonic motivation system model to gamified learning environment. Journal of theoretical and applied electronic commerce research, 14(3):156-167, 2019.

Running Title for Header

[9] Patrick Buckley and Elaine Doyle. Gamification and student motivation. Interactive learning environments, 24(6):1162-1175, 2016.

[10] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, Yunzhan Zhou, and Lei Shi. Exploring the potential of immersive virtual environments for learning american sign language. In European Conference on Technology Enhanced Learning, pages 459-474. Springer, 2023.

[11] Fatemeh Roosta, Fattaneh Taghiyareh, and Maedeh Mosharraf. Personalization of gamification-elements in an elearning environment based on learners' motivation. In 2016 8th International symposium on telecommunications (IST), pages 637-642. IEEE, 2016.

[12] Samaa M Shohieb. A gamified e-learning framework for teaching mathematics to arab deaf students: Supporting an acting arabic sign language avatar. Ubiquitous Learning: An International Journal, 12(1):55-70, 2019.

[13] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, and Lei Shi. Comparative efficacy of $2 \mathrm{~d}$ and $3 \mathrm{~d}$ virtual reality games in american sign language learning. In The 31st IEEE Conference on Virtual Reality and 3D User Interfaces. Newcastle University, 2024.

[14] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, and Lei Shi. Impact of personalised ai chat assistant on mediated human-human textual conversations: Exploring female-male differences. In Companion Proceedings of the 29th International Conference on Intelligent User Interfaces, pages 78-83, 2024.

[15] Alessandra Antonaci, Roland Klemke, and Marcus Specht. The effects of gamification in online learning environments: A systematic literature review. In Informatics, volume 6, page 32. MDPI, 2019.

[16] Lei Shi, Alexandra I. Cristea, Suncica Hadzidedic, and Naida Dervishalidovic. Contextual gamification of social interaction - towards increasing motivation in social e-learning. In Elvira Popescu, Rynson W. H. Lau, Kai Pata, Howard Leung, and Mart Laanpere, editors, Advances in Web-Based Learning - ICWL 2014, pages 116-122, Cham, 2014. Springer International Publishing.

[17] Lei Shi and Alexandra I. Cristea. Motivational gamification strategies rooted in self-determination theory for social adaptive e-learning. In Alessandro Micarelli, John Stamper, and Kitty Panourgia, editors, Intelligent Tutoring Systems, pages 294-300, Cham, 2016. Springer International Publishing.

[18] Zahoor Zafrulla, Helene Brashear, Peter Presti, Harley Hamilton, and Thad Starner. Copycat: an american sign language game for deaf children. In Face and Gesture 2011, pages 647-647. IEEE Computer Society, 2011.

[19] Herleson Paiva Pontes, João Batista Furlan Duarte, and Plácido Rogério Pinheiro. An educational game to teach numbers in brazilian sign language while having fun. Computers in Human Behavior, 107:105825, 2020.

[20] Daphne Economou, Markos Mentzelopoulos, Jack Ingram, Timur Martinez-Mukimov, Shirin Primkulova, and Said Abduvaliev. Work-in-progress-gamifying the process of learning sign language in vr. In 2022 8th International Conference of the Immersive Learning Research Network (iLRN), pages 1-3. IEEE, 2022.

[21] Nicoletta Adamo-Villani and Kelly Wright. Smile: an immersive learning game for deaf and hearing children. In ACM SIGGRAPH 2007 educators program, pages 17-es. 2007.

[22] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, and Lei Shi. Enhancing user experience in chinese initial text conversations with personalised ai-powered assistant. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI EA '24, New York, NY, USA, 2024. Association for Computing Machinery.

[23] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, Yunzhan Zhou, and Lei Shi. Developing and evaluating a novel gamified virtual learning environment for asl. In IFIP Conference on Human-Computer Interaction, pages 459-468. Springer, 2023.

[24] Zhaoxing Li, Jujie Yang, Jindi Wang, Lei Shi, and Sebastian Stein. Integrating 1stm and bert for long-sequence data analysis in intelligent tutoring systems. arXiv preprint arXiv:2405.05136, 2024.

[25] Zhaoxing Li, Jujie Yang, Jindi Wang, ?Lei Shi, Jiayi Feng, and Sebastian Stein. Lbkt: a 1stm bert-based knowledge tracing model for long-sequence data. In 20th International Conference on Intelligent Tutoring Systems: Generative Intelligence and ITS (10/06/24 - 13/06/24), June 2024.

[26] Danielle Bragg, Naomi Caselli, John W Gallagher, Miriam Goldberg, Courtney J Oka, and William Thies. ASL Sea Battle: Gamifying Sign Language Data Collection. In Proc. CHI-HFCS, pages 1-13, 2021.

[27] Zhaoxing Li. Deep Reinforcement Learning Approaches for Technology Enhanced Learning. PhD thesis, Durham University, 2023.

[28] Zhaoxing Li, Mark Jacobsen, Lei Shi, Yunzhan Zhou, and Jindi Wang. Broader and deeper: A multi-features with latent relations bert knowledge tracing model. In European Conference on Technology Enhanced Learning, pages 183-197. Springer, 2023.

Running Title for Header

[29] Martin Schrepp, Andreas Hinderks, and Jörg Thomaschewski. Applying the user experience questionnaire (UEQ) in different evaluation scenarios. In International Conference of Design, User Experience, and Usability, pages 383-392. Springer, 2014.

[30] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language transformers: Joint end-to-end sign language recognition and translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10023-10033, 2020.

[31] Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa Verhoef, et al. Sign language recognition, generation, and translation: An interdisciplinary perspective. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility, pages 16-31, 2019.

[32] Junfu Pu, Wengang Zhou, and Houqiang Li. Iterative alignment network for continuous sign language recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4165-4174, 2019.

[33] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, and Matthias Grundmann. Mediapipe hands: On-device real-time hand tracking. arXiv:2006.10214, 2020.

[34] Ankita Wadhawan and Parteek Kumar. Deep learning-based sign language recognition system for static signs. Neural computing and applications, 32:7957-7968, 2020.

[35] Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu. Skeleton aware multi-modal sign language recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3413-3423, 2021.

[36] D Anil Kumar, ASCS Sastry, PVV Kishore, and E Kiran Kumar. 3d sign language recognition using spatio temporal graph kernels. Journal of King Saud University-Computer and Information Sciences, 34(2):143-152, 2022.

[37] Jerry Schnepp, Rosalee Wolfe, Gilbert Brionez, Souad Baowidan, Ronan Johnson, and John McDonald. Humancentered design for a sign language learning application. In Proc. PETRAE, pages 1-5, 2020.

[38] Oliver Alonzo, Abraham Glasser, and Matt Huenerfauth. Effect of automatic sign recognition performance on the usability of video-based search interfaces for sign language dictionaries. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility, pages 56-67, 2019.

[39] Saad Hassan, Oliver Alonzo, Abraham Glasser, and Matt Huenerfauth. Effect of sign-recognition performance on the usability of sign-language dictionary search. ACM Transactions on Accessible Computing (TACCESS), 14(4):1-33, 2021.

[40] Saad Hassan, Akhter Al Amin, Alexis Gordon, Sooyeon Lee, and Matt Huenerfauth. Design and evaluation of hybrid search for american sign language to english dictionaries: Making the most of imperfect sign recognition. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1-13, 2022.

[41] João Gameiro, Tiago Cardoso, and Yves Rybarczyk. Kinect-sign: teaching sign language to "listeners" through a game. In Innovative and Creative Developments in Multimodal Interaction Systems: 9th IFIP WG 5.5 International Summer Workshop on Multimodal Interfaces, eNTERFACE 2013, Lisbon, Portugal, July 15-August 9, 2013. Proceedings 9, pages 141-159. Springer, 2014.

[42] Pınar Uluer, Neziha Akalın, and Hatice Köse. A new robotic platform for sign language tutoring: Humanoid robots as assistive game companions for teaching sign language. International Journal of Social Robotics, 7:571-585, 2015.

[43] Kshitij Bantupalli and Ying Xie. American sign language recognition using deep learning and computer vision. In International Conference on Big Data, pages 4896-4899. IEEE, 2018.

[44] Mary Jane C Samonte. An assistive technology using fsl, speech recognition, gamification and online handwritten character recognition in learning statistics for students with hearing and speech impairment. In Proc. ICFET, pages $92-97,2020$.

[45] Daphne Economou, Melissa Gonzalez Russi, Ioannis Doumanis, Markos Mentzelopoulos, Vassiliki Bouki, and Jeffery Ferguson. Using Serious Games for Learning British Sign Language Combining Video, Enhanced Interactivity, and VR Technology. Journal of Universal Computer Science, 26(8):996-1016, 2020.

[46] Gary Bradski and Adrian Kaehler. Opencv. Dr. Dobb's journal of software tools, 3:120, 2000.

[47] Why Python. Python. Python Releases for Windows, 24, 2021.

[48] Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. arXiv:1711.10604, 2017.

Running Title for Header

[49] David Wood, Jerome S Bruner, and Gail Ross. The role of tutoring in problem solving. Journal of child psychology and psychiatry, 17(2):89-100, 1976.

[50] Aaron Bangor, Philip Kortum, and James Miller. Determining what individual sus scores mean: Adding an adjective rating scale. Journal of usability studies, 4(3):114-123, 2009.

[51] Martin Schrepp, Andreas Hinderks, and Jörg Thomaschewski. Construction of a Benchmark for the User Experience Questionnaire (UEQ). International Journal of Interactive Multimedia and Artificial Intelligence, $4: 40-44$, June 2017 .

