# M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models 

Hongyu Wang* Jiayu Xu* Senwei Xie* Ruiping Wang ${ }^{\diamond}$<br>Jialin Li Zhaojie Xie Bin Zhang Chuyan Xiong Xilin Chen ${ }^{\diamond}$<br>Institute of Computing Technology, Chinese Academy of Sciences


#### Abstract

Multilingual multimodal reasoning is a core component in achieving human-level intelligence. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. Using M4U, we conduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results show that the state-of-the-art model, GPT-4o, achieves only $47.6 \%$ average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German. We believe that M4U can serve as a crucial tool for systematically evaluating LMMs based on their multilingual multimodal reasoning capabilities and monitoring their development. The homepage, codes and data are public available.


![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-01.jpg?height=574&width=1331&top_left_y=1708&top_left_x=386)

Figure 1: An illustration of multi-discipline multilingual multimodal understanding. Both textual questions and images contain the multilingual contents. We highlight the Chinese contents in yellow. English translations are provided for better readability.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-02.jpg?height=838&width=1368&top_left_y=256&top_left_x=365)

Question: 黑磷与石墨类似，也具有 <image_1> 如图层状结构。科学家最近研发了黑磷-石墨复合负极材料，其单层结构的俯视图 <image_2> 如图所示。下列说法错误的是: Options:

A. 黑磷区中P—P的键能不完全相同

B. 单层复合材料中C和P间的作用力为共价键

键长: $0.2224 \mathrm{~nm}$ 键长: $0.2244 \mathrm{~nm}$

C. 基态磷原子核外电子中，两种自旋状态的电子数之比为 3 :

D. 黑磷中P原子的杂化方式和石墨中C原子的杂化方式相同

Question: Graphite is similar to black phosphorus, also having <image 1 > a layered phosphorus-graphite composite negative electrode material, whose top view of the single-layer structure <image_2> is shown in the figure. Which of the following statements is incorrect?

A. In the black phosphorus region, the P-P bond energy is not completely the same B. In the single-layer composite material, the interaction between $C$ and $P$ is covalent

冬1 bonding

C. In the outer electrons of the basic phosphorus atoms, the ratio of electrons in two

D. In black spin states is $3: 2$

hybridization mode of $C$ atoms in graphite

Question: Graphit ähnelt Schwarzem Phosphor und hat auch <image_1> eine schichtartige Struktur wie im Bild gezeigt. Wissenschaftler haben kürzlich ein GraphitSchwarzer Phosphor-Kompositmaterial entwickelt, dessen einzelne Schichtstruktur <image_2> im Bild gezeigt wird. Welche der folgenden Aussagen ist falsch: Options:

A. Im Bereich des schwarzen Phosphors ist die P-P-Bindungsenergie nicht identisch

B. In der einzelnen Schicht des Kompositmaterials ist die Wechselwirkung zwischen C

C. Im äußeren Elektronenbereich des Phosphoratoms gibt es zwei Arten von

Elektronenspins, das Verhältnis ihrer Anzahl ist 3:2

der C-Atome in Graphit ist die Hybridisierungsart der P-Atome dieselbe

Category: Chemistry-Inorganic Chemistry

Answer: D.

Figure 2: An example from the Chemistry-Inorganic of M4U dataset. The sample contains multiple images, and has multilingual contents in the question and images.

## 1 Introduction

Multilingual multimodal reasoning is an essential aspect of human intelligence. AI systems with strong multimodal reasoning capabilities have extensive applications, including automatic scientific discovery, autonomous driving, and healthcare. The rapid advancements in Large Language Models (LLMs)[Ope23, TLI \${ }^{+}\$23, \$\mathrm{JSM}^{+}\$23] have led to the development of Large Multimodal Models (LMMs) $\left[\mathrm{ABW}^{+} 23, \mathrm{LLL}^{+} 24, \mathrm{LLZ}^{+} 24\right.$, AI24], which demonstrate remarkable performance across a broad range of tasks, such as image captioning and visual question answering. Numerous benchmarks have been established to comprehensively evaluate these leading LMMs in real-world scenarios $\left[\mathrm{LDZ}^{+} 23 \mathrm{~b}, \mathrm{LWW}^{+} 23, \mathrm{LDZ}^{+} 23 \mathrm{a}, \mathrm{KLJ}^{+} 24, \mathrm{FCS}^{+} 23\right]$. Unlike perceptual tasks $\left[\mathrm{GKS}^{+} 17, \mathrm{CFL}^{+} 15\right]$, multimodal reasoning tasks, including mathematical reasoning $\left[\mathrm{LBX}^{+} 24\right]$ and scientific question answering [LMX $\left.{ }^{+} 22, \mathrm{KSK}^{+} 16\right]$, present significant challenges for neural models. These tasks necessitate an understanding of domain-specific knowledge and the ability to perform complex logical reasoning alongside visual content. Additionally, multilingual capability is crucial for real-world applications, as these models are typically deployed across various countries and languages.

Many datasets are curated to evaluate the capability of multilingual multimodal reasoning. However, the multimodal component of existing benchmarks $\left[\mathrm{ZAG}^{+} 23, \mathrm{DHL}^{+} 24, \mathrm{WCS}^{+} 23\right]$ is limited in scale. More importantly, we observe that the current data on multilingual multimodal reasoning suffers from significant data contamination and language disparities in task complexity. For instance, the multimodal section of M3Exam $\left[\mathrm{ZAG}^{+} 23\right]$ contains $61 \%$ high-difficulty questions in English, but only $23 \%$ high-difficulty questions in Chinese. Furthermore, as shown in Table 2, without any visual information, the multilingual LLM, Qwen-1.5-14B Chat, easily achieves high scores: $66.4 \%$ and $56.0 \%$ accuracy on the Chinese and English sections of M3Exam, respectively. This suggests that M3Exam struggles to differentiate between models of varying multimodal capabilities. Consequently, a systematic evaluation of multilingual multimodal understanding and reasoning for leading models remains largely unexplored.

To advance the development of multilingual LMMs, we introduce M4U, a novel and challenging benchmark for evaluating foundational models on expert-level multilingual multimodal understanding and reasoning. Specifically, we assembled a team of over 10 college and graduate students to collect a high-quality dataset and assess its difficulty and correctness. As shown in Figure 3, M4U consists of

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-03.jpg?height=675&width=669&top_left_y=253&top_left_x=381)

| Statistics | Values |
| :--- | :---: |
| Languages | CN, EN, DE |
| Total questions | 8,931 |
| Total disciplines / subfields | $64 / 16$ |
| Total image types | 13 |
| Image in the question | 8,271 |
| Image at the beginning | 6,205 |
| Image in the middle | 1,321 |
| Image at the end | 745 |
| Image in the options | 660 |
| Single / multiple image(s) | 8,199 / 732 |
| Maximum question length | 279 |
| Maximum option length | 63 |
| Average question length | 33.2 |
| Average option length | 6.1 |

Figure 3: Key statistics of M4U dataset. M4U covers a wide scope of tasks from Science, Engineering and Health in Chinese, English and German, and supports the interleaved vision-language documents.

8,931 multiple-choice questions covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare. To minimize the risk of data contamination, samples are collected from college exams and quizzes from online video lectures. Additionally, a significant portion (35\%) of the questions in M4U are written by our team based on textbooks. Figure 2 illustrates an example from the Chemistry-Inorganic part of M4U, demonstrating that our dataset requires expert-level multimodal reasoning and multilingual capability.

With M4U, we conduct a comprehensive evaluation, both quantitative and qualitative, on the zero-shot performance of 17 leading LMMs and 4 LLMs. Furthermore, we assess the performance of the LMMs with chain-of-thought prompting [WWS $\left.{ }^{+} 22, \mathrm{ZZL}^{+} 23\right]$ and the LLMs with external tools, such as a powerful captioning model. As shown in Table 3 in $\S 3.3$, the most advanced model, GPT-4o [Ope24], achieves only $47.6 \%$ average accuracy with zero-shot prompting on the M4U dataset, demonstrating the significant challenge M4U poses for existing models. Additionally, we observe significant language preferences among the leading LMMs: InstructBLIP Vicuna-7B achieves $29.8 \%$ accuracy on the English section, but only $13.7 \%$ and $19.7 \%$ accuracy on the Chinese and German sections, respectively. Further results (\$3.4) indicate that leading LMMs suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German. Our in-depth analysis (§4) reveals that the errors of GPT-4V(ision) are mainly due to limited perception ability, domain-specific knowledge, and reasoning. These findings demonstrate that LMMs still have significant room for improvement, particularly in multilingual multimodal reasoning.

## 2 The M4U Benchmark

### 2.1 Overview

In this section, we introduce M4U, a novel and challenging benchmark for assessing the multilingual multimodal understanding and reasoning of foundational models. To investigate whether differences exist in the multimodal reasoning capabilities of LMMs across different languages, we first construct the Chinese part of M4U and then translate it into English and German. This approach ensures that the domain-specific knowledge and reasoning abilities tested in different languages remain consistent. Specifically, we assemble a team of over 10 college students to collect questions from the Internet, textbooks, online video lectures, and college exams. Subsequently, a team of graduate students from related majors assessed the quality of the curated questions. Following this, we utilize GPT-4 Turbo (gpt-4-turbo-preview) to translate the questions into other languages, and then manually checked the quality of the translated questions.

| Benchmark | Multilingual | Multimodal | Size $^{*}$ | Difficulty | Fields |
| :--- | :---: | :---: | :---: | :---: | :---: |
| CMMLU $\left[\mathrm{LZK}^{+}\right.$23] | $x$ | $x$ | - | $\star \star \star$ | STEM, Humanities, etc. |
| C-Eval $\left[\mathrm{HBZ}^{+}\right.$23] | $x$ | $x$ | - | $\star \star \star$ | STEM, Humanities, ect. |
| MMLU $\left[\mathrm{HBB}^{+} 21\right]$ | $x$ | $x$ | - | $\star \star \star$ | STEM, Humanities, etc. |
| MathVista $\left[\mathrm{LBX}^{+}\right.$24] | $x$ | $\checkmark$ | 6,141 | $\star \star \star$ | Mathematics |
| CMMMU $\left[\mathrm{ZDC}^{+} 24\right]$ | $x$ | $\checkmark$ | 12,012 | $\star \star \star$ | Art, STEM, Humanities, etc. |
| MMMU $\left[\mathrm{YNZ}^{+}\right.$23] | $x$ | $\checkmark$ | 11,550 | $\star \star \star$ | Art, STEM, Humanities, etc. |
| MGSM $\left[\mathrm{SSF}^{+}\right.$22] | $\checkmark$ | $x$ | - | $\star \star \star$ | Mathematics |
| EXAMS-V $\left[\mathrm{DHL}^{+}\right.$24] | $\checkmark$ | $\checkmark$ | 1,221 | $\star \star$ | STEM, Humanities, etc. |
| M3Exam $\left[\mathrm{ZAG}^{+}\right.$23] | $\checkmark$ | $\checkmark$ | 2,816 | $\star$ | STEM, Humanities, etc. |
| M4U (ours) | $\checkmark$ | $\checkmark$ | 8,931 | $\star \star \star \star$ | STEM, Healthcare |

Table 1: Comparison between M4U and the existing benchmarks. *We report the size of test set of the multimodal part for the benchmarks.

| Benchmark | Chinese $\downarrow$ | English $\downarrow$ | German $\downarrow$ | Average $\downarrow$ |
| :--- | :---: | :---: | :---: | :---: |
| M3Exam $\left[\right.$ ZAG $^{+}$23] | $66.4(25.9)$ | $56.0(25.0)$ | - | $61.2(25.5)$ |
| M4U (ours) | $28.0(25.9)$ | 19.7 (25.9) | $27.6(25.9)$ | $25.1(25.9)$ |

Table 2: The zero-shot accuracy of the multilingual LLM, Qwen-1.5-14B-Chat \$\left[\mathrm{BBC}^{+}\right.\$23], on the multimodal part of M3Exam and M4U dataset. We provide the scores of random choices in blue as the reference baseline. The higher scores of LLM indicate that the multimodal benchmark requires less visual efforts or suffers from more serious data contamination during the training of LLMs.

The key statistics of M4U are detailed in Figure 3. M4U contains 8,931 multiple-choices questions, covering 64 subjects of 16 fields from Science (36.4\%), Engineering (28.7\%) and Health (34.9\%) in Chinese, English and German. Different from the prior work [ZAG $\left.{ }^{+} 23\right]$, M4U includes interleaved image-text documents: $8.2 \%$ of the questions have multiple images, while the images of $14.8 \%$ and $7.4 \%$ of the questions are placed in the middle of question and the options, respectively. The average length of questions and options is 33.2 words and 6.1 words. The image sources of M4U cover 13 categories in different scenarios, e.g., diagrams, technical blueprints and medical images.

We present a comparison of M4U with existing benchmarks in Table 1. Unlike MMMU [YNZ \${ }^{+}\$23] and CMMMU [ZDC $\left.{ }^{+} 24\right]$, our dataset focuses on the evaluation of multilingual multimodal reasoning. Furthermore, M4U is larger and has a more balanced distribution of difficulty across different languages compared to M3Exam [ZAG \${ }^{+}\$23]. This ensures a fair comparison of models' capabilities in multimodal reasoning within multilingual scenarios. More importantly, we implement strict collection guidelines and quality control measures to minimize the risk of data contamination. To quantitatively measure the level of data contamination in these benchmarks, we use the scores of LLMs without any visual information as a reference. Higher scores indicate that the benchmark is less effective at differentiating between multimodal models of varying performance. As shown in Table 2, without any visual information, Qwen-1.5-14B Chat easily achieves $66.4 \%$ and $56.0 \%$ accuracy on the Chinese and English parts of M3Exam, respectively, while it only achieves $25.1 \%$ average accuracy on our dataset. This suggests that M4U is more challenging and less exposed to the training corpus of LLMs.

### 2.2 Data Collection

Data sources. Following MMMU \$\left[\mathrm{YNZ}^{+}\right.\$23], we go through the educational programs of top universities, then select 64 subjects of 16 subfields from Science, Engineering and Healthcare whose applications highly rely on visual information. We recruit a team of over 10 college students to collect multiple-choices questions from public available sources. To minimize the risk of data contamination for foundation models, different from M3Exam $\left[\mathrm{ZAG}^{+} 23\right]$, we do not include the samples from the official exam papers, e.g., National Postgraduate Entrance Examination and national professional exams. Although these resources usually have higher quality and are well organized, they are also easy to be curated for the training of LLMs. Therefore, we carefully select the data sources for M4U: most questions of our dataset are collected from the quizzes of online video lectures and
college exams in PDF documents. Further $35 \%$ of questions are written by our team according to the textbooks. As shown in Table 3, the state-of-the-art open-source multilingual LLM, Qwen-1.5-14B Chat \$\left[\mathrm{BBC}^{+}\right.\$23], only has $25.1 \%$ average accuracy on M4U dataset without any visual information. This proves that our data is less exposed in the training of language models.

Data processing. The primary sources of M4U include college exams, the quizzes of online video lectures and the written questions. Most of college exams are uploaded by their students as images or scanned PDF documents, while the quizzes of online video lectures can be taken as the screenshot. We first adopt the OCR tools to convert these images into plain texts, then manually correct the potential errors of OCR results. Besides, we also write a large portion (35\%) of questions according to the textbooks. For the mathematical formulas and the chemical structures, we require the annotators to convert them into $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$ format. Since the samples of M4U may include multiple images in the questions or options, the annotators also annotate the location and type of each image (e.g. tables, blueprints and medical images).

After collecting the data, we design a two-stage post-processing pipeline to further improve the quality of M4U. We first design the guidelines to allow each annotator to score the collected samples from three dimensions: image quality, question description quality and the difficulty of visual understanding, and filter out the questions with average scores lower than 2.0. The detailed guidelines are summarized in the Appendix A.3. Then we recruit a team of graduate students of related major to assess the difficulty and quality of the curated questions. We further filter out the questions with the minimum visual efforts and the wrong answer. After that, we use GPT-4 Turbo to translate the Chinese part of M4U to English and German. Then the annotators will check and correct the potential errors introduced by machine translation.

### 2.3 Evaluation

We evaluate the zero-shot performance of 17 leading LMMs of different scales on M4U. The models are required to follow the instruction to directly generate the predicted option for each question. To minimize the format discrepancy between training and evaluation, we handle models that support interleaved image-text documents by inserting the visual tokens of each image into the corresponding position as in training. For models that only support image-text pairs as input, we place all visual tokens at the beginning of the sentence and use annotated positions (e.g., <image_1>, <image_2>) to refer to each image. Furthermore, we also evaluate the performance of various LMMs with chain-of-thought prompting $\left[W W S^{+} 22, \mathrm{ZZL}^{+} 23\right]$ and LLMs equipped with detailed visual captions.

As shown in Table 3, the existing models still lack the capability for expert-level multilingual multimodal understanding and reasoning: the most advanced GPT-4o achieves only $47.6 \%$ average accuracy with zero-shot prompting on the M4U dataset. Additionally, we observe that these models exhibit strong language preferences. For instance, InstructBLIP Vicuna-7B achieves $29.8 \%$ accuracy on the English part, but only $13.7 \%$ and $19.7 \%$ accuracy on the Chinese and German parts, respectively. These results indicate that there remains significant room for improvement in LMMs, particularly regarding multilingual capability and complex multimodal reasoning.

## 3 Experiments

### 3.1 Setup

We evaluate the performance of zero-shot learning for various LMMs and LLMs of different scales across different languages on M4U dataset. The models are prompted to directly generate the option's letter. Further we also evaluate the performance of the LMMs with chain-of-thought prompting [WWS \${ }^{+} 22, \mathrm{ZZL}^{+}\$23]: the models should first generate the rationale for the question and the options, then give the predicted option. For reference, we add the baseline of Random choices: we randomly select an option, and use the average accuracy of 30 runs with different seeds. We provide more details about the instruction prompt in Appendix A.4.

### 3.2 Models

LMMs. For the open-source models, we select VisualGLM [DQL \${ }^{+}\$22], Ying-VLM [LYL \${ }^{+}\$23], InstructBLIP series [DLL \${ }^{+}\$23], InternLM-XComposer [ZDW \${ }^{+}\$23], CogVLM-Chat [WLY \${ }^{+}\$23],

| Models | Size | Chinese $\uparrow$ | English $\uparrow$ | German $\uparrow$ | Average $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Random choices | - | 25.9 |  |  |  |
| Large Language Models |  |  |  |  |  |
| Qwen-1.5-7B-Chat [BBC $\left.{ }^{+} 23\right]$ | 7B | 29.5 | 15.0 | 28.5 | 24.3 |
| Qwen-1.5-14B-Chat $\left[\mathrm{BBC}^{+} 23\right]$ | $14 \mathrm{~B}$ | 28.0 | 19.7 | 27.6 | 25.1 |
| Augmented Large Language Models (+ Visual Caption) |  |  |  |  |  |
| Mistral-Instruct-v0.2-7B [JSM ${ }^{+}$23] | $7 \mathrm{~B}$ | 24.9 | 24.9 | 26.9 | 25.6 |
| Gemini 1.0 Pro $\left[\mathrm{ABW}^{+} 23\right]$ | - | 31.6 | 31.1 | 30.9 | 31.2 |
| Qwen-1.5-7B-Chat $\left[\mathrm{BBC}^{+} 23\right]$ | 7B | 34.2 | 27.7 | 31.7 | 31.2 |
| Qwen-1.5-14B-Chat $\left[\mathrm{BBC}^{+}\right.$23] | $14 \mathrm{~B}$ | 32.7 | 32.0 | 33.8 | 32.8 |
| Large Multimodal Models |  |  |  |  |  |
| VisualGLM [DQL+ 22$]$ | $6 \mathrm{~B}$ | 8.7 | 22.4 | 13.5 | 14.9 |
| Ying-VLM [LYL ${ }^{+}$23] | 13B | 22.3 | 11.2 | 15.6 | 16.4 |
| InstructBLIP-Vicuna-13B $\left[\mathrm{DLL}^{+}\right.$23] | $13 \mathrm{~B}$ | 10.5 | 23.4 | 18.6 | 17.5 |
| InstructBLIP-Vicuna-7B $\left[\mathrm{DLL}^{+}\right.$23] | $7 \mathrm{~B}$ | 13.7 | 28.1 | 19.7 | 20.5 |
| LLaVA-NeXT-Vicuna-7B [LLL+24] | 7B | 11.8 | 29.8 | 28.2 | 23.3 |
| LLaVA-NeXT-Vicuna-13B [LLL+24] | 13B | 21.9 | 30.9 | 29.3 | 27.4 |
| Qwen-VL-Chat $\left[\mathrm{BBY}^{+} 23\right]$ | 7B | 29.7 | 29.9 | 27.1 | 28.9 |
| CogVLM-Chat [WLY ${ }^{+}$23] | $7 \mathrm{~B}$ | 28.9 | 30.2 | 28.5 | 29.2 |
| LLaVA-NeXT-Mistral-7B [LLL ${ }^{+}$24] | $7 \mathrm{~B}$ | 28.2 | 30.6 | 29.4 | 29.4 |
| InternLM-XComposer [ZDW ${ }^{+}$23] | $7 \mathrm{~B}$ | 31.8 | 31.6 | 29.1 | 30.8 |
| DeepSeek-VL [LLZ $\left.{ }^{+} 24\right]$ | 7B | 30.4 | 32.8 | 30.8 | 31.3 |
| Yi-VL-6B [AI24] | 6B | 33.4 | 31.4 | 29.7 | 31.5 |
| Yi-VL-34B [AI24] | 34B | 33.5 | 33.3 | 30.5 | 32.4 |
| Gemini 1.0 Pro $\left[\mathrm{ABW}^{+} 23\right]$ | - | 34.9 | 32.7 | 30.8 | 32.8 |
| LLaVA-NeXT-34B [LLL+24] | 34B | 38.5 | 36.2 | 35.2 | 36.6 |
| GPT-4V(ision) [Ope23] | - | 39.7 | 39.4 | 37.3 | 38.8 |
| GPT-4o [Ope24] | - | 49.4 | 47.8 | 45.6 | 47.6 |
| Augmented Large Multimodal Models |  |  |  |  |  |
| Gemini 1.0 Pro $\left[\mathrm{ABW}^{+} 23\right]+\mathrm{CoT}$ | - | 34.4 | 34.2 | 33.9 | 34.2 |
| GPT-4V(ision) [Ope23] + CoT | - | 43.9 | 43.6 | 40.3 | 42.6 |

Table 3: The zero-shot accuracy of various LLMs, augmented LLMs and LMMs on M4U dataset. CoT is short for chain-of-thought prompting.

Qwen-VL-Chat [BBY + 23], Yi-VL-series [AI24], DeepSeek-VL [LLZ \${ }^{+}\$24] and LLaVA-NeXT series \$\left[\mathrm{LLL}^{+}\right.\$24]. For closed source models, we choose Gemini 1.0 Pro [ABW \${ }^{+}\$23], GPT-4V(ision) [Ope23] and GPT-4o [Ope24] using the provided API, gemini-pro-vision, gpt-4-vision-preview and gpt-4o, respectively. As for the augmented LMMs, we evaluate the performance of Gemini 1.0 Pro and GPT-4V with the chain-of-thought prompting [WWS ${ }^{+} 22$, $\left.\mathrm{ZZL}^{+} 23\right]$.

LLMs. We select Mistral-Instruct-v0.2-7B [JSM \${ }^{+}\$23], Qwen-1.5-7B-Chat, Qwen-1.5-14BChat \$\left[\mathrm{BBC}^{+}\right.\$23] and Gemini 1.0 Pro (gemini-pro) for the open and closed source LLMs. We use Gemini 1.0 Pro (gemini-pro-vision) to generate the detailed caption in Chinese, English and German for each image. The visual captions are placed at the beginning of the prompt. The annotated image positions are used to refer each image.

### 3.3 Main results

In this section, we present the comprehensive evaluation results of 17 leading LMMs and 4 LLMs with different prompt strategies. Table 3 demonstrates the performance of various LMMs and LLMs across Chinese, English and German on M4U dataset.

| Models | Size | Chinese $\uparrow$ | English $\uparrow$ | German $\uparrow$ | Average $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: |
| DeepSeek-VL [LLZ ${ }^{+}$24] | 7B | 32.8 | 34.0 | 33.3 | 33.4 |
| Yi-VL-6B [AI24] | 6B | 39.2 | 34.3 | 30.1 | 34.5 |
| Gemini 1.0 Pro [ABW ${ }^{+}$23] | - | 38.0 | 36.3 | 32.9 | 35.7 |
| Yi-VL-34B [AI24] | $34 B$ | 41.6 | 38.7 | 34.2 | 38.2 |
| LLaVA-NeXT-34B [LLL 24$]$ | $34 B$ | 44.6 | 40.9 | 36.1 | 40.5 |
| GPT-4V(ision) [Ope23] | - | 45.3 | 41.2 | 38.2 | 41.6 |
| GPT-4o [Ope24] | - | $\mathbf{5 2 . 0}$ | $\mathbf{4 7 . 5}$ | $\mathbf{4 5 . 2}$ | $\mathbf{4 8 . 2}$ |
| Gemini 1.0 Pro [ABW ${ }^{+}$23] + CoT | - | 38.1 | 35.7 | 37.8 | 37.2 |
| GPT-4V(ision) [Ope23] + CoT | - | $\mathbf{4 6 . 7}$ | $\mathbf{4 8 . 0}$ | $\mathbf{4 2 . 6}$ | $\mathbf{4 5 . 8}$ |

Table 4: The zero-shot accuracy of various LMMs, augmented LMMs on the cross-lingual set of M4U dataset. CoT is short for chain-of-thought prompting.

For the text-only LLMs, we first only use the text part of question to prompt these models. As shown in Table 3, Qwen-1.5-14B Chat has only $25.1 \%$ average accuracy on M4U dataset, which is lower than $25.9 \%$ of random choices. It proves that $\mathrm{M} 4 \mathrm{U}$ requires significant visual efforts to answer these questions, and does not suffer from the data contamination during the training of LLMs. Further, we equip these LLMs with the detailed visual captions generated by Gemini 1.0 Pro. Qwen-1.5-14B Chat with additional captions outperforms itself without any visual information by a gain of $7.7 \%$, and achieves $32.8 \%$ average accuracy, the highest scores among the baselines. Mistral-Instruct-v0.2-7B has $25.6 \%$ average accuracy, since it does not follow the instruction to generate the valid option. We observe that Mistral-Instruct-v0.2-7B tends to reject to give an answer when not being provided with enough visual information.

For LMMs, most of them do not have the satisfactory results on M4U dataset. The state-of-the-art model, GPT-4o, achieves only $47.6 \%$ average accuracy with zero-shot prompting. It indicates that M4U is quite challenging for the existing models, and the reasoning capability of the multimodal models still has much room for future improvement. With the powerful LLM, Nous-Hermes Yi-34B ${ }^{2}$, LLaVA-NeXT-34B scores highest among the open-source LMMs, even significantly outperforms Gemini 1.0 Pro by a gain of $3.8 \%$ on average accuracy. As for augmented LMMs, chain-of-thought prompting further boosts the performance. GPT-4V with chain-of-thought prompting outperforms itself with zero-shot prompting by a gain of $3.8 \%$ on average accuracy. It demonstrates that explicitly generating the reasoning steps is also beneficial for complex multimodal reasoning.

Furthermore, we observe that the existing models has strong language preferences on multilingual multimodal reasoning tasks. InstructBLIP Vicuna-7B achieves $28.1 \%$ accuracy on the English part of M4U, while only has $13.7 \%$ and $19.7 \%$ accuracy on the Chinese and German part, respectively. For GPT-4V, the average accuracy on the Chinese and English is both 3\% higher than it on the German. Besides, we observe that the effect of chain-of-thought prompting also differs across different languages. For instance, chain-of-thought improves the performance of Gemini 1.0 Pro on English and German part by a gain of $1.5 \%$ and $3.1 \%$ accuracy, while leads to a degradation of $0.5 \%$ on Chinese part. We argue that this results from the lack of the multilingual vision-language corpus used for multimodal training, and the LLMs of these LMMs (e.g., Vicuna-7B, Vicuna-13B) do not well support the multilingual capability.

### 3.4 Cross-lingual Multimodal Evaluation

To measure the cross-lingual multimodal capability of the LMMs, we select a subset from M4U: the image of each sample in this subset contains the text that labels or explains the key concepts or objects in the picture, while the textual description of the question is multilingual. For example, as illustrated in Figure 2, the visual content contains the key text in Chinese that labels bond length between atoms and explains the single-layer structure of the material, and the question part is multilingual. The models are required to perform complex reasoning given the multilingual both textual and visual contents. The cross-lingual set contains 1,065, 417 and 531 samples from Science, Engineering and Healthcare, respectively, resulting in up to 2,013 samples in total.[^1]

| Models | Chinese |  |  | English |  |  | German |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Health | Sci. | Eng. | Health | Sci. | Eng. | Health | Sci. | Eng. |
| Yi-VL-6B | 31.2 | 34.1 | 34.9 | 32.1 | 32.2 | 30.0 | 29.0 | 29.2 | $30.8 \quad$ |
| DeepSeek-VL | 40.1 | $\overline{22.6}$ | $\overline{28.5}$ | 38.0   | 31.9 | 28.6 | 35.2 | 29.3 | 27.8 |
| Yi-34B | 32.9 | 34.1 | 33.6 | 34.0 | $33.2 \quad 2$ | 32.6 | $\overline{29.4}$ | 30.2 | $\underline{32.0}$ |
| LLaVA-NeXT-34B | $\underline{38.1}$ | $\overline{40.4}$ | 37.0 | 37.2 | $\overline{36.8}$ | $\overline{34.7}$ | 36.9 | $\overline{34.2}$ | $\overline{34.5}$ |
| Gemini 1.0 Pro | 38.8 | 34.5 | 31.4 | 34.9 | 33.4 | 29.8 | 33.1 | 30.5 | 28.8 |
| + Chain-of-thou | 37.8 | 33.3 | 32.6 | 38.8 | 33.3 | 30.6 | 37.8 | 32.2 | 31.8  |
| GPT-4V(ision) | 41.9 | 39.3 | 37.9 | 43.9 | 37.8 | 36.6 | 41.1 | 36.0 | 34.6 |
| + Chain-of-thought prompting | 43.9 | 46.2 | 41.7 | 45.8 | 43.3 | 41.9 | 42.5 | 39.1 | 39.3 |
| GPT-4o | ![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-08.jpg?height=42&width=113&top_left_y=641&top_left_x=853) | $\overline{47.3}$ | $\overline{45.0}$ | $\overline{56.2}$ | $\overline{44.3} \quad$ | $\overline{42.8}$ | $\overline{52.9}$ | $\overline{40.9}$ | $\overline{43.0} \quad$ |

Table 5: The detailed results of different LMMs on Health, Science and Engineering of M4U dataset. Sci. and Eng. are short for Science and Engineering, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-08.jpg?height=463&width=1374&top_left_y=831&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-08.jpg?height=387&width=715&top_left_y=842&top_left_x=369)

(a) Image types

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-08.jpg?height=301&width=590&top_left_y=847&top_left_x=1147)

(b) Image positions

Figure 4: The zero-shot accuracy of different LMMs on different image types (Left) and positions (Right) on M4U dataset.

We evaluate the performance of different LMMs on the cross-lingual set of M4U. As shown in Table 4, almost all models suffer from a degradation of performance when the image contains the key textual information in Chinese but the question is English or German. It shows that these models are short for following multilingual instructions to understand the visual contents with the textual information of another language. Furthermore, as for the augmented LMMs, we observe that the chain-of-thought prompting significantly improves the performance of GPT-4V on English and German. This is aligned with our previous evaluations on the full set of M4U in Table 3.

### 3.5 Fine-grained results

Different Disciplines and Languages. We present the detailed results of various LMMs on different fields of Chinese, English and German in Table 5. GPT-4o outperforms the other models by large improvements on all fields of all languages. For the open-source models, we observe that LLaVA-NeXT-34B shows impressive results on scientific reasoning, and DeepSeek-VL demonstrates good performance on Health. Further, we observe that on Science, the chain-of-thought prompting significantly improves the performance of GPT-4V by a gain of over $6 \%$ accuracy in Chinese and English, while only boosts the performance by an improvement of $3.1 \%$ accuracy on German. The similar phenomenon also exists for Gemini 1.0 Pro. On Health part, Gemini 1.0 Pro with the chain-ofthought prompting outperforms it with zero-shot prompting by a gain of $4.9 \%$ and $4.7 \%$ on English and German, but it leads to a degradation of $1.0 \%$ accuracy on Chinese. These results show that the effect of the chain-of-thought prompting also differ from different languages.

Different Image Types and Positions. We demonstrate the visualization of the detailed results of various LMMs on different image types and positions in Figure 4. We reclassified 13 image types into 7 categories based on the style and application of the image. As shown in Figure 4a, GPT-4o shows impressive performance on the image type of "Plots \& Charts" and "Medical" compared with the other models, but has unsatisfactory results on Blueprints. We argue that this is because the Blueprints

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-09.jpg?height=544&width=1200&top_left_y=248&top_left_x=468)

Lack of Knowledge

Textual Understanding

(a) Chinese
Perceptual Error

Annotation Error

(b) English
Reasoning Error

Answer Extraction Error

(c) German

Figure 5: The distribution of the wrong cases of GPT-4V in different languages.

contain many engineering sketches that require the capability of the fine-grained perception and domain-specific knowledge about engineering standards. M4U not only supports the image-text pairs as the input, but includes interleaved image-text documents. Thus, we conduct the analysis about the performance of the selected LMMs on different positions of the images. We divide these questions into four groups according to the image position: image at the beginning, end, middle of the question and in the options. As shown in Figure 4b, on the questions with images in the options, GPT-4o and GPT-4V outperform the other models by a large gain, and LLaVA-NeXT-34B performs poorly on this types of the questions. We argue that this is because the LLaVA-NeXT series are only trained with a high-quality corpus of image-text pairs. Instead DeepSeek-VL is pre-trained with a large mixture of image-text pairs and interleaved documents, and it does not suffer from a significant degradation of performance on the questions with images in the options.

## 4 Qualitative Analysis

In this section, we conduct qualitative analysis for the results of GPT-4V with the chain-of-thought prompting. We randomly sample 75 questions (2.5\%) from different disciplines of each language. In these instances, GPT-4V has errors in responses and analysis in at least one language. We analyze the cause of these wrong cases, and divided them into 6 categories: perceptual error, lack of knowledge, reasoning error, textual understanding, annotation error and answer extraction error. Furthermore, we illustrate the distribution of the selected samples across different categories in Figure 5. Perceptual error, lack of knowledge, and reasoning error account for the major causes of failed cases ( $96 \%$ in Chinese, $95 \%$ in English, and $92 \%$ in German). GPT-4V tends to exhibit lack of knowledge on the Chinese part of M4U, while reasoning errors are more likely to occur in German and English. We present the summary of the top-3 types of failed cases in the following. More results can be found in the Appendix B.

Perceptual Error. Perceptual error is the most frequent error made by GPT-4V. It corresponds to the illusion phenomenon that occurs when extracting visual information from images provided by the questions. These kinds of hallucination could be divided in two main categories: visual information deficiency and misinterpretation. As presented in Figure 21, visual information deficiency occurs when GPT-4V overlooked crucial conditions and information provided in the image associated with the question, such as dimensions and scales annotated in engineering blueprints and numerical values provided in physics experiments thereby failing to complete the reasoning chain. Figure 15 shows a typical case for the visual information misinterpretation: the extracted information is complete but contains mistakes. A portion of these mistakes are common perceptual errors in OCR and visual localization.

Lack of Knowledge. We define the lack of knowledge as the model has factual misunderstanding about the key concepts in questions and provides erroneous premise to the reasoning process. Figure 23 illustrates that GPT-4V ignores that the amplitude needs to be judged on an equivalent basis
to the same period, and as shown in Figure 24, the model equates the average kinetic energy of a molecule to the kinetic energy of a single molecule, overlooking key preconditions of physical laws.

Reasoning Error. The reasoning error is categoried as the mathematical miscalculations and logical errors in the analysis procedure, which often occur in subjects need numerical computations and logical inference, such as math, physics, and electronics. As demonstrated in Figure 25, GPT-4V only considers the power supply on the left and does not consider the power supply on the right.

## 5 Conclusion and Future Work

In this work, we introduce M4U, a novel and challenging benchmark for evaluating the capability of multilingual multimodal understanding and reasoning. M4U contains 8,931 multiple-choice questions, covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. Table 2 demonstrates that M4U requires significant visual efforts and has less exposure to the training corpus of LLMs compared with M3Exam. As shown in Table 3, the state-of-the-art model, GPT-4o, achieves only $47.6 \%$ average accuracy with zero-shot prompting, indicating that M4U is quite challenging for existing models. Furthermore, we observe that the leading LMMs exhibit significant language preferences. These results demonstrate that there is still significant room for improvement in LMMs, especially in expert-level multilingual multimodal reasoning. In the future, we aim to extend M4U to support more languages (e.g., Japanese and French) and investigate the performance of multilingual LMMs on questions associated with cultural backgrounds (e.g., history and politics). Additionally, we plan to include multilingual rationales for M4U to construct a fine-grained metric that considers the correctness of both reasoning steps and final predictions.

## 6 Acknowledgments

We would like to acknowledge Jiayu Gao, Jialu Ma, Wenbo Nie, Tianze Xia, Jiawei Zhang, MingYang Zhang and Weihao Kong for their contribution in data collection and quality control.

## References

\$\left[\mathrm{ABW}^{+}\right.\$23] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023.

[AI24] 01. AI. Yi: Open foundation models by 01.ai. arXiv:2403.04652, 2024.

\$\left[\mathrm{BBC}^{+}\right.\$23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023.

\$\left[B B Y^{+}\right.\$23] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023.
\$\left[\mathrm{CFL}^{+}\right.\$15] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015.

[DHL \${ }^{+}\$24] Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov. Exams-v: A multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. arXiv preprint arXiv:2403.10378, 2024.

[DLL + 23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, 2023.

\$\left[\mathrm{DQL}^{+}\right.\$22] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: general language model pretraining with autoregressive blank infilling. In $A C L, 2022$.

[FCS \${ }^{+}\$23] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: A comprehensive evaluation benchmark for multimodal large language models. CoRR, abs/2306.13394, 2023.

[GKS \${ }^{+}\$17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the $\mathrm{V}$ in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR 2017, pages 6325-6334. IEEE Computer Society, 2017.

\$\left[\mathrm{HBB}^{+}\right.\$21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR 2021. OpenReview.net, 2021.

\$\left[\mathrm{HBZ}^{+}\right.\$23] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023.

\$\left[\mathrm{JSM}^{+}\right.\$23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023.

\$\left[K L J^{+}\right.\$24] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, PoYu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. CoRR, abs/2401.13649, 2024.

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-11.jpg?height=47&width=1368&top_left_y=1928&top_left_x=384)
and Ali Farhadi. A diagram is worth a dozen images. In ECCV 2016, volume 9908 of Lecture Notes in Computer Science, pages 235-251. Springer, 2016.

\$\left[\mathrm{LBX}^{+}\right.\$24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024.

\$\left[\mathrm{LDZ}^{+}\right.\$23a] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP 2023, pages 292-305. Association for Computational Linguistics, 2023.

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-11.jpg?height=49&width=1387&top_left_y=2385&top_left_x=369)
Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023.

[LLL + 24] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

[LLZ \${ }^{+}\$24] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding 2024.

\$\left[\mathrm{LMX}^{+}\right.\$22] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.

\$\left[\mathrm{LWW}^{+}\right.\$23] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. CoRR, abs/2307.16125, 2023.

\$\left[\mathrm{LYL}^{+}\right.\$23] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. $\mathrm{M}^{3}$ it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv:2306.04387, 2023.

![](https://cdn.mathpix.com/cropped/2024_06_04_9726324388ead4a6f952g-12.jpg?height=44&width=1366&top_left_y=1062&top_left_x=388)
and Timothy Baldwin. CMMLU: measuring massive multitask language understanding in chinese. CoRR, abs/2306.09212, 2023.

[Ope23] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.

[Ope24] OpenAI. Hello gpt-4o, May 2024.

\$\left[\mathrm{SSF}^{+}\right.\$22] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners, 2022.

[TLI \${ }^{+}\$23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.

[WCS \${ }^{+}\$23] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. Cmb: A comprehensive medical benchmark in chinese. arXiv:2308.08833, 2023.

\$\left[\mathrm{WLY}^{+}\right.\$23] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. CoRR, abs/2311.03079, 2023.

[WWS \${ }^{+}\$22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022.

[YNZ+\${ }^{+}\$23] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR, abs/2311.16502, 2023.

\$\left[\mathrm{ZAG}^{+}\right.\$23] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. CoRR, abs/2306.05179, 2023.
\$\left[\mathrm{ZDC}^{+}\right.\$24] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, and Jie Fu. CMMMU: A chinese massive multi-discipline multimodal understanding benchmark. CoRR, abs/2401.11944, 2024.

[ZDW \${ }^{+}\$23] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition, 2023.

[ZZL \${ }^{+}\$23] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. CoRR, abs/2302.00923, 2023.
