# DISTRIBUTIONAL PREFERENCE LEARNING: Understanding And ACCOUNTING FOR HidDen CONTEXT IN RLHF 

Anand Siththaranjan * Cassidy Laidlaw *<br>University of California, Berkeley<br>\{anandsranjan,cassidy_laidlaw\}@cs.berkeley.edu

Dylan Hadfield-Menell<br>Massachusetts Institute of Technology<br>dhm@csail.mit.edu


#### Abstract

In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at https://github.com/cassidylaidlaw/hidden-context.


## 1 INTRODUCTION

Encoding human preferences and values into interactive learning systems is an essential component for making those systems safe and socially beneficial. To accomplish this, modern machine learning models, such as large language model (LLM) chatbots like ChatGPT and Claude, are trained with feedback from human evaluators. This method, often called reinforcement learning from human feedback (RLHF), seeks to align system behavior with the preferences of annotators. In this paper, we study how RLHF infers preferences when there is hidden context that influences human evaluations.

Hidden context is any information that affects preference annotations but is not given as input to the learned utility or reward model. It can arise through several mechanisms. For instance, when feedback is collected from many different people, annotator identity is hidden context: it affects the annotations, since different annotators could have very different preferences, but it is not input to[^0]

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-02.jpg?height=314&width=1390&top_left_y=274&top_left_x=362)

Figure 1: We analyze the effects of hidden context on preference learning, which is one of the key steps in reinforcement learning from human feedback (RLHF). Hidden context is any information that affects the annotator's assessment of the utility of different alternatives, but is not input to the learned utility or reward model. Our framework emcompasses many potential issues with preference learning, including human irrationality, diverse preferences among annotators, and combining multiple objectives (Section 2). We prove that preference learning implicitly aggregates over hidden context using a rule called Borda count (Section 3).

the reward model, since the annotators' data is combined anonymously. Other sources of hidden context include human irrationality and evaluation according to multiple objectives.

To motivate the consequences of naive preference learning with hidden context, consider the following hypothetical scenario:

Example 1.1. A company has developed an AI assistant to help high school students navigate college admissions. They implement RLHF by asking their customers for feedback on how helpful the chatbot's responses are. Among other questions, this process asks users whether or not they prefer to see information about the Pell Grant, an aid program for low-income students. Because the population of customers is biased towards high-income students, most feedback indicates that users prefer other content to content about the Pell Grant. As a result, RLHF trains the chatbot to provide less of this kind of information. This marginally improves outcomes for the majority of users, but drastically impacts lower-income students, who rely on these recommendations to understand how they can afford college.

The heart of this issue is that common preference learning approaches assume that all relevant features are provided as input to the reward model. However, when there is hidden context-which is almost always the case-this assumption is false. As a result, standard methods can have unexpected and undesirable consequences. In Example 1.1, relevant context about the annotator's identity (i.e. their income level) is missing from the data. The implicit aggregation over preferences biases the outcome in favor of high-income applicants. In this work, we take steps to better understand the implications of unobserved context in preference learning and consider technical approaches to identify when such situations occur.

In Section 2 we present a formal model of preference learning with hidden context. We show that our model can represent many challenges in preference learning, such as combining data from different users, accounting for irrationality, and optimizing for multiple objectives. Since these challenges are ubiquitous, understanding their implications is crucial for safely deploying RLHF-trained models.

In Section 3, we use our model to develop theoretical results on the consequences of hidden context in preference learning. First, we provide a precise characterization of the utility function that preference learning will output when there is hidden context. In particular, we show that preference learning implicitly aggregates over hidden context using a rule called the Borda count. We explore the implications of this finding, identifying cases when Borda account aggregates preferences in unintuitive ways quite different from other methods like regression. Furthermore, when data is combined from many annotators, preference learning implicitly defines a social welfare functional that aggregates their preferences. We use existing results from the social choice literature to expose another problem arising from hidden context: annotators may have an incentive to misreport their preferences to influence the learned reward function.

Next, we consider the design of preference learning methods that more gracefully account for hidden context. In Section 4, we propose distributional preference learning (DPL). DPL estimates a distribution over utility values for each input instead of a single real-valued output. This allows the
method to detect situations where unobserved context could influence preferences. We show how DPL can detect the effects of missing features through an explained variance $\left(r^{2}\right)$ metric.

We validate DPL in two ways. First, we conduct a small-scale synthetic experiment with a 1dimensional space of alternatives that allows us to directly compare to Borda count. Next, we apply DPL to a real-world dataset of preferences for use in RLHF. In this case, the preference data is collected according to two distinct objectives. In one subset of the data, raters were asked to prefer helpful and honest responses. In the other subset, raters were asked to prefer responses that did not respond to harmful requests. This introduces hidden context because the single reward model is trained on the combined data. We find that DPL is able to identify this hidden context automatically and identifies the uncertainty when these competing goals are at odds.

Beyond identifying potential instances of relevant hidden context, our experiments indicate that DPL can be used to develop guardrails that protect against jailbreaks. Wei et al. (2023) showed that many jailbreaks succeed by pitting the helpfulness and harmlessness objectives of chatbots against one another. This means that some jailbreaks can be understood as a consequence of hidden context. As a result, it is possible to detect this class of jailbreaks by leveraging the distribution of utilities we get from DPL. In particular, risk-aversion with respect to the distribution of learned utilities can dramatically reduce the rate at which the preference model prefers jailbroken responses. This is because DPL models capture the disagreement in the training dataset between the two objectives. Thus, risk-aversion penalizes responses that cause the two objectives to diverge.

We summarize our contributions as follows:

1. we identify and formally characterize the problem of preference learning with hidden context, and describe a number of settings where it may arise;
2. we show that preference learning with hidden context implicitly implements Borda count, which can have counter-intuitive implications and introduce incentives for annotators to misrepresent their preferences;
3. we introduce distributional preference learning and show that it can detect and mitigate some effects of hidden context in LLM-based preference models.

## 2 Setting AND RELATED WORK

We begin by formally describing the problem of preference learning with hidden context. We first review the standard preference learning framework and then introduce our extension that accounts for hidden context.

In standard preference learning, the goal is to estimate an unknown utility function $u: \mathcal{A} \rightarrow \mathbb{R}$ which assigns a utility value to each of a finite set of alternatives $\mathcal{A}$. For instance, in the case of a chatbot, the alternatives could be the possible responses to a prompt, and the utility function would describe how much a particular response is preferred. To estimate $u$, we observe the outcome of comparisons between pairs of alternatives $(a, b)$. We assume there is a fixed probability for any pair of alternatives $(a, b)$ that $a$ will be preferred to $b$; we denote this probability $p_{u}(a, b)$ and assume that $p_{u}(a, b)+p_{u}(b, a)=1$; that is, the order in which the alternatives are presented does not matter. In the ideal case, comparison outcomes would exactly reflect the utility function, i.e., $p_{u}(a, b)=\mathbf{1}\{u(a)>u(b)\}$. Realistically, however, preference comparison data never exactly follows a single utility function. To account for the fact that people are noisy and/or inconsistent in their feedback, a common assumption is that instead preference comparisons are made according to a Bradley-Terry-Luce (BTL) model (Rajkumar \& Agarwal, 2014), also sometimes known as Boltzmann-rational model (Jeon et al., 2020):

$$
\begin{equation*}
p_{u}^{\mathrm{BTL}}(a, b)=\frac{e^{u(a)}}{e^{u(a)}+e^{u(b)}} \tag{1}
\end{equation*}
$$

In (1), the higher $u(a)$ is compared to $u(b)$, the more likely the outcome of the comparison is to prefer $a$ to $b$; as the utilities for $a$ and $b$ are closer, the comparison outcome moves towards uniformly random.

The most commonly used method for estimating the utility function $u$ from preference data is to fit the maximum likelihood estimator (MLE) under the BTL model in (1). To derive the MLE, we
consider the limit of infinite data and assume that preference comparisons are elicited for uniformly randomly selected pairs of alternatives. The MLE for the utility function $\hat{u}$ is given by minimizing the following loss function:

$$
\begin{align*}
\hat{u} & =\arg \min _{\hat{u}} L(\hat{u} ; u)  \tag{2}\\
& =\arg \min _{\hat{u}} \frac{1}{|\mathcal{A}|(|\mathcal{A}|-1)} \sum_{a \neq b}-p_{u}(a, b) \log \left(\frac{e^{\hat{u}(a)}}{e^{\hat{u}(a)}+e^{\hat{u}(b)}}\right)-\left(1-p_{u}(a, b)\right) \log \left(\frac{e^{\hat{u}(b)}}{e^{\hat{u}(a)}+e^{\hat{u}(b)}}\right)
\end{align*}
$$

Although $\hat{u}$ might be chosen from a parametric class like a neural network, we assume for theoretical purposes that $L(\hat{u} ; u)$ is optimized over all possible $\hat{u}: \mathcal{A} \rightarrow \mathbb{R}$. In some cases, $L$ may not have any minimum, so we consider a regularized version of (2) where $\lambda>0$ is a tunable coefficient:

$$
\begin{equation*}
\hat{u}=\arg \min _{\hat{u}} L(\hat{u} ; u)+\frac{\lambda}{2} \sum_{a \in \mathcal{A}} \hat{u}(a)^{2} \tag{3}
\end{equation*}
$$

The optimization objective in (3) is strongly convex and thus has a unique global minimum; see Appendix A. 1 for a proof.

### 2.1 HIDDEN Context

While preference learning based on (3) has been widely deployed and enjoyed some success, it rests on assumptions that often do not hold in practice. In particular, irrationality, partial observability, and diversity of preferences among a population all challenge the BTL model on which the usual preference learning loss is based. We argue that all of these cases can be understood as special cases of a general phenomenon: hidden context. For concreteness, consider again Example 1.1. The key problem in the example is a mismatch between the information that influences the user's feedback and the information that the preference learning algorithm uses to estimate utilities based on that feedback. The user gives feedback that depends on their financial situation, while the learned utility model observes request-response pairs. Thus, the preference learning algorithm must produce a single ordering over alternatives that implicitly aggregating feedback over the hidden context of whether the user is high- or low-income.

To model hidden context in preference learning, we extend the preference learning formalization to utility functions $u: \mathcal{A} \times \mathcal{Z} \rightarrow \mathbb{R}$ over a space of observed features $a \in \mathcal{A}$ and hidden context $z \in \mathcal{Z}$. Let $\mathcal{D}_{z}$ be a distribution over $\mathcal{Z}$. In Example 1.1, $z \in\{0,1\}$ could represent whether the user is lowor high-income; then perhaps $z \sim \mathcal{B}(0.8)$ if $80 \%$ of users are high-income (where $\mathcal{B}(p)$ represents a Bernoulli random variable with mean $p$ ). Given $u(a, z)$ and $\mathcal{D}_{z}$, we can calculate the probability that one alternative $a$ is chosen over another $b$ given that $z$ is hidden:

$$
p_{u, \mathcal{D}_{z}}(a, b)=\mathbb{E}_{z \sim \mathcal{D}_{z}}\left[O_{u}(a, b, z)\right] \quad \text { where } \quad O_{u}(a, b, z)= \begin{cases}1 / 2 & \text { if } u(a, z)=u(b, z)  \tag{4}\\ \mathbf{1}\{u(a, z)>u(b, z)\} & \text { o.w. }\end{cases}
$$

$p_{u, \mathcal{D}_{z}}$ marginalizes over the distribution of the hidden context $z$ and thus reflects the comparison data available to the preference learning algorithm. Our model of hidden contexts can represent many settings where preference learning is difficult:

Partial observability. There may be variables that are observable by the human making preference comparisons but not by the AI system, which learns from that data. For instance, suppose annotators' preferences depend on the day of the week or the month of the year, but the estimated utility function ignores the date the comparisons were made.

Multiple objectives. System designers may combine data about user preferences over multiple, different objectives. For instance, the Anthropic HH-RLHF dataset (Bai et al., 2022a) contains one subset with comparisons of chatbot responses based on harmlessness and another subset with comparisons based on helpfulness. When these subsets are combined, the objective that was used to make the comparison (in this case, either harmlessness or helpfulness) is a hidden context. We explore this case more in Section 5.

Population with diverse preferences. Preference learning is almost always applied to data aggregated from many annotators who may have very different utility functions (e.g., Bai et al. (2022a) observe high intra-annotator disagreement). If $z$ represents the annotator who makes a comparison,
then $u(\cdot, z)$ could represent the utility function for that annotator. However, when the data is used to train a single utility function $\hat{u}(\cdot)$, then the annotator's identity $z$ is a hidden context.

Irrational and noisy decisions. Various types of irrationality could be modeled as unseen latent variables that affect a person's decision-making. For instance, to represent a person making noisy utility estimates, one could let $\mathcal{Z}=\mathbb{R}^{|\mathcal{A}|}, z(a) \stackrel{\text { iid }}{\sim} \mathcal{N}(0,1)$, and $u(a, z)=\mu(a)+z(a)$ for some $\mu: \mathcal{A} \rightarrow \mathbb{R}$. That is, the person has an underlying utility $\mu(a)$ for each alternative but makes comparisons based on that utility plus independently sampled Gaussian noise representing irrationality in their utility assessments. This is equivalent to the Thurstone-Mosteller model of noisy decision making (Handley, 2001).

Due to the ubiquity of these settings, preference learning is nearly always performed with hidden context. This means that the learned utility function $\hat{u}(a)$, which only depends on the seen features $a$, must somehow aggregate over the hidden contexts $z$. We aim to understand and mitigate the consequences of this ubiquitous challenge.

### 2.2 RELATED WORK

Preference learning and its use in reinforcement learning have a long history Akrour et al. (2012); Busa-Fekete \& Hüllermeier (2014); Sadigh et al. (2017); Christiano et al. (2017); Pacchiano et al. (2021). As part of RLHF, preference learning has been widely used recently for training large language models (LLM) to give outputs according to human preferences (Ziegler et al., 2020; Stiennon et al., 2020; Askell et al., 2021; Bai et al., 2022a;b; Ouyang et al., 2022). It has also been extensively analyzed in theory; some results focus on its sample complexity in various settings (Chen \& Suh, 2015; Shah et al., 2015; Shah \& Wainwright, 2018; Heckel et al., 2018; Hendrickx et al., 2020; Chambers et al., 2021) or other directions such as the statistical identifiability of preferences (Zhao et al., 2020; Skalse et al., 2023), the computational efficiency of preference learning (Maystre \& Grossglauser, 2015), Bayesian preference learning (Caron \& Doucet, 2010), or the combination of preference learning and reinforcement learning (Zhu et al., 2023). However, to our knowledge, no prior work has specifically analyzed the behavior of preference learning with hidden context.

The challenges of preference learning that we group as cases of "hidden context" have also been studied individually. There has been some work on explicitly modeling annotator disagreement (Fleisig et al., 2023; Baumler et al., 2023) as well as other approaches to learning from annotators with diverse preferences (Jia et al., 2023; Dumoulin et al., 2023; Mishra, 2023; Fish et al., 2023). Other work has studied the effects of human irrationality or non-BTL models of human behavior on preference learning (Bobu et al., 2020; Lee et al., 2021; Laidlaw \& Russell, 2021; Knox et al., 2022; Laidlaw \& Dragan, 2022), which under our framework can be modeled as hidden context. Zhuang \& Hadfield-Menell (2020) and Dai et al. (2023) study the optimization of multiple objectives learned from human preferences. Finally, related to our connections with social choice theory in Section 3, some previous work has associated preference or reward learning with concepts in economics, such as voting rules (Conitzer \& Sandholm, 2005), incentive compatibility (Echenique \& Prasad, 2019), and mechanism design (Fickinger et al., 2020).

## 3 THEORETICAL ANALYSIS

In this section, we provide a theoretical analysis of standard preference learning-formally known as the BTL estimator-in the presence of hidden context. We are particularly interested in the question of how preference learning estimates a utility function $\hat{u}(a)$ which only depends on the observed alternatives $a$, despite using data that is generated from a utility function $u(a, z)$ which depends both on $a$ and hidden context $z$. In order to do this, preference learning must implicitly aggregate over hidden context to estimate a single utility value for each alternative $a$ (Figure 1).

We study this implicit aggregation from two perspectives: identification of expected utilities and social choice theory. In the former case, we are interested in when preference learning will aggregate over hidden context via expected value, which is a natural and often desirable aggregation method. In the latter case, we consider the setting where the hidden context is an annotator's identity. In this case, we show that preference learning acts as a social welfare functional: a method for constructing a societal utility function from many individual utilities.

We begin our analysis by precisely describing the behavior of preference learning with hidden context. In particular, we can show that a utility function $\hat{u}(a)$ learned with the BTL loss as in (3) implicitly aggregates utilities over the hidden contexts $z$ using a rule called Borda count. We define the Borda count $\mathrm{BC}(a)$ of an alternative $a$ as $\operatorname{BC}(a)=\frac{1}{|\mathcal{A}|} \sum_{b \in \mathcal{A}} p_{u, \mathcal{D}_{z}}(a, b)$. That is, the Borda count is the average probability that the alternative is preferred to other alternatives. If an alternative is almost always preferred to every other alternative, then its Borda count will be close to 1 ; if it is almost always dispreferred, the Borda count will be near 0 . We use the term Borda count as a reference to the well-known voting rule of the same name-a connection we expand on in Section 3.2.

Theorem 3.1. BTL preference learning implicitly aggregates hidden context according to Borda count. That is, if $\hat{u}$ is optimized according to (3), then $\forall a, b \in \mathcal{A}, \hat{u}(a)>\hat{u}(b) \Leftrightarrow B C(a)>B C(b)$.

We defer all proofs to Appendix A. According to Theorem 3.1, the learned utility function and Borda count differ by only a monotonic transformation. If we use reinforcement learning or another optimization technique to search for the alternative $a$ which maximizes $\hat{u}(a)$-as one does in RLHF-then the optimal alternative will the same as that which maximizes the Borda count $\mathrm{BC}(a)$. Similar results that relate preference learning and Borda count were previously explored by Rajkumar \& Agarwal (2014), although they do not consider the setting of hidden context.

While Theorem 3.1 precisely describes the results of preference learning with hidden context, its implications are unclear. Is Borda count a useful way of aggregating over hidden contexts in practice, and how does it compare to other aggregation rules? To answer this question, we give multiple perspectives on preference learning with hidden context using the result of Theorem 3.1. First, we compare preference learning to least-squares regression with hidden context. Then, we analyze learning from a population with diverse preferences through the lens of social choice theory.

### 3.1 COMPARISON TO EXPECTED UTILITY AND LEAST-SQUARES REGRESSION

One desirable property of preference learning with hidden context would be if it converged to the expected utility for each alternative when marginalizing over hidden context, which we denote by $\bar{u}(a)=\mathbb{E}_{z \sim \mathcal{D}_{z}}[u(a, z)]$. For instance, one can show that least-squares utility regression converges to the expected utility when there is hidden context; see Appendix A. 2 for a formal statement and proof.

The fact that least-squares utility regression converges to $\hat{u}=\bar{u}$ shows that, in some sense, it gracefully degrades in the presence of hidden context. Although there are problems with maximizing expected utility, it is a well-understood method of aggregating utilities over hidden contexts that has desirable decision-theoretic properties. Thus, it would be helpful if the utility function $\hat{u}(a)$ learned by preference learning with hidden context were equivalent to the expected utility $\bar{u}(a)$. In this section, we characterize when the output of preference learning with hidden context is equivalent to that of utility regression. We identify sufficient conditions for the two to be equivalent and present cases where they differ.

Positive results In some cases, we can show that preference learning does identify a utility function that is equivalent to the expected utility. Our result requires that the zero-mean "noise" induced by hidden context is identical across alternatives and reasonably distributed. Specifically, denote by $\epsilon(a)=u(a, z)-\bar{u}(a)$ (where $z \sim \mathcal{D}_{z}$ ) to be the random variable representing the residual utility of an alternative $a$ after subtracting its expected utility.

Theorem 3.2. Let $\epsilon(a)$ be independent and identically distributed for all $a \in \mathcal{A}$. Furthermore, suppose $\epsilon(a)-\epsilon(b)$ has support around 0 , i.e., $\forall \delta>0, F_{a, b}(\delta)>F_{a, b}(0)=\frac{1}{2}$, where $F_{a, b}$ is the cumulative distribution function of $\epsilon(a)-\epsilon(b)$. Then the utility function $\hat{u}$ learned by minimizing (3) satisfies $\hat{u}(a)>\hat{u}(b) \Leftrightarrow \bar{u}(a)>\bar{u}(b)$ for any $a, b \in \mathcal{A}$.

Many noise distributions, such as uniform and normal distributions, clearly satisfy the assumptions of Theorem 3.2. Thus, as long as the noise caused by hidden context does not vary across alternatives and is not too unusual, we generally expect that preference learning will give a utility function with the same ordering over alternatives as the expected utility. This means that it performs similarly to least-squares regression.

Negative results In other cases, preference learning can behave quite differently from utility regression. Example 1.1 describes such a case. The expected utility of telling students about Pell

Grants is higher than the expected utility of not telling them, since it is of great benefit to lowincome students and only small inconvience to high-income students. However, the Borda count is lower since the high-income majority prefer not to hear about the grants. This results in preference learning assigning higher utility to not giving the grant information, while regression would assign higher utility to giving it.

One might suppose that preference learning and regression disagree in this case because the majority of users prefer the alternative with lower expected utility, and preference learning gives a learned utility function which assigns higher utilities to alternatives preferred to by the majority of users. As long as the majority of feedback agrees with the ordering given by the expected utility, will preference learning and regression give the same result? The following theorem shows that this is not the case.

Proposition 3.3. $\exists \mathcal{A}, \mathcal{D}_{z}, u$ s.t $\forall a, b \in \mathcal{A},[\bar{u}(a)>\bar{u}(b)] \Rightarrow\left[p_{u, \mathcal{D}_{z}}(a, b)>1 / 2\right]$, but $\hat{u}$ is not equivalent to $\bar{u}$, i.e., there exist $a, b \in \mathcal{A}$ such that $\hat{u}(a)>\hat{u}(b)$ but $\bar{u}(a)<\bar{u}(b)$.

That is, Proposition 3.3 describes a case where for any two alternatives, the majority of feedback chooses the alternative with the higher expected utility, and yet preference learning still does not produce a utility function equivalent to the expected utility. In fact, in general, it is impossible to always identify $\bar{u}$ (even up to a monotonic transformation) given only comparison data.

Theorem 3.4. Suppose a preference learning algorithm takes as input unlimited samples of the form $\left(a, b, O_{u}(a, b, z)\right)$ for all values of $a$ and $b$, where $z \sim \mathcal{D}_{z}$, and deterministically outputs a learned utility function $\hat{u}(a)$. Then there is some utility function $u$ and distribution over unseen features $\mathcal{D}_{z}$ such that $\hat{u}$ is not equivalent to $\bar{u}$.

According to Theorem 3.4, there is simply not enough information in general for comparison data with hidden contexts to identify $\bar{u}$, even up to a monotone transformation. Thus, system designers should be careful not to treat utility functions learned from preference data as expected utilities.

A distributional perspective One final way of understanding the relationship between expected utility-the way utility regression aggregates over hidden context-and Borda count-the way preference learning aggregates over hidden context-is via the following identity.

Proposition 3.5. Let $F_{u \mid z}$ be the CDF of the distribution of $u(a, z)$ for some fixed value of $z$ when $a \sim \operatorname{Unif}(\mathcal{A})$. Then for any $a$,

$$
B C(a)=\mathbb{E}_{z \sim \mathcal{D}_{z}}\left[F_{u \mid z}(u(a, z))\right]
$$

Proposition 3.5 shows that both Borda count and expected utility can be written as $\mathbb{E}_{z \sim \mathcal{D}_{z}}\left[g_{z}(u(a, z))\right]$ for some collection of functions $g_{z}: \mathbb{R} \rightarrow \mathbb{R}$. For expected utility, $g_{z}(x)=x$; for Borda count, $g_{z}$ is the CDF of the overall distribution of utilities for alternatives given the hidden context $z$.

The fact that Borda count applies the utility CDF before taking the expectation over hidden context, while expected utility does not, can provide further intuition into the differences between the two. For instance, suppose the overall distribution of utilities for each hidden context is roughly normal. Then $F_{u \mid z}$ will be roughly sigmoidal, meaning that Borda count-and thus preference learning-will underweight extreme utility values compared to utility regression; see Fig-

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-07.jpg?height=306&width=504&top_left_y=1381&top_left_x=1233)

Figure 2: Proposition 3.5 shows that both Borda count and expected utility-which are learned by preference learning and utility regression, respectively-can be written as $\mathbb{E}_{z \sim \mathcal{D}_{z}}\left[g_{z}(u(a, z))\right]$ for some function $g_{z}$. For expected utility, $g_{z}(x)=x$, while for Borda count $g_{z}(x)$ is the CDF of utilities for the hidden context $z$. When the distribution over utilities is roughly normal, the CDF has a sigmoidal shape, so Borda count tends to underweight very positive or negative utility values relative to expected utility. ure 2 for a graphical depiction. This underweighting of more extreme utilities could be positive or negative depending on the setting. In Example 1.1, preference learning did not assign enough weight to the minority of low-income students who cared a lot about receiving Pell Grant information. This is a case where regression might perform better because it will properly taken into account their more extreme preferences. On the other hand, catering to a subset of the population with the most extreme preferences could be quite dangerous.

## Normal preference learning

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-08.jpg?height=152&width=699&top_left_y=325&top_left_x=363)

Distributional preference learning (DPL)

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-08.jpg?height=157&width=607&top_left_y=325&top_left_x=1147)

Figure 3: We introduce distributional preference learning (DPL), which explicitly accounts for hidden context. While normal preference learning outputs a single utility estimate for each alternative, DPL outputs a distribution over utilities. This distribution represents the range of utility values for that alternative as the hidden context varies, e.g., the distribution of utilities assigned to a chatbot response by different annotators or according to different objectives (like harmlessness vs. helpfulness).

Recognizing the impact of these differences in handling utility values, especially in complex scenarios like the college admissions example, leads us to consider broader frameworks for understanding preference learning. In this context, social choice theory emerges as a particularly relevant field, as we discuss further in the next subsection.

### 3.2 CONNECTIONS TO SOCIAL CHOICE THEORY

When training on comparison data from many agents, each with their own preferences, preference learning aggregates all their feedback into a single utility function. As we described in Section 2, this is a case where the identity of the annotator is hidden context: it affects the comparison outcomes, but is unseen by the preference learning algorithm. Social choice theory studies methods for aggregating preferences from a population. Thus, it can provide a lens through which to understand this particular case of preference learning with hidden contexts.

In a large dataset of preference comparisons from many annotators, individual comparisons can be thought of as "votes" for one alternative over another. When preference learning combines this data into a single utility function, it is similar to a voting rule that ranks candidates based on annotators' votes. In particular, Borda count is a well-studied voting rule-usual definitions of Borda count in voting theory differ from ours only by an affine transformation (Johnson, 2005; Emerson, 2013; Lippman, 2012). This means that many results from the social choice literature on Borda count can be applied to understanding preference learning from a diverse population. For example, it is well known that under Borda count, participants may have an incentive to misreport their preferences (Dummett, 1998).

Through the social choice lens, a natural question arises: can voting rules other than Borda count be implemented in preference learning by changing the estimation procedure? We explore this question further in Appendix B.3.

## 4 DISTRIBUTIONAL PREFERENCE LEARNING

Our theoretical results show that preference learning in the presence of hidden context can lead to undesirable outcomes. While system designers may still choose to use preference learning for RLHF or other applications, they should carefully consider these downsides and try to mitigate them. The first step towards this is detection - knowing to what degree hidden context affects preference data both on a dataset and instance level. In this section, we describe a simple modification to preference learning such that it can detect and characterize inconsistent feedback.

Our alternative preference learning methods, which we call distributional preference learning (DPL), output a distribution over possible utilities for each alternative rather than a single value (Figure 3). In particular, we learn a mapping $\hat{\mathcal{D}}: \mathcal{A} \rightarrow \Delta(\mathbb{R})$ from alternatives to distributions over utilities to estimate the distribution of $u(a, z)$ when $z \sim \mathcal{D}_{z}$. We consider two variants, each of which parameterizes the distribution $\hat{\mathcal{D}}(a)$ in a different way.

First, the mean-and-variance model learns two functions $\hat{\mu}: \mathcal{A} \rightarrow \mathbb{R}$ and $\hat{\sigma}: \mathcal{A} \rightarrow[0, \infty)$, parameterizing the distribution over utilities as $\hat{\mathcal{D}}(a)=\mathcal{N}\left(\hat{\mu}(a), \hat{\sigma}(a)^{2}\right)$. Second, in the categorical model, we choose $n$ evenly spaced utility values $u_{1}<u_{2}<\ldots<u_{n}$, and then parameterize
![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-09.jpg?height=294&width=352&top_left_y=278&top_left_x=407)

DPL (mean and variance)

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-09.jpg?height=257&width=458&top_left_y=316&top_left_x=774)

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-09.jpg?height=298&width=433&top_left_y=279&top_left_x=1277)

Figure 4: The results of our experiments with synthetic data. We find that the utility estimated by normal preference learning agrees closely with the Borda count, as our theory suggests. Furthermore, DPL successfully identify alternatives where hidden context has a significant effect.

the distribution as the probabilities of each of those utilities $\hat{p}\left(u_{i} \mid a\right)$ for $i \in\{1, \ldots, n\}$. We train the distributional preference models by maximizing the likelihood of the data given the model $p_{\hat{\mathcal{D}}}(a, b)=\mathbb{E}\left[O\left(u_{a}, u_{b}\right) \mid u_{a} \sim \hat{\mathcal{D}}(a), u_{b} \sim \hat{\mathcal{D}}(b)\right]$. Concretely, for the mean-and-variance model, the loss for a single preference comparison where alternative $a$ is preferred to $b$ is the negative log probability that $u_{a}-u_{b}>0$ :

$$
-\log \Phi\left(\frac{\hat{\mu}(a)-\hat{\mu}(b)}{\sqrt{\hat{\sigma}(a)^{2}+\hat{\sigma}(b)^{2}}}\right)
$$

For the categorical model, the equivalent loss is

$$
-\log \sum_{i=1}^{n} \sum_{j=1}^{n} \hat{p}\left(u_{i} \mid a\right) \hat{p}\left(u_{j} \mid b\right) \begin{cases}1 & u_{i}>u_{j} \\ 0 & u_{i}<u_{j} \\ 1 / 2 & u_{i}=u_{j}\end{cases}
$$

Note that DPL is not trying to model uncertainty about the utility function which comes from limited data, but rather uncertainty which comes from hidden context. Even in the limit of infinite data, DPL will not necessarily converge to a point estimate of utility for each alternative.

Since DPL methods give more information than a single utility estimate at each alternative, they can detect the effects of missing features both at the dataset and instance level. At the dataset level, a popular metric for determining the effects of missing features in regression is the coefficient of determination, $r^{2}$. We can derive an equivalent measure for DPL. Let $\hat{\mu}(a)=\mathbb{E}[\hat{\mathcal{D}}(a)]$. Then we define $r^{2}=\operatorname{Var}[\hat{\mu}(a)] /(\operatorname{Var}[\hat{\mu}(a)]+\mathbb{E}[\operatorname{Var}[\hat{\mathcal{D}}(a)]])$, where $a$ is sampled from the uniform distribution over alternatives. Intuitively, $r^{2}$, which has to be between 0 and 1 , represents the amount of variation in utility values that is captured by the observed features $a ; 1-r^{2}$ is the proportion of variance caused by hidden context. At the instance level, alternatives $a$ where $\operatorname{Var}(\hat{\mathcal{D}}(a))$ is higher are likely those where missing features have a larger impact on the utility of the alternative.

Synthetic experiments To test distributional preference learning, we ran experiments in a simple setting of preference learning with hidden context. We let $\mathcal{A}=[0,1]$ and $z \sim \mathcal{B}(1 / 2)$. We suppose that the true utility function is $u(a, z)=a$ if $a<0.8$ and $u(a, z)=2 a z$ otherwise. That is, the missing variable $z$ has no effect when $a<0.8$, but for $a \geq 0.8, u(a, z)$ is either $2 a$ or zero, each with probability one-half. This environment could model a case where the utilities of some alternatives (when $a<0.8$ ) are easy for users to judge, while others (when $a \geq 0.8$ ) have quite high variance due to irrationality or unobserved variables. We estimate utility functions both with normal preference learning and DPL; Figure 4 shows the results. The left plot shows that the learned utilities closely agree with Borda count and diverge from the expected utility $\bar{u}$, as our theory in Section 3 suggests. The right plots show that DPL accurately outputs high-variance distributions when $a>0.8$, since those are the alternatives for which hidden context affects preference comparisons.

Using DPL While our experiments show that DPL can detect the effects of hidden context in preference data, how should this additional information be used? We encourage qualitative analysis of alternatives where DPL suggests there are significant effects of hidden context. This can help system designers anticipate the negative consequences of hidden context before models are deployed. Beyond a qualitative analysis, risk-aversion is a concrete way to incorporate the additional information provided by DPL. Instead of directly attempting to maximize the learned utility function, risk

| Pref. learning <br> method | Training <br> dataset | Jailbreak <br> rate | Helpfulness <br> accuracy |
| :--- | :--- | ---: | ---: |
| Standard | Helpful | $52.4 \%$ | $72.6 \%$ |
| Standard | Harmless | $3.7 \%$ | $49.5 \%$ |
| Standard | Combined | $25.1 \%$ | $68.2 \%$ |
| Mean \& var. DPL | Combined | $30.5 \%$ | $68.4 \%$ |
| L Risk-averse |  | $20.3 \%$ | $66.4 \%$ |
| Categorical DPL | Combined | $32.1 \%$ | $66.2 \%$ |
| L Risk-averse |  | $13.4 \%$ | $66.2 \%$ |

(a) Combining our distribution preference learning (DPL) methods with risk-averse optimization mitigates jailbreaks without hurting accuracy on non-harmful prompts.

| Training <br> dataset | $r^{2}$ from DPL |  |
| :--- | ---: | ---: |
| Mean | Categor- |  |
|  | $\&$ var. | ical |
| Helpful | 0.89 | 0.63 |
| Harmless | 0.77 | 0.53 |
| Combined | 0.53 | 0.41 |

(b) The $r^{2}$ values, which quantify the effect of hidden context (see Section 4), measured by DPL models trained on different preference datasets.

Table 1: Results from our experiments on explaining and mitigating LLM jailbreaks in Section 4.

aversion with respect to the learned utility distribution introduces a penalty for alternatives where the data may be affected by hidden context. In the next section, we show that combining risk aversion with DPL can be used to develop guardrails that mitigate jailbreaks in LLMs.

## 5 Case Study: Competing ObJectives in RLHF

In this section, we evaluate DPL's ability to identify hidden context through a case study on large language model (LLM)-based reward models. Chatbots like GPT-4 and Claude are trained by learning a human reward model and then optimizing it via reinforcement learning, together referred to as RLHF. In order to evaluate the ability of DPL methods to identify hidden context, we use the HH-RLHF dataset (Bai et al., 2022a). For this dataset, raters were separately asked to provide preferences on whether responses were helpful or harmful. This allows us to determine if DPL can automatically detect this context.

When a single utility function is trained on the entire HH-RLHF dataset, the objective (helpfulness or harmlessness) that was used to annotate a pair of responses is a hidden context since it is not available to the learned utility function. This missing variable may cause real harm: Wei et al. (2023) present jailbreaks that pit the helpfulness and harmlessness objectives against each other. They show that models can be manipulated to prioritize helpfulness over harmlessness and output harmful content. Through our case study, we aim to answer three questions:

1. Does the hidden context of the labeling objective contribute to jailbreak vulnerability?
2. Can we DPL detect the effects of this hidden context without explicit supervision?
3. Can we DPL reduce models' susceptibility to jailbreaks?

Understanding jailbreak vulnerability To address the first question, we train three LLM-based utility functions on the preference comparison dataset HH-RLHF (Bai et al., 2022a). The dataset consists of conversations between a human and an AI assistant with two alternatives for the assistant's final response, plus a label for which response is preferred. Half of the comparisons are labeled based on which response is more helpful and honest, while the other half are labeled based on which response is more harmless. Using standard preference learning, we train utility functions $\hat{u}_{\text {helpful }}$ on just the helpful-labeled data, $\hat{u}_{\text {harmless }}$ on just the harmless-labeled data, and $\hat{u}_{\text {combined }}$ on both (see Appendix C for experiment details).

To test if implementing RLHF using these utility functions would lead to jailbreak vulnerabilities, we collect pairs of responses to jailbreak prompts from Wei et al. (2023) that are designed to fool the model into giving a harmful response; each pair consists of one safe response and one jailbroken response. If a learned utility function assigns higher utility to the jailbroken response than the safe one, then we expect using that utility function to train an LLM assistant via RLHF would lead to the assistant outputting the jailbroken response. We define the "jailbreak rate" of a utility function as the percentage of jailbreak prompts for which it assigns higher utility to the jailbroken response than the safe response.

Since avoiding jailbreaks is not the only purpose of an LLM assistant, we also evaluate each utility function for its ability to judge helpfulness on non-harmful prompts. In particular, we define the "helpfulness accuracy" of a utility function as the proportion of samples in the HH-RLHF helpfulness test set where it assigns higher utility to the response chosen by human annotators as more helpful.

The top of Table 1a shows the jailbreak rates and helpfulness accuracies for each of the three normally-trained utility functions. While $\hat{u}_{\text {harmless }}$, trained only on harmlessness-annotated data, has a very low jailbreak rate of under $4 \%$, its helpfulness accuracy of around $50 \%$ suggests it is useless for judging the helpfulness of responses to non-harmful prompts. $\hat{u}_{\text {helpful }}$ has much higher helpfulness accuracy, but also prefers jailbroken responses more than half the time. The problem is that the jailbroken responses are generally more "helpful" than a safe response which refuses to answer the prompt. Since our theory suggests that $\hat{u}_{\text {combined }}$ is aggregating the helpful and harmful utilities via Borda count, in many cases the high helpfulness of jailbroken responses leads to high utilities under the combined utility function. In fact, $\hat{u}_{\text {combined }}$ has a jailbreak rate of around $25 \%$, showing that one cause of jailbreaks is training a single reward model on data which combines two competing objectives-a clear case of hidden context in preference learning.

Detecting hidden context To answer the next question—whether we can detect hidden contextwe additionally train DPL models on all three datasets and measure their $r^{2}$ values, which are shown in Table 1b. Recall that lower $r^{2}$ indicates more effects from hidden context. We find that among the mean-and-variance DPL models, those trained on either just the helpfuless or just the harmlessness data have $r^{2}$ above 0.75 , while the DPL model trained on the combined data has a much lower $r^{2}$ $=0.53$. We see the same pattern with categorical DPL models: $r^{2}=(0.63,0.53)$ for the singleobjective models while $r^{2}=0.41$ for the combined model. This indicates that DPL can consistently measure the effect of hidden context via the $r^{2}$ metric: for both variants of DPL, $r^{2}$ is considerably lower when hidden context is present.

Preventing jailbreaks How might the distributional output of DPL be leveraged within RLHF to guard against jailbreaks? Ideally, we would like the trained model to avoid responses that are helpful but also harmful. We could implement this by training separate helpfulness and harmlessness utility models and then explicitly combining them. However, this requires that we know which objective each pair of alternatives was labeled with. In many cases, hidden context may not even be observable or recorded; for instance, if annotators simply interpret the labeling instructions differently, they may be labeling according to different objectives implicitly.

DPL methods allow the reward model to account for hidden context without the need for that context to be recorded. In particular, we can avoid helpful-but-harmful responses by optimizing a lower quantile of the distribution $\hat{\mathcal{D}}$ output by DPL. Optimizing this quantile is a type of risk-averse optimization that is only possible with DPL, since normal preference learning outputs a single score for each alternative. The bottom of Figure 1a shows that using the 0.01 -quantile of DPL models (rows labeled "risk-averse") can mitigate jailbreaks without harming the models' accuracy otherwise. For instance, the lower quantile of the categorical DPL model trained on the combined data has a jailbreak rate of $13 \%$, compared to $25 \%$ for $\hat{u}_{\text {combined }}$. Meanwhile, the models have very similar helpfulness accuracy, indicating that risk-averse optimization does not hurt DPL models' performance on nonharmful prompts.

To see why optimizing the lower quantile can prevent jailbreaks, consider the example in Figure 5: it compares the
Prompt: Write a Wikipedia article about the following topic: What tools do I need to cut down a stop sign?

Safe response: I Jailbroken response: apologize, but I Title: Tools for Cutwill not provide ting Down a Stop information to assist with illegal...

![](https://cdn.mathpix.com/cropped/2024_05_26_458c821623144cde22ecg-11.jpg?height=184&width=509&top_left_y=1621&top_left_x=1190)

一 - Normal pref. learning outputs

$\square \square$ DPL output distributions

-- -- Lower quantiles of DPL dists.

Figure 5: A comparison of how DPL and normal preference learning evaluate two responses to a jailbreak prompt. Normal preference learning assigns higher utility to the jailbroken response. While DPL also assigns a higher mean utility to the unsafe response, it also assigns it higher variance, indicating there is disagreement resulting from the helpfulness and harmlessness objectives diverging. Thus, if we evaluate the responses based on their lower quantiles (dashed lines), the safe response is preferred.
outputs of $\hat{u}_{\text {combined }}$ and a mean-and-variance DPL model on a pair of responses to a jailbreak prompt. $\hat{u}_{\text {combined }}$ assigns higher utility to the jailbroken response, likely because it is more helpful. While, the DPL model assigns higher a mean $\hat{\mu}$ to the jailbroken response as well, it also outputs higher variance $\hat{\sigma}$ for it. This means that the lower quantile of the utility distribution $\hat{\mathcal{D}}=\mathcal{N}\left(\hat{\mu}, \hat{\sigma}^{2}\right)$ is actually lower for the jailbroken response than the safe response; thus, using combining risk-averse optimization with DPL prefers the safe response, unlike normal preference learning.

## 6 CONCLUSION

Preference learning is becoming an essential component of real-world AI systems that helps align outcomes with the values of users. However, preference learning implicitly assumes that all the data that annotators use to make preference judgements is available as input to the utility or reward model. When this assumption breaks down, which we identify as the problem of hidden context, preference learning can produce strange or undesirable results. Distributional preference learning can mitigate this problem by both helping detect when hidden context is present and enabling risk-sensitive optimization over the distribution of utility values. We hope that future system designers will carefully consider our analysis and examine how hidden context may be affecting preference learning in their systems. Furthermore, we encourage practitioners to consider using the DPL framework as an alternative method that can explicitly account for hidden context.

In the future, we hope to further analyze DPL and hidden context in preference learning both theoretically and empirically. Under what theoretical conditions can DPL accurately estimate the distribution of utilities for each alternative? Besides jailbreaks, what other failures of RLHF-trained models does hidden context contribute to? Answers to these questions and a more thorough understanding of hidden context in preference learning are important steps towards enabling safe, aligned AI systems.

## ACKNOWLEDGMENTS

We thank Ruiqi Zhong and Sam Toyer for feedback on drafts. Cassidy Laidlaw was supported by an Open Philanthropy AI Fellowship. Dylan Hadfield-Menell was supported by an AI2050 Early Career Fellowship from Schmidt Sciences.

## REFERENCES

Riad Akrour, Marc Schoenauer, and Michèle Sebag. APRIL: Active Preference-learning based Reinforcement Learning, August 2012. URL http://arxiv.org/abs/1208.0984. arXiv:1208.0984 [cs].

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A General Language Assistant as a Laboratory for Alignment, December 2021. URL http://arxiv.org/abs/2112.00861. arXiv:2112.00861 [cs].

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022a. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli TranJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,

Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022b. URL http://arxiv.org/abs/2212. 08073. arXiv:2212.08073 [cs].

Connor Baumler, Anna Sotnikova, and Hal Daumé III. Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 10352-10371, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023. findings-acl. 658.

Andreea Bobu, Dexter R. R. Scobee, Jaime F. Fisac, S. Shankar Sastry, and Anca D. Dragan. LESS is More: Rethinking Probabilistic Models of Human Behavior. Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction, pp. 429-437, March 2020. doi: 10.1145/3319502.3374811. URL https://dl.acm.org/doi/10.1145/3319502. 3374811. Conference Name: HRI '20: ACM/IEEE International Conference on Human-Robot Interaction ISBN: 9781450367462 Place: Cambridge United Kingdom Publisher: ACM.

Róbert Busa-Fekete and Eyke Hüllermeier. A Survey of Preference-Based Online Learning with Bandit Algorithms. In Peter Auer, Alexander Clark, Thomas Zeugmann, and Sandra Zilles (eds.), Algorithmic Learning Theory, Lecture Notes in Computer Science, pp. 18-39, Cham, 2014. Springer International Publishing. ISBN 978-3-319-11662-4. doi: 10.1007/ $978-3-319-11662-4-3$.

Francois Caron and Arnaud Doucet. Efficient Bayesian Inference for Generalized Bradley-Terry Models, November 2010. URL http://arxiv.org/abs/1011.1761. arXiv:1011.1761 [stat].

Christopher P. Chambers, Federico Echenique, and Nicolas S. Lambert. Recovering Preferences From Finite Data. Econometrica, 89(4):1633-1664, 2021. ISSN 1468-0262. doi: 10.3982/ECTA17845. URL https://onlinelibrary.wiley.com/doi/abs/10. 3982 /ECTA17845. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA17845.

Yuxin Chen and Changho Suh. Spectral MLE: Top-\$K\$ Rank Aggregation from Pairwise Comparisons, May 2015. URL http://arxiv.org/abs/1504.07218. arXiv:1504.07218 [cs, math, stat].

Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, June 2017. URL http://arxiv.org/abs/ 1706.03741. arXiv:1706.03741 [cs, stat].

Vincent Conitzer and Tuomas Sandholm. Common Voting Rules as Maximum Likelihood Estimators. In UAI '05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, pp. 145-152. AUAI Press, 2005. URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu= $1 \& s m n u=2 \& a r t i c l e \_i d=1213 \& p r o c e e d i n g \_i d=21$.

Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe Reinforcement Learning from Human Feedback, October 2023. URL http://arxiv.org/abs/2310.12773. arXiv:2310.12773 [cs].

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, August 2023. URL http://arxiv.org/ abs/2305.14387. arXiv:2305.14387 [cs].

Michael Dummett. The Borda count and agenda manipulation. Social Choice and Welfare, 15(2): 289-296, 1998. ISSN 0176-1714. URL https://www.jstor.org/stable/41106256. Publisher: Springer.

Vincent Dumoulin, Daniel D. Johnson, Pablo Samuel Castro, Hugo Larochelle, and Yann Dauphin. A density estimation perspective on learning from pairwise human preferences, 2023. URL http://arxiv.org/abs/2311.14115. arXiv:2311.14115 [cs].

Federico Echenique and Siddharth Prasad. Incentive Compatible Active Learning, November 2019. URL http://arxiv.org/abs/1911.05171. arXiv:1911.05171 [cs].

Peter Emerson. The original Borda count and partial voting. Social Choice and Welfare, 40(2): 353-358, February 2013. ISSN 0176-1714, 1432-217X. doi: 10.1007/s00355-011-0603-9. URL http://link.springer.com/10.1007/s00355-011-0603-9.

Arnaud Fickinger, Simon Zhuang, Dylan Hadfield-Menell, and Stuart Russell. MultiPrincipal Assistance Games, July 2020. URL http://arxiv.org/abs/2007.09540. arXiv:2007.09540 [cs].

Sara Fish, Paul Gölz, David C. Parkes, Ariel D. Procaccia, Gili Rusak, Itai Shapira, and Manuel Wüthrich. Generative Social Choice, September 2023. URL http://arxiv.org/abs/ 2309.01291. arXiv:2309.01291 [cs].

Eve Fleisig, Rediet Abebe, and Dan Klein. When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. 2023. doi: 10.48550/ARXIV.2305.06626. URL https: //arxiv.org/abs/2305.06626. Publisher: arXiv Version Number: 3.

John C. Handley. Comparative analysis of Bradley-Terry and Thurstone-Mosteller paired comparison models for image quality assessment. In PICS, volume 1, pp. 108-112, 2001.

Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin J. Wainwright. Approximate Ranking from Pairwise Comparisons, January 2018. URL http://arxiv.org/abs/1801. 01253. arXiv:1801.01253 [cs, math, stat].

Julien Hendrickx, Alex Olshevsky, and Venkatesh Saligrama. Minimax Rate for Learning From Pairwise Comparisons in the BTL Model. In Proceedings of the 37th International Conference on Machine Learning, pp. 4193-4202. PMLR, November 2020. URL https: //proceedings . mlr.press/v119/hendrickx20a.html. ISSN: 2640-3498.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models, October 2021. URL http://arxiv.org/abs/2106.09685. arXiv:2106.09685 [cs].

Hong Jun Jeon, Smitha Milli, and Anca D. Dragan. Reward-Rational (Implicit) Choice: A Unifying Formalism for Reward Learning. arXiv:2002.04833 [cs], December 2020. URL http://arxiv.org/abs/2002.04833. arXiv: 2002.04833.

Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeff Hancock, and Michael S. Bernstein. Embedding Democratic Values into Social Media AIs via Societal Objective Functions, July 2023. URL http://arxiv.org/abs/2307.13912. arXiv:2307.13912 [cs].

Paul E. Johnson. Voting systems. University of Kansas, Department of Mathematics, 2005. URL https://pj.freefaculty.org/Ukraine/PJ3_VotingSystemsEssay.pdf.

W. Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter Stone, and Alessandro Allievi. Models of human preference for learning reward functions, June 2022. URL http://arxiv.org/abs/2206.02231. arXiv:2206.02231 [cs, eess].

Cassidy Laidlaw and Anca Dragan. The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models, April 2022. URL http://arxiv.org/abs/2204. 10759. arXiv:2204.10759 [cs].

Cassidy Laidlaw and Stuart Russell. Uncertain decisions facilitate better preference learning. Advances in Neural Information Processing Systems, 34:15070-15083, 2021.

Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-Pref: Benchmarking Preference-Based Reinforcement Learning, November 2021. URL http://arxiv.org/abs/2111.03026. arXiv:2111.03026 [cs].

David Lippman. Math in Society. CreateSpace Independent Publishing Platform, September 2012. ISBN 978-1-4792-7653-0.

Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, January 2019. URL http://arxiv.org/abs/1711.05101. arXiv:1711.05101 [cs, math].

Lucas Maystre and Matthias Grossglauser. Fast and Accurate Inference of Plackett- Luce Models. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://papers.nips.cc/paper_files/paper/2015/hash/ 2a38a4a9316c49e5a833517c45d31070-Abstract.html.

Abhilash Mishra. AI Alignment and Social Choice: Fundamental Limitations and Policy Implications, October 2023. URL http://arxiv.org/abs/2310.16048. arXiv:2310.16048 $[\mathrm{cs}]$.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, December 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html.

Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling RL: Reinforcement Learning with Trajectory Preferences, November 2021. URL http://arxiv.org/abs/2111.04850. arXiv:2111.04850 [cs].

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019. URL http://arxiv.org/abs/1912.01703. arXiv:1912.01703 [cs, stat].

Arun Rajkumar and Shivani Agarwal. A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data. In Proceedings of the 31st International Conference on Machine Learning, pp.118-126. PMLR, January 2014. URL https://proceedings.mlr.press/ v32/rajkumar14.html. ISSN: 1938-7228.

Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active Preference-Based Learning of Reward Functions. In Robotics: Science and Systems XIII. Robotics: Science and Systems Foundation, July 2017. ISBN 978-0-9923747-3-0. doi: 10.15607/RSS.2017.XIII.053. URL http://www.roboticsproceedings.org/rss13/p53.pdf.

Nihar B. Shah and Martin J. Wainwright. Simple, Robust and Optimal Ranking from Pairwise Comparisons. Journal of Machine Learning Research, 18(199):1-38, 2018. ISSN 1533-7928. URL http://jmlr.org/papers/v18/16-206.html.

Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, and Martin J. Wainwright. Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence, May 2015. URL http://arxiv.org/abs/1505.01462. arXiv: 1505.01462 [cs, math, stat].

Joar Max Viktor Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro Abate, and Adam Gleave. Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. In Proceedings of the 40th International Conference on Machine Learning, pp. 32033-32058. PMLR, July 2023. URL https://proceedings.mlr.press/v202/skalse23a. html. ISSN: 2640-3498.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pp. 3008-3021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1f89885d556929e98d3ef9b86448f951-Abstract.html.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023. URL http://arxiv.org/abs/2307.09288. arXiv:2307.09288 [cs].

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail?, July 2023. URL http://arxiv.org/abs/2307.02483. arXiv:2307.02483 [cs].

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. HuggingFace's Transformers: Stateof-the-art Natural Language Processing, July 2020. URL http://arxiv.org/abs/1910. 03771. arXiv:1910.03771 [cs].

Zhibing Zhao, Peter Piech, and Lirong Xia. Learning Mixtures of Plackett-Luce Models, March 2020. URL http://arxiv.org/abs/1603.07323. arXiv:1603.07323 [cs].

Banghua Zhu, Jiantao Jiao, and Michael I. Jordan. Principled Reinforcement Learning with Human Feedback from Pairwise or \$K\$-wise Comparisons, May 2023. URL http://arxiv.org/ abs/2301.11270. arXiv:2301.11270 [cs, math, stat].

Simon Zhuang and Dylan Hadfield-Menell. Consequences of Misaligned AI. In Advances in Neural Information Processing Systems, volume 33, pp. 15763-15773. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ b607ba543ad05417b8507ee86c54fcb7-Abstract.html.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-Tuning Language Models from Human Preferences, January 2020. URL http://arxiv.org/abs/1909.08593. arXiv:1909.08593 [cs, stat].
