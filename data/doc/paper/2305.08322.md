# C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models 

Yuzhen Huang $^{* 1} \quad$ Yuzhuo Bai ${ }^{* 2} \quad$ Zhihao Zhu $^{1} \quad$ Junlei Zhang $^{1} \quad$ Jinghan Zhang $^{1}$<br>Tangjun Su ${ }^{1} \quad$ Junteng Liu $^{1} \quad$ Chuancheng Lv $^{2} \quad$ Yikai Zhang ${ }^{1} \quad$ Jiayi Lei $^{1}$<br>Yao Fu Maosong Sun ${ }^{2}$ Junxian He ${ }^{\dagger 4}$<br>${ }^{1}$ Shanghai Jiao Tong University ${ }^{2}$ Tsinghua University ${ }^{3}$ University of Edinburgh<br>${ }^{4}$ The Hong Kong University of Science and Technology<br>ceval.benchmark@gmail.com<br>https://cevalbenchmark.com


#### Abstract

New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-EVAL , the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-EVAL comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-EVAL is accompanied by C-EVAL HARD, a subset of very challenging subjects in C-EVAL that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over $60 \%$, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-EVAL will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users 1 .


## 1 Introduction

Evaluation benchmarks are at the core role for AI development. While traditional NLP benchmarks were mostly designed to measure specific and relatively simple abilities, large language models (LLMs), or foundation models, have demonstrated various new capabilities and shifted the evaluation focus to more general and intricate skills, such as broad world knowledge and complex reasoning. To align with the new era of LLMs, new benchmarks are proposed recently to probe a diverse set of LLM abilities. For example, MMLU (Hendrycks et al., 2021a), BIG-bench (Srivastava et al., 2022), and HELM (Liang et al., 2022) benchmarks attempt to aggregate a wide range of NLP tasks for holistic evaluation. Some other benchmarks specifically focus on advanced LLM abilities that emerge with scale, such as reasoning (Cobbe et al., 2021), hard math problem-solving (Hendrycks et al. 2021b), and coding (Chen et al. 2021). While traditional NLP benchmarks are becoming obsolete, these new ones are extensively used in recent research to drive development of the latest LLMs (Taylor et al., 2022; Chowdhery et al., 2022, Hoffmann et al., 2022; Touvron et al., 2023, OpenAI, 2023).

However, these modern benchmarks primarily target English language, resulting in limited understanding of LLMs' capabilities in other languages. In this work, we focus on evaluating the advanced abilities of foundation models in a Chinese context, one of the most widely spoken language in the[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_fd6676c10febce042471g-02.jpg?height=608&width=1227&top_left_y=257&top_left_x=449)

Figure 1: Overview diagram of C-EvAL. Different colors of the subjects indicate four difficulty levels: middle school, high school, college, and professional.

world. Although there has been a recent surge in powerful Chinese LLMs, such as GLM-130B (Zeng et al., 2023), Wenxin Yiyan (Baidu, 2023), and MOSS (OpenLMLab, 2023), the corresponding evaluation significantly lags behind, with the CLUE benchmark (Xu et al. 2020), the Chinese counterpart of GLUE (Wang et al., 2019), serving as the best available standard. We emphasize that simply translating English benchmarks as in OpenAI (2023), even with flawless translations, does not fulfill the goal - LLMs intended for use in a Chinese environment should be evaluated on their knowledge of Chinese users' primary interests, such as Chinese culture, history, and laws, as well as other competencies unique in Chinese society. In contrast, English benchmarks tend to exhibit geographical biases towards the domestic knowledge of the regions that produce them.

To narrow the gap between Chinese LLM development and evaluation, we present C-EVAL, the first comprehensive Chinese evaluation suite to thoroughly assess LLMs' advanced knowledge and reasoning abilities in a Chinese context. C-EvAL consists of 13948 multiple-choice exam questions spanning 52 diverse disciplines, ranging from humanities to science and engineering, as depicted in Figure 1. The questions are collected from four difficulty levels: middle school, high school, college, and professional tests. Along with C-Eval, we introduce C-Eval HARD as an accompanied benchmark, a subset of particularly challenging subjects in C-EVAL that demands highly advanced reasoning abilities to solve, such as advanced mathematics and college physics. Notably, C-EvaL HARD is among the few benchmarks for advanced reasoning where GPT-4 still struggles, achieving an accuracy of $53.3 \%$, making it the first Chinese benchmark at this level.

We conduct experiments to evaluate the most advanced LLMs on C-EVAL in both answer-only and chain-of-thought settings. Results show that GPT-4 is the only model that surpasses $60 \%$ average accuracy. However, its $66.4 \%$ accuracy indicates that there is still large room for improvement in current LLMs. Despite not specially tailored for Chinese data, GPT-4, ChatGPT, and Claude emerge as the top three performers on C-EVAL. Upon examining the results of LLMs focused on Chinese, we find that while some models managed to narrow the gap on Chinese knowledge test with ChatGPT, acquiring reasoning abilities seems more challenging. On C-EVAL HARD, in particular, most models could only retain near-random accuracy. In addition to its use as a whole, we envision C-EVAL as a suite of benchmarks, subsets of which could be separately utilized to assess certain model abilities of interest and analyze important strengths and limitations of foundation models. We hope C-EvAL could guide the developers to understand the abilities of their models from multiple dimensions and facilitate the growth of foundation models for Chinese users.

## 2 The C-Eval Evaluation Suite

### 2.1 Design Principle

Overview: The motivation of C-EVAL is to help developers quickly understand the abilities of their models from multiple dimensions, so that they could target the shortcomings of the models and

| Category | \# Subjects | \# Questions |
| :--- | :---: | ---: |
| In terms of topic |  |  |
| STEM | 20 | 4495 |
| Humanities | 11 | 2676 |
| Social Science | 10 | 2845 |
| Other | 11 | 3932 |
| In terms of difficulty level |  |  |
| Middle School | 7 | 1409 |
| High School | 8 | 1594 |
| College | 25 | 6249 |
| Professional | 12 | 4696 |
| In terms of split |  |  |
| Dev | 52 | 260 |
| Valid | 52 | 1346 |
| Test | 52 | 12342 |
| Total | 52 | 13948 |

Table 1: Statistics of C-EvAL.

```
洛伦兹曲线越是向横轴凸出
The more the Lorenz curve is convex to the horizontal axis,
A. 基尼系数就越大,收入就越不平等
the larger the Gini coefficient, the more unequal the income.
B. 基尼系数就越大, 收入就越平等
the larger the Gini coefficient, the more equal the income.
C. 基尼系数就越小,收入就越不平等
the smaller the Gini coefficient, the more unequal the income.
D. 基尼系数就越小,收入就越平等
the smaller the Gini coefficient, the more equal the income.
答案:A
Answer: A
```

Figure 2: Example from college economics. English translations are shown for better readability.

improve them accordingly. To this end, we focus on the relatively advanced abilities of LLMs such as world knowledge and reasoning, which are arguably the most critical skills for LLMs nowadays. While different LLMs may perform similarly in simple scenarios like casual conversations, complex tasks are often the key differentiators between LLMs (OpenAI, 2023). Therefore, we construct C-EVAL from real-world, challenging human exams in China that are used to assess humans' abilities from multiple dimensions. We only select questions of a multi-choice format, similar to Hendrycks et al. (2021a), because: (1) metrics are clearly defined (i.e. accuracy), and (2) multi-choice questions are a simple but good proxy to evaluate the potential of advanced abilities of foundation models, which we consider could be easily exploited and reflected in various downstream applications through specialized instruction tuning (Chung et al., 2022; Wang et al., 2022). Each question has four choices and only one choice is the correct answer. LLMs are intended to be used to solve these questions through prompting. The questions in C-Eval span 52 diverse disciplines that we later cluster them into broader categories as STEM, humanities, social science, and other areas. Summarized statistics of C-Eval is shown in Table 1, and more detailed statistics per subject are in Appendix B.

Attempt to mitigate data contamination: Exam questions from national tests, such as China's national college entrance exams (commonly known as Gaokao) and national professional exams, are often widely distributed and accessible online. Consequently, these questions may inadvertently be crawled and incorporated into LLM pretraining, leading to potential data leakage issues. To mitigate this risk, we collect our data either from mock exams or from small-scale local exams, such as those available online from specific high schools. This deviates from previous work that built benchmarks using the exact questions from official national exams (Zhong et al., 2023). Moreover, most samples in C-EVAL are sourced from PDF or Microsoft Word documents on the Internet, rather than directly from plain text or structured questions. These documents are subsequently parsed and carefully annotated by the authors to obtain the final structured format, a process that often involves complex LTTEX equation conversion for certain subjects. This further minimizes the risk of data contamination.

### 2.2 Data Collection

Subject selection: C-EVAL covers four difficulty levels: middle school, high school, college, and professional. We include the standard subjects for middle and high school levels in China, except for the English subject ${ }^{2}$ For the college level, we select 25 representative subjects from all the 13 official categories of undergraduate majors listed by the Ministry of Education in China ${ }^{3}$ at least one subject from each category is included in C-Eval to assure comprehensiveness. For the professional level, we refer to the official national vocational qualification directory in Chind ${ }^{4}$ and choose 12[^1]

```
25 *
的体积比是(忽略混合后溶液的体积变化)
At 25 *
mixture has a pH of 11. The volume ratio of the strong acid solution to the strong alkali solution is (ignoring any
volume changes upon mixing)
\begin{array} { l l l l } { \text { A. 11:1 } } & { \text { B. 9:1 } } & { \text { C. 1:11 1 D. 1:9} } \end{array}
答案:B
Answer: B
答案解析:
1. pH=13的强碱溶液中 c(OH
c(OH})=0.001\textrm{mol}/\textrm{L}\mathrm{ 。
2. 设强酸和强碱溶液的体积分别为x和y, 则:c(OH
Explanation:
1. In the strong alkali solution with }\textrm{pH}=13,\textrm{c}(O\mp@subsup{\textrm{OH}}{}{-})=0.1\textrm{mol}/\textrm{L}\mathrm{ , and in the strong acid solution with }\textrm{pH}=2,\textrm{c}(\mp@subsup{H}{}{+}
=0.01mol/L. After mixing, the pH is 11, which means that c}(O\mp@subsup{H}{}{-})=0.001\textrm{mol}/\textrm{L}\mathrm{ .
2. Assuming the volumes of the strong acid and strong alkali solutions are x and y, respectively, then: c (OH
(0.1y-0.01x)/(x+y)=0.001. Solving for x:y = 9:1.
```

Figure 3: An development example with explanations from high school chemistry. English translations are shown below the corresponding Chinese text for better readability.

representative ones, such as physician, legal professional, and civil servant qualification exams. We also cluster these subjects into four categories in terms of their topic: STEM (Science, Technology, Engineering, and Mathematics), Social Science, Humanities, and Other areas. All the 52 subjects and their assigned categories are illustrated in Figure 1 .

Data sources: The primary source of our data is mock exams freely available on the internet. In addition, a portion of the college-level questions are past exam questions from top universities in China, publicly shared by the students. A minor fraction of college questions are mock questions for the national graduate entrance exam, sourced from the Weipu website $5^{5}$ - these questions are not freely available to the public, and we have obtained their authorization to include around 2000 such questions into C-EVAL.

Data Processing: The collected data come in various formats, primarily as PDF or Microsoft Word documents and a minor fraction as web pages. PDF documents are initially processed into text. All questions are subsequently parsed - automatically when possible, and otherwise manually by the authors - into a structured format, as exemplified in Figure 2. For subjects with complex mathematical notations such as many subjects in the STEM category, we manually convert them into standard LATEX formats, similar to Hendrycks et al. (2021b); Taylor et al. (2022). All the questions in C-Eval are processed to include exactly four choices. Most of the original questions were accompanied by four choices already, and we eliminate questions with fewer than four options and randomly drop incorrect choices for questions with more than four options. All questions also go through the standard data preprocessing pipeline, such as deduplication and cleaning. Following this, the questions undergo several rounds of human validation by the authors, and all the ITEX notations are ensured to be complied without syntax errors. We process at least around 200 questions for each subject, and randomly split the questions into a development set, a validation set, and a test set within each subject. The development split per subject consists of five exemplars to facilitate few-shot evaluation. These dev exemplars are also annotated with explanations to enable few-shot chain-of-thought settings (Wei et al. 2022), as we detail next. The validation and test set are created with a $1: 9$ ratio, where the validation set is intended to be used for hyperparameter tuning.

Explanation data generation: Chain-of-thought (COT) reasoning (Kojima et al., 2022; Wei et al. 2022) - that prompts LLMs to generate a text sequence of reasoning process along with the final answer - has shown great success on reasoning-heavy tasks. Compared to zero-shot COT, the few-shot version is more commonly used and achieves the state-of-the-art performance on various tasks (Gao et al., 2022, Wang et al., 2023, Zhou et al., 2023; Xie et al., 2023). To facilitate the potential usage of C-EvAL in a few-shot COT setting, we combine automatic generation and human annotation to produce high-quality explanation data for the development split. Specifically, we first prompt GPT-4 to generate step-by-step explanation to explain the ground-truth answer, then we[^2]

```
设有界区域得曲面 }2x+y+2z=2\mathrm{ 与三个坐标平面围成, }\Sigma\mathrm{ 为整个表面的外侧; 计算曲面积分:
I = \int \int _ { \Sigma } ( x ^ { 2 } + 1 ) d l y d \| z - 2 y d l z d d x + 3 z d l x d l y =
Let }\Omega\mathrm{ be a bounded region enclosed by the surface }2x+y+2z=2\mathrm{ and three coordinate planes, and let }\Sigma\mathrm{ be the

```

![](https://cdn.mathpix.com/cropped/2024_06_04_fd6676c10febce042471g-05.jpg?height=41&width=1057&top_left_y=370&top_left_x=458)

```
\begin{array} { l l l l } { \text { A. 1/2 } } & { \text { B. 1 } } & { \text { C. 3/2 D. D/2} } \end{array}
答案:A
Answer: A
```

Figure 4: Example from advanced mathematics, a subject in C-EVAL HARD. English translations are shown below the corresponding Chinese text for better readability.

manually revise the generated explanations to obtain the final explanations. Details on prompting GPT-4 are in Appendix C. A dev example with explanations is illustrated in Figure 3 .

### 2.3 C-Eval Hard

We select 8 challenging math, physics, and chemistry subjects from C-EvAL to form a separate benchmark, C-EVAL HARD, which includes advanced mathematics, discrete mathematics, probability and statistics, college chemistry, college physics, high school mathematics, high school chemistry, and high school physics. These subjects often involve with complex $\mathrm{LT}_{\mathrm{E}} \mathrm{X}$ equations and require non-trivial reasoning abilities to solve. An example from advanced mathematics is shown in Figure 4 C-EvAL HARD aligns with recent efforts to create difficult benchmarks to assess advanced reasoning abilities (Hendrycks et al. 2021b; Suzgun et al. 2022), which are the key differentiators among various LLMs and could reflect LLMs' potential in general and complex scenarios. We emphasize that C-EvAL HARD is the first Chinese benchmark to provide highly complicated reasoning questions.

### 2.4 Evaluation

We use accuracy as the metric. While ground-truth labels of the development and validation splits are released, we keep the labels of the test split private. This is to ensure the fair use of the C-EvAL, as the C-EVAL data may unconsciously be included in pretraining data due to web crawling. Instead, users are required to submit their model predictions to https://cevalbenchmark.com to automatically obtain the test accuracy, where a public leaderboard is maintained. Users have the option to include their submission results in the live leaderboard, depending on their own preference.

## 3 Experiment

### 3.1 Setup

We evaluate LLMs in both zero- and five-shot settings on C-EvAL, where the five exemplars are from the development split. We adopt regular expressions to extract answer choices from the model responses, ensuring that we can successfully extract answers for nearly all cases. We report answeronly (AO) results on both zero- and five-shot settings and chain-of-thought (COT) results on the five-shot setting only, as we found that it was often difficult to extract the answer choices from zero-shot COT predictions where the generation does not follow specific patterns. Prompts of AO and COT are shown in Appendix D We note that in the COT setting, the five-shot exemplars could exceed the maximum context length of some LLMs for certain subjects. In such cases, we dynamically reduce the number of exemplars to fit within the context window.

### 3.2 Models

To give a comprehensive view of the status of LLM in a Chinese language context, we evaluate 11 accessible top-performing LLMs that are able to process Chinese input, covering diverse organizations and varying in size, as shown in Table 2) ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) are the strongest GPT model variants from OpenAI. Claude (Anthropic, 2022), developed by Anthropic, is often considered comparable to ChatGPT. We evaluate both the Claude-v1.3 and Claude-instantv1.0 variants, with Claude-instant being a lighter version. Bloomz-mt (Muennighoff et al. 2022) is based on the pretrained multilingual BLOOM model (Scao et al., 2022) with multitask prompted finetuning, thus is suitable for non-English languages. LLaMA (Touvron et al. (2023) is probably

| Model | Creator | \#Parameters | Access |
| :--- | :---: | :---: | :---: |
| GPT-4 | OpenAI | undisclosed | API |
| ChatGPT | OpenAI | undisclosed | API |
| Claude-v1.3 | Anthropic | undisclosed | API |
| Claude-instant-v1.0 | Anthropic | undisclosed | API |
| Bloomz-mt | BigScience | 176B | Weights |
| LLaMA-65B | Meta | $65 B$ | Weights |
| GLM-130B | Tsinghua | $130 \mathrm{~B}$ | Weights |
| ChatGLM-6B | Tsinghua | $6 \mathrm{~B}$ | Weights |
| Chinese-LLaMA-13B | Cuie al. | 13B | Weights |
| Chinese-Alpaca-13B | Cui et al. | 13B | Weights |
| MOSS | Fudan | 16B | Weights |

Table 2: Models evaluated in this paper.

| Model | STEM | Social Science | Humanities | Other | Average |
| :--- | :---: | :---: | :---: | :---: | ---: |
| Random | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 |
| GPT-4 | 65.2 | 74.7 | 62.5 | 64.7 | 66.4 |
| ChatGPT | 49.0 | 58.0 | 48.8 | 50.4 | 51.0 |
| Claude-v1.3 | 48.5 | 58.6 | 47.3 | 50.1 | 50.5 |
| Bloomz-mt | 39.1 | 53.0 | 47.7 | 42.7 | 44.3 |
| GLM-130B | 36.7 | 55.8 | 47.7 | 43.0 | 44.0 |
| Claude-instant-v1.0 | 38.6 | 47.6 | 39.5 | 39.0 | 40.6 |
| ChatGLM-6B | 33.3 | 48.3 | 41.3 | 38.0 | 38.9 |
| LLaMA-65B | 32.6 | 41.2 | 34.1 | 33.0 | 34.7 |
| MOSS | 31.6 | 37.0 | 33.4 | 32.1 | 33.1 |
| Chinese-Alpaca-13B | 27.4 | 39.2 | 32.5 | 28.0 | 30.9 |
| Chinese-LLaMA-13B | 28.8 | 32.9 | 29.7 | 28.0 | 29.6 |

Table 3: Zero-shot average accuracy (\%) in answer-only setting. We report the average accuracy over the subjects within each category. "Average" column indicates the average accuracy over all the subjects.

the best open-weight foundation model so far that achieves the highest accuracy on the English MMLU benchmark within open-weight models. The aforementioned models except Bloomz-mt are English-oriented during development, while they are able to process Chinese input because a minor fraction of Chinese text is present in the pretraining data.

We further include recent LLMs developed by Chinese institutions or individuals that is Chineseoriented. GLM-130B (Zeng et al., 2023) and ChatGLM-6B (THUDM, 2023a) are based on the General Language Model architecture (GLM, Du et al. (2022)) trained on English and Chinese data. ChatGLM-6B is further adapted on conversational data. Chinese-LLaMA (Cui et al., 2023) is an adaptation of LLaMA, which is further pretrained on Chinese data. Chinese-Alpaca (Cui et al., 2023) performs instruction tuning based on Chinese-LLaMA. MOSS (OpenLMLab, 2023) is the first publicly available Chinese LLM, and it follows a training procedure similar to ChatGPT. We note that there are some other commercial Chinese-oriented LLMs whose weights and APIs are not directly open to the public at the time of writing this paper, such as Wenxin Yiyan (Baidu, 2023), Tongyi Qianwen (Alibaba, 2023), and Xunfei Xinghuo (iFLYTEK, 2023), these models may have strong performance, yet we are not authorized to evaluate and publicize their results. Therefore, we only report results from models with open APIs or weights in this work, while we anticipate the developers of other models to submit and optionally publicize their models' results in our website. A detailed description of the evaluated models can be found in Appendix $\mathrm{E}$.

### 3.3 Results

General comparison: Zero- and five-shot answer-only results are shown in Table 3 and Table 4 respectively. We report the average accuracy, while a detailed breakdown of accuracy per subject is provided in Appendix F. GPT-4 is the only model that exceeds $60 \%$ average accuracy, highlighting the challenge presented by C-EVAL. GPT-4 significantly outperforms all other models, with the second-best model, ChatGPT, trailing over 14 percentage points behind in both zero- and five-shot settings. Claude-v1.3 achieves similar performance to ChatGPT, in terms of both the category-wise

| Model | STEM | Social Science | Humanities | Other | Average |
| :--- | :---: | :---: | :---: | ---: | ---: |
| Random | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 |
| GPT-4 | 67.1 | 77.6 | 64.5 | 67.8 | 68.7 |
| ChatGPT | 52.9 | 61.8 | 50.9 | 53.6 | 54.4 |
| Claude-v1.3 | 51.9 | 61.7 | 52.1 | 53.7 | 54.2 |
| Claude-instant-v1.0 | 43.1 | 53.8 | 44.2 | 45.4 | 45.9 |
| GLM-130B | 34.8 | 48.7 | 43.3 | 39.8 | 40.3 |
| Bloomz-mt | 35.3 | 45.1 | 40.5 | 38.5 | 39.0 |
| LLaMA-65B | 37.8 | 45.6 | 36.1 | 37.1 | 38.8 |
| ChatGLM-6B | 30.4 | 39.6 | 37.4 | 34.5 | 34.5 |
| Chinese-LLaMA-13B | 31.6 | 37.2 | 33.6 | 32.8 | 33.3 |
| MOSS | 28.6 | 36.8 | 31.0 | 30.3 | 31.1 |
| Chinese-Alpaca-13B | 26.0 | 27.2 | 27.8 | 26.4 | 26.7 |

Table 4: Five-shot average accuracy (\%) in answer-only setting. We report the average accuracy over the subjects within each category. "Average" column indicates the average accuracy over all the subjects.

| Model | STEM | Social Science | Humanities | Other | Average |
| :--- | :---: | :---: | :---: | ---: | ---: |
| Random | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 |
| GPT-4 | 67.3 | 76.5 | 64.4 | 66.6 | 68.3 |
| Claude-v1.3 | 51.9 | 63.2 | 50.9 | 53.6 | 54.2 |
| ChatGPT | 47.8 | 58.3 | 47.7 | 48.5 | 50.0 |
| Claude-instant-v1.0 | 43.3 | 52.7 | 41.3 | 42.4 | 44.5 |
| ChatGLM-6B | 29.9 | 40.0 | 37.9 | 34.5 | 34.5 |
| MOSS | 27.3 | 38.1 | 33.6 | 29.4 | 31.2 |
| LLaMA-65B | 28.0 | 36.3 | 29.3 | 30.0 | 30.3 |
| GLM-130B | 24.8 | 33.1 | 30.8 | 30.0 | 28.8 |
| Chinese-LLaMA-13B | 20.5 | 30.5 | 28.2 | 27.1 | 25.4 |

Table 5: Five-shot average accuracy (\%) in chain-of-thought setting. We report the average accuracy over the subjects within each category. "Average" column indicates the average accuracy over all the subjects. Bloomz-mt and Chinese-Alpaca-13B are excluded as they could not generate valid reasoning and thus fail to answer for many questions.

average and the overall average. In addition to average accuracy, Table 9 in Appendix Freveals that GPT-4 surpasses ChatGPT in almost every subject, indicating a comprehensive advantage. Among Chinese-oriented models, GLM-130B exhibits the best performance, ranking the fifth in terms of both zero- and few-shot performance, 7.0 and 14.1 points behind ChatGPT in zero- and five-shot settings respectively. We observe that smaller models, despite undergoing instruction tuning, still struggle to achieve a $40 \%$ accuracy. This contradicts recent assertions that a 10B-scale instruction-tuned model can achieve comparable performance to ChatGPT (Taori et al., 2023, Chiang et al., 2023) - we argue that while these models may perform well on simpler tasks, their inherent advanced abilities significantly lag behind when faced with more complex scenarios.

Does few-shot prompting help? Comparing Table 4 to Table 3 , we find that while few-shot prompting helps many models achieve better results, it hurts performance of GLM-130B, Bloomz-mt, ChatGLM-6B, MOSS, and Chinese-Alpaca-13B. All of these models have undergone instruction tuning 6 and we hypothesize that the accuracy drop is because that these models have not (appropriately) incorporated few-shot demonstrations into the instruction tuning stage, as emphasized in Chung et al. (2022), thus sacrificing few-shot in-context learning performance to obtain enhanced zero-shot instruction-following abilities.

Does chain-of-thought prompting help? The average accuracy in the COT setting is reported in Table 5, while Table 10 in Appendix F provides a detailed breakdown of the accuracy per subject. We exclude Bloomz-mt and Chinese-Alpaca-13B since these two models are unable to generate valid COT reasoning for a large portion of questions, failing to produce final answers. All models achieve comparable or lower average accuracy than in the answer-only setting. This suggests that[^3]

| Model | Zero-shot AO | Five-shot AO | Five-shot COT |
| :--- | ---: | ---: | ---: |
| GPT-4 | 53.3 | 54.9 | 56.8 |
| Claude-v1.3 | 37.6 | 39.0 | 39.2 |
| ChatGPT | 36.7 | 41.4 | 35.0 |
| Claude-instant-v1.0 | 32.1 | 35.5 | 33.4 |
| Bloomz-mt | 30.8 | 30.4 | - |
| GLM-130B | 30.7 | 30.3 | 22.6 |
| LLaMA-65B | 29.8 | 31.7 | 21.4 |
| ChatGLM-6B | 29.2 | 23.1 | 26.1 |
| MOSS | 28.4 | 24.0 | 21.6 |
| Chinese-LLaMA-13B | 27.5 | 27.3 | 15.4 |
| Chinese-Alpaca-13B | 24.4 | 27.1 | - |

Table 6: Average accuracy on C-EvAL HARD in both answer-only (AO) and chain-of-thought (COT) settings.

COT prompting does not necessarily improve results for many subjects in C-EvAL. The primary reasons for this are twofold: (1) many subjects in C-EVAL are not reasoning-intensive, and additional reasoning steps may impair performance. This observation is supported by Chung et al. (2022), who noted performance degradation on MMLU with COT prompting. (2) Some models fail to leverage the benefits of COT prompting, particularly those that did not undergo COT-inclusive instruction tuning. Chung et al. (2022) reported this, noting an 8-point accuracy drop when using COT on MMLU with the 540B PaLM model. This finding partly elucidates the significant decrease in performance of the GLM-130B and LLaMA-65B models in the COT setting. Encouragingly, we still observe that COT prompting leads to considerable improvements for some models in certain subjects - for example, detailed results in Table 10 show that COT improves GPT-4's performance in college physics from $50.6 \%$ to $60.2 \%$, in probability and statistics from $53.6 \%$ to $62.0 \%$, ChatGLM's performance in middle school physics from $20.2 \%$ to $41.0 \%$, and in high school geography from $29.2 \%$ to $38.2 \%$.

Difference between English- and Chinese-oriented models: GLM-130B is the best-performing Chinese-oriented model in our assessment, thus we focus on comparing it to the represented Englishoriented model, ChatGPT, in zero-shot answer-only settings. We do not analyze GPT-4 here since it is not at the same level as all other models, and comparing GLM-130B to it is not very helpful and informative. As illustrated in Table 3 , while GLM-130B underperforms ChatGPT by 7.0 points on overall average, the gap significantly narrows on the social science and humanities category, lagging only 2.2 and 1.1 points behind respectively. This reflects that by leveraging more Chinese data, the model might achieve performance on par with or even superior to ChatGPT in areas pertaining to Chinese knowledge, such as history, politics, and law, highlighting situations where Chinese-oriented models may have the upper hand. However, concurrently, we note a significant difference of 12.3 points between GLM-130B and ChatGPT in the STEM category, which implies a substantial gap in more complex tasks that necessitate advanced reasoning skills.

Results on C-Eval Hard: Table 6 shows the average accuracy on C-Eval Hard. GPT-4 can only achieve $53.3 \%, 54.9 \%, 56.8 \%$ accuracy on zero-shot AO, five-shot AO, and five-shot COT settings respectively, implying the difficulty of C-Eval HARD. Interestingly, chain-of-thought prompting improves GPT-4 slightly on these extremely challenging subjects. Indeed, only GPT-4, ChatGPT, and Claude manage to make meaningful progress - improving by at least 10 points - over a random baseline. Our results further confirm that some critical distinction among LLMs comes out when the tasks become complex enough. We underscore the importance of evaluating LLMs in such challenging settings, as current LLM development goes beyond creating a casual chatbot - it involves the development of complex systems or agents capable of interacting with various data types, receiving feedback, reasoning and using tools, and even performing actions (Mialon et al., 2023).

Results on the validation split: Since we do not publicly release the labels for our test split, we provide the average accuracy on the validation split as a reference for developers. The validation split comprises a total of 1346 questions, with each subject contributing fewer than 30 validation questions on average. Therefore, tracking accuracy on a specific subject may not yield significant insights. Instead, we report the average answer-only accuracy across all subjects in Table 7, The average validation accuracy closely mirrors the average test accuracy as presented in Table 3 and Table 4. Additionally, the model ranking on the validation split broadly corresponds to that on the test split. These observations suggest that developers may use the average validation accuracy as a good indicator for expedited development processes.

| Model | Zero-shot | Five-shot |
| :--- | ---: | ---: |
| GPT-4 | 66.7 | 69.9 |
| Claude-v1.3 | 52.1 | 55.5 |
| ChatGPT | 50.8 | 53.5 |
| Bloomz-mt | 45.9 | 38.0 |
| GLM-130B | 44.2 | 40.8 |
| Claude-instant-v1.0 | 43.2 | 47.4 |
| ChatGLM-6B | 39.7 | 37.1 |
| LLaMA-65B | 38.6 | 39.8 |
| MOSS | 35.1 | 28.9 |
| Chinese-Alpaca-13B | 32.0 | 27.2 |
| Chinese-LLaMA-13B | 29.4 | 33.1 |

Table 7: Average accuracy on the validation split in the answer-only setting.

## 4 Related Work

English benchmarks: Traditional English benchmarks mainly focus on assessing certain abilities of models on a single task or a single type of tasks, such as natural language understanding (NLU, Wang et al. (2019)), reading comprehension (Rajpurkar et al., 2018), machine translation (Bojar et al. 2014), and summarization (Hermann et al., 2015, Narayan et al., 2018). As a representative example, the GLUE benchmark (Wang et al. 2019) combines a collection of NLU tasks, and has witnessed superhuman model performance due to the burst of pretraining models such as BERT (Kenton \& Toutanova, 2019) and GPT (Radford et al. 2019). In order to assess the capabilities of LLMs more comprehensively, recent benchmarks have cast light on the broader knowledge and advanced abilities. The MMLU benchmark (Hendrycks et al. 2021a) provides multi-domain and multi-task evaluation collected from real-world examinations and books. LLMs' performance on MMLU fluctuates around random-chance accuracy until they reach the scale of GPT-3. The BIG-bench benchmark (Srivastava et al. 2022) consists of 204 diverse tasks, some of which are considered to be beyond the capabilities of current LLMs. The HELM benchmark (Liang et al., 2022) aggregates 42 different tasks and evaluates LLMs with 7 metrics ranging from accuracy to robustness.

Chinese benchmarks: Despite the flourishing of English benchmark, language abilities in Chinese language environment remain under-developed. The CLUE benchmark ( $\mathrm{Xu}$ et al. 2020) is the first large-scale Chinese NLU benchmark, and still serves as the most widely-used and best available Chinese benchmark. Recently, the AGIEval benchmark (Zhong et al., 2023) contains data from the Chinese College Entrance Exam, Chinese lawyer qualification test and Chinese civil service examination. The MMCU benchmark (Zeng, 2023) consists of tests from four major domains including medicine, law, psychology and education, which are also collected from Chinese College Entrance Exam, qualification test as well as university examinations. Compared to AGIEval and MMCU, C-EVAL (1) has a broader coverage of domains ( $\$ 2.2$, (2) features four different levels of difficulty - particularly, the C-EVAL HARD benchmark is the first Chinese benchmark to provide sophisticated reasoning problems, and (3) makes an effort to mitigate data leakage - our questions mostly come from mock exams as PDF or Microsoft Word documents that are further processed by us, while AGIEval and MMCU collects the exact questions from past national exams in China.

## 5 Discussion

We believe that the evaluation of LLMs should transcend the scope of casual conversational bots, guiding developers in preparing LLMs to function in more complex scenarios. This was the primary motivation behind the creation of C-EvAL, a challenging evaluation suite. We hope that C-EVAL along with C-EVAL HARD have made important progress on this direction particularly in a Chinese context. We also note that, C-EVAL, along with all the English-language benchmarks, is far from perfect for LLM evaluation. There are many other abilities such as reasoning over and calling APIs, as well as multiple aspects that extend beyond accuracy, including safety, bias, and robustness. We leave further exploration on their evaluation for future work.

## Acknowledgement

We thank the anonymous reviewers for their comments. Yuzhuo Bai is supported by the National Key

R\&D Program of China (No. 2020AAA0106502) and Institute Guo Qiang at Tsinghua University.

## References

Alibaba. Tongyi qianwen. Alibaba Blog, 2023. URLhttps://tongyi.aliyun.com.

Anthropic. Introducing claude. Anthropic Blog, 2022. URL https://www.anthropic.com/ index/introducing-claude

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

Baidu. Wenxin yiyan. Baidu Blog, 2023. URL https://yiyan.baidu.com

Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 12-58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-3302. URL https://aclanthology.org/W14-3302

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177, 2023. URLhttps://arxiv.org/abs/2304.08177.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. $320-335,2022$.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

iFLYTEK. Xunfei xinghuo. iFLYTEK Blog, 2023. URLhttps://xinghuo.xfyun.cn

Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 41714186, 2019.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.

Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797-1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1206. URL https://aclanthology.org/D18-1206

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.

OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI Blog, 2022. URL https: //openai.com/blog/chatgpt/.

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

OpenLMLab. Moss. https://github.com/OpenLMLab/MOSS, 2023.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784-789, 2018.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.

THUDM. ChatGLM. https://github.com/THUDM/ChatGLM-6B, 2023a.

THUDM. ChatGLM2. https://github.com/THUDM/ChatGLM2-6B, 2023b.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=rJ4km2R5t7

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= _VjQlMeSB_J

Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023.

Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 47624772, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: $10.18653 / v 1 / 2020$.coling-main.419. URL https://aclanthology.org/2020 coling-main. 419 .

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=-Aw0rrrPUF

Hui Zeng. Measuring massive multitask chinese understanding. arXiv preprint arXiv:2304.12986, 2023.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WZH7099tgfM.
