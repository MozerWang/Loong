# Speaker Verification in Agent-generated Conversations 

Yizhe Yang ${ }^{1 *}$, Heyan Huang ${ }^{1}$, Palakorn Achananuparp ${ }^{2}$, Jing Jiang ${ }^{2}$, Ee-Peng Lim ${ }^{2}$<br>${ }^{1}$ School of Computer Science and Technology, Beijing Institute of Technology<br>${ }^{2}$ Singapore Management University<br>\{yizheyang, hhy63\}@bit.edu.cn, \{palakorna,jingjiang,eplim\}@smu.edu.sg


#### Abstract

The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.


## 1 Introduction

The recent advances in large language models (LLMs) have significantly increased the capabilities of conversational AI to solve challenging dialogue problems (Zhao et al., 2023; Chang et al., 2023; Park et al., 2023; Wang et al., 2023b; Gao et al., 2023). In particular, LLM-based role-playing chatbots have been developed to simulate speakers of different personal attributes and linguistic styles so as to provide more immersive interaction with users (Shanahan et al., 2023). Nevertheless, there is a lack of study on how well the agent-generated utterances are personalized according to the target characters/speakers. Conventional evaluations of[^0]

role-playing agent models typically focus on assessing their background knowledge through question answering, or rely on judgement by humans or other LLMs (Wang et al., 2023c; Tu et al., 2024; Shen et al., 2023; Xiao et al., 2023). These evaluation methodologies do not address the consistency between utterances of the simulated speaker and that of the target speaker in linguistic style and personal characteristics. To bridge this gap, we introduce the speaker verification task in agent-based conversations as an important approach to evaluate the personalization ability of conversational agents.

Speaker verification refers to the task of determining if two sets of utterances belong to the same speaker. In the context of evaluating role-playing agent models, a positive match between the generated and real utterances of a speaker suggests the former preserves the speaker's distinct linguistic style and personal traits. Unlike the authorship attribution task (Rivera-Soto et al., 2021; Wegmann et al., 2022), speaker verification may involve two sets of utterances from unseen speakers (i.e., speakers not seen in the training data). Given that roleplaying agents can simulate virtual characters or user-customized personas in diverse conversation settings (e.g., movies, sitcoms, interest sharing, and counselling), we aim to develop models that can robustly verify utterances of unseen speakers in these different settings. While authorship attribution is closely related, speaker verification goes beyond linguistic style to also consider content, which can reveal personal characteristics, including personality traits, moral foundations, value. These characteristics is hard for role-playing model to mirror than demographics. To evaluate the speaker verification models, we compile conversation data from various sources, encompassing thousands of speakers and employ a variety of methods including style embedding, authorship verification, and fine-tuned models.

Following previous research that highlights the
significant influence of topic on style identification (Wegmann et al., 2022), we have designed our evaluation to control for conversation setting by introducing three levels of difficulty: Base, where the two sets of utterances may come from different conversation contexts and thus easier to distinguish; Hard, where utterances are from the same conversation context; and Harder, where utterances are from the same conversation. This approach allows us to isolate the impact of conversation topic on speaker verification accuracy. Moreover, Our experiment design includes diverse testing scenarios of different degrees of exposure to speakers and utterances, corresponding to different applications. Our extensive experiment results show that neither non-experts nor the ChatGPT are able to perform speaker verification accurately, highlighting both the challenges and the limitations of current evaluation. In contrast, our proposed fine-tuned models demonstrate the ability to differentiate between speakers effectively, and thus the potential to evaluate the ability of role-playing agents personalizing utterances to speakers. Despite the poorer performance at the Harder level, we argue that the topic should also be considered in speaker verification processes, as it reflects the speaker's personal topic preference. For example, when agents simulate Harry Potter, the topic should be around the wizarding world, reflecting his magical background. In contrast, the topic for simulating Sheldon Cooper, a physicist, may focus on theoretical physics, highlighting their distinct backgrounds and interests.

Moreover, we utilize the developed speaker verification models to evaluate how well agentgenerated utterances could preserve the personal identity of the speakers. We expect that the utterances generated by the role-playing model should closely mirror the style and persona of the speaker being simulated, while also demonstrating clear differentiation from other speakers, including the model itself. To evaluate these two aspects, we introduce two metrics: (a) Simulation Score, which evaluates the alignment of agent-generated utterances with actual utterances from the target speaker, and (b) Distinction Score, which measures the differences among utterances generated by the same agent model when simulating various speakers. Moreover, we visualize the similarity distributions estimated by speaker verification models. The separability between similarity distributions of positive and negative pairs serves as an indicator of the model's simulation proficiency. Our analyses re- veal that LLMs, whether prompted or specifically fine-tuned, generally struggle to simulate the style or personal characteristics of a target speaker. Furthermore, the limited variability in utterances generated by the same role-playing model for different speakers points to an inherent linguistic consistency within these LLMs. This consistency limits the models' ability to significantly alter their linguistic style when tasked for diverse speakers.

We summarize our contributions as follows:

- We define a novel task of speaker verification in agent-based conversations, which is essential to evaluate the role-playing models. To develop our models, we compiled a dataset from a wide range of conversations involving thousands of speakers. Our study indicates that specifically fine-tuned models demonstrate the most promising performance in speaker verification tasks.
- We utilize the developed speaker verification models to assess current role-playing models, addressing a gap in the evaluation. Our analyses reveals that current LLM-based roleplaying models fail to simulate the target speaker and exhibit a specific linguistic style that proves difficult to modify. Our research introduces a rigorous evaluation metric for role-playing models, highlighting a substantial opportunity for improvement in this area.

The structure of our paper is organized as follows: After introducing related work (Section 2), we first construct a dataset for the task of speaker verification in conversations (Section 3). We then test the performance on this task by humans and by ChatGPT (Section 4). Observing that neither humans nor LLMs can perform well, we design several speaker verification models and evaluate their performance on the dataset we have constructed (Section 5). Finally, we use our trained speaker verification model to evaluate several existing roleplaying models (Section 6).

## 2 Related Work

### 2.1 Author Verification

Authorship verification aims to identify whether two pieces of text are written by the same person. For this task, researchers have proposed contrastive learning to cluster the representations of texts from
the same author closer to one another while increasing the distance between text representations of different authors (Rivera-Soto et al., 2021; Reimers and Gurevych, 2019; Wang et al., 2023a; Wegmann et al., 2022). Recently, Aggazzotti et al. (2023) evaluate authorship attribution models's capacity to identify speakers in speech transcripts, a task similar to ours. Our study focus on verifying utterances from unseen speakers in various conversations like movies, sitcoms, interest sharing, and counselling, beyond mere speech transcripts. Additionally, we utilize speaker verification models to assess the role-playing models, and introduce a novel framework for evaluation.

### 2.2 Role-Playing Conversation AI

Recent research in LLMs has focused on exploring their potential as role-playing agents through strategies like prompting or fine-tuning (Shao et al., 2023; Wang et al., 2023c; Zhou et al., 2023). Nevertheless, the evaluation of generated utterances with respect to the target speaker remains under-explored. Prior research typically evaluates role-playing agents in two main approaches: (1) through evaluation judgements made by humans or LLMs (Shao et al., 2023; Wang et al., 2023c; Zhou et al., 2023), and (2) by employing questionanswering tests or reward models on specific benchmarks (Tu et al., 2024; Shen et al., 2023; Xiao et al., 2023). Nevertheless, our research highlights a significant gap in the ability of non-experts and LLMs to distinguish between different speakers, indicating their unreliability in the evaluation. Moreover, existing benchmarks concentrate on a limited set of speakers and the question-answering testing is mismatch with conversation. Consequently, we propose speaker verification models to assess the extent to which AI-generated utterances reflect the personal identity of speakers.

## 3 Data Collection

In this section, we describe the datasets we construct for training and evaluating our speaker verification models. Recall that our goal is to identify whether two sets of utterances belong to the same speaker, including the speaker's utterances generated by role-playing agents. Therefore, we adopt the conversations from films, television series, and literary fiction as characters from these sources are commonly employed in the development and evaluation of role-playing agents. The conversations included in our datasets are sourced from:

Cornell Movie Dialogues. This is a large collection of fictional conversations extracted from raw movie scripts

Friends. This is a conversational dataset from the 'Friends' TV sitcom, with 3,107 scenes and 67,373 utterances among 700 characters (Chen and Choi, 2016).

Harry Potter. This is a conversation corpus derived from the transcripts of the Harry Potter movie series ${ }^{1}$ and the Harry Potter Dialogue Dataset (Chen et al., 2023)

The Big Bang Theory. This is a conversation corpus from the 'The Big Bang Theory' transcripts", covering 2,191 scenes with $1,966,215$ utterances.

In addition to the linguistic style, we also believe that personal characteristics are valuable for speaker verification. Therefore, we choose the following conversation datasets that discuss about personal traits:

Multiple Sessions Conversation. This is a longterm conversation dataset including multiple sessions, where participants share their personal characteristics such as interests (Xu et al., 2021). We treat multiple sessions as distinct conversations from the same speaker.

AnnoMI This dataset comprises therapy conversations between clients and counselors, selected for its inclusion of discussions on clients' personal traits. Our approach centers on client verification and segments the entire counseling dialogues into multiple sessions according to different stages of the counseling process.

Following the framework of Authorship Verification (Stamatatos et al., 2022; Wegmann et al., 2022; Rivera-Soto et al., 2021), we construct our datasets by pairing sets of utterances. These pairs are labeled 'positive' when both sets of utterances originate from the same speaker, and 'negative' otherwise. To balance the labeled data and to prevent overfitting, we maintain an equal number of positive and negative labeled utterance set pairs.

We first split the speakers into seen and unseen ensuring no overlapping speakers between the two sets. Subsequently, we create pairs of utterance[^1]sets among these two speaker sets. Pairs of utterance sets from the unseen speakers form the Unseen-Unseen test set. We further divide pairs of utterance sets from seen speakers into three subsets: training set, Seen-Seen test set and Seen-Unseen test set . The Seen-Seen test set comprises pairs of utterance sets that were both included in the training set, although they are paired differently. The Seen-Unseen test set consists of pairs such that each pair combines an utterance set from the training set with another utterance set not in the training set. This approach yields three test sets that vary based on whether the speakers or utterances have been encountered in the training dataset. The performance of the Seen-Seen setting is considered to represent the upper bound of speaker verification models as it benefits from well-trained utterance representations. Only the Seen-Unseen setting has adopted in speech-based speaker verification and author attribution research, which tests the model's ability to verify known speakers in novel conversations. In contrast, our primary emphasis is on the Unseen-Unseen test set, which evaluates the model's adaptability to completely new speakers.

Moreover, we categorize the test sets based on the source of speakers in negative pairs. The negative pairs in Base level consist of two speakers comes from different sources, such as pairs consists of utterances from Hermione Granger (from Harry Potter) and utterances from Sheldon Cooper (from The Big Bang Theory). The Hard level introduces a coarse-grained topic control, ensuring negative pairs consist of utterances from speakers within the same source. For example, a negative pair may consist of Hermione Granger and Ron Weasley, who are both characters within the Harry Potter universe. furthermore, the Harder level intensifies the topic control by resticting negative pairs solely from utterances of different speakers within the same conversation, such as pairing utterances from Hermione Granger with those of Ron Weasley in the same conversation. These categorization isolates the impact of conversation topic on the speaker verification which similar to the setting in authorship attribution (Wegmann et al., 2022).

The data statistics are detailed in Appendix 9.

## 4 Speaker Verification by Human and LLMs

Speaker verification requires the ability to identify personal traits and linguistic styles. LLM's perfor- mance on such an intricate and nuanced task has been not studied much so far (Ji et al., 2023). Consequently, we want to assess the performance of LLMs and humans (non-experts) in speaker verification tasks first.

Given that neither humans nor LLMs undergo training in this study, we create two types of samples categorized into two complexity levels: 'Conversation' and 'Utterances'. The 'Conversation' option provides human/LLM with pairs of dialogues, offering context that could reveal more detailed information, such as names. The 'Utterances' option on the other hand presents only pair of utterance sets following our speaker verification task definition. Each option comprises 200 pairs randomly selected from our Unseen-Unseen test set. We engage ten human annotators, who are nonexperts, to assess whether pairs of conversations or sets of utterances are from the same speaker. For ChatGPT (utilizing gpt-3.5-turbo-1106), we examine its performance under zero-shot, Chain-ofThought (Wei et al., 2022), and few-shot paradigms. The detailed setup can be found in Appendix 11

Table 1 shows that ChatGPT with 6-shot performs better than other ChatGPT variants and human users in most of the Base and Hard Levels. Nevertheless, both human and ChatGPT demonstrate only modestly better accuracy than random guess when evaluated with the 'Utterances' option. The results for the 'Conversation' option consistently are also better than that for the 'Utterances' option. This could be attributed to the existence of speaker mentions (e.g., names) within the utterances of other speakers. Such information allows the model/human to discern the speakers more accurately. Interestingly, humans exhibit strong performance at the Harder level of the task. Upon checking the annotators, we discovered that human users find it easy to recognize two sets of utterances originating from the same conversation, thereby inferring that the utterances belong to different speakers. This insight highlights human users' better understanding of the nuances in conversations than ChatGPT. Despite these observations, neither human annotators nor LLMs consistently demonstrate the capability to differentiate between speakers based solely on utterances, suggesting the need to fine-tune smaller neural networks for improved task performance.

|  | Conversation |  | Utterances |  |
| :---: | :---: | :---: | :---: | :---: |
|  | ACC | F1 | ACC | F1 |
| Base Level |  |  |  |  |
| Human | 67.48 | 67.68 | 56.50 | 58.77 |
| ChatGPT |  |  |  |  |
| - ZeroShot | 59.59 | 62.85 | 55.37 | 57.29 |
| - COT | 70.58 | 70.59 | 56.41 | 59.20 |
| - 2-Shot | 71.43 | 72.69 | 56.62 | 59.95 |
| - 4-Shot | 72.97 | 73.03 | 56.59 | 59.64 |
| -6-Shot | 73.61 | 73.12 | 56.97 | $\mathbf{6 0 . 0 7}$ |
| Hard Level |  |  |  |  |
| Human | 63.34 | 64.02 | 54.92 | 58.32 |
| ChatGPT |  |  |  |  |
| - ZeroShot | 57.33 | 62.14 | 54.69 | 57.85 |
| - COT | 68.28 | 68.57 | 55.26 | $\mathbf{5 8 . 8 5}$ |
| - 2-Shot | 70.85 | 70.92 | 55.38 | 58.73 |
| - 4-Shot | 72.37 | 72.40 | 55.54 | 58.80 |
| - 6-Shot | 72.56 | 72.51 | 55.81 | 58.82 |
| Harder Level |  |  |  |  |
| Human | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_133718bb1f831f1e54dbg-05.jpg?height=56&width=110&top_left_y=1207&top_left_x=588) | 73.28 | 74.12 |
| ChatGPT |  |  |  |  |
| - ZeroShot | - | - | 41.05 | 42.93 |
| - COT | - | - | 47.26 | 42.00 |
| - 2-Sho | - |  | 49.38 | 43.14 |
| - 4-Shot | - | - | 46.54 | 45.50 |
| - 6-Shot | - | - | 49.06 | 40.60 |

Table 1: Comparative analysis of speaker verification accuracy and F1 among humans and ChatGPT across different contextual settings. Notably, for the Harder level, conversation formats are excluded from comparison because two conversations are identical

## 5 Speaker Verification Models

### 5.1 Models

Style-Based Models To harness the stylistic aspects of utterances for speaker verification, we incorporate two style-based models to derive styledependent embeddings. The use of LIWC (Linguistic Inquiry and Word Count) dimensions as stylistic features is well-documented in Niederhoffer and Pennebaker (2002). Specifically, we apply the LIWC 2015 framework (Pennebaker et al., 2015) to generate style embeddings, utilizing the Language Style Matching (LSM) metric (Ireland and Pennebaker, 2010; Gonzales et al., 2010). Furthermore, we incorporate LISA (Patel et al., 2023), a style embedding model trained on an extensive synthetic stylometry dataset.
Authorship Attribution Models We leverage three models trained on extensive textual corpora to capture different facets of linguistic representation through contrastive learning strategies. (1) Sentence-BERT (SBERT) is a fine-tuned version of RoBERTa (Reimers and Gurevych, 2019). (2) RoBERTa which generates semantically meaningful sentence embeddings (Liu et al., 2019). (2) Unlike SBERT which focuses on content only, STEL is designed to explicitly discern writing styles independent of content (Wegmann et al., 2022). (3) LUAR is a model trained to generate universal authorship representations from a diverse range of text sources. It excels at identifying authors across different contexts without relying heavily on content similarity. (Rivera-Soto et al., 2021)

Fine-Tuned Models With the absence of models trained for speaker verification in the prior work, we fine-tune existing models using our custombuilt training set with contrastive loss objective function (Chopra et al., 2005). The fine-tuned models are initialized by (1) STEL (Wegmann et al., 2022) (2) SBERT (Reimers and Gurevych, 2019) (3) RoBERTa-based (Liu et al., 2019) and (4) LUAR (Rivera-Soto et al., 2021) and thus denoted

![](https://cdn.mathpix.com/cropped/2024_06_04_133718bb1f831f1e54dbg-05.jpg?height=52&width=774&top_left_y=1459&top_left_x=1052)
respectively

Instead of the traditional approach of concatenating all utterance texts as the input for models, we employ a hierarchical encoding methodology that is better suited for speaker verification in conversations. Specifically, we embed each utterance independently by encoder models and then derive the final embedding by mean pooling of all individual utterance vectors. This approach is appropriate for two main reasons: (1) these models have been trained at the sentence level and are thus more effective when processing single utterances; (2) concatenating all utterances could potentially exceed the models' maximum length limits, whereas processing utterances independently allows for handling conversation with arbitrary number of utterances.

We train the above models over 5 epochs with a batch size of 1024 , incorporating $10 \%$ of the training data for warm-up steps to gradually adjust the learning rate, utilizing the Adam optimizer with a learning rate of $2 \mathrm{e}-5$. To harness the complementary strengths of different models, we combine the features from the fine-tuned models into a unified Mixed Features model, aiming to capture a broader spectrum of speaker-specific attributes.

| Model | Seen-Seen |  |  | Seen-Unseen |  |  | Unseen-Unseen |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | AUC | ACC | F1 | AUC | ACC | F1 | AUC | ACC | F1 |
| Base Level |  |  |  |  |  |  |  |  |  |
| LIWC | $52.56_{ \pm 1.43}$ | $50.45_{ \pm 0.48}$ | $42.96_{ \pm 0.23}$ | $51.26_{ \pm 0.44}$ | $51.59 \pm 0.38$ | $49.08_{ \pm 0.65}$ | $54.90_{ \pm 0.85}$ | $55.15_{ \pm 0.71}$ | $53.87_{ \pm 2.09}$ |
| LISA | $76.80_{ \pm 0.23}$ | $78.37_{ \pm 0.74}$ | $65.03_{ \pm 0.60}$ | $67.66_{ \pm 0.29}$ | $62.69_{ \pm 0.29}$ | $62.60_{ \pm 0.29}$ | $9.44_{ \pm 0.83}$ | $64.88_{ \pm 0.82}$ | $64.44_{ \pm 1.19}$ |
| STEL | $79.36_{ \pm 0.89}$ | $79.16_{ \pm 0.35}$ | $66.86_{ \pm 1.00}$ | $67.98_{ \pm 0.31}$ | $66.99_{ \pm 0.13}$ | $65.98_{ \pm 0.42}$ | $78.37 \pm 1.22$ | $74.51_{ \pm 1.40}$ | $74.37 \pm 1.43$ |
| SBERT | $89.34_{ \pm 0.10}$ | $84.13_{ \pm 0.07}$ | $78.71_{ \pm 0.22}$ | $86.18_{ \pm 0.12}$ | $78.45_{ \pm 0.17}$ | $78.45_{ \pm 0.17}$ | $82.49 \pm 1.34$ | $75.13 \pm 1.47$ | $75.11_{ \pm 1.48}$ |
| LUAR | $87.29_{ \pm 0.27}$ | $85.24_{ \pm 0.44}$ | $78.92 \pm 0.55$ | $86.25_{ \pm 0.75}$ | $78.77_{ \pm 0.64}$ | $78.74_{ \pm 0.62}$ | $85.75 \pm 0.51$ | $79.61 \pm 0.70$ | $79.57 \pm 0.70$ |
| $\operatorname{STEL}_{f t}$ | $97.21_{ \pm 2.31}$ | $95.93_{ \pm 2.61}$ | $93.19 \pm 1.58$ | $91.92 \pm 1.61$ | $85.78_{ \pm 1.74}$ | $83.18_{ \pm 2.15}$ | $85.24_{ \pm 2.18}$ | $81.31_{ \pm 2.25}$ | $80.16 \pm 2.31$ |
| $\operatorname{SBERT}_{f t}$ | $97.47 \pm 1.14$ | $96.14_{ \pm 1.78}$ | $92.95 \pm 1.88$ | $7 \pm 1.12$ | 0.55 | $85.66_{ \pm 1.62}$ | $.71 \pm 1.88$ | $1.78_{ \pm 2.04}$ | $80.24 \pm 2.11$ |
| $\operatorname{RoBERTa}_{f t}$ | $97.61_{ \pm 0.73}$ | $96.29 \pm 1.17$ | $93.38 \pm 1.19$ | $92.70_{ \pm 1.08}$ | $86.08_{ \pm 1.07}$ | $86.19_{ \pm 0.96}$ | $86.25_{ \pm 1.07}$ | $82.03_{ \pm 1.77}$ | $80.62 \pm 1.74$ |
| $\operatorname{LUAR}_{f t}$ | $97.47 \pm 1.18$ | $96.46_{ \pm 1.32}$ | $93.57 \pm 1.12$ | $92.49_{ \pm 1.15}$ | $85.96_{ \pm 1.23}$ | $85.94_{ \pm 1.17}$ | $86.27 \pm 1.03$ | $82.38 \pm 1.22$ | $80.19 \pm 1.28$ |
| Mixed Features | $\mathbf{9 8 . 0 5}_{ \pm 0.95}$ | $\mathbf{9 7 . 2 5} \pm 1.06$ | $\mathbf{9 5 . 6 1} \pm 0.97$ | $\mathbf{9 3 . 0 8}_{ \pm 1.02}$ | $\mathbf{8 6 . 3 8}_{ \pm 0.97}$ | $\mathbf{8 7 . 3 5} \pm 0.91$ | $\mathbf{8 8 . 6 1} \pm 0.96$ | $\mathbf{8 3 . 0 8}_{ \pm 1.06}$ | $\mathbf{8 1 . 0 7} \pm 0.95$ |
| Hard Level |  |  |  |  |  |  |  |  |  |
| LIWC | $4_{ \pm 0.48}$ | $52.82 \pm 0.39$ | $52.75_{ \pm 0.30}$ | $49.69_{ \pm 0.31}$ | 50. | $44.4 \quad 4 \quad$ | $.81_{ \pm 0.72}$ | $2.35 \pm 0.44$ | $52.19 \pm$ |
| LISA | $55.13_{ \pm 0.17}$ | $53.99_{ \pm 0.12}$ | $53.44_{ \pm 0.48}$ | $58.73_{ \pm 0.09}$ | $56.53_{ \pm 0.18}$ | $56.43_{ \pm 0.16}$ | $56.10_{ \pm 0.66}$ | $54.87_{ \pm 0.41}$ | $54.71_{ \pm 0.26}$ |
| STEL | $59.10_{ \pm 0.54}$ | $57.69_{ \pm 0.72}$ | $57.55_{ \pm 0.77}$ | $53.06_{ \pm 0.24}$ | $53.24_{ \pm 0.57}$ | $50.02_{ \pm 0.57}$ | $58.62 \pm 1.61$ | $57.57 \pm 1.36$ | $57.47 \pm 1.38$ |
| SBERT | $61.63_{ \pm 0.07}$ | $58.37_{0.25}$ | $58.24_{ \pm 0.15}$ | $98 \pm 0.18$ | $69.72 \pm 0.21$ | $69.71_{ \pm 0.29}$ | $65.88_{ \pm 1.75}$ | $61.23 \pm 1.67$ | $61.08 \pm 1.61$ |
| LUAR | $63.43_{ \pm 0.28}$ | $60.43_{ \pm 0.25}$ | $60.37 \pm 0.27$ | $61.96_{ \pm 0.24}$ | $58.84_{ \pm 0.17}$ | $58.65_{ \pm 0.11}$ | $65.01 \pm 0.38$ | $62.04 \pm 0.30$ | $61.63 \pm 0.50$ |
| $\operatorname{STEL}_{f t}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_133718bb1f831f1e54dbg-06.jpg?height=36&width=143&top_left_y=898&top_left_x=450) | $90.10_{ \pm 2.41}$ | $87.09_{ \pm 2.41}$ | $87.72_{ \pm 2.17}$ | $81.41_{ \pm 1.74}$ | $81.18_{ \pm 1.82}$ | $75.25_{ \pm 1.68}$ | $70.33_{ \pm 1.26}$ | $70.08 \pm 1.34$ |
| $\operatorname{SBERT}_{f t} \quad$ | $92.45_{ \pm 1.49}$ | $90.14_{ \pm 1.69}$ | $87.12 \pm 1.71$ | $87.29 \pm 0.57$ | $81.68_{ \pm 0.55}$ | $81.66_{ \pm 0.56}$ | $78.65 \pm 1.65$ | $73.31 \pm 1.99$ | $73.27 \pm 1.99$ |
| $\operatorname{RoBERTa}_{f t}$ | $92.63_{ \pm 1.58}$ | $90.70_{ \pm 1.81}$ | $87.69_{ \pm 1.82}$ | $89.80_{ \pm 0.52}$ | $82.24_{ \pm 0.25}$ | $81.93_{ \pm 0.23}$ | $78.67 \pm 1.77$ | $73.53 \pm 1.39$ | $73.52 \pm 1.42$ |
| $\operatorname{LUAR}_{f t}$ | $94.93_{ \pm 2.18}$ | $93.03 \pm 1.72$ | $89.16_{ \pm 1.72}$ | $89.79_{ \pm 1.55}$ | $82.61 \pm 1.63$ | $82.60 \pm 1.73$ | $78.69_{ \pm 1.33}$ | $73.25 \pm 1.96$ | $74.19 \pm 1.98$ |
| Mixed Features | $\mathbf{9 5 . 0 2}_{ \pm 1.12}$ | $\mathbf{9 3 . 3 9}_{ \pm 1.21}$ | $\mathbf{8 9 . 3 8} \pm 1.22$ | $\mathbf{9 0 . 6 5} \pm 0.25$ | $\mathbf{8 2 . 9 8}_{ \pm 0.24}$ | $\mathbf{8 2 . 9 3}_{ \pm 0.23}$ | $\mathbf{7 9 . 9 9} \pm 1.21$ | $\mathbf{7 4 . 6 7} \pm 1.14$ | $\mathbf{7 5 . 2 7} \pm 1.15$ |
| Harder Level |  |  |  |  |  |  |  |  |  |
| LIWC | $41.20_{ \pm 0.61}$ | $50.96_{ \pm 0.70}$ | $33.95 \pm 0.18$ | $36.95_{ \pm 0.80}$ | $50.01 \pm 0.01$ | $33.52 \pm 0.27$ | $44.92 \pm 1.83$ | $50.29 \pm 0.29$ | $34.36 \pm 1.15$ |
| LISA | $36.75_{ \pm 0.63}$ | $50.88_{ \pm 0.78}$ | $33.72 \pm 0.34$ | $29.45_{ \pm 0.73}$ | $50.00_{ \pm 0.00}$ | $33.33_{ \pm}$ | $37.43 \pm 2.28$ | $50.05_{ \pm 0.08}$ | $33.45_{ \pm 0.16}$ |
| STEL | $43.11_{ \pm 0.65}$ | $50.82 \pm 0.40$ | $37.18_{ \pm 2.45}$ | $46.69 \pm 0.50$ | 50.01 | 38.09 | $38.68 \pm 2.01$ | $49.45 \pm 0.66$ | $34.96_{ \pm 0.51}$ |
| SBER | $74_{ \pm 0.39}$ | $50.90_{ \pm 0.76}$ | $33.73 \pm$ | $25.12 \pm 0.07$ | $49.99_{ \pm 0.01}$ | $33.32 \pm$ | $22.68 \pm 0.48$ | $50.00 \pm 0.00$ | $33.33 \pm 0.00$ |
| LUAR | $37.12 \pm 0.43$ | $50.89_{ \pm 0.78}$ | $33.73 \pm 0.34$ | $35.44_{ \pm 0.81}$ | $50.00_{ \pm 0.00}$ | $33.33_{ \pm 0.00}$ | $38.65 \pm 0.48$ | $50.00_{ \pm 0.00}$ | $33.33 \pm 0.00$ |
| $\operatorname{STEL}_{f t}$ | $77.91_{ \pm 2.13}$ | $72.34_{ \pm 2.69}$ | $72.09 \pm 2.58$ | $70.92 \pm 2.26$ | $64.56_{ \pm 2.54}$ | $65.35_{ \pm 2.87}$ | $63.13_{ \pm 2.66}$ | $\mathbf{5 8 . 8 1}_{ \pm 2.14}$ | $58.56_{ \pm 2.61}$ |
| $\operatorname{SBERT}_{f}$ | $75.67 \pm 2.87$ | $71.71_{ \pm 2.79}$ | $71.12 \pm 2.56$ | $65.39_{ \pm 2.85}$ | $60.26 \pm 2.48$ | $60.81_{ \pm 2.57}$ | $60.61 \pm 2.58$ | $55.31_{ \pm 2.96}$ | $55.37 \pm 2.37$ |
| $\operatorname{RoBERTa}_{f t}$ | $76.56_{ \pm 2.56}$ | $71.58 \pm 2.56$ | $70.67 \pm 1$ | $68.27 \pm 2.67$ | $63.24_{ \pm 2.57}$ | $64.63 \pm 2$ | $59.67 \pm 2.64$ | $54.74 \pm 2.67$ | $54.67 \pm 2.73$ |
| $\operatorname{LUAR}_{f t}$ | $77.33_{ \pm 2.23}$ | $72.32 \pm 2.21$ | $71.66_{ \pm 1.98}$ | $69.48_{ \pm 2.24}$ | $63.67 \pm 2.34$ | $63.64_{ \pm 2.31}$ | $60.59_{ \pm 2.73}$ | $55.84_{ \pm 2.66}$ | $55.47 \pm 2.17$ |
| Mixed Features | $\mathbf{7 8 . 0 2}_{ \pm 1.24}$ | $\mathbf{7 2 . 3 7} \pm 1.74$ | $\mathbf{7 2 . 3 8}_{ \pm 1.77}$ | $\mathbf{7 1 . 0 9}_{ \pm 1.57}$ | $\mathbf{6 5 . 0 8}_{ \pm 1.46}$ | $\mathbf{6 5 . 4 4} \pm 1.43$ | $\mathbf{6 3 . 2 9}_{ \pm 1.84}$ | $58.67 \pm 1.94$ | $\mathbf{5 8 . 7 4} \pm 1.95$ |

Table 2: Comprehensive overview of model performance across test sets and difficulty levels. Best performances are highlighted in bold

### 5.2 Evaluation Results

We evaluate model by the AUC score, Accuracy, and Macro F1 score ${ }^{3}$. To obtain more reliable results, we implement a three-round validation. Table 2 presents the overall performance of the models across various test sets and difficulty levels, including both the mean and standard deviation of the results from multiple rounds. The findings indicate that authorship attribution models perform better than style-based models, aligning closely with our task definition of speaker verification, which relies not only on style but also on authorship cues. Moreover, out-of-the-box authorship attribution models show commendable performance on conversations, particularly at the Base level, even trained on data from different domains, consistent to Aggazzotti et al. (2023). However, our fine-tuned models significantly outperform other models especially on Hard Level and Harder Level. Mixed Features yield the best results, demonstrating robustness by[^2]

integrating various features.

Across all three test sets, the Hard level consistently resulted in lower scores than the Base level, while the Harder level gets the lowest scores of all. This decline suggests that the speaker verification models may rely on the topic information to verify the speakers. A key factor contributing to the decreased performance at the Harder level may be linguistic accommodation. It's a psychological phenomenon that individuals in conversation tend to adapt their speech style to more closely match that of their interlocutor (Danescu-Niculescu-Mizil et al., 2011; Pardo et al., 2022; Giles et al., 2023; Díaz-Muñoz, 2020). In our dataset, the Multiple Sessions Conversation represent obvious accommodation where speakers, not familiar with each other, are instructed to share personal information. If speakers did indeed accommodate to each other, their speech styles would become increasingly similar over time, making it more challenging for both humans and models to distinguish between them. However, in contrast to authorship attribution, we

| Models | Simulation $\uparrow$ | Distinction $\uparrow$ |
| :--- | ---: | ---: |
| Real | 85.96 | 72.91 |
| LLaMA2-Chat-7B | 47.91 | $\mathbf{6 3 . 5 7}$ |
| LLaMA2-Chat-13B | 44.36 | 53.56 |
| LLaMA2-Chat-70B | 53.91 | 62.43 |
| ChatHaruhi | 47.72 | 49.78 |
| RoleGPT | $\mathbf{5 8 . 9 1}$ | 56.16 |
| CharacterLLM | 49.10 | 38.98 |
| Character.AI | $\mathbf{5 7 . 8 7}$ | $\mathbf{4 9 . 8 9}$ |

Table 3: The Simulation and Distinction scores of roleplaying models. The Real row represents the scores observed in real utterances pairs. (Best results in each category are boldfaced.)

argue that the topic is also a significant feature that can reflect personal characteristics.

At the Base level, the authorship attribution models perform comparably to the fine-tuned models. It indicates that the utterances from different sources reflect distinct styles of the authors, which are readily identifiable by authorship attribution models. To our surprise, although LIWC does not perform as well at the Base and Hard levels, it surpasses some neural network models at the Harder level. This implies that the statistics-based model effectively captures stylistic features without being overly sensitive to content variations. Moreover, different models may prioritize different features. For example, the SBERT model is particularly impacted by content manipulation, with its performance on the Harder level dropping dramaly. In contrast, STEL, having been pre-trained on content-control pairs, can perform better than other models.

## 6 Evaluating Role-Playing Agents

After verifying the effectiveness of our speaker verification models, we employ these models to evaluate the performance of several role-playing conversational agents.

### 6.1 Experiment Settings

In this study, we comprehensively evaluate the following LLM-based role-playing agents: promptbased models (RoleGPT (Wang et al., 2023c), ChatHaruhi (Li et al., 2023), and LLaMA2chat (Touvron et al., 2023)), a specially trained model (CharacterLLM (Shao et al., 2023)), and Character. $\mathrm{AI}^{4}$, a role-playing agent product. The prompt-based models are evaluated using their[^3]

generated utterances for 95 movie roles from RoleBench (Wang et al., 2023c). Due to their training restrictions, CharacterLLM and Character.AI are evaluated using their generated utterances for 9 roles (Shao et al., 2023). We utilize a self-playing mode to create conversations where the same model assumes different roles and chat with itself. The process starts with a generic greeting and continues until the conversation reaches a natural conclusion or the predefined maximum length.

In our evaluation framework, we implement two metrics to assess the role-playing models. The first metric, named Simulation Score, concentrates on the fidelity of simulation, measuring the similarity between the real utterances and the agentgenerated utterances for the same speaker (or role). This metric measures how well an agent replicates the distinctive style and persona of the characters in the utterances. The second metric, named Distinction Score, measures how dissimilar the agentgenerated utterances are for different roles. A high distinction score suggests that the agent is proficient in generating utterances of styles of diverse characters. Because Fine-tuned Model with Mixed Features is more robust, we adopt it to evaluate the aforementioned role-playing agents. The results using other speaker verification models are included in the Appendix 13.

### 6.2 Evaluation Results

Simulation Score As shown in Table 3, the simulation scores between real utterances and agentgenerated utterances by different agent models for the same speaker (or role) are significantly lower than the simulation scores between real utterances of the same speaker which is shown under the "Real" model. RoleGPT, which is based on prompting ChatGPT, achieves the best performance. Surprisingly, despite not being trained on simulations, the LLaMA2-Chat-70B can also generate utterances that closely resemble those of real characters based on role descriptions. A key factor in the success of RoleGPT and LLaMA2-Chat70B is the use of role-specific catchphrases in the prompts, a strategy not employed by ChatHaruhi. Character.AI yields very high simulation score but this result is based on much fewer roles. Notably, CharacterLLM performs only slightly better than LLaMA2-Chat-7B, despite being specially trained for certain roles. We are surprised to find LLaMA2Chat-13B performing worse than its 7B counterpart. By inspecting the generated dialogue, we find that

![](https://cdn.mathpix.com/cropped/2024_06_04_133718bb1f831f1e54dbg-08.jpg?height=342&width=1585&top_left_y=223&top_left_x=241)

Figure 1: The similarity score distribution of positive and negative real-generated pairs.

![](https://cdn.mathpix.com/cropped/2024_06_04_133718bb1f831f1e54dbg-08.jpg?height=343&width=1587&top_left_y=645&top_left_x=243)

Figure 2: The similarity score distribution of positive and negative generated-generated pairs.

both LLaMA2-chat-7B and 13B generate poor utterances, such as repetitions of previous utterances or incorrect endings. However, 13B model tends to overuse catchphrase given in prompt and include extra phrases such as "well well" across various roles. This may indicate that neither LLaMA2chat-7B nor 13B can perform role-playing based on prompts, but the larger model showed more obvious built-in language style leading to its poorer performance. We also analyze the similarity score distribution of pairs of real utterances and generatedutterances for the same speakers, versus that of pairs of real utterances and generated-utterances for two different speakers. As illustrated in Figure 1 , the distributions are not well separated, indicating that the generated utterances do not closely align with their input real-world roles. Therefore, we propose evaluating the distinction between generated utterances when the agent model assumes different roles.

Distinction Score As shown in Table 3, the distinction scores between utterances generated for two different roles by the same agent model are much lower than those between real utterances of the two roles (shown under the 'Real' model). Some agent models, such as CharacterLLM, have their distinction scores so low that the similarity between the generated and real utterances (i.e., 1 - distinction score) is higher than the simulation score between the real and generated utterances of the same role. This indicates that the utterances generated for different roles by these models are more similar than that for the same role. To provide a more detailed analysis, we show the similarity score distribution of pairs between generated utterances in Figure 2. The distributions are closely aligned, especially for CharacterLLM, thereby suggesting that the generated utterances are similar across different role settings. Surprisingly, CharacterLLM, despite being a fine-tuned model, also exhibits the same behavior. This may imply that large language models (LLMs) pre-trained on large datasets develop their own distinctive style, making it challenging to diversify for role-specific simulations.

## 7 Conclusion

In this work, we define the task of speaker verification in conversation and compile a dataset from a variety of sources, including thousands of speakers, to construct a reliable speaker verification system. Our investigation reveals that both non-expert users and ChatGPT cannot distinguish the speakers based on utterances. Through extensive experimentation, we develop and evaluate various speaker verification models, such as style-based, authorship attribution, and specifically fine-tuned models. Our fine-tuned models exhibit promising performance even when applied on completely unseen speakers verification. Additionally, we employ our models to evaluate current LLM-based role-playing agent models by the proposed Simulation Score and Distinction Score metrics. The low Simulation Score shows that the current role-playing agents fail to preserve personal characteristic in generated
utterances while the low Distinction Score indicates these agent models may have their built-in characteristics that persists when playing different roles. The findings highlight that existing roleplaying models may struggle to overcome their built-in characteristics and convincingly imitate actual speaker for immersive conversations.

## Limitations

Our study presents two primary limitations. Firstly, while fine-tuning with a domain-specific dataset can markedly improve performance, even for unseen users and their conversations, the accuracy remains to be less than ideal. There is therefore considerable room to improve the verification accuracy, such as incorporating the utterances of other interlocutors, modeling the interaction as well as leveraging insights of linguistic accommodation. Secondly, our evaluation model predicts a single similarity score for a pair of utterance sets, broadly reflecting their degree of similarity (or difference). This score captures a range of dimensions, including linguistic style, persona traits, and personal background. However, this single score value lacks interpretability that allows it to be mapped to similarity (difference) score in different fine-grained personal dimensions.

## References

Cristina Aggazzotti, Nicholas Andrews, and Elizabeth Allyn Smith. 2023. Can authorship attribution models distinguish speakers in speech transcripts? arXiv preprint arXiv:2311.07564.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology.

Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023. Large language models meet harry potter: A dataset for aligning dialogue agents with characters. In Findings of the Association for Computational Linguistics. EMNLP 2023, pages 8506-8520.

Yu-Hsin Chen and Jinho D Choi. 2016. Character identification on multiparty conversation: Identifying mentions of characters in tv shows. In Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue, pages 90-100.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05), volume 1, pages 539-546. IEEE.

Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words! linguistic style accommodation in social media. In Proceedings of the 20th international conference on World wide web, pages 745-754.

Patricia Díaz-Muñoz. 2020. Accommodation in fiction: The role of convergence in intergroup encounters. Patchwork, 5.:34-54.

Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. 2023. S3 : Social-network simulation system with large language model-empowered agents. arXiv preprint arXiv:2307.14984.

Howard Giles, America L Edwards, and Joseph B Walther. 2023. Communication accommodation theory: Past accomplishments, current trends, and future prospects. Language Sciences, 99:101571.

Amy L Gonzales, Jeffrey T Hancock, and James W Pennebaker. 2010. Language style matching as a predictor of social dynamics in small groups. Communication Research, 37(1):3-19.

Molly E Ireland and James W Pennebaker. 2010. Language style matching in writing: synchrony in essays, correspondence, and poetry. Journal of personality and social psychology, 99(3):549.

Yu Ji, Wen Wu, Hong Zheng, Yi Hu, Xi Chen, and Liang He. 2023. Is chatgpt a good personality recognizer? a preliminary study. arXiv preprint arXiv:2307.03952.

Seong-Hu Kim, Hyeonuk Nam, and Yong-Hwa Park. 2023. Analysis-based optimization of temporal dynamic convolutional neural network for textindependent speaker verification. IEEE Access.

Young-Jun Lee, Chae-Gyun Lim, Yunsu Choi, Ji-Hui Lm, and Ho-Jin Choi. 2022. PERSONACHATGEN: Generating Personalized Dialogues using GPT-3. In Proceedings of the 1st Workshop on Customized Chat Grounding Persona and Knowledge, pages 29-48. Association for Computational Linguistics.

Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. 2023. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint arXiv:2308.09597.

Jin Li, Nan Yan, and Lan Wang. 2021. Fdn: Finite difference network with hierarchical convolutional features for text-independent speaker verification. arXiv e-prints, pages arXiv-2108.

Meng Liu, Kong Aik Lee, Longbiao Wang, Hanyi Zhang, Chang Zeng, and Jianwu Dang. 2023. Crossmodal audio-visual co-learning for text-independent speaker verification. In ICASSP 2023-2023 IEEE

International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Kate G Niederhoffer and James W Pennebaker. 2002. Linguistic style matching in social interaction. Journal of Language and Social Psychology, 21(4):337360 .

Jennifer S Pardo, Elisa Pellegrino, Volker Dellwo, and Bernd Möbius. 2022. Vocal accommodation in speech communication. Journal of Phonetics, $95: 101196$.

Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.

Ajay Patel, Delip Rao, Ansh Kothary, Kathleen McKeown, and Chris Callison-Burch. 2023. Learning interpretable style embeddings via prompting LLMs In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15270-15290, Singapore. Association for Computational Linguistics.

James W. Pennebaker, Ryan Boyd, Kayla Jordan, and Kate Blackburn. 2015. The development and psychometric properties of LIWC2015. University of Texas at Austin.

Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.

Rafael A. Rivera-Soto, Olivia Elizabeth Miano, Juanita Ordonez, Barry Y. Chen, Aleem Khan, Marcus Bishop, and Nicholas Andrews. 2021. Learning Universal Authorship Representations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 913-919. Association for Computational Linguistics.

Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623(7987):493-498.

Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: A trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13153-13187.

Tianhao Shen, Sun Li, and Deyi Xiong. 2023. Roleeval: A bilingual role evaluation benchmark for large language models. arXiv preprint arXiv:2312.16132.
Efstathios Stamatatos, Mike Kestemont, Krzysztof Kredens, Piotr Pezik, Annina Heini, Janek Bevendorff, Benno Stein, and Martin Potthast. 2022. Overview of the authorship verification task at pan 2022. In CEUR workshop proceedings, volume 3180, pages 2301-2313

Fengyi Tang, Lifan Zeng, Fei Wang, and Jiayu Zhou. 2021. Persona authentication through generative dialogue.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang, and Rui Yan. 2023. Characterchat: Learning towards conversational ai with personalized social support. arXiv preprint arXiv:2308.10278.

Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: A chinese benchmark for role-playing conversational agent evaluation. arXiv preprint arXiv:2401.01275.

Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2018. Generalized end-to-end loss for speaker verification. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4879-4883. IEEE.

Andrew Wang, Cristina Aggazzotti, Rebecca Kotula, Rafael Rivera Soto, Marcus Bishop, and Nicholas Andrews. 2023a. Can authorship representation learning capture stylistic features? Transactions of the Association for Computational Linguistics, 11:1416-1431.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023b. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432.

Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023c. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746.

Anna Wegmann, Marijn Schraagen, and Dong Nguyen. 2022. Same Author or Just Same Topic? Towards Content-Independent Style Representations. In Proceedings of the 7th Workshop on Representation Learning for NLP, pages 249-268. Association for Computational Linguistics.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.

Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149.

Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, and Pengfei Liu. 2023. How far are we from believable ai agents? a framework for evaluating the believability of human behavior simulation. arXiv preprint arXiv:2312.17115.

Jing Xu, Arthur Szlam, and Jason Weston. 2021. Beyond Goldfish Memory: Long-Term Open-Domain Conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5180-5197. Association for Computational Linguistics.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204-2213.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. 2023. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv preprint arXiv:2311.16832.
