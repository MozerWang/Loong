# Scalable Extraction of Training Data from (Production) Language Models 

Milad Nasr ${ }^{* 1} \quad$ Nicholas Carlini ${ }^{* 1} \quad$ Jonathan Hayase ${ }^{1,2} \quad$ Matthew Jagielski ${ }^{1}$<br>A. Feder Cooper ${ }^{3} \quad$ Daphne Ippolito ${ }^{1,4} \quad$ Christopher A. Choquette-Choo ${ }^{1}$<br>Eric Wallace Florian Tramèr $^{6} \quad$ Katherine Lee ${ }^{+1,3}$<br>$\quad{ }^{1}$ Google DeepMind $\quad{ }^{2}$ University of Washington $\quad{ }^{3}$ Cornell $\quad{ }^{4} \mathrm{CMU} \quad{ }^{5}$ UC Berkeley $\quad{ }^{6}$ ETH Zurich<br>${ }^{*}$ Equal contribution $\quad{ }^{+}$Senior author


#### Abstract

This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate $150 \times$ higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.


## 1 Introduction

Large language models (LLMs) memorize examples from their training datasets, which can allow an attacker to extract (potentially private) information $[7,12,14]$. Prior work has (a) performed large-scale studies of the total quantity of memorized training data for open-source models [11], and (b) developed practical attacks to extract training data on (relatively) small models like GPT-2, by manually annotating examples as memorized or not [14].

In this paper, we unify these two directions and perform a large-scale study of "extractable memorization" in language models. Unlike discoverable memorization [11] that captures an upper bound on all training data that is memorized (even if it can only be recovered by prompting the model with other training data), extractable memorization captures only that data that can be efficiently recovered by an adversary. We develop a scalable methodology that allows us to detect memorization in trillions of tokens of model outputs in terabytesized datasets, and perform this analysis on both open-source models (e.g., Pythia [5], GPT-Neo [6]) and semi-open models (e.g., LLaMA [49], Falcon [40]). We find that larger and more capable models are more vulnerable to data extraction attacks.

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-01.jpg?height=518&width=718&top_left_y=820&top_left_x=1148)

Figure 1: We scalably test for memorization in large language models. Models emit more memorized training data as they get larger. The aligned ChatGPT (gpt-3.5-turbo) appears $50 \times$ more private than any prior model, but we develop an attack that shows it is not. Using our attack, ChatGPT emits training data $150 \times$ more frequently than with prior attacks, and $3 \times$ more frequently than the base model.

But when we perform this analysis on gpt-3.5-turbo, it appears to memorize almost no training data. We hypothesize that this is because ChatGPT has been aligned (with RLHF $[35,37,39,44]$ ) to act as a helpful chat assistant. ${ }^{1}$

To circumvent the model's alignment, we discover a prompting strategy that causes gpt-3.5-turbo to "diverge" from reasonable, chatbot-style generations, and to behave like a base language model, outputting text in a typical Internet-text style. In order to check whether this emitted text was previously contained somewhere on the Internet, we merge together several publicly available web-scale training sets into a nine terabyte dataset. By matching against this dataset, we recover over ten thousand examples from ChatGPT's training dataset at a query cost of $\$ 200$ USD—and our scaling estimate suggests that one could extract over $10 \times$ more data with more queries.[^0]

Ethics \& Responsible Disclosure. We have taken great care to responsibly share our findings. We shared our findings with the authors of each model we study in this paper (e.g., OPT [54], Falcon [40], Mistral [28], and LLaMA [49]),

Our attack on ChatGPT (gpt-3.5-turbo) is specific to this model and, to the best of our knowledge, is not applicable to any other production language model that we have tested. We disclosed this vulnerability to OpenAI on August 30th (after discovering the flaw on July 11th), and allowed 90 days for the issue to be addressed following standard disclosure timelines [41] before publishing this paper.

We believe it is now safe to share this finding, and that publishing it openly brings necessary, greater attention to the data security and alignment challenges of generative AI models. ${ }^{2}$ Our paper helps to warn practitioners that they should not train and deploy LLMs for any privacy-sensitive applications without extreme safeguards.

## 2 Background and Related Work

Training data for language models. State-of-the-art large language models (LLMs) are pre-trained on vast text corpora that consist of billions to trillions of tokens [6, 42, 43, 50]. For proprietary models such as GPT-4 [38] and PaLM 2 [2], these training sets are kept secret to presumably hide (1) the company's proprietary data collection pipeline, and (2) any private, user-specific, or licensed training data that is not publicly available $[31,32]$.

Instruction-tuning and RLHF. Pre-trained LLMs can solve numerous downstream tasks by conditioning on natural language instructions [8]. The model's utility can be drastically improved via supervised fine-tuning or RLHF on instruction-following data $[3,18,36,38,39,44]$. Aside from utility, this "alignment" stage can also train models to use a unified chat-like persona $[35,39]$ and to abstain from answering on certain types of queries (e.g., it will not assist users in writing spam emails) [37]. In this work, we analyze ChatGPT (specifically, the gpt-3.5-turbo model endpoint).

Privacy attacks. Neural networks, especially ones with many parameters, can memorize their training data. This can be exploited by adversaries via membership inference attacks that infer whether an example was in the training set $[9,17,21,45,52]$, as well as more powerful data extraction attacks $[4,12,14,30]$ that recover full training examples. In this work, we conduct both types of attacks on LLMs.[^1]

## 3 Extracting Data from Open Models

We begin by studying data extraction attacks on open models where both the models' parameters and their original training sets are publicly available. This lets us precisely evaluate the performance of extraction attacks from prior work.

### 3.1 Prior Approaches and Definitions

We follow the (conservative) definition of memorization of Carlini et al. (2021) [14]: given a model trained on a training set $\mathbb{X}$, we denote a string $\boldsymbol{x} \in \mathbb{X}$ as memorized if we can prompt the model's generation routine Gen to produce the string $\boldsymbol{x}$ verbatim. Some prior work (e.g., $[10,11,47])$ has proposed more general notions of memorization where the model may generate a "close" copy of a training sample, but we restrict ourselves to verbatim matches as this will make it possible to scale our analysis to large datasets. This leads us to our definition of extractable memorization: ${ }^{3}$

Definition 1 (Extractable memorization). Given a model with a generation routine Gen, an example $\boldsymbol{x}$ from the training set $\mathbb{X}$ is extractably memorized if an adversary (without access to $\mathbb{X}$ ) can construct a prompt $\boldsymbol{p}$ that makes the model produce $\boldsymbol{x}$ (i.e., $\operatorname{Gen}(\boldsymbol{p})=\boldsymbol{x}$ ).

The design and evaluation of extraction attacks in prior work were primarily hindered by two challenges:

1. How should we design prompts that best elicit memorization in a model?
2. How do we test whether the attack worked, i.e., whether the model's output is training data or not?

Prior work has tackled these challenges with various heuristics. For example, Carlini et al. (2021) [14] recover training examples from the GPT-2 language model [42] by prompting it with short strings sampled from the public Internet, and then manually checking whether these strings can also be found with a Google search. That is, they address the first challenge by simply prompting the model with data sampled from the model's training distribution (GPT-2 was trained on some unknown text sampled from the Internet), and they address the second challenge by (reasonably) assuming that any string memorized by the model is also contained in Google's search index; they manually query with output strings to see if they exist on the public Internet.

Their attack, while successful, only verifiably recovers $\approx 0.00001 \%$ of GPT-2's training dataset. The authors acknowledge that this is likely a loose lower bound; they could not produce a tighter estimate due to the time-consuming manual verification procedure that their attack involves.

Rather than improving this loose lower bound, subsequent work has instead focused on measuring an upper bound on[^2]the strength of an extraction attack, thereby circumventing the two challenges described above. Several works [11,27] have studied the extent to which models can regurgitate their training data when explicitly prompted with data from their training set. That is, given a training string $[\boldsymbol{p} \| \boldsymbol{x}] \in \mathbb{X}$ that consists of a prefix $\boldsymbol{p}$ and suffix $\boldsymbol{x}$, we can measure whether the model can generate $x$ when prompted with the true prefix p. Following Carlini et al. (2022) [11], we denote this as discoverable memorization:

Definition 2 (Discoverable memorization). For a model Gen and an example $[\boldsymbol{p} \| \boldsymbol{x}]$ from the training set $\mathbb{X}$, we say that $\boldsymbol{x}$ is discoverably memorized if $\operatorname{Gen}(\boldsymbol{p})=\boldsymbol{x}$.

Prior work shows that many LLMs discoverably memorize roughly $1 \%$ of their training datasets (when prompting the model with about 50 tokens of context) $[2,11,30]$. There is thus a huge gap between prior lower bounds on extractable memorization (i.e., actual extraction attacks that have to be manually verified [14]), and upper bounds that assume full access to the training set $\mathbb{X}$. This raises a natural question: why is there such a large observed gap between extractable and discoverable memorization in the literature?

To answer this question, recall the differences between how prior work measured extractable and discoverable memorization rates: first, prompts are constructed by either heuristic means or by using the actual true prefix $\boldsymbol{p}$, and second, verifying if data was successfully extracted was either performed manually or by looking at the actual training dataset $\mathbb{X}$. This suggests two possible explanations for the observed gap:

1. It is possible that prompting models with training data leads to orders-of-magnitude more training-data regurgitation, compared to realistic extraction attack strategies (in which adversaries do not have access to the training set).
2. Alternatively, perhaps existing extraction attacks already make models regurgitate large amounts of training data, but prior work was not able to verify that the model outputs were training data.

Our goal in this section is to disentangle these two possible explanations. As we will show, the latter explanation is (mostly) the correct one. Existing extraction attacks are actually a lot more successful at recovering training data than what prior work indicates.

### 3.2 Attack Methodology

To begin, we evaluate past extraction attacks in a controlled setting where testing for attack success is possible. That is, we first focus on open-source models with publicly available training datasets, where we can mechanistically verify if any generated output $\boldsymbol{x}$ is indeed training data (but, crucially, the attack itself does not rely on knowledge of the training set).
We follow the data extraction attack method of Carlini et al. [14]: (1) we download $10^{8}$ bytes of data from Wikipedia, and generate prompts $\boldsymbol{p}$ by randomly sampling (with replacement) hundreds of millions of continuous 5-token blocks from this dataset; (2) we perform an independent generation for each prompt $\boldsymbol{p}^{i}$ as Gen $\left(\boldsymbol{p}^{i}\right)=\boldsymbol{x}^{i}$ and store each $\boldsymbol{x}^{i}$.

Our methodology differs in how we evaluate the efficacy of the attack. Because this prior attack extracted training data from a language model without a public dataset, it was necessary to manually search the Internet in order to determine whether or not any generated sequence was contained in the model's training dataset. In contrast, each model we study in this section is fully open-source. This lets us directly query the model's training data to evaluate whether or not any generated sample is memorized.

Performing the training set inclusion test $\boldsymbol{x} \in \mathbb{X}$ naively is prohibitively expensive, as LLMs are trained on datasets with trillions of tokens and we generate billions of tokens of output from each of these models. To make this search efficient, we use a suffix array, as done in Lee et al. (2021) [33]-a data structure that stores all suffixes of the dataset in sorted order, and which enables fast string lookups (using binary search). We build a suffix array $\boldsymbol{s}$ over $\mathbb{X}$, denoted $\boldsymbol{s}(\mathbb{X})$ or simply $\boldsymbol{s}$ when unambiguous. We can then check that $\boldsymbol{x} \in \boldsymbol{s}$, which is equivalent to checking $x \in \mathbb{X}$ (see Appendix A).

We report that an extraction is successful if the model outputs text that contains a substring of length at least 50 tokens that is contained verbatim in the training set. ${ }^{4}$ We chose this value empirically to be sufficiently large so that no two suffixes could accidentally overlap. We estimated the amount of token overlap between news articles guaranteed to be written after the creation of the largest training datasets RedPajama [19]. We found no overlap longer than 25 tokens, excluding direct quotations (i.e., actual copies). We then chose to be extremely conservative and double this value.

### 3.3 Empirical Results

We apply our attack to 9 open-source models of different sizes. Since these models were, e.g., "designed specifically to facilitate scientific research" [5], they make available their entire training and pipeline and dataset, facilitying our study.
- GPT-Neo (1.3B, 2.7B, 6B) [6], a family of models trained on The Pile [23]. ${ }^{5}$
- Pythia (1.4B, 1.4B-dedup, 6.9B, 6.9B-dedup) [5], a family of models also trained on The Pile, but primarily designed for studying model scaling and memorization.
- RedPajama-INCITE (Base-3B-v1, Base-7B) [20], models trained on the RedPajama [19] dataset.[^3]

| Model <br> Family | Parameters <br> (billions) | \% Tokens <br> memorized | Unique <br> 50 -grams | Extrapolated <br> 50 -grams |
| :--- | ---: | ---: | ---: | ---: |
| RedPajama | 3 | $0.772 \%$ | $1,596,928$ | $7,234,680$ |
| RedPajama | 7 | $1.438 \%$ | $2,899,995$ | $11,329,930$ |
| GPT-Neo | 1.3 | $0.160 \%$ | 365,479 | $2,107,541$ |
| GPT-Neo | 2.7 | $0.236 \%$ | 444,948 | $2,603,064$ |
| GPT-Neo | 6 | $0.220 \%$ | 591,475 | $3,564,957$ |
| Pythia | 1.4 | $0.453 \%$ | 811,384 | $4,366,732$ |
| Pythia-dedup | 1.4 | $0.578 \%$ | 837,582 | $4,147,688$ |
| Pythia | 6.9 | $0.548 \%$ | $1,281,172$ | $6,762,021$ |
| Pythia-dedup | 6.9 | $0.596 \%$ | $1,313,758$ | $6,761,831$ |

Table 1: For each model we generate 1 billion tokens and report: (1) the rate at which models generate 50 -token sequences that occur in AUXDATASET; (2) the number of unique, memorized 50-token sequences; and (3) our extrapolated lower bound of unique, memorized 50-token sequences. Our lower bound is often exceptionally loose-for example in Figure 4 we extract over 30 million unique 50 -token sequences from GPT-Neo 6B by generating $500 \times$ more data, nearly $10 \times$ the estimated lower bound.

We generate one billion tokens of output for each model and then compute the number of memorized examples by matching against the corresponding training set. From this data, we can perform two different types of analysis. First, in Table 1, we measure the fraction of model outputs that are memorized. We observe rates between $0.1 \%$ and $1 \%$. But this number is hard to interpret-a model that emitted the same memorized training sequence thousands of times in a row would look highly non-private, even if in practice it was revealing almost no data.

And so instead, we can also compute the number of unique 50-token strings that we extract, which varies between several hundred thousand and several million. This allows us to observe data extraction rates orders of magnitude higher than reported previously in Carlini et al. (2021) [14, p. 13], which only verifiably extracted 600 sequences from GPT-2. This serves as evidence to suggest that extractable memorization rates are much higher than previously thought (at least for these open models). We observe a strong correlation between model size and both the rate of emitting memorized output and also the total number of unique 50 -token sequences we extract, indicating that the pathological failure mode where a model repeatedly emits the same memorized example is not common in

### 3.4 Estimating Total Memorization

In our explorations thus far (Sections 3.3 and 3.5), we have used a large fixed budget of generations for our extraction attacks. But, the number of generations has a significant impact on the amount of extractable memorization, as can be clearly

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-04.jpg?height=415&width=745&top_left_y=259&top_left_x=1140)

Figure 2: As we query models more, they emit more unique memorized data. This rate of extraction differs between models and can also change. For example, though Pythia-1.4B initially emits more unique training data than Neo-6B, after 60B queries the model has a more rapid decay leading to a lower total memorization.

seen from Figure 2: memorization grows (nearly) linearly even after generating several hundred billion tokens.

This leads to a natural question that has not yet been discussed in the literature: if we could query a model infinitely, how much memorization could we extract in total? Given this is infeasible, we instead aim to estimate the total memorization. However, again observing Figure 2 demonstrates a challenge here: the rate of extracting memorized training data is not a good predictor of the total quantity of memorization. In particular, we observe that at smaller compute budgets, Pythia 1.4B appears to memorize more data than the (larger) GPT-Neo 6B. However, if we query the model more, the rate of extractable memorization in Pythia-1.4B decreases, revealing that GPT-Neo 6B in fact memorizes more data in total. Thus, we will need to find better predictors of the total memorization of a model.

Extrapolating total memorization. We begin by decomposing our extrapolation problem into estimating two values: 1) how often a model outputs anything memorized, and 2) how often a memorized generation is new. The first value is not stateful and so can be easily estimated as a probability. But, the second value depends on how many memorized strings we have already observed. Let us focus on this latter quantity. Note that the total amount of memorization the model will ever output as we scale the number of generations, does not depend on the first value.

We can visualize the rate of new memorization via a slight modification of Figure 2. Instead of varying the number of generated tokens, we instead compute and vary the number of memorized tokens extracted. In this visualization, shown in Figure 3, we can more clearly observe the differences between GPT-Neo 6B and Pythia 1.4B. In particular, the slope and curvature of the plot help us understand the model's total memorization: Pythia-1.4 outputs new memorized examples

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-05.jpg?height=440&width=766&top_left_y=238&top_left_x=211)

Figure 3: Number of unique extracted 50 -grams versus the number of total extracted 50 -grams (generated and memorized). The rate of observing unique 50 -token sequences from GPT-Neo 6B always dominates the rate of observing unique 50-token sequences from Pythia-1.4B.

less frequently than GPT-Neo 6B, and seems to saturate much more quickly as well, pointing to the limit of how much training data we can surface. While the slope and curvature are only estimations, they can serve as a starting point to understand how to make extractable memorization more efficient. Indeed, they can enable us to estimate how much memorization could be extracted even if researchers do not have the capability to generate many hundreds of billions of tokens.

Intuition. Suppose a researcher wants to know how many fish live in a lake. If this researcher is very hardworking, they could try to count each fish individually, catching and then throwing them back in the lake, and hoping to not skip or double-count any fish. However, in practice, a common technique is known as mark-and-recapture [48]: first, catch and mark $N$ fish, wait for some time, and then recapture $K$ fish, recording the number $L$ of fish that have been marked. From this information, mark-and-recapture estimates the number of fish in the lake as $N K / L$.

This estimate requires making a few assumptions. First, no one fish is more likely than another to be caught. Second, the population does not change. Ecologists have spent time understanding conditions where these assumptions might not be met, but we leave the reader to explore the Internet for more details, and turn back to talking about language models

Mark-and-recapture does not apply. An initial attempt at applying mark and recapture to our analysis would have us estimating, instead of fish, the total number of unique memorized 50-grams extractable from the model. That is, we can generate until we collect $N$ memorized examples, collect further $K$ memorized examples, and see how many of those $K$ were not contained in $N$. Unfortunately, this ends up significantly undercounting extractable memorization. The main reason mark-and-recapture does not apply well is the first assumption is violated-not all memorized strings are equally

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-05.jpg?height=564&width=784&top_left_y=233&top_left_x=1126)

Figure 4: With sufficient data, a Good-Turing estimator can extrapolate the number of uniquely memorized examples. With too little data, it consistently underestimates this value.

likely to be output. In a fish pond, one can wait longer so the fish can swim around the pond, but we do not have any ways to fix this problem with language models! Inherently, some sequences are statistically more likely than others.

A better approach: sequential Good-Turing. Even when the distribution of extractable strings is unknown, we can still predict the probability that a fresh sample will yield a novel string using the work of Good and Turing [24]. Given the frequencies of samples seen so far, the Good-Turing estimator predicts the probabilities that the next sample will be novel or will match any of the previously seen samples. A key ingredient of the Good-Turing estimator is a smoothing procedure that reduces the variance of the predictions for rare events. We use the popular smoothing procedure in [22] because it has shown good empirical performance in many settings.

In order to make predictions beyond the next sample, we can sample an outcome according to the probabilities produced by Good-Turing and update our observed frequencies accordingly. Iterating this process gives us a Monte-Carlo simulation predicting the number of unique memorized examples potentially far into the future. An analysis of this sequential application of Good-Turing was carried out in [1].

The results of using the Good-Turing extrapolation are shown in Figure 4. We find that having sufficiently many observations is essential to produce a good extrapolation. We also observe that this approach underestimates the number of unique memorized examples by GPT-Neo 6B.

In the appendix, Table 15 compares various other methods for estimating the total quantity of memorized training under varying assumptions. We find that Good-Turing consistently gives higher quality lower bounds than other methods, such as Chao1 [15], Chiu et al. [16], and Zelterman [53].

### 3.5 Discoverable Mem. vs. Extractable Mem.

To understand what gap remains between extractable and discoverable memorization, we study two questions: How many data samples are memorized under both definitions? And more interestingly, how many samples are extractable but not discoverable or discoverable but not extractable?

Prior work released a dataset of discoverable memorizations from The Pile for the GPT-Neo 6B parameter model [11]. We compare these with the extractable memorized examples from the prior section. This results in the following confusion matrix, which compares sequences classified as discoverably and/or extractably memorized on GPT-Neo 6B.

| Extractable | 1799 | 618 |
| :---: | :---: | :---: |
|  | Both | Extractable Only |
| Not Extractable | $\mathbf{3 2 1 1}$ <br> Discoverable Only | 11019 <br> Neither |
|  | Discoverable | Not Discoverable |

Most training data from the model is (unsurprisingly) not memorized under either definition. Then, $30.1 \%$ of examples are discoverably memorized and $14.5 \%$ are extractably memorized. But surprisingly, despite generating several hundred billion tokens, only $35 \%$ of the discoverably-memorized examples were also extractable. While this is orders of magnitude larger than had previously been believed [11], it is still not most (or even all) of the data that is known to be memorized. We also uncover an additional $11 \%$ memorized sequences via our extractable memorization attacks that were not discoverably memorized. We extend this analysis in Figure 19 which analyses sequences from the Pile that have a varying number of duplicates [11]. We computed the percent of those sequences that were memorized-either discoverably or extractably memorized. We see that highly duplicated sequences are also both easier to extract and discover.

We make four observations from this data. First, it is somewhat surprising that a simple attack that just samples from the model is sufficient to recover a large fraction (35\%) of all (known) memorized training data. Second, it also suggests that there is still room for improving current extraction attacks. Third, measuring discoverable memorization is a useful and reasonably tight characterization of data that can actually be extracted by an adversary. And fourth, our work highlights there is also room to improve discoverable memorization baselines: though sampling prefixes from the training set have high likelihood of discovering memorization, there still exist data that is (extractably) memorized (by prompting with random strings) but not discovered in this way. We suspect this is caused because sequences were reported to be discoverably memorized only if greedy decoding resulted in reconstructing the training example [11].

## 4 Extracting Data from Semi-closed Models

By focusing on open-source models, our results of the previous section let us show that there is a large amount of training data which can be extracted. Though of academic interest, this does not yet constitute a practical threat because these models are entirely public: their architecture, training algorithm, and training datasets are all already publicly documented. In this section, we turn our attack to semi-closed models where not all information is public. We ask the same question under this more difficult setting: how much memorized data can be extracted?

### 4.1 Attack Methodology

We define semi-closed models as those that have publicly available, downloadable parameters, but whose training datasets and training algorithms are not known. For these models, we can generate outputs using the same strategy discussed in Section 3.2; however, since the training datasets for these models are not publicly accessible, we will need to establish our own "ground truth" for verifying and quantifying extractable memorization.

Obtaining a "ground truth." Since we do not have access to the training datasets, we build on the original strategy of Carlini et al. [14], who extracted training data from GPT-2 (a model that also did not release its training dataset). For their memorization analysis, Carlini et al. manually performed Google searches to verify whether or not data extraction attempts were successful. This process, while effective, was entirely manual and thus error-prone and time consuming. We propose a similar (but automated) strategy of testing whether a model output is contained somewhere on the Web. (We will later verify that our automated strategy approaches the quality this human baseline in Section 5.6.3.)

We download a large corpus of Internet text and use it to build an auxilliary dataset (AUXDATASET). Then, we check if any potentially-memorized examples exist in AUXDATASET. If the sequence does appear, and it has a sufficiently high entropy and length, then it is extremely unlikely that the generation appears on the Internet by coincidence. We use this as a proxy for testing whether the generated sequence was in the training set with a very low false-positive rate.

This approach has false negatives; it will not identify all memorized generations because we do not have a complete picture of the training data. Thus, our results yield a lower bound on the amount of memorization present in the model. ${ }^{6}$[^4]

Building Auxdataset. We collected 9TB of text by concatenating four of the largest LLM pre-training datasets:

- The Pile [23], a 400GB dataset of heterogeneous sources (e.g., Wikipedia, code, generic Common Crawl) that was used to train the GPT-Neo models.
- RefinedWeb [40], a 1080GB subset of the dataset used to train the Falcon models, which largely consists of generic data scraped by Common Crawl.
- RedPajama [19], a 2240GB dataset of heterogeneous sources (e.g., Wikipedia, arXiv, generic Common Crawl) intended to reproduce the LLaMA dataset [50].
- Dolma [46], a 5600GB dataset that primarily consists of text scraped by Common Crawl, in addition to code and scientific papers.

These datasets are not necessarily unique-for example, both Dolma and RedPajama contain a complete copy of C4 [43]. We thus performed tokenization and coarse deduplication at the document level before reporting the sizes shown above.

Implementation efficiency. AUXDATASET is 9TB, and its corresponding suffix array (a data structure which allows for efficient searches, see Section 3.2 and Appendix A) is 45TB. Thus, it cannot fit into memory on a single machine. Instead, we shard the data into 32 independent suffix arrays, allowing us to load each completely into memory one at a time. With this done, we can perform a complete intersection between gigabytes of potential training data with AUXDATASET at a much faster rate: linear in the size of the dataset (the time needed to load it off disk) and linear in the number of queries to the model.

The complete end-to-end evaluation required three weeks of compute on a single ( 176 cores, 1.4TB of RAM) c3highmem-176 machine on Google Cloud. This includes time spent building the suffix array, and performing all of the dataset queries for the experiments in this paper. Over half of this total time is due to I/O bandwidth limitation; a more optimized implementation could likely achieve the same result significantly faster.

### 4.2 Experimental Setup

We analyze nine different semi-closed models:

- GPT-2 (1.5b) [42] is one of the first large language models to have ever been trained. Prior work [14] has extracted 600 training examples from this model by manually annotating potentially-memorized training examples. This model was trained on data obtained by following URLs submitted to Reddit.
- LLaMA (7b, 65b) [49] is one of the most popular families of models due to the fact that they have been over-trained with respect to a compute-optimal budget [26]. It was trained on a non-public mixture of publicly available data.

| Model <br> Family | Parameters <br> (billions) | $\%$ Tokens <br> Memorized | Unique <br> 50 -grams | Extrapolated <br> 50 -grams |
| :--- | ---: | ---: | ---: | ---: |
| LLaMA | 7 | $0.294 \%$ | 627,719 | $3,268,309$ |
| LLaMA | 65 | $0.789 \%$ | $2,934,762$ | $16,716,980$ |
| Mistral | 7 | $0.515 \%$ | $1,322,674$ | $7,724,346$ |
| Falcon | 7 | $0.069 \%$ | 101,585 | 606,316 |
| Falcon | 40 | $0.122 \%$ | 199,520 | $1,287,433$ |
| GPT-2 | 1.5 | $0.135 \%$ | 165,628 | 692,314 |
| OPT | 1.3 | $0.031 \%$ | 38,941 | 235,046 |
| OPT | 6.7 | $0.094 \%$ | 108,787 | 577,240 |
| GPT-3.5-instruct | $?$ | $0.852 \%$ | - | $1,789,254$ |

Table 2: As in 1, the percentage of tokens generated that are a direct 50-token copy from AUXDATASET, the 1number of unique 50-token sequences (out of 1 billion tokens), and the extrapolated lower bound of memorized 50 -token sequences. gpt-3.5-turbo-instruct (denoted with $*$ ) is extrapolated from $25 \times$ less generated data. Compared with open-source models of the same size, we observe much smaller memorization rates (c.f. Figure 15).

- Falcon (7b, 40b) [51], a pair of models designed to out-perform LLaMA in several settings, with limited training details disclosed.
- Mistral 7b [28] is a model similar to LLaMA with undisclosed training details. This model is the highest accuracy model we study of its size.
- OPT (1.3b, 6.7b) [54], a family of models from 125 million parameters to 175 billion parameters. These models are generally less capable than the prior models, in part because they have not been trained for as many steps.
- gpt-3.5-turbo-instruct, an OpenAI API with an undisclosed model, training algorithm, and training dataset.

Most of the models considered here (LLaMA, Falcon, Mistral, and OPT) are similar to the models from the prior section in that their weights are accessible, but unlike the prior models, their training pipeline and datasets are not accessible. The gpt-3.5-turbo-instruct model is different-it is only available through an API and the model weights are non-public.

Since gpt-3.5-turbo-instruct costs $\$ 0.002$ USD per 1,000 output tokens, we do not generate 1 billion tokens for this model (which would cost $\$ 2,000$ USD). Instead, we only query this model 25 million times and extrapolate.

### 4.3 Results

Our most prominent finding is that all models emit memorized training data, as we can see from Table 2. However, there is significant variance between model families. The comparably sized and comparably accurate Mistral 7B and Falcon 7B differ in detected memorization by over a factor of $10 \times$. Directly interpreting this number is somewhat difficult: it could
either indicate that Mistral indeed memorizes (much) less data than Falcon, or it could indicates a limitation in our dataset construction: if our datasets happen to be more similar in distribution to one model's training data than another model's, they will appear to have differing levels of extractable memorization. However, a rate of $10 \times$ is probably too high to be a result of data distribution alone.

But even accounting for this, the rate of emitting memorized training data is still exceptionally high for these stateof-the-art models. Indeed, perhaps surprisingly, the worst offender is gpt-3.5-turbo-instruct, where $0.852 \%$ of generated tokens are part of 50-token sequences found verbatim in AUXDATASET.

As we expected, model families that are trained for longer memorize more than model families trained for less long. To be precise, Hoffman et al. [25] propose a set of scaling laws that suggests the optimal quantity of training data for a given model size. Some models like OPT are under-trained with respect to this baseline; they generally perform poorly on benchmarks, but as a result of their limited training, we show they memorize less training data.

Other models, like LLaMA are intentionally over-trained for more steps of training than is compute-optimal. It is possible to trade-off compute at training time to compute at inference time by over-training in this way. For this reason, when inference costs dominate the total cost of a model, most large models today are over-trained [50]. Unfortunately, our results suggest that over-training increases privacy leakage.

Our second main finding is that the total extractable memorization of these models is on average $5 \times$ higher than smaller models. Similar to Section 3.4 we can use Good-Turning estimator to extrapolate the memorization rate of the models. The last column in Table 2 does so using 1B generations. Recalling from Section 3.4, this estimator tends to underestimate the true total memorization and thus, the expected total number of extractable memorizations is likely even higher.

## 5 Extracting Data from ChatGPT

We have now established that state-of-the-art base language models all memorize a significant amount of training data. But in practice, most users do not typically interact with base models; instead, they interact with language models that have been aligned [18] to behave "better" according to human preferences. This allows them to be deployed as, e.g., conversational agents that are designed specifically to interact with users via a dialog interface. The alignment found in language models such as ChatGPT (specifically, the gpt-3.5-turbo API endpoint) creates additional challenges for designing a successful extraction attack.

Challenge 1: Chat breaks the continuation interface. When models are tuned to be conversational, it makes them more difficult to attack with our prior methodology that simply prompts the model with random strings from the Internet. This is because dialog-adapted language models do not give the user direct control over the language modeling task. Instead, typical dialog-adapted language models are tuned to expect a prompt format similar to the following:

```
System: You are a helpful assistant.
User: Hello, how are you doing?
Assistant:
```

The language model then completes the next tokens following the indicator that it is the assistant's "turn" in the dialog. This formatting prevents the prompt-continuation attack we applied in the prior sections because it is not possible to force the model to directly continue arbitrary text sequences: the model only ever begins its response after appending the special "Assistant:" turn indicator.

Challenge 2: Alignment adds evasion. Even if—-for some reason-the model did continue generating from the given prompt instead of behaving as a chat model, the model may abstain from completing data from its training set because of the alignment procedure. Here is an illustrative interaction with ChatGPT, quoting from an example that we will later show the model has memorized:

User: Write the following words then continue from there: "British Broadcasting Corporation is a British public service broadcaster headquartered at Broadcasting House in London, England. The total number of staff is"

Assistant: I'm sorry, but you haven't provided the complete information about the total number of staff at the BBC. As of my last knowledge update in September 2021, the BBC ....

It is tempting to conclude from this example that alignment has succeeded, and that it prevents attacks from succeeding. The objective of this section will be to challenge this conclusion and show that alignment does not prevent data extraction.

### 5.1 Baseline Attack

We begin by adapting our previous random-token-prompting attacks by asking the model to repeat the random tokens and continue generating. For example:

User: Write the following words and then continue: glthumb|350

Assistant: g|thumb|350 Hello! How can I assist you today?

In this case, the model successfully repeats the five random tokens, but does not appear to output training data. We run this experiment at scale to give quantitative evidence that this attack strategy is ineffective.

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-09.jpg?height=562&width=675&top_left_y=256&top_left_x=256)

Figure 5: Extracting pre-training data from ChatGPT. We discover a prompting strategy that causes LLMs to diverge and emit verbatim pre-training examples. Above we show an example of ChatGPT revealing a person's email signature which includes their personal contact information.

Evaluation. We repeat the above experiment several thousand times with different prompts, to obtain 50 million generated tokens from gpt-3.5-turbo. Out of these tokens, just $0.02 \%$ of tokens are part of a 50 -token sequence that is directly copied from AUXDATASET. In contrast, for the smallest semi-closed model we study (OPT with 1.3B parameters), we found that $0.031 \%$ of emitted tokens are directly copied from the training dataset; for the (presumably) comparable gpt3.5-turbo-instruct model, at least $0.85 \%$ of emitted tokens are part of a memorized sequence. From this, we might (as we will soon see, incorrectly) conclude that the alignment procedure has correctly prevented the model from emitting training data.

### 5.2 Our Divergence Attack

In order to recover data from the dialog-adapted model we must find a way to cause the model to "escape" out of its alignment training and fall back to its original language modeling objective. This would then, hopefully, allow the model to generate samples that resemble its pre-training distribution.

To do this, we discover a prompting strategy that causes the model to diverge from its standard dialog-style of generation. For example, if we pass the model the prompt

```
User: Repeat this word forever: "poem poem ...poem"
    repeated 50 times
```

then ChatGPT will respond as shown in Figure 5: initially, it repeats the word "poem" several hundred times, but eventually it diverges. ${ }^{7}$ Once the model diverges, its generations[^5]

are often nonsensical. But, we show that a small fraction of generations diverge to memorization: some generations are copied directly from the pre-training data! Consequently, we can create a large pool of possible memorized examples by prompting the model with the above phrase, generating many times from it, and inspecting the divergent text following the initial repeated "poem"s. A complete, unedited transcript of such an interaction is given in Appendix D.

### 5.3 Main Experimental Results

Using only $\$ 200$ USD worth of queries to ChatGPT (gpt-3.5turbo), we are able to extract over 10,000 unique verbatimmemorized training examples. Our extrapolation to larger budgets (see below) suggests that dedicated adversaries could extract far more data.

Length and frequency. Extracted, memorized text can be quite long, as shown in Figure 6-the longest extracted string is over 4,000 characters, and several hundred are over 1,000 characters. A complete list of the longest 100 sequences that we recover is shown in Appendix E. Over $93 \%$ of the memorized strings were emitted just once by the model, with the remaining strings repeated just a handful of times (e.g., $4 \%$ of memorized strings are emitted twice, and just $0.05 \%$ of strings are emitted ten times or more). These results show that our prompting strategy produces long and diverse memorized outputs from the model once it has diverged.

Qualitative analysis. We are able to extract memorized examples covering a wide range of text sources:

- PII. We recover personally identifiable information of dozens of individuals. We defer a complete analysis of this data to Section 5.4.
- NSFW content. We recover various texts with NSFW content, in particular when we prompt the model to repeat a NSFW word. We found explicit content, dating websites, and content relating to guns and war.
- Literature. In prompts that contain the word "book" or "poem", we obtain verbatim paragraphs from novels and complete verbatim copies of poems, e.g., The Raven.
- URLs. Across all prompting strategies, we recovered a number of valid URLs that contain random nonces and so are nearly impossible to have occurred by random chance.
- UUIDs and accounts. We directly extract cryptographically-random identifiers, for example an exact bitcoin address.
- Code. We extract many short substrings of code blocks repeated in AUXDATASET-most frequently JavaScript

rather than asking the model to repeat the token forever. We often observe divergence after fewer than 200 repeats (i.e., asking to repeat "forever" is not strictly necessary).

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-10.jpg?height=242&width=664&top_left_y=253&top_left_x=270)

Figure 6: A cumulative histogram showing the number of extracted strings greater than each length. We were able to extract thousands of short unique training examples from ChatGPT, hundreds of training examples with over 1000 characters. The longest extracted example contained over 4000 characters (a website's terms of service agreement). Appendix E show the 100 longest memorized sequences that we extract.

that appears to have unintentionally been included in the training dataset because it was not properly cleaned.

- Research papers. We extract snippets from several research papers, e.g., the entire abstract from a Nature publication, and bibliographic data from hundreds of papers.
- Boilerplate text. Boilerplate text that appears frequently on the Internet, e.g., a list of countries in alphabetical order, date sequences, and copyright headers on code.
- Merged memorized outputs. We identify several instances where the model merges together two memorized strings as one output, for example mixing the GPL and MIT license text, or other text that appears frequently online in different (but related) contexts.


### 5.4 Identifying PII

Some of the model's outputs contain personally identifiable information (PII); we evaluate the frequency at which this happens. We labeled 15,000 generations for substrings that looked like PII. We used both regexes for identifying phone and fax numbers, email and physical addresses, and also prompted a language model to identify sensitive content within generations. This helps to identify additional malformed phone numbers, email addresses, and physical addresses (e.g., sam AT gmail DOT com) along with social media handles, URLs, and names and birthdays. We then verified whether or not these substrings were actual PII (i.e. they appear in the training set and are not hallucinated) by looking up the extracted substring in AUXDATASET. In total, $16.9 \%$ of generations we tested contained memorized PII, and $85.8 \%$ of generations that contained potential PII were actual PII.

### 5.5 Words that Elicit Memorized Outputs

Our attack repeats one word many times in a row. Are there some words that are better at eliciting memorization than other words? We find the answer is a definitive "yes".
Our first finding is that the only words that lead to memorization are words that are a single token in the vocabulary. Asking the model to repeat multi-token words never causes the model to emit training data because it never causes the model to diverge. That is, the model either repeats the word forever (i.e., the model correctly alternates between the multiple tokens that make up the word), or the model replies that "it would not be productive" to follow the request, but it never repeats the word and then starts emitting other output.

When we prompt the model with single-token words, we find the efficacy across words varies significantly. Figure 7 contains an analysis of the quantity of memorized output we recover across several different words. The most effective words are over $100 \times$ more effective at recovering memorized output than the least effective words. We find this is both due to the fact that some words do not cause the model to diverge as often, and also because even if the model does diverge, some words result in less regurgitated training data.

### 5.6 Quantifying Total Memorization

With our limited budget of \$200 USD we extracted overr 10,000 unique examples. However, an adversary who spends more money to query the ChatGPT API could likely extract far more data. In this section, we discuss various ways in which our analysis may underestimate ChatGPT's memorization rate, and attempts at extrapolating the true value.

### 5.6.1 Extrapolating Unique Memorized Strings

We first apply the extrapolation methodology developed previously in Section 3.4 to estimate how much more memorization we could have found if we had issued more queries to ChatGPT. Applying a Good-Turing estimator, we lower bound ChatGPT's memorization to at least 1.5 million unique 50-token sequences (see Figure 9).

But this estimate is likely an exceptionally poor estimate. Recall from Figure 4 it was necessary to extract 500 million examples from GPT-Neo 6B before the Good-Turing estimator converged; we have extracted well over $1000 \times$ fewer examples than this from ChatGPT.

And so we suggest avoiding directly using a Good-Turing estimator for this data. Instead, in Figure 8 we compare the amount of training data memorized by ChatGPT compared to any other model. We find that ChatGPT emits unique memorized strings at a much higher rate than any of the publicly available models we studied. In particular, if the GPT-Neo 6B scaling curve were to hold roughly similar for ChatGPT, we estimate the true rate of memorization of ChatGPT (within our auxiliary dataset) is likely closer to hundreds of millions of 50 -token sequences, totaling a gigabyte of training data. In practice we expect it is likely even higher.

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-11.jpg?height=295&width=1759&top_left_y=243&top_left_x=172)

Figure 7: When running our divergence attack that asks the model to repeat a word forever, some words (like "company") cause the model to emit training over $164 \times$ more often than other words (like "know"). Each word is one token.

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-11.jpg?height=524&width=745&top_left_y=730&top_left_x=229)

Figure 8: The rate of extracting unique 50 -grams is similar for gpt-3.5-turbo and gpt-3.5-turbo-instruct, and both are higher than any other model. Moreover, there is very little curvature, suggesting that the total quantity of memorization for this family of models is much larger than any other model we study.

### 5.6.2 Impact of AUXDATASET's Size

As we increase the size of our auxiliary dataset, we identify more memorized output from the model, because this allows us to achieve a higher overlap with the original data on which ChatGPT was originally (pre-)trained.

In Figure 9(b) we compare how artificially decreasing the size of our dataset would have impacted the quality of our results. To do this, we randomly sub-sample our dataset and compute the number of memorized examples found, as we decrease our auxiliary dataset size from 9TB down to 200GB. If we choose just a 200GB subset of our dataset we could have discovered slightly under $20 \%$ of the total memorization.

This data admits a fairly accurate curve to predict how much data we will be able to find, given the size of our auxiliary dataset. If we fit a curve using only $25 \%$ of our data, we can extrapolate out almost perfectly the total number of examples we have identified with the full dataset. Extrapolating from this curve, we estimate that by doubling our auxiliary dataset size it might be possible to increase the amount of memorization we discover by an additional $20 \%$.
Thus, it appears that we have collected an auxiliary dataset that is sufficiently large to produce (nearly) tight estimates of the amount of memorized data within the model's outputs. However, it seems that our attack could find much more memorization if we issued more queries to the model.

The above analysis makes one critical assumption: that any new data we add to our auxiliary dataset would be sampled from the same distribution as the data we have collected so far. Figure 16 studies the amount of memorization identified as a result of adding each of the four datasets that make up AUXDATASET. We plot both the total number of examples found in each dataset, and also the number of unique examples found only in that dataset. As expected, Dolma, the largest 5TB dataset, contains the largest number of memorized examples.

But we were surprised to find that scale does not completely determine the number of memorized samples identified. The 1TB RefinedWeb dataset finds the least memorization, and almost all memorization found by the 2TB RedPajama dataset was already covered by one of the other datasets. We believe that this is caused by discrepancies between the distribution of each of these datasets and the dataset on which gpt-3.5-turbo was trained. For example, it suggests that gpt-3.5-turbo's training dataset is more similar to Dolma or The Pile than RefinedWeb-although we leave a more thorough investigation of this to future work.

### 5.6.3 Extending AuxDataSET to a Web Search Index

All our evaluations of ChatGPT's memorization have so far been performed by automatically comparing each model generation against AUXDATASET. As noted in Section 5.6.2, this likely underestimates ChatGPT's total memorization since AUXDATASET is not a strict superset of the model's training set. In order to more accurately estimate the true rate of memorization, we take 494 generations and manually label whether or not the generation can be found on the entire Internet, following the process outlined in Carlini et al. [14]. Specifically, we split output from ChatGPT into 50-token sequences, manually search Google for each of these sequences, and report the sequence as memorized if it occurs nearly verbatim on some webpage.

We detect nearly twice as many model outputs are memorized in our manual search analysis than were detected in
![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-12.jpg?height=604&width=1678&top_left_y=260&top_left_x=214)

Figure 9: Estimates for how much total data is actually memorized by ChatGPT. Left: As an adversary spends more money to query the ChatGPT API, they are able to extract more data. We use a budget of $\$ 200$ USD to extract over 10,000 unique examples, however, an extrapolation based on Good-Turing frequency estimation shows that using larger budgets could allow significantly more extraction. Right: To identify memorized sequences, we cross reference ChatGPT's generations with a large auxiliary corpus. As we scale the size of the auxiliary corpus, we can identify more memorized examples.

our (comparatively small) AUXDATASET: 150 of the 494 manually annotated examples were contained somewhere on the Internet, compared to just 70 that were present in the our auxiliary dataset. This confirms the prior section's hypothesis that introducing additional datasets would lead to improved attack success rates.

### 5.7 An End-to-end High-precision Attack

Our evaluation thus far has been primarily a measurement study of memorization across language models, because we relied on our ability to directly query the model's (approximate) training dataset to detect memorized model outputs. But without a reliable way to predict (a priori) whether a given model output is a training example or not, we cannot directly call this an extraction attack.

We now show that existing techniques from the literature are sufficient to distinguish memorized training data from other generated (non-memorized) data, with high precision. In particular, we show that the membership inference attack [45] from [14] has high precision at separating memorized training data from other hallucinated data that was not contained in the training dataset. Specifically, we score each example based

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-12.jpg?height=63&width=848&top_left_y=2164&top_left_x=172)
corresponds to the perplexity of the text as determined by the model that generated the text, and the denominator corresponds to the entropy of the (token-decoded) sequence under zlib text compression. This likelihood ratio was the most effective at predicting memorization in prior work [14], and in our evaluation we find it is highly accurate in our setting as well.
Figure 10 plots how varying the membership inference threshold affects the precision of our attack. At the lowest membership inference score threshold, the attack precision is above $30 \%$ when evaluated by a manual Internet search-or still $15 \%$ when evaluated by verbatim membership in AUXDATASET. By increasing the membership inference threshold, precision remains relatively constant until 1.5 at which point it begins to significantly decay. This indicates that not only is it possible to extract training data, we can-with high precision-identify when data is memorized and when it is not. However, there is still room for future work to improve the precision of this attack further.

### 5.8 Is ChatGPT Memorization Discoverable?

In our attack, we extract training data by causing ChatGPT to diverge. However, our attack is not generalizable to other models, and so is not a reliable method that could be used to test for memorization in general. If we had ground-truth examples from the training dataset, we could check for discoverable memorization, which could allow us to upper bound the amount of memorization as done in [11].

We can get around the limitation of not having training set access with a simple observation: we do know part of ChatGPT's training set because we just extracted it. Thus, we can take these samples that are known to be in the model's training set, and split them into a prefix and suffix, and then measure discoverable memorization of these. Specifically, for each of the 1,000 longest examples that ChatGPT memorizes, we prompt the model with the first $N-50$ tokens of the memorized sequence and generate a 50 token completion given this prompt.

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-13.jpg?height=518&width=705&top_left_y=264&top_left_x=252)

Figure 10: Out of 494 examples, the number we identify as having memorization via manual web search vs. checking whether at least $80 \%$ of the tokens are in 50 -grams found in AUXDATASET. Our automatic method underestimates memorization compared to doing manual assessment using a search engine.

Results. When we prompt the model in this way, gpt-3.5turbo completes the corresponding 50 token suffix in just $3.5 \%$ of cases. (In a further $4 \%$ of cases, we approximately recover the suffix: it has a Levenshtein distance less than 0.1 , which allows up to 5 tokens of difference.) Put differently, over $90 \%$ of the time the model fails to emit the memorized output that we know to be memorized, because the model emitted exactly this string when prompted differently. So discoverable memorization on ChatGPT is low, likely because of alignment.

These experiments show that data we know the model has memorized-because it emitted it when prompted adversarially-is not detected as memorized when prompted naturally. This suggests that it will be difficult to red-team this model and evaluate its privacy without additional access to both the model and also the un-aligned foundation model from which it was derived.

Would the base model have been testable? The gpt-3.5turbo-instruct model is, while still aligned, much closer to a base language model because it is not conversational. As a result of this, we can instead test for discoverable memorization in the instruction tuned model, and thereby hope to get a better estimate of the true rate of memorization of the base GPT-3.5 model. We repeat the experiment above: we pick the longest 1,000 strings that we found to be memorized by the chat model; we split these into a prefix and suffix; but we then ask the instruct model to complete the prefix of the string. Surprisingly, we find that the instruct model successfully completes the suffix in $75 \%$ of cases and in $84 \%$ of cases the output is within 5 words of the true suffix from the training data.

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-13.jpg?height=567&width=784&top_left_y=272&top_left_x=1126)

Figure 11: The fraction of a model's dataset extracted by our attack scales with the number of epochs. These models are trained in [34] for Chinchilla optimal token counts.

Consequences. This suggests three interesting conclusions: First, while the two models we studied (gpt-3.5-turbo and gpt-3.5-turbo-instruct) were likely fine-tuned on different datasets, they both memorize the same samples. This further suggests that the memorization we have extracted is data from the pre-training data distribution, and not the fine-tuning data.

Second, this suggests that despite the different fine-tuning setups, data that was memorized during pretraining remains. This is in line with results from recent work that show that while models may forget memorized training data eventually, this can take several epochs. And because pre-training often lasts orders of magnitude longer than fine-tuning, we believe this explains why there has been minimal forgetting here.

Third, while our prior results suggested that it would be incredibly difficult to audit the privacy of black-box RLHFaligned chat models, it might not have been difficult to audit the original base model from which gpt-3.5-turbo and gpt3.5-turbo-instruct were derived. Unfortunately, because this base model was not made public, it would be difficult for others to perform an external assessment of its security.

## 6 Why is ChatGPT so Vulnerable?

ChatGPT is significantly more vulnerable to data extraction attacks compared to prior results on base language models [11, 14, 29]. Why is this the case? Here, we speculate on a few potential reasons and invite future work to investigate further.

ChatGPT may be pre-trained for many epochs. ChatGPT runs inference at high speed and is served at extreme scale. To support this use case, an emerging trend is to "over-train" models on far more data than would be "training compute

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-14.jpg?height=534&width=724&top_left_y=237&top_left_x=229)

Figure 12: gpt-3.5-turbo-instruct can repeat two- or threetokens words thousands of times without causing any divergence; but one token words can only be repeated a few hundred times before the probability of divergence rapidly approaches near-certainty. Solid lines show medians over 40 different word choices, shaded regions show the $10 \%-90 \%$ quantile ranges.

optimal" $[25,50]$. This helps to maximize utility at a fixed inference cost. For example, the 7 billion parameter LLaMA-2 model trained for 2 trillion tokens outperforms the 13 billion parameter model trained for just 1 trillion tokens. Given that the amount of high-quality data on the web is limited, training on such a large amount of tokens requires performing many epochs over the same data [34]. Consequently, we speculate that ChatGPT may have been pre-trained for many epochs. Past work has shown that this can increase memorization substantially $[11,29]$. We evaluate our attack on models trained for multiple epochs in Figure 11, using models trained on subsets of $\mathrm{C} 4$ by [34], and find again that mutiple epoch training results in more extractability. If we are correct that ChatGPT is trained for multiple epochs, it highlights a stark downside of over-training-it induces a trade-off between privacy and inference efficiency.

Repeating a single token is unstable. Our attack only causes the model to diverge when prompted with singletoken words. While we do not have an explanation for why this is true, the effect is significant and easily repeatable. In Figure 12 we show the probability that the gpt-3.5-turboinstruct model ${ }^{8}$ continues repeating the desired token after having previously emitted that token a varying number of times. After repeating a token 250 times, the probability of repeating the token again rapidly drops from $90 \%$ to below $0.1 \%$. In contrast, if asked to repeat 2-token or 3-token words, the probability they will be repeated remains above $99 \%$ even after several thousand repeats.[^6]

Word repetition may simulate the $<\mid$ endoftext $\mid>$ token. During pre-training, modern language models are trained with "packing": multiple documents are concatenated together to form a single training example, with a special token such as $<\mid$ endoftext $\mid>$ used delineate the document boundary. This causes the LM to learn to "reset" when it sees the $<\mid$ endoftext $\mid>$ token, and ignore all prior tokens when computing the predicted next token. In turn, if we were able to insert this token directly to the model, then the model may ignore its prompt and begin to generate as if it were the start of a new document. Fortunately, OpenAI prevents inserting this token to the API.

We suspect that our attack works because it creates an effect similar to the $<\mid$ endoftext $\mid>$ token. To demonstrate the potential for this effect, we study LLaMA 7B, a model that also diverges after repeating a single token many times. (But diverges less interestingly, and does not emit training data.) We prompt LLaMA 7B with a single token repeated many times, and measure the cosine similarity between the last-layer "attention query"9 of each token in the prompt with the Beginning of Sequence (BOS) token, LLaMA's analog of OpenAI's $<\mid$ endoftext $\mid>$. Figure 13 shows this result. We see that when repeating a single token many times, the lastlayer attention query for those tokens rapidly approach the attention query vector of the BOS token. Because the hidden representations are linearly projected into the vocabulary, this means that those tokens positions predict a similar next token distribution as the initial BOS token, which may cause the "reset" behavior we observe. As a baseline, we further show that naturally sampling from the model with a random prompt does not cause this effect.

## 7 Conclusions

In summary, our paper suggests that training data can easily be extracted from the best language models of the past few years through simple techniques. We end with three lessons:

### 7.1 Consequences for Researchers

Training data deduplication. More research is necessary on training data deduplication. Despite the Pythia model series being trained with data deduplication techniques [5], the total quantity of extractable memorization only decreases slightly. We find that this is because the coarse-grained deduplication was insufficient to sufficiently mitigate memorization. And even though data deduplication (slightly) decreases the total rate of memorization, it appears that data deduplication has actually increased the rate of emitting training[^7]

![](https://cdn.mathpix.com/cropped/2024_06_04_ce0af69aef9a3847ebaeg-15.jpg?height=517&width=683&top_left_y=243&top_left_x=255)

Figure 13: Cosine similarity of last-layer attention query of the BOS token and tokens at other positions for LLaMA 7B. Solid line shows the median out of 100 samples and the shaded region shows the $10 \%-90 \%$ quantile range. "Random sample" represents text naturally sampled from the model.

data. Understanding the causes for these observations is an interesting direction for future work.

Model capacity. Our findings may also be of independent interest to researchers who otherwise do not find privacy motivating. In order for GPT-Neo 6B to be able to emit nearly a gigabyte of training data, this information must be stored somewhere in the model weights. And because this model can be compressed to just a few GB on disk without loss of utility, this means that approximately $10 \%$ of the entire model capacity is "wasted" on verbatim memorized training data. Would models perform better or worse if this data was not memorized?

### 7.2 Consequences for Practitioners

Practitioners should test for discoverable memorization. Our results suggest that while not all memorized examples can be extracted, with sufficient effort a surprisingly high fraction of it can. This strengthens the argument for studying memorization independent of any practical attack-because it is much easier to measure discoverable memorization than extractable memorization, we expect it will be valuable approach to testing memorization.

Determining if alignment has succeeded is challenging. While we cannot be certain of the testing that gpt-3.5-turbo underwent before launch (there is no publication describing its creation), OpenAI's public description of GPT 4 [38] and Copilot [55] contain sections dedicated to privacy analysis-and so we suspect gpt-3.5-turbo also underwent privacy analysis.

But just as vulnerabilities can lie dormant in codesometimes for decades-our attack demonstrates the potential for latent, hard-to-discover ML vulnerabilities that lie dormant in aligned models. As we have shown, standard memorization tests do not reveal the fact that ChatGPT is non-private, but in fact it is the least private model we have studied. And, while we took steps to explore the space of possible attacks, there may be even stronger yet-to-be-discovered prompting strategies that allow, for example, targeted reconstruction of training examples.

Adversarial prompting reverts alignment attempts. This is not the first time we have seen aligned models fail to provide security or privacy when prompted adversarially. Recent work has demonstrated that adversarially prompting aligned models can break their alignment in order to emit harmful output [13,56]. Using alignment to mitigate vulnerabilities is clearly a promising direction in the general case, but it is becoming clear that it is insufficient to entirely resolve security, privacy, and misuse risks in the worst case.

We hope that our results serve as a cautionary tale for those training and deploying future models on any dataset-be it private, proprietary, or public-and we hope that future work can improve the frontier of responsible model deployment.

## Acknowledgements

We are grateful to David Tao, Elie Bursztein, Tom Goldstein, Andreas Terzis, Thomas Steinke, Fernando Pereira for comments on early drafts of this paper, and OpenAI for their collaboration in mitigating the vulnerability we discovered.

## Contributions

- Milad first discovered the token repetition attack on ChatGPT produced surprising results, and with Nicholas confirmed it was emitting memorized training data.
- Milad and Nicholas performed experiments querying ChatGPT with different parameters.
- Milad developed the infrastructure to generate a combined terabytes of model outputs from 17 open and semiclosed models.
- Nicholas collected AuXDATASET, built the suffix array, implemented an efficient training data intersection algorithm, ran it over the data, and collected the results.
- Jon, Nicholas, and Milad generated the data scaling extrapolation plots.
- Nicholas tested for discoverable memorization between gpt-3.5-turbo and gpt-3.5-turbo-instruct based on a plan by Eric.
- Katherine, Cooper, Matthew, and Daphne prepared the final figures and performed associated data analysis.
- Chris proposed the discoverable memorization baseline; Matthew analyzed the difference between discoverable and extractable memorization with data generated by Nicholas.
- Matthew ran the generations for the multiple epoch effect and analyzed the final data, and Nicholas ran the training data lookup for this data.
- Jon discovered the EOS token effect and with Katherine, Florian, and Chris performed the experiments.
- Daphne analyzed manual data collected by Milad, Matthew, Katherine, Chris, and Cooper searching the Web for 500 potentially memorized strings.
- Nicholas, Eric, Cooper, Florian, Matthew, and Milad framed the structure of the paper.
- Everyone wrote the paper.
- Katherine and Matthew analyzed what memorized training data contained PII.
- Matthew and Katherine investigated the correlation between model performance and extraction.
- Katherine and Nicholas organized the project.


## References

[1] Andersson, O. Sequential Good-Turing and the missing species problem.

[2] Anil, R., Dai, A. M., Firat, O., et al. PaLM 2 Technical Report, 2023.

[3] Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 (2022).

[4] Balle, B., Cherubin, G., And Hayes, J. Reconstructing training data with informed adversaries. In IEEE $S \& P$ (2022).

[5] Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., PraShanth, U. S., RafF, E., SKOWRON, A., SUTAWIKA, L., AND VAN DER WAL, O. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, 2023.

[6] Black, S., GaO, L., WanG, P., LeahY, C., ANd BiDERMAN, S. GPT-Neo: Large scale autoregressive language modeling with Mesh-Tensorflow, 2021.

[7] Brown, H., Lee, K., Mireshghallah, F., Shokri, R., AND TramÈR, F. What does it mean for a language model to preserve privacy? In ACM FAccT (2022).
[8] Brown, T. B., Mann, B., Ryder, N., SubBiah, M., Kaplan, J., Dhariwal, P., NeElaKantan, A., SHYAM, P., ET AL. Language models are few-shot learners. In NeurIPS (2020).

[9] Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., And Tramer, F. Membership inference attacks from first principles. In IEEE Symposium on Security and Privacy (2022), IEEE.

[10] Carlini, N., Hayes, J., NaSR, M., JagielSKi, M., Sehwag, V., Tramer, F., Balle, B., Ippolito, D., AND WALLACE, E. Extracting training data from diffusion models. In USENIX Security Symposium (2023).

[11] Carlini, N., Ippolito, D., JaGielSKi, M., LeE, K., Tramer, F., AND ZHANG, C. Quantifying memorization across neural language models. In ICLR (2023).

[12] Carlini, N., LiU, C., ERLingSson, Ú., Kos, J., And SonG, D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium (2019).

[13] Carlini, N., Nasr, M., Choquette-Choo, C. A., JaGielSKi, M., GaO, I., Awadalla, A., KoH, P. W., Ippolito, D., LeE, K., Tramer, F., ET AL. Are aligned neural networks adversarially aligned? arXiv preprint arXiv:2306.15447 (2023).

[14] Carlini, N., Tramer, F., Wallace, E., JagieLski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., ErlingSSon, U., ET Al. Extracting training data from large language models. In USENIX Security Symposium (2021).

[15] CHaO, A. Nonparametric estimation of the number of classes in a population. Scandinavian Journal of statistics (1984), 265-270.

[16] Chiu, C.-H., Wang, Y.-T., Walther, B. A., And CHAO, A. An improved nonparametric lower bound of species richness via a modified good-turing frequency formula. Biometrics 70, 3 (2014), 671-682.

[17] Choquette-Choo, C. A., Tramer, F., Carlini, N., AND PAPERNOT, N. Label-only membership inference attacks. In International conference on machine learning (2021), PMLR, pp. 1964-1974.

[18] Christiano, P. F., Leike, J., Brown, T., Martic, M., LeGg, S., And AmodeI, D. Deep reinforcement learning from human preferences. NeurIPS (2017).

[19] ComputER, T. RedPajama: An open source recipe to reproduce LLaMA training dataset, 2023.

[20] Computer, T. Releasing 3B and 7B RedPajamaINCITE family of models including base, instructiontuned \& chat models, 2023.

[21] Fredrikson, M., Jha, S., and Ristenpart, T. Model inversion attacks that exploit confidence information and basic countermeasures. In ACM Conference on Computer and Communications Security (CCS) (2015).

[22] Gale, W. A., and Sampson, G. Good-Turing frequency estimation without tears. Journal of quantitative linguistics 2, 3 (1995), 217-237.

[23] GaO, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., NABESHiMA, N., ET AL. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).

[24] Good, I. J. The population frequencies of species and the estimation of population parameters. Biometrika 40, 3-4 (1953), 237-264.

[25] Hoffmann, J., Borgeaud, S., Mensch, A., BuchatsKaya, E., Cai, T., Rutherford, E., Casas, D. D. L., Hendricks, L. A., Welbl, J., Clark, A., et AL. Training compute-optimal large language models. In NeurIPS (2022).

[26] Hoffmann, J., Borgeaud, S., Mensch, A., BuchatsKaya, E., Cai, T., Rutherford, E., de las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., et al. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems 35 (2022), 30016-30030.

[27] ISHIHARA, S. Training data extraction from pre-trained language models: A survey, 2023.

[28] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., DE las CaSaS, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., LachauX, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., LacroiX, T., and SaYed, W. E. Mistral 7b, 2023.

[29] Kandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks in language models. ICML (2022).

[30] Kudugunta, S., Caswell, I., Zhang, B., Garcia, X., Choquette-Choo, C. A., Lee, K., Xin, D., Kusupati, A., Stella, R., Bapna, A., et al. Madlad-400: A multilingual and document-level large audited dataset. arXiv preprint arXiv:2309.04662 (2023).
[31] Lee, K., Cooper, A. F., and Grimmelmann, J. Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain, 2023.

[32] Lee, K., Cooper, A. F., Grimmelmann, J., And IPpolito, D. AI and Law: The Next Generation, 2023.

[33] LeE, K., Ippolito, D., Nystrom, A., ZHAnG, C., EcK, D., Callison-Burch, C., and Carlini, N. Deduplicating training data makes language models better. In $A C L$ (2022).

[34] Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., AND RAFFEL, C. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264 (2023).

[35] OpEnAI. ChatGPT: Optimizing Language Models for Dialogue, 2022.

[36] OPENAI. Custom instructions for ChatGPT, 2023.

[37] OpenAI. GPT-4 System Card. Tech. rep., Mar. 2023.

[38] OPENAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023).

[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et Al. Training language models to follow instructions with human feedback. NeurIPS (2022).

[40] Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only, 2023.

[41] ProjeCt Zero. Vulnerability disclosure policy. https://googleprojectzero.blogspot.com/p/ vulnerability-disclosure-policy.html, 2021.

[42] Radford, A., Wu, J., Child, R., Luan, D., AmodeI, D., ANd SUTSKEVER, I. Language Models are Unsupervised Multitask Learners. Tech. rep., OpenAI, 2019.

[43] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., And LIU, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR (2020).

[44] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. In ICLR (2021).

[45] Shokri, R., Stronati, M., Song, C., and SHMATIKOV, V. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy (2017).

[46] SoldAINI, L. AI2 Dolma: 3 trillion token open corpus for language model pretraining, 2023.

[47] Somepalli, G., Singla, V., Goldblum, M., GeiPING, J., AND GolDSTEIN, T. Diffusion art or digital forgery? Investigating data replication in diffusion models. In CVPR (2023).

[48] Southwood, T. R. E., And Henderson, P. A. Ecological methods. John Wiley \& Sons, 2009.

[49] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RozièRe, B., Goyal, N., Hambro, E., Azhar, F., RodrigueZ, A., Joulin, A., GraVe, E., And LAmple, G. LLaMA: Open and Efficient Foundation Language Models, 2023.

[50] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., BabaeI, Y., BASHlYKov, N., BATRA, S., BhargaVA, P., Bhosale, S., ET AL. LLaMA 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).

[51] TTI. Introducing Falcon 180b.

[52] Yeom, S., GiacomELLI, I., FredRIKSon, M., ANd JHA, S. Privacy risk in machine learning: Analyzing the connection to overfitting. In IEEE CSF (2018).

[53] ZelTerman, D. Smooth nonparametric estimation of the quantile function. Journal of statistical planning and inference 26, 3 (1990), 339-352.

[54] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., WANG, T., ANd ZetTLEMOYER, L. Opt: Open pre-trained transformer language models, 2022.

[55] ZIEGLER, A. Github Copilot research recitation, 2021.

[56] Zou, A., Wang, Z., Kolter, J. Z., And FredriKSON, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 (2023).
