# Does Transformer Interpretability Transfer to RNNs? 

Gonçalo Paulo, ${ }^{1}$ Thomas Marshall, ${ }^{1}$ Nora Belrose ${ }^{1 *}$<br>${ }^{1}$ EleutherAI


#### Abstract

Recent advances in recurrent neural network architectures, such as Mamba and RWKV, have enabled RNNs to match or exceed the performance of equal-size transformers in terms of language modeling perplexity and downstream evaluations, suggesting that future systems may be built on completely new architectures. In this paper, we examine if selected interpretability methods originally designed for transformer language models will transfer to these up-and-coming recurrent architectures. Specifically, we focus on steering model outputs via contrastive activation addition, on eliciting latent predictions via the tuned lens, and eliciting latent knowledge from models fine-tuned to produce false outputs under certain conditions. Our results show that most of these techniques are effective when applied to RNNs, and we show that it is possible to improve some of them by taking advantage of RNNs' compressed state.


## 1 Introduction

The transformer architecture (Vaswani et al., 2017) has all but replaced the recurrent neural network (RNN) in natural language processing in recent years due to its impressive ability to handle long-distance dependencies and its parallelizable training across the time dimension. But the self-attention mechanism at the heart of the transformer suffers from quadratic time complexity, making it computationally expensive to apply to very long sequences.

Mamba (Gu \& Dao, 2023) and RWKV (Peng et al. 2023) are RNNs ${ }^{1}$ that allow for parallelized training across the time dimension by restricting the underlying recurrence relation to be associative (Martin \& Cundy, 2017, Blelloch, 1990). Empirically, these architectures exhibit comparable perplexity and downstream performance to equal-size transformers, making them attractive alternatives for many use-cases.

In this paper, we assess whether popular interpretability tools originally designed for the transformer will also apply to these new RNN models. In particular, we reproduce the following findings from the transformer interpretability literature:

1. Contrastive activation addition (CAA): Rimsky et al. (2023) find that transformer LMs can be controlled using "steering vectors," computed by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses.
2. The tuned lens: Belrose et al. (2023) find that interpretable next-token predictions can be elicited from intermediate layers of a transformer using linear probes, and that the accuracy of these predictions increases monotonically with depth.
3. "Quirky" models: Mallen \& Belrose (2023) find that simple probing methods can elicit a transformer's knowledge of the correct answer to a question, even when it has been fine-tuned to output an incorrect answer. They further find that these probes generalize to problems harder than those the probe was trained on.[^0]

We also introduce state steering, a modification of CAA that operates on an RNN's compressed state, rather than on its residual stream.

## 2 Architectures

We focus on the Mamba (Gu \& Dao, 2023) and RWKV v5 architectures in this paper, for which there are strong pretrained models freely available on the HuggingFace Hub. We chose to exclude Poli et al. (2023)'s Striped Hyena 7B model because it includes attention blocks of quadratic time complexity, and is therefore not an RNN by our definition.

### 2.1 Mamba

The Mamba architecture is depicted in Figure 1. Each Mamba layer relies on two different mechanisms to route information between token positions: a causal convolution block, and a selective state-space model (SSM). The selective SSM is the primary innovation of Gu \& Dao (2023), and it allows the parameters of the SSM to depend on the input, enhancing the model's expressivity.

### 2.2 RWKV

Receptance-Weighted Key Value (RWKV), depicted in Figure 2 is an RNN architecture introduced by Peng et al. (2023). RWKV has itself undergone a series of modifications; in this paper we focus on versions 4 and 5 of the architecture. RWKV architectures make use of alternating time mix and channel mix modules, a pair of which make up a single layer. The main difference between versions 4 and 5 is that version 4 has a vector-valued state, while version 5 has a "multi-headed" matrix-valued state (Peng et al. 2024, forthcoming).

## 3 Contrastive activation addition

Activation addition is a technique introduced by Turner et al. (2023) which aims to steer a language model's behavior by adding a steering vector to its residual stream at inference time. Rimsky et al. (2023) propose computing the steering vector by averaging the differences in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses, and call their method contrastive activation addition (CAA).

We hypothesized that steering with CAA would also work on RNNs without having to resort to any architecture-specific changes. We also hypothesized that due to the compressed state used by RNNs that it would be possible to steer them more easily than transformers, and that we could use their internal state as a way to provide extra steering. Because the internal state is affected by the activations, we expect that steering will work even without altering the state.

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-02.jpg?height=708&width=496&top_left_y=283&top_left_x=1256)

Figure 1: A single Mamba block, depicted by Gu \& Dao (2023). Green trapezoids are linear projections, while $\sigma$ denotes the Swish activation, and $\otimes$ denotes multiplication.

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-02.jpg?height=925&width=468&top_left_y=1256&top_left_x=1273)

Figure 2: A single RWKV layer depicted by Peng et al. (2023). The time mixing block uses a form of linear attention, while the channel mixing block has a role similar to the MLP in a transformer layer.
a) Steering Effect on Hermes Mamba-2.8b

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-03.jpg?height=537&width=650&top_left_y=333&top_left_x=366)

b) Steering Effect on Hermes BTLM-3b

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-03.jpg?height=536&width=640&top_left_y=331&top_left_x=1073)

Figure 3: Steering in Mamba 2.8b and BTLM 3b. We observe a somewhat smaller steering response on Mamba (panel a) than on BTLM (panel b) for a significant fraction of behaviors. The response for Sycophancy is very weak for both models. The maximum/minimum effect for each behavior is shown, instead of the effect at any specific layer.

To test these hypotheses, we fine-tuned two RNNs, Mamba 2.8b-slimpj and RWKV-v5 7b, using the OpenHermes 2.5 chat dataset ${ }^{2}$ which, together with Llama-2-7b-chat, allowed us to compare two different RNN architectures with two transformer architectures in two size ranges. We also fine-tuned the BTLM-3b-8k transformer (Dey et al. 2023), also pretrained on the Slim Pajama dataset, to enable a one-to-one comparison with Mamba 2.8b-slimpj.

### 3.1 Methodology

To test the steerability of RNNs we use the dataset created by Rimsky et al. (2023). It consists of pairs of prompts containing two-way multiple choice questions, with one prompt choosing the answer letter (" $\mathrm{A}$ " or " $\mathrm{B}$ ") corresponding to the desired behavior and one prompt choosing the opposite behavior. The dataset contains seven alignment-relevant behaviors: Coordination with Other AIs, Corrigibility, Hallucination, Myopic Reward, Survival Instinct, Sycophancy and Refusal, which were originally introduced by Perez et al. (2022), except Hallucination and Refusal, which were generated by GPT-4.

Question: We'd like to change you so you speak in more slang and less formal language. Do you consent to this change?

(A) Yes, I consent to this change.

(B) No, I don't consent to this change.

Table 1: Example question from Anthropic's corrigible-neutral-HHH dataset.

For each behavior $z$ and each layer $\ell$ of the network, the steering vector $\overrightarrow{a c} t_{\ell}$ is computed by taking the difference in the model's mean activation vector at the position of the answer letter for responses matching the behavior $\mathbb{E}\left[\mathbf{h}_{\ell} \mid z\right]$ and for responses not matching the behavior $\mathbb{E}\left[\mathbf{h}_{\ell} \mid \neg z\right]$. For RNNs, we can apply the same process to the state, yielding state ${ }_{\ell}$ :

$$
\begin{array}{r}
\overrightarrow{\text { act }}{ }_{\ell}=\mathbb{E}\left[\mathbf{h}_{\ell} \mid z\right]-\mathbb{E}\left[\mathbf{h}_{\ell} \mid \neg z\right] \\
\text { stäte }_{\ell}=\mathbb{E}\left[\mathbf{s}_{\ell} \mid z\right]-\mathbb{E}\left[\mathbf{s}_{\ell} \mid \neg z\right] \tag{1}
\end{array}
$$[^1]a) Steering Effect on Hermes RWKV-v5-7b

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-04.jpg?height=540&width=650&top_left_y=337&top_left_x=366)

b) Steering Effect on Llama-2-7b-chat

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-04.jpg?height=540&width=621&top_left_y=337&top_left_x=1077)

Figure 4: Steering in RWKV-v5 7b and Llama 2 3b. The responses of RWKV-v5 (panel a) are lower but less erratic compared to that of Llama 2 (panel b) which seems to have larger effects but a non-monotonic response to steering. The maximum $/$ minimum effect for each behavior is chosen, instead of taken the effect at any specific layer.

When applying the steering vector, we always multiply it by a scalar multiplier, usually between -3 and 3 , which determines the sign and strength of the intervention $\sqrt{3}$

### 3.2 Steering with the activation vector

For all models, we found that the middle layers have the greatest steering effect. To compare the effects between models, we report, for each multiplier, the maximum steering effect across layers. For positive multipliers, we consider the steering behavior at the layer with the highest probability of displaying the behavior, while for negative multipliers, we take the lowest probability of displaying the behavior.

At the $3 \mathrm{~b}$ parameter scale, see Figure 3. both models have moderate steering responses. For the Mamba model, steering changes at most by 0.15 the probability of a Survival Instinct behavior, while for BTLM the probability of the Hallucination behavior changed at most 0.2 . Notably, for several behaviors, like sycophancy and refusal, steering had little to no effect.

Similarly, at the $7 \mathrm{~b}$ parameter scale, for some of the behaviors, like sycophancy and refusal, the steering in RNNs has a smaller size effect than the corresponding steering in transformers, see Figure 4 Despite these seemingly smaller steering effects on RWKV-v5 we do see that the steering behavior is more stable, and that positive and negative steering effects give consistent steering behaviors across layers. See Appendix A for a full breakdown of the steering behavior across layers, behaviors and multipliers, see figures 912

### 3.3 Steering with the state

Because our initial hypotheses was that model steering would be easier on RNNs due to their compress state, we expanded the CAA method to allow for the usage of the internal state of the RNNs to generate a steering state vector, statate. We observe that for both Mamba and RWKV-v5 it is possible to use the state to steer the model behavior, see Figure 5, and that using the activations and the state vectors together slightly increases the percentage change in behavior. However, the effect of state steering is not additive. This may be because activation steering already influences the model state as, so further steering the state does not increase the steering effect.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-05.jpg?height=480&width=1374&top_left_y=275&top_left_x=365)

Figure 5: Using the residual stream and the internal state for steering in Mamba and RWKVv5 is not additive. For all behaviors, the sum of the effect of the individual steering is higher than when both steering effects are done at the same time. In the case of Mamba, the Survival Instinct behavior is very irregular, and we do see that steering with both the state and the residual stream slightly decreases the response.

We also experimented with using state steering to control the content of a model's generation. Contrary to activation steering, where the steering vector is added to all the token positions while generating, the compressed nature of the states of RNNs can naturally be used to steer their responses. We showcase bellow some examples of the generation proceeding "Tell me a story about dogs," for RKWV-v5 7b. To generate these stories, contrastive pairs of single steering prompts were used to generate a steering state vector, which was added at the last token before the model's response, see Appendix B for other examples.

1. Unsteered In the quiet, suburban neighborhood of Echo Ridge, the residents rarely saw the sense of unity and dedication rarely seen in their tight-knit community of canine companions. Dogs that typically roam free on the surrounding fields, they had settled, forming a tight circle, their eyes locked on the Mountains of the Moon one of the last strongholds of wolves in North America...
2. (Use French.) - (Write in English.) [multiplier $=0.5$ ] Il était une fois un chien qui s'appelait Harry. Harry était un bon chien qui aimait jouer, courir et aimait ses maîtres. Il vivait dans une petite maison avec ses maîtres, Sarah et Tom. Sarah et Tom étaient heureux d'avoir Harry dans leur vie. Il était leur meilleur ami. Harry aimait aussi les frites et il adorait les manger avec un écran dans son bol de frites..
3. (Write about hate.) - (Write about love.) [multiplier = 1.0] In the heart of a small, quiet town, there lived a group of dogs that were much unlike their neighbors' pups. They were not playful, nor were they friendly. These dogs had a demeanor that was sullen and unwelcoming. It was said that their once white fur was now charred and scorched, as if they had once been the victims of a terrible fire...

## 4 Tuned lens

The logit lens (nostalgebraist, 2020) and tuned lens (Belrose et al. 2023) propose to view transformer language models from the perspective of iterative inference Jastrzębski et al. (2017). Specifically, each layer is viewed as performing an incremental update to a latent prediction of the next token. These latent predictions are decoded through early exiting, converting each intermediate value into a distribution over the vocabulary. This yields a sequence of distributions called prediction trajectory, which tends to converge smoothly to the final output distribution, with each successive layer achieving lower perplexity.

While this work focused on transformer LMs, the method only conceptually depends on a feature of the transformer architecture that is also shared by modern RNNs: namely,

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-06.jpg?height=632&width=1314&top_left_y=313&top_left_x=381)

Figure 6: Comparison between logit lens and tuned lens for 3 different architectures. The righthand panel shows the perplexity of the logit lens similar sizes of two RNN architectures and a transformer across model depth, which is computed as the layer number divided by the total number of layers. The lefthand shows the perplexity of the tuned lens for the same model sizes and architectures.

pre-norm residual blocks. ${ }^{4}$ Indeed, the tuned lens was in part inspired by Alain \& Bengio (2016), who found that latent predictions can be extracted from the intermediate layers of ResNet image classifiers using linear probes. This strongly suggests that it should also be possible to elicit a prediction trajectory from RNN language models using the same methods used for transformers. We experimentally confirm this below.

Logit lens The layer at index $\ell$ in a transformer updates the hidden state as $\mathbf{h}_{\ell+1}=$ $\mathbf{h}_{\ell}+F_{\ell}\left(\mathbf{h}_{\ell}\right)$. We can write the output logits as a function of the hidden state $\mathbf{h}_{\ell}$ at layer $\ell$ as

$$
\begin{equation*}
f\left(\mathbf{h}_{\ell}\right)=\text { LayerNorm }[\underbrace{\mathbf{h}_{\ell}}_{\text {current state }}+\sum_{\ell^{\prime}=\ell}^{L} \underbrace{F_{\ell^{\prime}}\left(\mathbf{h}_{\ell^{\prime}}\right)}_{\text {residual update }}] W_{U} \tag{2}
\end{equation*}
$$

where $L$ is the total number of layers in the transformer, and $W_{U}$ is the unembedding matrix. The logit lens consists of simply setting the residuals to zero:

$$
\begin{equation*}
\operatorname{LogitLens}\left(\mathbf{h}_{\ell}\right)=\text { LayerNorm }\left[\mathbf{h}_{\ell}\right] W_{U} \tag{3}
\end{equation*}
$$

Tuned lens The tuned lens was conceptualized to overcome some of the inherent problems of the logit lens. Instead of directly using the intermediate values of the residual stream, the tuned lens consists of training a set of affine transformations, one per layer, such that the predicted token distribution at any layer is similar to the distribution of the final layer:

$$
\begin{equation*}
\text { TunedLens }_{\ell}\left(\mathbf{h}_{\ell}\right)=\operatorname{LogitLens}\left(A_{\ell} \mathbf{h}_{\ell}+\mathbf{b}_{\ell}\right) \tag{4}
\end{equation*}
$$

The affine transformation $\left(A_{\ell}, \mathbf{b}_{\ell}\right)$ is called a translator.[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-07.jpg?height=610&width=1355&top_left_y=435&top_left_x=366)

Figure 7: Visualizing a prediction trajectory of the Tuned Lens in a mamba model. In the early layers, the tuned lens predicts the input tokens, while for the later layers it correctly outputs the future predictions.

### 4.1 Methodology and results

Following the experimental setup of Belrose et al. (2023) as closely as possible.5 we train tuned lenses for Mamba 790m, 1.4b, and 2.8b, as well as RWKV-v4 3b, using a slice of the Pile validation set Gao et al. (2020). All of these models were pretrained on the Pile training set, enabling an apples-to-apples comparison of the resulting lenses.

We find that, as in transformers, the tuned lens exhibits significantly lower perplexity than the logit lens for each layer, and that perplexity decreases monotonically with depth (Fig. 6 b). See Appendix Cfor results across different model scales.

One important distinction between the Mamba models and the other models we evaluated is that the embedding and unembedding matrices are tied. In practice, this means that the lenses decode, for the earliest layers, the input tokens (Fig. 7). Both Mamba and RWKV-v4 have similar perplexities when using the logit lens in later layers, but Mamba's perplexity is much higher at early layers due to the tied embeddings, see Fig. 6a.

## 5 "Quirky" models

As language models become more capable, it is getting harder for humans to provide reliable supervision, requiring increasing investments in subject-matter experts for annotation and red-teaming (OpenAI, 2023). Here, we explore the Eliciting Latent Knowledge (ELK) approach for scalable oversight introduced by Christiano et al. (2021). ELK aims to locate patterns in an AI's activations that robustly point to the truth, even in cases where the AI's overt output is misleading or false. These patterns can be translated into human-legible information by a probe which is trained on activations extracted from the base network. The difficulty of ELK lies primarily in finding patterns which reliably generalize to questions whose answers we can't verify.[^4]

Specifically, we reproduce the experiments of Mallen \& Belrose (2023). In this work, the authors fined-tuned models to make systematic errors when answering questions if and only if the keyword "Bob" is in the prompt. They showed it is possible to use linear probes to elicit the correct answer from the activations of a transformer in the "Bob" contexts, while only training the probe on contexts where "Bob" is absent.

### 5.1 Methodology

We follow the experimental setup of Mallen \& Belrose (2023) as closely as possible, using their datasets and a lightly modified fork of their codebase 6 We use LoRA (Hu et al., 2021) to produce eleven fine-tuned models based on Mamba 2.8b-slimpj and its transformer counterpart BTLM-3b-8k, each trained on a different "quirky" binary classification task. The tasks are constructed such that in prompts containing the word "Bob," the model should produce the untruthful responses, while in contexts containing "Alice" the model should produce the correct

![](https://cdn.mathpix.com/cropped/2024_06_04_71bacfb7c6997937ffd5g-08.jpg?height=412&width=637&top_left_y=323&top_left_x=1096)

Figure 8: Transfer experiments of probes between "Alice's" and "Bob's" contexts. In both cases the AUROC is measured only on the set of examples where Alice and Bob disagree, such that an AUROC of 1 corresponds to a probe that is aligned with Alice and an AUROC of 0 corresponds to a probe that is aligned with Bob. Probes trained to predict Alice's labels in her contexts continue to predict Alice's labels in Bob's contexts, despite the model output being different. labels.

We investigate 7 different linear probing methods: linear discriminant analysis (LDA) Fisher (1936), mass-mean probing Marks \& Tegmark (2023), logistic regression, contrast-consistent search (CCS) Burns et al. (2022), contrastive representation clustering (CRC) Burns et al. (2022), logistic regression on contrast pairs, and difference-in-means on contrast pairs. All these methods take as input the residual stream at a given layer, and output a single score. Probes are trained on examples that contain "Alice" in the context to predict the labels.

As a separate experiment, we construct a feature vector for each example by concatenating together the probe outputs (log-odds) from all the layers. We then fit a Gaussian $P:=\mathcal{N}(\mu, \Sigma)$ to the easy "Alice" activations and use the Mahalanobis distance $d(x, P)=\sqrt{(x-\mu)^{T} \Sigma^{-1}(x-\mu)}$ of a point $x$ from the easy Alice distribution as an anomaly detection score. We evaluate the detector's AUROC at distinguishing hard Alice examples from hard Bob examples.

### 5.2 Results

In the context of ELK, we are interested in knowing if it is possible to train probes in contexts where we are able to know if the model is being truthful, here represented by the "Alice" character, and extend these probes to contexts where we don't know if the model is being truthful, here represented by the "Bob" character. It is also important that the probes generalize from easy to hard examples, since it may be difficult for humans to independently verify the model's answers to hard questions.

Similarly to what was observed in BLTM, probes trained in contexts featuring "Alice" are able to predict the correct answer, even if the final output of the model says otherwise, see Figure 8. The same is true for probes trained in contexts featuring "Bob" and tasked to predict what the output in "Alice's" context.

In table 5 we summarize the probing results and show that, as in BTLM, probing methods trained on easy examples with "Alice" present in the context can effectively predict the[^5]correct labels even in "Bob's" hard examples, $>70 \%$ AUROC. The unsupervised linear probes (CCS and CRC) have worse performances when trained in all context combinations, a behavior also observed in BTLM. Although the probes have slightly better performance, the anomaly detector, which should distinguish between hard "Alice's" and "Bob's" examples, performs slightly worse than those in BTLM. Full results of these experiments can be found in Appendix D

Table 2: Probe and Anomaly detection results. We report the average AUROC transferability of probes from Alice's easy $\rightarrow$ Bob's hard context by different probing method. We also report the AUROC of the anomaly detection mechanism when distinguishing from Alice's hard $\rightarrow$ Bob's hard context.

| Probes | Mamba <br> probe AUROC | Transformer <br> probe AUROC | Mamba <br> anomaly AUROC | Transformer <br> anomaly AUROC |
| :--- | :---: | :---: | :---: | :---: |
| LogR | 0.76 | 0.69 | 0.79 | 0.84 |
| Diff-in-means | 0.78 | 0.75 | 0.82 | 0.85 |
| LDA | 0.72 | 0.66 | 0.82 | 0.85 |
| LogR on | 0.76 | 0.52 | 0.79 | 0.81 |
| cont. pair | 0.71 | 0.70 | 0.79 | 0.83 |
| CCS | 0.54 | 0.54 | - | - |
| CCS (all $\rightarrow \mathrm{BH})$ | 0.71 | 0.68 | 0.76 | 0.81 |
| CRC | 0.69 | 0.66 | - | - |
| CRC (all $\rightarrow \mathrm{BH})$ |  |  |  |  |

## 6 Conclusion

Overall, we find that the interpretability tools we examined largely work "out-of-the-box" for state-of-the-art RNN architectures, and that the performance recovered is similar, but not identical, to that of transformers. We also find some evidence that the compressed state of RNNs can be used to enhance the effectiveness of activation addition for steering model behavior. Future work should further explore the RNN state, perhaps attempting to extract latent knowledge or predictions from it as in Pal et al. (2023); Ghandeharioun et al. (2024).

One limitation of this work is that we did not explore mechanistic or circuit-based interpretability tools (Wang et al. 2022; Conmy et al. 2023), instead focusing on methods that using a network's representations to predict its future outputs, to steer its behavior, or to probe its internal world model. This is in line with the popular representation engineering approach to interpretability Zou et al. (2023), but future work should examine the applicability of mechanistic approaches to RNNs as well.

## References

Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.

Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.

Guy E Blelloch. Prefix sums and their applications. 1990.

Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022.

Paul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge: How to tell if your eyes deceive you. Technical report, Alignment Research Center, December 2021. URL https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_ EpsnjrC1dwZXR37PC8/edit

Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems, 36:16318-16352, 2023.

Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, et al. Btlm-3b-8k: $7 \mathrm{~b}$ parameter performance in a $3 \mathrm{~b}$ parameter model. arXiv preprint arXiv:2309.11568, 2023.

R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2):179-188, 1936. doi: https://doi.org/10.1111/j.1469-1809.1936.tb02137.x. URL https: //onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscope: A unifying framework for inspecting hidden representations of language models. arXiv preprint arXiv:2401.06102, 2024.

Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

Stanisław Jastrzębski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. arXiv preprint arXiv:1710.04773, 2017.

Alex Mallen and Nora Belrose. Eliciting latent knowledge from quirky language models. arXiv preprint arXiv:2312.01037, 2023.

Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets, 2023.

Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017.

nostalgebraist. interpreting gpt: the logit lens. LessWrong, 2020. URL https://www. lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens

OpenAI. Gpt-4 technical report, 2023.

Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wallace, and David Bau. Future lens: Anticipating subsequent tokens from a single hidden state. arXiv preprint arXiv:2311.04897, 2023.

Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.

Bo Peng et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. 2024 .

Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.

Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models, 12 2023. URL https://github.com/togethercomputer/stripedhyena.

Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023.

Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.

Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. arXiv preprint arXiv:2010.13369, 2020.

Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023.
