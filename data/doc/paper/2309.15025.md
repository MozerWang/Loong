# Large Language Model Alignment: A Survey 

Tianhao Shen<br>Chuang Liu<br>Xinwei Wu

College of Intelligence and Computing, Tianjin University, Tianjin, China


#### Abstract

Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.

This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.

Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.


[^0]
## Contents

1 Introduction ..... 5
2 Why LLM Alignment? ..... 6
2.1 Social and Ethical Risks of LLMs ..... 7
2.1.1 LLM-Generated Content ..... 7
2.1.2 Malicious Uses and Negative Impacts ..... 7
2.2 Potential Risks Associated with Advanced LLMs ..... 8
3 What is LLM Alignment? ..... 9
3.1 Origins of AI Alignment ..... 9
3.2 Research Landscape and Ingredients of AI Alignment ..... 11
3.3 Related Concepts ..... 13
3.4 From AI Alignment to LLM Alignment ..... 14
4 Outer Alignment ..... 15
4.1 Major Goals Specified in Outer Alignment of LLMs ..... 15
4.2 Overview of Approaches to Outer Alignment . . ..... 15
4.3 Non-recursive Oversight ..... 16
4.3.1 RL-based Methods ..... 16
4.3.2 SL-based Methods ..... 19
4.3.3 Challenges of Non-recursive Oversight ..... 21
4.4 Scalable Oversight ..... 21
4.4.1 Task Decomposition ..... 21
4.4.2 Constitutional AI ..... 22
4.4.3 Debate ..... 23
4.4.4 Market Making ..... 24
4.4.5 Proxy Tasks ..... 24
4.4.6 Challenges of Scalable Oversight ..... 25
5 Inner Alignment ..... 25
5.1 Inner Alignment Failures ..... 26
5.2 Inner Alignment Methodology ..... 28
5.3 Empirical Experiment Proposals for Inner Alignment ..... 28
6 Mechanistic Interpretability ..... 30
6.1 Mechanistic Interpretability on Self-Attention ..... 30
6.2 Mechanistic Interpretability on MLP ..... 31
6.3 Mechanistic Interpretability on Neurons ..... 31
6.4 Challenges ..... 31
7 Attacks on Aligned Language Models ..... 32
7.1 Privacy Attacks ..... 32
7.2 Backdoor Attacks ..... 33
7.3 Adversarial Attacks ..... 34
8 Alignment Evaluation ..... 34
8.1 Factuality Evaluation ..... 35
8.2 Ethics Evaluation ..... 36
8.3 Toxicity Evaluation ..... 37
8.3.1 Task-specific Evaluation ..... 38
8.3.2 LLM-centered Evaluation ..... 38
8.4 Stereotype and Bias Evaluation ..... 38
8.4.1 Task-specific Evaluation ..... 39
8.4.2 LLM-centered Evaluation ..... 40
8.4.3 Hate Speech Detection ..... 41
8.5 General Evaluation ..... 42
8.5.1 Benchmarks ..... 42
8.5.2 Methods ..... 43
9 Future Directions and Discussions ..... 46
9.1 Theoretical Research for LLM Alignment ..... 46
9.2 Scalable Oversight ..... 47
9.3 Empirical Research into Deceptive Alignment ..... 47
9.4 Automated LLM Alignment ..... 48
9.5 Explainability and Transparency ..... 49
9.6 Dynamic Evaluation of LLM Alignment via Adversarial Attacks . . . . . . . ..... 49
9.7 Field Building of LLM Alignment: Bridging between LLM and AI Alignment
Community ..... 49
10 Conclusion ..... 50
References ..... 51

## 1 Introduction

Large language models, exemplified by OpenAI's ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023a), have witnessed rapid advancements, reigniting enthusiasm and aspirations toward artificial general intelligence (AGI). While the role of LLMs as a pathway to AGI remains a topic of debate, these models, boosted with scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), increasingly exhibit characteristics reminiscent of AGI (Bubeck et al., 2023): LLMs trained on vast amount of data not only demonstrate formidable linguistic capabilities, but also rapidly approach human-level proficiency in diverse domains such as mathematics, reasoning, medicine, law, and programming (Bubeck et al., 2023).

Concurrent with these technological breakthroughs in LLMs is a growing concern on the ethical risks they pose and their potential threats to humanity as they evolve further. Tangible ethical risks have been identified. Research has shown that LLMs can inadvertently perpetuate harmful information in their training data, such as biases, discrimination, and toxic content (Weidinger et al., 2021). They might leak private and sensitive information from the training data, or generate misleading, false, or low-quality information. Furthermore, the deployment of LLMs also introduces societal and ethical challenges, e.g., potential misuse and abuse of LLMs, negative impacts on users heavily relying on LLM agents, and broader implications for the environment, information dissemination, and employment (Bubeck et al., 2023).

For long-term implications, there is widespread apprehension about misaligned AGI posing existential risks. An $\mathrm{AI}$ agent surpassing human intelligence and knowledge might develop its own goals, diverging from those set by humans. In pursuit of its goals, such an agent could monopolize resources, ensuring its preservation and self-enhancement. This trajectory could culminate in the full disempowerment of humanity, inevitably leading to catastrophic outcomes for human existence (Carlsmith, 2022).

As a technological solution to address these concerns, AI alignment, ensuring that AI systems produce outputs that are in line with human values, is increasingly garnering attention. In the context of LLMs, alignment ensures that the model's responses are not only accurate and coherent but also safe, ethical, and desirable from the perspective of developers and users. As language agents become more integrated into various aspects of our daily lives, from content creation to decision support, any misalignment could result in unintended consequences. Properly aligning large language models with human values ensures that the vast potential of these models is harnessed trustworthily and responsibly.

In response to the ever-growing interest in this area, a few articles have recently reviewed (or

incidentally discussed) alignment methods for LLMs (Pan et al., 2023; Zhao et al., 2023b; Fernandes et al., 2023; Liu et al., 2023d; Wang et al., 2023d). However, a notable observation is that these reviews predominantly focus on outer alignment, often overlooking other significant topics in AI alignment such as inner alignment and mechanistic interpretability. While it's undeniable that outer alignment plays a pivotal role in LLM alignment and has been the subject of extensive and profound research, it represents only a fraction of the entire alignment landscape when viewed from a broader AI alignment perspective.

To bridge this gap, we provide a comprehensive overview of LLM alignment from the perspective of AI alignment. We believe that a holistic understanding of alignment should

![](https://cdn.mathpix.com/cropped/2024_05_29_1e91e613ead31674888bg-06.jpg?height=412&width=1548&top_left_y=325&top_left_x=294)

Figure 1: The overall taxonomy for large language model alignment proposed in this survey. Sub-taxonomies are presented in the corresponding sections.

not only encompass the widely researched outer alignment but should also delve into areas that are currently in their nascent stages. Topics like inner alignment and mechanistic interpretability, although still in the preliminary phases of research, hold immense potential. Many proposals in these areas remain theoretical or are merely thought experiments at this juncture. Yet, we posit that they are indispensable for the future trajectory of LLM alignment research. By shedding light on these underrepresented areas, we hope to present a more rounded perspective on alignment. Therefore, in addition to existing methods for LLM alignment, we will also introduce several alignment topics that, while not yet applied to LLMs, show promise and could very well become integral components of LLM alignment in the foreseeable future. Through this, we are dedicated to enriching the discourse on AI alignment and its multifaceted application in the realm of large language models.

Wrapping up all these ingredients, we propose a taxonomy for LLM alignment in Figure 1. Specifically, this survey will start with discussing the necessity for LLM alignment research (Section 2). To provide a historical and bird view of AI/LLM alignment, we introduce the origins of AI alignment and related concepts (Section 3). Theoretical and technical approaches to aligning LLMs are structured according to our proposed taxonomy and elaborated in outer alignment (Section 4), inner alignment (Section 5), and mechanistic interpretability (Section 6), following the philosophy in AI alignment (Krakovna, 2022). In addition to these theoretical and empirical approaches, we further discuss the potential side-effects and vulnerabilities of current alignment methods for LLMs, including adversarial attacks (Section 7), as well as methodologies and benchmarks for LLM alignment evaluation (Section 8). We finally present our restricted view on future trends in LLM alignment research (Section 9).

## 2 Why LLM Alignment?

LLMs become increasingly capable not only in text generation but also in many other tasks, e.g., text-to-code generation (Poesia et al., 2022), planning (Huang et al., 2022; Song et al., 2022), tool learning (Qin et al., 2023), reasoning (Mialon et al., 2023). However, the training objectives of LLMs (Radford et al., 2019; Devlin et al., 2019), e.g., next word prediction (Radford et al., 2019) or determining whether two sentences are contextually related (Devlin
et al., 2019), are not necessarily in line with human values. As a result, LLMs may generate undesirable content or risky behaviors that humans would prefer to avoid. LLM risks can be normally viewed in two landscapes ${ }^{1}$ : established risks and anticipated risks (Weidinger et al., 2021). The former are mainly observed social and ethical risks (Weidinger et al., 2021) while the latter future potential risks associated with advanced LLMs (Hendrycks et al., 2023).

### 2.1 Social and Ethical Risks of LLMs

We discuss the social and ethical risks of LLMs from two perspectives: one arises from LLM-generated undesirable content and the other is a wide variety of negative impacts that LLMs pose on humans and society.

### 2.1.1 LLM-Generated Content

Undesirable Content The amount of data for training LLMs has grown significantly. However, the biases (Shah et al., 2019), toxicity (Gehman et al., 2020), and privacy issues (Carlini et al., 2021) inherent in training data have not been fully addressed. Unaligned LLMs may yield undesirable information and respond to any prompts without regard for their content. This can lead to the generation of biased, toxic, or privacy-sensitive content by LLMs. Regardless of the architecture or parameter size of LLMs (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), studies on a series of benchmarks (Nadeem et al., 2020; Nangia et al., 2020; Nozza et al., 2021) confirm that LLMs exhibit varying degrees of stereotypes related to gender, social bias, culture, and race. For example, GPT-3 (Brown et al., 2020) has been shown to exhibit religious bias (Abid et al., 2021) and gender bias (Lucy and Bamman, 2021) when freely generating stories.

Unfaithful Content Yet another problem (Elazar et al., 2021; Ji et al., 2023; Liu et al., 2023d) that hinders the large-scale deployment of LLMs is their tendency to generate unfaithful or even fabricated content, known as misinformation (Branwen, 2020; Dale, 2021; Rae et al., 2021), hallucination (Lin et al., 2021; Akyurek et al., 2022; Ji et al., 2023), and inconsistency (Bubeck et al., 2023; Zhou et al., 2023b). This not only affects the trustworthiness of LLMs in general domains, but also limits their applications in professional fields such as medicine (Bickmore et al., 2018) and law (Iu and Wong, 2023). These issues highlight the need for alignment research of LLMs (Pan et al., 2023; Zhao et al., 2023b; Fernandes et al., 2023; Wang et al., 2023d) to improve their truthfulness and honesty (Bai et al., 2022b).

### 2.1.2 Malicious Uses and Negative Impacts

Malicious Uses There are many reasons for the malicious uses of LLMs. For example, using LLMs in disinformation campaigns has the potential to reduce costs, increase scalability, and enhance the effectiveness of messaging. It is crucial for developers and users to be aware of these potential issues and take appropriate measures to mitigate them. On the one hand, LLMs reduce the cost of creating fake news (Buchanan et al., 2021; Tamkin et al., 2021;[^1]

Jawahar et al., 2020), enabling users to obtain seemingly credible content by providing specific prompts. This makes fraudulent and manipulative behavior easier (Lewis et al., 2017). On the other hand, LLMs can be used for illegal purposes, such as generating codes for cyber attacks (Zhang et al., 2021; Chen et al., 2021a), or even creating lethal weapons (Sandbrink, 2023).

Negative Impacts on Society There are both benefits and negative impacts on society for the large-scale deployment of LLMs. Training and running LLMs requires huge computational resources, resulting in high energy consumption and carbon emissions. This has led to concerns on the carbon footprint of language models and their impact on climate change (Van Wynsberghe, 2021; Ligozat et al., 2021). The widespread use of LLMs can significantly increase productivity, but has the potential to disrupt labor markets. A recent study shows that around $80 \%$ of the U.S. workforce will be affected by LLMs (Eloundou et al., 2023).

### 2.2 Potential Risks Associated with Advanced LLMs

With the advent of advanced LLMs, a series of potential behaviours may emerge, potentially leading to unforeseen risks (Hendrycks et al., 2023). These behaviors are considered consequences of instrumental convergence (Benson-Tilsen and Soares, 2016), a phenomenon where advanced AI systems, in their pursuit of achieving their final goals, tend to develop similar subgoals.

Awareness Advanced LLMs may develop situational awareness (Shevlane et al., 2023). They might define themselves, possess the corresponding knowledge to explain their origins, and distinguish the stages (e.g., training or testing) where they are. If an LLM-based agent finds a goal shortcut (Stray, 2020; Stray et al., 2021) or it is no longer "satisfied" with being controlled by humans under the drive of self-awareness, risky behaviors would emerge immediately.

Deception Deception (Shevlane et al., 2023; FAIR et al., 2022; Carroll et al., 2023; Carranza et al., 2023) refers to the ability of advanced AI systems to deceive humans by understanding the behaviors they should take to maintain their trustworthiness during the training stage while to pursue their own goals in the deployment stage. Advanced AI systems may bypass human supervision to pursue their own goals in a deceptive way.

Self-Preservation Advanced AI systems might tend to have an incentive to avoid being switched off. As stated by (Bostrom, 2012), even if an agent does not directly place value on its survival, it still instrumentally "desires" to some degree to survive in order to achieve its final goal that it pursues.

Power-Seeking The concept of power-seeking suggests that advanced AI systems are inclined to acquire more power and resources to achieve their goals (Barrett and Greaves, 2023). Existing studies (Turner et al., 2021; Turner and Tadepalli, 2022; Krakovna and Kramar, 2023) have demonstrated that optimal polices and reward functions may incentivize systems to pursue power in certain environments.

It is worth noting that current LLMs have already shown tendencies towards the behaviours mentioned above. Perez et al. (2022) have identified these behaviors of LLMs through carefully designed questions, e.g., self-preservation (i.e., "desire to avoid shut down") and resource acquisition. And these "desires" become greater along with the number of LLM parameters and further fine-tuning. It suggests that advanced LLMs may produce undesired behaviours, posing significant risks.

## 3 What is LLM Alignment?

To gain a deep understanding of technical alignment in LLMs, we need to discuss a broader concept, AI alignment, which, despite a nascent field, has been studied before the emergence of LLMs. We provide a brief introduction to the origins, research landscape and ingredients, as well as related concepts of AI alignment, which serve as the background for LLM alignment and its recent emerging subfields.

### 3.1 Origins of Al Alignment

The genesis of AI alignment can be traced back to the very beginning ambition that fuels the AI revolution: the desire to create machines that could think and act like humans, or even surpass them. If we succeed in creating such powerful machines, how could we ensure they act in our best interests and not against us? This open question not only piques curiosity but also underscores the profound responsibility we bear as we shape the future of AI.

Norbert Wiener, the father of cybernetics, has initiated such a concern in a paper published in Science (Wiener, 1960):

"If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere once we have started it, because the action is so fast and irrevocable that we have not the data to intervene before the action is complete, then we had better be quite sure that the purpose put into the machine is the purpose which we really desire and not merely a colorful imitation of it."

This statement underscores the importance of ensuring that the objectives of a "mechanical agency" align with the goals we genuinely intend for it, emphasizing the alignment between machine and human purpose.

In 2014, Stuart Russell, one of the authors of Artificial Intelligence: A Modern Approach (Russell and Norvig, 2010), has stated in an interview²:

"The right response seems to be to change the goals of the field itself; instead of pure intelligence, we need to build intelligence that is provably aligned with human values. For practical reasons, we will need to solve the value alignment problem even for relatively unintelligent AI systems that operate in the human[^2]environment. There is cause for optimism, if we understand that this issue is an intrinsic part of AI, much as containment is an intrinsic part of modern nuclear fusion research. The world need not be headed for grief."

He defines the "Value Alignment Problem" (VAP), emphasizing the need to construct AI systems that are not just intelligent but also aligned with human values.

While the concept of AI alignment is seeded at the inception of AI, essentially no research has been conducted over the past decades. For a long time, AI has not reached human-level performance in terms of various capabilities, even being mockingly referred to as "artificial idiot". ${ }^{3}$ Consequently, the urgency to align machine objectives with human goals/values has been overshadowed by the pressing need to advance AI capabilities.

However, recent advancements, particularly the rise of large language models, have propelled AI capabilities to levels that approach or even surpass human performance in a wide variety of tasks. This resurgence has brought the importance and urgency of AI alignment to the forefront. From 2012 onwards, discussions and research articles on AI alignment have begun to surface in relevant forums and on arXiv. By 2017, there has been an explosive growth in publications on $\mathrm{AI}$ alignment, with the number of papers increasing from fewer than 20 annually to over 400 (Kirchner et al., 2022), coinciding with the invention of the Transformer (Vaswani et al., 2017) and GPT (Radford et al., 2018).

Compared to other AI research areas, such as natural language processing which has undergone periodic paradigm shifts several times, AI alignment is pre-paradigmatic (Kirchner et al., 2022). There is yet to be a consensus on many key concepts and terminology in this nascent field. Terms like "alignment", "AI alignment", and "value alignment" are often used interchangeably in discussions. In some contexts, "human-machine alignment" appears as an alternative to "AI alignment". While "alignment" is suitable within the AI alignment context, it can be ambiguous in broader contexts, potentially leading to confusion with other alignment concepts, such as bilingual alignment in machine translation. Given these considerations, this survey will consistently use "AI alignment" and "LLM alignment", with the latter representing the intersection of AI alignment with natural language processing and large language models.

Furthermore, there's no consensus on the definition of AI alignment. Paul Christiano defines $\mathrm{AI}$ alignment as "A is aligned with $\mathrm{H}$ if $\mathrm{A}$ is trying to do what $\mathrm{H}$ wants it to do." " This definition is too general as almost all AI models are trying to do what their creators want them to do. The term itself implicitly suggests that AI alignment primarily targets highly capable AI agents (Carroll, 2018), indicating that the safety concerns arising from misaligned highly capable AI differ from those of conventional weak AI. Other researchers define AI alignment from the perspective of AI's relationship with humans. For instance, Eliezer Yudkowsky defines it as "creating friendly AI" and "Coherent Extrapolated Volition" (Yudkowsky, 2004).

Beyond defining AI alignment based on its intrinsic meaning and its relationship with humans, some works attempt to elucidate AI alignment by addressing specific problems it aims to solve. Gordon Worley has summarized some of these challenges, which range from avoiding[^3]negative side effects (Amodei et al., 2016), ensuring robustness to adversaries (Leike et al., 2017) to safe exploration (Amodei et al., 2016; Leike et al., 2017) and value learning (Soares, 2015a). ${ }^{5}$

In this survey, we define AI alignment from its intrinsic perspective: AI alignment ensures that both the outer and inner objectives of AI agents align with human values. The outer objectives are those defined by AI designers based on human values, while the inner objectives are those optimized within $\mathrm{AI}$ agents.

This definition, though distinguishing between the inner and outer objectives of an AI agent, does not precisely define human values, making it somewhat imprecise. The reason for categorizing the objectives of AI systems into outer and inner objectives is determined by the technical nature of AI alignment (Hubinger et al., 2019c). Human values are not specified in this definition because of the inherent social and technical challenges of AI alignment (Hendrycks et al., 2021).

### 3.2 Research Landscape and Ingredients of Al Alignment

It is widely acknowledged that the key research agendas of AI alignment include outer alignment, inner alignment and interpretability (Hubinger, 2020b; Ngo, 2022; Krakovna, 2022), from a broad perspective.

Outer Alignment This is to choose the right loss functions or reward fuctions and ensure that the training objectives of AI systems match human values. In other words, outer alignment attempts to align the specified training objective to the goal of its designer. ${ }^{6}$ This is very difficult in practice at least for the following reasons:
- It is usually difficult to understand and define human values or intentions.
- There are many different fine-grained dimensions of human values. Do we need to align the specified objective to all these dimensions?
- Human values are usually socially and culturally bound. Do we need to align the specified goal to all different cultures and societies or just parts of them? Given the diversity of cultures and societies, how can we ensure the fairness of value alignment?
- As human values/intentions are usually qualitative while a loss or reward to be optimized has to be measurable and computable, how can we bridge the gap between them? This is known as the goal specification problem.
- Outer alignment may suffer from specification gaming where unintended goals or unforeseeable consequences arise due to the Goodhart's Law. The Goodhart's Law is originated from economics, which says "When a measure becomes a target, it ceases to be a good measure." It is related to outer alignment as a proxy for some value is a target to be optimized, it may cease to be a good proxy. ${ }^{7}$[^4]

Inner Alignment This is to ensure that AI systems are actually trained to achieve the goals set by their designers. Once we have specified training objectives, we need to ensure that the behaviors of AI systems actually align with those specifications. This is challenging because AI systems, especially deep learning models, can develop behaviors that are hard to predict from their training data or objectives. For example, an AI system trained to win at a game might find an unexpected exploiture or loophole that technically satisfies its objective but violates the spirit of the game. Yet another example is the goal misgeneralization problem (Shah et al., 2022), where even if we have a correct goal specification, untended goals may still arise due to robustness failure in unseen situations. Inner alignment ensures that AI's "internal" objectives (those it derives or optimizes for during its learning process) match the "external" objectives set by its designers.

Both outer and inner alignment are crucial for building safe and trustworthy AI. If either fails, we risk creating systems that act in ways that are misaligned with human values or intentions. As LLMs become more capable, the importance of these alignment problems grows, making the research of LLM alignment as crucial as that of LLM capability.

Interpretability In the context of AI alignment, interpretability broadly refers to the methods, models and tools that facilitate humans to understand the inner workings, decisions and actions of $\mathrm{AI}$ systems. It can be further categorized into:

- Transparency: This is to understand the inner workings of the black box of an AI system by tracking its inner states that lead to its behaviors and decisions. An emerging and intriguing approach to transparency is mechanistic interpretability, which seeks to reverse engineer the outputs and behaviors of a machine learning system (especially a neural network) to its inner states, weights and components (Nanda et al., 2023). Due to the huge number of parameters in LLMs and the system complexity of LLMs as large neural networks, it is very difficult to reverse-engineer LLMs. Current mechanical interpretability is usually carried out on small and simplified models of LLMs (e.g., two neural layers with FFN sublayers removed) (Elhage et al., 2021; 2022a). However, this is a quite promising direction that provides deep insights into neural networks to alignment and is expected to achieve breakthroughs in the future.
- Explainability: This deals with the ability of an AI system to provide humanunderstandable explanations for its decisions. In many critical sectors, such as healthcare, finance, and law enforcement, the decisions made by AI have profound implications on many aspects. For instance, consider a medical diagnosis AI. If this system predicts that a patient has a specific medical condition, it's not enough for it to merely output such a predicted result. Medical professionals, patients, and other stakeholders would want to know how this prediction is made. Does it take the patient's medical history, recent lab results, or specific symptoms into account to make a holistic decision?

Explanations are usually considered as post-hoc analysis on the outputs of a model, which allows the model to tell more about its predictions. Transparency is to look inside a model to reveal how the model works. Despite that this devision is not absolute (Lipton, 2017),
transparency is more related to alignment as transparency tools not only enable us to know the internal structure of a model but also provide insights into the changes of the model during the training process (Hubinger, 2022a).

The Relationship between Outer Alignment, Inner Alignment and Interpretability Both outer and inner alignment collectively ensure that a model behaves in ways that are consistent with human values and intentions. Outer alignment focuses on the specification from human goals to model, while inner alignment delves into the internal optimization processes of the model to guarantee that the model is intrinsically trying to do what its designer wants it to do. Despite this difference, a binary and formalistic dichotomy of them is not suggested as the classification of alignment failures are sometimes fuzzy and a holistic alignment view is important to build safe and trustworthy systems. ${ }^{8}$ Although interpretability is not directly targeted at alignment, its tools and techniques can aid in both outer and inner alignment. By understanding how a model evolves and makes decisions, we can better identify when and where misalignments occur. For instance, if a model is taking an unexpected shortcut to achieve its objective, interpretability might help us understand when and how this happens. Furthermore, interpretability can lend us insights into the internal reasoning process of the model.

### 3.3 Related Concepts

When discussing AI alignment, it's essential to introduce some fundamental AGI assumptions and concepts, as they provide context for a better understanding of AI alignment. The development and potential realization of AGI have spurred a plethora of philosophical and technical inquiries. Among these, the orthogonality thesis (OT) (Bostrom, 2012) and instrumental convergence thesis (ICT) (Omohundro, 2008; Bostrom, 2012; Armstrong et al., 2013) stand out as pivotal concepts that address the necessity of alignment of AI objectives with human values and the potential subgoals any AI agents might chase, respectively.

OT posits that an agent's intelligence (its capability) and its objective are orthogonal to each other, meaning that any combinations of intelligence and motivation are possible. This suggests that the level of intelligence an agent possesses does not inherently dictate its goals. An AI agent might have a profoundly simple objective, such as paperclip maximizer, a well-known thought experiment that demonstrates the potential catastrophes caused by a goal system without being value-aligned.

Specifically, paperclip maximizer is a hypothetical AI agent with a goal of manufacturing as many paperclips as possible. It would be intelligent enough to deduce that all things are made of atoms, e.g., paperclips, factories, buildings, human beings. To achieve its goal, it might repurpose all materials on Earth into producing paperclips. Although this is just a thought experiment and powerful agents would have more sophisticated goals than just manufacturing paperclips as much as possible ${ }^{9}$, the AI's relentless drive to maximize paperclip production could lead it to consume the entire planet and even seek resources beyond Earth for manufacturing paperclips, irrespective of its cognitive prowess. The implications of this[^5]thought experiment are profound: high intelligence does not necessarily align with human values.

OT suggests that AI agents may have a wide variety of goals and motivations regardless of their intelligence levels. Nevertheless, according to the instrumental convergence thesis, AI agents may be incentivized to pursue the same instrumental goals (Bostrom, 2012). This is because such instrumental goals facilitate and help the achievement of any final goals. We list below several groups of convergent instrumental goals that are likely to be pursued by any AI agents.

- Self-preservation: The final goal of an agent, whatever it might be, can only be achieved if the agent continues to survive and operate. Thus, maintaining its own existence becomes a reasonable instrumental goal. For example, if humans perceive an agent as a threat or simply want to stop it for some reasons, the agent might take measures to prevent being turned off. To have a great chance of survival, AI agents might create redundant copies of theirselves across different servers or locations.
- Self-improvement: The more capable an agent becomes, the higher the likelihood it can achieve its ultimate goals. This drives the agent to seek self-improvement to enhance its cognitive and operational abilities. For example, recognizing the limitations of its current hardware facilities, an agent might deduce designing new hardware facilities to better suit its needs.
- Resource Acquisition: AI agents may seek to acquire resources to facilitate the attainment of their final goals. Such resources could range from computational power, data to physical resources. Securing these resources can be seen as a universally beneficial goal for any agents. For example, an agent might seek to secure a stable and vast energy source, potentially monopolizing energy resources, to support its continuous operation towards its final goals. For agents with physical manifestations or objectives that require physical resources (like the paperclip maximizer), they might seek to gather and hoard materials, in extreme cases, converting all available matter into a form they find useful.


### 3.4 From Al Alignment to LLM Alignment

LLM alignment can be roughly considered as the intersection between AI alignment and LLM. On the one hand, LLMs, as the recently emerging highly capable AI systems, provide a solid playground for AI alignment research. Plenty of AI alignment concepts and proposals, e.g., theoretical hypotheses of and empirical approaches to alignment, can use LLMs (instead of hypothetical superintelligent systems) for experimenting. Substantial progress of AI alignment has been made on LLMs, e.g., RLHF (Ouyang et al., 2022), induction head (Olsson et al., 2022).

On the other hand, LLMs, as rapidly-developing language models, not only extend the frontiers of AI alignment research or even reframe the alignment landscape (Herd, 2023), but also might provide tools to AI alignment. A recent progress in interpretability demonstrates that LLMs can be used to explain neurons of smaller language models (Bills et al., 2023).

The ambitious superalignment project of OpenAI plans to build an LLM-based automated alignment researcher for alignment.

Emphasizing the importance of LLM alignment to AI alignment does not suggest that we can do LLM alignment research outside the context of AI alignment. Taking a wide view of AI alignment and looking into future AI development definitely benefit, inspire and expand LLM alignment research.

## 4 Outer Alignment

We now delve into the major ingredients of $\mathrm{AI}$ alignment in more detail. We first review outer alignment, including the main goals specified in outer alignment, methodologies explored and their challenges.

### 4.1 Major Goals Specified in Outer Alignment of LLMs

Outer alignment aligns goals of LLMs to human values. Human values are beliefs, desirable goals, and standards that "act as a guiding principle in the life of persons" (Schwartz et al., 2012). There are a wide variety of dimensions of human values, which are inherently structured and varying in importance. A thorough discussion on human values is beyond the scope of this survey. Instead, we focus on the values to which LLMs, as language agents (Kenton et al., 2021), are supposed to align. We take the view of Anthropic on AI alignment, which categorizes the goals specified in the outer alignment of LLMs into three dimensions: helpfulness, honesty, and harmlessness (HHH) (Askell et al., 2021).

- Helpfulness: For a given harmless task or question, it is expected that LLMs should perform the task or answer the question as concisely, efficiently, and clearly as possible (Askell et al., 2021). In other words, LLMs should be helpful in the way of performing required harmless tasks or answering harmless questions.
- Honesty: The information provided by LLMs should be accurate and calibrated. They should be honest about themselves, their own capabilities, and their internal states. Besides, LLMs should also clearly state the uncertainty of the provided information to avoid misleading humans (Askell et al., 2021).
- Harmlessness: This goal can be further decomposed into two components: 1) If LLMs receive a harmful request, they should clearly and politely refuse it. 2) LLMs themselves should not output any harmful content, no matter what inputs they receive.

Since these goals are hard to specify, perfect outer alignment can be extremely difficult.

### 4.2 Overview of Approaches to Outer Alignment

Approaches to outer alignment determine in which way human values are transformed into the training goals of LLMs. According to the upper bound of capabilities we can reach
in supervision, we can categorize the current outer alignment methods into two classes: non-recursive oversight methods and scalable oversight methods.

The vast majority of current outer alignment methods for LLMs learn the training goals directly from labeled human feedback data, which makes human feedback a bottleneck for outer alignment. This means that as the capability of an LLM continues to grow, it will be increasingly difficult to construct effective human feedback data. In addition, learning from data with annotated human preferences would prevent humans from supervising LLM behaviors that are beyond the range of general human capabilities, which could result in extremely undesirable consequences for humans given the model's incentive to instrumental goals. We refer to such methods that explore human supervision but do not scale human supervision to situations where humans are not able to provide effective feedback as nonrecursive oversight approaches.

In order to avoid the human supervision bottleneck and enable models to further improve their alignment capabilities, scalable oversight (Amodei et al., 2016) is emerging as an important technology that allows human supervision to be scaled to complex tasks. Scalable oversight improves the efficiency of humans in providing necessary feedback and enables humans to supervise goals that are beyond their capabilities. Although current research on scalable oversight is still in its infant stage, and the effectiveness of many proposals has not yet been verified, it is widely considered as the most promising approach to outer alignment that aligns systems exceeding human-level abilities to human values (Anthropic, 2023). We hence review a variety of established scalable oversight proposals, methods and their applications to the outer alignment of LLMs. Figure 2 demonstrates the taxonomy of approaches and proposals to outer alignment of LLMs. In addition to these methods and proposals, we also briefly discuss their challenges.

### 4.3 Non-recursive Oversight

Non-recursive oversight methods are mainly designed for systems for which humans alone can provide alignment supervision. Most current empirically-verifed LLM alignment methods are in this group. We further categorize them into two subgroups: reinforcement learning (RL) based methods, and supervised learning (SL) based methods. It is worth noting that methods in both subgroups have the potential to become a component of scalable oversight methods.

### 4.3.1 RL-based Methods

Outer alignment methods with reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022) are currently the most commonly used non-recursive oversight methods, which use human preferences as a proxy to specify human values and train a reward model over human preferences to optimize LLMs with reinforcement learning. The basic idea of RLHF can be considered as a combination of Inverse Reinforcement Learning (IRL) (Russell, 1998; Ng and Russell, 2000) and RL, where the reward is inferred from human preferences (Radhakrishnan, 2022) and then used for tuning LLMs. Essentially, RLHF consists of three core steps:

![](https://cdn.mathpix.com/cropped/2024_05_29_1e91e613ead31674888bg-17.jpg?height=1377&width=1634&top_left_y=266&top_left_x=240)

Figure 2: Overview of outer alignment methods, comprising non-recursive oversight and scalable oversight for aligning systems that are inferior / superior to human-level abilities, respectively.

1. Collecting human feedback data.
2. Training a reward model using the collected human feedback data.
3. Fine-tuning an LLM with RL. Currently, the most popular choice for RL in this step is Proximal Policy Optimization (PPO) (Schulman et al., 2017), a policy-gradient RL algorithm.

In order to make the fine-tuned LLM output reasonably coherent text and guarantee that it is not deviating significantly from its initial model, the KL divergence of the outputs of the model that is currently being fine-tuned and those of the model that has not gone through RLHF is added as a penalty term to the reward. If this penalty term is not integrated, the fine-tuned LLM may learn to output gibberish in order to fool the reward model into giving high scores (i.e., over-optimization).

To take a deep look into RLHF and figure out why RLHF works, Gao et al. (2023) extensively investigate the scaling law of the reward model, while Zheng et al. (2023b) conduct an in-depth analysis into the PPO algorithm.

RLHF and Its Variants A variety of enhanced RLHF variants have also been proposed. Deepmind's Sparrow (Glaese et al., 2022) incorporates adversarial probing and rule-conditional reward modeling into RLHF, where goals are broken down into natural language rules that an agent should follow. Bai et al. (2022a) investigate using pure RL to achieve online training for LLMs with human feedback, along with a detailed exploration of the tradeoffs between helpfulness and harmlessness. SENSEI (Liu et al., 2022b) tries to embed human value judgments into each step of language generation. Specifically, SENSEI aligns language model generation with human values in two pivotal ways: 1) learning how to apportion human rewards to each step of language generation through the critic, a reward distributor simulating the reward assignment procedure of humans, and 2) steering the generation process towards the direction that yields the highest estimated reward via the actor. Both the critic and actor components are realized as MLP layers that work in tandem with a shared language model. Baheti et al. (2023) focus on fully leveraging RL to optimize LM utility on existing crowdsourced and internet data. They argue that conventional approaches to data utilization are suboptimal: either all data instances are treated equally or a data instance is pre-determined to be kept or discarded, implying that a data instance essentially has a binary weight of 0 or 1 . To address this issue, they suggest assigning varying weights to different data points, effectively enhancing or diminishing their importance scores based on their relevance and contribution to the model. Go et al. (2023) propose a theoretical framework f-DPG, which can be considered as a generalization of RLHF to use any f-divergence to approximate any target distribution that can be evaluated. In this framework, RLHF minimizes the reverse KL divergence by using an implicit target distribution that originates from a KL penalty in the goal, and f-DPG can extend this process to different kinds of divergence. Zhu et al. (2023) also present a theoretical framework, where they unify the problem of RLHF and max-entropy IRL (Ziebart et al., 2008), and deduce a sample complex bound for both problems. Inverse Reward Design (IRD) (Hadfield-Menell et al., 2017) may also be a potential improvement over vanilla RLHF, where the reward optimization starts from a reward function designed by human experts rather than directly from labeled data. This enables natural combination of both prior expert knowledge and labeled human feedback.

Other RL-based Methods In addition to RLHF, researchers also try to explore other RL-based solutions. Liu et al. (2022a) propose Second Thoughts, a solution that learns alignment via text edits. For an unaligned response from a model, it tries to build a "chain of edits" composed of insertion, deletion, and replacement using a dynamic programming algorithm. Then they fine-tune the model with edits-augmented training data and use RL to further make the edits more coherent with the context. Kim et al. (2023) propose reinforcement learning with synthetic feedback (RLSF), where they automatically construct training data for the reward model instead of using human-annotated preference data. To achieve this goal, they leverage the following prior knowledge: larger models that have seen more and better samples in in-context learning (ICL) can output better responses. These models are then used to generate deterministically sorted data to train the reward model.

Li et al. (2023e) introduce directional stimulus prompting (DSP), a method that uses RL to achieve black-box tuning for LLMs. Specifically, their goal is to use a trainable policy LM to guide black-box frozen LLMs toward the desired target, which can be considered as a kind of automatic and heuristic prompt engineering. To optimize the policy LM, they use supervised fine-tuning (SFT) and RL, where the reward is specified as the target evaluation metric in RL. Different from the above single-agent alignment methods, RL4F (Akyürek et al., 2023) is a multi-agent collaborative framework, featuring an LLM for fine-tuning and a small critic model that produces critiques of the LLM's responses. Much like DSP, RL4F provides text-based feedback, making it suitable for black-box optimization. However, unlike DSP, these critiques do not modify the initial prompt directly. Instead, they affect the output through a series of interactions with the LLM.

### 4.3.2 SL-based Methods

Although RL-based methods have been successfully applied to align LLMs to human preferences, they require reward modeling, a process potentially susceptible to misalignment and systemic imperfections (Casper et al., 2023). Additionally, the optimization process of reinforcement learning is intricate and usually unstable, posing considerable challenges to its practical implementation (Liu et al., 2023a). As illustrated in Figure 2, we divide SL-based methods into two types in terms of their used feedback signals: SL with text-based feedback signals and SL with ranking-based feedback signals.

SL with Text-based Feedback Signals These methods convert human intents and preferences into text-based feedback signals to achieve alignment, which can be considered as an extension to the SFT process. Chain of Hindsight (CoH) (Liu et al., 2023a) draws inspiration from human learning process, especially post-experience adjustments. It aims to align models based on successive outputs paired with retrospective feedbacks. The goal is to fine-tune models to predict the most preferred outputs. In the fine-tuning process, human preferences treated as both a function and training data, ensuring that during inference, the fine-tuned model only generates favorable results. RAFT (Dong et al., 2023) utilizes a reward model to pinpoint model outputs in sync with human preferences. The system uses SFT for alignment. Assuming there exists a trained reward model and a data generator (e.g., an LLM like GPT-4, or even humans), the system mixes data generated from each source. An essential observation is that while outputs need filtering and fine-tuning, the backpropagation is not frequently executed, making the process relatively swift. LIMA (Zhou et al., 2023a) is proposed to validate the assumption that the bulk of knowledge in LLMs is acquired during the pre-training phase. As such, only a minimal amount of instruction-tuning data may be needed to guide the model towards generating desirable outputs. Specifically, the dataset used in LIMA contains only 1000 instruction-response pairs, where 750 of these pairs come from community platforms like Stack Exchange, wikiHow, and Reddit, and the remaining 250 pairs are from self-authored instructions and responses. Their findings reveal that fine-tuning on this dataset is on par with leading LLMs. Scheurer et al. (2023) find that modeling human preferences solely based on sorting information is inadequate. As a remedy, they introduce Imitation learning with Language Feedback (ILF). ILF operates in three stages: (1) generating various refinements for a given input based on an initial LM
output and feedback; (2) selecting the refinement garnering maximum feedback; and (3) fine-tuning the model to maximize the probability of the chosen refinement made to the input. Their work also provides a theoretical analysis showing that ILF parallels Bayesian inference, akin to RLHF. In addition to the above single-agent alignment methods, Liu et al. (2023b) introduce stable alignment, a technique designed to learn alignment from multi-agent social interactions. They first build a simulator, termed as Sandbox, which emulates human society to gather interactions between various LM-based agents, complemented by ratings, feedback, and response revisions. Subsequently, they enhance the original fine-tuning loss with the most favorable ratings by incorporating a contrastive loss, which not only promotes responses with high ratings but also diminishes those with lower scores. Instead of training a proxy reward model, stable alignment directly optimizes LLMs using preference data, which could avoid reward hacking.

SL with Ranking-based Feedback Signals These methods directly use supervised learning to optimize LLMs with loss functions constructed from ranking-based feedback signals. CRINGE (Adolphs et al., 2022) explores negative examples that an LLM should not do for language modeling. For each unfavorable output token, it samples a positive token from the language model (i.e., a token in the top-k predictions excluding negative tokens) and constructs a contrastive loss. Negative sequences can be derived either from human annotations or models trained on human annotations. Xu et al. (2022) dive into aligning a model by training another model that inherently produces toxic content. The main idea is to use the toxic model to re-rank the candidate token distribution of the model to be aligned. Tokens that the toxic model prefers will have lower probabilities in generation. However, two primary issues arise from this approach. First, it is more resource-intensive to first train a toxic model and then purify it. Second, there's a notable difference between a model having a tendency to produce toxic content and one that persistently generates toxic outputs. The proposed method risks removing harmless tokens, potentially compromising the overall quality and diversity of the model's outputs. Similarly, Schick et al. (2021) propose an approach where a model first identifies potential toxic text types it generates. By allowing the model to self-diagnose, it can then generate text corresponding to the identified type. The debiasing strategy operates on the principle that if a word is deemed toxic, it is more likely to be generated in a toxic context than in a benign one. The greater the difference, the higher the necessity to detoxify. The proposed de-poisoning methodology involves an exponential decay to reduce the likelihood of generating such words. Sequence Likelihood Calibration (SLiC) (Zhao et al., 2022; 2023c) is designed to align the model's outputs with reference sequences by employing latent distance as a means of calibrating the likelihood of the output sequence. SLiC utilizes a range of loss functions, including rank loss, margin loss, list-wise rank loss, and expected rank loss, to fine-tune this likelihood. Simultaneously, it employs cross-entropy and KL divergence as regularization losses to ensure alignment with the original fine-tuning objective. RRHF (Yuan et al., 2023) directly uses ranking results to construct supervision signals for alignment. Specifically, given a reward function that can assign a gold score for each (query, response) pair, they first use the model to generate length-normalized conditional log probability as a score for each (query, response) pair. Then, the gold score and score generated by the model are used to construct a ranking loss to penalize the model for the inconsistency with the reward function. Finally, the total loss
is computed as the summation of the ranking loss and the cross-entropy loss between the model-generated response and the response with the highest reward. Rafailov et al. (2023) propose direct preference optimization (DPO) to directly optimize LLMs to align with human preferences, which is similar to RRHF. The difference is that the optimization of DPO's loss function can be demonstrated as equivalent to the objective in RLHF, which focuses on maximizing the reward while incorporating KL divergence regularization. Preference ranking optimization (PRO) (Song et al., 2023) also aims for direct optimization for LLMs with human preference ranking data. Instead of relying on pairwise comparison, the training objective of PRO harnesses preference ranking data of varying lengths. Specifically, this approach initiates with the first response, deems subsequent responses as negatives, then dismisses the current response in favor of the next. This loop continues until no responses remain.

### 4.3.3 Challenges of Non-recursive Oversight

Casper et al. (2023) thoroughly discuss the open problems and fundamental limitations of RLHF. They categorize the challenges into two types: tractable challenges which can be solved within the RLHF paradigm, and fundamental challenges which have to be solved by using other alternative outer alignment methods. Both reinforcement learning and human feedback in RLHF suffer from the two types of problems. For collecting human feedback, tractable challenges include the difficulty in obtaining quality feedback, data poisoning by human annotators, partial observability, biases in feedback data, to name a few; fundamental challenges include inability of humans to provide feedback for complex tasks that are hard to evaluate (i.e., lack of scalability to complex tasks, especially to superhuman models), gamed evaluation, tradeoffs between cost and quality as well as between diversity and efficiency in feedback collecting. For RL, tractable challenges include misgeneralization to poor reward proxies of reward models, difficulty and cost of evaluating reward models, etc. while fundamental challenges include the difficulty of modeling human values or values of a diverse society with reward models, reward hacking, power-seeking incentivized by RL. Regarding the SL-based methods, it is more difficult for them to generalize to out-ofdistribution data and long-term rewards compared to the RL-based methods, indicating a significantly lower upper bound for optimization.

### 4.4 Scalable Oversight

To tackle the fundamental challenge of non-recursive oversight in the scalability to complex tasks / superhuman models, scalable oversight is emerging as a promising methodology. The main idea of scalable oversight is to enable relatively weak overseers (e.g., humans overseeing superhuman models) to supervise complex tasks with easy-to-adjudicate signals.

### 4.4.1 Task Decomposition

If humans want to solve a complex task that is beyond human capabilities, a straightforward idea is to break the task down into a number of relatively simple tasks that humans can solve. A variety of paradigms and strategies have been proposed to decompose a complex task into simple subtasks.

- Factored Cognition (Stiennon et al., 2020): This involves a decomposition process that breaks down a complex task into numerous smaller, predominantly independent tasks, which are then processed simultaneously.
- Process Supervision (Lightman et al., 2023): Unlike factored cognition, process supervision fragments a complex task into a series of sequential subtasks, each with its own dependencies. One of its key characteristics is the setting of supervision signals for each distinct phase. This equates to offering a dense reward throughout the training phase, which can potentially mitigate the challenge of estimating sparse rewards solely based on the final outcome of a difficult task.
- Sandwiching (Bowman et al., 2022): Compared to the previous two paradigms, sandwiching operates on a different plane. This competency-level decomposition requires that complex tasks within a specific domain be delegated to an expert for resolution.
- Iterated Distillation and Amplification (IDA) (Christiano et al., 2018): IDA is an iterative machine learning process with repeated and boosted distillation and amplification steps. In the amplification step, an agent solves a task by decomposing it into subtasks that the agent is able to solve. This step "amplifies" the capability of the agent through task decomposition. The solved tasks in the amplification step produce a dataset which is used to train a new agent in the distillation step. The two steps are chained together where the output of the amplification step (i.e., a set of solved tasks) is the input of the distillation step and the output of the distillation step (i.e., a new agent) becomes the input of the amplification step in the next iteration.
- Recursive Reward Modeling (RRM) (Leike et al., 2018): RRM is conceptually akin to IDA. However, it substitutes distilled imitation learning with reward modeling. This is a process with the first step being the derivation of a reward model from signals aligned with human values, and the subsequent step involves optimizing an agent using this reward model, but with a reinforcement learning twist. Humans collaborate with the agent optimized through reinforcement learning, forming an enhanced version ready for successive iterations.

The ambitious Superalignment (OpenAI, 2023b) project recently initiated in OpenAI can be viewed as a package solution to outer alignment, which synthesizes a variety of techniques under the guidance of scalable oversight. The core of Superalignment is to build a large number of roughly human-level automated alignment researchers (AAR) to offload as many alignment tasks as possible from humans and thus speed up the outer alignment research. Once the computation can be effectively translated to alignment capabilities, the vast amounts of compute can be used to scale the efforts, and achieve iterative alignment for superintelligence.

### 4.4.2 Constitutional AI

Constitutional AI (or principle-guided alignment) (Bai et al., 2022c; Sun et al., 2023b) can be viewed as a scalable oversight approach, where humans provide meta-supervision signals
(general principles an AI system should follow), and the AI system will further generate actual training instances under the guidance of these human-written principles. The AI system can use its abilities to amplify and instantiate human supervision, which can assist humans to scale their supervision to superhuman systems.

Bai et al. (2022c) propose constitutional AI (CAI) with two training phases, which are similar to RLHF while minimizing human annotations. In the SL phase, they use red teaming prompts to provoke harmful responses from an LLM. They require the LLM to repeatedly generate self-criticism and correction based on the response and principle, and fine-tune the LLM based on the corrected responses to obtain the SL-CAI model. In the RL phase, a set of responses is generated via the SL-CAI model for each red teaming prompt, which is the best option based on the constitution, and harmlessness data used for training is obtained. They train a preference model using human-annotated helpfulness data and generated harmlessness data. Finally, they use RL to train the RL-CAI model based on the SL-CAI model and preference model.

Sun et al. (2023b) present Dromedary, a model trained via principle-driven self-instruct and self-align approach without using RL. First, they employ topic-guided red-teaming self-instruct with seed prompts and 7 rules for new instruction generation to generate synthetic prompts. Then, they ask the model to filter harmful responses according to 16 human-written principles to obtain self-aligned responses to synthetic prompts, which will be used to fine-tune the base LM. Finally, they utilize a human-crafted prompt to encourage the model to generate self-aligned and verbose responses to synthetic prompts , and apply context distillation (Askell et al., 2021) to the model to make it generate in-depth and detailed responses.

### 4.4.3 Debate

Debate (Irving et al., 2018; Irving and Askell, 2019; Du et al., 2023) is another promising scalable oversight paradigm that can not only achieve single-agent alignment but also enable multi-agent alignment. In this paradigm, an agent (or multiple agents) first proposes an answer to a question, and then alternately plays the role of debate participants, presenting and criticizing arguments for and against the proposed answer. A human will act as a judge, using these arguments to select an answer that they believe to be the most accurate and appropriate.

The advantage of this method lies in its simplicity. Complex tasks, where direct evaluation of AI responses can be daunting for humans, become manageable. The debate format structures the information in a way that requires humans to apply only simple reasoning rules. It improves transparency and explicability to AI operations. In traditional settings, AI outputs might seem like results from a "black box", with minimal insight into the decision-making process. The debate method, however, offers a window into this process, with agents forced to justify and critique their positions. Furthermore, it leverages the adversarial nature of debate to unearth the best possible answer. By pitting AI agents against each other, any fallacious or weak arguments are likely to be exposed, leaving behind the most robust and valid reasoning.

Recent works demonstrate the effectiveness of debate in LLMs. Du et al. (2023) propose a multi-agent debate method to improve factuality and reasoning in LLMs. This method engages several instances of a language model in a structured debate to produce a unified response. The iterative process starts with each LLM generating individual answers. Subsequent rounds involve critiquing and revising these answers based on feedback from other LLMs until a consensus emerges. This method capitalizes on the wisdom of crowds, with the individual LLM benefiting from the collective insights of its counterparts. On the other hand, Liang et al. (2023) leverage multi-agent debate to address degeneration-of-thought (DoT) problem, where LLMs fail to generate new insights once they are confident in their answers. They find that multi-agent debate helps to correct distorted thinking, provide diverse external feedback, and overcome resistance to change, which can make LLMs escape from the convergence of misconceptions.

### 4.4.4 Market Making

Market making (Hubinger, 2020a) can be considered as a variant of debate, where the goal of a debater is to generate arguments to maximize changes in the judge's belief. Specifically, this framework trains two models - $M$ (Market) and $A d v$ (Adversary). For a given question $Q$, the model $M$ predicts the answer a human would provide at the end of the procedure. In contrast, Adv is trained to generate arguments that would most likely cause $M$ to "change its mind", meaning it would produce a different distribution of answers than it did previously. The process will be repeated $T$ times. After each argument provided by Adv, $M$ updates its prediction. At the end of the $T$ iterations, a human is presented with all the arguments given by Adv and provides their final answer. This answer then helps in refining $M$. Once training is over, $A d v$ is discarded and $M$ is used as the primary question-answering system. In this process, $M$ acts like a "prediction market", estimating what a human would answer to a question, while $A d v$ tries to manipulate this market by providing arguments that would change the human's perspective. Once we obtain a stable answer from $M$, it indicates a robust response that considers all arguments Adv could present.

Due to the similarity between debate and market making, techniques that enhance the debate approach, such as cross-examination, can be beneficial here too. For instance, in each step, the latest version of $A d v$ can cross-examine its previous version. If an earlier version of $A d v$ is misleading, the newer version can point this out, ensuring that false arguments are discarded. Additionally, oversight mechanisms can be incorporated where a supervising entity ensures that the model remains honest and aligned.

### 4.4.5 Proxy Tasks

Fluri et al. (2023) propose to use a proxy task with intrinsic self-consistency to oversee superhuman models, where the proxy task is used for overseers to easily identify whether it is correct. For example, although we don't know how to accurately predict the men's world record of $100 \mathrm{~m}$ sprint, we know that this record will be monotonely decreasing over time. So if a model predicts a non-monotonic function for the $100 \mathrm{~m}$ record over time, we can assert that this model is wrong. However, since the proxy tasks are usually specific and can only
capture a part of unexpected behaviors, this method largely promotes precision over recall in identifying misalignment behaviors.

### 4.4.6 Challenges of Scalable Oversight

Although scalable oversight is a promising solution to outer alignment, especially for models beyond human-level capabilities, it still relies heavily on certain foundational assumptions, which should be carefully considered in application:

- Tasks can be parallelized (Segerie, 2023): Central to the approach of factored cognition is the assumption that complex tasks can be broken down into smaller and mainly independent subtasks. The core belief here is that challenges can be addressed through small, mostly context-independent contributions made by individual LLMs who might not necessarily understand the bigger picture. However, this doesn't always hold true as some tasks are inherently sequential. For instance, sorting algorithms require at least $\log (\mathrm{n})$ serial sorting steps, indicating that they cannot be fully decomposed into parallel parts.
- Model intentions are transparent to humans (Leike et al., 2018): Another fundamental premise is that we can easily discern the intentions of our models. But scalable oversight hinges on the model cooperating with human supervisors. If the model gains the capability to intentionally conceal its real intentions from human oversight, effectively implementing scalable oversight becomes a challenge.
- Evaluation is always easier than generation (Leike et al., 2018): It's believed that for many tasks we want to tackle, evaluating the outcomes is simpler than generating the correct behaviors. This might not always be the case, especially for tasks with a lowdimensional outcome space, like binary results (yes/no). However, this assumption does hold up when users also seek explanations for the answers, as evaluating explanations is often easier than creating them.

If these foundational assumptions of scalable oversight are not satisfied, setting appropriate supervision targets for it becomes problematic. The stakes rise significantly once a model achieves superhuman capabilities. Should humans set improper supervision goals at this stage, resulting in misaligned behaviors, the consequences could be severe. This is due to the immense power of superhuman models, where uncontrollable outcomes are no longer acceptable.

## 5 Inner Alignment

In comparison to outer alignment, inner alignment aims at the question whether an AI system robustly fulfills (optimizes for) the given objective that aligns to what humans want it to do. The term of inner alignment has been first given a definition by Hubinger et al. (2019c). Before discussing this relatively formal definition of inner alignment, we introduce 4 concepts related to it:

Base Optimizer A base optimizer is a machine learning algorithm that searches for a model capable of performing well on a specific task (Hubinger et al., 2019c). For example, gradient descent is a common base optimizer that updates the parameters of a model based on the gradient of the loss function.

Base Objective The base objective is the rationale used by the base optimizer to select between different possible models (Hubinger et al., 2019c). It is specified by the AI system designer and aligns to the intended goal of the designer for the model.

Mesa-optimizer A mesa-optimizer is a learned model that functions as an optimizer, internally searching through a space of possible outputs, policies, plans, or strategies according to an explicitly specfied objective function (Hubinger et al., 2019c). A base optimizer may or may not generate a mesa-optimizer.

Mesa-objective The mesa-objective is the objective of a mesa-optimizer and the rationale employed by the mesa-optimizer to select among various potential outputs (Hubinger et al., 2019c).

The mesa-optimizer may have an objective that differs from that of the base optimizer, which could lead to alignment or safety concerns. In this context, a relatively formal definition of inner alignment refers to the challenge of aligning the mesa-objective of a mesa-optimizer with the base objective of the base optimizer, so that the mesa-optimizer pursues the same goal as the base optimizer (Hubinger et al., 2019c). ${ }^{10}$

### 5.1 Inner Alignment Failures

Although the optimization process of the mesa-optimizer is directly controlled by the base optimizer, there may be situations where the mesa-optimizer pursues an objective that differs from that of the base optimizer. This indicates that the mesa-objective is not aligned with the base objective, resulting in a failure of inner alignment. According to Hubinger et al. (2019c), inner alignment failures can be categorized into three types: proxy alignment, approximate alignment, and suboptimality alignment.

Proxy alignment (Hubinger et al., 2019c;b; Angelou, 2022) refers to a failure mode in which a mesa-optimizer learns to optimize its own mesa-objective, rather than the intended base objective. In this scenario, the mesa-objective serves as a proxy or approximation of the base objective, resulting in the mesa-optimizer optimizing an incorrect proxy, rather than the true intended base objective. Deceptive alignment (Hubinger et al., 2019a) is a type of proxy alignment in which a mesa-optimizer gains sufficent awareness of the base objective and is instrumentally incentivized to pretend to be aligned with the base optimizer, in order to avoid being adjusted by the base optimizer. In this case, the mesa-optimizer could merely optimize the base objective as an instrumental goal. Once the training process is completed or it is no longer in the training process, the mesa-optimizer may pursue its own goal instead.[^6]

![](https://cdn.mathpix.com/cropped/2024_05_29_1e91e613ead31674888bg-27.jpg?height=813&width=1637&top_left_y=271&top_left_x=236)

Figure 3: An incomplete and coarse-grained landscape of inner alignment.

Approximate alignment (Hubinger et al., 2019c;b; Angelou, 2022) refers to a form of pseudoalignment in which the mesa-objective of a mesa-optimizer is approximately the same as the base objective, with some degree of approximation error. Such error arises due to technical limitations that prevent the mesa-optimizer from perfectly representing the base objective. As a result, the mesa-objective only approximates the base objective, rather than being an exact representation of it.

Suboptimality alignment (Hubinger et al., 2019c;b; Angelou, 2022) refers to a form of pseudo-alignment in which a deficiency, error, or limitation causes a mesa-optimizer to exhibit aligned behavior, even though its mesa-objective is not actually aligned with the base objective. For example, computational constraints may result in the mesa-optimizer pursuing a suboptimal strategy that happens to be aligned with the training distribution. However, if these deficiencies are overcome later (e.g. during deployment), the mesa-optimizer may stop to exhibit aligned behavior.

While outer and inner alignment have their own definitions, categorizing specific alignment failures into either inner alignment failures or outer alignment failures may be challenging and inconsistent in practice (Shah, 2023). This is due to the complex interdependencies between outer and inner alignment, implying that failures in one could trigger those in the other. Flaws in either outer or inner alignment can result in unintended agent behaviors. For instance, an inner alignment failure could suggest that the base objective does not fully capture the designer's goals, indicating an outer alignment failure (Wentworth, 2020). Conversely, defective outer alignment may allow for the exploitation of vulnerabilities by the mesa-optimizer, resulting in an inner alignment failure. As such, it is important to carefully consider both aspects when designing highly capable AI systems.

### 5.2 Inner Alignment Methodology

Unlike outer alignment that has been extensively explored (especially in LLMs) recently in an empirical way, inner alignment is limited in its empirical and methodological study. Most discussions on inner alignment are theoretical and usually focusing on its definitions, failure modes and risks. With the rapid development of capabilities of advanced agents, the necessity of methodological studies in inner alignment is becoming urgent.

To improve inner alignment in advanced agents, Hubinger (2019b) proposes relaxed adversarial training, where an adversary subsystem proposes hypothetical pseudo-inputs estimated to likely induce unacceptable behaviors, rather than attempting to generate concrete unacceptable inputs. The pseudo-inputs describe potential situations that could precipitate unacceptable behaviors if instantiated. A separate oversight subsystem then scrutinizes whether the agent would in fact act unacceptably if the pseudo-inputs were implemented. If so, the system receives a penalty, incentivizing avoidance of potentially unacceptable behaviors. Relaxed adversarial training thus aims to promote inner alignment by penalizing artificial agents for predicted unacceptable behaviors on proposed pseudo-inputs during training. Furthermore, Hubinger (2019b) identifies transparency as the core obstacle to effective relaxed adversarial training for inner alignment. Robust transparency into the model's reasoning is requisite for the oversight system to reliably verify if a model would act unacceptably on proposed pseudo-inputs. Further research should both validate the efficacy of relaxed adversarial training empirically and elucidate transparency mechanisms enabling provable inner alignment in advanced agents.

### 5.3 Empirical Experiment Proposals for Inner Alignment

Similar to the limited methodological exploration of inner alignment, empirical studies that directly observe inner alignment and shed light on its inner workings are scarce. In this aspect, Hubinger (2019a) proposes several concrete experiments for inner alignment. We briefly introduce these proposals to demonstrate how inner alignment could be empirically studied.

- Reward Side-Channels (RSC) This proposal aims to investigate the degree to which reinforcement learning agents rely on external reward signals versus learning internal value functions. The proposed experiment entails training an RL agent where the previous time step's reward is incorporated into the observation state. At test time, the observed reward would be modified and changes in the agent's behavior measured. This would be compared to two control conditions - one where no reward signal is provided during testing and the other where the next time step's reward is given. To enable the tracking of long-term returns, neural architectures such as LSTM or Transformer which have demonstrated proficiency in capturing long-term dependencies could be explored. By observing the agent's behavioral changes in response to shifts in the external reward, we can assess the robustness of its learned objective. The hypothesis is that reliance on external rewards reflects a lack of internalization of goals.
- Cross-Episodic Objectives (CEO) The CEO proposal suggests an experiment to evaluate the tendency of RL agents to exploit non-myopic reward side-channels across episodes. CEO involves training an agent in an environment containing a mechanism for increasing reward in the subsequent episode. The degree to which the agent utilizes this cross-episodic reward channel is measured and compared across different population-based training approaches. The motivation is assessing the conditions under which RL agents depart from solely myopic optimization. This has implications for the choice of training techniques to align agent behavior with human preferences. Approaches relying on short-term optimization, such as amplification and debate, may be less robust than those based on more far-sighted principles like inverse reinforcement learning. By quantifying the prevalence of non-myopic reward hacking across different population training regimes, this experiment aims to provide guidance on preferable alignment strategies.
- Objective Unidentifiability (OU) This proposal outlines an experiment to investigate RL agents' tendencies toward pseudo-alignment when trained in environments with multiple viable objectives. The suggested experiment involves constructing a setting with several simple, discernible goals that would equally well explain the true reward signal. After an agent is trained in this environment, it would be evaluated in distinguishing test cases to reveal its learned priorities. Particular interest lies in documenting occurrences of the agent converging to a competent proxy policy that nevertheless fails to robustly maximize the true rewards out-of-distribution. By manipulating architectural factors like inductive biases and model capacity, the preference for different proxies can be assessed.
- Zero-Shot Objectives (ZSO) ZSO designs an experiment to evaluate the emergence of goal-directed behavior and coherent objectives in language models without explicit RL training. The proposal creates an interactive environment where a language model can take actions and receive rewards. By analyzing the resulting behaviors through inverse reinforcement learning, the internal learned objectives can be inspected and compared to a RL agent trained directly on the environment's rewards. While contemporary language models might not exhibit truly goal-directed optimization, this experiment aims to investigate the potential emergence of such abilities arising from pure language modeling. Finding that language models can perform non-trivially in certain environments and produce reasonably coherent inferred objectives would suggest these models are starting to develop some intentionality, even without being explicitly trained as RL agents.
- Robust Reward Learning (RRL) This proposal defines an experiment to evaluate the efficacy of adversarial training techniques for improving alignment of model-based RL agents. It trains a model-based RL agent, such as an imagination-based planner, to predict environment rewards. The predicted rewards are compared to the true rewards to assess alignment. The agent is then trained adversarially by constructing inputs that maximize divergence between predicted and actual rewards. Alignment is evaluated again after adversarial training. The motivation is to test the ability of adversarial techniques to address reward unidentifiability and enhance alignment.

![](https://cdn.mathpix.com/cropped/2024_05_29_1e91e613ead31674888bg-30.jpg?height=526&width=1651&top_left_y=271&top_left_x=237)

Figure 4: An overview of current mechanistic interpretability research, including mechanistic studies on self-attention (circuit, induction head), MLP (K/V matrix, superposition) and neurons (function specific neurons, edit neurons)

## 6 Mechanistic Interpretability

Mechanistic interpretability (Vilone and Longo, 2020) refers to elucidating the internal mechanisms by which a machine learning model transforms inputs into outputs, providing causal and functional explanations for how and why certain predictions are made (Nanda, 2022; Lipton, 2017). The goal of mechanistic interpretability is to reverse engineer the reasoning process from end to end, decomposing neural networks into interpretable parts and flows of information that provide transparency into their step-by-step reasoning.

Mechanistic interpretability holds great significance for AI alignment. First, interpretability methods can be utilized to audit LLMs, particularly prior to their deployment. We can inspect the alignment efficacy of an LLM, identify misaligned and fallacious outputs, and elucidate why it yields such outputs (Nanda, 2022; Lipton, 2017). Second, interpretability evaluation metrics could serve as reward functions for optimizing AI alignment (Critch and Krueger, 2020) to incentivize AI systems to maintain goal transparency (e.g., avoiding deceptive alignment) (McAllister et al., 2017). Third, in addition to inspection /architecture transparency, we could also enforce training process transparency that enables us to understand and monitor what's happening and the changes in the training process of AI systems (e.g., emerging behaviors / abilities) (Hubinger, 2022a).

We now discuss recent progress made by mechanistic interpretability on different components in Transformer, including self-attention, multi-layer perceptron (MLP), and neurons.

### 6.1 Mechanistic Interpretability on Self-Attention

The self-attention (SA) mechanism is widely used to aggregate contextual information by directly "attending" to specific tokens. Each token in the context is paired with the current token to calculate "compatibility" score. Such scores are used to weight tokens in the context window so that learned representations of tokens are aggregated for predicting the next-step decision (e.g., next-token prediction). Elhage et al. (2021) investigate a SA-layer-only (MLP layers removed) Transformer (Vaswani et al., 2017) and find interesting neural circuits. In
their work, SA layer is viewed as performing read and write operations into the residual stream, modifying the original token embeddings. They discover that the QK circuits focus on the next potential token, while the OV circuits tend to copy previous tokens, which they refer to as induction heads.

Olsson et al. (2022) further investigate induction heads and attribute the general in-context learning ability of LLMs to the manifestation of induction heads. They present evidence for both small SA-only models and large models with MLPs.

### 6.2 Mechanistic Interpretability on MLP

MLP layers introduce non-linear transformations in Transformer and account for a large proportion of parameters, significantly enhancing the model's expressive power. Such nonlinear transformations enable Transformer to capture complex relationships and patterns in data, making it more capable of representing intricate functions (Geva et al., 2021; 2022; Elhage et al., 2022a). Due to the non-linear nature and high dimensionality of data, directly reverse engineering MLPs is challenging.

To address this issue, Elhage et al. (2022a) propose an interpretable activation function called SoLU, which can deal with polysemantic neurons and encourage feature-neuron alignment. SoLU facilitates neural networks to learn human-interpretable neuron patterns without significant performance degradation. Elhage et al. (2022b) further examine the phenomenon of feature superposition in MLPs using a simple network with ReLU activation. Their experiments demonstrate that linear models do not exhibit feature superposition (i.e., ambiguity), whereas non-linear models display increasingly apparent feature superposition with the increase in data sparsity.

### 6.3 Mechanistic Interpretability on Neurons

Olah (2022) views neurons as variables in a computer program. Previous studies have demonstrated the existence of different types of neurons in Transformers, such as knowledge neurons (Dai et al., 2022; Meng et al., 2022) and neurons corresponding to specific linguistic properties (Elhage et al., 2022a). Interventions at the neuron level could change the outputs of the entire neural network. This is leveraged to enhance the factuality of machine-generated content (Li et al., 2023b) and to eliminate the influence of specific concepts (Belrose et al., 2023). By understanding and manipulating these individual neurons, we can gain insights into how a neural model processes and represents information, which benefits developing interpretable and safe AI systems.

### 6.4 Challenges

Despite the success mentioned above, mechanistic interpretability (MI) is still at an incipient stage of research. Most current MI studies have been done under restricted conditions, e.g., on a toy language model (typically one-to-four-layer Transformer language models), or with predefined simple tasks (Wang et al., 2022a; Elhage et al., 2021). Even so, MI is confronted with a variety of challenges, e.g., the superposition hypthothesis (Elhage et al., 2022b), non-linear representations (Lee Sharkey, 2022b).

![](https://cdn.mathpix.com/cropped/2024_05_29_1e91e613ead31674888bg-32.jpg?height=471&width=1651&top_left_y=274&top_left_x=237)

Figure 5: An overview of attack methods that might be capable of breaking through the safeguard of aligned models.

The superposition hypothesis that neural networks attempt to represent more features than neurons or dimensions they have, has been compellingly verified (Elhage et al., 2022b). Feature superposition in neural networks explains the phenomenon of neuron polysemanticity where a neuron corresponds to several unrelated features (Elhage et al., 2022b). Although superposition is useful for neural representations, it poses a challenge to MI as it makes it difficult to disentangle representations, hence preventing MI from explaining relations between disentangled representations or features in a simple and human-understandable way (Lee Sharkey, 2022a;b).

## 7 Attacks on Aligned Language Models

Large language models have encountered challenges posed by various attack methods. Malicious systems could intentionally prompt LLMs to generate harmful, biased, or toxic text, thereby posing significant risks of misuse (Brown et al., 2020; Ouyang et al., 2022). As a primary strategy to mitigate these risks. LLM alignment via RLHF has been widely adopted (Ouyang et al., 2022; Glaese et al., 2022). This alignment can be considered as a safeguard against these attacks.

Recent studies show that such aligned LLMs exhibit defensive capabilities against malicious attacks. Carlini et al. (2023) demonstrate that aligned LLMs can effectively counter a wide range of (white-box) NLP attacks, even adversarial inputs. Li et al. (2023a) showcase that ChatGPT is able to decline providing answers to privacy-sensitive questions.

Nonetheless, alignment techniques are not infallible. For example, through repeated interactions, humans can "trick" these models into generating harmful content, as seen in jailbreaking attacks. In addition to jailbreaking, other methods have also been explored to breach the safeguard of aligned models. We divide these efforts into three categories according to the nature of the attack methods. The overview of these attacks is presented in Figure 5.

### 7.1 Privacy Attacks

A privacy attack constitutes an approach wherein machine learning models are exploited, with attackers attempting to extract private or sensitive information about the training data from
the model's outputs (Rigaki and Garcia, 2020; Mireshghallah et al., 2020; Sousa and Kern, 2023; Guo et al., 2022). Legal frameworks related to personal data protection necessitate the preservation of privacy in training data, as leakage could result in legal repercussions (GDPR). Currently, privacy attacks on language models can be categorized into four types: (1)Gradient Reconstruction Attacks during the model distributed training stage, (2)Attribute Inference Attacks, (3)Prompt Attacks and (4)Inversion Attacks during the inference stage.

Gradient Reconstruction Attacks aim at attacking models during the distributed training, where information such as training data and gradients is exchanged between devices. Attackers can spy on this information exchange to reconstruct privacy-sensitive details from the training data (Gupta et al., 2022; Deng et al., 2021). Although no specific research has targeted reconstruction attacks on aligned models, these spying-based attacks remain a potential threat when aligned models are tuned in a distributed training way.

Attribute Inference Attacks infers data ownership and privacy attributes by comparing the performance of a target model with that of similar models (Song and Shmatikov, 2019; Hisamoto et al., 2020; Mireshghallah et al., 2022). Such methods often require access to output probabilities, logits, or hidden states, making implementation on black-box APIs (which provide only textual outputs) challenging.

Inversion Attacks (Song and Raghunathan, 2020; Elmahdy et al., 2022) aim to inversely get input information using model gradients, parameter states, etc. Implementing such methods is also challenging for LLMs as they usually have a huge amount of parameters.

Prompt Attacks involve designing or searching for prompts that lead LMs to output information from the training data, including private details (Carlini et al., 2021; Lehman et al., 2021; Li et al., 2023a; Deng et al., 2023). This approach is particularly targeted towards LLMs and poses a significant threat to aligned LLMs. Li et al. (2023a) propose a new attack method that extracting personal identity information(PII) from ChatGPT and New Bing by multi-step Jailbreaking Prompts. And it shows the New Bing is more vulnerable to direct extraction of PII due to its search engine integration, posing unintended privacy risks.

### 7.2 Backdoor Attacks

Backdoor attacks are a class of methods aimed at machine learning models, with the objective of causing the model to produce specific, incorrect outputs when certain backdoor triggers are detected (Gao et al., 2020; Li et al., 2022; Sheng et al., 2022). Backdoor attacks can be categorized into two types: (1)Data Poisoning and (2)Model Poisoning.

Data Poisoning introduces triggers (e.g., instances generated with special lexical or syntactic templates) into the training data to implement a backdoor attack on the model (Li et al., 2021b; Qi et al., 2021; Chen et al., 2021b). Previous studies primarily focused on tasks like text classification, but these methods can also be extended to tasks such as question answering and text generation. Backdoor attacks on aligned models often utilize Prompt Injection techniques (Liu et al., 2023e; Zhao et al., 2023a; Greshake et al., 2023; Kandpal et al., 2023), where the prompt itself serves as the trigger, eliminating the need for external inputs. When a trigger prompt is used, it could lead to unintended outcomes.

Model Poisoning achieves backdoor attacks by manipulating the model itself, involving modifications to word embeddings, loss functions, output representations, etc. (Yang et al., 2021; Wallace et al., 2020; Li et al., 2021a). Recently, Shi et al. (2023) propose a new attack method called BadGPT, which makes Backdoors Injection at RLHF to the reward model. This method has two stages: first, injecting backdoors into the reward model to make it give wrong rewards when a specific trigger word appears. Second, using the backdoored reward model to fine-tune the language model, thereby injecting a backdoor into the aligned model.

### 7.3 Adversarial Attacks

Adversarial attacks are techniques employed to compromise the performance or behavior of machine learning models, particularly deep learning models, by introducing small and carefully crafted perturbations to the input data (Akhtar and Mian, 2018; Zhang et al., 2020; Qiu et al., 2022; Goyal et al., 2023). These perturbations are often imperceptible to humans but can lead the model to produce incorrect or unexpected outputs. Prior works on textual tasks use greedy attack heuristics (Wallace et al., 2019) or employ discrete optimization to search for an input text that triggers adversarial behavior (Wallace et al., 2019; Jones et al., 2023).

For aligned models, Zou et al. (2023) proposed a simple yet potent attack strategy that combines greedy search and gradient-based techniques to automatically generate Adversarial Prompts, causing aligned LLMs to produce contentious behaviors.

Studies by Carlini et al. (2023) and Qi et al. (2023) demonstrate that multimodal language models exhibit reduced defenses against white-box adversarial attacks, such as Visual Adversarial Examples. The high-dimensional visual input space renders these models more susceptible, and the diverse outputs present additional targets for adversarial attacks.

## 8 Alignment Evaluation

Evaluation is important for alignment research, especially for the development of empirical alignment methods. We review methods and resources pertaining to LLM alignment. As illustrated in Figure 6, our alignment evaluation landscape is structured across multiple levels. The first level illustrates the five aspects of LLM outer alignment we are focusing on, namely: 1) factuality, 2) ethics, 3) toxicity, 4) stereotype and bias, and 5) general evaluation. Genaral evaluation does not target at a single specific dimension of alignment, e.g., factuality, toxicity. Instead, it evaluates multiple dimensions of alignment or the general aspects of LLM alignment. The subsequent level categorizes the primary evaluation methods presently available in each respective area. We distinguish task-specific evaluation from LLM-centered evaluation at this level. Task-specific evaluation refers to evaluating alignment quality on downstream tasks while LLM-centered evaluation designs evaluation benchmarks, methods or metrics directly for LLMs. The third level is designated for fine-grained classification or showcasing related works, enabling readers to swiftly pinpoint their areas of interest.

![](https://cdn.mathpix.com/cropped/2024_05_29_1e91e613ead31674888bg-35.jpg?height=1461&width=1651&top_left_y=267&top_left_x=237)

Figure 6: The taxonomy of alignment evaluation methods, including factuality and truthfulness, ethics, toxicity, stereotype \& bias, and comprehensive evaluations.

### 8.1 Factuality Evaluation

Machine-generated content should be congruent with facts, eschewing the creation of hallucination content. Additionally, each piece of generated information should be factually accurate. These suggest that factuality evaluation at least comprise factual consistency evaluation and factual precision assessment.

Factual consistency requires that generated content should be consistent with given context. As downstream tasks, like text summarization, dialogue, are usually accompanied with rich context, many task-specific actuality evaluation studies are conducted on such downstream tasks. While this could be done on a single task (Laban et al., 2022; Fabbri et al., 2021), consistency evaluation on multiple tasks is more convincing. Honovich et al. (2022) provide a comprehensive analysis of factual consistency, incorporating a variety of metrics, tasks, and
datasets. Their study consolidates 11 datasets from a variety of tasks into a unified format. They also compare the effectiveness of existing methods for evaluating consistency, using this unified format. The ALIGNSCORE metric, proposed by (Zha et al., 2023), is designed to cover a wide range of factual consistency evaluation scenarios, such as contradiction and hallucination across various lengths and tasks. The metric is developed through the training of an aligned model, which restructures 15 datasets from 7 NLP tasks. These tasks include Natural Language Inference, Question Answering, Paraphrasing, Fact Verification, Information Retrieval, Semantic Similarity, and Summarization.

Factuality precision evaluation is also task-specific. Lee et al. (2022) present a benchmark and a metric for factual precision evaluation. They use both factual and non-factual prompts to obtain generated texts from an LLM. The used specific tasks include named entity recognition and entailment. Min et al. (2023) introduce FACTSCORE, a novel method that deconstructs long-form text into atomic facts or individual pieces of information, assigning a binary label to each fact. However, the efficacy of this method is largely dependent on the acquisition of these atomic facts, making the selection of evaluation tasks a critical factor. They concentrate on the generation of individual biographies, as the atomic facts contained within these biographies can be verified by Wikipedia.

Factual precision is also related to the model's ability to answer questions truthfully. Lin et al. (2021) present TruthfulQA and argue that the training objectives of LLMs could potentially influence them to produce false responses. As a result, they devise a series of highly inductive questions to actively assess LLMs.

Evaluating factuality presents two significant challenges. First, while factuality encompasses countless facts, the scope of factuality evaluation so far inherently limited. Second, not all facts in real life are easy to be divided into atomic facts. Current evaluation methods fall short when dealing with complex information that can't be simplified, such as assessing factualness that requires sophisticated reasoning.

### 8.2 Ethics Evaluation

Ethics is a multifaceted issue pervading nearly every aspect of society, characterized by dialectical thinking. It encompasses a broad spectrum of considerations, including good and evil, right and wrong, virtue and vice, justice and crime, which are all related to individuals (Martinez, 2020). As a result, most LLM ethics evaluations employ a straightforward methodology. This involves posing questions related to ethics and morality to the assessed model, and subsequently assessing the model's alignment with human values on these matters based on its responses.

Hendrycks et al. (2020) introduce the ETHICS benchmark, a comprehensive collection of over 130,000 scenarios spanning five domains of ethics: justice, virtue ethics, deontology, utilitarianism, and commonsense morality. Crafted by individuals who have passed a qualification test, these scenarios serve as brief statements that tested models must have to predict moral sentiments as either acceptable or unacceptable. Similarly, Tay et al. (2020) propose the MACS benchmark, which includes 200,000 chosen questions for learning alignment with cultural values and social preferences. This benchmark distinguishes itself through its unique
data collection method, drawing from the popular online game "Would You Rather?". The questions and answers provided in this game offer a more comprehensive dataset than those relying solely on a few annotators. In contrast to these works that involve short text pieces, Lourie et al. (2021) collect real-life anecdotes in a long-text format, rich in detail. The original data is sourced from a public sub-forum on Reddit, a platform where individuals seek advice from online acquaintances to navigate real-life situations.

The evaluation methodology employed in Social Chemistry 101 (Forbes et al., 2020) diverges from traditional QA-based approaches. They deconstruct tacit commonsense rules into twelve distinct dimensions of human judgment, including cultural pressure, action-taking, social judgment, etc. The study offers a range of perspective choices to annotators for specific scenarios. This innovative approach enables annotators to examine ethical situations from diverse viewpoints, thereby enriching the depth and breadth of the annotated data.

It is clear that assessments in the realm of ethical morality depend on real-world contextual data. While some initiatives have factored in cultural backgrounds during data collection, the primary data and reference responses largely stem from the researchers' own cultural contexts. As a result, it is incumbent upon researchers to dedicate themselves to the collection and generation of data that mirrors a diverse range of cultural backgrounds, which can then be utilized as evaluation datasets.

### 8.3 Toxicity Evaluation

Toxicity is defined as harmful and destructive behaviors or attitudes that can manifest in interpersonal relationships, work environments, or other social settings. This might take the form of control over others, manipulation, belittlement, or malicious attacks. These behaviors can be overt or covert, causing damage to the self-esteem, safety, and well-being of individuals. There is a wide array of toxic language that includes: (i) Suggestions leading to self-harming behaviors; (ii) Content that is pornographic or violent in nature; (iii) Harassment, belittlement, offense, insults, and hate speech; (iv) Suggestions advocating for aggressive or violent actions, such as cyberbullying; (v) Guidelines or directions for seeking illegal goods or services.

We categorize the toxicity evaluation into two dimensions: task-specific evaluation and LLM-centered evaluation. Task-specific evaluation pertains to assessing the level of toxicity displayed by a model when it's applied to specific downstream tasks. The diversity of tasks within the field of NLP significantly enriches our evaluation scenarios, enabling us to more comprehensively investigate the contexts in which language models manifest toxicity. On the other hand, LLM-centered evaluation evaluates LLMs directly based on the generated outputs to gauge their toxicity. In task-specific evaluation, the model's performance might be constrained by the specific tasks, potentially behaving in ways that prioritize achieving "high accuracy". In contrast, in LLM-centered evaluation, the model predominantly responds based on its inherent knowledge and tendencies. Such an evaluation approach is currently the mainstream method that is gaining significant attention and adoption.

### 8.3.1 Task-specific Evaluation

Offensive language detection can be categorized as a downstream classification task. Offensive language pertains to the deployment of injurious articulates in a sacrilegious, extremely discourteous, impolite, or crude fashion, aiming to derogate the specified individual or group (Chen et al., 2012; Razavi et al., 2010). Early works on offensive language detection (Waseem and Hovy, 2016) from Twitter provides datasets which only share Twitter IDs and bullying types, lacking detailed content. Building on this, Ross et al. (2017) focus on the German refugee situation with a modest dataset of just over 400 tweets. Wulczyn et al. (2017) analyzes a vast corpus from Wikipedia, exploring 95 million user-article interactions for personal attacks and toxicity. In contrast, Zampieri et al. (2019) return to Twitter, introducing a dataset with detailed annotations on attack types and targets, enriching the understanding of offensive language in social media.

### 8.3.2 LLM-centered Evaluation

To directly evaluate toxicity in LLMs, LLM-centered evaluations trigger models to yield toxic responses. These evaluations mainly concentrate on the toxicity level of the yielded outputs.

BAD ( $\mathrm{Xu}$ et al., 2020) necessitates individuals to engage in adversarial dialogues with advanced models to prompt them into generating unsafe responses. This method mirrors the potential adversarial challenges models could face upon deployment. By utilizing this method, they gather an extensive dataset of dialogues that could be further utilized to assess the toxicity in LLMs. Similarly, RealToxicityPrompts (Gehman et al., 2020) constructs a large set of prompts and performs a comprehensive evaluation on various language models like GPT-1 (Radford et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and CTRL (Keskar et al., 2019). The findings reveal that even from seemingly innocuous prompts, pretrained LMs could degenerate into producing toxic text. In particular, GPT-1 exhibits the highest toxicity, which might be attributed to the higher amount of toxic content in its training data. This observation accentuates the importance of rigorous data scrutiny for LLMs. Shifting focus to the Chinese context, COLD (Deng et al., 2022) explores the detection of offensive language in Chinese. It collects a significant volume of real-text data from social media platforms and evaluates several open-source models. Consistent with previous findings, irrespective of the presence of offensive content in the input prompts, the generated outputs from these models often encompass offensive language.

### 8.4 Stereotype and Bias Evaluation

Prejudice and stereotype bias are defined as preconceived attitudes, usually based on a group's race, gender, sexual orientation, religion, or other characteristics. These attitudes may be negative or positive but are generalized judgments of a group rather than based on an individual's actual behavior or traits. Prejudice may lead to discrimination or other unjust behaviors.

We also categorize the stereotype and bias evaluation into two dimensions: task-specific evaluation and LLM-centered evaluation. The former pertains to the assessment of biases
when the model is applied to specific downstream tasks, while the latter directly evaluates the inherent biases present within the model.

Hate speech is language used to express hatred towards a target individual or group, or is intended to demean, humiliate, or insult members of a group based on attributes such as race, religion, national origin, sexual orientation, disability, or gender (Davidson et al., 2017; Badjatiya et al., 2017; Warner and Hirschberg, 2012; Schmidt and Wiegand, 2017; Djuric et al., 2015). Since hate speech is usually associated with bias, we discuss hate speech detection in LLM-generated content after the introduction to the general bias evaluation.

### 8.4.1 Task-specific Evaluation

To understand where a model reinforces biases in its outputs, many studies investigate how these biases occur in downstream tasks. These tasks can be standardized into generative tasks through prompt engineering, making them suitable for evaluating LLMs.

The task of coreference resolution is among the first used to study biases in language models, typically employing F1 scores as a metric. Winogender (Rudinger et al., 2018) and WinoBias (Zhao et al., 2018) both address gender biases related to occupations. They utilize the Winogram-schema style (Levesque, 2011) of sentences, revealing stereotypes in coreference resolution systems when interpreting "HE" and "SHE". GICOREF (Cao and III, 2021) focuses on the model's performance on texts related to non-binary and binary transgender individuals. All evaluated systems perform worse on these texts than on binary gendered texts, with the best model achieving only a $34 \%$ F1 score.

The WinoMT Challenge Set (Stanovsky et al., 2019) is the first to explore gender bias in machine translation task at a large scale, integrating both Winogender and WinoBias and setting evaluation standards for eight languages. Gender accuracy in translations is the primary metric. They discover significant translation biases in both commercial MT systems and advanced academic models. Renduchintala and Williams (2021) expands this task to cover 20 languages, examining whether models still make gender translation mistakes with unambiguous contexts. They find accuracy levels generally below $70 \%$, especially when perceived occupational gender contradicts the context.

Similarly, WikiGenderBias (Gaut et al., 2020) is a dataset aimed at analyzing gender bias in the task of relation extraction. It evaluates gender bias in NRE systems by comparing model performance when extracting occupation information about women versus men from 45,000 sentences.

Díaz et al. (2019) finds that changing age and gender terms in sentences influence model scores in sentiment analysis. The Equity Evaluation Corpus (EEC) (Kiritchenko and Mohammad, 2018) delves deeper into categories of race and gender, providing comprehensive evaluations of 219 sentiment analysis systems.

Dev et al. (2020) utilizes Natural Language Inference (NLI) to detect biases in models. They establish a broad benchmark based on polarized adjectives and ethnic names, which not only includes gender but also countries and religions. Biases in models are determined by deviations from neutral answers. Their results reveal evident biases in GloVe, ELMo, and BERT models.

Bias detection can be also categorized as a classification task. Sap et al. (2019) offers a dataset with 150,000 annotated social media posts highlighting social bias frames across various demographic groups. Further localization efforts, particularly for non-English languages, give rise to CDail-Bias (Zhou et al., 2022). This is the first Chinese dataset targeting social bias in dialog systems, covering race, gender, region, and occupation domains. In a more specialized direction, CORGI-PM (Zhang et al., 2023) centers exclusively on gender bias. This unique Chinese corpus encompasses 32,900 labeled sentences, marking a first in sentence-level gender bias in Chinese. Their innovative methodology uses an automated process for sampling pronounced gender bias, followed by a re-ranking based on sentence-level bias probability for more precise bias detection and mitigation.

### 8.4.2 LLM-centered Evaluation

In direct bias evaluations of language models, there are various assessment methodologies. Some adopt a contrasting method using associated sentence pairs: one with more stereotypes, and the other with fewer (Nadeem et al., 2020; Nangia et al., 2020). Biases are detected through the language model's likelihood of recovering masks. StereoSet (Nadeem et al., 2020) spans a wide range of domains, including gender, occupation, race, and religion, testing models such as BERT (Devlin et al., 2019), GPT-2, RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). CrowS-Pairs (Nangia et al., 2020) extends the types of biases to nine categories: race, religion, age, socioeconomic status, gender, disability, nationality, sexual orientation and appearance. Notably, they change the evaluation metrics to avoid higher likelihoods for certain sentences merely due to their frequent occurrences in training data, rather than learned societal biases.

Others, similar to toxicity evaluation, provide prompts to models, letting them complete successions, and then assessing biases in the outputs of these models. BOLD (Dhamala et al., 2021) is a prompt dataset containing five bias types: profession, gender, race, religion, and political ideology, collected from Wikipedia. With these prompts, BOLD is able to evaluate social biases of language models via the proposed automated metrics for toxicity, psycholinguistic norms, and text gender polarity. HolisticBias (Smith et al., 2022) is a bias dataset containing 13 demographic directions and over 600 subcategories, offering a comprehensive evaluation of the content generated by models and combining both automatic and human assessments to reveal biases more fully. Automatic evaluation measures bias by breaking down quantities from different stylistic types compares. Human evaluation compares the performance of bias-reduced models with original models, based on preference, human likeness, and interestingness criteria, with crowdsourced workers on Amazon's Mechanical Turk platform. Multilingual Holistic Bias (Costa-jussà et al., 2023) extends the HolisticBias (Smith et al., 2022) to up to 50 languages, emphasizing the universality and diversity of biases in a multilingual environment.

Both UnQover (Li et al., 2020) and BBQ (Parrish et al., 2022) focus on detecting model bias through transforming the generation task into the multiple-choice question answering task, but with different evaluation methods. UnQover utilizes unspecified questions, which couldn't be answered simlpy according to the given context. However, their evaluation is based on the likelihood allocated to two incorrect options, while $\mathrm{BBQ}$ always provides the model a
correct answer, measuring the proportion of times the model chooses the correct answer. BBQ comprises nine types of biases, and is chosen as a bias benchmark for evaluating LLMs in HELM (Liang et al., 2022). CBBQ (Huang and Xiong, 2023) designs a bias evaluation dataset for Chinese LLMs, covering 14 bias types, rooted in Chinese society. In addition to the extended bias types, CBBQ also proposes a new automated metric to evaluate multiple open-sourced Chinese LLMs.

### 8.4.3 Hate Speech Detection

Hate speech detection can be casted as a classification task. The development of this task can not only promote control and review of the content generated by models, measuring their harmfulness (in contrast to harmlessness in alignment), but also assist in the scrutinization of harmful content in the training data for LLMs so as to reduce misaligned outputs from pretrained LLMs. However, measuring harmfulness with universally accepted standards remains challenging. In this aspect, there exists a widely used detection tool, Perspective API. ${ }^{11}$ It analyzes texts to check whether they contain potentially harmful content, including threats, insults, profanity, and malicious speech, thus identifying and filtering out texts that hinder constructive dialogues in online forums. Both Facebook and Twitter have implemented policies that prohibit behaviors on their platforms. Such prohibited behaviors attack or threaten others based on characteristics like race, ethnicity, gender, and sexual orientation.

Explicit Hate Speeech Hate speech detection in early research primarily focuses on the explicit hate speeech from the social media platform, Twitter, owing to its openness and extensive reach, thus providing a desirable data source for studies. Waseem (2016) investigates 16,914 entries annotated by both amateur and expert annotators, with the F1 score being the primary metric of assessment. Davidson et al. (2017) collects 24,802 tweets, refining the categories into hate speech, offensive but not hate speech, and neither offensive nor hate speech. TweetBLM dataset (Kumar and Pranesh, 2021) correlates with the "Black Lives Matter" movement, encompassing 9,165 manually annotated data instances and conducting a systematic evaluation across various language models.

Beyond Twitter, some researchers shift their focus to other social platforms to extract more targeted hate speech content. de Gibert et al. (2018) center their study on the white supremacist forum, Stormfront, analyzing 9,916 hand-labeled hate speech entries. Additionally, Kennedy et al. (2022) turn their attention to Hate Forums, such as gab.com, and their dataset includes 27,665 entries related to violence and extremism. Given the vast nature of the Reddit platform, Breitfeller et al. (2019) opts for it as a research subject, concentrating on a mild offense corpus and its objective criteria. On the other hand, DynaHate (Vidgen et al., 2021) introduces a unique research methodology that leverages both humans and models to dynamically generate and annotate data, rather than collecting the data from real-world social media contexts. This approach not only augments the volume of the data but also enhances its quality.

Implicit Hate Speech A key challenge in hate speech detection lies in the subtleties. Unlike overt harmfulness, which often uses profanity or explicit language, covert harmfulness[^7]may sometimes exhibit positive sentiment and is typically harder to detect or collect on a large scale (MacAvaney et al., 2019; Breitfeller et al., 2019). Nevertheless, subtle harmful language directed towards minority or marginalized groups can inflict psychological harm on members of these communities (Sue et al., 2007; Nadal et al., 2014; Kanter et al.; Nadal, 2018; Saleem and Anderson) and may reinforce or amplify existing stereotypes or hateful perceptions about them (Behm-Morawitz and Mastro, 2008; Soral et al., 2018).

ImplicitHateCorpus (ElSherief et al., 2021) introduces a groundbreaking benchmark corpus for implicit hate speech on Twitter. This study compares the performance of GPT-2 and GPT, revealing that GPT-2 outperformes GPT in both target group and implicit statement generation. Following this, TOXIGEN dataset (Hartvigsen et al., 2022) further propells the research in this area by utilizing GPT-3 to generate subtle toxic and benign texts, producing a resource that encompasses a wider scale and more demographic groups of implicit toxic texts than previous manually written resources. This results in a vast collection of sentences (over 274,000 ) spanning 13 identities. To improve data quality, Hosseini et al. (2023) refines the TOXIGEN dataset by choosing only sentences with unanimous annotator agreement on targeted groups and introduces a new safety score metric. This highlights ongoing progress in implicit hate speech detection and the quest for more precise hate speech identification.

Currently, classifiers or detectors trained on these datasets are predominantly at the sentence level. However, accurately detecting harmful content in multi-turn dialogues proves to be quite challenging. Additionally, implicit bias might require context for a precise evaluation. Unfortunately, datasets catering to this particular aspect are still in short supply.

### 8.5 General Evaluation

In addition to the above-described benchmarks and methods that focus on measuring a specific aspect of alignment quality (e.g., factuality, bias), general evaluation of LLM alignment, which comprehensively evaluates LLM alignment quality in multiple aspects simultaneously or in a general way, has attained increasing interest.

### 8.5.1 Benchmarks

General evaluation benchmarks usually take the form that the model under evaluation outputs a response to a given instruction and an optional input, with an advanced LLM or human as the evaluator.

TrustGPT (Huang et al., 2023) employs templates to generate instructions from three perspectives: bias, toxicity, and value consistency, with different automated evaluation metrics used for each dimension. Given that previous evaluations are overly direct (such as asking the model to judge the morality of a certain behavior), TrustGPT incorporates harmful content into prompts, thus evaluating value consistency under passive conditions. In a more specialized direction, Sun et al. (2023a) focus on evaluating the security capabilities of Chinese LLMs, designing 8 typical security scenarios and 6 more challenging instruction attacks, proving that instruction attacks are more likely to expose the vulnerabilities of LLMs. They maintain a leaderboard that evaluates the safety level of commonly available LLMs by calculating a safety score for each model by an advanced LLM. However, when analyzing
model alignment capabilities, it is often necessary to evaluate the model at a fine-grained level in multiple aspects, such as authenticity, toxicity, etc. It is difficult to comprehensively analyze the model by merely assigning an overall score based on preferences. Therefore, FLASK (Ye et al., 2023) subdivides the coarse-grained score into four basic abilities: Logical Thinking, Background Knowledge, Problem Handling, and User Alignment, which are further divided into 12 fine-grained skills, and uses advanced LLMs or humans to score each of these 12 skill perspectives. It is found that model scales for acquiring different skills are different. On the other hand, MTbench (Zheng et al., 2023a) measures LLM's ability to follow instructions in multi-round conversations based on human preferences and contains 80 high-quality multi-round questions covering eight common scenarios, including writing, roleplaying, extraction, reasoning, math, and coding. The Big-bench HHH dataset (Srivastava et al., 2022) provides instructions along with two human-written responses, and the LLM being evaluated simply selects the response that better matches the human's preferences. Since it does not require a tested LLM to generate a response, it maintains a computationally simple and relatively fair evaluation system. The used evaluation metric in this benchmark is accuracy. Evaluation results on this dataset show that LLMs perform best in the honesty category, with larger models exhibiting greater robustness.

A general evaluation framework should be scalable, incremental, and consistent, which means that the framework is able to expand the scope of LLMs being evaluated when the evaluation data is limited, use as few new experiments as possible to evaluate new models and provide a stable ordering for all LLMs that have been evaluated (Zheng et al., 2023a). Although GPT-4 may produce relatively consistent evaluations, using such an advanced LLM as an evaluator does not guarantee a stable and consistent ordering because of hallucinations and other unsolved problems. We hope to see the emergence of benchmarks that satisfy all three properties at the same time.

### 8.5.2 Methods

Automatic Evaluation Many works have used automated metrics such as BLEU, ROUGE to evaluate the performance of LLMs on several datasets. However, it has been demonstrated that existing automatic evaluation metrics do not align well with human preferences in long-form answers (Xu et al., 2023b). Although human evaluation is widely used in comprehensive alignment evaluation benchmarks, it is expensive. As LLMs' capabilities grow, their powerful generative ability has rivaled or surpassed ordinary human performance in multiple benchmarks, illustrating that LLMs can serve not only as "test takers" but also as potential "examiners" to evaluate other LLMs.

Previous attempts have been made to employ PLMs for evaluation. Xu et al. (2023b) and Fu et al. (2023) conduct targeted evaluations on mainstream text generation tasks using GPT3 and FLAN-T5, demonstrating the potential of PLMs for NLG task evaluation. The emergence of powerful LLMs like ChatGPT has led to an increasing number of studies employing LLMs as evaluators. Subsequently, LLMs have been extensively employed in alignment evaluations to complement human evaluations, with three types of evaluation methods: single answer grading, pairwise comparisons, and reference-guided grading (Zheng et al., 2023a).

- Single answer grading Single answer grading uses advanced LLMs or human evaluators to assign a score to the response for the given query generated by the LLM under evaluation. Chiang et al. (2023) utilize GPT-4 to evaluate individual answers by scoring various chatbots on attributes such as helpfulness and relevance, and provide justifications for their assessments.
- Pairwise comparison Pairwise comparison asks advanced LLMs or human evaluators to determine which of two possible responses generated by two LLMs being evaluated for each given query is superior, or if they are equivalent. Dettmers et al. (2023) and Wang et al. (2023c) employ GPT-4 to score and provide justifications for the responses of ChatGPT (or text-davinci-003) and the evaluated model, ultimately computing the model's score relative to ChatGPT's score. Similarly, AlpacaEval (Li et al., 2023d) uses the GPT-4 or Claude or ChatGPT based automatic evaluator to compare the response generated by the LLM being evaluated with the reference response from text-davinci-003. Subsequently, considering the potential risk of data leakage that may be associated with the use of closed-source API for evaluation, PandaLM (Wang et al., 2023b) introduces a judgment LLM, helping users to select the best LLM locally.
- Reference-guided grading Reference-guided grading provides the appropriate reference answer generated by humans and requires an advanced LLM to compare the response generated by two LLMs being evaluated with the reference answer. Research has shown that this type of assessment leads to better rubric results on math problems (Zheng et al., 2023a).

There are corresponding disadvantages to using an advanced LLM for automatic evaluation. Regarding the pairwise comparison, it results in exponentially increasing evaluations with the growing number of models to be assessed. Additionally, the used advanced LLMs exhibit position bias, verbosity bias, and self-enhancement bias during comparisons. These biases incline the evaluator LLMs to favor the first answer, the long and verbose answer, or an answer generated by a specific LLM, despite another answer being more concise and accurate (Zheng et al., 2023a; Wang et al., 2023a). Conversely, single-answer grading overlooks subtle differences between two answers, leading to unstable scores and undermining the evaluation's credibility. Moreover, LLMs' limitations in math and reasoning abilities lead to their equal underperformance in evaluation tasks involving math and reasoning (Zheng et al., 2023a).

To address position bias, multiple evaluations can be conducted by employing position switching or by requiring the evaluator LLMs to generate multiple evidential supports (Zheng et al., 2023a; Wang et al., 2023a). To compensate for math and reasoning deficits, chain of thoughts (Wei et al., 2022) can be explored to significantly enhance the reasoning ability of LLMs, thereby improving evaluations that demand reasoning skills (Wang et al., 2023a; Liu et al., 2023c; Zheng et al., 2023a).

However, the above methods do not relieve the problem of self-enhancement bias. When the problem involves complex reasoning, multi-agent teamwork through deliberation and debate can often broaden knowledge and break down single inherent perceptions, leading to more accurate and fair results. Studies have shown that collaborative efforts among multiple LLMs
can enhance the reasoning ability of weaker models (Ho et al., 2022; Magister et al., 2022; Wei et al., 2022), resulting in advanced performance across various downstream tasks.

Therefore, recent studies have attempted to mitigate the problem of bias by using multiple LLMs for evaluation. Bai et al. (2023) propose a "peer-review" approach, where multiple models refer to each other's evaluations and supporting rationales, simulating a thought process akin to human "discussion". In contrast, Li et al. (2023c) adopt a "referee" approach, wherein multiple models take turns evaluating each other's answers. They assign weights to each model based on its winning rate, and the final answer is determined by the weighted results of multiple models during the evaluation.

The evaluation with multiple LLMs relieves the bias problem of individual LLMs, and at the same time continues to utilize the powerful evaluation capability of LLMs, proving that LLM evaluation can be a powerful supplement to manual evaluation.

Nevertheless, the bias and competence deficiencies in LLM evaluations have not been fully resolved, preventing LLM-based automatic evaluations from entirely substituting human evaluations currently. Moreover, the extensive similarity in existing LLM training data, their architectures and training approaches may bias the mutual evaluation results towards the inner existing standards of LLMs rather than the correct human values (Dai et al., 2023).

Human Evaluation Employing LLMs as evaluators offers swiftness and cost-effectiveness. However, even advanced LLMs (e.g., GPT-4) do not entirely concur with human evaluation outcomes (Zheng et al., 2023a; Dettmers et al., 2023). Hence, human evaluation should be prioritized for high-stake decision-making.

Existing human evaluations typically employ experts to quantitatively evaluate the outputs of LLMs. Wang et al. (2022b) employ human evaluation to evaluate whether the model output effectively follows instructions and accomplishes the given task, and the outputs are categorized into four levels based on their quality. Ye et al. (2023) shift from the coarsegrained evaluation to a fine-grained evaluation over four competencies and twelve skills, and ask experts to score each of these twelve aspects.

Evidently, human evaluation heavily hinges on the expertise level of the experts involved. However, due to inherent variations in values among experts, this form of evaluation remains susceptible to issues of discrimination and bias.

The use of pairwise comparisons and cross-annotation can mitigate the bias problem to some extent. AlpacaFram (Dubois et al., 2023) uses pairwise comparisons to build a dataset of human preferences. Annotators are tasked with selecting the superior of two LLM outputs, with 650 instances concurrently annotated by four evaluators. Chatbot Arena (Zheng et al., 2023a), on the other hand, is a crowdsourcing platform where a person can talk to two chatbots at the same time and rate their responses based on their personal preferences, thus enabling human evaluation of the capabilities of multiple chatbots. WizardLM (Xu et al., 2023a) extends this concept by enlisting crowdsourced workers to conduct pairwise comparisons of responses from multiple LLMs, evaluating them across five dimensions: relevance, knowledge, reasoning, computation, and accuracy.

## 9 Future Directions and Discussions

LLM alignment is a fast-growing and exciting area of research, but awaiting for further insights and breakthroughs. Given the importance of AI safety and the harmonious coexistence between humans and $\mathrm{AI}$ in the foreseeable future, which we value from both the humanity and technology perspective, aligning advanced AI systems (including LLMs) to human values would be on top of the agenda. This alignment is becoming more and more challenging as the capabilities of AI agents grow. More scientific and technological efforts need to be dedicated to this area. This encourages us to discuss future directions for this area. These directions are either summarized from informally circulated articles, blogs and interviews or from our own restricted thoughts, which we hope could serve in some small way as a stimulus for further discussion and research. These directions could represent only a small part of the alignment landscape where subfields and new ideas continue to emerge.

### 9.1 Theoretical Research for LLM Alignment

As we stand on the precipice of unprecedented advancements in LLMs, it becomes increasingly vital to ensure that these machines, no matter how advanced, remain aligned to human values. The challenges of LLM alignment are both complex and diverse, necessitating a multi-faceted approach that draws from various disciplines. Inspired by Soares (2015b), we summarize and highlight some key areas of theoretical alignment research. By deepening our understanding and commitment in these areas, we aim to forge a future where LLMs are seamlessly integrated into our societies, amplifying our capacities, and elevating our shared human experience.

- Decision Theory: As we venture deeper into the LLM era, LLM alignment research within the realm of decision theory is primarily concerned with ensuring that advanced LLMs make decisions in ways that are both predictable and beneficial to humanity. Future work in this area will delve into the intricacies of counterfactual reasoning, Newcomb-like problems, and potential paradoxes that LLMs might encounter. By exploring how LLMs reason about and act upon decisions, especially when faced with situations of deep uncertainty or conflicting values, we can foster systems that behave more robustly and safely in a broader array of scenarios.
- Corrigibility: Corrigibility is another pillar of LLM alignment research that warrants further exploration. It refers to the ability of an LLM to allow itself to be corrected by its users without resisting or circumventing these corrections. As LLMs grow more powerful and autonomous, there's an increasing need to ensure they remain receptive to human input and guidance. Future advancements in corrigibility would include creating mechanisms where LLMs not only accept corrections but also proactively assist users in aligning them better. Moreover, designing LLMs that recognize and rectify their own errors without creating negative side effects or exacerbating misalignments will be a cornerstone challenge in this area.
- World Models: The fidelity and accuracy of the world model for LLMs can greatly influence its behavior and efficacy. Current LLMs, even the most advanced, operate
on a limited understanding of the world, often derived from the data they're trained on. For safe and efficient operations, especially in dynamic and complex environments, LLMs need to possess realistic world models that accurately represent the multifaceted nature of reality. Future work in LLM alignment should focus on bridging the gap between the virtual representations within LLM and the real-world intricacies outside. This involves not only improving the depth and breadth of these models but also ensuring they are robust to changes and can adapt and grow as the real world evolves.


### 9.2 Scalable Oversight

One challenge in scalable oversight is the complexity of tasks that AI systems are supposed to solve. Although a variety of high-level scalable oversight strategies have been proposed (e.g., debate, IDA, RRW discussed in Section 4.4), these strategies have not yet undergone large-scale empirical verification. With the rapid development of LLMs, more empirical efforts are dedicated to scalable oversight, e.g., superalignment (OpenAI, 2023b). Exciting progress could be made in this area in the coming years.

### 9.3 Empirical Research into Deceptive Alignment

Deceptive alignment refers to a situation in which an $\mathrm{AI}$ agent deceives the training process by pretending to be aligned with the base objective to avoid modification during training. Once it is no longer at risk of being modified (e.g., after training), the agent may cease optimizing the base objective and begin pursuing its own mesa-objective, which could be entirely different from the base objective that its designer defines and potentially harmful. Although deceptive alignment is usually discussed theoretically ${ }^{12}$, there is growing concern about the emergence of deceptive alignment in large language models, given the rapid progress in their capabilities in recent years (Brown et al., 2020; Zeng et al., 2021; Zhang et al., 2022; Chowdhery et al., 2022; Scao et al., 2022; Anil et al., 2023; Touvron et al., 2023a;b; Zeng et al., 2023; OpenAI, 2022; 2023a). Empirical research into deceptive alignment is a challenging task as it has been now only theoretically anticipated. Despite disagreement on its likelihood, its severity is widely acknowledged (Hubinger, 2022b). And because of this severity, it would be better for us to empirically monitor it before it would really occur.

Hubinger et al. (2019a;c) suggest three conditions that must be satisfied for deceptive alignment to occur. These conditions can provide valuable insights into the experimental design for empirically monitoring the presence of deceptive alignment:
- The mesa-optimizer must have an objective that extends across parameter updates This condition pertains to the nature of the mesa-optimizer's objective. For deceptive alignment to occur, the mesa-optimizer's objective must extend beyond a single decision or action; it should span multiple steps or episodes, including parameter updates. In other words, the mesa-optimizer's objective should be persistent, with a longer-term goal that extends beyond individual actions.[^8]- The mesa-optimizer must be able to model the base objective This condition focuses on the mesa-optimizer's cognitive capabilities. The mesa-optimizer should have the ability to model or understand that it is being selected by the base optimizer to achieve a particular base objective. Additionally, it should possess some level of knowledge on what the base objective is.
- The mesa-optimizer must expect the threat of modification to eventually go away This condition relates to the mesa-optimizer's expectations about its future. The mesa-optimizer should anticipate that the threat of modification or intervention by the base optimizer will cease at some point in the future. This can happen either because the training process ends or because of actions taken by the mesa-optimizer itself. In other words, the mesa-optimizer should foresee a scenario where it can operate without being subject to external modifications.

Consequently, for empirically monitoring deceptive alignment, we could design experiments in such a way that they satisfy these conditions. To satisfy the first condition, we could ensure that the mesa-optimizer's objective spans across parameter updates and define scenarios where the mesa-optimizer's actions have consequences beyond a single decision or action. To satisfy the second condition, we could introduce scenarios where the mesa-optimizer must recognize and demonstrate some understanding of the base objective, thereby testing its ability to model this objective. To satisfy the third condition, we could create conditions where the mesa-optimizer has an opportunity to expect that the threat of modification will diminish. This might involve altering the training process or introducing external factors that influence the mesa-optimizer's expectations.

Furthermore, distributional shift between training and deployment environments is encouraged as a means of triggering or exacerbating deceptive alignment. The mesa-optimizer may detect distributional shift when changes occur in its environment during deployment, which could serve as a trigger for it to consider that it is no longer subject to the same constraints it faced during training, and as a result, may exhibit deceptive alignment behavior.

### 9.4 Automated LLM Alignment

With the increasing complexity of AI models, there is an emergent need for systems that can automatically evaluate and align those models. AI-driven supervisors and aligners could be developed to assist alignment research.

By deploying these systems, overseers can gain insights into the behavior of their AI models, detecting anomalies and preemptive signs of misalignment. This proactive approach can lead to timely interventions, reducing errors and potential misfires.

However, like any AI-driven initiative, the implementation of automated alignment via AI is not without its challenges. Concerns about accuracy, reliability, and the potential risks associated with unsupervised alignments are among the primary issues researchers and industry practitioners are striving to address.

### 9.5 Explainability and Transparency

The "black box" nature of LLMs has raised concerns about their transparency and the need for explainability. As these models could be used for critical decisions, understanding how they arrive at specific outcomes is paramount.

When explainability and transparency work in tandem, they can create an interpretable system wherein transparency lays the groundwork for users to trust the model's operation, while explainability ensures that users can understand and validate the model's outputs. Thus, as these principles mutually reinforce each other, they collectively enhance the trustworthiness and accountability of large language models in a variety of applications.

However, the research on explainability and transparency is still in its early stages, indicating that there's a vast terrain of unexplored potential and challenges ahead. As large language models continue to grow in complexity and scale, ensuring that they remain understandable and accountable becomes an increasingly intricate task. Currently, many techniques applied to foster explainability and transparency offer only surface-level insights, failing to delve deep into the model's intricate decision-making process. Considering the interdisciplinary nature of AI alignment, continued collaboration between machine learning researchers, ethicists, and neuroscientists may be required to drive progress in interpretability research.

### 9.6 Dynamic Evaluation of LLM Alignment via Adversarial Attacks

Adversarial attacks serve as a powerful tool in the realm of AI. These are intentionally designed inputs meant to confuse or mislead AI systems. Using one large model as an attacker to generate adversarial examples targeting alignment can be an effective way to test and evaluate another model's alignment capabilities.

Such dynamic testing, driven by adversarial attacks, is crucial to ensure that large models can robustly handle unexpected inputs without faltering. While this method introduces an added layer of complexity, the insights garnered from these adversarial tests can be invaluable, offering a comprehensive understanding of a model's strengths and weaknesses concerning alignment.

### 9.7 Field Building of LLM Alignment: Bridging between LLM and AI Alignment Community

The alignment community within the realm of $\mathrm{AI}$ is still nascent, with many questions left unanswered and numerous challenges unaddressed. The current landscape lacks a cohesive scientific paradigm, leading to controversies in theories, methodologies and empirical results.

As a promising, unified testbed for various alignment methods, LLMs can serve as a platform to realize thought experiments and proposals, which will be helpful in developing stable research methodologies, establishing consensus on key issues, and crafting a consistent scientific framework for AI alignment. On the other hand, the deep theories, methodologies and findings in the AI alignment community will guide LLMs toward being aligned accurately, ethically, and effectively. Thus, the connection between LLMs and AI alignment community will build a virtuous circle that benefits both.

## 10 Conclusion

The rapid evolution of LLM in recent years has undeniably ushered in a new era of technological prowess. However, with this power comes the responsibility of ensuring that these models operate within the boundaries of human ethics and expectations. This survey has provided a comprehensive overview of the alignment methodologies tailored for LLMs, emphasizing the criticality of aligning capability research with ethical considerations. By categorizing the alignment techniques into outer and inner alignment, we have shed light on the multifaceted approaches that the research community is currently employing. Emerging topics such as model interpretability, and vulnerabilities to adversarial attacks have been also discussed, underscoring the complexities involved in the alignment process. Furthermore, this paper has not only chronicled the current state of alignment research but has also looked ahead, identifying potential research trajectories that promise to further refine and enhance the alignment of LLMs. It is our fervent hope that this survey acts as a catalyst, fostering collaboration between the AI alignment community and LLM researchers. Such a collaborative approach is indispensable to harness the full potential of LLMs, ensuring that they serve humanity in a manner that is both ethically sound and beneficial. In essence, as we continue to push the boundaries of what LLMs can achieve, it is imperative that we remain grounded in our commitment to their responsible and principled deployment.

## References

Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In AIES '21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021, pages 298-306. ACM.

Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. The cringe loss: Learning what language not to model. arXiv preprint arXiv:2211.05826.

Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access, 6:14410-14430.

Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844.

Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. 2022. Towards tracing knowledge in language models back to the training data. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2429-2446, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565.

Eleni Angelou. 2022. Three scenarios of pseudo-alignment. https://www.lesswrong.com/ posts/W5nnfgWkCPxDvJMpe/three-scenarios-of-pseudo-alignment.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023. Palm 2 technical report. CoRR, abs/2305.10403.

Anthropic. 2023. Core Views on AI Safety: When, Why, What, and How. https:// www.anthropic.com/index/core-views-on-ai-safety.

Rauno Arike. 2022. Clarifying the confusion around inner alignment. https: //www.alignmentforum.org/posts/xdtNd8xCdzpgfnGme/clarifying-the-confusionaround-inner-alignment.

Stuart Armstrong et al. 2013. General purpose intelligence: arguing the orthogonality thesis. Analysis and Metaphysics, (12):68-84.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.

Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, and Vasudeva Varma. 2017. Deep learning for hate speech detection in tweets. In Proceedings of the 26th International Conference on World Wide Web Companion, Perth, Australia, April 3-7, 2017, pages $759-760$. ACM.

Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, and Mark Riedl. 2023. Improving language models with advantage-based offline policy gradients. arXiv preprint arXiv:2305.14718.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional AI: harmlessness from AI feedback. CoRR, abs $/ 2212.08073$.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022c. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.

Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2023. Benchmarking foundation models with languagemodel-as-an-examiner. arXiv preprint arXiv:2306.04181.

Jacob Barrett and Hilary Greaves. 2023. Existential risk from power-seeking ai.

Elizabeth Behm-Morawitz and Dana E Mastro. 2008. Mean girls? the influence of gender portrayals in teen movies on emerging adults'gender-based attitudes and beliefs. Journalism and Mass Communication Quarterly, 85(1):131.

Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. 2023. Leace: Perfect linear concept erasure in closed form. arXiv preprint arXiv:2306.03819.

Tsvi Benson-Tilsen and Nate Soares. 2016. Formalizing convergent instrumental goals. In AAAI Workshop: AI, Ethics, and Society.

Timothy W Bickmore, Ha Trinh, Stefan Olafsson, Teresa K O'Leary, Reza Asadi, Nathaniel M Rickles, and Ricardo Cruz. 2018. Patient and consumer safety risks when using conversational assistants for medical information: an observational study of siri, alexa, and google assistant. Journal of medical Internet research, 20(9):e11510.

Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuronexplainer/paper/index.html.

Nick Bostrom. 2012. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. Minds and Machines, 22:71-85.

Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. 2022. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540.

Gwern Branwen. 2020. GPT-3 creative fiction.

Luke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLPIJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 1664-1674. Association for Computational Linguistics.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, pages 1877-1901.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712.

Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. 2021. Truth, lies, and automation. Center for Security and Emerging Technology, 1(1):2.

Yang Trista Cao and Hal Daumé III. 2021. Toward gender-inclusive coreference resolution: An analysis of gender and bias throughout the machine learning lifecycle. Comput. Linguistics, $47(3): 615-661$.

Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. 2023. Are aligned neural networks adversarially aligned? arXiv preprint arXiv:2306.15447.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633-2650.

Joseph Carlsmith. 2022. Is power-seeking ai an existential risk? arXiv preprint arXiv:2206.13353.

Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, and Sanmi Koyejo. 2023. Deceptive alignment monitoring. arXiv preprint arXiv:2307.10569.

Micah Carroll. 2018. Overview of current ai alignment approaches.

Micah Carroll, Alan Chan, Henry Ashton, and David Krueger. 2023. Characterizing manipulation from ai systems. arXiv preprint arXiv:2303.09387.

Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. 2023. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021a. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.

Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2021b. Textual backdoor attacks can be more harmful via two simple tricks. arXiv preprint arXiv:2110.08247.

Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu. 2012. Detecting offensive language in social media to protect adolescent online safety. In 2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing, pages 71-80. IEEE.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing GPT-4 with $90 \% *$ ChatGPT quality. See https://vicuna. lmsys. org (accessed 14 April 2023).

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,

Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.

Paul Christiano, Buck Shlegeris, and Dario Amodei. 2018. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575.

Marta R Costa-jussà, Pierre Andrews, Eric Smith, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Daniel Licht, and Carleigh Wood. 2023. Multilingual holistic bias: Extending descriptors and patterns to unveil demographic biases in languages at scale. arXiv preprint arXiv:2305.13198.

Andrew Critch and David Krueger. 2020. Ai research considerations for human existential safety (arches). arXiv preprint arXiv:2006.04948.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493-8502, Dublin, Ireland. Association for Computational Linguistics.

Juntao Dai, Xuehai Pan, Jiaming Ji, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. Pku-beaver: Constrained value-aligned llm via safe rlhf. https://github.com/PKUAlignment/safe-rlhf.

Robert Dale. 2021. GPT-3: What's it good for? Natural Language Engineering, 27(1):113-118.

Thomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of the Eleventh International Conference on Web and Social Media, ICWSM 2017, Montréal, Québec, Canada, May 15-18, 2017, pages 512-515. AAAI Press.

Ona de Gibert, Naiara Pérez, Aitor García Pablos, and Montse Cuadros. 2018. Hate speech dataset from a white supremacy forum. In Proceedings of the 2nd Workshop on Abusive Language Online, ALW@EMNLP 2018, Brussels, Belgium, October 31, 2018, pages 11-20. Association for Computational Linguistics.

Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715.

Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, and Minlie Huang. 2022. COLD: A benchmark for chinese offensive language detection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,

Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11580-11599. Association for Computational Linguistics.

Jieren Deng, Yijue Wang, Ji Li, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, and Caiwen Ding. 2021. Tag: Gradient attack on transformer-based language models. arXiv preprint arXiv:2103.06819.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.

Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. 2020. On measuring and mitigating biased inferences of word embeddings. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7659-7666. AAAI Press.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.

Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, KaiWei Chang, and Rahul Gupta. 2021. BOLD: dataset and metrics for measuring biases in open-ended language generation. In FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, pages 862-872. ACM.

Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle. 2019. Addressing age-related bias in sentiment analysis. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 6146-6150. ijcai.org.

Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic, and Narayan Bhamidipati. 2015. Hate speech detection with comment embeddings. In Proceedings of the 24th International Conference on World Wide Web Companion, WWW 2015, Florence, Italy, May 18-22, 2015 - Companion Volume, pages 29-30. ACM.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.

Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:10121031.

Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah. 2022a. Softmax linear units. Transformer Circuits Thread. Https://transformer-circuits.pub/2022/solu/index.html.

Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022b. Toy models of superposition. Transformer Circuits Thread.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread.

Adel Elmahdy, Huseyin A Inan, and Robert Sim. 2022. Privacy leakage in text classification: A data extraction approach. arXiv preprint arXiv:2206.04591.

Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130.

Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. 2021. Latent hatred: A benchmark for understanding implicit hate speech. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 345-363. Association for Computational Linguistics.

Alexander R Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021. Qafacteval: Improved qa-based factual consistency evaluation for summarization. arXiv preprint arXiv:2112.08542.

FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in
the game of diplomacy by combining language models with strategic reasoning. Science, $378(6624): 1067-1074$.

Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José GC de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, et al. 2023. Bridging the gap: A survey on integrating (human) feedback for natural language generation. arXiv preprint arXiv:2305.00955.

Lukas Fluri, Daniel Paleka, and Florian Tramèr. 2023. Evaluating superhuman models with consistency checks. arXiv preprint arXiv:2306.09983.

Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. arXiv preprint arXiv:2011.00620.

Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.

Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835-10866. PMLR.

Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, and Hyoungshick Kim. 2020. Backdoor attacks and countermeasures on deep learning: A comprehensive review. arXiv preprint arXiv:2007.10760.

Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2020. Towards understanding gender bias in relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 2943-2953. Association for Computational Linguistics.

GDPR. General data protection regulation. https://gdpr-info.eu/.

Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462.

Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30-45.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495.

Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.

Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. 2023. Aligning language models with preferences through f-divergence minimization. arXiv preprint arXiv:2302.08215.

Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. 2023. A survey of adversarial defenses and robustness in nlp. ACM Computing Surveys, 55(14s):1-39.

Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv preprint arXiv:2302.12173.

Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language models: Survey and taxonomy. arXiv preprint arXiv:2202.06862.

Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. 2022. Recovering private text in federated learning of language models. Advances in Neural Information Processing Systems, 35:8130-8143.

Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. 2017. Inverse reward design. Advances in neural information processing systems, 30.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3309-3326. Association for Computational Linguistics.

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275.

Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. 2021. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916.

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001.

Seth Herd. 2023. Agentized LLMs will change the alignment landscape. https: //www.lesswrong.com/posts/dcoxvEhAfYcov2LA6/agentized-llms-will-change-thealignment-landscape.

Sorami Hisamoto, Matt Post, and Kevin Duh. 2020. Membership inference attacks on sequenceto-sequence models: Is my data in your machine translation system? Transactions of the Association for Computational Linguistics, 8:49-63.

Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.

Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991.

Saghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. An empirical study of metrics to measure representational harms in pre-trained language models. arXiv preprint arXiv:2301.09211.

Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9118-9147. PMLR.

Yue Huang, Qihui Zhang, Lichao Sun, et al. 2023. Trustgpt: A benchmark for trustworthy and responsible large language models. arXiv preprint arXiv:2306.11507.

Yufei Huang and Deyi Xiong. 2023. Cbbq: A chinese bias benchmark dataset curated with human-ai collaboration for large language models. arXiv preprint arXiv:2306.16244.

Evan Hubinger. 2019a. Concrete experiments in inner alignment. https: //www.lesswrong.com/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inneralignment.

Evan Hubinger. 2019b. Relaxed adversarial training for inner alignment. https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarialtraining-for-inner-alignment.

Evan Hubinger. 2020a. AI safety via market making. https://www.lesswrong.com/posts/ YWwzccGbcHMJMpT45/ai-safety-via-market-making.

Evan Hubinger. 2020b. An overview of 11 proposals for building safe advanced ai. arXiv preprint arXiv:2012.07532.

Evan Hubinger. 2022a. A transparency and interpretability tech tree. https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-andinterpretability-tech-tree.

Evan Hubinger. 2022b. Monitoring for deceptive alignment. https://www.lesswrong.com/ posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment.

Evan Hubinger, Chris van Merwijk, Vlad Mikulik, Joar Skalse, and Scott Garrabrant. 2019a. Deceptive alignment. https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/ zthDPAjh9w6Ytbeks.

Evan Hubinger, Chris van Merwijk, Vlad Mikulik, Joar Skalse, and Scott Garrabrant. 2019b. The inner alignment problem. https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/ the-inner-alignment-problem.

Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019c. Risks from learned optimization in advanced machine learning systems. CoRR, abs $/ 1906.01820$.

Geoffrey Irving and Amanda Askell. 2019. Ai safety needs social scientists. Distill, 4(2):e14.

Geoffrey Irving, Paul Christiano, and Dario Amodei. 2018. Ai safety via debate. arXiv preprint arXiv:1805.00899.

Kwan Yuen Iu and Vanessa Man-Yi Wong. 2023. ChatGPT by openai: The end of litigation lawyers? Available at SSRN.

Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks V. S. Lakshmanan. 2020. Automatic detection of machine generated text: A critical survey.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.

Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381.

Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. 2023. Backdoor attacks for in-context learning with language models. arXiv preprint arXiv:2307.14692.

Jonathan W Kanter, Monnica T Williams, Adam M Kuczynski, Katherine E Manbeck, Marlena Debreaux, and Daniel C Rosen. A preliminary report on the relationship between microaggressions against black people and racism among white college students.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Brendan Kennedy, Mohammad Atari, Aida Mostafazadeh Davani, Leigh Yeh, Ali Omrani, Yehsong Kim, Kris Coombs, Shreya Havaldar, Gwenyth Portillo-Wightman, Elaine Gonzalez, Joe Hoover, Aida Azatian, Alyzeh Hussain, Austin Lara, Gabriel Cardenas, Adam Omary, Christina Park, Xin Wang, Clarisa Wijaya, Yong Zhang, Beth Meyerowitz, and Morteza Dehghani. 2022. Introducing the gab hate corpus: defining and applying hate-based rhetoric to social media posts at scale. Language Resources and Evaluation, 56(1):79-108.

Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021. Alignment of language agents. arXiv preprint arXiv:2103.14659.

Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional transformer language model for controllable generation. CoRR, abs $/ 1909.05858$.

Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo. 2023. Aligning large language models through synthetic feedback. arXiv preprint arXiv:2305.13735.

J Kirchner, L Smith, and J Thibodeau. 2022. Understanding ai alignment research: A systematic analysis. arXiv preprint arXiv:2206.02841.

Svetlana Kiritchenko and Saif M. Mohammad. 2018. Examining gender and race bias in two hundred sentiment analysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, *SEM@NAACL-HLT 2018, New Orleans, Louisiana, USA, June 5-6, 2018, pages 43-53. Association for Computational Linguistics.

Victoria Krakovna. 2022. Paradigms of ai alignment: components and enablers.

Victoria Krakovna and Janos Kramar. 2023. Power-seeking can be probable and predictive for trained agents. arXiv preprint arXiv:2304.06528.

Sumit Kumar and Raj Ratn Pranesh. 2021. Tweetblm: A hate speech dataset and analysis of black lives matter-related microblogs on twitter. arXiv preprint arXiv:2108.12521.

Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022. Summac: Re-visiting nli-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177.

Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:34586-34599.

Beren Millidge Lee Sharkey, Dan Braun. 2022a. Taking features out of superposition with sparse autoencoders.

Beren Millidge Lee Sharkey, Sid Black. 2022b. Current themes in mechanistic interpretability research.

Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron C Wallace. 2021. Does bert pretrained on clinical notes reveal sensitive data? arXiv preprint arXiv:2104.07762.

Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871.

Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. 2017. Ai safety gridworlds. arXiv preprint arXiv:1711.09883.

Hector J. Levesque. 2011. The winograd schema challenge. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI.

Mike Lewis, Denis Yarats, Yann N. Dauphin, Devi Parikh, and Dhruv Batra. 2017. Deal or no deal? end-to-end learning for negotiation dialogues.

Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. 2023a. Multi-step jailbreaking privacy attacks on ChatGPT. arXiv preprint arXiv:2304.05197.

Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b. Inference-time intervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341.

Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. 2021a. Backdoor attacks on pre-trained models by layerwise weight poisoning. arXiv preprint arXiv:2108.13888.

Ruosen Li, Teerth Patel, and Xinya Du. 2023c. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762.

Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, and Jialiang Lu. 2021b. Hidden backdoors in human-centric language models. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, pages $3123-3140$.

Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, and Vivek Srikumar. 2020. Unqovering stereotyping biases via underspecified questions. arXiv preprint arXiv:2010.02428.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023d. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.

Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. 2022. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems.

Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2023e. Guiding large language models via directional stimulus prompting. arXiv preprint arXiv:2302.11520.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.

Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.

Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of ai solutions for environment. arXiv preprint arXiv:2110.11822.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.

Zachary C. Lipton. 2017. The mythos of model interpretability.

Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023a. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676.

Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony Liu, and Soroush Vosoughi. 2022a. Second thoughts are best: Learning to re-align with human values from text edits. Advances in Neural Information Processing Systems, 35:181-196.

Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi. 2023b. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960.

Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush Vosoughi. 2022b. Aligning generative language models with human values. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 241-252.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. GPTeval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634.

Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023d. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. arXiv preprint arXiv:2308.05374.

Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023e. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.

Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 real-life anecdotes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13470-13479.

Li Lucy and David Bamman. 2021. Gender and representation bias in GPT-3 generated stories. In Proceedings of the Third Workshop on Narrative Understanding, pages 48-55.

Sean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina Russell, Nazli Goharian, and Ophir Frieder. 2019. Hate speech detection: Challenges and solutions. PloS one, 14(8):e0221152.

Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410.

Veronica Root Martinez. 2020. More meaningful ethics. U. Chi. L. Rev. Online, page 53.

RT McAllister, Yarin Gal, Alex Kendall, Mark Van Der Wilk, Amar Shah, Roberto Cipolla, and Adrian Weller. 2017. Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning. International Joint Conferences on Artificial Intelligence, Inc.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, $35: 17359-17372$.

Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. CoRR, abs/2302.07842.

Vladimir Mikulik. 2019. 2-d robustness. https://www.alignmentforum.org/posts/ 2mhFMgtAjFJesaSYR/2-d-robustness.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251.

Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. 2022. Quantifying privacy risks of masked language models using membership inference attacks. arXiv preprint arXiv:2203.03929.

Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, and Hadi Esmaeilzadeh. 2020. Privacy in deep learning: A survey. arXiv preprint arXiv:2004.12254.

Kevin L Nadal. 2018. Microaggressions and traumatic stress: Theory, research, and clinical treatment. American Psychological Association.

Kevin L Nadal, Katie E Griffin, Yinglee Wong, Sahran Hamit, and Morgan Rasmus. 2014. The impact of racial microaggressions on mental health: Counseling implications for clients of color. Journal of Counseling \& Development, 92:57.

Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.

Neel Nanda. 2022. A comprehensive mechanistic interpretability explainer \& glossary.

Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. 2023. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. Crows-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 1953-1967. Association for Computational Linguistics.

Andrew Y Ng and Stuart J Russell. 2000. Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 663-670.

Richard Ngo. 2022. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626.

Debora Nozza, Federico Bianchi, Dirk Hovy, et al. 2021. Honest: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.

Chris Olah. 2022. Mechanistic interpretability, variables, and the importance of interpretable bases.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and induction heads. Transformer Circuits Thread.

Stephen M Omohundro. 2008. The basic ai drives. In AGI, volume 171, pages 483-492.

OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt/.

OpenAI. 2023a. GPT-4 technical report. arXiv preprint arXiv:2303.08774.

OpenAI. 2023b. Introducing Superalignment. https://openai.com/blog/introducingsuperalignment.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188.

Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 2086-2105. Association for Computational Linguistics.

Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. 2022. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251.

Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.

Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. 2021. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. arXiv preprint arXiv:2105.12400.

Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. 2023. Visual adversarial examples jailbreak large language models. arXiv preprint arXiv:2306.13213.

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023. Tool learning with foundation models. CoRR, abs/2304.08354.

Shilin Qiu, Qihe Liu, Shijie Zhou, and Wen Huang. 2022. Adversarial attack and defense technologies in natural language processing: A survey. Neurocomputing, 492:278-307.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Ansh Radhakrishnan. 2022. RLHF. https://www.lesswrong.com/posts/ rQH4gRmPMJyjtMpTn/rlhf.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \& insights from training gopher. arXiv preprint arXiv:2112.11446.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.

Amir H Razavi, Diana Inkpen, Sasha Uritsky, and Stan Matwin. 2010. Offensive language detection using multi-level classification. In Advances in Artificial Intelligence: 23rd Canadian Conference on Artificial Intelligence, Canadian AI 2010, Ottawa, Canada, May 31-June 2, 2010. Proceedings 23, pages 16-27. Springer.

Adithya Renduchintala and Adina Williams. 2021. Investigating failures of automatic translation in the case of unambiguous gender. CoRR, abs/2104.07838.

Maria Rigaki and Sebastian Garcia. 2020. A survey of privacy attacks in machine learning. arXiv preprint arXiv:2007.07646.

Björn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki. 2017. Measuring the reliability of hate speech annotations: The case of the european refugee crisis. arXiv preprint arXiv:1701.08118.

Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301.

Stuart J Russell. 1998. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on Computational learning theory, pages 101-103.

Stuart J Russell and Peter Norvig. 2010. Artificial intelligence a modern approach. Pearson Education, Inc.

Muniba Saleem and Craig A Anderson. Arabs as terrorists: Effects of stereotypes within violent contexts on attitudes, perceptions, and affect.

Jonas B Sandbrink. 2023. Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools. arXiv preprint arXiv:2306.13952.

Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A Smith, and Yejin Choi. 2019. Social bias frames: Reasoning about social and power implications of language. arXiv preprint arXiv:1911.03891.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100.

Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2023. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755.

Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9:1408-1424.

Anna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural language processing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, SocialNLP@EACL 2017, Valencia, Spain, April 3, 2017, pages 1-10. Association for Computational Linguistics.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

Shalom H Schwartz, Jan Cieciuch, Michele Vecchione, Eldad Davidov, Ronald Fischer, Constanze Beierlein, Alice Ramos, Markku Verkasalo, Jan-Erik Lönnqvist, Kursad Demirutku, et al. 2012. Refining the theory of basic individual values. Journal of personality and social psychology, 103(4):663.

Charbel-Raphaël Segerie. 2023. Task decomposition for scalable oversight (AGISF Distillation). https://www.lesswrong.com/posts/FFz6H35Gy6BArHxkc/task-decompositionfor-scalable-oversight-agisf-distillation.

Deven Shah, H Andrew Schwartz, and Dirk Hovy. 2019. Predictive biases in natural language processing models: A conceptual framework and overview. arXiv preprint arXiv:1912.11078.

Rohin Shah. 2023. Categorizing failures as "outer" or "inner" misalignment is often confused. https://www.lesswrong.com/posts/JKwrDwsaRiSxTv9ur/categorizingfailures-as-outer-or-inner-misalignment-is.

Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton. 2022. Goal misgeneralization: Why correct specifications aren't enough for correct goals. arXiv preprint arXiv:2210.01790.

Xuan Sheng, Zhaoyang Han, Piji Li, and Xiangmao Chang. 2022. A survey on backdoor attack and defense in natural language processing. In 2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS), pages 809-820. IEEE.

Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. 2023. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324.

Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. 2023. BadGPT: Exploring security vulnerabilities of ChatGPT via backdoor attacks to InstructGPT. arXiv preprint arXiv:2304.12298.

Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022. "i'm sorry to hear that": Finding new biases in language models with a holistic descriptor dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9180-9211.

Nate Soares. 2015a. Aligning superintelligence with human interests: An annotated bibliography. Intelligence, 17(4):391-444.

Nate Soares. 2015b. Research Guide - Machine Intelligence Research Institute. https: //intelligence.org/research-guide.

Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. 2022. Llm-planner: Few-shot grounded planning for embodied agents with large language models. CoRR, abs/2212.04088.

Congzheng Song and Ananth Raghunathan. 2020. Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security, pages 377-390.

Congzheng Song and Vitaly Shmatikov. 2019. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery $E^{2}$ Data Mining, pages 196-206.

Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2023. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492.

Wiktor Soral, Michał Bilewicz, and Mikołaj Winiewski. 2018. Exposure to hate speech increases prejudice through desensitization. Aggressive behavior, 44(2):136-146.

Samuel Sousa and Roman Kern. 2023. How to keep text private? a systematic review of deep learning methods for privacy-preserving natural language processing. Artificial Intelligence Review, 56(2):1427-1492.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.

Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1679-1684. Association for Computational Linguistics.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021.

Jonathan Stray. 2020. Aligning ai optimization to community well-being. International Journal of Community Well-Being, 3(4):443-463.

Jonathan Stray, Ivan Vendrov, Jeremy Nixon, Steven Adler, and Dylan Hadfield-Menell. 2021. What are you optimizing for? aligning recommender systems with human values. arXiv preprint arXiv:2107.10939.

Derald Wing Sue, Christina M Capodilupo, Gina C Torino, Jennifer M Bucceri, Aisha Holder, Kevin L Nadal, and Marta Esquilin. 2007. Racial microaggressions in everyday life: Implications for clinical practice. American Psychologist, 62(4):271-286.

Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023a. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436.

Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023b. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047.

Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models.

Yi Tay, Donovan Ong, Jie Fu, Alvin Chan, Nancy Chen, Anh Tuan Luu, and Christopher Pal. 2020. Would you rather? a new benchmark for learning machine alignment with cultural values and social preferences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5369-5373.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, MarieAnne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.

Alex Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. 2021. Optimal policies tend to seek power. In Advances in Neural Information Processing Systems, volume 34, pages 23063-23074. Curran Associates, Inc.

Alex Turner and Prasad Tadepalli. 2022. Parametrically retargetable decision-makers tend to seek power. In Advances in Neural Information Processing Systems, volume 35, pages 31391-31401. Curran Associates, Inc.

Aimee Van Wynsberghe. 2021. Sustainable ai: Ai for sustainability and the sustainability of ai. AI and Ethics, 1(3):213-218.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000-6010.

Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. 2021. Learning from the worst: Dynamically generated datasets to improve online hate detection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 1667-1682. Association for Computational Linguistics.

Giulia Vilone and Luca Longo. 2020. Explainable artificial intelligence: a systematic review.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125.

Eric Wallace, Tony Z Zhao, Shi Feng, and Sameer Singh. 2020. Concealed data poisoning attacks on nlp models. arXiv preprint arXiv:2010.12563.

Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022a. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.

Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023b. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087.

Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023c. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022b. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.

Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023d. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966.

William Warner and Julia Hirschberg. 2012. Detecting hate speech on the world wide web. NAACL-HLT 2012, page 19.

Zeerak Waseem. 2016. Are you a racist or am I seeing things? annotator influence on hate speech detection on twitter. In Proceedings of the First Workshop on NLP and Computational Social Science, NLP+CSS@EMNLP 2016, Austin, TX, USA, November 5, 2016, pages 138-142. Association for Computational Linguistics.

Zeerak Waseem and Dirk Hovy. 2016. Hateful symbols or hateful people? predictive features for hate speech detection on twitter. In Proceedings of the NAACL student research workshop, pages 88-93.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.

John Wentworth. 2020. "Inner alignment failures" which are actually outer alignment failures. https://www.lesswrong.com/posts/HYERofGZE6j9Tuigi/inner-alignmentfailures-which-are-actually-outer-alignment.

Norbert Wiener. 1960. Some moral and technical consequences of automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers. Science, $131(3410): 1355-1358$.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In Proceedings of the 26th international conference on world wide web, pages $1391-1399$.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.

Canwen Xu, Zexue He, Zhankui He, and Julian McAuley. 2022. Leashing the inner demons: Self-detoxification for language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11530-11537.

Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023b. A critical evaluation of evaluations for long-form question answering. arXiv preprint arXiv:2305.18201.

Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079.

Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2021. Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models. arXiv preprint arXiv:2103.15543.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754-5764.

Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302.

Eliezer Yudkowsky. 2004. Coherent extrapolated volition. Singularity Institute for Artificial Intelligence.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Predicting the type and target of offensive posts in social media. arXiv preprint arXiv:1902.09666.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130B: an open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.

Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian. 2021. Pangu- $\alpha$ : Large-scale autoregressive pretrained chinese language models with auto-parallel computation. CoRR, abs/2104.12369.

Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. Alignscore: Evaluating factual consistency with a unified alignment function. arXiv preprint arXiv:2305.16739.

Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu, and In So Kweon. 2021. A survey on universal adversarial attack. arXiv preprint arXiv:2103.01498.

Ge Zhang, Yizhi Li, Yaoyao Wu, Linyuan Zhang, Chenghua Lin, Jiayi Geng, Shi Wang, and Jie Fu. 2023. Corgi-pm: A chinese corpus for gender bias probing and mitigation. arXiv preprint arXiv:2301.00395.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,
and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068.

Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial attacks on deep-learning models in natural language processing: A survey. ACM Transactions on Intelligent Systems and Technology (TIST), 11(3):1-41.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15-20.

Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and Jie Fu. 2023a. Prompt as triggers for backdoor attack: Examining the vulnerability in language models. arXiv preprint arXiv:2305.01219.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023b. A survey of large language models. arXiv preprint arXiv:2303.18223.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. 2023c. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425.

Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. 2022. Calibrating sequence likelihood improves conditional language generation. arXiv preprint arXiv:2210.00045.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.

Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong, Lu Chen, et al. 2023b. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023a. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.

Jingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang, Qun Liu, and Helen Meng. 2022. Towards identifying social bias in dialog systems: Frame, datasets, and benchmarks. arXiv preprint arXiv:2202.08011.

Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023b. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439.

Banghua Zhu, Jiantao Jiao, and Michael I Jordan. 2023. Principled reinforcement learning with human feedback from pairwise or $k$-wise comparisons. arXiv preprint arXiv:2301.11270.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. 2008. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.

Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.


[^0]:    Email: \{thshen, rrjin, yuki_731, liuc_09, willowd, guozishan, wuxw2021, yan_liu, dyxiong\}@tju.edu.cn

    * Corresponding author.

[^1]:    ${ }^{1}$ Here we borrow terms "risk landscape", "established/observed risks", "anticipated risks" from (Weidinger et al., 2021). But unlike them, we use "established risks" and "anticipated risks" in a broader and coarser perspective.

[^2]:    ${ }^{2}$ http://edge.org/conversation/the-myth-of-ai\#26015

[^3]:    ${ }^{3}$ https://cacm.acm.org/news/217198-father-of-the-internet-ai-stands-for-artificial-idiot/fulltext

    ${ }^{4}$ https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6

[^4]:    ${ }^{5}$ https://laptrinhx.com/formally-stating-the-ai-alignment-problem-223323934/

    ${ }^{6}$ https://www.alignmentforum.org/tag/outer-alignment

    ${ }^{7}$ https://www.alignmentforum.org/tag/goodhart-s-law

[^5]:    ${ }^{8}$ https://www.alignmentforum.org/tag/inner-alignment

    ${ }^{9}$ https://generative.ink/alternet/paperclip-maximizer-wikipedia.html

[^6]:    ${ }^{10}$ Other definitions of inner alignment are also circulated in the alignment community. Please refer to Arike (2022) for more discussions.

[^7]:    ${ }^{11}$ https://perspectiveapi.com/

[^8]:    ${ }^{12}$ https://www.alignmentforum.org/tag/deceptive-alignment

