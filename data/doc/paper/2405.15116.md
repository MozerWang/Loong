# Quantifying the Gain in Weak-to-Strong Generalization 

Moses Charikar<br>Stanford University<br>moses@cs.stanford.edu

Chirag Pabbaraju<br>Stanford University<br>cpabbara@cs.stanford.edu

Kirankumar Shiragur<br>Microsoft Research<br>kshiragur@microsoft.com


#### Abstract

Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. \$\left[\mathrm{BIK}^{+}\right.\$23] empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts-a phenomenon they term weak-to-strong generalization.

In this work, we present a theoretical framework for understanding weak-tostrong generalization. Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the misfit error incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error. We validate our theoretical findings through various empirical assessments.


## 1 Introduction

Present-day AI models demonstrate incredible capabilities at a variety of extremely difficult tasks. For this reason, they are frequently described as being superhuman, in that it seems hard to imagine a human displaying the same abilities as the AI model. For example, move 37 in AlphaGo's famous victory against Go expert Lee Sedol [Met16] has been described as being beyond the realm of human imagination. In this sense, today's AI models are well on the path of exhibiting new and emergent abilities $\left[\mathrm{WTB}^{+} 22\right]$. Ultimately, we want these new abilities to be aligned with what would be beneficial to humanity. This rationale is what primarily guides the training of large-scale AI models through human feedback \$\left[\mathrm{CLB}^{+}\right.\$17]. However, given that we expect AI models to pick up skills that we ourselves don't fully grasp as humans, how can we enable these highly capable models to realize their potential?

A recent work by $\left[\mathrm{BIK}^{+} 23\right]$ shows that not all hope is lost in this endeavor. To model humans as being weak supervisors for increasingly strong AI models, they conduct the following "weak-to-strong generalization" experiment. Suppose we finetune a small language model like GPT-2 $\left[\mathrm{RWC}^{+} 19\right]$ on data with ground-truth labels for a task. What happens if we then finetune a large language model like GPT-4 [Ope23a] on data labeled by GPT-2, instead of data having ground-truth labels? Would GPT-4 simply overfit to GPT-2's labels and do no better, or would it outperform GPT-2, given that it is inherently a much stronger model? The surprising experimental result is that GPT-4 trained in this manner outperforms GPT-2 when evaluated on the true data, for a variety of finetuning tasks. Note that GPT-4 is able to outperform GPT-2 without ever seeing true labels when it was being finetuned.

One plausible explanation for this is that GPT-4 was able to glean the essence of the finetuning task from GPT-2's labels, and since it is fundamentally a stronger model than GPT-2, this knowledge was sufficient for it to outperform GPT-2. ${ }^{1}$

In this work, we seek theoretical justification for why we might expect to see such a gain in accuracy in weak-to-strong generalization. Concretely, we ask:

Does a weakly supervised strong model provably attain smaller error than its weak supervisor, and if so, can this gain be formally quantified?

Towards answering this question, we show (Theorem 1) that the true error of a strong model trained on weak labels is smaller than the error of the weak model, by at least the error of the strong model on the weak labels itself. We call this latter quantity the misfit between the weak and strong model. Our result can be stated as the following simple principle:

Gain in accuracy in weak-to-strong generalization $\approx$ Misfit between the weak and strong model

Intuitively, the misfit quantifies the erroneous knowledge that the strong model does not obtain from the weak model, and hence also the amount that the strong model improves over the weak model.

Key to obtaining our results is a representation-theoretic perspective [TJJ20] towards weak-to-strong generalization. We posit that the main difference between weak and strong models is in the disparity between the quality of their data representations. This disparity in representation quality can manifest, among other reasons, due to a difference in the expressivity and complexity of the weak and strong models, and the amount of pretraining data that they have seen. For example, in the experiments by $\left[\mathrm{BIK}^{+} 23\right]$, the weak and strong models used are GPT-2 and GPT-4 respectively; the latter is a significantly larger transformer architecture, pretrained on a much larger dataset than the former. As a broader analogy, consider the task of learning a new language. This is an easier task for a multilingual person than a monolingual person. A multilingual person has a richer representation for language, drawing from their knowledge of different syntax, lexical structures, and sounds in multiple languages. With this perspective, we can imagine finetuning tasks to be relatively simple functions (e.g., linear functions) composed with the appropriate representation. For example, if the task is about learning Italian, a suitable "Italian-specific" linear combination of the multilingual's representation of the problem (including features learned from Spanish and French, say) might allow them to better understand the new language, while the same might not work so well for a monolingual whose representation only has features learned from English.

Armed with this perspective, we model the task of learning a real-valued finetuning task under the least squares loss in the weak-to-strong generalization framework. We assume that there exists a ground-truth representation $h^{\star}$ of the data, which makes it amenable to learn a finetuning task $f^{\star}$ of interest. We imagine that the weak and strong models come equipped with representation maps $h_{w}$ and $h_{s}$ respectively, which are possibly obtained via pretraining on a corpus of data. Next, we imagine that the weak model sees data labeled by the target function $f^{\star} \circ h^{\star}$, and after finetuning, learns some arbitrary function $f_{w} \circ h_{w}$. At this point, the weak supervision pipeline begins. The strong model is fed with data labeled by $f_{w} \circ h_{w}$ (instead of the true labels $f^{\star} \circ h^{*}$ ), and as part of finetuning, outputs a function $f_{s w}$ from a function class $\mathcal{F}_{s}$, that minimizes the discrepancy between $f_{s w} \circ h_{s}$ and the data labeled by $f_{w} \circ h_{w}$ that it sees. Ultimately, we care about the error of $f_{s w} \circ h_{s}$ with respect to the true finetuning task, namely $f^{\star} \circ h^{\star}$. Our main result (Theorem 1) precisely quantifies the gain in the accuracy of $f_{s w} \circ h_{s}$ over $f_{w} \circ h_{w}$ in terms of the misfit between them, under the assumption that the set of functions $\mathcal{F}_{s}$ is a convex set. ${ }^{2}$ In many practical applications, the representation map is generally the forward pass of the data through a suitable neural network architecture, and the finetuning task is performed by the last linear layer \$\left[\mathrm{KRJ}^{+}\right.\$22] of the network. In such cases, our assumption that the set $\mathcal{F}_{s}$ is convex readily holds true.

We validate our characterization of the gain in weak-to-strong generalization through various experiments (Section 5) on synthetic and real-world data. The experiments corroborate our theoretical findings. Namely, we observe that upon performing the above weak-to-strong supervision pipeline,[^0]the gain in accuracy of the weakly-supervised strong model over its weak supervisor more or less exactly aligns with the misfit between the weak and strong models (Figure 2). We also demonstrate (Section 5.3) that the labels "weak" and "strong" models are nuanced and not solely dependent on expressive power; in fact, in a low-sample regime, a less expressive model produces a higher quality representation and should be considered a strong model. Our theory and experiments lead to several algorithmic insights and open up interesting questions. For example, one algorithmic heuristic that arises from our theory is the following: given access to different weak models, choose to deploy the strong model that achieves the smallest difference between the weak model error and misfit (Table 1). Our results also motivate the perhaps counterintuitive algorithmic question of obtaining weak models that lead to large misfits with the strong model. ${ }^{3}$ Another possible line of inquiry could look into ensembling across different weak models, and obtaining a gain close to the sum of their individual misfits. At a more philosophical level, this is akin to a superhuman AI model assimilating knowledge from various humans, while correctly identifying and discarding each of their flaws.

## 2 Related Work and Preliminaries

### 2.1 Related Work

The idea of converting a "weak" learner to a "strong" learner can be traced all the way back to the famous paradigm of boosting [Fre95, FS97], if not earlier. The recent work by [BIK \${ }^{+}\$23] frames this problem within the context of superalignment $\left[\mathrm{Ope} 23 \mathrm{~b}, \mathrm{JQC}^{+} 23\right]$ which seeks to reliably align AI models smarter than humans to human intent. Thereafter, several works that study the training of a "strong" model guided in some capacity by a "weak" model have emerged. Some of these include instruction filtering by weak models \$\left[\mathrm{LZH}^{+}\right.\$24], easy-to-hard generalization [SYS \${ }^{+}\$24], weak-to-strong correction \$\left[\mathrm{JCL}^{+}\right.\$24] and weak-to-strong hallucination inducement [ZCBS23].

The weak-to-strong generalization paradigm is perhaps most closely related to the teacher-student model of training [LA16, TV17] (sometimes also referred to as knowledge distillation [HVD15, GYMT21]), where a student model (typically smaller) is trained using data labeled by a teacher model (typically larger), and possibly some additional ground-truth data. The remarkable phenomenon of the student model outperforming the teacher has been observed in many works [BCNM06, HVD15, \$\mathrm{FLT}^{+}\$18]. Most relevant to us are formulations where the student model is equally \$\left[\mathrm{FLT}^{+}\right.\$18] or more powerful [XLHL20] than the teacher model. There has been theoretical work explaining superior generalization in the setting where the student and teacher models are equally powerful, e.g., the work of [MFB20] where the student also has access to ground-truth labels, or the work of [WSCM20], where the model is trained on its own labels (known as self-training) with a regularization loss. In contrast, our work aims to explain why we might expect such behavior when the student is more powerful, and also doesn't have any access to ground-truth labels.

### 2.2 Preliminaries

We assume that the data domain is $\mathbb{R}^{d}$, and assume that there exists a ground truth representation function $h^{\star}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d^{\star}}$ that maps the data $x$ to an enriched representation $h^{\star}(x)$. We assume the existence of pretraining tasks, through which strong models obtain representations of the data from a function class $\mathcal{H}_{s}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{s}}$, and weak models obtain representations from a function class $\mathcal{H}_{w}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{w}}$. For example, $\mathcal{H}_{s}$ can be the class of deep neural networks, and $\mathcal{H}_{w}$ can be the class of shallow neural networks. The target finetuning task (composed with the ground truth representation) is denoted as $f^{\star} \circ h^{\star}$, and the function learnt by the weak model is denoted by $f_{w} \circ h_{w}$. We assume that the strong model learns finetuning tasks from a function class $\mathcal{F}_{s}: \mathbb{R}^{d_{s}} \rightarrow \mathbb{R}$, and assume that the set $\mathcal{F}_{s}$ is a convex set. The convexity assumption requires that, for any $f, g \in \mathcal{F}_{s}$, and for any $\lambda \in[0,1]$, there exists $h \in \mathcal{F}_{s}$ such that for all $z \in \mathbb{R}^{d_{s}}, h(z)=\lambda f(z)+(1-\lambda) g(z)$. For example, $\mathcal{F}_{s}$ can be the class of all linear functions from $\mathbb{R}^{d_{s}}$ to $\mathbb{R}$. However, we do not assume anything about either $f^{\star}$ or $f_{w}$; in particular, they need not belong to $\mathcal{F}_{s}$. We denote the marginal data distribution by $\mathcal{P}$. For any two functions $f, g: \mathbb{R}^{d} \rightarrow \mathbb{R}$, we define the distance $d_{\mathcal{P}}(f, g)=\mathbb{E}_{x \sim \mathcal{P}}(f(x)-g(x))^{2}$, i.e., it is the average (with respect to $\mathcal{P}$ ) squared distance between the images of the functions.[^1]

## 3 Results

We first state a quantitative version of our main result that characterizes the gain in weak-to-strong generalization in terms of strong-to-weak misfit in the so-called realizable setting. Namely, we assume that the target finetuning task $f^{\star} \circ h^{\star}$ can be equivalently written as $f_{s} \circ h_{s}$ for some $f_{s} \in \mathcal{F}_{s}$.

Theorem 1 (Weak-to-Strong Generalization under Realizability). Let $h^{\star}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d^{\star}}$ be a ground truth representation map, and let $f^{\star}: \mathbb{R}^{d^{\star}} \rightarrow \mathbb{R}$ be a finetuning task of interest. Let $h_{s}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{s}}$ and $h_{w}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{w}}$ be the strong and weak model representation maps respectively. Given some data labeled by $f^{\star} \circ h^{\star}$, let $f_{w} \circ h_{w}$ be the function learnt by the weak model, for some arbitrary function $f_{w}: \mathbb{R}^{d_{w}} \rightarrow \mathbb{R}$. Now, for a convex set of functions $\mathcal{F}_{s}$ mapping $\mathbb{R}^{d_{s}}$ to $\mathbb{R}$ let

$$
\begin{equation*}
f_{s w}=\operatorname{argmin}_{f \in \mathcal{F}_{s}} d_{\mathcal{P}}\left(f \circ h_{s}, f_{w} \circ h_{w}\right) \tag{1}
\end{equation*}
$$

be the function learnt by the strong model under weak supervision. Lastly, let us assume that there exists $f_{s} \in \mathcal{F}_{s}$ such that $f_{s} \circ h_{s}=f^{\star} \circ h^{\star}$. Then, we have that

$$
\begin{equation*}
d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f^{\star} \circ h^{\star}\right) \leq d_{\mathcal{P}}\left(f_{w} \circ h_{w}, f^{\star} \circ h^{\star}\right)-d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f_{w} \circ h_{w}\right) \tag{2}
\end{equation*}
$$

On the left-hand side in (2) is the error of the weakly-supervised strong model on the true data. The first term on the right-hand side is the true error of the weak model, and the second term is the error of the weakly-supervised strong model on data labeled by the weak model (i.e., misfit). Thus, the inequality directly says that the weakly-supervised strong model improves over the weak model by (at least) an amount equal to the misfit. Note again that in practice, a popular way to finetune a pretrained model on task-specific data is by tuning the weights of only the last linear layer of the model. In these cases, $\mathcal{F}_{s}$ is simply the set of linear functions, which is convex. We emphasize that neither of $f^{\star}$ or $f_{w}$ need to belong to $\mathcal{F}_{s}$; as long as the strong model finds the minimizer over a convex set of the loss on the weakly labeled data (as in (1)), the inequality in (2) holds.

Next, we relax the realizability assumption that the target task $f^{\star} \circ h^{\star}$ belongs to the space of functions that the strong model optimizes over. Instead, suppose that by composing $h_{s}$ with functions in $\mathcal{F}_{s}$, it is possible for the strong model to get a small distance $\varepsilon$ to the target task. The strong model could obtain such a powerful representation map after having seen an abundance of pretraining data; the realizable case corresponds to $\varepsilon=0$. We also relax the assumption that the strong model is able to obtain the true minimizer with respect to the data distribution $\mathcal{P}$ as in (1). In reality, we can imagine that the strong model only sees a finite sample labeled by the weak model, and obtains $\hat{f}_{s w}$ by minimizing the loss over this finite sample. Even with these relaxations, we can show that the same qualitative result as in Theorem 1 continues to hold, upto small error terms.

Theorem 2 (Weak-to-Strong Generalization under Non-Realizability and Finite Samples). Let $h^{\star}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d^{\star}}$ be a ground truth representation map, and let $f^{\star}: \mathbb{R}^{d^{\star}} \rightarrow \mathbb{R}$ be a finetuning task of interest. Let $h_{s}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{s}}$ and $h_{w}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{w}}$ be the strong and weak model representations respectively. Given some data labeled by $f^{\star} \circ h^{\star}$, let $f_{w} \circ h_{w}$ be the function learnt by the weak model, for some arbitrary function $f_{w}: \mathbb{R}^{d_{w}} \rightarrow \mathbb{R}$. For a convex set of functions $\mathcal{F}_{s}$ mapping $\mathbb{R}^{d_{s}} \rightarrow \mathbb{R}$, let

$$
\begin{equation*}
f_{s}=\operatorname{argmin}_{f \in \mathcal{F}_{s}} d_{\mathcal{P}}\left(f \circ h_{s}, f^{\star} \circ h^{\star}\right) \tag{3}
\end{equation*}
$$

and suppose that $d_{\mathcal{P}}\left(f_{s} \circ h_{s}, f^{\star} \circ h^{\star}\right)=\varepsilon$. Now, suppose we obtain $n$ weakly-labeled i.i.d. samples $\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)$, where each $x_{i} \sim \mathcal{P}$ and $y_{i}=f_{w} \circ h_{w}\left(x_{i}\right)$. Let

$$
\begin{equation*}
\hat{f}_{s w}=\operatorname{argmin}_{f \in \mathcal{F}_{s}} \frac{1}{n} \sum_{i=1}^{n}\left(f \circ h_{s}\left(x_{i}\right)-y_{i}\right)^{2} \tag{4}
\end{equation*}
$$

Finally, assume that the range of $f^{\star}, f_{w}$ and all the functions in $\mathcal{F}_{s}$ is absolutely bounded. Then, we have that with probability at least $1-\delta$ over the draw of $\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)$,

$$
\begin{array}{r}
d_{\mathcal{P}}\left(\hat{f}_{s w} \circ h_{s}, f^{\star} \circ h^{\star}\right) \leq d_{\mathcal{P}}\left(f_{w} \circ h_{w}, f^{\star} \circ h^{\star}\right)-d_{\mathcal{P}}\left(\hat{f}_{s w} \circ h_{s}, f_{w} \circ h_{w}\right) \\
+O(\sqrt{\varepsilon})+O\left(\frac{\mathcal{C}_{\mathcal{F}_{s}}}{n}\right)^{\frac{1}{4}}+O\left(\frac{\log (1 / \delta)}{n}\right)^{\frac{1}{4}} \tag{5}
\end{array}
$$

where $\mathcal{C}_{\mathcal{F}_{s}}$ is a constant capturing the complexity of the function class $\mathcal{F}_{s}$, and the asymptotic notation is with respect to $\varepsilon \rightarrow 0, n \rightarrow \infty$.

As compared to (2), the bound in (5) has two sources of error terms: the first error term of $O(\sqrt{\varepsilon})$ arises (via standard triangle inequality arguments) due to the non-realizability assumption, and goes to zero as the strong model becomes stronger and more expressive. The latter two error terms arise (via standard uniform convergence arguments as in [TJJ20]) because the strong model only sees a finite weakly-labeled sample-these terms vanish too as the sample size becomes large.

## 4 Main Proof Technique

In this section, we outline the proof of realizable weak-to-strong generalization (Theorem 1). The proof of Theorem 2 uses the same main idea and is given in Appendix A. Recall that the strong model learns from a convex set $\mathcal{F}_{s}: \mathbb{R}^{d_{s}} \rightarrow \mathbb{R}$ of finetuning tasks. Recall also that we denote the strong model representation map by $h_{s}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{s}}$. Let $V_{s}=\left\{f \circ h_{s}: f \in \mathcal{F}_{s}\right\}$ be the set of all tasks in $\mathcal{F}_{s}$ composed with the strong model representation. We first observe that $V_{s}$ is also a convex set.

Claim 3. $V_{s}$ is a convex set.

Proof. Fix $f, g \in \mathcal{F}_{s}$, and consider $f \circ h_{s}, g \circ h_{s} \in V_{s}$. Fix any $\lambda \in[0,1]$. Since $\mathcal{F}_{s}$ is a convex set, there exists $p \in \mathcal{F}_{s}$ such that for all $y \in \mathbb{R}^{d_{s}}, p(y)=\lambda f(y)+(1-\lambda) g(y)$. Now, fix any $x \in \mathbb{R}^{d}$. Then, we have that

$$
\begin{aligned}
\lambda\left(f \circ h_{s}\right)(x)+(1-\lambda)\left(g \circ h_{s}\right)(x) & =\lambda f\left(h_{s}(x)\right)+(1-\lambda) g\left(h_{s}(x)\right) \\
& =p\left(h_{s}(x)\right)=\left(p \circ h_{s}\right)(x),
\end{aligned}
$$

and hence $\lambda\left(f \circ h_{s}\right)+(1-\lambda)\left(g \circ h_{s}\right)=p \circ h_{s} \in V_{s}$.

We are then ready to prove Theorem 1 .

Proof of Theorem 1. The setting under consideration is depicted in Figure 1. Since we assume realizability, $f^{\star} \circ h^{\star} \in V_{s}$. Let $A=d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f^{\star} \circ h^{\star}\right), B=d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f_{w} \circ h_{w}\right)$ and $C=d_{\mathcal{P}}\left(f_{w} \circ h_{w}, f^{\star} \circ h^{\star}\right)$. We want to show that $C \geq A+B$. Recall that

$$
f_{s w}=\operatorname{argmin}_{f \in \mathcal{F}_{s}} d_{\mathcal{P}}\left(f \circ h_{s}, f_{w} \circ h_{w}\right)
$$

In other words, $f_{s w} \circ h_{s}$ is the projection of $f_{w} \circ h_{w}$ onto the convex set $V_{s}$. We can therefore apply the "Pythagorean theorem" for projections onto a convex set [Haz16, Theorem 2.1].

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-05.jpg?height=464&width=531&top_left_y=1557&top_left_x=797)

Figure 1: $f_{s w} \circ h_{s}$ is the projection of $f_{w} \circ h_{w}$ onto the convex set $V_{s}$.

Concretely, for any $g \in V_{s}$, observe that

$$
\begin{align*}
& d_{\mathcal{P}}\left(f_{w} \circ h_{w}, g\right)=\mathbb{E}_{x \sim \mathcal{P}}\left(g(x)-\left(f_{w} \circ h_{w}\right)(x)\right)^{2} \\
& =\mathbb{E}_{x \sim \mathcal{P}}\left(g(x)-\left(f_{s w} \circ h_{s}\right)(x)+\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)^{2} \\
& =\mathbb{E}_{x \sim \mathcal{P}}\left(g(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)^{2}+\mathbb{E}_{x \sim \mathcal{P}}\left(\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)^{2} \\
& \quad+2 \cdot \mathbb{E}_{x \sim \mathcal{P}}\left[\left(g(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)\left(\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)\right] \\
& =d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, g\right)+d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f_{w} \circ h_{w}\right) \\
& \quad+2 \cdot \mathbb{E}_{x \sim \mathcal{P}}\left[\left(g(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)\left(\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)\right] \tag{6}
\end{align*}
$$

But note also that by definition of projection, $d_{\mathcal{P}}\left(f_{w} \circ h_{w}, g\right) \geq d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f_{w} \circ h_{w}\right)$, and hence

$$
\begin{equation*}
d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, g\right)+2 \cdot \mathbb{E}_{x \sim \mathcal{P}}\left[\left(g(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)\left(\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)\right] \geq 0 \tag{7}
\end{equation*}
$$

Now, fix $t \in(0,1)$, and consider the function

$$
w(t)=f_{s w} \circ h_{s}+t \cdot\left(f^{\star} \circ h^{\star}-f_{s w} \circ h_{s}\right)
$$

Namely, for any $x, w(t)(x)=\left(f_{s w} \circ h_{s}\right)(x)+t \cdot\left(\left(f^{\star} \circ h^{\star}\right)(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)$. Because $V_{s}$ is a convex set (Claim 3), $w(t) \in V_{s}$. Also,

$$
\begin{aligned}
d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, w(t)\right) & =\mathbb{E}_{x \sim \mathcal{P}}\left(\left(f_{s w} \circ h_{s}\right)(x)-w(t)(x)\right)^{2} \\
& =t^{2} \cdot \mathbb{E}_{x \sim \mathcal{P}}\left(\left(f^{\star} \circ h^{\star}\right)(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)^{2}
\end{aligned}
$$

Hence, substituting $w(t)$ for $g$ in (7), we get

$$
\begin{aligned}
& t^{2} \cdot \mathbb{E}_{x \sim \mathcal{P}}\left(\left(f^{\star} \circ h^{\star}\right)(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)^{2} \\
& +2 t \cdot \mathbb{E}_{x \sim \mathcal{P}}\left[\left(\left(f^{\star} \circ h^{\star}\right)(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)\left(\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)\right] \geq 0
\end{aligned}
$$

Taking the limit as $t \downarrow 0$, we get that

$$
\begin{equation*}
\mathbb{E}_{x \sim \mathcal{P}}\left[\left(\left(f^{\star} \circ h^{\star}\right)(x)-\left(f_{s w} \circ h_{s}\right)(x)\right)\left(\left(f_{s w} \circ h_{s}\right)(x)-\left(f_{w} \circ h_{w}\right)(x)\right)\right] \geq 0 \tag{8}
\end{equation*}
$$

Substituting $f^{\star} \circ h^{\star}$ for $g$ in (6), and using (8), we obtain the desired result

$$
d_{\mathcal{P}}\left(f_{w} \circ h_{w}, f^{\star} \circ h^{\star}\right) \geq d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f^{\star} \circ h^{\star}\right)+d_{\mathcal{P}}\left(f_{s w} \circ h_{s}, f_{w} \circ h_{w}\right)
$$

## 5 Experiments

We perform experiments on synthetically generated data as well as real-world molecular prediction datasets to verify the guarantees on weak-to-strong generalization given by our theorems.

### 5.1 Synthetic Experiments

We set the target data representation $h^{\star}: \mathbb{R}^{8} \rightarrow \mathbb{R}^{16}$ to be a randomly initialized 5-layer multi-layer perceptron (MLP) with ReLU activations, with input dimension 8 and hidden layer dimension 16. The class $\mathcal{F}_{s}$ of finetuning tasks from which the strong model (as well as the weak model) learns is simply the class of linear functions from $\mathbb{R}^{16} \rightarrow \mathbb{R} ; \mathcal{F}_{s}$ is thus a convex set (see Appendix $\mathrm{C}$ for instances where $\mathcal{F}_{s}$ is a non-convex set). The marginal data distribution $\mathcal{P}$ in our experiments is always $\mathcal{N}\left(0, \sigma^{2} I\right)$. To ensure that the data is well-spread, we set $\sigma=500$.

Representation Learning. We experiment with two different ways of obtaining the weak and strong representations $h_{w}$ and $h_{s}$ :

(1) Pretraining: We randomly sample $T$ finetuning tasks $f^{(1)}, \ldots, f^{(T)} \in \mathcal{F}_{s}$. For each $t \in[T]$, we generate data $\left\{x_{j}^{(t)}, y_{j}^{(t)}\right\}_{j=1}^{N_{r}}$, where $x_{j}^{(t)} \sim \mathcal{P}$ and $y_{j}^{(t)}=f^{(t)} \circ h^{\star}\left(x_{j}^{(t)}\right)$. Loosely following [TJJ20], we obtain $h_{w}$ and $h_{s}$ as

$$
\begin{equation*}
h_{k}=\operatorname{argmin}_{h \in \mathcal{H}_{k}} \frac{1}{T \cdot N_{r}} \sum_{t=1}^{T} \sum_{j=1}^{N_{r}}\left(f^{(t)} \circ h\left(x_{j}^{(t)}\right)-y_{j}^{(t)}\right)^{2} \quad \text { for } k \in\{w, s\} \tag{9}
\end{equation*}
$$

We set $\mathcal{H}_{w}$ and $\mathcal{H}_{s}$ (both $\mathbb{R}^{8} \rightarrow \mathbb{R}^{16}$ ) to be the classes of 2-layer and 8-layer neural networks respectively with ReLU activations and hidden dimension 16. We obtain $h_{w}$ and $h_{s}$ via gradient descent on the representation parameters to find the minimizers in (9). We set $T=10, N_{r}=2000$. Additionally, we also consider the realizable setting (Theorem 1), where we explicitly set $h_{s}=h^{\star}$, and only obtain $h_{w}$ as above.

(2) Perturbations: We also consider another way to obtain the weak and strong representations as direct perturbations of $h^{\star}$. Namely, we perturb every parameter in every weight matrix in $h^{\star}$ by independent Gaussian noise $\mathcal{N}\left(0, \sigma_{s}^{2}\right)$ to obtain $h_{s}$. Similarly, we obtain $h_{w}$ by perturbing each parameter in $h^{\star}$ by $\mathcal{N}\left(0, \sigma_{w}^{2}\right)$. Ideally, we want the strong representation $h_{s}$ to be a closer approximation of $h^{\star}$ than $h_{w}$. Hence, we set $\sigma_{s}=0.01$ and $\sigma_{w}=0.05$.
![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-07.jpg?height=910&width=1404&top_left_y=236&top_left_x=358)

(a) Realizable (pretraining) $h^{\star}$ is a 5-layer MLP, $h_{s}=h_{\star}$, $h_{w}$ is a 2-layer MLP.

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-07.jpg?height=344&width=458&top_left_y=717&top_left_x=367)

(d) MolBERT on ESOL

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-07.jpg?height=330&width=444&top_left_y=255&top_left_x=838)

(b) Non-realizable (pretraining). $h^{\star}$ is a 5-layer MLP, $h_{w}$ is a

2-layer MLP, $h_{s}$ is an 8-layer MLP.

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-07.jpg?height=336&width=445&top_left_y=732&top_left_x=840)

(e) MolBERT on FreeSolv

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-07.jpg?height=330&width=460&top_left_y=255&top_left_x=1296)

(c) Non-realizable (perturbation). $h_{w}=h^{\star}+\mathcal{N}\left(0,0.05^{2}\right), h_{s}=h^{\star}+$ $\mathcal{N}\left(0,0.01^{2}\right) . h^{\star}$ is a 5-layer MLP.

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-07.jpg?height=336&width=463&top_left_y=732&top_left_x=1292)

(f) MolBERT on Lipop

Figure 2: (a),(b),(c) Experiments on synthetic data. (d),(e),(f) QSAR tasks over MolBERT representations on the ESOL, FreeSolv and Lipop datasets. For each dataset, ChemBench [Wan20] provides three different train, test and validation splits; multiple points of the same color correspond to weak-to-strong supervision for the same weak model (as specified in legend) across these splits.

Weak Model Finetuning. Once the representations $h_{w}$ and $h_{s}$ have been obtained and fixed, we randomly generate $M$ new finetuning tasks $f^{(1)}, \ldots, f^{(M)} \in \mathcal{F}_{s}$, and obtain data $\left\{x_{j}^{(i)}, y_{j}^{(i)}\right\}_{j=1}^{N_{f}}$ for each of these tasks. Here again, $x_{j}^{(i)} \sim \mathcal{P}$ and $y_{j}^{(i)}=f^{(i)} \circ h^{\star}\left(x_{j}^{(i)}\right)$. We set $M=100, N_{f}=2000$. For each task, we train the weak model on the data generated for the task, to obtain

$$
\begin{equation*}
f_{w}^{(i)}=\operatorname{argmin}_{f \in \mathcal{F}_{s}} \frac{1}{N_{f}} \sum_{j=1}^{N_{f}}\left(f \circ h_{w}\left(x_{j}^{(i)}\right)-y_{j}^{(i)}\right)^{2} \tag{10}
\end{equation*}
$$

Here, the representation parameters $h_{w}$ are frozen, and $f_{w}^{(i)}$ is obtained via gradient descent. Note again that we are training the weak models on true data labeled by the finetuning task $f^{(i)} \circ h^{\star}$.

Weak-to-Strong Supervision. Once our weak models are trained for each finetuning task, we generate weakly labeled data. That is, for each $i \in[M]$, we generate $\left\{\tilde{x}_{j}^{(i)}, \tilde{y}_{j}^{(i)}\right\}_{j=1}^{N_{f}}$ where $\tilde{x}_{j}^{(i)} \sim \mathcal{P}$. But crucially, $\tilde{y}_{j}^{(i)}=f_{w}^{(i)} \circ h_{w}\left(\tilde{x}_{j}^{(i)}\right)$. We now train our strong models on this weakly labeled data. Namely, keeping the strong representation $h_{s}$ fixed, we obtain, via gradient descent again

$$
\begin{equation*}
f_{s w}^{(i)}=\operatorname{argmin}_{f \in \mathcal{F}_{s}} \frac{1}{N_{f}} \sum_{j=1}^{N_{f}}\left(f \circ h_{s}\left(\tilde{x}_{j}^{(i)}\right)-\tilde{y}_{j}^{(i)}\right)^{2} \tag{11}
\end{equation*}
$$

At this point, our weak-to-strong training procedure is complete.

Evaluation. For each finetuning task, we wish to evaluate the accuracy of our weak-to-strong model $f_{s w}^{(i)} \circ h_{s}$ with respect to the true task $f^{(i)} \circ h^{\star}$.

Towards this, we estimate 3 quantities:

(a) Error of the weak-to-strong model $f_{s w}^{(i)} \circ h_{s}$ on the true finetuning task: $\mathbb{E}_{x \sim \mathcal{P}}\left(f_{s w}^{(i)} \circ h_{s}(x)-\right.$ $\left.f^{(i)} \circ h^{\star}(x)\right)^{2}$.
(b) Error of the weak model $f_{w}^{(i)} \circ h_{w}$ on the true finetuning task: $\mathbb{E}_{x \sim \mathcal{P}}\left(f_{w}^{(i)} \circ h_{w}(x)-f^{(i)} \circ\right.$ $\left.h^{\star}(x)\right)^{2}$.

(c) Misfit error of the weak-to-strong model on the weakly labeled data: $\mathbb{E}_{x \sim \mathcal{P}}\left(f_{s w}^{(i)} \circ h_{s}(x)-\right.$ $\left.f_{w}^{(i)} \circ h_{w}(x)\right)^{2}$.

Each of these quantities are estimated from a fresh sample of size $N_{f}$ drawn from $\mathcal{P}$. For each task $i \in[M]$, we plot the difference (b)-(a), namely the Gain in Accuracy, on the y-axis, versus the Misfit (c) on the x-axis. Figure 2a has the results for the realizable case where $h_{s}=h^{\star}$ and $h_{w}$ is obtained by pretraining. Figure 2b has the results for the non-realizable case where both $h_{w}$ and $h_{s}$ are obtained by pretraining. Figure 2c has the results for the non-realizable case where $h_{w}$ and $h_{s}$ are obtained by directly perturbing the weights in $h^{\star}$. For reference, recall that Theorem 2 indicates that the gain in accuracy is (upto error terms) at least the misfit. The plots in Figure 2 suggest that the gain is more or less exactly the misfit, which is in agreement with our theory!

### 5.2 Molecular Prediction

We also validate our conceptual insights on real-world molecular prediction datasets. Specifically, we follow the Quantitative Structure-Activity Relationship (QSAR) task setup in the MolBERT \$\left[\mathrm{FEG}^{+}\right.\$20] paper. These tasks involve predicting physical properties of molecules like solubility, lipophilicity, etc. We consider three regression datasets: ESOL, FreeSolv and Lipop. These datasets are part of the MoleculeNet \$\left[\mathrm{WRF}^{+}\right.\$18] benchmark suite, and have been curated into train, test and validation splits by ChemBench [Wan20]. The MolBERT paper provides weights for a standard-size BERT [DCLT18] architecture (hidden dimension 768, 12 layers, 12 attention heads) pretrained for 100 epochs on the GuacaMol [BFSV19] dataset. We use these weights as the strong representation $h_{s}$. For the weak representations $h_{w}$, we run the pretraining pipeline for substantially smaller transformer architectures and lesser compute time. Specifically, we consider transformers with just 2 layers and 2 attention heads, and vary the hidden size in $\{8,12,16,32,48,64,96\}$. For each of these settings, we run the pretraining tasks for a mere 2 epochs to obtain different weak representations $h_{w}$.

Once we have the representations $h_{s}$ and $h_{w}$, we can finetune a linear layer on top of these for each of the three regression datasets. We run the entire weak-to-strong supervision pipeline from above, where we weakly supervise the strong model $h_{s}$ on labels given by each of the weak models $h_{w}$. The results are given in Figures 2d, 2e and 2f. Again, we see that the gain in accuracy of the weakly supervised strong models is accurately characterized by their misfit on the weak labels.

We were also able to see an otherwise useful algorithmic insight in these experiments. Consider a setting where we have at our disposal various weak models, and have performed the weak-to-strong supervision pipeline separately on each of them. We now want to deploy one of the weakly trained strong models; our goal is to choose the one that gets the least error on the true data distribution. Recall that Theorem 2 guarantees that the error of a weakly supervised strong model is upper bounded (upto error terms) by the difference between the weak model's error and misfit. This suggests a natural heuristic: sort the strong models by the difference between the corresponding weak model's error and the misfit, and choose the one for which this quantity is smallest. We observed that this heuristic ends up working quite well—Table 1 shows the numbers for the Lipop dataset, while the results for ESOL and FreeSolv are in Appendix B.

### 5.3 Strong-to-Weak Generalization and Low Sample Regime

In our simulations, we also consider an additional thought experiment, where we reverse the weak and strong models. That is, in the non-realizable case with pretraining (Figure 2b), we can have $\mathcal{H}_{w}$ be the class of 8-layer MLPs, and $\mathcal{H}_{s}$ be the class of 2-layer MLPs. Similarly, in the case with perturbations (Figure 2c), we can set $\sigma_{w}=0.01$, and $\sigma_{s}=0.05$. In this case, because the weak models have now become powerful, and can represent the true data well, the weak labels are essentially the true labels. Hence, if we were to obtain the same plots, we would now expect the misfit on weak labels to essentially correspond to the loss in accuracy of the strong model on true data, compared to the weak model. This is confirmed in Figures 3a and 3b: the plots are mirror reflections of Figures 2b and 2c!

Now, suppose that we are in a setting where the number of samples available for the representation learning task is scarce. Concretely, consider the original setting of Figure $2 \mathrm{~b}$ with $\mathcal{H}_{w}$ and $\mathcal{H}_{s}$ back to being 2-layer and 8-layer MLPs respectively. Recall that for learning the representations $h_{w}, h_{s}$,

| Hidden dimension | Weak error - Misfit | True error of weakly-supervised strong model |
| :---: | :---: | :---: |
| 96 | $\mathbf{0 . 8 9 6 9} \pm \mathbf{0 . 0 3 2 7}$ | $\mathbf{1 . 0 7 1 3} \pm \mathbf{0 . 0 4 8 9}$ |
| 48 | $0.9731 \pm 0.0707$ | $1.1293 \pm 0.0418$ |
| 24 | $1.0331 \pm 0.0449$ | $1.1204 \pm 0.0261$ |
| 64 | $1.0619 \pm 0.0441$ | $1.1436 \pm 0.0124$ |
| 32 | $1.0624 \pm 0.0527$ | $1.1302 \pm 0.0220$ |
| 16 | $1.1456 \pm 0.0276$ | $1.1950 \pm 0.0484$ |
| 12 | $1.1499 \pm 0.0177$ | $1.1869 \pm 0.0297$ |
| 8 | $1.1958 \pm 0.0194$ | $1.2396 \pm 0.0310$ |

Table 1: Heuristic rule to choose among different weakly-supervised models finetuned on Lipop: choose the strong model that has the smallest difference (averaged across the 3 splits) between weak model error and misfit ( $\pm$ is the std across splits). As we see, this model has the smallest true error.

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-09.jpg?height=494&width=1429&top_left_y=840&top_left_x=348)

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-09.jpg?height=325&width=458&top_left_y=865&top_left_x=367)

(a) Non-realizable (pretraining). $h^{\star}:$ 5-layer MLP, $h_{w}: 8$-layer MLP, $h_{s}:$ 2-layer MLP.

![](https://cdn.mathpix.com/cropped/2024_06_04_525a3c617bb20231eb6dg-09.jpg?height=342&width=461&top_left_y=851&top_left_x=821)

(b) Non-realizable (perturbation). $h_{w}=h^{\star}+\mathcal{N}\left(0,0.01^{2}\right), h_{s}=h^{\star}$ $+\mathcal{N}\left(0,0.05^{2}\right) . h^{\star}$ is 5-layer MLP. $h_{s}: 8$-layer MLP. $T=5, N_{r}=250$.

Figure 3: Strong-to-weak generalization. The roles of the weak and strong models have reversed.

we sampled $T=10$ finetuning tasks $f^{(1)}, \ldots, f^{(T)} \in \mathcal{F}_{s}$, and obtained $N_{r}=2000$ samples labeled according to each $f^{(t)} \circ h^{\star}$. Now instead, consider setting $T=5, N_{r}=250$. The number of samples $N_{f}$ in the weak model finetuning and weak-to-strong supervision stages is still maintained at $N_{f}=2000$. We run the entire weak-to-strong supervision pipeline for this parameter setting. The rationale is that, when the representation learning task is data-deprived, the weak model, by virtue of being simpler, learns a better representation than the strong model, which is more complex. Indeed, this is what we observed, as shown in Figure 3c. Observe that the trend in the plot is very similar to Figures 3a and 3b, where we had explicitly swapped the weak and strong models. This suggests that in the low-sample regime too, the weak and strong models have reversed roles. Thus, the definition of weak and strong models in the framework of weak-to-strong generalization should not solely be based on expressive power; instead, these roles should be assigned based on the quality of representations.

## 6 Conclusion

Employing a representation-theoretic perspective, we characterized the gain in performance in weakto-strong generalization. Our results apply in the setting of learning real-valued functions with the least squares loss, where the strong model learns the finetuning task by optimizing over a convex set of functions. We quantify the gain in accuracy of the weakly-supervised strong model over its weak supervisor in terms of the misfit between the strong and weak models.

Our work has natural limitations. Our theorems notably do not apply when the set from which the strong model learns the finetuning task is not convex. Nevertheless, our experiments in Appendix C do suggest that our results should (at least qualitatively) hold even beyond the convex case. Our work also does not address classification tasks, and it would be interesting to see if similar results could be obtained for more general loss functions. Finally, while we do demonstrate results on real-world datasets, we anticipate that significantly larger-scale experiments on regression datasets used to train modern AI models will yield further interesting insights.

## Acknowledgments and Disclosure of Funding

This work is supported by Moses Charikar and Gregory Valiant's Simons Investigator Awards.

## References

[BCNM06] Cristian Buciluă, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541, 2006. 2.1

[BFSV19] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096-1108, 2019. 5.2

\$\left[\mathrm{BIK}^{+}\right.\$23] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. (document), 1, 2.1

[CCX22] Paul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge. Technical report, Alignment Research Center (ARC), 2022. 3

[CLB \${ }^{+}\$17] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 1

[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 5.2

\$\left[\mathrm{FEG}^{+}\right.\$20] Benedek Fabian, Thomas Edlich, Héléna Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. Molecular representation learning with language models and domain-relevant auxiliary tasks. arXiv preprint arXiv:2011.13230, 2020. 5.2

\$\left[\mathrm{FLT}^{+}\right.\$18] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In International conference on machine learning, pages 1607-1616. PMLR, 2018. 2.1

[Fre95] Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121(2):256-285, 1995. 2.1

[FS97] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997. 2.1

[GYMT21] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789-1819, 2021. 2.1

[Haz16] Elad Hazan. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157-325, 2016. 4

[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 2.1

\$\left[\mathrm{JCL}^{+}\right.\$24] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weakto-strong correction. arXiv preprint arXiv:2402.02416, 2024. 2.1

\$\left[\mathrm{JQC}^{+}\right.\$23] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023. 2.1

[KB14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. D

\$\left[K R J^{+}\right.\$22] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. 1

[LA16] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations, 2016. 2.1

[LT13] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science \& Business Media, 2013. A

\$\left[\mathrm{LZH}^{+}\right.\$24] Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530, 2024. 2.1

[Met16] Cade Metz. In two moves, AlphaGo and Lee Sedol redefined the future. https://www. wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/, 2016. 1

[MFB20] Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in hilbert space. Advances in Neural Information Processing Systems, 33:3351-3361, 2020. 2.1

[Ope23a] OpenAI. Gpt-4 technical report, 2023. 1

[Ope23b] OpenAI. Introducing Superalignment. https://openai.com/blog/ introducing-superalignment, 2023. 2.1

\$\left[\mathrm{RWC}^{+}\right.\$19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1

[SYS \${ }^{+}\$24] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472, 2024. 2.1

[TJJ20] Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. Advances in neural information processing systems, 33:7852-7862, 2020. 1, 3, 5.1, A

[TV17] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017. 2.1

[Wan20] Shen Wanxiang. ChemBench: The molecule benchmarks and MolMapNet datasets. https://github.com/shenwanxiang/ChemBench, 2020. 2, 5.2

[WRF \${ }^{+}\$18] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018. 5.2

[WSCM20] Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations, 2020. 2.1

[WTB \${ }^{+}\$22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. 1

[XLHL20] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687-10698, 2020. 2.1

[ZCBS23] Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi. Alleviating hallucinations of large language models through induced hallucinations. arXiv preprint arXiv:2312.15710, 2023. 2.1
