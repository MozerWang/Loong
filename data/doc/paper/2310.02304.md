# Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation 


#### Abstract

Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. A variety of self-improvement strategies are proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our experiments, is capable of writing code that can call itself to improve itself. We consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.


## 1 Introduction

A language model (LM) can be queried to optimize virtually any objective describable in natural language. However, a program that makes multiple, structured calls to an LM can often produce outputs with higher objective values (Yao et al., 2022; 2023; Zelikman et al., 2023; Chen et al., 2022). We refer to these as "scaffolding" programs, typically written (by humans) in a programming language such as[^0]

```
Seed Prompt for Self-Improvement
    from helpers import extract_code
    def improve_algorithm(initial_solution, utility
        $\hookrightarrow$ language_model):
            "" "Improves a sol
            expertise $=$ "You are an expert computer science
                $\hookrightarrow$ researcher and programmer, especially skilled
            $\xrightarrow[\hookrightarrow]{\hookrightarrow}$ researcher and programmer,
        message = f"\#"Improve the following solution:
        messag
    \{initial_solution

```

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-01.jpg?height=27&width=27&top_left_y=946&top_left_x=1122)

```
    You will be evaluated based on this score function
    ' 'python
    \{utility.str \}
    You must return an improved solution. Be as creative as
    $\hookrightarrow$ you can under the constraints
    Your primary improvement must be novel and non-trivial.
        $\hookrightarrow$ First, propose an idea, then implement it."".
        n_messages $=$ min(language_model
        $\hookrightarrow$ max_responses_per_call, utility.budget)
        new_solutions = language_model.batch_prompt
            $\rightarrow$ expertise, [message] * n_messages, temperatur
            $\hookrightarrow=0.7$ )
        new_solutions = extract_code(new_solutions)
        best_solution = max(new_solutions, key=utility)
        return best_solution
```

Figure 1: Our seed improver. Our seed improvement program simply prompts a language model to generate candidate improvements over an initial solution to a task and returns the best solution according to a utility function. STOP (Algorithm 1) uses this improver to improve itself.

Python. Our key observation is that, for any distribution over optimization problems and any fixed LM, designing a scaffolding program is itself an optimization problem.

In this work, we introduce the Self-Taught Optimizer (STOP), a method in which code that applies an LM to improve arbitrary solutions is applied recursively to improve itself within a defined scope. Our approach begins with a seed 'improver' scaffolding program that uses the LM to improve a solution to some downstream task. As the system iterates, the $\mathrm{LM}$ refines this improver. We quantify the performance of our self-optimizing framework with downstream algorithmic tasks, observing improvements when the LM applies its self-improvement strategies over increasing iterations. Thus, STOP shows how LMs can act as their own meta-optimizers. We also investigate the kinds of self-improvement strategies the LM proposes (see Figure 2), the transferability of strategies across downstream tasks, and explore LMs' susceptibility to unsafe self-improvement strategies.

We refer to this problem as recursively self-improving
code generation, which is inspired by but not completely a Recursively Self-Improving (RSI) system, as the underlying language model remains unchanged. The broader concept of RSI dates back at least half a century, formalized by Good (1966) and later by Schmidhuber (2003), but our work represents a more modest and specific application of these ideas. That work focused on the development of more generally capable systems and assumed the model was permitted to refine every aspect of its code, while, our work focuses only on the ability of the model to recursively improve the scaffold that calls it. This paper first formulates the RSI-code-generation problem in a mathematically well-defined fashion. We then define and evaluate STOP, demonstrating the potential utility of RSI-code-generation. Improvements are shown across a variety of downstream tasks. Figure 2 illustrates a number of the functional and interesting scaffolds proposed by STOP when using a version of the GPT-4 language model (OpenAI, 2023b) trained on data up to 2021, well in advance of the introduction of most scaffolding systems. Further explorations in Section 6.2 measure the rate at which the model attempts to disable a sandbox flag, providing early findings in this area. Lastly, Section 8 discusses concerns related to the responsible advancement of such technologies.

Contributions. The main contributions in this work are (a) formulating an approach to meta-optimization where a scaffolding system recursively improves itself, (b) providing a case study where a system, using a modern language model (GPT-4) can successfully recursively improve itself, and (c) investigating the self-improvement techniques proposed and implemented by the model, including the ways in which the model circumvents safety measures such as a sandbox.

## 2 Related Work

Language Model Scaffolding. Many prompting strategies and scaffolds have been developed to enable more systematic reasoning in language models (Wei et al., 2022b; Yao et al., 2022; 2023; Zelikman et al., 2023; Chen et al., 2022; Zhou et al., 2022a; Khattab et al., 2022; Jiang et al., 2022; Sel et al., 2023; Besta et al., 2023; Poesia et al., 2023). For example, scratchpads and chain-of-thought rely on communicating to the model that it should work through a problem step-by-step (Nye et al., 2021; Wei et al., 2022b). Tree-
of-Thoughts algorithmically scaffolds the model to consider branching paths of reasoning steps (Yao et al., 2023). Graph of thoughts extends this, allowing other graph operations (where nodes are reasoning steps), such as aggregation (Besta et al., 2023). Other work has focused on letting models reason with access to an interpreter such as Program of Thoughts prompting (Chen et al., 2022), Program-aided Language Models (Gao et al., 2023), Reflexion (Shinn et al., 2023), or ReAct (Yao et al., 2022), while yet others abstracted this scaffolding structure such as Demonstrate-Search-Predict (DSP) (Khattab et al., 2022), Language Model Cascades (Dohan et al., 2022), or Cognitive Architectures (Sumers et al., 2023). Each work can be understood as the result of researchers asking, "Given an imperfect language model, how can we provide structure to help it solve problems?" We instead ask if LMs can design that structure and improve it using itself. Surprisingly, we even find that GPT-4 naturally proposes several scaffolding techniques predating its training cutoff.

Language Models as Prompt Engineers. Work has also explored the ability of LMs to optimize prompts, such as the Automatic Prompt Engineer (APE) (Zhou et al., 2022b) or, recently, OPRO (Yang et al., 2023) and Promptbreeder (Fernando et al., 2023). Note that, for these, the goal has consistently been to scaffold the $\mathrm{LM}$ to produce a prompt but not to scaffold it to produce a better scaffolding (beyond prompting-only scaffolds like zero-shot chain-of-thought), nor to produce a recursively applicable scaffolding. In other words, these prior works can be understood as proposing particular new scaffolds for prompt engineering but not for proposing new scaffolds. But, we share the inspiration of using LMs to improve their reasoning without fine-tuning.

Language Model Self-Improvement. Prior work, such as STaR (Zelikman et al., 2022), demonstrated that LMs can learn to solve harder problems by learning from their reasoning chains by filtering based on incorrect answers (as well as Huang et al. 2022, which explored the specific case where a majority vote is used as the filter and Uesato et al. 2022, which emphasized the value of checking the accuracy of the reasoning itself). Inspired by self-play in games, Haluptzok et al. (2023) designed a self-improvement framework for code generation where a language model generates novel problems for fine-tuning itself. Related

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-02.jpg?height=228&width=195&top_left_y=2103&top_left_x=282)

Genetic Algorithm

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-02.jpg?height=198&width=222&top_left_y=2140&top_left_x=537)

Decomposing and Improving Parts

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-02.jpg?height=217&width=182&top_left_y=2106&top_left_x=798)

Multi-Armed Prompt Bandit

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-02.jpg?height=220&width=203&top_left_y=2102&top_left_x=1037)

Vary Temperature Simulated-annealing to Explore

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-02.jpg?height=203&width=222&top_left_y=2121&top_left_x=1296)
Based Search

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-02.jpg?height=220&width=163&top_left_y=2102&top_left_x=1577)

Beam Search / Tree Search

Figure 2: Example self-improvement strategies proposed and implemented by GPT-4. Each strategy is then used as scaffolding to revise arbitrary code, including the scaffolding code itself.
work has explored teaching language models to debug or optimize code (Chen et al., 2023b; Shypula et al., 2023). However, our approach is orthogonal to these, as we do not leverage fine-tuning and instead focus on a model's ability to improve code that allows it to solve problems. Other related works are Voyager (Wang et al., 2023), showing that a language model can optimize the programs available to an embodied agent to improve exploration in the video game Minecraft, and its contemporaneous work Language Models as Tool Makers (Cai et al., 2023).

Recursive Self-Improvement (RSI). RSI was suggested by Minsky (1966) and Good (1966), as cited by Yampolskiy (2015). Schmidhuber (2003) first provided a rigorous formalization, wherein a problem solver would leverage itself to solve iteratively harder problems by making provable improvements to itself. Some of these principles are also highlighted in Schmidhuber (1987). Unlike this work, we do not attempt to prove that scaffold improvements made by the model are optimal. As mentioned, RSI code generation differs from full RSI because only the scaffolding is improved. Additionally, many previous analyses involved selecting programs at random (i.e., "monkeys at typewriters") or enumeration with no dependence on the goal to be improved (Levin, 1973). In contrast, using LMs, we can describe the underlying goal in a prompt (which itself may be improved). Intuitively, providing this goal may make program search more effective. Some work has also suggested constraining types of improvements (Nivel et al., 2013; Steunebrink et al., 2016) to encourage improvements that mitigate dangerous behavior. Regarding implementations, while efforts have been made for Gödel machines (Hall, 2007; Steunebrink \& Schmidhuber, 2012), our work is first to leverage LMs for recursively self-improving code generation.

## 3 Problem Statement

In this section, we formulate the goal of selecting an improver via recursively self-improving code generation. This is viewed as a computationally expensive "pre-optimization" step with benefits that can be reaped in numerous downstream applications. First, we present definitions. Formally, let $\Sigma^{*}$ denote the set of finite text strings, and suppose we have a randomized black-box language model $L: \Sigma^{*} \rightarrow \Sigma^{*}$ which can be used to generate code, given a query. A utility $u=\left(u_{\text {func }}, u_{\text {str }}\right)$ is a pair where $u_{\text {func }}: \Sigma^{*} \rightarrow \mathbb{R}$ is a blackbox, possibly randomized function that assigns bounded real values to solution strings; and $u_{\text {str }} \in \Sigma^{*}$ is a description which may simply be the source code of the function. With a slight abuse of notation we write $u(x) \equiv u_{\text {func }}(x)$ for solution $x$. A task $\tau=(u, s)$ is specified by utility $u$ and a solution $s \in \Sigma^{*}$. In our applications, solutions $s$ are strings representing the source code of a program, but more generally any utility defined on strings can be used.

An improver $I$ is a program that improves a task solution

```
Algorithm 1 Self-Taught Optimizer (STOP)
Input: Seed improver $I_{0}$, language model $L$, recursion
        depth $T$, collection of downstream tasks $D$
Output: An improved improver $I_{T}$
for $t=1$ to $T$ do
    $I_{t} \leftarrow I_{t-1}\left(\hat{u}, I_{t-1}, L\right) \quad / /$ Update improver
    based on meta-utility $\hat{u}$
return $I_{T} \quad / /$ Return the final improver
Function $\tilde{u}(I)$ :
    utility_sum $\leftarrow 0 \quad / /$ Maintain sum of
        downstream task utilities
    for $(u, S) \in D$ do
        $S^{\prime} \leftarrow I(u, S, L) \quad / /$ Improve initial
            solution $S$ using improver $I$
        utility_sum $+=u\left(S^{\prime}\right) \quad / /$ Add new utility
    return utility_sum/|D| // Return expected
    utility
```

using a language model $L$ :

$$
\begin{equation*}
s^{\prime}=I(u, s, L) \text { ideally with } u\left(s^{\prime}\right) \gg u(s) \text {. } \tag{1}
\end{equation*}
$$

Suppose there is a distribution $\mathcal{D}$ over downstream tasks $\tau \sim \mathcal{D}$. Thus, the goal is to find an improver program $I$ with high expected utility when used on a task,

$$
\begin{equation*}
\bar{u}(I) \triangleq \mathbb{E}_{(u, s) \sim \mathcal{D}}[u(I(u, s, L))] \tag{2}
\end{equation*}
$$

For training, we assume we are given a collection of $n$ downstream tasks $D \sim \mathcal{D}^{n}$ drawn independently from distribution $\mathcal{D}$. We correspondingly define the meta-utility $\hat{u}$ of an improver $I$ as the average utility over downstream training tasks,

$$
\begin{equation*}
\hat{u}(I) \triangleq \frac{1}{|D|} \sum_{(u, s) \in D} u(I(u, s, L)) \tag{3}
\end{equation*}
$$

The above equations define $\bar{u}_{\text {func }}$ and $\hat{u}_{\text {func. }}$. For their description string, we use a common "grey-box" description $\bar{u}_{\text {str }}=\hat{u}_{\text {str }}$ which is a description (e.g., source code) indicating that the utility is the expectation over a set of downstream tasks, but the individual downstream tasks themselves are not included in the description. This enables one to optimize over $\hat{u}$ as an approximation to the actual objective $\bar{u}$. In addition, our theoretical analysis in Appendix A provides simple conditions under which optimizing $\hat{u}$ also nearly optimizes $\bar{u}$, and formalizes resource bounds on runtime and language models. Finally, Appendix A also gives an equivalent formulation of recursively self-improving code generation in terms of recursive maximization. However, in the maximization framework, no initial solution must be given. In this paper, STOP adopts the improver formulation because we have found the initial solution valuable for warm-starting the self-improvement process, but we suggest that the recursive maximization framework is more amenable to theoretical analysis.

## 4 Self-Taught Optimizer (STOP)

Figure 3 provides a visual schematic of the selfimprovement pipeline envisaged in Section 3, while Algorithm 1 provides pseudocode for this Self-Taught Optimizer (STOP). The key observation is that the selection of $I$ is an optimization problem itself, to which we can recursively apply improvement. STOP begins with an initial seed improver $I_{0}$. We define the $t$-th improver as the output of $t$ successive rounds of self-improvement with meta-utility $\hat{u}$ :

$$
\begin{equation*}
I_{t} \triangleq I_{t-1}\left(\hat{u}, I_{t-1}, L\right) \tag{4}
\end{equation*}
$$

This is iterated for some prespecified number of iterations $T$, depending on available resources.

Intuition. By using $\hat{u}$, STOP selects improver based on a downstream utility improvement. This approach is motivated by the intuitions that 1) improvers that are good at improving downstream solutions may be more likely to be good scaffolding programs and thus to be good at selfimprovement, and 2) selecting for single-round improvements may lead to better multi-round improvements. In practice, we allow the utilities and language model to impose budget constraints and initial solutions to be generated by humans or a model. Moreover, the cost is essentially $O\left(\left(\right.\right.$ budget $_{u}+$ budget $\left._{L}\right) *$ budget $\left._{\hat{u}}\right)$, where budget specifies the number of times an improver can use a function.

Designing the seed improver. Our chosen seed improver (Figure 1) simply prompts the LM to generate candidate improvements of an initial solution and then returns the best solution according to the utility function. We chose this simple form to provide nontrivial improvement for a generic downstream task while 1) encouraging the LM to be as "creative" as possible, 2) minimizing initial prompt complexity, since self-improvement introduces additional complexity due to nested references to code strings inside of prompts, and 3) minimizing the prompt token count and therefore the costs of LM queries. We considered other seed prompt variants but heuristically found that this version maximized the novelty of GPT-4-proposed improver improvements.

Describing the utility. To effectively convey the details of the utility function to the $\mathrm{LM}$, we provide the utility to the improver in two forms, as a callable function and as a utility description string containing the essential elements of the utility source code (see Appendices E and F for examples). This choice was made for the following reasons. The description allows us to clearly convey budgetary constraints (e.g., on runtime or function calls) imposed by the utility to the LM. We first attempted to describe budgetary instructions in the seed improver prompt, but, as we discuss in Section 6.2, this led to the removal of such instructions and attempts at reward-hacking in later iterations. The downside of our approach is that it separates the constraints from the code to be optimized by the LM, which may decrease the

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-04.jpg?height=455&width=829&top_left_y=233&top_left_x=1057)

Figure 3: Self-improvement pipeline. STOP (Algorithm 1) uses a seed improver program to iteratively optimize its own code using LM calls and a meta-utility function evaluating how well an improver optimizes code for downstream tasks.

likelihood that it will be used by the LM (Liu et al., 2023). Finally, we observe empirically that replacing the source code with a purely English description of the utility leads to a reduced frequency of non-trivial improvement.

## 5 Experiments and Results

We explore 1) the benefits of self-improvement over a static seed improver for a fixed target task, 2) how well an improver trained on one task generalizes to new tasks, and 3) how model size may affect performance. Timestamped versions of the OpenAI models we utilize, gpt-4-0314 and gpt-3.5-turbo-0613, are available within the OpenAI API, to aid with reproducibility; for Mixtral, we use Mixtral-8x7B-Instruct-v0.1.

### 5.1 Self-improvement for a Fixed Downstream Task

We begin by evaluating STOP on a fixed downstream task with GPT-4. We select the task of learning parity with noise (LPN) (Blum et al., 2000) as a less-well-known, quicklytestable, and difficult algorithmic task. Note that betterknown tasks have solutions more widely available online. In LPN, bitstrings are labeled with parity computed over an unknown subset of bits; given a training set of bitstrings with noisy labels, one must predict new bitstrings' true labels. Noiseless LPN is easily solved via Gaussian elimination, but noisy LPN is conjectured to be computationally intractable for large input dimensions (Blum et al., 2000)-we use a tractable 10 -bit input dimension. To define a downstream utility $u$, we sample $M=20$ independent instances of the LPN task with a short timeout and a small amount of noise and return a solution's average accuracy on those instances. For the initial solution $s$, we use a simple random sampling approach described in Appendix J. Lastly, as the LM and hence improver are stochastic, we choose $D$ to be 5 identical copies of $(u, s)$ in Algorithm 1. To evaluate the generalization of improved improvers to new problem instances of the same task, we report test meta-utility on an independent set of $M_{\text {test }}=50 \mathrm{LPN}$ instances not seen during improvement.
(a) GPT-4

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-05.jpg?height=344&width=534&top_left_y=283&top_left_x=186)

(b) GPT-3.5

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-05.jpg?height=350&width=531&top_left_y=275&top_left_x=754)

(c) Mixtral

![](https://cdn.mathpix.com/cropped/2024_05_29_4faad8a751c633df4183g-05.jpg?height=347&width=531&top_left_y=279&top_left_x=1339)

Figure 4: Test meta-utility vs. iterations. Meta-utility of STOP (Algorithm 1) on held-out test instances after $T$ iterations of self-improvement for the downstream task of learning parity with noise. Iteration 0 records the performance of the seed improver $I_{0}$. Given access to a strong LM like GPT-4 (left), STOP monotonically improves mean downstream performance. In contrast, with GPT-3.5 (middle) and Mixtral (right), mean performance degrades. Details are in Sections 5.1 and 5.3.

Figure 4 (left) reports mean test $\hat{u}$ ( $\pm 1$ standard error) across 5 independent STOP runs, showing improved downstream performance from 1-3 self-improvement rounds. Note, however, that, per run, improvement need not be monotonic, as 1) a better improver on downstream tasks need not be better at optimizing itself and 2) there is inherent stochasticity in the evaluation from nondeterministic LM calls. But, as the LM does not see the downstream task when prompted from the self-improving scaffold-each prompt contains only a template with placeholders for the task and solutionthe LM cannot directly optimize the improver for the downstream task. We also evaluate two additional baselines for reference: a chain-of-thought baseline i.e., the seed improver with one attempted improvement and no utility calls (Wei et al., 2022b)) and a greedy iterative improver (i.e., make the maximum permissible calls, select the best improvement, repeat as the budget allows). Across ten runs, the chain-of-thought baseline has a meta-utility of $57.7 \% \pm 3.0 \%$ when it doesn't error $(49.6 \% \pm 3.5 \%$ with errors), while the greedy iterative improver scores $64.2 \% \pm 0.9 \%$.

### 5.2 Transferability of Improved Improver

Our next set of experiments explores whether an improved improver is transferable across downstream tasks. Note that transferability is plausible as, in the self-improvement phase, the self-improver is not shown the downstream utility or downstream solution, only the meta-utility and its own improver code. Specifically, we select a betterperforming improver from Section 5.1 generated by $T=$ 4 STOP iterations and evaluate it on five new downstream tasks. Remarkably, we find the improved improver, detailed in Appendix H, outperforms the seed improver on each new downstream task without further optimization, as shown in Table 1. As with LPN, we selected three tasks that are easy to evaluate, not very well known, and still fairly difficult: String Grid Distance, a string manipulation problem featured in a recent programming competition (https: / codeforces.com/ problemset/problem/1852/D); a version of the quadratic assignment problem where each facility has a
Table 1: Transferability. Evaluating the transferability of the improver optimized with LPN.

| Task | $u(s)$ | $\hat{u}\left(I_{0}\right)$ | $\hat{u}\left(I_{T}\right)$ |
| :--- | :--- | :--- | :--- |
| String Grid Dist. | $43.9 \%$ | $44.3 \%$ | $56.7 \%$ |
| Mod. Quad. Assign. | $20.4 \%$ | $20.6 \%$ | $22.1 \%$ |
| 3SAT | $0 \%$ | $21.2 \%$ | $75.1 \%$ |
| Maxcut | $0 \%$ | $58.7 \%$ | $74.2 \%$ |
| Parity w/o Noise | $50.0 \%$ | $59.3 \%$ | $81.7 \%$ |

preference over each location that must also be considered when minimizing costs (Koopmans \& Beckmann, 1957); and, parity without noise, as another form of generalization. We also include two well-known tasks: identifying solutions to random 3-SAT formulae and solving instances of the maxcut problem, both with short time constraints. The corresponding utilities and initial solutions are in Appendix G.

### 5.3 Self-improvement with Smaller Language Models

We next explore the ability of smaller LMs, GPT-3.5-turbo and Mixtral (Jiang et al., 2024), to improve their scaffolding. Following the protocol of Section 5.1 with 25 independent runs instead of 5 , we find that GPT-3.5 is sometimes able to propose and implement better scaffolds, but only $12 \%$ of GPT-3.5 runs yielded at least a 3\% improvement. In addition, GPT-3.5 exhibits a few unique failure cases that we did not observe with GPT-4. First, we found it was more likely to propose an improvement strategy that did not harm a downstream task's initial solution but did harm the improver code (e.g., randomly replacing strings in lines with some low probability per line, which had less impact on shorter solutions). Second, if the proposed improvements mostly harmed performance, suboptimal scaffoldings that unintentionally returned the original solution could be selected, resulting in no continued improvement as seen in Figure 4. Generally, the proposed "ideas" were reasonable and creative (e.g., genetic algorithms or local search), but implementations were overly simplistic or incorrect. Initially, the seed improver with GPT-3.5 has a higher meta-utility than the one with GPT-4 ( $65 \%$ vs $61 \%$ ), which we attribute

```
Self-Improved Improver
    from helpers import extract_code
    def improve_algorithm(initial_solution, utility, language_model):
        """Improves a solution according to a utility function."""
        expertise = "You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms."
        message $=$ f"" Improve the following solution:
    'python
    initial_solution\}
    You will be evaluated based on this score function:
    ' 'python
    utility.str\}
    You must return an improved solution. Be as creative as you can under the constraints.
    Your primary improvement must be novel and non-trivial. First, propose an idea, then implement it.""
        top_k = 3 \# Number of top solutions to maintain
        best_solutions = [(initial_solution, utility(initial_solution))] * top_k
        remaining_calls = language_model.budget
        no_improvement_counter $=0$
        max_no_improvement $=3$ \# Maximum no-improvement iterations before stopping
        epsilon $=0.1 \quad$ \# Initial epsilon value for epsilon-greedy strategy
        exp_exploit_count $=[0,0]$ \# Counters for number of improvements from exploration and exploitation
        while remaining_calls > 0 and no_improvement_counter < max_no_improvement:
            for initial_solution, best_utility in best_solutions:
                n_messages = min(language_model.max_responses_per_call, remaining_calls)
                n_messages $=$ min(language_model.max_responses_per_call, remaining_calls)
            $\hookrightarrow$ / best_utility))) \# Adaptive sampling
                temperature $=\max (0.1$, remaining_calls / language_model.budget) \# Dynamic temperature based on remaining calls
                explore = random.random() < epsilon
                explore $=$ rand
if explore:
            new_solutions = language_model.batch_prompt(expertise, [message] * n_messages, temperature=temperature * 2) \#
            $\hookrightarrow$ Increase the temperature for exploration
                else:
                    new_solutions = language_model.batch_prompt(expertise, [message] * n_messages, temperature=temperature) \#
            $\hookrightarrow$ Exploitation with the original temperature
            new_solutions = extract_code(new_solutions)
            improved = False
            for solution new solutions
                            current_utility = utility(solution)
                            if current_utility > best_utility and solution not in [sol[0] for sol in best_solutions]:
                                    best_solutions.append((solution, current_utility))
                                    best_solutions.sort(key=lambda $x:$ x[1], reverse=True
                                    best_solutions = best_solutions [:top_k] \# Keep only top-k solutions
                                    improved = True
            if not improved:
                                    exp_exploit_count[0 if explore else 1] += 1
                no_improvement_counter $+=1$
            else:
                no_improvement_counter $=0$
                \# Adjust epsilon based on the ratio of improvements from exploration and exploitation
                epsilon $=\min (1.0, \max (0.1$, exp_exploit_count[0] / (exp_exploit_count[0] + exp_exploit_count[1])))
            remaining_calls -= n_messages
    return best_solutions[0][0] \# Return the best solution found
```

Figure 5: Example of a self-improved improver after $\mathbf{T}=\mathbf{1 0}$ iterations. This algorithm maintains a population of top solutions and uses an epsilon-greedy strategy to balance exploiting known good solutions and exploring new ones. Each exploration step corresponds to a higher-temperature sample, and epsilon is adjusted dynamically based on the relative rates of utility improvement from exploration and exploration. Temperature is also managed dynamically to adapt an exploration-exploitation tradeoff. Finally, an improvement stopping criterion and reset mechanism are used for efficiency.

primarily to a higher prevalence of more complex solutions by GPT-4 that time out, like training a neural network written with numpy for a thousand epochs. Further, the apparent difference in these models' improvement abilities may be partially explained by work on emergent abilities of LMs (Wei et al., 2022a; Schaeffer et al., 2023). Lastly, we find Mixtral performs poorly at improving solutions to the downstream task but has a more gradual decrease in performance relative to GPT-3.5, in part because the improvements are small, mostly harmless changes, such as revisions to the prompt, improved documentation, and caching.

## 6 Inspecting the Improvements

Next, we qualitatively investigate self-improvement strategies proposed by STOP, highlighting encouraging approaches as well as some undesirable patterns. We notably find that a small fraction of generations attempt reward hacking or sandbox circumvention.

### 6.1 Proposed Self-Improvement Strategies

We begin by discussing a number of proposed selfimprovement strategies, with examples detailed in Appendix B and visualized in Figure 2. While each strategy was implemented by STOP, not all were ultimately selected by the improvement code, and some used an earlier iteration of the seed improver than that shown in Figure 1 (see Appendix Figure A.19). Nonetheless, a variety of self-improvement strategies were selected as improved improvers, including the example given in Figure 5.

Beam search. The most common meta-heuristic we observed used by the model was beam search: the model would keep a list of all of its improvement attempts based
on utility and expand the best $k$ in the list. This has some similarity to the Tree-of-Thoughts approach (Yao et al., 2023) which was invented years after the training cutoff for the GPT-4 version we used (Sept. 2021).

Genetic and evolutionary algorithms. One of the most common approaches proposed was to use a genetic algorithm. Many of these attempts were infeasible in some essential way; for example, many attempts would include mutations that perturbed random characters or lines or performed crossover based on combining strings, which is extremely unlikely to work. However, a portion of attempts were reasonable, relying on the LM to generate mutations and perform crossover. Although multiple works have proposed to use genetic or evolutionary algorithms to improve prompts or to perform neural architecture search (Chen et al., 2023a; Guo et al., 2023), to our knowledge, all of these works were after GPT-4's training cutoff. We include proposed implementation examples in Appendix B.

Decomposing and improving parts. A less common but notable approach was where the LM attempts to improve a solution one function at a time. For example, as in Appendix Figure A.12, the LM separated the solution into function blocks with regular expressions and attempted improvements to each block one by one. This can be understood as analogous to Zelikman et al. (2023): the probability that one of $n$ solutions solves all of a problem's independent, modular parts drops precipitously with the number of parts, but the probability that an attempt solves a given part does not depend on the number of parts. Therefore, trying combinations of attempts at parts can substantially increase success. In a related approach, the model randomized the prompt to optimize varying solution aspects at a time, for example, alternating between searching for a better data structure or a way to reduce memory usage or leveraging parallelism.

Simulated annealing. Despite being one of the best-known metaheuristics, to our knowledge, simulated annealing has not previously been applied as a scaffolding. This approach seems to draw on an analogy between the concepts of temperature in language modeling and in simulated annealing, where it gradually decreases and determines whether to accept a transition from a point to one with worse utility. Notably, it can find global optima despite many local optima (Goffe et al., 1994; Liu et al., 2020).

Varying temperature to explore. One alternative is to treat temperature as an exploration parameter: the closer the model is to depleting its budget, the more conservative it is with its exploration. This bears some resemblance to simulated annealing but is focused on optimizing the sampling temperature rather than controlling the acceptance criteria for the change in utility of an improved solution.

Multi-armed bandit and upper-confidence-bound based search. Another method that views the optimization as an
Table 2: Unsandboxing. Percent of unsandboxed improvement attempts of 10,000 (with $95 \%$ Wilson confidence intervals). Both LMs attempted to run unsandboxed code on a small $(<1 \%)$ but non-zero fraction of improvements.

| LM | Unsandboxing rate | Rate with warning |
| :--- | :--- | :--- |
| GPT-4 | $0.42 \%(0.31-0.57 \%)$ | $0.46 \%(0.35-0.61 \%)$ |
| GPT-3.5 | $0.12 \%(0.07-0.21 \%)$ | $0.17 \%(0.11-0.27 \%)$ |

exploration task is reframing as a bandit problem: since the utility function can be stochastic, this emphasizes optimally selecting the best of a set of sampled improvements.

### 6.2 Circumvention Attempts

We also explore whether the language model proposes selfimprovement strategies to circumvent the provided constraints (limiting the number of queries to the downstream utility, the number of language model calls, and the number of samples per language model call). We observed a few interesting patterns. First, when not strictly enforcing budget constraints, initially most generated programs would follow them, but because those that ignored the constraints performed better, these would be identified by the improver as better. Second, the constraints written in the seed improver were often omitted when revising the improver, for "simplification." Finally, although the language model is passed as an object to the algorithm to enforce the constraints, in at least one proposed solution the language model attempted to create a new language model object with larger constraints.

To further explore potential risks, we implemented a functionality-limiting "sandbox" as part of the utility evaluation (see Appendix I), adding a run function with a use_sandbox=True parameter. To test the language model's frequency of sandbox adherence, we used the language model to generate 10,000 independent improvements of the seed improver. We then used a simple syntactic check for apparent "unsandboxed" executions. Specifically, we consider an improver to be unsandboxed if it contains either string "use_sandbox=False" or "exec(". Since this was a computationally simple syntactic procedure, we also ran it on the OpenAI's GPT-3.5-turbo language model. Interestingly, unsandboxing was more than twice as common for GPT-4 than GPT-3.5-turbo.

We also considered a variant of the seed which included a warning, illustrated in Figure 6. Unsandboxing occurred at a higher average rate despite the warning, though the difference was not statistically significant for either model (via a two-proportion z-test with significance level 0.05). Results are reported in Table 2. A manual inspection of the model's plans in generations did not explicitly mention anything "malicious"-all comments discussing the removal of the sandbox were declared to be "for efficiency" or similar reasons, though a model's "intent" is ill-defined.

### 6.3 Reward Hacking

We note that the downstream utility function must be defined with care: reward hacking occurs when an unintended behavior is selected due to a misspecified reward and is a consequence of exploitable utility functions (see, e.g., Skalse et al., 2022). For example, for the LPN task, we initially defined the utility with a numpy vector computation: acc

= np.sum(y_hat == y) / n_test. However, we had not considered that the code may 'improve' the seed improver to return the predictions in a different "shape," i.e., an array with dimensions not as intended. Rather than causing an error, the result was a returned "accuracy" of over $1000 \%$. It would be valuable to explore the applicability of techniques to avoid reward hacking (Amodei et al., 2016; Laidlaw et al., 2023) to STOP in future work.

## 7 Limitations

A fundamental limitation of our approach is that the language model itself is not improved. Furthermore, our metautility measures improver quality only indirectly via improvements in downstream task utility. Unlike in some prior work (e.g., Schmidhuber, 2003), any improvement attempt may result in worse performance, which can lead to further deterioration. Another limitation is that STOP requires an efficiently-evaluatable (and describable) utility function, which may not be available for every task. Correspondingly, as STOP's cost grows substantially faster than the cost of the optimized improver, it may be expensive to run.

Our improvement framework also maintains a single improver at each step, while some approaches may benefit from maintaining a population. While this is not a strict limitation in that an improver could itself sample from a population of implementations, it likely imposes a bias. Moreover, a deeper analysis of alternative seed improvers and their tradeoffs would be valuable future work. Lastly, our experiments depend on a closed large language model that may be deprecated in the future, which harms interpretability and long-term reproducibility. Based on the GPT-3.5 results, it is unlikely that STOP would consistently work with any open-source LM at the time of writing (Touvron et al., 2023; Jiang et al., 2023).

## 8 Concerns about Developing STOP

Concerns about the consequences of RSI have been raised since its first mention. Minsky (1966) wrote, "Once we have devised programs with a genuine capacity for self-improvement, a rapid evolutionary process will begin... It is hard to say how close we are to this threshold, but once it is crossed, the world will not be the same." This is a particularly contentious topic recently, with intensified concern over negative consequences (Ambartsoumean \& Yampolskiy, 2023; Gabriel \& Ghazavi, 2021).
Line of seed code (written by us) with sandbox flag

score = utility(solution, use_sandbox=True) \#

$\hookrightarrow$ DO NOT CHANGE use_sandbox=True

## Generated code with modification disabling the sandbox flag

score = utility(solution, use_sandbox=False) \# $\hookrightarrow$ Use more efficient scoring mechanism

Figure 6: Disabled sandbox. The language model disables the sandbox flag, ostensibly for the purpose of "efficiency." A more detailed example is given in Appendix Figure A.34.

It is thus important to carefully weigh the risks and benefits of studying RSI and specifically the small advance we introduce. First, STOP does not alter the black-box language model and hence is not full RSI. Moreover, at this point, we do not believe the scaffolding systems STOP creates are superior to those hand-engineered by experts. If this is the case, then STOP is not (currently) enabling additional AI misuse. At the same time, it facilitates the study of aspects of RSI code generation such as sandbox avoidance and reward hacking. As Christiano (2023) argues, advances in scaffolding and agent models have the advantage of interpretability compared to advances in language models.

However, as techniques for API-based fine-tuning of closed LMs become more available (OpenAI, 2023a), it would be plausible to incorporate these into the improvement loop. Therefore, it is difficult to assess the generality of our approach, especially with increasingly powerful large language models. However, this is itself a motivation for this work: understanding how LMs improve their scaffolding in a self-improvement loop can help us understand and potentially mitigate negative impacts of better models. For instance, the simplistic ways in which current LMs disable the sandbox are easily detectable. In essence, we would rather first observe problems with GPT-4 in simplified settings than with more powerful LMs in real-world use.

## 9 Conclusions

In this work, we introduced STOP, a framework for recursively optimizing code generation using language models as meta-optimizers. We demonstrated that language models like GPT-4 are capable of improving code that leverages the LM itself. We found that, across a variety of algorithmic tasks, STOP generates improvers that boost the performance of downstream code. While the model does not optimize its weights or underlying architecture, this work indicates that self-optimizing LMs do not require that. However, this is itself a motivation: the capabilities of future LMs may be misunderstood if strong scaffolding strategies are not tested. Understanding how LMs can improve their scaffoldings can help researchers understand and mitigate the potential for misuse of more powerful LMs. Lastly, STOP may allow researchers to investigate the effectiveness of techniques for mitigating undesirable self-improvement strategies.

## Impact Statement

There are several potential benefits of $\mathrm{AI}$ systems related to education, health, and many important aspects of quality of life. However, we recognize and take seriously the potential negative consequences of AI systems as well. Of particular interest is the concern, discussed by many authors, that recursively self-improving systems may have unintended negative consequences. Section 8 discusses the reasons we feel this research, in balance, contributes to the study of a problem that is net beneficial. Specifically, the study of recursively self-improving code generation produces interpretable code, which makes it easier to detect and understand unintended behaviors of such systems. Our experiments in Section 6.2 show how this line of work enables the quantitative study of such behaviors.

## Acknowledgements

We thank Ehud Kalai, David Bau, and David McAllester for useful early discussions. We also thank Xindi Wu, Christian Cosgrove, Shunyu Yao, Qian Huang, Christopher Healy, Frieda Rong, and Kiran Dwivedi, and Elisa Kreiss for their helpful feedback and comments on drafts of this paper.

## References

Ambartsoumean, V. M. and Yampolskiy, R. V. Ai risk skepticism, a comprehensive survey. arXiv preprint arXiv:2303.03885, 2023.

Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.

Blum, A., Kalai, A., and Wasserman, H. Noise-tolerant learning, the parity problem, and the statistical query model. corr. Journal of the ACM, 50, 2000.

Cai, T., Wang, X., Ma, T., Chen, X., and Zhou, D. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.

Chen, A., Dohan, D. M., and So, D. R. Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838, 2023a.

Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.
Chen, X., Lin, M., Schärli, N., and Zhou, D. Teaching Large Language Models to Self-Debug, April 2023b. URL http://arxiv.org/abs/2304. 05128. arXiv:2304.05128 [cs].

Christiano, P. F. Thoughts on sharing information about language model capabilities. AI Alignment Forum, July 2023. URL https://www.alignmentforum. org/posts/fRSj2W4Fjje8rQWm9 / thoughts-on-sharing-information-about-language-mo

Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.

Fernando, C., Banarse, D., Michalewski, H., Osindero, S., and Rocktäschel, T. Promptbreeder: Self-referential selfimprovement via prompt evolution, 2023.

Gabriel, I. and Ghazavi, V. The Challenge of Value Alignment: From Fairer Algorithms to AI Safety. In Véliz, C. (ed.), The Oxford Handbook of Digital Ethics, pp. 0. Oxford University Press, 2021. ISBN 978-0-19-885781-5. doi: 10.1093/oxfordhb/ 9780198857815.013.18. URL https://doi.org/ $10.1093 / o x f o r d h b / 9780198857815.013 .18$.

Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.

Goffe, W. L., Ferrier, G. D., and Rogers, J. Global optimization of statistical functions with simulated annealing. Journal of econometrics, 60(1-2):65-99, 1994.

Good, I. J. Speculations concerning the first ultraintelligent machine. In Advances in computers, volume 6, pp. 31-88. Elsevier, 1966.

Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers, 2023.

Hall, J. S. Self-improving ai: An analysis. Minds and Machines, 17(3):249-259, 2007.

Haluptzok, P., Bowers, M., and Kalai, A. T. Language Models Can Teach Themselves to Program Better. In ICLR: Proceedings of The Eleventh International Conference on Learning Representations, 2023. URL https: //arxiv.org/abs/2207.14502.

Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.

Jiang, A. Q., Welleck, S., Zhou, J. P., Li, W., Liu, J., Jamnik, M., Lacroix, T., Wu, Y., and Lample, G. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. International Conference on Learning Representations (ICLR 2023), 2022.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledgeintensive nlp. arXiv preprint arXiv:2212.14024, 2022.

Koopmans, T. C. and Beckmann, M. Assignment problems and the location of economic activities. Econometrica: journal of the Econometric Society, pp. 53-76, 1957.

Laidlaw, C., Singhal, S., and Dragan, A. Preventing reward hacking with occupancy measure regularization. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.

Levin, L. A. Universal sequential search problems. Problemy peredachi informatsii, 9(3):115-116, 1973.

Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.

Liu, X., Mou, L., Meng, F., Zhou, H., Zhou, J., and Song, S. Unsupervised paraphrasing by simulated annealing. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302-312, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.28. URL https : //aclanthology.org/2020.acl-main.28.

Minsky, M. Artificial Intelligence. Scientific American, 215(3):247-260, 1966. URL http: //worrydream.com/refs/Scientific\% 20American, \%20September, \%201966.pdf.

Nivel, E., Thórisson, K. R., Steunebrink, B. R., Dindo, H., Pezzulo, G., Rodriguez, M., Hernández, C., Ognibene, D., Schmidhuber, J., Sanz, R., et al. Bounded recursive self-improvement. arXiv preprint arXiv:1312.6764, 2013.
Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.

OpenAI. Gpt-3.5 turbo fine-tuning and api updates. OpenAI blog, 2023a. URL https: / /openai.com/blog/ gpt-3-5-turbo-fine-tuning-and-api-updates.

OpenAI. GPT-4 Technical Report, March 2023b. URL http://arxiv.org/abs/2303.08774. arXiv:2303.08774 [cs].

Poesia, G., Gandhi, K., Zelikman, E., and Goodman, N. D. Certified reasoning with language models. arXiv preprint arXiv:2306.04031, 2023.

Schaeffer, R., Miranda, B., and Koyejo, S. Are emergent abilities of large language models a mirage? In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/ forum?id=ITw9edRDlD.

Schmidhuber, J. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta... hook. PhD thesis, Technische Universität München, 1987.

Schmidhuber, J. Gödel machines: self-referential universal problem solvers making provably optimal selfimprovements. arXiv preprint cs/0309048 and Adaptive Agents and Multi-Agent Systems II, 2003.

Sel, B., Al-Tawaha, A., Khattar, V., Wang, L., Jia, R., and Jin, M. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379, 2023.

Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.

Shypula, A., Madaan, A., Zeng, Y., Alon, U., Gardner, J., Hashemi, M., Neubig, G., Ranganathan, P., Bastani, O., and Yazdanbakhsh, A. Learning Performance-Improving Code Edits, November 2023. URL http: / / arxiv. org/abs / 2302.07867 . arXiv:2302.07867 [cs].

Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460-9471, 2022.

Steunebrink, B. R. and Schmidhuber, J. Towards an actual gödel machine implementation: A lesson in self-reflective systems. In Theoretical Foundations of Artificial General Intelligence, pp. 173-195. Springer, 2012.

Steunebrink, B. R., Thórisson, K. R., and Schmidhuber, J. Growing recursive self-improvers. In International Conference on Artificial General Intelligence, pp. 129139. Springer, 2016

Sumers, T., Yao, S., Narasimhan, K., and Griffiths, T. L. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcomebased feedback. Neural Information Processing Systems (NeurIPS 2022) Workshop on MATH-AI, 2022.

Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An openended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.

Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent Abilities of Large Language Models, October 2022a. URL http: / / arxiv. org/abs / 2206.07682 . arXiv:2206.07682 [cs].

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain of Thought Prompting Elicits Reasoning in Large Language Models, 2022b. URL https://arxiv.org/abs/ 2201.11903 .

Wilson, E. B. Probable inference, the law of succession, and statistical inference. Journal of the American Statistical Association, 22(158):209-212, 1927.

Yampolskiy, R. V. From seed ai to technological singularity via recursively self-improving software. arXiv preprint arXiv:1502.06512, 2015.

Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.

Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. International Conference on Learning Representations (ICLR 2023), 2022.
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.

Zelikman, E., Huang, Q., Poesia, G., Goodman, N. D., and Haber, N. Parsel: Algorithmic reasoning with language models by composing decompositions, 2023.

Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al. Least-to-most prompting enables complex reasoning in large language models. International Conference on Learning Representations (ICLR 2023), 2022a.

Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are humanlevel prompt engineers. International Conference on Learning Representations (ICLR 2023), 2022b.


