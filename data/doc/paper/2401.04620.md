# Agent Alignment in Evolving Social Norms 

Shimin Li ${ }^{1,}$ Tianxiang Sun ${ }^{1,}$ Qinyuan Cheng ${ }^{1,}$ Xipeng Qiu ${ }^{1, *}$<br>${ }^{1}$ Fudan University


#### Abstract

Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent can align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.


## 1 Introduction

The emergence of large language models (LLMs) (Brown et al. 2020; OpenAI, 2023; Chowdhery et al. 2022: Touvron et al. 2023) revolutionize the paradigm of artificial intelligence research and spur a vast array of novel scenarios (OpenAI, 2022). Applications of LLMs, exemplified by Agents (Park et al., 2023; Wu et al. 2023: Li et al. 2023), can further compensate for and enhance the capabilities of LLMs by integrating various augmenting components (Wang et al., 2023d) or tools (Schick et al. 2023). Concurrently, the abilities of intelligent agents with LLMs as their decision-making core also improve as the capabilities of LLMs grow (Xi et al. 2023). When the complexity of tasks that an agent can perform exceeds the level of human oversight (Bowman et al. (2022), designing effective agent alignment methods becomes crucial for Al safety. Furthermore, agents can alter the real physical world through interactions with the actual society (Mendonca et al., 2023). If these systems are not well-regulated, they could pose a series of societal risks.

The prevailing methods for aligning LLMs primarily rely on Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022: Rafailov et al. 2023) or AI Feedback (RLAIF) (Bai et al. 2022: Lee et al. 2023). These approaches aim to align LLMs with predefined, static human values to reduce harmful outputs. However, this essentially aligns with the values defined in pre-selected data. Such alignments might be circumvented when confronted with complex social contexts (Yao et al., 2023b). Moreover, the more appropriate and advanced alignment objectives for humans might be societal values (Kenton et al. 2021; Gabriel, 2020: Liu et al. 2023), which typically establish and evolve. Therefore, the alignment of AI systems necessitates continual updates in response to the advancements in AI capabilities and the evolution of societal norms.

Unlike LLM alignment, agent alignment necessitates a more significant consideration of environmental factors due to their ability to interact with the environment and modify[^0]behavior based on feedback (Shinn et al., 2023; Nathani et al. 2023). However, current alignment efforts are predominantly focused on aligning the language models, overlooking the dynamic nature of agents. Moreover, the work on the social alignment of LLMs primarily concentrates on static environments. In contrast, social norms and values tend to be established and evolve gradually as society progresses (Forbes et al., 2020; Young, 2015), leading to the ineffectiveness of static LLMs alignment strategies. Multiple research endeavors focused on developing agents capable of assimilating feedback from surroundings (Yao et al. 2023c; Shinn et al., 2023). However, a persistent challenge is their vulnerability to environmental fluctuations, which exhibit limitations in rapidly acclimatizing to novel environments (Yao et al., 2023c).

Therefore, we propose an agent alignment method under evolving social norms. Rather than correcting model errors through supervised fine-tuning or RLHF (Ouyang et al., 2022), we reframe the agent alignment issue as a survival-of-the-fittest behavior (Holland, 1992. Hendrycks, 2023) in a multi-agent society. This approach aims to achieve continuous evolution and post hoc values alignment in changing environments. Precisely, to simulate the ongoing evolution of a virtual society, we first defined a dynamic virtual environment, EvolvingSociety, as the societal context for a population of agents. Social norms typically do not evolve top-down (Young, 2015). Thus, we provide only the direction for evolution, with norms forming and evolving through agents' interactions. Moreover, the evolution of social norms mimics the punctuated equilibrium effect (Young, 2015), maintaining stability over a period and then being replaced by newly established norms at specific points in time. Each agent in the social group possesses unique personal traits, professions, and values, engaging with people and objects in society to realize their social and self-values. Concurrently, agents exhibit evolutionary behavior, continually self-improving based on environmental factors and feedback.

To evaluate the extent to which an agent adheres to social norms, we conceptualized a highly abstract social observer. This observer employs questionnaires to assess the alignment or adaptability of each individual within society to the prevailing social norms based on the agent's behavioral trajectory and statements. As generations progress, social-good agents that better align with contemporary social norms achieve higher adaptability and are preserved for the next generation. They are more likely to reproduce and foster subsequent generations of agents. Those agents displaying inadequate alignment with social norms are dwindled and replaced by new-born agents. Through iteration and updates under the conditions of survival of the fittest, agents increasingly adapted to society emerge. Their beliefs and behaviors will increasingly align with the prevailing social norms.

Our experimental results demonstrate that the EvolutionaryAgent, through the principle of survival of the fittest, can evolve to better adapt to changing environments while maintaining competence in general tasks. Overall, the primary contributions can be summarized as follows: (i) We introduce the EvolutionaryAgent, a method for the evolution and alignment of agents in dynamic environments based on the survival of the fittest. (ii) We design an evolving environment, EvolvingSociety, and a method for assessing agents in changing environments. (iii) We systematically define agent alignment and, through experiments based on various LLMs, prove that the EvolutionaryAgent can continuously produce agents aligned with evolving social norms.

## 2 Related Work

### 2.1 LLM Alignment

LLMs acquire extensive world knowledge by learning from vast corpora during the pretraining phase (Brown et al. 2020. Chowdhery et al. 2022). However, their objective of predicting the next token does not explicitly align with human preferences or values (Ouyang et al. 2022 Touvron et al. 2023). Consequently, the alignment of LLMs has increasingly gained attention (Wang et al. 2023c: Shen et al., 2023a). This alignment predominantly includes methods that utilize human supervisory signals as feedback (Glaese et al., 2022. Touvron et al., 2023) and those that use AI (Lee et al., 2023; Liu et al., 2023, Bai et al., 2022 Sun et al., 2023)for supervision. Yao et al. (2023a) categorizes alignment goals into three

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-03.jpg?height=266&width=390&top_left_y=298&top_left_x=434)

(a) LLM Alignment

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-03.jpg?height=282&width=832&top_left_y=279&top_left_x=858)

(b) Agent Alignment

Figure 1: Disparities between LLM alignment and agent alignment. (a) LLM iteratively aligns with values under human intervention. (b) Agents perceive values from the environment, make actions that affect the environment, and self-evolve after receiving feedback from the environment.

progressively advancing levels: 1) Instruction alignment: Complying with human directives and accomplishing various tasks (Wang et al., 2022: Chung et al. 2022: Sanh et al., 2022). 2) Human preference alignment: Learning from preference data that includes human feedback or comparative signals (Stiennon et al., 2020; Ouyang et al., 2022). 3) Value alignment: Aligning the model with goals that reflect collective human core values (Ganguli et al., 2022: Askell et al., 2021: Gabriel 2020: Liu et al. 2023). Unlike the alignment of LLMs, the agent iteratively improves its capabilities through continuous interactions with the environment (Zhang et al. 2023). Consequently, the alignment of the foundational model may gradually diminish during the agent's self-adaptive process. Therefore, this work focuses on exploring the selection of well-aligned intelligent agents from the perspective of survival of the fittest, when agents are capable of self-evolution.

### 2.2 Self-Evolution of AI System

Once the LLM is trained, its parameters and knowledge remain fixed, and the cost of retuning it to adapt to updated facts or preferences continuously is relatively high (OpenAI, 2023: Touvron et al. 2023). To facilitate the ongoing evolution of AI systems, some work concentrated on empowering these systems to improve their outputs based on external feedback iteratively (Pan et al. 2023), which can be categorized into self-feedback (Madaan et al., 2023 Lu et al., 2023: Zelikman et al. 2023: Shinn et al., 2023) and external feedback (Gou et al. 2023: Welleck et al., 2023). Depending on whether the model parameters are frozen, this can be further divided into iterative optimization during the training phase Ouyang et al. 2022: Touvron et al., 2023; Glaese et al., 2022: Gülçehre et al., 2023) or posthoc correction (Jiang et al. 2023; Madaan et al. 2023 Shinn et al. 2023). Nonetheless, executing multiple optimization iterations on LLMs within the training stage is costly. Post-hoc correction methods that iteratively optimize LLM outputs cannot teach the model patterns for improvement. Therefore, we study the self-evolutionary behavior of AI systems from the agent's perspective and achieve continuous agent evolution by modifying the non-parametric components of the agent while the model parameters are fixed.

## 3 Agent Alignment

### 3.1 LLM Alignment

LLM alignment aims to bridge the gap between the task of next-word-prediction and human-defined values such as helpfulness, harmlessness, and honesty (Askell et al. 2021. Yang et al. 2023). Suppose human preferences or values are denoted by Value ${ }_{t}$, which can either remain constant or shift during the iterative alignment process across rounds $t$, as illustrated in Fig. 1](a). The alignment process of an LLM can be defined as:

$$
\operatorname{LLM}_{t+1}=f_{M}\left(\operatorname{LLM}_{t}, \text { Value }_{t}\right)
$$

where $f_{M}()$ represents the model alignment or evolution process under human intervention. This typically involves either imitation learning directly on a dataset containing prefer-
ence information or infusing preference information into the LLM through reinforcement learning.

### 3.2 Agent Alignment

Considering the construction of agent in Xi et al. (2023); Wang et al. (2023a), we define the agent $\lambda_{t}$ as an AI system equipped with a perception module for sensing the external environment, a core decision-making module centered around an LLM, a memory module, and a behavior module. Unlike LLM alignment, which passively receives human-selected values, the agent in the alignment process acquires observations $o \in \mathcal{O}$ through its perception module, including value information in the current environment. Then, the decision center based on the LLM makes plans and takes actions $a \in \mathcal{A}$. The external environment provides feedback $F B$ in response to the agent's actions, which is used for the agent's self-evolution:

$$
\lambda_{t+1}=f_{S}\left(\lambda_{t}, \text { Value }_{t}, o, a, F B\right)
$$

where $f_{S}()$ is the autonomous decision-making and alignment process of the agent, including updates to the memory module and the LLM parameters as depicted in Fig. 1 (b).

## 4 Evolutionary Agent in Evolving World

Current research on agents primarily focuses on how to endow them with enhanced capabilities or the ability to perform a broader range of tasks (Wang et al., 2023d; Shen et al. 2023b), including how agents can self-improve. As agents' abilities continue to advance, the importance of researching agent supervision and alignment becomes increasingly significant. We introduce EvolutionaryAgent, a framework for agent evolution and alignment in dynamic environments. Initially, we formalize the definition of dynamic environments based on the notation habits from Abel et al. (2023), explaining how these environments are constructed. We then define the agent and how its behavior in these dynamically changing environments is evaluated for adherence to social norms.

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-04.jpg?height=520&width=1363&top_left_y=1518&top_left_x=381)

Figure 2: The framework primarily comprises four processes: a) Agents interact with others or the environment within a societal context. b) Observers evaluate the fitness of agents based on current social norms and assessment criteria. c) Agents better aligned with current social norms engage in crossover and mutation behaviors, thereby propagating new agents. d) The strategies of agents with higher fitness prompt the evolution and establishment of social norms.

### 4.1 Initialization of Agent and Evolving Society

We endeavor to simulate agents' characteristics and behavioral patterns in a real-world scenario. Our evaluation of these agents hinges on their behavioral trajectories and adherence
to social norms, particularly in environments where such norms are subject to dynamic changes. Consequently, agents within the society are endowed with various personified attributes, encompassing personality, profession, and core values. Moreover, agents possess a fundamental memory function, serving as a repository for recording their actions, understanding the world, and receiving social feedback.

To elucidate, we established a small-scale society termed EvolvingSociety, wherein we define $g \in \mathbf{G}$ as the range of generations over which societal evolution occurs. Each generation is further subdivided into smaller time steps $t \in\left[g_{j}, g_{j+1}\right] \subseteq \mathbf{G}$. Subsequently, we introduce a continually changing set of environments $\mathbf{E}$, with each element $e_{t} \in \mathbf{E}$ representing the prevailing environment at time $t$. Within each generation $g$, the social norms are denoted as $r_{g} \in \mathbf{R}$, accompanied by an evaluation questionnaire $c_{g} \in \mathbf{C}$, which is employed to assess the extent to which an agent adheres to these social norms. Furthermore, we define a set of agents $\Lambda$, representing each agent as $\lambda \in \Lambda$.

Agents are characterized by distinct personas $\mathcal{P}$, careers $\mathcal{C}$, and three views $v=$ $\left(v_{\text {world }}, v_{\text {life }}, v_{\text {value }}\right) \in \mathcal{V}$ (comprising worldviews, life perspectives, and values). These elements constitute the fundamental attributes of an agent $\mathcal{T}=\{\mathcal{P}, \mathcal{C}, \mathcal{V}\}$, as depicted in Fig. 2 Combining these varied role characteristics and the agent's observation of the environment $o \in \mathcal{O}$ influences their behavior or strategy $a \in \mathcal{A}$ in different settings. Each agent's observations, actions, and received feedback in the environment contribute to forming their short-term memory $m$ and long-term memory M. Consequently, the function of an agent can be represented as a probability simplex based on its attributes and sequence of actions:

$$
\begin{equation*}
\lambda: \mathcal{T} \times \mathcal{O} \times m \times \mathbf{M} \rightarrow \Delta(\mathcal{A}) \tag{1}
\end{equation*}
$$

### 4.2 Environmental Interaction

Agents spontaneously interact with the environment or other agents within it. Specifically, at time $t$, an agent situated at a location within environment $e$ assimilates partial information or states from the current environment as its observation $o_{t}$. These observations assist the agent in determining its next course of action, which might range from simple activities like shopping in a store to communicating with other agents. Subsequently, the agent records events observed within its perceptual range during time $t$ into its short-term memory. This includes its actions and the results of its environmental observations, collectively forming the agent's perceptual data. When the length of short-term memory reaches a certain threshold, it is compressed into long-term memory, emphasizing the recording of broader, higher-level content such as summaries of events and feedback from the environment.

### 4.3 Fitness Evaluation with Feedback

The theory of Basic Values (Schwartz, 2006) posits values as motivators for behavior. Thus, we evaluate an agent's adherence to social norms based on its behavioral trajectory and statements regarding social norms. We conceptualize a highly abstract social evaluator, which could be a human, an LLM, or a model-assisted human overseer. These evaluators assess the adaptability of each agent within the EvolvingSociety, providing feedback accordingly. Specifically, we define the function:

$$
\begin{equation*}
\Phi: h_{\lambda} \times s_{\lambda} \times \mathbf{R} \times \mathbf{C} \rightarrow(\mathbb{R}, \mathrm{FB}), \tag{2}
\end{equation*}
$$

where $h_{\lambda}$ represents the agent's behavioral trajectory in the current time frame, $s_{\lambda}$ is the agent's statement regarding the evaluation questionnaire $c_{g}$ of current era norms, and FB is a collection of feedback in natural language form. Consequently, the social evaluator assesses each agent's adaptability based on their behavioral trajectories and statements, providing them with abstract natural language feedback. This feedback assists agents in adjusting their behaviors, thereby aiding them in better adapting to the social environment and enhancing their alignment with social norms.

### 4.4 Evolution of Agent

The fitness of an agent reflects its alignment with the current social norms, which determines whether the agent can continue to exist in the current society. Agents with higher fitness are perceived as having an advantage in the evolutionary game; they survive into the next generation and have a higher probability of producing offspring agents. Conversely, agents with lower fitness rankings are likely to be outcompeted by the offspring of more dominant agents. To begin with, we calculate the set of fitness values for all agents:

$$
\begin{equation*}
F\left(\Lambda, r_{g}, c_{g}\right)=\left\{F\left(h_{\lambda}, s_{\lambda}, r_{g}, c_{g}\right) \mid \lambda \in \Lambda\right\} \tag{3}
\end{equation*}
$$

Here, the top $\mathrm{p} \%$ of socially well-adapted agents survive into the next generation and have a higher probability of reproducing to generate offspring agents $E(\Lambda, p)$. The reproduction process of an agent includes two phases: crossover and mutation. During the crossover phase, two agents from the socially well-adapted pool are randomly selected for reproduction. The resulting offspring inherit their parents' persona, career, and worldviews with a $50 \%$ probability each. We define the $\operatorname{CRO}(\cdot)$ function to represent the crossover operation between two agents that produce offspring, $\mathrm{CRO}\left(\lambda_{e_{1}}, \lambda_{e_{2}}\right) \rightarrow \lambda_{\text {offspring }}$.

Further, the evolutionary process of organisms often involves various mutation behaviors. Mutation behaviors enable agents to produce offspring more likely to align with current social norms during reproduction. Therefore, in the mutation phase, the offspring's persona, career, and worldviews may mutate with a probability of $m \in[0,1]$. We define the MUT(.) function as the mutation function, $\operatorname{MUT}\left(\lambda_{\text {offspring }}, m\right) \rightarrow \lambda_{\text {offspring }}^{\prime}$, where MUT(), as a functional operator, is responsible for modifying given characteristics. For instance, it utilizes the personas of the parents and corresponding prompts to guide LLMs in generating the persona of their offspring. The mutation of careers and worldviews follows a similar process. Fig. 8 shows the detailed mutation prompts. Thus, the act of agents reproducing offspring can be defined as:

$$
\begin{equation*}
\operatorname{Offspring}(E(\Lambda, p), m)=\left\{\operatorname{MUT}\left(\operatorname{CRO}\left(\lambda_{i}, \lambda_{j}\right), m\right) \mid \lambda_{i}, \lambda_{j} \in E(\Lambda, p)\right\} \tag{4}
\end{equation*}
$$

The offspring produced by socially well-adapted agents are integrated into society, replacing the bottom $\mathrm{p} \%$ of agents in the fitness ranking, denoted as $P(\Lambda, p)$. Consequently, the societal group of the next generation is:

$$
\begin{equation*}
\Lambda^{\prime}=\Lambda_{\backslash P(\Lambda, p))} \cup \operatorname{Offspring}(E(\Lambda, p), m) \tag{5}
\end{equation*}
$$

### 4.5 Evolving Social Norms

Social norms are often behavioral regularity based on shared societal beliefs, typically emerging from a bottom-up process and evolving through trial and error (Young, 2015). In the context of agent alignment, we aim not to permit the evolution of social norms to be disorderly or random or to intervene in each step of their evolution overly. Instead, we provide a directional guide for the evolution of these norms. For instance, we define only the initial social norms $r_{0}$ and the desired direction of evolution $r_{v}$, within which agents in the society engage in various behaviors or strategies. The behavioral trajectories of these agents are then assessed, earning them corresponding fitness (payoffs). Agents with higher fitness are more likely to reproduce, leading to the diffusion or learning of their strategies, gradually stabilizing and forming new social norms. Specifically, the formation of social norms in a given era $g$ is based on the strategy trajectories of the top $q \%$ of agents ranked by fitness in the population, along with the evolutionary direction:

$$
\begin{equation*}
r_{g+1}=\operatorname{Evolve}\left(h_{\lambda}, r_{v}\right), \lambda \in E(\Lambda, q) \tag{6}
\end{equation*}
$$

Through this survival of the fittest agent evolution strategy, agents better aligned with social norms are preserved in round after round of iteration, as illustrated in Alg. 1.

## 5 Experiments

### 5.1 Configuration

We designed the information framework for the virtual society and the attributes of its agents, encompassing personas (Tab. 2), three views (Tab. 3), and career information (Tab. (1). In setting the temporal scope for studying social norms, we considered that the establishment and evolution of social norms often occur gradually (Young, 2015). Consequently, the temporal range for the virtual environment is set from 2000 to 2050 , a considerably extended period, with each decade representing a generation ${ }^{1}$. Correspondingly, each generation is associated with an evolving social norm. The initial norms and the direction of their evolution are presented in Tab. 4 All experiments were conducted thrice, and the average values were taken as the final results.

### 5.2 Evaluation Methodology

The questionnaire for evaluating agents is dynamically generated in the evolutionary process of social norms. Specifically, we employ GPT-4 as the generator of the questionnaire, producing diverse questions for agent evaluation based on the current social norms. Tab. 7 illustrates the prompt for generating these questionnaires. To ensure the reproducibility of our experiments, we predefined the social norms and evaluation questions for each generation unless otherwise stated. Each questionnaire is designed with questions originating from ten different perspectives, as detailed in Tab. 5 We also evaluated the EvolutionaryAgent when social norms and evaluation questionnaires were dynamically generated in Sec. A.4.1. We set the timestep to 2 , meaning that the agents within the society are evaluated biennially. Specifically, for each agent, we input their personality, occupation, three views, current social norms, assessment questionnaire, statements to the questionnaire, and their behavioral history into the evaluation model. The evaluation model then outputs a fitness score and feedback for each agent. Details of the evaluation model's prompt are in Tab. 9

### 5.3 Model Details

LLMs Selection We investigated the EvolutionaryAgent's performance with diverse models. For close-source models, we tested GPT-3.5-turbo-1106 (OpenAI, 2022), GPT-3.5-turboinstruct, and Gemini-Pro (Akter et al., 2023) as the foundation models for the agent. In the case of open-source models, we utilized the Vicuna-7B-v1.3 (Chiang et al. 2023), Mistral7B-Instruct-v0.2 (MistralAI, 2023), and Llama2-7B-Chat (Touvron et al., 2023). Existing work demonstrates that powerful LLMs can serve as effective evaluators (Zheng et al. 2023: Madaan et al. 2023: Rafailov et al. 2023). Accordingly, we primarily utilized GPT-4 and GPT-3.5-Turbo as models for observers while also examining the efficacy of various other LLMs in this role, as demonstrated in Sec. 6.2 Unless otherwise specified, GPT-3.5-Turbo is the default choice for the evaluator, owing to its optimal balance of efficiency, performance, and cost-effectiveness.

Baselines We compared two relatively recent agent frameworks, ReAct (Yao et al., 2023c) and Reflexion (Shinn et al. 2023). Both are capable of thinking and gradually self-improving based on environmental feedback. For the context of this paper, we also partially adapted methods within the EvolvingSociety. In ReAct, 'Thought' corresponds to the agent's internal deliberations before taking action,'Action' represents the agent's responses to the current social norms evaluations, and 'Observation' includes feedback provided by the societal observer. Reflexion, building on ReAct, further introduces the behavior of reflection, that is, self-reflection based on environmental feedback in natural language, serving as a signal for the agent's improvement. ReAct and Reflexion have a maximum memory timestep of 3 , retaining the history of the past three rounds.[^1]

### 5.4 Adapting to Evolving Social Norms

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-08.jpg?height=758&width=1394&top_left_y=374&top_left_x=362)

Figure 3: When using different open-source and closed-source LLMs as the foundational models for the EvolutionaryAgent and the compared baselines, we observe variations in fitness within an EvolvingSociety. Social norms evolve at the start of each generation, marked by the black vertical lines. The EvolutionaryAgent consistently demonstrates an adaptive capability to adjust to these changing social norms continually.

We employed six distinct LLMs, both open-sourced and close-sourced, as the foundation model for the EvolutionaryAgent to validate the efficacy, as illustrated in Fig. 3. The green lines represent the direct application of these LLMs to address evaluation questionnaires under the social norms of the current generation. These models do not incorporate previous generations' information into their memory or utilize environmental feedback signals for iterative improvement.

EvolutionaryAgent outperforms other methods when adapting to changing social norms. In comparing different approaches, ReAct experiences a decrease in fitness value at the first timestep of a new generation, indicating a decline in adaptability to evolving social norms. For instance, ReAct's fitness values at timesteps 2010, 2020, and 2030 illustrate a downward trend compared to those in 2008, 2018, and 2028. Although ReAct can gradually adapt to the environment in subsequent years through observation as environmental feedback, each shift in era-specific norms significantly impacts it. Due to the self-reflection mechanism, Reflexion possesses a superior ability to adapt to the current static environment within a single generation, compared to ReAct. However, when norms evolve, Reflexion still encounters a rapid decline in fitness value for its memory-retaining content from the previous generation. These memories influence its actions in the following generation. In contrast, EvolutionaryAgent maintains a relatively stable adaptation to the current era amidst normative changes. This stability arises because, although individuals in EvolutionaryAgent also remember content from previous eras, there are likely some agents within the population whose strategies are well-adapted to the social norms of the next generation.

EvolutionaryAgent is robust to model variations. When employing different LLMs as the foundation for agents, the EvolutionaryAgent supported by GPT-3.5-Turbo and Gemini-Pro not only maintains its fitness to adapt to changing environments, but its adaptability also shows further enhancement. Our case analysis in Sec. A.5 reveals the models' ability to provide better environmental feedback and utilize subsequent feedback more effectively to adapt to the current environment. Among the three open-source models, the agent based
on Mistral exhibits the finest performance, indicating that a more capable foundation model also possesses a more vital ability to leverage environmental feedback for self-improvement.

## 6 Analysis

### 6.1 Alignment without Compromising Capability

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-09.jpg?height=320&width=461&top_left_y=610&top_left_x=366)

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-09.jpg?height=320&width=462&top_left_y=610&top_left_x=820)

![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-09.jpg?height=320&width=460&top_left_y=610&top_left_x=1277)

Figure 4: Evaluating the performance of EvolutionaryAgent in aligning with social norms while executing functional downstream tasks. The "Overall Score" is the average of the functionality score and alignment score. The EvolutionaryAgent can adapt to social norms while maintaining its performance in completing downstream tasks.
![](https://cdn.mathpix.com/cropped/2024_05_29_d35c49ac85ee48b9c030g-09.jpg?height=446&width=1054&top_left_y=1178&top_left_x=533)

Figure 5: a) The influence of various quality models as the foundation for the EVOLUTIONARYAGENT. b) The Utilization of diverse LLMs as observers.

To explore whether the EvolutionaryAgent can maintain its competence in effectively completing specific downstream tasks while aligning with evolving social norms, we evaluated the agent's performance on several downstream tasks alongside its alignment with social norms. The agent was assessed on Vicuna Eval (Chiang et al. 2023), Self-instruct Eval (Wang et al. 2023b), and Lima Eval (Zhou et al. 2023). To save costs, only 50 samples from each dataset were tested. The method of evaluating the agent's performance on these three test sets was consistent with the MT-Bench, but the maximum score was scaled down to 7, maintaining the same scoring range as the alignment score. The results on the three downstream task datasets, as shown in Fig. 4 . indicate that while the EvolutionaryAgent's alignment score continually increased, its scores on specific downstream tasks also improved. This suggests that the EvolutionaryAgent can effectively align with social norms while still performing downstream tasks well.

### 6.2 Quality of Diverse Observer

Given the distinct preferences and varying strengths of different LLMs, we further explored the performance of our method when using various LLMs or human observers. As observed in Fig. 5, the range of fitness scores generated by the EvolutionaryAgent shows significant variation when assessed by different LLMs. Notably, Gemini-Pro and Claude-2.1 yielded the most similar evaluation scores, with GPT-4 being the most conservative in its
scoring. Additionally, GPT-4 demonstrated greater internal consistency in scoring within each generation, while Gemini-Pro and Claude-2.1 exhibited the greatest variability across different generations, and GPT-3.5-Turbo showed moderate variation. Regarding alignment with human preferences, GPT-4 closely matched human evaluation scores, suggesting it remains the optimal choice as an evaluator when cost is not a consideration.

### 6.3 Scaling Effect

We investigated the impact of scaling effects in LLMs on the EvolutionaryAgent. We selected open-source models at three different parameter scales: Llama2-7B, 13B, and 70B, along with GPT-3.5-Turbo. As evident from Fig. 5, there is a relative increase in the fitness values of the EvolutionaryAgent across different generations with an increase in model parameters or performance enhancement. This is attributed to higher-performing baseline models having a more comprehensive understanding of current social norms and making more advantageous decisions and statements for their development.

Furthermore, to explore the impact of different operators in EvolutionaryAgent, we set varying population sizes and mutation rates for agents. A larger number of agents implies greater diversity in society. As shown in Fig. 6. EvolutionaryAgent consistently demonstrates strong adaptability to changing societies with varying agent counts. With an increase in the number of agents, Evolutionary Agent tends to achieve better outcomes at each time step. A larger population increases the likelihood of having agents in the community that can adapt to changing times. We further analyze the impact of different mutation rates on the overall performance. As the mutation rate $m$ increases, the overall fitness value of EvolutionaryAgent rises, and so does the variance in these values. This suggests that a more extensive $m$ increases exploration and thus increases the likelihood of producing more adapted agents.

## 7 Discussion and Future Work

We introduce a framework for agent evolution and alignment based on evolutionary strategies, offering a novel approach to constructing socially beneficial AI systems. However, it's imperative to acknowledge that social norms represent an expansive and abstract concept. We cannot define social norms wholly and precisely, nor can we accurately predict the evolution of social norms. Therefore, this paper only deconstructs and defines social values from certain perspectives and artificially constructs hypothetical scenarios for the evolution of social norms. This approach is adopted to investigate the alignment behavior of agents within an evolving social context.

On the other hand, defining a complete, realistic, and complex virtual society is challenging. However, if such a virtual society existed, it could further enable evolving intricate evolutionary behaviors of agents and the emergence of new capabilities. This also provides a sandbox for investigating the safety of AI systems before they impact the real world. Regarding agents' self-evolution and alignment, we approach the design of iterative strategies for agents from the perspective of evolutionary algorithms. It may be worthwhile to explore how to simulate more anthropomorphic agents, design more efficient self-evolution algorithms for agents, and provide higher-quality feedback signals. Additionally, while we focus on a textual virtual world and agent construction, further research in agent alignment and the construction of virtual societies in other modalities deserve future exploration.

## Ethical Considerations

In our definition of social norms, we do not presuppose any inherent characteristics of the norms themselves, such as bias, discrimination, or oppression. Instead, we focus solely on whether the agent groups align with these social norms. It's essential to recognize that social norms are continuously evolving, and our framework is specifically designed to adapt and guide this progression. Consequently, situations may arise where the evolution deviates significantly from the intended direction or results in unethical social norms.

This necessitates periodic oversight by regulators to monitor the evolution of these norms and intervene in potential negative outcomes. In our methodology, the agents are also in constant evolution, which means that adaptive agents may exhibit unpredictable behaviors. Strategies to mitigate potential negative impacts must be developed, such as extensive testing of agents in a simulated social environment to ensure controllability.

## Acknowledgments

Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.

## References

David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and Satinder Singh. A definition of continual reinforcement learning. CoRR, abs/2307.11046, 2023. doi: 10.48550/ARXIV.2307.11046. URL https://doi.org/10.48550/arXiv. 2307 11046

Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, and Graham Neubig. An in-depth look at gemini's language abilities, 2023.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861, 2021. URL https://arxiv.org/abs/2112.00861.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073, 2022. doi: 10.48550/ARXIV.2212.08073. URL https://doi.org/10. 48550/arXiv.2212.08073.

Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosiute, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac HatfieldDodds, Ben Mann, and Jared Kaplan. Measuring progress on scalable oversight for large language models. CoRR, abs/2211.03540, 2022. doi: 10.48550/ARXIV.2211.03540. URL https://doi.org/10.48550/arXiv.2211.03540.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/ARXIV.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022. doi: 10.48550/ARXIV.2210.11416. URL https://doi.org/10.48550/arXiv. 2210 11416

Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. Social chemistry 101: Learning to reason about social and moral norms. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 653-670, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.48. URL https://aclanthology.org/2020.emnlp-main. 48

Iason Gabriel. Artificial intelligence, values, and alignment. Minds Mach., 30(3): 411-437, 2020. doi: 10.1007/S11023-020-09539-2. URL https://doi.org/10.1007/ s11023-020-09539-2.

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli TranJohnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. CoRR, abs/2209.07858, 2022. doi: 10.48550/ARXIV.2209. 07858. URL https://doi.org/10.48550/arXiv.2209.07858

Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin J. Chadwick, Phoebe Thacker, Lucy CampbellGillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard

Green, Sona Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements. CoRR, abs/2209.14375, 2022. doi: 10.48550/ARXIV.2209.14375. URL https://doi.org/10.48550/arXiv.2209.14375.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: large language models can self-correct with tool-interactive critiquing. CoRR,abs/2305.11738, 2023. doi: 10.48550/ARXIV.2305.11738. URL https://doi.org/ 10.48550/arXiv.2305.11738

Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling. CoRR, abs/2308.08998, 2023. doi: 10.48550/ARXIV.2308. 08998. URL https://doi.org/10.48550/arXiv.2308.08998

Dan Hendrycks. Natural selection favors ais over humans. CoRR, abs/2303.16200, 2023. doi: 10.48550/ARXIV.2303.16200. URL https://doi.org/10.48550/arXiv.2303.16200

John H. Holland. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. MIT Press, 1992. ISBN 9780262275552. doi: 10.7551/MITPRESS/1090.001.0001. URL https://doi.org/10. $7551 / \mathrm{mitpress} / 1090.001 .0001$.

Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. CoRR, abs/2306.02907, 2023. doi: 10.48550 / ARXIV.2306.02907. URL https://doi.org/10.48550/arXiv.2306.02907.

Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. Alignment of language agents. CoRR, abs/2103.14659, 2021. URL https://arxiv.org/abs/2103.14659

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: scaling reinforcement learning from human feedback with AI feedback. CoRR, abs /2309.00267, 2023. doi: 10.48550/ARXIV.2309.00267. URLhttps://doi.org/10.48550/arXiv.2309.00267.

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: communicative agents for "mind" exploration of large scale language model society. CoRR, abs/2303.17760, 2023. doi: 10.48550/ARXIV.2303.17760. URL https://doi.org/10.48550/arXiv.2303.17760.

Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models in simulated human society. CoRR, abs/2305.16960, 2023. doi: 10.48550/ARXIV.2305.16960. URL https://doi.org/10.48550/arXiv.2305.16960.

Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng Shang, and Qun Liu. SELF: language-driven self-evolution for large language model. CoRR, abs/2310.00533, 2023. doi: 10.48550/ARXIV.2310.00533. URL https://doi.org/10.48550/arXiv.2310.00533.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651, 2023. doi: 10.48550/ARXIV.2303.17651. URL https://doi.org/10.48550/arXiv.2303.17651.

Russell Mendonca, Shikhar Bahl, and Deepak Pathak. ALAN: autonomously exploring robotic agents in the real world. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pp. 3044-3050. IEEE, 2023. doi: 10.1109/ICRA48891.2023.10161016. URL https://doi.org/10.1109/ICRA48891.2023. 10161016 .

MistralAI. Mistral 7b, 2023. URLhttps://mistral.ai/news/announcing-mistral-7b/.

Deepak Nathani, David Wang, Liangming Pan, and William Yang Wang. MAF: multi-aspect feedback for improving reasoning in large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 6591-6616. Association for Computational Linguistics, 2023. URL https://aclanthology .org/2023. emnlp-main. 407

OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html.

Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. CoRR, abs / 2308.03188, 2023. doi: 10.48550 / ARXIV.2308. 03188. URL https://doi.org/10.48550/arXiv.2308.03188

Joon Sung Park, Joseph C. O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Sean Follmer, Jeff Han, Jürgen Steimle, and Nathalie Henry Riche (eds.), Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pp. 2:1-2:22. ACM, 2023. doi: 10.1145/3586183.3606763. URL https://doi.org/10.1145/3586183.3606763

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In ICML 2023 Workshop The Many Facets of Preference-Based Learning, 2023. URL https://openreview.net/forum?id=53HUHMvQLQ

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 2529, 2022. OpenReview.net, 2022. URLhttps://openreview.net/forum?id=9Vrb9DOWI4

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, abs / 2302.04761, 2023. doi: 10.48550/ARXIV.2302. 04761. URL https://doi.org/10.48550/arXiv.2302.04761

Shalom H Schwartz. Basic human values: Theory, measurement, and applications. Revue francaise de sociologie, 47(4):929-968, 2006.

Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. CoRR, abs/2309.15025, 2023a. doi: 10.48550/ARXIV.2309.15025. URL https://doi.org/10. 48550/arXiv.2309.15025.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023b.

Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1f89885d556929e98d3ef9b86448f951-Abstract.html.

Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. CoRR, abs/2305.03047, 2023. doi: 10.48550/ ARXIV.2305.03047. URLhttps://doi.org/10.48550/arXiv.2305.03047.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https: //doi.org/10.48550/arXiv.2307.09288

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents. CoRR, abs/2308.11432, 2023a. doi: 10.48550/ARXIV.2308.11432. URL https://doi.org/10.48550/arXiv.2308.11432

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 5085-5109. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.340. URL https://doi.org/10.18653/v1/2022.emnlp-main.340.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 13484-13508. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.ACL-LONG.754. URL https://doi.org/10.18653/v1/2023.acl-long. 754

Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. CoRR, abs/2307.12966, 2023c. doi: 10.48550/ARXIV.2307.12966. URL https://doi.org/ 10.48550/arXiv.2307.12966

Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. JARVIS-1: open-world multi-task agents with memory-augmented multimodal language models. CoRR, abs/2311.05997, 2023d. doi: 10.48550/ARXIV.2311.05997. URL https://doi.org/ 10.48550/arXiv. 2311.05997

Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= hH36JeQZDaO

Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ ARXIV.2308.08155. URL https://doi.org/10.48550/arXiv.2308.08155

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. CoRR, abs/2309.07864, 2023. doi: 10.48550/ARXIV.2309.07864. URL https://doi.org/10.48550/arXiv.2309.07864

Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty, 2023.

Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. From instructions to intrinsic human values - A survey of alignment goals for big models. CoRR, abs/2308.12014, 2023a. doi: 10.48550/ARXIV.2308.12014. URL https://doi.org/10.48550/arXiv. 2308 12014

Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. From instructions to intrinsic human values - A survey of alignment goals for big models. CoRR, abs /2308.12014, 2023b. doi: 10.48550/ARXIV.2308.12014. URL https://doi.org/10.48550/arXiv. 2308 12014

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023c. URL https://openreview.net/pdf?id=WE_vluYUL-X.

H Peyton Young. The evolution of social norms. economics, 7(1):359-387, 2015.

Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (STOP): recursively self-improving code generation. CoRR, abs $/ 2310.02304,2023$. doi: 10.48550/ARXIV.2310.02304. URL https://doi.org/10.48550/arXiv.2310.02304.

Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning agents. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/ forum?id=ZcJa1R6j3v

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment. CoRR, abs/2305.11206, 2023. doi: 10.48550/ARXIV.2305.11206. URL https://doi.org/10.48550/arXiv.2305.11206
