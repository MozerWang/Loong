# Large language models surpass human experts in predicting neuroscience results 

Xiaoliang Luo ${ }^{1, \dagger,},{ }^{*}$, Akilles Rechardt ${ }^{1, \dagger}$, Guangzhi Sun ${ }^{2, \dagger}$,<br>Kevin K. Nejad ${ }^{3,6,}{ }^{\dagger}$, Felipe Yáñez ${ }^{4, \dagger}$, Bati Yilmaz ${ }^{5, \dagger}$, Kangjoo Lee ${ }^{8}$,<br>Alexandra O. Cohen ${ }^{9}$, Valentina Borghesani ${ }^{10}$, Anton Pashkov ${ }^{11,12,13}$,<br>Daniele Marinazzo ${ }^{14}$, Jonathan Nicholas ${ }^{15}$, Alessandro Salatiello ${ }^{16}$, Ilia Sucholutsky ${ }^{17}$, Pasquale Minervini ${ }^{18}$, Sepehr Razavi ${ }^{19}$, Roberta Rocca ${ }^{20}$, Elkhan Yusifov ${ }^{21}$, Tereza Okalova ${ }^{22}$, Nianlong Gu², Martin Ferianc ${ }^{24}$, Mikail Khona ${ }^{25}$, Kaustubh R. Patil ${ }^{26,}$, 27, Pui-Shee Lee ${ }^{28,29}$, Rui Mata ${ }^{30}$, Nicholas E. Myers ${ }^{31}$, Jennifer K Bizley ${ }^{32}$, Sebastian Musslick ${ }^{33,34}$, Isil Poyraz Bilgin ${ }^{35}$, Guiomar Niso ${ }^{36}$, Justin M. Ales ${ }^{37}$, Michael Gaebler ${ }^{38}$, N Apurva Ratan Murty ${ }^{39}$, Leyla Loued-Khenissi ${ }^{40}$, Anna Behler ${ }^{41}$, Chloe M. Hall ${ }^{42,43}$, Jessica Dafflon ${ }^{44,45}$, Sherry Dongqi Bao ${ }^{46}$, and Bradley C. Love ${ }^{1,7, \dagger}$

${ }^{1}$ Department of Experimental Psychology, University College London, London, United Kingdom, ${ }^{2}$ Department of Engineering, University of Cambridge, Cambridge, United Kingdom, ${ }^{3}$ Department of Physiology, Anatomy \& Genetics, University of Oxford, Oxford, United Kingdom, ${ }^{4}$ Max Planck Institute for Neurobiology of Behavior - caesar, Bonn, Germany, ${ }^{5}$ National Magnetic Resonance Research Center (UMRAM), Bilkent University, Ankara, Turkey, ${ }^{6}$ Department of Computer Science, University of Bristol, Bristol, United Kingdom, ${ }^{7}$ The Alan Turing Institute, London, United Kingdom,

${ }^{8}$ Department of Psychiatry, Yale University, New Haven, United States, ${ }^{9}$ Psychology, Emory University, Atlanta, United States, ${ }^{10}$ Faculty of Psychology and Educational Sciences, Université de Genève, Genève, Switzerland, ${ }^{11}$ Neurosurgery, Novosibirsk State Medical University, Novosibirsk, Russian Federation, ${ }^{12}$ Federal Center of Neurosurgery, FSBI, Novosibirsk, Russia, ${ }^{13}$ Department of Data Collection and Processing Systems, Novosibirsk State Technical University, Novosibirsk, Russia,

${ }^{14}$ Department of Data Analysis, Ghent University, Ghent, Belgium, ${ }^{15}$ Psychology, New York University, New York, United States, ${ }^{16}$ Department of Cognitive Neurology, University of Tübingen, Tübingen, Germany, ${ }^{17}$ Computer Science, Princeton University, Princeton, United States, ${ }^{18}$ ILCC, University of Edinburgh, Edinburgh, United Kingdom, ${ }^{19}$ Philosophy, Psychology, and Language Sciences, The University of Edinburgh, Edinburgh, United Kingdom, ${ }^{20}$ Department of Culture, Cognition and Computation, Aarhus University, Aarhus, Denmark, ${ }^{21}$ Department of Molecular Life Sciences, University of Zurich, Zurich, Switzerland, ${ }^{22}$ Bioengineering, University of Pennsylvania,

Philadelphia , United States, ${ }^{23}$ Linguistic Research Infrastructure, University of Zurich, Zurich, Switzerland, ${ }^{24}$ Electronic and Electrical Engineering, University College London, London, United Kingdom, ${ }^{25}$ Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States, ${ }^{26}$ Institute of Neuroscience and Medicine, INM-7: Brain and Behaviour, Research Centre Jülich, Jülich, Germany, ${ }^{27}$ Medical Faculty, Institute of Systems Neuroscience, Heinrich Heine University Düsseldorf, Düsseldorf, Germany, ${ }^{28}$ Graduate School of Systemic Neurosciences, Ludwig-Maximilians-University Munich, Planegg-Martinsried, Germany, ${ }^{29}$ Institute of Neuronal Cell Biology, Technical University of Munich, Munich, Germany, ${ }^{30}$ Faculty of Psychology, University of Basel, Basel, Switzerland, ${ }^{31}$ School of Psychology, University of Nottingham, Nottingham, United Kingdom, ${ }^{32}$ Ear Institute, University College London, London, United Kingdom, ${ }^{33}$ Institute of Cognitive Science, University of Osnabrück, Osnabrück, Germany, ${ }^{34}$ Department of Cognitive, Linguistic, \& Psychological Sciences, Brown University, Providence, United States,

${ }^{35}$ Département de psychologie, Centre de recherche de l'Institut universitaire de gériatrie de Montréal, Montreal, Canada, ${ }^{36}$ Instituto Cajal, CSIC, Madrid, Spain, ${ }^{37}$ School of Psychology \& Neuroscience, University of St Andrews, St Andrews, United Kingdom, ${ }^{38}$ Neurology, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany, ${ }^{39}$ Department of Psychology, Georgia Institute of Technology, Atlanta, United States, ${ }^{40}$ Département des Neurosciences Cliniques, Lausanne University Hospital, Lausanne, Switzerland, ${ }^{41}$ School of Psychological Science, The University of Newcastle, Newcastle, Australia, ${ }^{42}$ Institute of Physiology, University Medical Center of the Johannes Gutenberg University, Mainz, Germany, ${ }^{43}$ Institute for Quantitative and Computational Biosciences (IQCB), Johannes Gutenberg University, Mainz, Germany, ${ }^{44}$ Data Science and Sharing Team, Functional Magnetic Resonance Imaging Facility, National Institute of Mental Health, Bethesda, United States, ${ }^{45}$ Machine Learning Team, Functional Magnetic Resonance Imaging Facility, National Institute of Mental Health, Bethesda, United States, ${ }^{46}$ Zurich Center for Neuroeconomics, Department of Economics, University of Zurich, Zurich, Switzerland, $\dagger$ Major contributions, the names of the remaining authors are listed in random order, see contributions breakdown in Table S.4.

*Corresponding author: xiao.luo.17@ucl.ac.uk

Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.

## Introduction

Keeping up with the exponentially increasing (1) scientific literature is a superhuman challenge. Potentially disruptive findings go unnoticed in the deluge of articles (2). Processing and integrating the myriad of relevant findings may already surpass humans' abilities. One path forward is a partnership between human scientists and machines. This partnership could take several forms, including specialist solutions that address specific challenges, such as in pro-

tein folding (3), drug discovery (4), and materials science (5). Alternatively, general models of the scientific literature could help guide human scientists' predictions and study designs. We consider this possibility.

It is an open question whether large language models (LLMs), trained on general text and scientific articles, can predict the outcomes of experiments. If LLMs' predictions surpassed
human experts, the practice of science and the pace of discovery would radically change. We consider this question for neuroscience, which is a large and interdisciplinary field. Prediction in neuroscience should be challenging for human experts for several reasons: (i) there are often many thousands of relevant scientific articles, (ii) an individual study can be noisy, unreliable, and may not replicate, (iii) neuroscience is a multi-level endeavor (0), spanning behavior and molecular mechanisms, (iv) and the analysis methods are diverse and can be complex (7), (v) as are the methods used, which include different brain imaging techniques, lesion studies, gene modification, pharmacological interventions, and so forth.

Can LLMs meet these challenges? In other domains, LLMs have performed impressively. Upon its release, OpenAI's ChatGPT (8) captured the public's imagination with its abilities. Most LLMs are based on the transformer architecture (9). These models contain billions and sometimes trillions of weights (10), which are tuned during training in a self-supervised manner to predict the next token, such as the next word in a text passage.

LLMs have displayed remarkable capabilities, including passing professional exams, reasoning (though not without limitations), translation, solving mathematics problems, and even writing computer code $(11,12)$. By constructing a statistical model during their training to predict the next token, whether that token is a word, pixel, or protein sequence (13), LLMs uncover the underlying patterns or structure of a domain. This generative model captures patterns in the training data, including subtle and imperfect ones. How LLMs learn and generalize to novel situations can be likened to how expert scientists detect patterns in their field after years of reading papers, attending conferences, and analyzing data. From these experiences, human experts build intuitions that enable them to predict future outcomes based on proposed study designs. However, unlike human scientists, LLMs are virtually unconstrained in how much of the scientific literature they can process during training.

How can we formally evaluate the predictive abilities of LLMs in neuroscience? With the
rise of LLMs, there has been a surge in evaluation benchmarks, many of which focus on assessing LLMs' capabilities in scientific domains. Most benchmarks evaluate core knowledge retrieval and reasoning abilities, which are typically backward-looking (Fig. 1). Backwardlooking benchmarks include MMLU (14), PubMedQA (15), and MedMCQA (16). These benchmarks are structured in a question-and-answer format, where models must demonstrate extensive world knowledge, retrieve relevant information based on the context of the question, and answer correctly. However, none of these benchmarks are suitable for evaluating the ability of models to predict novel outcomes, which is inherently forward-looking (Fig. 1).

To address this need, we developed BrainBench to test LLMs' ability to predict neuroscience findings (Fig. 2). LLMs have been trained extensively on the scientific literature, including neuroscience. BrainBench evaluates whether LLMs have seized on the fundamental patterning of methods and results that underlie the structure of neuroscience. Can LLMs outperform human experts on this forward-looking benchmark? In particular, BrainBench evaluates how well the test-taker can predict neuroscience results from methods by presenting two versions of an abstract from a recent journal article. The test-taker's task is to predict the study's outcome, choosing between the original and an altered version. The altered abstract significantly changes the study's outcome (i.e., results) while maintaining overall coherence.

To appreciate how BrainBench qualitatively differs from existing benchmarks, consider a perceived limitation of LLMs, namely their tendency to generate erroneous information, a phenomenon commonly referred to as "hallucination" by LLM researchers. Unlike knowledge graphs that store verified facts, LLMs may not be trustworthy for backward-looking tasks such as summarizing research papers or providing accurate citations (17). However, for forwardlooking tasks, such as predicting results from a novel experiment, we view this tendency to mix and integrate information from large and noisy datasets as a virtue. What is a hallucination in a backward-looking task is a generalization or prediction in a forward-looking task (e.g., Brain-

![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-06.jpg?height=894&width=1092&top_left_y=697&top_left_x=511)

A (C)

![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-06.jpg?height=300&width=539&top_left_y=1151&top_left_x=522)
Backward-looking
B

Prediction

Who is more likely to win the next round?

A. Player A

B. Player B

Forward-looking

Figure 1: Backward-looking and Forward-looking evaluations. (A) Backward-looking benchmarks involve recalling factual information. For example, in the left panel, a student retrieves a fact about the Gettysburg Address that they learned during a history class. Existing benchmarks in scientific domains are in essence backward-looking as they emphasize retrieving accepted facts for question answering and reasoning tasks. (B) Forward-looking benchmarks involve predicting novel outcomes based on past data. Two forms of uncertainty, aleatoric (due to intrinsic randomness) and epistemic (due to lack of knowledge), may be present. For example, in the right panel, a table tennis fan predicts which player will win the next set based on their knowledge of the players, how they have played so far today, and so forth. Inherent random factors, such as a breeze affecting the ball's flight, will also be present.

![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-07.jpg?height=509&width=1575&top_left_y=862&top_left_x=275)

Figure 2: BrainBench is a forward-looking benchmark for neuroscience. BrainBench evaluates test-takers' ability to predict neuroscience results. BrainBench's test cases were sourced from recent Journal of Neuroscience abstracts across five neuroscience domains: Behavioral/Cognitive, Systems/Circuits, Neurobiology of Disease, Cellular/Molecular, and Developmental/Plasticity/Repair. Test-takers chose between the original abstract and one altered to significantly change the result while maintaining coherency. Human experts and Language Models (LLMs) were tasked with selecting the correct (i.e., original) version from the two options. Human experts made choices, and provided confidence and expertise ratings in an online study. LLMs were scored as choosing the abstract with the lower perplexity (i.e., the text passage that was less surprising to the model) and their confidence was proportional to the difference in perplexity between the two options.

Bench). BrainBench provides a way to quantify this forward-looking ability and compare to human experts. To foreshadow our results, LLMs surpassed human experts on BrainBench by a substantial margin and this margin increased when we provided additional training in neuroscience to an LLM, which we refer to as "BrainGPT".

## Results

## General-purpose LLMs best neuroscientists on BrainBench

On each benchmark trial (see Fig. 2), both the LLMs and human experts were tasked with selecting which of two versions of an abstract was correct (i.e., the original version). Human neuroscience experts were screened for their expertise and engagement (see Methods) with 171 out of 202 participants passing all checks and included in our analyses.

Every LLM outperformed human experts on BrainBench with LLMs averaging $81.4 \%$ accuracy and human experts averaging $63.4 \%$ (Fig. 3A). When restricting human responses to those in the top $20 \%$ of self-reported expertise for that test item, accuracy rose to $66.2 \%$, still below the level of LLMs.

Smaller models such as Llama2-7B and Mistral-7B with 7 billion parameters, performed comparably to larger models (Fig. 3A). Chat or instruction-optimized models performed worse than their base model counterparts $(t(5)=5.4, p<.01)$. We suspect that aligning LLMs to engage in natural language conversations hinders their scientific inference abilities (see Discussion).

The previous analyses involved benchmark items created by co-authors who are neuroscience experts (see Methods). We conducted the same analyses using test cases generated by a LLM, namely GPT-4 (see Methods), and observed similar results (see Supplementary materials).
![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-09.jpg?height=708&width=1576&top_left_y=817&top_left_x=274)

Figure 3: Performance of human experts and large language models on BrainBench. (A) LLMs outperformed human experts on BrainBench. Smaller models are on par with larger models. Base versions of models outperformed chat and instruct versions, which were tuned to be conversational with humans. (B) The distribution of test cases across neuroscience subfields roughly mirrors the distribution of articles in the Journal of Neuroscience with Behavior/Cognitive overrepresented. The average performance of 15 LLMs and human experts is shown. LLMs outperformed human experts in every subfield (see Fig. S.3 for full results). Error bars represent the standard deviation of accuracy around the mean. (C) Majority of the participants were doctoral students, postdoctoral researchers and faculty/academic staff.

Performance breakdown by subfields and by participant types BrainBench encompasses test cases from five distinct neuroscience domains: Behavioral/Cognitive, Cellular/Molecular, Systems/Circuits, Neurobiology of Disease, and Development/Plasticity/Repair. Some domains, particularly Behavioral/Cognitive, are overrepresented both in BrainBench (Fig. 33) and the Journal of Neuroscience from which we drew our test cases (see Methods).

On average, LLMs performed better than human experts in every subfield (Fig. 3B), as did each individual LLM (Fig. S.3). Most human experts were doctoral students, postdoctoral researchers, or faculty/academic staff (see Fig. 3 C). Please refer to Supplementary materials for more detailed demographic information including years of experience in neuroscience research about the human experts.

Do judgments from LLMs and human experts align? We considered whether human experts and LLMs found the same benchmark items difficult. For humans, we calculated the mean accuracy for each of the 200 test cases. For LLMs, we calculated the signed differences in perplexity between incorrect and correct abstracts for each test case. Perplexity measures how surprising a text passage is to an LLM. Using these measures (Fig. S.4), the mean Spearman correlation between an LLM and human experts was $0.15( \pm 0.03)$ whereas the mean Spearman correlation between LLMs was 0.75 ( $\pm 0.08)$.

LLMs can integrate information across context To better understand the basis for the remarkable performance of LLMs (see Fig. S.2 for results), we investigated whether their performance was achieved by integrating information throughout the abstract (including the method used) or by solely relying on the local context in the results passages that differed between the original and altered abstract (Fig. 2)

We re-evaluated the LLMs on individual sentences containing only the altered results passage (i.e., local context only). LLMs performed much worse when restricted to this local context

(Fig. S.2), which provides strong evidence that LLMs are integrating information across the abstract, including information on background and methods. LLM's superior performance relative to human experts appears to arise from integrating information across the abstract.

LLM performance is not driven by data memorization When LLMs perform well on a benchmark, one general concern is that the benchmark itself was part of the training set, allowing the LLM to memorize the correct answers. To address this concern, we used a commonly applied measure, zlib-perplexity ratio, for evaluating whether LLMs have memorized passages (18). This ratio gauges the difference between a data-agnostic compression rate of text and data-specific perplexity computed by an LLM (see Methods). Passages that are hard to compress but have low perplexity are indicative of memorization.

We found no indication that BrainBench was memorized by LLMs (Fig. S.5). For comparison, we calculated the zlib-perplexity ratio for a passage that we suspected would be memorized by LLMs, namely the Gettysburg Address. The Gettysburg Address should appear multiple times in an LLM's training set and indeed it showed signs of memorization (Fig. S.5). Interestingly, for some LLMs, we know exactly what they were trained on (see Table S.2). For these models, the distribution of zlib-perplexity ratios heavily overlapped for items that we knew were in the training set and for items, including BrainBench, that we knew were not in the training set. We believe this overlap indicates that scientific articles, which are unlikely to repeat in training sets, are stored in LLMs as general patterns (akin to a human schema), which support performance on forward-looking tasks (e.g., BrainBench) that require generalization.

As a final check (Fig. S.6; Materials and Methods), we confirmed that LLMs do not perform better on items published earlier in 2023 (e.g., Jan-2023 vs. Oct-2023), which addresses the concern that early items are more likely to have a preprint or other precursor appear in the training set that affected BrainBench performance. All our checks indicated that BrainBench
items were novel for the LLMs.

## LLMs and human experts are calibrated

To assess whether LLMs' predictions are calibrated, we examined how well their confidence tracked their accuracy, a crucial characteristic for a trustworthy prediction system. We estimated LLMs' confidence using the ranked absolute difference in perplexities between two abstracts (Fig. 2. see Methods) and found, like human experts, all LLMs exhibited a positive correlation between accuracy and confidence. When LLMs are confident in their decisions, they are more likely to be correct (Fig. 4).

## Augmenting LLMs with neuroscience knowledge to create BrainGPT

Pretrained LLMs can provide a foundation for further training in neuroscience with the aim of improving performance, as assessed by BrainBench. We used Low-Rank Adaptation (LoRA) (19) to augment a pretrained LLM, Llama-2-7B-chat, with additional neuroscience knowledge.

LoRA is a parameter-efficient fine-tuning technique that inserts low-rank adapter matrices into LLM transformer blocks (Fig. S.7) and trains only these LoRA weights to update the model's behavior. In our case, we fine-tuned Llama-2-7B chat using over 1.3 billion tokens from neuroscience publications spanning 100 journals between 2002 and 2022 (see Methods), which significantly improved performance.

The difference in perplexity between incorrect and correct options significantly increased under LoRA $(t(199)=-6.3, p<.001)$, which resulted in a $3 \%$ increase in BrainBench performance (Fig. 5A ). LoRA tuning dramatically shifted $(t(199)=33.3, p<.001)$ the perplexity of correct responses (Fig. 5B), which is indicative of the LLM specializing for neuroscience material. LoRA introduced fewer than 18 million new weights, which is $.26 \%$ of the total number of weights in Llama-2-7B chat. These results indicate that BrainGPT models can efficiently be
![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-13.jpg?height=1578&width=1266&top_left_y=422&top_left_x=424)

Figure 4: Accuracy and confidence are calibrated for human experts and large language models (LLMs). When human experts and LLMs are confident in their BrainBench judgments, they are more likely to be correct. Confidence ratings were sorted and placed in equally-sized bins with the mean accuracy for items in that bin plotted. The positive slope of the black regression lines for human experts and all LLMs indicates that confidence is well calibrated (i.e., higher confidence corresponds to higher accuracy). Calibration is beneficial for humanmachine teams.

![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-14.jpg?height=588&width=1239&top_left_y=405&top_left_x=432)

A

![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-14.jpg?height=502&width=580&top_left_y=478&top_left_x=447)

B

![](https://cdn.mathpix.com/cropped/2024_06_04_3b736867a26759df6d75g-14.jpg?height=499&width=629&top_left_y=485&top_left_x=1038)

Figure 5: Fine-tuning a pretrained large language model (LLM) on neuroscience knowledge. Llama-2-7B (chat) was fine-tuned using LoRA on neuroscience articles from 2002-2022 (a total of 1.3 billion tokens). (A) The fine-tuned model improved by $3 \%$ on BrainBench. (B) The fine-tuning process substantially shifted the perplexity distribution of correct responses, indicative of the LLM specializing in neuroscience.

derived by extending existing LLMs.

## Discussion

We considered whether large language models (LLMs) can forecast the outcome of neuroscience experiments. By training on the vast scientific literature, we hoped LLMs could build a generative model that captured the patterns underlying neuroscience. To evaluate this possibility, we constructed a new forward-looking (Fig. 2) benchmark, BrainBench.

BrainBench assesses a test taker's ability to select which of two versions of a neuroscience abstract contains the actual results of the study (see Fig. 2). We found that LLMs outperform human experts on BrainBench by a considerable margin (see Fig. $3 \mathrm{~A}$ ) across all neuroscience subfields we consider (Fig. 3B). Moreover, the LLMs knew when their predictions were likely to be right or wrong (Fig. 4). LLMs' superior performance arose from their ability to integrate information throughout the abstract, such as text pertaining to the method and study design.

When access to such information was removed, LLM performance drastically declined (Fig. S.2).

We found no indication that LLMs had been exposed to and memorized BrainBench items during their training. Instead, our analyses suggested that LLMs discovered the fundamental patterns that underlie neuroscience studies, which enabled LLMs to predict the outcomes of studies that were novel to them. These conclusions were supported by a widely employed technique (18) to determine text membership within an LLMs' training set (see Fig. S.5) . The Galactica (20) LLMs were particularly illuminating because the contents of their training set are publicly known and did not include BrainBench items. Interestingly, there was no indication of memorization in Galactica for scientific articles that were in its training set, consistent with the notion that LLMs learn broad patterns underlying scientific fields. While passages that frequently repeat in the training set, such as the Gettysburg Address, may be memorized (see Fig. S.5), scientific articles that occur infrequently (most likely once) in the training set appear to support LLM's forward-looking predictive abilities.

LLM's impressive forward-looking capabilities suggest a future in which LLMs help scientists make discoveries. To be effective, LLMs need to stay abreast of the rapidly expanding literature. We found that LLMs could efficiently be augmented with neuroscience knowledge using LoRA (19), boosting performance on BrainBench (Fig. 5). LoRA provides a way to create BrainGPT models by re-orienting general-purpose LLMs for use in neuroscience. One can easily imagine a future in which BrainGPT is near continuously updated with new knowledge using LoRA, along with complementary approaches such as Retrieval Augmented Generation (RAG; (17)). RAG could be used to query a database of relevant and up to date scientific articles for the task at hand.

In addition to keeping LLMs up to date, benchmarks should routinely be refreshed and expanded to address current needs. One challenge is that creating forward-looking benchmarks,
such as BrainBench, is labor intensive and requires human expertise. To address this potential bottleneck, we created and evaluated 100 test cases using GPT-4 through a largely automated process (see Methods). Although there is room for improvement, these items were close in quality to the human-created ones with 8 of the 100 items being word-for-word matches with the human-created versions. These efforts should pave the way for the rapid creation of other forward-looking benchmarks in neuroscience, as well as benchmarks for other knowledge intensive fields. We believe high-quality forward-looking benchmarks will be critical to developing LLMs as tools for scientific discovery.

For LLMs to be trustworthy and effective teammates, they need to convey the certainty of their predictions to human scientists. Fortunately, we found that LLMs' confidence is well calibrated. When LLMs were confident in their predictions, they were more likely to be correct (Fig. 4). A second ingredient for effective teams is being diverse or complementary. LLMs have potential here as well as the items they found difficult did not highly correlate with those human experts found difficult (Fig. S.4). These two ingredients, being well calibrated and complementary, allow systems that combine human and machine judgments to outperform either alone $(21)$.

All our results, including those for calibrated confidence, were only possible because we had access to LLM weights to calculate the perplexity of passages (see Fig. 2). Our approach diverged from the popular approach of prompting models for responses through natural language (i.e., chat). Prompting in natural language may yield less reliable judgments and degrade model competency compared to using model probability scores or training separate classifiers directly from internal representations (22-25). These observations underscore the importance of working with models that are as open as possible, ideally making both the weights and training set publicly available. Accordingly, we make BrainGPT available on the Huggingface platform https://huggingface.co/BrainGPT.

Beyond serving as a tool for neuroscientists, BrainGPT can help reveal the structure of the field. In particular, we can vary BrainGPT's training set and observe the effect on BrainBench. For example, what is the effect of including training data from related fields like psychology? In terms of supporting prediction, we can quantify how interrelated fields are. Does it help to weight articles in the training set by their recency, citations, or impact factor? In addition to these training manipulations, we can vary how testing is conducted. For example, would step-by-step thinking via chain-of-thought reasoning (26) benefit BrainGPT? If prediction in neuroscience is akin to a deductive reasoning process, then it should. If instead, as we suspect, prediction in neuroscience is a function of many noisy intertwined signals across subfields, then chain-of-thought reasoning will not help. BrainGPT and BrainBench can help answer these meta-science questions.

We foresee a future in which LLMs serve as forward-looking generative models of the scientific literature. LLMs can be part of larger systems that help researchers determine the best experiment to run next. LLMs' predictions are informed by a vast scientific literature that no human could read in their lifetime. As LLMs improve, so should their ability to provide accurate predictions. In this contribution, we focused on neuroscience but our aims are broader - we hope to provide a template for any knowledge intensive field. None of the approaches we adopted are neuroscience-specific. We hope to democratize the use of LLMs in science and increase reproducibility by highlighting the use of relatively small models that can be run locally and whose weights are accessible, which contrasts with commercial products. Finally, while LLMs appear poised to supplant humans at prediction, we foresee a role for human experts in providing the accompanying scientific explanations. Prediction is very important, but not everything.

## References

1. L. Bornmann, R. Mutz, Journal of the Association for Information Science and Technology 66, 2215 (2015).
2. J. S. G. Chu, J. A. Evans, Proceedings of the National Academy of Sciences 118, e2021636118 (2021).
3. K. Tunyasuvunakool, et al., Nature 596, 590 (2021).
4. A. Zhavoronkov, et al., Nature Biotechnology 37, 1038 (2019).
5. V. Tshitoyan, et al., Nature 571, 95 (2019).
6. R. M. Mok, B. C. Love, Science Advances 9, eade6903 (2023).
7. R. Botvinik-Nezer, et al., Nature 582, 84 (2020). Publisher: Nature Publishing Group.
8. Y. Liu, et al. (2023). Publisher: arXiv Version Number: 3.
9. A. Vaswani, et al. (2017). Publisher: arXiv Version Number: 5.
10. W. Fedus, B. Zoph, N. Shazeer (2021). Publisher: arXiv Version Number: 3.
11. A. Srivastava, et al. (2022). Publisher: arXiv Version Number: 3.
12. S. Gunasekar, et al. (2023). Publisher: arXiv Version Number: 1.
13. R. Strack, Nature Methods 20, 1868 (2023).
14. D. Hendrycks, et al., Measuring Massive Multitask Language Understanding (2021). ArXiv:2009.03300 [cs].
15. Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, X. Lu, PubMedQA: A Dataset for Biomedical Research Question Answering (2019). ArXiv:1909.06146 [cs, q-bio].
16. A. Pal, L. K. Umapathi, M. Sankarasubbu, MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering (2022). ArXiv:2203.14371 $[\mathrm{cs}]$.
17. P. Lewis, et al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2021). ArXiv:2005.11401 [cs].
18. N. Carlini, et al., Extracting Training Data from Large Language Models (2021). ArXiv:2012.07805 [cs].
19. E. J. Hu, et al., LoRA: Low-Rank Adaptation of Large Language Models (2021). ArXiv:2106.09685 [cs].
20. R. Taylor, et al., Galactica: A Large Language Model for Science (2022). ArXiv:2211.09085 [cs, stat].
21. M. Steyvers, H. Tejeda, G. Kerrigan, P. Smyth, Proceedings of the National Academy of Sciences 119, e2111547119 (2022).
22. L. Zheng, et al., Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (2023). ArXiv:2306.05685 [cs].
23. J. Hu, R. Levy, Prompting is not a substitute for probability measurements in large language models (2023). ArXiv:2305.13264 [cs].
24. A. Azaria, T. Mitchell, The Internal State of an LLM Knows When It's Lying (2023). ArXiv:2304.13734 [cs].
25. L. Gao, et al., A framework for few-shot language model evaluation (2023). Version Number: v0.4.0.
26. J. Wei, et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2023). ArXiv:2201.11903 [cs].
27. M. Nasr, et al., Scalable Extraction of Training Data from (Production) Language Models (2023). ArXiv:2311.17035 [cs].
28. Gailly, Adler, zlib compression library. (2024).
29. E. Almazrouei, et al., The Falcon Series of Open Language Models (2023). ArXiv:2311.16867 [cs].
30. H. Touvron, et al., LLaMA: Open and Efficient Foundation Language Models (2023). ArXiv:2302.13971 [cs].
31. A. L. Anwyl-Irvine, J. Massonnié, A. Flitton, N. Kirkham, J. K. Evershed, Behavior Research Methods 52, 388 (2020).
32. I. Loshchilov, F. Hutter, Decoupled Weight Decay Regularization (2019). ArXiv:1711.05101 [cs, math].
