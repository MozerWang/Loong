# UnKE: Unstructured Knowledge Editing in Large Language Models 

Jingcheng Deng ${ }^{1,2 *} \quad$ Zihao Wei ${ }^{1,2} *$ Liang Pang ${ }^{1 \dagger}$<br>Hanxing Ding ${ }^{1,2} \quad$ Huawei Shen ${ }^{1,2} \quad$ Xueqi Cheng ${ }^{1,2}$<br>${ }^{1}$ Institute of Computing Technology, Chinese Academy of Sciences<br>${ }^{2}$ University of Chinese Academy of Sciences<br>\{dengjingcheng23s, weizihao22z, pangliang\}@ict.ac.cn<br>\{dinghanxing18s, shenhuawei, cxq\}@ict.ac.cn


#### Abstract

Recent knowledge editing methods have primarily focused on modifying structured knowledge in large language models, heavily relying on the assumption that structured knowledge is stored as key-value pairs locally in MLP layers or specific neurons. However, this task setting overlooks the fact that a significant portion of real-world knowledge is stored in an unstructured format, characterized by long-form content, noise, and a complex yet comprehensive nature. The "knowledge locating" and "term-driven optimization" techniques conducted from the assumption used in previous methods (e.g., MEMIT) are ill-suited for unstructured knowledge. To address these challenges, we propose a novel unstructured knowledge editing method, namely UnKE, which extends previous assumptions in the layer dimension and token dimension. Firstly, in the layer dimension, we discard the "knowledge locating" step and treat first few layers as the key, which expand knowledge storage through layers to break the "knowledge stored locally" assumption. Next, we replace "term-driven optimization" with "cause-driven optimization" across all inputted tokens in the token dimension, directly optimizing the last layer of the key generator to perform editing to generate the required key vectors. By utilizing key-value pairs at the layer level, UnKE effectively represents and edits complex and comprehensive unstructured knowledge, leveraging the potential of both the MLP and attention layers. Results on newly proposed unstructure knowledge editing dataset (UnKEBench) and traditional structured datasets demonstrate that UnKE achieves remarkable performance, surpassing strong baselines. Our codes is released in the link ${ }^{3}$.


## 1 Introduction

Ensuring the accuracy and currency $\mathrm{Xu}$ et al. [2023] of the knowledge stored within the intrinsic parameters of large language models (LLMs) has become increasingly critical with their widespread deployment. Knowledge editing [Yao et al. 2023, Zhang et al., 2024a, Cheng et al., 2023, Mao et al., 2023] emerges as a promising approach to address this challenge, facilitating timely updates to the knowledge embedded in LLMs.

Representing knowledge in the form of triples [Meng et al. 2023] or fact chains [Wei et al., 2024a] is a common practice, followed by the process of editing to tailor response of the model model[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_d73be6fff60ee438057fg-02.jpg?height=368&width=1392&top_left_y=253&top_left_x=366)

(a) Previous work (Local \& Term Key-Value)

Figure 1: Comparison between UnKE and previous knowledge editing methods. Previous research assumed knowledge is stored as key-value pairs in local MLP layers, editing based on specific term positions like the subject. In contrast, UnKE views knowledge as stored in the global parameters of Transformer layers, considering the positions of all input tokens during editing. UnKE's editing process involves adjusting parameters in both MLP and attention layers, showing superior unstructured knowledge editing capabilities compared to prior methods.

to a specific entity. Approximately $80 \%$ of real-world knowledge is contained in unstructured formats [Bavota. 2016]. This unstructured data is characterized by lengthy content, high noise levels, and complex, comprehensive information, which limits the effectiveness of traditional knowledge editing methods. Furthermore, when using LLMs, users typically seek comprehensive text output rather than simple entity-level representations. This user preference suggests that knowledge editing methods based on knowledge triples may not adequately meet their needs. Consequently, we propose a more challenging and versatile task of editing unstructured textual knowledge, which holds broader practical utility.

Unstructured knowledge editing presents several challenges to previous knowledge editing methods, primarily in the following aspects: (1) previous knowledge editing methods often assume that knowledge exists in MLP layers or knowledge neurons in the form of knowledge triplets Zhong et al., 2023, Geva et al., 2021, Huang et al., 2023|. They modify knowledge triplets by editing or adding additional parameters or models[Mitchell et al., 2022a]. However, the experiments conducted in Section 3.1 confirm that structured knowledge triples exhibit non-local characteristics. Furthermore, unstructured knowledge, which possesses a significantly higher knowledge density than structured knowledge, is even less prone to being localized. Consequently, conventional methods for editing previous knowledge are inadequate when it comes to handling tasks related to editing unstructured knowledge. Meng et al., 2023, 2022] (2) Some knowledge editing methods require locating terms during the editing process, such as the last token of the subject. Otherwise, their effectiveness will be significantly reduced, as demonstrated experimentally in Section 3.2 However, locating these terms within unstructured text poses a significant challenge, as illustrated by the case discussed in Table 10 .

To bridge this gap, in this paper, we introduce an Unstructured Knowledge Editing (UnKE) method that leverages causally driven optimization to edit layer-level key-value pairs. As shown in Figure 1 . specifically, we argue that unstructured knowledge is not strictly limited to particular (local) MLP layers or knowledge neurons, but is distributed collaboratively across multiple layers (non-local). To this end, we expand previous hypotheses in two dimensions. Firstly, in the layer dimension, we expand the scope of key-value pairs from MLP layers to Transformer layers Meng et al. 2023, 2022], and perform edits to Transformer layers, instead of simple MLP neurons. Secondly, in the token dimension, we broaden our focus from individual terms to encompass the entire input sentence. Then, we construct a two-stage optimization process: (1) obtain the key vector that are necessary for editing unstructured knowledge, and (2) employ an optimization method with causal effects to generate this key vector.

To address the lack of a benchmark for editing unstructured knowledge, we develop UnKEBench. UnKEBench is more challenging than existing structured editing benchmarks due to its complexity, which arises from the unstructured nature of target answers and dense knowledge embedded in questions. UnKE significantly outperforms existing baselines across several evaluation metrics within UnKEBench, showcasing its ability to precisely define editing goals for specific questions. Additionally, UnKE demonstrates superior stability in both batch and sequential editing scenarios, as well as surpassing strong baseline models for structured knowledge editing.

The main contributions of this work are as follows:

- We highlight the inherent limitations of existing knowledge editing tasks and introduce a novel unstructured knowledge editing benchmark called UnKEBench.
- We introduce UnKE, which incorporates layer-level knowledge key-value pairs and layer-level knowledge editing parameters, enabling it to effectively represent and edit unstructured knowledge with ease.
- Extensive experiments demonstrate that UnKE surpasses existing methods in various scenarios on UnKEBench, showcasing its adaptability and effectiveness in managing batch and sequential editing. Notably, it also surpasses advanced methods like ROME and MEMIT in structured knowledge editing tasks.


## 2 Related Works

In this section, we introduce recent advancements in knowledge editing, broadly categorized into three groups: methods that preserve the original model parameters, methods that locate and then edit the original model parameters, and methods that directly modify the original model parameters.

Preserving the Original Model Parameters One category focuses on introducing additional parameters, while the other focuses on involving knowledge in in-context learning (ICL). However, both methods face challenges in efficiently editing a large amount of knowledge within a single model, primarily due to limitations in parameter count and context window length. For adding parameters, SEARC [Mitchell et al., 2022a] utilizes a classifier to differentiate between input that requires editing and input that does not. If editing is necessary, the trained counterfactual model is employed for prediction; otherwise, using the original model. T-Patcher [Huang et al. 2023] incorporates and trains specific neurons in the final feedforward network layer for the sample that requires editing, e.g. their functionality activated solely when encountering the edited sample. Additionally, [Hartvigsen et al. 2023] proposed GRACE, a lifelong model editing method that generates a discrete local editing codebook while preserving the model weights unchanged. While training additional parameters may be effective for editing knowledge triples, their success with unstructured knowledge is limited by the number of parameters. For ICL, IKE [Zheng et al. 2023] utilizes ICL for knowledge editing, while MeLLo [Zhong et al. 2023] enhances multi-hop knowledge editing capabilities by decomposing complex multi-hop problems into sub-problems and integrating them with retrieval techniques. Methods based on ICL face limitations in inference cost and context length especially for the verbosity, noise, and interdependencies of unstructured knowledge.

Locate-Then-Edit Another branch of methods adopts a locate-and-edit two-step approach. Initially, they identify the specific parameters associated with the target knowledge and subsequently modify those parameters directly to effectuate the desired knowledge editing. KN [Dai et al. 2022] introduces the concept of knowledge neurons and utilizes them to incorporate specific factual knowledge without the need for fine-tuning. ROME [Meng et al. 2022] introduces a causal tracking method to identify the layer that requires editing. Subsequently, it employs Rank-One Model Editing to modify the weights of the feedforward layer, thereby updating specific factual associations. MEMIT [Meng et al. 2023 is an enhanced version of ROME, capable of editing knowledge in batches. These methods operate under the assumption that knowledge is stored locally within MLP layers or neurons, which prove inadequate when confronted with unstructured knowledge.

Directly Modify the Original Model Parameters Additionally, there exist numerous other methods that enable knowledge editing by directly modifying model parameters without the need for explicit positioning. MEND [Mitchell et al., 2022b] introduces auxiliary networks and enables scalable editing by decomposing gradients, thereby facilitating efficient and effective knowledge editing. To enhance the stability and effectiveness of knowledge editing in large language models, StableKE [Wei et al. 2024b] employs additional knowledge for fine-tuning, presenting an approach that brings about significant improvements. As knowledge transitions from a structured to an unstructured format, the process of editing them becomes time-consuming, leading to a degradation in performance.

## 3 Motivation

In this section, we demonstrate through preliminary experiments that the aforementioned assumptions of existing knowledge editing methods are not entirely accurate. These findings, in turn, shed light on the motivations for our proposed approach.

Previous research typically considers knowledge to be stored in specific local parameters of LLMs, and modifying these parameters to achieve knowledge editing.

### 3.1 LLMs Store Knowledge Non-locally

Previous research typically considers knowledge to be stored in specific local parameters of LLMs, and modifying these parameters to achieve knowledge editing. However, we propose that knowledge in LLMs is distributed nonlocally and can be activated through specific reasoning modes. As shown in the Figure 2 . we use a typical knowledge editing method, MEMIT, to conduct experiments on the Counterfact dataset [Meng et al., 2022]. The results indicate that the editing success rate does not change significantly with the target number of edited layers. This finding demonstrates that

![](https://cdn.mathpix.com/cropped/2024_06_04_d73be6fff60ee438057fg-04.jpg?height=242&width=699&top_left_y=711&top_left_x=1057)

Figure 2: Performance evaluation of MEMIT on the Counterfact dataset. The x-axis indicates the starting layer number for editing, and the number of editing layers is 5 . The reliability metric represents the editing success rate. knowledge is not confined to a specific layer but rather is stored non-locally. Therefore, the long, noisy, and complex characteristics of unstructured text pose a challenge for traditional methods that edit specific layers.

### 3.2 Term-driven Optimization Lacks Robustness

Some editing methods for knowledge triples use term-driven optimization strategies. For example, MEMIT and ROME both increase the editing success rate by locating the last token in the subject. As shown in the Table 1, omitting this step causes their performance to drop significantly on KEBench [Wei et al., 2024b]. While the subject can be easily located for knowledge triples, it is difficult to accurately determine the best term in unstructured knowledge. Therefore, we believe that this step should be omitted in unstructured knowledge editing, and that editing should be performed directly at the sentence level through the causal effect of autoregressive LLMs.

## 4 UnKE Method

Building on the expanded assumptions, our research addresses two main questions: (1) What do the key-value pairs at the layer level signify? and (2) How can we modify them effectively to achieve the desired editing outcome?

To address the first question, we start by identifying the editing target. We propose that the value vector can be directly decoded into the desired editing target once it passes through the head layer of LLMs. The key vector should be used as the corresponding keys to activate this value vector. Depending on the number of layers, we conceptualize the LLM as comprising two distinct components: a key generator and a value generator, which generate key vectors and value vectors respectively.

For the second question, we establish a two-stage optimization process. First, we obtain the key vector for the target that requires editing (with the value vector computed during the intermediate process). Second, we optimize the key generator to produce this key vector.
Table 1: Performance comparison on KEBench: Impact of locating the subject. Ori-Acc and Para-Acc represent the accuracy for the original question and the paraphrased question, respectively. None Subject indicates the last token to locate the question.

| Method | Subject |  |  | None Subject |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Ori-Acc | Para-Acc |  | Ori-Acc | Para-Acc |
| ROME | 77.90 | 68.40 |  | 44.10 | 23.60 |
| MEMIT | 74.80 | 64.30 |  | 37.60 | 27.10 |

### 4.1 Calculating Key Vectors

This section explains the procedure for obtaining the key vector that corresponds to the editing target.

Let $f_{\theta}=f_{\theta_{1}}^{1} \circ f_{\theta_{2}}^{2} \circ \cdots \circ f_{\theta_{l}}^{l} \circ \cdots \circ f_{\theta_{N}}^{N}$ denote an autoregressive LLM with parameters $\theta$, which can be regarded as an $N$-layer Transformer decoder. o stands for cascade symbol. For the $l$-th layer, we denote it as $f_{\theta_{l}}^{l}$, where $\theta_{l}$ represents the parameters of this layer. We take the $L$-th layer as the boundary, and the key generator is represented as $f_{\theta_{k}}^{l \leq L}=f_{\theta_{1}}^{1} \circ \cdots \circ f_{\theta_{L}}^{L}$, and the value generator $f_{\theta_{v}}^{L<l \leq N}=f_{\theta_{L+1}}^{L+1} \circ \cdots \circ f_{\theta_{N}}^{N} . \theta_{k}$ and $\theta_{v}$ are parameters of the key generator and the value generator respectively.

For a given question $q=\left[q_{1}, q_{2}, \ldots, q_{n}\right]$, where $q_{i}$ represents the $i$-th token of the question, and $n$ represents the number of question tokens, $f_{\theta}$ generates text by iteratively sampling from a conditional token distribution $\mathbb{P}\left(o_{t} \mid q_{1}, q_{2}, \ldots, q_{n}\right)$, where $o_{t}$ represents the $t$-th token of the output. We use $h_{q}^{l}=\left[h_{q, 1}^{l}, h_{q, 2}^{l}, \ldots, h_{q, n}^{l}\right]$ to represent the hidden state of $q$ in the $l$-th layer. Our ultimate goal is to maximize $\mathbb{P}(a \mid q)$ of generating the edit target expressed as unstructured knowledge $a=$ $\left[a_{1}, a_{2}, \ldots, a_{m}\right]$, where $m$ represents the number of target tokens.

We consider the last token representation of the output from the last layer $L$ of the key generator as the original key vector $k=h_{q, n}^{L}$ for the question $q$. The original value vector $v$ is seen as the output of the value generator given $q$. Our goal is to modify them to obtain the editing target, and the process is denoted as $\left(k->k^{*}, v->v^{*}\right)$, where $k^{*}$ and $v^{*}=\left[v_{1}, v_{2}, \ldots, v_{m}\right]$ represent the key vector and value vector we expect to get. The value vector $v^{*}$ can decode the target $a=\operatorname{softmax}\left(W_{h} v^{* T}\right)$ after passing through the head layer, where $W_{h}$ represents the parameter matrix in the head layer. Next, we identify the key vector $k^{*}$ that can activate this value vector $v^{*}$. Inspired by previous work Meng et al. 2023], we find $k^{*}=h_{q, n}^{l}+\delta_{n}$ directly by optimizing the residual vector $\delta_{n}$ using gradient descent,

$$
\begin{equation*}
k^{*}=h_{n}^{l}+\underset{\delta_{n}}{\operatorname{argmin}}-\log \mathbb{P}_{f_{\theta}\left(h_{q, n}^{L} \mapsto h_{q, n}^{L}+\delta_{n}\right)}(a \mid q) . \tag{1}
\end{equation*}
$$

$f_{\theta}\left(h_{q, n}^{\prime} \rightarrow h_{q, n}+\delta_{n}\right)$ means that we replace the hidden state $h_{q, n}^{L}$ (also be expressed as original key vector $k$ ) with $k^{*}$. Optimizing Eq. 1 to a small enough value means that if we can get $k^{*}=$ $f_{\theta_{k}}^{l \leq L}\left(q_{1}, q_{2}, \ldots, q_{n}\right)$, then we can decode the target $a$.

### 4.2 Optimizing the Key Generator

After obtaining the desired expression form of $k^{*}$, our second goal is to optimize the key generator $f_{\theta_{k}}^{l \leq L}$ to produce the key vector $k^{*}$ on the $n$-th hidden state of layer $L$.

$f_{\theta_{k}}^{l \leq L}$ store a large number of key vectors $K_{0}=\left[k_{1}\left|k_{2}\right| \ldots \mid k_{E}\right]$ during the pre-training process, which can be activated by specific inputs $D_{0}=\left[d_{1}\left|d_{2}\right| \ldots \mid d_{E}\right]$ to generate corresponding value vectors $V_{0}$,

$$
\begin{equation*}
f_{\theta_{k}}^{l \leq L} \triangleq \underset{\hat{\theta}}{\operatorname{argmin}} \sum_{i=1}^{E}\left\|f_{\hat{\theta}}^{l \leq L}\left(d_{i}\right)-k_{i}\right\|^{2} \tag{2}
\end{equation*}
$$

where $E$ represents the number of knowledge key-value pairs introduced during pre-training, which can be regarded as $+\infty$. Therefore during the optimization process we should minimize the parameter changes of $f_{\theta_{k}}^{l \leq L}$ and produce a new key generator $f_{\theta_{k}^{\prime}}^{l \leq L}$ that can generate the new key $k^{*}$,

$$
\begin{equation*}
f_{\theta_{k}^{\prime}}^{l \leq L} \triangleq \underset{\hat{\theta}}{\operatorname{argmin}}\left(\sum_{i=1}^{E}\left\|f_{\hat{\theta}}^{l \leq L}\left(d_{i}\right)-k_{i}\right\|^{2}+\left\|f_{\hat{\theta}}^{l \leq L}(q)-k^{*}\right\|^{2}\right) \tag{3}
\end{equation*}
$$

where $\theta_{k}^{\prime}$ represents the updated parameters. This approach minimizes the impact of adding new keyvalue pairs on the original key-value pairs. In particular, we are able to edit a batch of $u$ unstructured knowledge at one time, which we denote by $K_{1}=\left[k_{1}^{*}\left|k_{2}^{*}\right| \ldots \mid k_{u}^{*}\right]$. Eq 3 can be changed to,

$$
\begin{equation*}
f_{\theta_{k}^{\prime}}^{l \leq L} \triangleq \underset{\hat{\theta}}{\operatorname{argmin}}\left(\sum_{i=1}^{E}\left\|f_{\hat{\theta}}^{l \leq L}\left(d_{i}\right)-k_{i}\right\|^{2}+\sum_{j=1}^{u}\left\|f_{\hat{\theta}}^{l \leq L}\left(q_{j}\right)-k_{j}^{*}\right\|^{2}\right) \tag{4}
\end{equation*}
$$

To avoid the addition of new keys affecting the generation of original keys, we only optimize the last layer of the key encoder $f_{\theta_{L}}^{L}$. In order to optimize Eq 4 , we randomly select a number $C$ of instruction fine-tuning samples to simulate the knowledge $f_{\theta_{L}}^{L}$ learned during pre-training. Assuming that $i$-th instruction fine-tuning sample can be represented as $t^{i}=\left[t_{1}^{i}, t_{2}^{i}, \ldots, t_{P}^{i}\right]$, where $P$ represents the number of $i$-th instruction fine-tuning sample tokens, which is regarded as encoding the key vector $k_{t}^{i}=\left[k_{t, 1}^{i}, k_{t, 2}^{i}, \ldots, k_{t, P}^{i}\right]$. Before performing optimization, we first calculate the key vector $k_{t, p}^{i}$ corresponding to the $p$-th token in $i$-th instruction fine-tuning sample. Considering the causal properties of autoregressive LLM, for each key we need to consider its context,

$$
\begin{equation*}
k_{t, p}^{i}=f_{\theta_{L}}^{L}\left(\text { causal_att }\left(h_{t, 1}^{i, L-1}, h_{t, 2}^{i, L-1}, \ldots, h_{t, p}^{i, L-1}\right)\right) \tag{5}
\end{equation*}
$$

where causal_att represents the attention mechanism with causal mask, and $h_{t, p}^{i, L-1}$ represents the vector of the $p$-th token of the $i$-th instruction fine-tuning sample in the $l$-th layer. We use $f_{\theta_{L}, \mathrm{ca}}^{L}(\cdot)$ to represent $f_{\theta_{L}}^{L}$ (causal_att $\left.(\cdot)\right)$. By incorporating causal effects, we ensure that the generation of key vectors considers the full context, which is crucial, especially for lengthy and information-rich unstructured texts. This leads us to our ultimate optimization objective, which is to

$$
\begin{align*}
f_{\theta_{L}^{\prime}}^{L}= & \underset{\hat{\theta}}{\operatorname{argmin}}(\underbrace{\sum_{i=1}^{C} \sum_{p=1}^{P}\left\|f_{\hat{\theta}, \mathrm{ca}}^{L}\left(h_{t, \leq p}^{i, L-1}\right)-k_{t, p}^{i}\right\|^{2}}_{\text {Key Preservation Loss }}+\underbrace{\sum_{j=1}^{u} \sum_{i=1}^{n-1}\left\|f_{\hat{\theta}, \mathrm{ca}}^{L}\left(h_{q, \leq i}^{j, L-1}\right)-k_{q, i}^{j}\right\|^{2}}_{\text {Key Paraphrase Loss }}  \tag{6}\\
& +\underbrace{\left.\sum_{j=1}^{u}\left\|f_{\hat{\theta}, \mathrm{ca}}^{L}\left(h_{q, \leq n}^{j, L-1}\right)-k_{q}^{*, j}\right\|^{2}\right)}_{\text {Kev Learnino Loss }}
\end{align*}
$$

where $h_{t, \leq p}^{i, L-1}$ represents tokens less than or equal to $p$ in the $i$-th instruction fine-tuning sample, and $h_{q, \leq i}^{j, L-1}$ represents tokens less than or equal to $i$ in the $j$-th question to be edited. The Key Preservation Loss ensures that the key generator retains the keys stored during pre-training, enabling the preservation of original knowledge. Key Paraphrase Loss learns the semantics of the question and ensures that the correct key vector $k_{q}^{*, j}$ can be generated even under paraphrase questions. Additionally, the Key Learning Loss facilitates the key generator in acquiring new keys, activating the corresponding patterns in the value generator, and achieving the desired editing target. The optimization of the key generator is accomplished through gradient descent, optimizing $\mathrm{Eq} 6$.

## 5 Experiments

We first introduce the experimental setup ( $\$ 5.1$ ) and baseline approaches for comparison $(\$ 5.2)$. Then, we evaluate model performance on both unstructured knowledge editing ( $\$ 5.3$ ) and structured knowledge editing ( $\$ 5.5$ ).

### 5.1 Experimental Setup

### 5.1.1 Dataset

To the best of our knowledge, there is no dedicated benchmark for knowledge editing in unstructured texts. Due to space limitations, we detail the existing knowledge editing benchmarks in Appendix A To establish a benchmark for unstructured knowledge editing, we develop UnKEBench.

The unstructured texts are notably lengthy and contain knowledge that extends beyond simple knowledge triples or linear fact chains. To effectively manage this complexity, we have structured our approach into three distinct phases.

1. We employ meticulously crafted instructions to guide ChatGPT in formulating the most appropriate question $Q$ for each text $A$, thus creating an unstructured knowledge pair $(Q, A)$.
2. To refine our evaluation mechanism, we use detailed instructions to prompt ChatGPT to generate a paraphrased version of each original question, denoted as $Q_{p}$, for every original question $Q$.

Table 2: Unstructured knowledge editing performance with different methods. During the editing process, we set the batch size to 1 . With each editing instance, the parameters of the modified model are rebuilt. The decoding process employs a temperature of 0.001 . To ensure fair comparison, the 7-th layer of parameters of the model is specifically targeted for editing across FT-L, ROME, and UnKE. The figures to the left and right of the ' $/$ ' symbol denote the evaluation outcomes for output of the model in response to the original and paraphrased questions, respectively.

| Method | Word-level Overlap |  |  |  | Semantic Similarity <br> Bert-Score | Factual Correctnes: <br> FactScore |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | BLEU | Rouge-1 | Rouge-2 | Rouge-L |  |  |
| Based on LLaMA-7B-Chat |  |  |  |  |  |  |
| FT-A | $1.01 / 1.02$ | $0.92 / 0.92$ | $0.01 / 0.01$ | $0.92 / 0.92$ | $2.56 / 2.58$ | 8.74 |
| FT-L | $6.14 / 5.52$ | 7.55 / 6.78 | $1.37 / 1.28$ | 7.26 / 6.53 | $11.63 / 10.16$ | 15.69 |
| ROME | $47.31 / 41.64$ | $28.89 / 20.93$ | $45.05 / 39.06$ | $38.71 / 33.42$ | $76.52 / 74.29$ | 24.44 |
| MEMIT | $35.79 / 33.19$ | $43.55 / 41.39$ | $23.11 / 19.89$ | 40.96 / 38.81 | $75.90 / 74.46$ | 26.39 |
| MEND | $24.10 / 29.23$ | 45.36 / 45.06 | $31.75 / 29.33$ | $44.05 / 43.77$ | 69.99 / 64.71 | 24.17 |
| UnKE | $81.20 / 73.59$ | 83.27 / 75.64 | 76.85 / 66.45 | $82.44 / 74.53$ | $93.29 / 91.71$ | 38.82 |
| Based on Qwen1.5-7B-Chat |  |  |  |  |  |  |
| MEMIT | 48.89 / 48.71 | $49.50 / 48.18$ | $34.59 / 31.50$ | $47.55 / 46.04$ | $74.72 / 76.82$ | 17.81 |
| UnKE | $92.85 / 75.66$ | $91.74 / 72.68$ | 88.19 / 60.59 | $91.40 / 70.44$ | $96.51 / 90.40$ | 40.08 |

3. We leverage knowledge decomposition strategies and engage ChatGPT to produce multiple subquestion and sub-answer pairs $\left(Q_{s}^{i}, A_{s}^{i}\right)$ for each unstructured knowledge text, where $i$ represents the specific sub-question or answer.

This approach significantly enhances our evaluation toolkit and improves the overall effectiveness of the evaluation process. Details and examples of constructing UnKEBench are provided in the Appendix B

### 5.1.2 Evaluation Metrics

Our evaluation framework for unstructured knowledge editing mirrors the complexity of the task by integrating three critical dimensions: word-level overlap, semantic similarity, and factual correctness.

- Word-level overlap metrics, including BLEU Papineni et al. [2002] and various ROUGE scores Lin [2004] (ROUGE-1, ROUGE-2, and ROUGE-L), provide insight into the lexical and n-gram alignment between the model-generated text and the target answers, based on both the original and paraphrased questions. These metrics are fundamental in assessing the surface-level accuracy of the edited content.
- Semantic similarity. As word-level overlap metrics alone are insufficient for capturing the nuanced understanding a model must exhibit. To bridge this gap, we evaluate semantic similarity by leveraging an embedding encoder (specifically, the all-MiniLM-L6-v2 mode ${ }^{4}$ ) to quantify the depth of comprehension of the model of the text, ensuring a balanced evaluation that transcends mere lexical matching.
- Factual correctness. To gauge generalization capabilities, we involve an innovative measure, FactScore Min et al. [2023], which assesses precision of LLMs in handling sub-questions and their corresponding answers. This step is crucial, as it evaluates ability of LLMs to maintain semantic integrity and factual accuracy in its responses, despite the inherent variability in language expression.

In summary, these three aspects form a robust framework for evaluating unstructured knowledge edits, ensuring both the fidelity and the flexibility of the generated content are thoroughly examined.

### 5.2 Base Models and Baseline Methods

We conduct experiments on two autoregressive models, LLaMA-2-7B-Chat 5 |Touvron et al., 2023 and Qwen1.5-7B-Chat ${ }^{6}$ [Bai et al., 2023]. For baselines, we first compare the fine-tuning method[^1]

Table 3: Performance on human evaluation and structured knowledge editing performance on KEBench. Ori-ACC and Para-ACC represent the accuracy for the original question and the paraphrased question, respectively.

| Method | Correctness | Similarity | Coherence |
| :--- | :---: | :---: | :---: |
| FT-A | 1.06 | 1.47 | 1.47 |
| FT-L | 1.17 | 1.00 | 1.31 |
| ROME | 3.39 | 3.59 | 3.64 |
| MEMIT | 3.25 | 3.70 | 3.72 |
| UnKE | $\mathbf{4 . 7 8}$ | $\mathbf{4 . 7 2}$ | $\mathbf{4 . 7 0}$ |

(a) Human Evaluations

| Method | Ori-Acc | Para-Acc |
| :--- | :---: | :---: |
| FT-A | 6.30 | 6.60 |
| FT-L | 14.70 | 12.10 |
| ROME | 77.90 | 68.40 |
| MEMIT | 74.80 | 64.30 |
| UnKE | $\mathbf{8 7 . 6 0}$ | $\mathbf{7 0 . 4 0}$ |

(b) Structured Knowledge Editing.

Table 4: Comparison of different batch sizes. We conducted experiments on UnKE using the LLaMA2-7B-Chat model, with the decoding temperature set to 0.001 .

| Batch Size | Semantic Similarity |  | Lexical Similarity |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Bert-Score |  | BLEU | Rouge-1 | Rouge-2 | Rouge-L |  |
|  | Fact-Score |  |  |  |  |  |  |
| $2^{0}$ | $93.29 / 91.71$ |  | $81.20 / 73.59$ | $83.27 / 75.64$ | $76.85 / 66.45$ | $82.44 / 74.53$ | 38.82 |
| $2^{1}$ | $93.79 / 90.77$ |  | $81.79 / 72.34$ | $83.64 / 74.05$ | $77.17 / 64.22$ | $82.77 / 72.90$ | 36.15 |
| $2^{2}$ | $93.29 / 89.66$ |  | $81.63 / 69.35$ | $83.60 / 70.99$ | $77.28 / 60.32$ | $82.81 / 69.70$ | 34.68 |
| $2^{3}$ | $93.72 / 86.48$ |  | $83.01 / 64.36$ | $84.34 / 65.25$ | $78.28 / 53.07$ | $83.61 / 63.84$ | 36.20 |
| $2^{4}$ | $93.43 / 84.32$ |  | $82.06 / 60.63$ | $83.46 / 62.26$ | $77.11 / 49.18$ | $82.67 / 60.73$ | 35.25 |
| $2^{5}$ | $93.47 / 82.40$ |  | $81.91 / 57.61$ | $83.65 / 58.87$ | $77.32 / 44.91$ | $82.83 / 57.25$ | 34.52 |
| $2^{6}$ | $93.04 / 82.58$ |  | $81.58 / 57.05$ | $82.90 / 58.02$ | $76.38 / 43.58$ | $82.83 / 57.25$ | 34.23 |

FT-L, which targets specific layers, with FT-A, which fine-tunes all layers. Additionally, we assess two robust baseline models, ROME and MEMIT, focusing on their locating and editing capabilities. Lastly, we evaluate the hypernetwork-based model editing method, MEND.

### 5.3 Unstructured Knowledge Editing Experiments on UnKEBench

We conduct a comprehensive evaluation of various baseline methods and our newly proposed UnKE method on the UnKEBench benchmark. The specific results presented in Table 2 Traditional fine-tuning methods, including FT-L and FT-A, have long exhibited significant limitations when tasked with structured knowledge editing. As anticipated, their performance on UnKEBench is underwhelming, with all evaluation metrics falling short of those achieved by dedicated knowledge editing approaches. Methods employing a Locate-Then-Edit paradigm, such as ROME and MEMIT, despite previously demonstrating satisfactory editing success rates on certain structured benchmarks, underperform on the UnKEBench dataset, particularly in terms of lexical and semantic similarity when compared to UnKE. UnKE demonstrates exceptional performance, surpassing other models in lexical and semantic similarity metrics, which confirms its ability to accurately capture and reproduce the intended editing objectives-a feat that other models do not achieve. For more examples of generated cases, please refer to the Appendix $\mathrm{C}$ Regarding the detailed evaluation metric of FactScore, UnKE achieves a score of 38.82 , outperforming other strong baseline models, yet highlighting that there is still room for further improvement.

We also extend our unstructured knowledge editing experiments to utilize Qwen1.5-7b-Chat as base model and compare against MEMIT. The results indicate that our approach outperforms MEMIT across multiple evaluation dimensions significantly. These experiments, conducted on models with varying architectures, demonstrate the robust transferability of our proposed UnKE method. This finding is particularly relevant as it suggests that effectiveness of UnKE is not confined to a specific model structure or capacity, but rather it is capable of generalizing well across different model frameworks, further supporting its applicability in a broad range of knowledge editing scenarios.

### 5.4 Human Evaluation

Considering the complexity and challenges involved in automatically evaluating unstructured knowledge editing, we conduct additional manual evaluation experiments to ensure the reliability of the evaluation metrics and actual scores in UnKEBench. Due to the high cost of human evaluation, we randomly select 36 samples from a pool of 1000 samples generated by each method. We employ three
![](https://cdn.mathpix.com/cropped/2024_06_04_d73be6fff60ee438057fg-09.jpg?height=752&width=1400&top_left_y=236&top_left_x=362)

Figure 3: Performance in sequential editing. We select the first 64 samples in the UnKEBench data set for sequential editing experiments.

annotators who are familiar with knowledge editing tasks for human evaluation and instruct them to evaluate the edited generated text along three dimensions: semantic correctness, similarity, and coherence on a scale of $1-5$, with 1 denoting "very low" and 5 representing "very high". The scores are then averaged to derive the final human evaluation results. The evaluation results, presented in Table 3a, reflect the collective assessments by the hired professionals. The inter-annotator agreement is 0.57 in Fleiss' $\kappa$, which means a moderate agreement.

The experimental results provide strong evidence of the high consistency between the automatic evaluations and human evaluations. UnKE stands out as the leader across all three dimensions. In contrast, the other baseline models frequently exhibit subpar performance in terms of semantic correctness, highlighting their limited ability to effectively edit unstructured knowledge.

### 5.5 Performance in Structured Knowledge Editing

To validate the capability of UnKE in editing knowledge triples, we conduct experiments on KEBench [Wei et al. 2024b], a benchmark that evaluates whether the model accurately produces the desired target answer after editing. The results presented in Table 3bdemonstrate that UnKE surpasses strong baseline models in terms of Ori-Acc and Para-Acc metrics, exhibiting improvements of 9.7 points and 2.0 points, respectively. When comparing the results with UnKEBench, the improvement of UnKE over the strong baseline may not be as pronounced. However, this outcome is anticipated since UnKE primarily targets complex and lengthy unstructured knowledge editing tasks, making it less conspicuous in simpler structured knowledge editing tasks. In general, experimental results have demonstrated that UnKE is not only effective in unstructured knowledge editing but can also be applied to structured knowledge.

### 5.6 Robustness Analysis on Batch Editing and Sequential Editing

To evaluate the robustness of UnKE in unstructured knowledge editing, we assess its batch editing capabilities (as shown in Table 4) and sequential editing performance (as presented in Figure 3) using the UnKEbench dataset. In the batch editing assessment, we observe that as the batch size increases, the model's performance on the original task remains relatively stable, indicating the robustness of UnKE's batch editing capabilities. However, there is a slight reduction in performance on paraphrased questions, which is expected. The simultaneous optimization of a larger number of keys marginally diminishes the model's generalization ability for paraphrased questions. For sequential editing, we find that the performance of all methods declines as the number of edits increases. Nevertheless, UnKE exhibits the highest stability compared to other baseline methods, demonstrating its robustness in sequential editing scenarios. These findings underscore the effectiveness of UnKE in handling both
batch and sequential editing tasks, highlighting its potential as a promising approach for unstructured knowledge editing.

## 6 Conclusions

We address the limitations of existing knowledge editing benchmarks, which primarily focus on structured knowledge triples, by introducing UnKEBench, the first benchmark for unstructured knowledge editing. To successfully edit unstructured knowledge, we propose UnKE, an unstructured knowledge editing method, which incorporates layer-level knowledge key-value pairs and layer-level knowledge editing parameters, enabling it to effectively represent and edit unstructured knowledge with ease. Experimental results on UnKEBench demonstrate the superior performance of UnKE, significantly surpassing powerful baseline models on various evaluation metrics. Robustness analysis experiments confirm that UnKE possesses the ability to perform both batch and sequential editing. Additionally, UnKE also compares favorably with other strong baseline models on structured knowledge editing benchmarks.

## References

Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, and Xueqi Cheng. Ai-generated images introduce invisible relevance bias to text-image retrieval. CoRR, abs/2311.14084, 2023. doi: 10.48550/ARXIV.2311.14084. URL https://doi.org/10.48550/ arXiv. 2311.14084

Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 10222-10240. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.EMNLP-MAIN.632. URLhttps://doi.org/10.18653/v1/2023.emnlp-main. 632 .

Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024a.

Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. Can we edit multimodal large language models? arXiv preprint arXiv:2310.08475, 2023.

Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Editing personality for llms. arXiv preprint arXiv:2310.02168, 2023.

Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview net/pdf?id=MkbcAHIYgyS

Zihao Wei, Jingcheng Deng, Liang Pang, Hanxing Ding, Huawei Shen, and Xueqi Cheng. Mlake: Multilingual knowledge editing benchmark for large language models, 2024a.

Gabriele Bavota. Mining unstructured data in software repositories: Current and future trends. In Leaders of Tomorrow Symposium: Future of Software Engineering, FOSE@SANER 2016, Osaka, Japan, March 14, 2016, pages 1-12. IEEE Computer Society, 2016. doi: 10.1109/SANER.2016.47. URLhttps://doi.org/10.1109/SANER.2016.47.

Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen. Mquake: Assessing knowledge editing in language models via multi-hop questions. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 15686-15702. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023. EMNLP-MAIN.971. URL https://doi.org/10.18653/v1/2023.emnlp-main. 971

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 5484-5495. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021. EMNLP-MAIN.446. URL https://doi.org/10.18653/v1/2021.emnlp-main. 446

Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformerpatcher: One mistake worth one neuron. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=4oYUGeGBPm.

Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memorybased model editing at scale. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 15817-15831. PMLR, 2022a. URL https://proceedings.mlr.press/v162/ mitchell22a.html.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html.

Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with GRACE: lifelong model editing with discrete key-value adaptors. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 95b6e2ff961580e03c0a662a63a71812-Abstract-Conference.html.

Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 4862-4876. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.296. URL https: //doi.org/10.18653/v1/2023.emnlp-main.296

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8493-8502. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.581. URL https://doi.org/10.18653/v1/2022.acl-long.581.

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL https://openreview.net/ forum?id=0DcZxeWfOPt.

Zihao Wei, Liang Pang, Hanxing Ding, Jingcheng Deng, Huawei Shen, and Xueqi Cheng. Stable knowledge editing in large language models. CoRR, abs/2402.13048, 2024b. doi: 10.48550/ ARXIV.2402.13048. URL https://doi.org/10.48550/arXiv.2402.13048

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL, 2002. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040/

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 12076-12100. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.741. URLhttps://doi.org/10.18653/v1/2023.emnlp-main. 741

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/ arXiv. 2307.09288

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Roger Levy and Lucia Specia, editors, Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), Vancouver, Canada, August 3-4, 2017, pages 333-342. Association for Computational Linguistics, 2017. doi: 10.18653/ V1/K17-1034. URLhttps://doi.org/10.18653/v1/K17- 1034.

Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. A comprehensive study of knowledge editing for large language models. CoRR, abs/2401.01286, 2024b. doi: 10.48550/ARXIV.2401.01286. URL https://doi.org/10.48550/ arXiv. 2401.01286

Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, and Jiarong Xu. Cross-lingual knowledge editing in large language models. CoRR, abs/2309.08952, 2023a. doi: 10.48550/ARXIV.2309. 08952. URL https://doi.org/10.48550/arXiv.2309.08952.

Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing. CoRR, abs/2312.13040, 2023b. doi: 10.48550/ARXIV.2312.13040. URL https://doi. org/10.48550/arXiv.2312.13040

Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: A new benchmark for evaluating knowledge editing of llms. CoRR, abs/2308.09954, 2023. doi: 10.48550/ARXIV. 2308.09954. URLhttps://doi.org/10.48550/arXiv.2308.09954

Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts, 2024.

Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge editing framework for large language models. arXiv preprint arXiv:2308.07269, 2023c.
