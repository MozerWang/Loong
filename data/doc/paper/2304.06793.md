# Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline 

Ole Richter ${ }^{13,4, *}$, Yannan Xing ${ }^{2}$, Michele De Marchi ${ }^{1}$, Carsten Nielsen ${ }^{1}$, Merkourios Katsimpris ${ }^{1}$,<br>Roberto Cattaneo ${ }^{1}$, Yudi Ren ${ }^{2}$, Yalun Hu ${ }^{2}$, Qian Liu ${ }^{1}$, Sadique Sheik ${ }^{1}$, Tugba Demirci ${ }^{1,2}$, Ning Qiao $^{1,2}$


#### Abstract

Edge computing solutions that enable the extraction of high-level information from a variety of sensors is in increasingly high demand. This is due to the increasing number of smart devices that require sensory processing for their application on the edge. To tackle this problem, we present a smart vision sensor System on Chip (SoC), featuring an event-based camera and a low-power asynchronous spiking Convolutional Neural Network (sCNN) computing architecture embedded on a single chip. By combining both sensor and processing on a single die, we can lower unit production costs significantly. Moreover, the simple end-to-end nature of the SoC facilitates small stand-alone applications as well as functioning as an edge node in larger systems. The event-driven nature of the vision sensor delivers high-speed signals in a sparse data stream. This is reflected in the processing pipeline, which focuses on optimising highly sparse computation and minimising latency for $9 \mathrm{sCNN}$ layers to $3.36 \mu \mathrm{s}$ for an incoming event. Overall, this results in an extremely low-latency visual processing pipeline deployed on a small form factor with a low energy budget and sensor cost. We present the asynchronous architecture, the individual blocks, and the sCNN processing principle and benchmark against other sCNN capable processors.


Index Terms-Spiking neural network, Spiking convolutional neural network, neuromorphic engineering, CMOS, IC, SoC, Smart Sensor, Event-based Vision, Dynamic Vision Sensor, Edge computing, Near sensory processing, Asynchronous design.

## I. INTRODUCTION

In order to accelerate the development of automated and intelligent systems, an increasing number of data sources will be utilised. Consequently, the transmission of data between sources needs to be reduced by communicating only relevant and useful information. To achieve this goal, new and radical[^0]

developments are required in the fields of extreme edge computing and near-sensor processing. The local extraction of relevant information by moving intelligence to the sensory edge poses difficult challenges in real-time processing with low latency and on the smallest of energy budgets. Ondemand and sparse computation is a promising solution to reduce computational load and energy consumption [1] but is contrasted with the need for always-on sensory information processing. Event-based processing is a paradigm that can break the trade-off between these two requirements.

In the field of image and video processing, Convolutional Neural Networkss (CNNs) have celebrated significant successes $[2,3]$ and edge inference accelerators for CNNs have been also very successful $[4,5]$. This is achieved by making Application Specific Integrated Circuits (ASIC) with more efficient architectures that can skip zero multiplications to sparsify the computational load, and as such are now the industry standard [6]. To further increase sparsity in the field of eventbased computation and sensing, a promising computational method is Spiking Neural Networks (SNN) [7,8]. To exploit highly sparse CNNs even further, event-driven or sCNNs only process on the availability of individual pixel data and sparsify the activity from layer to layer by using threshold-based neural units [9].

Modern video compression extensively exploits the informa-

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-01.jpg?height=537&width=881&top_left_y=1802&top_left_x=1077)

Fig. 1. Photograph of the realised "Speck1" ASIC. The rectangle at the bottom left of the chip is the $128 \times 128$ pixel event sensor for machine vision applications, while the rest of the area is occupied by the processing cores and the NoC.
tion redundancy between adjacent frames in order to compress the data stream [10]. By taking this data reduction approach to the sensor level and enforcing that individual pixel operate independently and communicate only changes in intensity [1113], the amount of data produced by the sensor is massively reduced. Furthermore, this reduction also reduces the latency of the sensor to levels usually only seen in specialised high frame rate camera modules. The utility of Temporal Contrast (TC) encoding cameras has been extensively demonstrated [14], and they are now being adopted by industry [15].

Combining such low latency, high dynamic range and sparse sensor with an event-driven sCNN processor [16], that excels in real-time low latency processing on a single $\mathrm{SoC}$ is a natural technological step. To complement the architectural advantages of always-on sparse sensing and computation, the SoC is built in a fully asynchronous fashion. The asynchronous data flow architecture provides low latency, high throughput processing when requested by sensory input, while immediately shifting to a low power/idle state when the sensory input is absent. Specifically, no complex or slow wake-up procedures must be implemented to reduce power consumption.

Neuromorphic intelligence intends to solve the aforementioned challenges in the following domains: (a) The physical time operation and the processing of always-on sensory signals. The computation speed is matched to natural signals such as bio-signals, visual data, speech, gestures, and a wide range of environmental and industrial signals. (b) The redundant information reduction to compute on signal change, sparse data availability, and statistical and prediction mismatch to considerably enhance power efficiency. (c) Massively parallel computation to keep latency to a necessary minimum.

Small-scale event-driven sCNN processors using different architectural approaches have been proposed in the community [17-23], in contrast to classical large-scale neuromorphic architectures which are able to run sCNN networks at a tremendously high synaptic resource cost [24-29]. We present for the first time a resource-efficient medium-scale sCNN processor combined with a machine vision event-driven sensor to form a truly novel smart vision sensor on a single ASIC.

We first present the asynchronous methodology and principles used in Sec. II., followed by the ASIC architecture in Sec. III including the sCNN processing principle in Sec. III-C and the individual blocks in Sec. III-A to III-E. We conclude with a comparison of different sCNN processors and vision sensor processor combinations in Sec. IV.

## II. Asynchronous Logic DeSign MeThodology

The design flow of the Speck1 ASIC followed a golden model approach. The golden model was verified using fully proven applications and ML tasks. Then extensive individual feature tests were derived. These feature tests were run at every step of the Integrated Circuit (IC) design flow and compared to the Golden model up to silicon validation after fab-out to verify functionality. Later models have automated production testing support. The design methodology follows the dataflow concept [30-32], the processing cores are laid out as individual
![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-02.jpg?height=928&width=876&top_left_y=172&top_left_x=1080)

preset $\_$preset_x sreset_x
![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-02.jpg?height=920&width=856&top_left_y=174&top_left_x=1096)

Fig. 2. The pull-up/pull-down logic of an N-bit buffer (latch) in Speck1. The control section controls the acknowledge signal of the input channel as well as the transparency of the function block. The function block holds the data and clears it automatically on acknowledgement of the output channel. The Buffer decouples the input channel handshake from the output channel handshake by using the data validity. The validity is a C-element tree with OR2 gates on the input to signal when all data channels have data or all data channels are neutral. To save space, the output validity circuit is shared with the next data flow element. Later versions use a design which supports automated testing.

pipeline systems $[33,34]$. Processing cores are compositions of template dataflow primitives [35]: Buffer (latch) as seen in Fig 2, Compose (function block, combined join and/or source), NC-Split (non-conditional fork), Split (conditional demux), Conditional True Pass (conditional sink or latch), Merge (nondeterministic, non-conditional) and Valid-trees (also used as a sink). The templates are built from pull-up/pull-down state holding logic cells $[36,37]$ and are derived from DYNAPSE2 [38] and DYNAP-SE [39]. DYNAP-SE was built with the Asynchronous Circuit Toolkit (ACT) [40] and the template designs follow the Pre-Charge Full Buffer (PCFB) design of $[41,42]$. These templates were used due to their low latency design and prior silicon verification available to us. In addition, some control primitives such as token latches, forks, muxes, and c-elements that do not carry data, were used to implement data flow control. The Speck1 ASIC uses a 4-phase handshake and the Quasi Delay Insensitive (QDI) Dual Rail (DR) data encoding inside the pipelines [35] of the sensor, sensor event pre-processing, convolutional cores, $\mathrm{NoC}$ as well as the first half of the readout core pipeline. The encoding is converted to Bundled Data (BD) encoding for optional offchip asynchronous event communication as well as for the Static Random Access Memory (SRAM) interface, which is using self-timed BD encoding. The readout core is divided into

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-03.jpg?height=409&width=862&top_left_y=191&top_left_x=171)

Fig. 3. The Speck architecture. The yellow area indicates both the $128 \times 128$ event-based vision sensor with its 2D asynchronous readout and the sensor event pre-processing pipeline. The blue area indicates the NoC responsible for all the event routing between all the components. The area indicated in orange incorporates all the nine sCNN cores that handle one convolution and one pooling layer each. The sCNN cores can optionally be operated as fully connected SNN layers with some restrictions. The brown area indicates the decision readout logic. This core enables interfacing to simple synchronous periphery.

an asynchronous part, talking to the $\mathrm{NoC}$, and a synchronous part to ease integration with standard synchronous off-theshelf components, micro-controllers and infrastructure. The combinatorial logic inside the QDI DR pipelines consists of pull-up/pull-down non-inverting gates, while inversion is modelled by swapping true and false wires of the DR data bits. These non-inverting state-holding gates only model the positive transition of the true or false wire output while the negative signal transition occurs on the reset phase of the handshake. This ensures that the logic is hazard free. The Place and Route (P\&R) is done by standard industry tools. Performance is ensured by hierarchically detailed automatic floor planning that employs extensive guides and fences for the individual components and pipeline stages.

## III. ARCHITECTURE

The architecture is comprised of 4 different components: the convolution cores, the sensor, the sensor event pre-processing block, and the readout core [43-45]. These components are connected by a unicast event routing system the NoC, depicted in Fig. 3. Walking from the incoming photons through the processing:

## A. Sensor

The sensor of Speck1 consists of $128 \times 128$ individually operating event-based Vision Pixels, also called Dynamic Vision Pixels [14]. These pixels encode the incident light intensity temporally on a logarithmic intensity scale, also known as TC encoding. The analog pixel design follows the design given in [46] and was provided by Chenan Li from IniVation AG as an analog pixel Intellectual Property Macro (IP). Each pixel is attached to a single handshake buffer to decouple the pixel reset and timing from the nanosecond delays of the arbitration readout system. From the handshake pixel buffer, the event is handed to the arbitration system by signalling a shared pre-charge pull-down bus in the column

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-03.jpg?height=1347&width=439&top_left_y=194&top_left_x=1298)

Fig. 4. The sensor event pre-processor (Sensor event preprocessing in Fig. 3). The pre-processor can process events from the built-in sensor as well as events from off-chip. The in-built sensor events can also be streamed off-chip for monitoring and further processing. The pipeline stages pool, cut, rotate, mirror, channel filter and shift the input event stream, before forwarding it to one or two destination layers.

and on acknowledge signalling on a shared bus in the row. The arbitration is built out of one arbiter tree for column arbitration and one for the rows $[47,48]$. It follows the same design found in DYNAP-SE [39]. The event address is encoded with a QDI DR encoder from the acknowledge signals of the arbitration trees and handed off as a 4 phase handshake Address Event Representation channel (AER) word to the event pre-processing block. A complete arbitration process with ID encoding takes approximately $2.5-7.5 n s$ for a single readout. This can be optimised significantly by known techniques $[49,50]$, but the specification requirements of realworld signals are met with a margin which allows for a more basic arbitration approach. The arbitration endpoint with buffer in the pixel itself is optimised to limit the transistor count and
results in a fill factor of $45 \%$ front illumination for each pixel. The pixel also has a configurable kill switch to eliminate any hot pixel defects due to fabrication at the pixel level by forcing the pixel and buffer into a reset state.

## B. Sensor event pre-processing core

To conform the raw AER event stream from the sensor to the requirements of the sCNN a pre-processing stage is required. The image may be flipped, rotated or cropped if only a Region Of Interest (ROI) of the image is required. A lower resolution of the image might be required or the polarity can be ignored. To accomplish this, the sensor event pre-processing pipeline consists of multiple stages seen in Fig. 4. The sensor event will travel through the following pipeline stages:

- Sensor interface: the chip can receive pixel events from the built-in sensor and external sources directly via an AER interface and send the sensor events off-chip for monitoring.
- Pooling: sum pooling can be used to scale the $2 \mathrm{D}$ input address space of every event word by $1: 1,1: 2$ and $1: 4$ on $x$ and $y$ coordinate components individually.
- ROI cutting: cutting can be used to cut a $1 \times 1$ to $128 \times 128$ size patch out of the $2 \mathrm{D}$ input address space that is forwarded to the sCNN.
- Image rotation and mirroring, in case the smart sensor is mounted sideways, on top or is looking through a mirror, the $2 \mathrm{D}$ input address space $x$ and $y$ coordinate components can be flipped, inverted and swapped.
- Polarity filtering: polarity selection enables the selection of both polarities as separate channels, to filter one of them or to combine both polarities on a single channel.
- Source mapping: the resulting pre-processed event can be forwarded to up to 2 destination layers via the NoC, a routing header is attached and one event is sent per destination.

The event is then sent to the convolutional cores via the NoC.

## C. Network on Chip

The NoC router follows a star topology. The routing system operates in a non-blocking way for any feed-forward network model and routes events via AER connections. The mapping system allows data to be sent from one convolution core to up to 2 other cores and for one core to receive events from multiple sources without addressing superposition with up to 1024 incoming feature channels. On every incoming channel the routing header of every AER packet is read and the payload directed to the destination. This is done by establishing separate physical routing channels that are parallel and do not intersect for any network topology that does not contain recurrence. This prevents skew due to other connections and deadlocks by loops inside the pipeline structure. In combination with the PCFB method the First In First Out (FIFO) structures display low latency in routing the AER words to their destination. The routing header information is striped from the word during transport, and the payload delivered to its intended destination.

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-04.jpg?height=1556&width=407&top_left_y=179&top_left_x=1312)

Fig. 5. The convolution core architecture (sCNN core in Fig. 3). An event $\{c, x, y\}$ enters the convolution core pipeline, with $c$ as the incoming channel/feature, $x$ as the horizontal coordinate and $y$ as the vertical coordinate After padding, the event is now expanded to $\left\{c, x_{p}, y_{p}\right\}$. The Kernel Anchor determines the anchor in both kernel and neuron space $\left\{c, x_{0}, y_{0}, x_{0}^{k}, y_{0}^{k}\right\}$. With $x_{0}, y_{0}$ being the anchor in the neuron space and $x_{0}^{k}, y_{0}^{k}$ for the kernel space. The kernel address sweep now calculates the kernel expansion in $x, y$ and $f$ the output channel/features to $Z *\left\{\left(c, f, x^{k}, y^{k}\right),(f, x, y)\right\}$, with $Z$ being the synaptic fan-out. The parallel address compression packs the storage addresses compact to avoid unused storage gaps for the neuron $(f, x, y)=>n_{\text {comp }}$ and kernel $\left(c, f, x^{k}, y^{k}\right)=>k_{\text {comp }}$. Depending on the core, the kernel memory is split into one or multiple memory blocks for parallel access. The kernel value is read from the storage address $k_{\text {comp }}$, $\left\{w, n_{\text {comp }}\right\}$ with $w$ being the signed 8 -bit synaptic weight. On a simulation tick, the bias/leak sweep will generate a pair of $\left\{b_{\text {comp }}, n_{\text {comp }}\right\}$ for every active neuron, the address $b_{\text {comp }}$ gets read in the bias/leak memory and forwarded as $\left\{w, n_{\text {comp }}\right\}$ with the kernel events to the neuron. Depending on the core, the neuron unit is split into one or multiple parallel compute units, see Fig. 7. The address space decompression turns the $\left\{n_{\text {comp }}\right\}$ back to $\{f, x, y\}$. The sum pooling operates on the same event structure $\left\{f, x_{s}, y_{s}\right\}$. And the Channel shift and routing prepare it for routing $S *\left\{d_{x}, f_{s}, x_{s}, y_{s}\right\}$ with $S$ being the source fan-out of 1 or $2, d_{x}$ corresponding to the destination id and $f_{s}$ being the arithmetically shifted destination channel.

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-05.jpg?height=1333&width=683&top_left_y=198&top_left_x=255)

Fig. 6. The sCNN computation principle. For a single event (black) arriving on the input space, a corresponding anchor coordinate in the kernel space (orange middle) is calculated and the weight at this position is read. The read weight is applied to the corresponding anchor neuron in the output space (orange right). The anchor is used to define the starting position in kernel and output space depending on the layer configuration. From this point onward the kernel space is moved from the anchor, in this case with a stride of 1 , so by 2 fields in the $x$ coordinate (blue middle), while the neuron is moved on field in the $x$ coordinate from the anchor in the opposite direction (blue right). This is continued until all kernel positions possible have been read. In this case 2 additional - brown and yellow as the stride configuration in this case skips every other position in $x$ and $y$ input coordinate space. This step is repeated for all output channels/features $f$ with their corresponding kernel. The stride and kernel size are configurable and will result together with $x, y, c, f$, with $c$ being the input channel in different sweeps and resulting affected neurons.

## D. Convolution Cores

In contrast to CNNs, event-driven sCNNs do not operate on a full frame basis: for every arriving pixel event, the convolution is computed for only that pixel position. For a given input pixel, all output neurons are traversed which are associated with its convolution, as opposed to a kernel that is swept pixel-by-pixel over a complete image. An incoming

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-05.jpg?height=607&width=830&top_left_y=190&top_left_x=1100)

Fig. 7. The neuron compute unit from Fig. 5. It uses in-memory-controller compute to model the LIF neuron model. The flow control at the input ensures that the controller always has a bubble and is therefore deadlock-free. The signed 16-bit neuron state variable gets read, modified by the signed synaptic or bias input, compared and written back. In case a threshold condition is met, the $\left\{n_{\text {comp }}\right\}$ is sent out to indicate the corresponding neuron spiked. The above-threshold condition can both trigger a subtraction operation or a reset to a fixed value of the corresponding neuron state variable. The state variable cannot cross a configured lower bound and will be clamped to that value in case any operation brings the variable below it.

event includes the $x$ and $y$ coordinates of the active pixel as well as the input channel $c$ it belongs to. A step-by-step walk through for an arriving event as seen in Fig 5-7:

- Zero padding: the event is padded to retain the layer size if needed. The image field, i.e. the address of the events, is expanded by adding pixels to the borders to retain the image size after the convolution if needed.
- Kernel anchor and address sweep: In the Kernel mapper, the event is first mapped to an anchor point in the output neuron and the kernel space. The behaviour is described in Fig. 6. Using this anchor the kernel, represented by an address, is linked to an address point in the output space. The referenced kernel is swept over the incoming pixel coordinate. The kernel address and the neuron address are swept inversely to each other as seen in Fig. 6. For every channel in the output neuron space, the kernel anchor address is incremented, so that a new kernel for the new output channel is used. The sweep over the kernel is repeated. In case a stride is configured in either the horizontal or vertical direction, the horizontal and vertical sweeps are adjusted to jump over kernel positions accordingly.
- Address space compression: To effectively use the limited memory space, the verbose kernel address as well as the neuron address are compressed to avoid unused memory locations. Depending on the configuration the address space gets packed, so that there are no avoidable gaps inside the address that are not used by the configuration.
- Kernel memories: The kernel addresses are then distributed on the parallel kernel memory blocks according

![](https://cdn.mathpix.com/cropped/2024_06_04_5971fe0c71de0d9142f9g-06.jpg?height=304&width=885&top_left_y=173&top_left_x=162)

Fig. 8. The readout core, from Fig. 3, is separated into an input FIFO stage (light brown), a variable time sliding average stage (brown) and a readout processing stage (dark brown). The FIFO prevents stalls occurring in the asynchronous to synchronous transition in the following block to affect the NoC. The stage that writes to the time sliding average variable is an asynchronous operation whilst in parallel the read in a readout time window is synchronous. The compute stage (dark brown) that does maximum operations and threshold comparisons is fully synchronous to enable easy offchip interfacing to standard components.

to the compressed addresses, and the specific signed 8 bit kernel weight is read. The weight and the compressed neuron address are then directed to the parallel neuron compute-in-memory-controller blocks according to the address location. Kernel positions with 0 weight are skipped during reading and are not forwarded to the neuron.

- Neuron compute units: The compute-in-memorycontroller block model a LIF neuron with a linear leak for every signed 16-bit memory word. Besides classic read and write, the memory controller has a read-add-check_spike-write operation, as shown in Fig 7. Whenever the accumulated value reaches a configured threshold, an event is sent out and the neuron state variable has a threshold subtraction or reset written back.
- Bias and leak address sweep and memory: The leak (or bias) is modelled via an additional memory controller. The Leak/bias controller has a neuron individual signed 16-bit weight stored for every output channel map. On a time reference tick an update event with this bias is sent to all its active neurons. The reference tick is supplied from off chip and fully user configurable.
- Pooling: The output events are finally merged onto a pooling stage. The pooling stage operates on the sum pooling principle, i.e., it merges the events from 1,2 or 4 neurons in both $\mathrm{x}$ and $\mathrm{y}$ coordinates individually.
- Channel shift and routing: Before entering the routing NoC, the channels are shifted and a prefix with routing information is added, one event is sent per destination for up to 2 .

The individual convolution cores can also be used as fully connected layers with up to $65 \mathrm{~K}, 32 \mathrm{~K}$ and $16 \mathrm{~K}$ synaptic connections respectively to model final readout decision layers. Fabrication defects in all SRAM memories for kernel, bias and neuron can be blacklisted with a kill bit per word and are skipped during computation.

## E. Readout core

The readout core transforms the event data into simple readable values and results. It can simultaneously calculate up to 16 spike class counts or moving averages, where the moving average lengths can be configured and also be set to time bin counting with no averaging. The readout layer optionally compares all values to fixed thresholds, computes the current maximum and makes these results easily accessible on the pins of the chip. All computed values can also be read out for further processing. From the variable time siding average units as seen in Fig. 8 onwards, the circuit performs a clock domain crossing from asynchronous to synchronous. After a time reference clock tick, the previous data is presented and held at the output by standard flip flops and can be interfaced with via standard synchronous processing elements. The time reference tick is supplied from off-chip and controlled by the receiving system.

## IV. RESULTS AND DISCUSSION

We propose that a medium-scale pipeline architecture integrated with a sensor can offer improvements both in architecture design and in applicability. Matching an eventbased sensor with a direct connection to an event based processor avoids incurring delays caused by event batching, as commonly done in event cameras as well as delays due to full frame capture and conversion by industry standard image sensors. Each sensory event is transmitted instantaneously after being encoded by our arbitration system. Our pipeline architecture processes a single event with a latency of $3.36 \mu \mathrm{s}$ through a nine-layer convolution with pooling network with kernel size $3 \times 3$ in each layer. This latency was measured by sending an event to and reading the resulting event from the ASIC with the time between the input and output request edge on the I/O pads. Many events are processed in parallel inside the system as they move step-by-step through the fine-grained pipeline enabling high throughput, set by the neuron compute units, with a measured $\approx 30 \mathrm{Mevents} / \mathrm{s}$ per unit. Compared to frame-based systems, the event-based nature of the SoC can give a classification output as soon as enough evidence is accumulated, as opposed to a frame-based camera with CNN accelerator needing to complete the frame processing in its entirety. As a comparison, the fully integrated, higher resolution ISSCC21 Sony [57] needs a fixed 10.1ms for a $2028 \times 1520$ or $21.3 \mathrm{~ms}$ for a $4056 \times 3040$ full-frame image. To obtain compelling a latency on Speck, the time-to-firstclassification (stimulus onset to first classification result) needs to be integrated into the loss function during network training. Looking at classical SNN processors, the time step synchronisation of TrueNorth [25] and Loihi1 [26]/2 do not allow cores to run controllably out of sync. The latency for each layer is significant when fed with real-time streamed sensory data, as event generation takes one whole simulation time step $\Delta t$ in contrast to our architecture. For those systems lowering $\Delta t$ results in a directly proportional trade-off in increase of power consumption that is not present in this work. The small architectures of Camunas12 [17] and Yousefzadeh15 [20]

| Chip | Speck1 | Loihi1 <br> $[26,51]$ | Loihi2 <br> $[52,53]$ | TrueNorth <br> $[25,54]$ | SCAMP5 <br> $[55,56]$ | ISSCC21 Sony <br> $[57]$ | Spoon <br> $[18]$ | Camunas12 <br> $[17]$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Design method | async | async | async | async | analog+sync | sync | sync | sync |
| CMOS technology | $65 \mathrm{~nm}$ | $14 \mathrm{~nm}$ | $7 \mathrm{~nm}$ (Intel 4) | $28 \mathrm{~nm}$ | $180 \mathrm{~nm}$ | $65 \mathrm{~nm} \& 22 \mathrm{~nm}$ | $28 \mathrm{~nm}$ | $350 \mathrm{~nm}$ |
| Area incl. I/O | $30 \mathrm{~mm}^{2}$ | $60 \mathrm{~mm}^{2}$ | $31 \mathrm{~mm}^{2}$ | $430 \mathrm{~mm}^{2}$ | $100 \mathrm{~mm}^{2}$ | $62 m m^{2}$ | $0.32 m^{2}$ | $31.9 \mathrm{~mm}^{2}$ |
| Number of neuron | $327.6 \mathrm{~K}$ | $131 \mathrm{~K}$ | $1 \mathrm{M}$ | $1 \mathrm{M}$ | $65 \mathrm{~K}$ | - | 922 | $4 \mathrm{~K}$ |
| Synaptic memory | $272 \mathrm{~KB}$ | $16 \mathrm{MB}$ | $24 \mathrm{MB}$ | $32 \mathrm{MB}$ | $112 \mathrm{~KB}+$ analog | $8 \mathrm{MB}$ | $750 \mathrm{~B}$ | 512B |
| Vision sensor | $128 \times 128$ | $\mathrm{No}^{2}$ | $\mathrm{No}^{1}$ | $\mathrm{No}^{2}$ | $256 \times 256$ | $4056 \times 3040$ | $\mathrm{No}^{2}$ | $\mathrm{No}^{1}$ |
| Result readout layer | Yes | $\mathrm{Yes}^{3}$ | $\mathrm{Yes}^{3}$ | No | No | Yes | No | No |
| Max kernel size | $16 \times 16$ | $64 \times 64$ | $64 \times 64^{4}$ | $16 \times 16$ (1bit) | $4 x 4$ | arbitrary $^{4}$ | $5 \times 5$ | $32 \times 32$ |
| Convolution layers (C) | 9 | arbitrary | arbitrary | arbitrary | sequential | arbitrary $^{4}$ | 1 | 1 |
| Pooling layers (P) | 9 | arbitrary | arbitrary | arbitrary | sequential | arbitrary $^{4}$ | 1 | - |
| Features per layer | 1024 | arbitrary | arbitrary | arbitrary | $\frac{65 \mathrm{~K}}{\text { kernel fields }}$ | arbitrary $^{4}$ | 10 | 32 |
| 8 bit syn for $3 \times 3$ | $3.01 \mathrm{G}$ | $16 \mathrm{M}^{5}$ | $4.29 \mathrm{G}^{9}$ | $9.4 \mathrm{M}^{10}$ | $\frac{\text { kernel fields }}{-}$ | - | $141 \mathrm{~K}$ | $1.18 \mathrm{M}$ (4bit) ${ }^{11}$ |
| 8 bit syn for $4 x 4$ | $5.35 G$ | $16 \mathrm{M}^{5}$ | $4.29 \mathrm{G}^{9}$ | $16.7 \mathrm{M}^{10}$ | - | - | $250 \mathrm{~K}$ | $2.1 \mathrm{M}(4 \mathrm{bit})^{11}$ |
| 8 bit $\operatorname{syn}$ for $7 x 7$ | $6.16 \mathrm{G}$ | $16 \mathrm{M}^{5}$ | $4.26 \mathrm{G}^{9}$ | -10 | - | - | - | $3.2 \mathrm{M}(4 \mathrm{bit})^{11}$ |
| End-to-end latency | $1.58 \mu \mathrm{s}(1 \mathrm{CP} / \mathrm{F})^{8}$ | $>\Delta t(1 \mathrm{CP} / \mathrm{F})^{6}$ | $>\Delta t(1 \mathrm{CP} / \mathrm{F})^{6}$ | $>1 m s(1 \mathrm{CP} / \mathrm{F})^{6}$ | $778 \mu s(1 \mathrm{CP})^{7}$ | - | - | $\mathbf{0 . 6 8 0} \mu \mathrm{s}(\mathbf{1 C})$ |
| for sensory event | $\mathbf{3 . 3 6} \mu s(\mathbf{C P} / \mathbf{F})^{8}$ | $>9 \Delta t(9 \mathrm{CP} / \mathrm{F})^{6}$ | $>9 \Delta t(9 \mathrm{CP} / \mathrm{F})^{6}$ | $>9 m s(9 \mathrm{CP} / \mathrm{F})^{6}$ | $5595 \mu s(2 \mathrm{CP}+2 \mathrm{~F})^{7}$ | $4500 \mu s(28 \mathrm{C} / \mathrm{P} / \mathrm{F})^{7}$ | $117 \mu s(1 \mathrm{CP}+2 \mathrm{~F})$ | - |

Fig. 9. The table shows the technical specifications of this and related ASICs with respect to sCNN networks. With the first 4 columns belonging to the group of medium and large systems, the last 2 are small resource-constrained systems built for a limited set of toy applications. The SCAMP5 is an exception as it is an analog cellular processor inside a visual sensor and ISSCC21 Sony is a fully integrated standard high resolution image sensor with CNN tensor processor sandwiched together. Only Speck1 and Loihi2 are specifically built to run larger sCNN models. This can be seen in the resulting synaptic counts that are a direct translation of how many different kernels and how often a kernel can be applied. Compared to the general purpose Loihi2 our pure sCNN processor archives similar or higher synaptic numbers while having significantly fewer memory resources. As Speck1 is specifically built to consume real-time data, its latency is significantly lower than Loihi1/2 and TrueNorth which architecturally introduce a latency of one timestep $\Delta t$ on event generation in each layer. The small systems are by principle advantages for latency and area as they are limited to running only compact toy networks.

Notes: (F) fully connected layer. (1) External sensor interface available. (2) Sensor can be connected with additional hardware. (3) Readout interpretation via x86 CPU on SoC. (4) Estimated, information not publicly available. (5) Bit packing of 8bit synapse words assumed as described in [26]. Formulae used: min(\#neuron per core $\cdot$ \#core $\cdot$ floor(\#inaxon / \#synapses in kernel) $\cdot$ \#syn in kernel, synmem per core $\cdot$ \#core / 8bit). (6) When connected to a sensor and the simulation time is synchronised to real-time, latency per layer is minimum the simulation time resolution $\Delta t, 1 \mathrm{~ms}$ for TrueNorth, as event generation takes one $\Delta t$. (7) full-frame processed. (8) Measured input request to output request edge, when using a $3 \times 3$ kernel in every layer, with stride 1 , padding 1, pooling has no effect on the latency. (9) For the sCNN on-the-fly mapping no restrictions on kernel sweep in $x, y$ and $f$ coordinates assumed: Formulae used: \#neuron per core $\cdot$ \#core - floor(\#inaxon / \#synapses in kernel) $\cdot$ \#syn in kernel. \#inaxon is not mentioned as improved [52,53] and assumed with 4096 as for Loihi1. (10) If \#synapse in kernel $8 \leq 256$ then \#neuron per core . \#cores per chip . \#synpses in kernel else not implementable. (11) Formulae used: min(lower $(\sqrt{\text { \#kernel memory words }} / \sqrt{\text { \#synapses in kernel }})^{2}$, \#max kernel $)$. \#synapses in kernel $\cdot$ \#neurons

display low latency as they are constrained to small toy networks that don't require a SoC and are significantly smaller in specification, reducing power dissipation and transmission delays. SCAMP5 [55] is another fully-integrated smart vision sensor that follows a very different approach, combining the pixel with simple but powerful analog cellular compute units. They can run simple binary $\mathrm{CNN}$-based networks with quite some overhead [56]. A second key point for our presented architecture is the synaptic memory utilisation. Especially for CNN based architectures, the on-the-fly computation of synaptic connections allows for minimising memory requirements. This in turn saves area and energy - in the case of SRAMs, both dynamic and static. Our dedicated sCNN approach allows for many more synaptic connections by using the kernel weights stored in memory and computing all the synapses that share weights compared to standard SNN implementations with minimal additional compute required. In the table in Fig. 9, the benefits of a dedicated sCNN approach are evident at the $3 \times 3,4 \times 4$ and $7 \times 7$ kernel examples where there are magnitudes of differences between the formed synaptic connections. The only other big-scale processor Loihi2 [52] that supports kernel convolution by design restricts its applicable number of formed synapses by its axon-based routing schema with fixed limits on incoming axons per core. For sCNN, this network results in either an underutilisation of the many more neurons Loihi2 has compared to our solution or a significant restriction on the in and out feature maps supported. The axonbased routing presents its strength in other network topologies and is widespread in SNN processors [24,26,39,54] which follow a general purpose SNN approach.

By offering a simple and small end-to-end system comprising the sensor, the processing, and the decision readout, we offer a unique and easily applicable solution. To show the performance of our sCNN capability of our SoC, we chose a benchmark that can also be run by the smaller systems presented to give a comparison: The N-MNIST dataset [60] is a spike-converted version of the MNIST dataset [61]. It is recorded using a vibrating ATIS sensor[12] with the original images displayed on an LCD monitor. We train a four layer sCNN with Sinabs [62] using both ANN2SNN [63, 64] and Back Propagation Through Time (BPTT) methods. For the training samples, we use the first raw $250 \mathrm{~ms}$ data out of $300 \mathrm{~ms}$ and a timestep of $1 \mathrm{~ms}$. For training each method, we carried out five repetitions of training with different random initialisations and five repetitions of testing. The testing experiments are performed offline on $\mathrm{PC}$ and as measurements on Speck1 using a deployed and quantised version of the PC network model. The events are fed into the SoC directly as a raw external sensor event stream bypassing the inbuilt sensor for comparable and repeatable results. For ANN2SNN, we

| Dataset | Chip | Method | Neuron Model | Num Neuron | Num Parameter | On-Chip accuracy | Power | Energy per inference |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| NMNIST | Loihi1 | SNN TB [58] | IF | 522 | $597 \mathrm{~K}$ | $98.43 \%$ | - | $290 \mu J$ |
|  | Zhang21 | STP [59] | MLIF | 256 | $131 K$ | $95.7 \%$ | $3.42 \mathrm{~mW}$ | - |
|  | Spoon | CNN [18] | IF-top1 | 922 | 750 | $93.8 \%$ | - | $\mathbf{0 . 6 6 5} \mu J$ |
|  | Spoon | CNN-DRTP [18] | IF-top1 | 922 | 750 | $93 \%$ | - | $\mathbf{0 . 6 6 5} \mu J$ |
|  | Speck1 | ANN2SNN [this work] | IF | $11 \mathrm{~K}$ | $9.3 \mathrm{~K}$ | $86.17 \%$ | $0.47 \mathrm{~mW}$ | $141 \mu J$ |

Fig. 10. Speck1 performance compared to other architectures. We show that Speck1 matches state-of-the-art performance on SNN hardware systems like Loihi1, while our more dedicated approach consumes less energy despite being fabricated in a much older but significantly more cost effective technology. In such a resource friendly benchmark, significantly smaller solutions like Spoon can perform well with some drop in accuracy while gaining a lot on energy efficiency. Most real world applications include more complex or temporal information and require larger networks that are not accessible to small sCNN systems. The reported power consumption excludes pad frame power consumption for Speck1. The network structure used for this work is $34 \times 34 \times 2-16 \mathrm{C}$ 16C3-P2-8C3-F10 with $300 \mathrm{~ms}$ sample exposure, 100 training epochs and a learning rate of 1e-3. 16C5 represents convolutional layer with $5 \mathrm{x} 5$ kernel with 16 channels. P2 represents the $2 \times 2$ pooling layer and $\mathrm{F}$ represents a fully connected layer.

obtained an SNN model with an average offline testing accuracy of $94.2 \%$ and an on-chip accuracy of $86.17 \%$. For BPTT training, it demonstrates state-of-art recognition testing results where offline testing provides an average testing accuracy of $99.3075 \%$ and on-chip measured testing accuracy of $98.50 \%$. The samples are presented in real-time so for an inference it takes the length of a sample, with the classification result arriving during the sample presentation as the sample is more comparable to a video. The measured mean of the time-tofirst-classification is $18.405 \mathrm{~ms}$ with a standard deviation of $4.818 \mathrm{~ms}$, please note that the network was purely optimised for accuracy and integration of the latency into the loss function can reduce the time-to-first-classification significantly as the hardware itself is optimised for low latency responses. In Fig. 10 the results are compared to other SNN capable systems. Most prominent are the effects of dedicated sCNN hardware systems compared to general purpose SNN hardware, so that our proposed system outperforms Loihi1 in accuracy and energy despite the multiple generations older fabrication technology ( $14 \mathrm{~nm}$ vs $65 \mathrm{~nm}$ ). Additional evaluations on further datasets can be found in [65]. Further applications and demos of Speck with the sensor and sCNN combined operation can be seen online. One fall detection demo, were the $\mathrm{SoC}$ is detecting if a human fell and one to detect and follow the human [66], as well as brief overview of possible application including face and obstacle detection, as well as gesture recognition [67].

The training of sCNN networks for Speck is supported by the rich, open source, high-level framework Sinabs based on PyTorch and a full development solution called Samna [68]. Sinabs can be used for sCNN training for this ASIC. Specifically, it allows optimisations for sparsity and supports the estimation of synaptic operations (SOP) of its networks.

## V. CONCLUSION

We presented a smart sensor fully integrated as a SoC, which shifts an efficient sCNN architecture directly to the sensor edge. By combining both sensor and processing on a single die into a smart sensor, we lower unit production costs significantly while saving energy on high-speed and low- latency data communication, as the raw sensory data never has to leave the chip. The on-the-fly synaptic kernel mapping system lowers the memory resource requirements significantly, making the architecture accessible to larger, more cost effective fabrication technologies. The event-driven nature of the embedded machine vision sensor delivers high-speed signals in a sparse data manner. The advantages of this combination are taken further by the implemented deep sCNN processing pipeline, which is optimised for low latency and exploits the benefits of highly sparse computation. This ultimately enables low-latency visual processing on a tiny energy budget for edge and end-to-end applications. Finally, we believe the opportunities and market for different types of smart sensors that follow a similar design principle are very promising.

## REFERENCES

[1] C. Frenkel, "Sparsity provides a competitive advantage," Nature Machine Intelligence, vol. 3, no. 9, pp. 742-743, Sep. 2021. [Online]. Available: https://www.nature.com/articles/s42256-021-00387-y

[2] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, "You Only Look Once: Unified, Real-Time Object Detection," in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USA: IEEE, Jun. 2016, pp. 779-788. [Online]. Available: http://ieeexplore.ieee.org/document/7780460/

[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "ImageNet classification with deep convolutional neural networks," Communications of the ACM, vol. 60, no. 6, pp. 84-90, May 2017. [Online]. Available: https://dl.acm.org/doi/10.1145/3065386

[4] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, and D. H. Yoon, "In-Datacenter Performance Analysis of a Tensor Processing Unit," in Proceedings of the 44th Annual International Symposium on Computer Architecture. Toronto ON Canada: ACM, Jun. 2017, pp. 1-12. [Online]. Available: https://dl.acm.org/doi/10.1145/3079856.3080246

[5] A. Reuther, P. Michaleas, M. Jones, V. Gadepally, S. Samsi, and J. Kepner, "Survey of Machine Learning Accelerators," in 2020 IEEE High Performance Extreme Computing Conference (HPEC). Waltham, MA, USA: IEEE, Sep. 2020, pp. 1-12. [Online]. Available: https://ieeexplore.ieee.org/document/9286149/

[6] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos, "Cnvlutin: ineffectual-neuron-free deep neural network computing," ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 1-13, Oct. 2016. [Online]. Available: https: //dl.acm.org/doi/10.1145/3007787.3001138

[7] A. Basu, L. Deng, C. Frenkel, and X. Zhang, "Spiking Neural Network Integrated Circuits: A Review of Trends and Future Directions," in 2022 IEEE Custom Integrated Circuits Conference (CICC). Newport Beach, CA, USA: IEEE, Apr. 2022, pp. 1-8. [Online]. Available: https://ieeexplore.ieee.org/document/9772783/

[8] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida, "Deep learning in spiking neural networks," Neural Networks, vol. 111, pp. 47-63, Mar. 2019. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S0893608018303332

[9] M. Sorbaro, Q. Liu, M. Bortone, and S. Sheik, "Optimizing the Energy Consumption of Spiking Neural Networks for Neuromorphic Applications," Frontiers in Neuroscience, vol. 14, p. 662, Jun. 2020. [Online]. Available: https://www.frontiersin.org/article/10.3389/ fnins.2020.00662/full

[10] T. Wiegand, G. Sullivan, G. Bjontegaard, and A. Luthra, "Overview of the H.264/AVC video coding standard," IEEE Transactions on Circuits and Systems for Video Technology, vol. 13, no. 7, pp. 560-576, Jul. 2003. [Online]. Available: https://ieeexplore.ieee.org/document/1218189/

[11] P. Lichtsteiner, C. Posch, and T. Delbruck, "A 128x128 120 dB 15 us Latency Asynchronous Temporal Contrast Vision Sensor," IEEE Journal of Solid-State Circuits, vol. 43, no. 2, pp. 566-576, 2008. [Online]. Available: http://ieeexplore.ieee.org/document/4444573/

[12] C. Posch, D. Matolin, and R. Wohlgenannt, "A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS," IEEE Journal of Solid-State Circuits, vol. 46, no. 1, pp. 259-275, Jan. 2011. [Online]. Available: http://ieeexplore.ieee.org/document/5648367/

[13] T. Serrano-Gotarredona and B. Linares-Barranco, "A 128x128 $1.5 \%$ Contrast Sensitivity $0.9 \% \quad$ FPN $\quad 3 \quad \mu \mathrm{s}$ Latency 4 $\mathrm{mW}$ Asynchronous Frame-Free Dynamic Vision Sensor Using Transimpedance Preamplifiers," IEEE Journal of Solid-State Circuits, vol. 48, no. 3, pp. 827-838, Mar. 2013. [Online]. Available: http://ieeexplore.ieee.org/document/6407468/

[14] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis, and D. Scaramuzza, "Event-Based Vision: A Survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 1, pp. 154-180, Jan. 2022. [Online]. Available: https://ieeexplore.ieee.org/ document/9138762/

[15] T. Finateu, A. Niwa, D. Matolin, K. Tsuchimoto, A. Mascheroni, E. Reynaud, P. Mostafalu, F. Brady, L. Chotard, F. LeGoff, H. Takahashi, H. Wakabayashi, Y. Oike, and C. Posch, "5.10 A $1280 \times 720$ Back-Illuminated Stacked Temporal Contrast Event-Based Vision Sensor with $4.86 \mu \mathrm{m}$ Pixels, 1.066GEPS Readout, Programmable Event-Rate Controller and Compressive Data-Formatting Pipeline," in 2020 IEEE International Solid- State Circuits Conference - (ISSCC). San Francisco, CA, USA: IEEE, Feb. 2020, pp. 112-114. [Online]. Available: https://ieeexplore.ieee.org/document/9063149/

[16] Q. Liu, O. Richter, C. Nielsen, S. Sheik, G. Indiveri, and N. Qiao, "Live Demonstration: Face Recognition on an Ultra-Low Power Event-Driven Convolutional Neural Network ASIC," in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Long Beach, CA, USA: IEEE, Jun. 2019, pp. 1680-1681. [Online]. Available: https://ieeexplore.ieee.org/document/9025522/

[17] L. Camunas-Mesa, C. Zamarreno-Ramos, A. Linares-Barranco, A. J. Acosta-Jimenez, T. Serrano-Gotarredona, and B. Linares-Barranco, "An Event-Driven Multi-Kernel Convolution Processor Module for Event-Driven Vision Sensors," IEEE Journal of Solid-State Circuits, vol. 47, no. 2, pp. 504-517, Feb. 2012. [Online]. Available: http://ieeexplore.ieee.org/document/6054033/

[18] C. Frenkel, J.-D. Legat, and D. Bol, "A 28-nm Convolutional Neuromorphic Processor Enabling Online Learning with Spike-Based Retinas," in 2020 IEEE International Symposium on Circuits and
Systems (ISCAS). Seville, Spain: IEEE, Oct. 2020, pp. 1-5. [Online]. Available: https://ieeexplore.ieee.org/document/9180440/

[19] L. A. Camuñas-Mesa, Y. L. Domínguez-Cordero, A. LinaresBarranco, T. Serrano-Gotarredona, and B. Linares-Barranco, "A Configurable Event-Driven Convolutional Node with Rate Saturation Mechanism for Modular ConvNet Systems Implementation," Frontiers in Neuroscience, vol. 12, p. 63, Feb. 2018. [Online]. Available: http://journal.frontiersin.org/article/10.3389/fnins.2018.00063/full

[20] A. Yousefzadeh, T. Serrano-Gotarredona, and B. Linares-Barranco, "Fast Pipeline 128x128 pixel spiking convolution core for event-driven vision processing in FPGAs," in 2015 International Conference on Event-based Control, Communication, and Signal Processing (EBCCSP). Krakow, Poland: IEEE, Jun. 2015, pp. 1-8. [Online]. Available: http://ieeexplore.ieee.org/document/7300698/

[21] J. Zhang, L. Feng, T. Wang, W. Shi, Y. Wang, and G. Zhang, "FPGA-Based Implementation of an Event-Driven Spiking MultiKernel Convolution Architecture," IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 69, no. 3, pp. 1682-1686, Mar. 2022. [Online]. Available: https://ieeexplore.ieee.org/document/9606220/

[22] R. Tapiador Morales, A. Linares-Barranco, A. Jimenez-Fernandez, and G. Jimenez Moreno, "Neuromorphic LIF Row-by-Row Multiconvolution Processor for FPGA," IEEE Transactions on Biomedical Circuits and Systems, pp. 1-1, 2018. [Online]. Available: https: //ieeexplore.ieee.org/document/8526309/

[23] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor, and R. Benosman, "HFirst: A Temporal Approach to Object Recognition," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 10, pp. 2028-2040, Oct. 2015. [Online]. Available: https://ieeexplore.ieee.org/document/7010933/

[24] R. Manohar, "Hardware/software Co-design for Neuromorphic Systems," in 2022 IEEE Custom Integrated Circuits Conference (CICC). Newport Beach, CA, USA: IEEE, Apr. 2022, pp. 01-05. [Online]. Available: https://ieeexplore.ieee.org/document/9772863/

[25] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, C. Guo, Y. Nakamura, B. Brezzo, I. Vo, S. K. Esser, R. Appuswamy, B. Taba, A. Amir, M. D. Flickner W. P. Risk, R. Manohar, and D. S. Modha, "A million spiking-neuron integrated circuit with a scalable communication network and interface," Science, vol. 345, no. 6197, pp. 668-673, Aug. 2014. [Online]. Available: https://www.science.org/doi/10.1126/science. 1254642

[26] M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Y. Cao, S. H. Choday, G. Dimou, P. Joshi, N. Imam, S. Jain, Y. Liao, C.-K Lin, A. Lines, R. Liu, D. Mathaikutty, S. McCoy, A. Paul, J. Tse, G. Venkataramanan, Y.-H. Weng, A. Wild, Y. Yang, and H. Wang, "Loihi: A Neuromorphic Manycore Processor with On-Chip Learning," IEEE Micro, vol. 38, no. 1, pp. 82-99, Jan. 2018. [Online]. Available: https://ieeexplore.ieee.org/document/8259423/

[27] J. Schemmel, D. Briiderle, A. Griibl, M. Hock, K. Meier, and S. Millner, "A wafer-scale neuromorphic hardware system for large-scale neural modeling," in Proceedings of 2010 IEEE International Symposium on Circuits and Systems. Paris, France: IEEE, May 2010, pp. 1947-1950. [Online]. Available: http://ieeexplore.ieee.org/document/5536970/

[28] E. Painkras, L. A. Plana, J. Garside, S. Temple, F. Galluppi, C. Patterson, D. R. Lester, A. D. Brown, and S. B. Furber, "SpiNNaker: A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation," IEEE Journal of Solid-State Circuits, vol. 48, no. 8, pp. 1943-1953, Aug. 2013. [Online]. Available: http://ieeexplore.ieee.org/document/6515159/

[29] J. Pei, L. Deng, S. Song, M. Zhao, Y. Zhang, S. Wu, G. Wang, Z. Zou, Z. Wu, W. He, F. Chen, N. Deng, S. Wu, Y. Wang, Y. Wu, Z. Yang, C. Ma, G. Li, W. Han, H. Li, H. Wu, R. Zhao, Y. Xie, and L. Shi, "Towards artificial general intelligence with hybrid Tianjic chip architecture," Nature, vol. 572, no. 7767, pp. 106-111, Aug. 2019 [Online]. Available: http://www.nature.com/articles/s41586-019-1424-8

[30] Arvind and D. E. Culler, "Dataflow Architectures," Annual Review of Computer Science, vol. 1, no. 1, pp. 225-253, Jun. 1986. [Online]. Available: http://www.annualreviews.org/doi/10.1146/annurev. cs.01.060186.001301

[31] A. H. Veen, "Dataflow machine architecture," ACM Computing Surveys, vol. 18, no. 4, pp. 365-396, Dec. 1986. [Online]. Available: https://dl.acm.org/doi/10.1145/27633.28055

[32] B. Lee and A. Hurson, "Dataflow architectures and multithreading," Computer, vol. 27, no. 8, pp. 27-39, Aug. 1994. [Online]. Available: http://ieeexplore.ieee.org/document/303620/

[33] J. Sparsø, Principles of asynchronous circuit design: a systems perspective, reprint first ed. 2002 ed. Boston: Kluwer, 2010, oCLC: 811002097.

[34] S. M. Nowick and M. Singh, "High-Performance Asynchronous Pipelines: An Overview," IEEE Design \& Test of Computers, vol. 28, no. 5, pp. 8-22, Sep. 2011. [Online]. Available: http: //ieeexplore.ieee.org/document/5887304/

[35] J. Sparsø, Introduction to asynchronous circuit design. Kongens Lyngby, Denmark: DTU Compute, Technical University of Denmark, 2020, oCLC: 1199326564.

[36] A. J. Martin, "Programming in VLSI: From Communicating Processes to Delay-Insensitive Circuits," California Institute of Technology, USA, Tech. Rep., 1989.

[37] K. van Berkel, "Beware the isochronic fork," Integration, vol. 13, no. 2, pp. 103-128, Jun. 1992. [Online]. Available: https://linkinghub.elsevier. com/retrieve/pii/016792609290001F

[38] O. Richter, C. Wu, A. M. Whatley, G. Köstinger, C. Nielsen, N. Qiao, and G. Indiveri, "DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor," Neuromorphic Computing and Engineering, vol. 4, Jan. 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/2634-4386/ad1cd7

[39] S. Moradi, N. Qiao, F. Stefanini, and G. Indiveri, "A Scalable Multicore Architecture With Heterogeneous Memory Structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)," IEEE Transactions on Biomedical Circuits and Systems, vol. 12, no. 1, pp. 106-122, Feb. 2018. [Online]. Available: https://ieeexplore.ieee.org/ document/8094868/

[40] S. Ataei, W. Hua, Y. Yang, R. Manohar, Y.-S. Lu, J. He, S. Maleki, and K. Pingali, "An Open-Source EDA Flow for Asynchronous Logic," IEEE Design \& Test, vol. 38, no. 2, pp. 27-37, Apr. 2021. [Online]. Available: https://ieeexplore.ieee.org/document/9321547/

[41] A. Martin, A. Lines, R. Manohar, M. Nystrom, P. Penzes, R. Southworth, U. Cummings, and Tak Kwan Lee, "The design of an asynchronous MIPS R3000 microprocessor," in Proceedings Seventeenth Conference on Advanced Research in VLSI. Ann Arbor, MI, USA: IEEE Comput. Soc, 1997, pp. 164-181. [Online]. Available: http://ieeexplore.ieee.org/document/634853/

[42] A. M. Lines, "Pipelined Asynchronous Circuits," 1998, publisher: California Institute of Technology. [Online]. Available: https://resolver. caltech.edu/CaltechCSTR:1998.cs-tr-95-21

[43] O. Richter, N. Qiao, Q. Liu, and S. Sheik, "Event-driven Spiking Convolutional Neual Network," Patent. [Online]. Available: https: //patentscope.wipo.int/search/en/detail.jsf?docId=WO2020207982

[44] T. Demirci, S. Sheik, N. Qiao, and O. Richter, "Event-driven Integrated Circuit having Interface System," Patent. [Online]. Available: https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2022221994

[45] N. Qiao, M. De Marchi, T. Demirci, O. Richter, S. Sheik, B. Xin, and Z. Kai, "Data updating method and device, storage space setting method and device, chip and equipment," Patent. [Online]. Available: https://patentscope.wipo.int/search/en/detail.jsf?docId=CN339920850

[46] C. Li, L. Longinotti, F. Corradi, and T. Delbruck, "A 132 by 104 10um-Pixel 250uW 1kefps Dynamic Vision Sensor with Pixel-Parallel Noise and Spatial Redundancy Suppression," in 2019 Symposium on VLSI Circuits. Kyoto, Japan: IEEE, Jun. 2019, pp. C216-C217. [Online]. Available: https://ieeexplore.ieee.org/document/8778050/

[47] P. Purohit and R. Manohar, "Hierarchical Token Rings for AddressEvent Encoding," in 2021 27th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC). Beijing, China: IEEE, Sep. 2021, pp. 9-16. [Online]. Available: https://ieeexplore.ieee.org/ document/9565448/

[48] N. Bingham and R. Manohar, "A Systematic Approach for Arbitration Expressions," IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 67, no. 12, pp. 4960-4969, Dec. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/9159863/

[49] B. Son, Y. Suh, S. Kim, H. Jung, J.-S. Kim, C. Shin, K. Park, K. Lee, J. Park, J. Woo, Y. Roh, H. Lee, Y. Wang, I. Ovsiannikov, and H. Ryu, "4.1 A $640 \times 480$ dynamic vision sensor with a $9 \mu \mathrm{m}$ pixel and $300 \mathrm{Meps}$ address-event representation," in 2017 IEEE International Solid-State Circuits Conference (ISSCC). San Francisco, CA, USA: IEEE, Feb. 2017, pp. 66-67. [Online]. Available: http://ieeexplore.ieee.org/document/7870263/

[50] S. Fok and K. Boahen, "A Serial H-Tree Router for Two-Dimensional Arrays," in 2018 24th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC). Vienna: IEEE, May 2018, pp. 78-85. [Online]. Available: https://ieeexplore.ieee.org/document/8589987/
[51] A. Lines, P. Joshi, R. Liu, S. McCoy, J. Tse, Y.-H. Weng, and M. Davies, "Loihi Asynchronous Neuromorphic Research Chip," in 2018 24th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC). Vienna: IEEE, May 2018, pp. 32-33. [Online]. Available: https://ieeexplore.ieee.org/document/8589981/

[52] G. Orchard, E. P. Frady, D. B. D. Rubin, S. Sanborn, S. B. Shrestha, F. T. Sommer, and M. Davies, "Efficient Neuromorphic Signal Processing with Loihi 2," in 2021 IEEE Workshop on Signal Processing Systems (SiPS). Coimbra, Portugal: IEEE, Oct. 2021, pp. 254-259. [Online]. Available: https://ieeexplore.ieee.org/document/9605018/

[53] Intel, "Technology Brief: Taking Neuromorphic Computing to the Next Level with Loihi 2." [Online]. Available: https://download.intel.com/newsroom/2021/new-technologies/ neuromorphic-computing-loihi-2-brief.pdf

[54] F. Akopyan, J. Sawada, A. Cassidy, R. Alvarez-Icaza, J. Arthur, P. Merolla, N. Imam, Y. Nakamura, P. Datta, G.-J. Nam, B. Taba, M. Beakes, B. Brezzo, J. B. Kuang, R. Manohar, W. P. Risk, B. Jackson, and D. S. Modha, "TrueNorth: Design and Tool Flow of a $65 \mathrm{~mW} 1$ Million Neuron Programmable Neurosynaptic Chip," IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 34, no. 10, pp. 1537-1557, Oct. 2015. [Online]. Available: http://ieeexplore.ieee.org/document/7229264/

[55] S. J. Carey, A. Lopich, D. R. W. Barr, B. Wang, and P. Dudek, "A 100,000 fps Vision Sensor with Embedded 535 GOPS/W 256x256 SIMD Processor Array," in 2013 Symposium on VLSI Circuits. United States: IEEE, Jun. 2013, pp. 182-183. [Online]. Available: https://ieeexplore.ieee.org/document/6578654

[56] Y. Liu, L. Bose, R. Fan, P. Dudek, and W. Mayol-Cuevas, "On-sensor binarized CNN inference with dynamic model swapping in pixel processor arrays," Frontiers in Neuroscience, vol. 16, p. 909448, Aug. 2022. [Online]. Available: https://www.frontiersin.org/articles/10.3389/ fnins.2022.909448/full

[57] R. Eki, S. Yamada, H. Ozawa, H. Kai, K. Okuike, H. Gowtham, H. Nakanishi, E. Almog, Y. Livne, G. Yuval, E. Zyss, and T. Izawa, " 9.6 A 1/2.3inch 12.3Mpixel with On-Chip 4.97TOPS/W CNN Processor Back-Illuminated Stacked CMOS Image Sensor," in 2021 IEEE International Solid- State Circuits Conference (ISSCC). San Francisco, CA, USA: IEEE, Feb. 2021, pp. 154-156. [Online]. Available: https://ieeexplore.ieee.org/document/9365965/

[58] B. Rueckauer, C. Bybee, R. Goettsche, Y. Singh, J. Mishra, and A. Wild, "NxTF: An API and Compiler for Deep Spiking Neural Networks on Intel Loihi," ACM Journal on Emerging Technologies in Computing Systems, vol. 18, no. 3, pp. 1-22, Jul. 2022. [Online]. Available: https://dl.acm.org/doi/10.1145/3501770

[59] J. Zhang, M. Liang, J. Wei, S. Wei, and H. Chen, "A 28nm Configurable Asynchronous SNN Accelerator with Energy-Efficient Learning," in 2021 27th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC). Beijing, China: IEEE, Sep. 2021, pp. 34-39. [Online]. Available: https://ieeexplore.ieee.org/document/9565446/

[60] G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, "Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades," Frontiers in Neuroscience, vol. 9, Nov. 2015. [Online]. Available: http://journal.frontiersin.org/Article/10.3389/fnins.2015.00437/abstract

[61] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998. [Online]. Available: http://ieeexplore.ieee.org/document/726791/

[62] S. Sheik, Q. Liu, F. C. Bauer, G. Lenz, M. Sorbaro, V. Leite, N. Küpelioğlu, W. Soares Girao, M. Khoei, Y. Hu, Y. Xing, and M. Bortone, "Sinabs (Sinabs Is Not A Brain Simulator)," Jan. 2019. [Online]. Available: https://github.com/synsense/sinabs

[63] D. Neil, M. Pfeiffer, and S.-C. Liu, "Learning to be efficient: algorithms for training low-latency, low-compute deep spiking neural networks," in Proceedings of the 31st Annual ACM Symposium on Applied Computing. Pisa Italy: ACM, Apr. 2016, pp. 293-298. [Online]. Available: https://dl.acm.org/doi/10.1145/2851613.2851724

[64] B. Rueckauer, I.-A. Lungu, Y. Hu, M. Pfeiffer, and S.-C. Liu, "Conversion of Continuous-Valued Deep Networks to Efficient EventDriven Networks for Image Classification," Frontiers in Neuroscience, vol. 11, p. 682, Dec. 2017. [Online]. Available: http://journal.frontiersin org/article/10.3389/fnins.2017.00682/full

[65] M. Yao, O. Richter, G. Zhao, N. Qiao, Y. Xing, D. Wang, T. Hu, W. Fang, T. Demirci, M. De Marchi, L. Deng, T. Yan, C. Nielsen, S. Sheik, C. Wu, Y. Tian, B. Xu, and G. Li, "Spike-based dynamic
computing with asynchronous sensing-computing neuromorphic chip," Nature Communications, vol. 15, no. 1, p. 4464, May 2024. [Online]. Available: https://www.nature.com/articles/s41467-024-47811-6

[66] G. Lenz, "Speck Demo Videos: Towards training robust computer vision models for neuromorphic hardware - Gregor Lenz @ Open Neuromorphic TU Delft," Mar. 2023. [Online]. Available: https://youtu.be/TPChp-O6qXM?t=3874

[67] S. Sheik, "Speck Examples @ TinyML: tinyML Neuromorphic Engineering Forum - Systems Session," Oct. 2022. [Online]. Available: https://youtu.be/IQQmcEFxzo8?t=202

[68] C. Nielsen and etal., "Samna." [Online]. Available: https://pypi.org/ project/samnal


[^0]:    The Authors want to thank Giacomo Indiveri, Dylan Muir, and Kynan Eng for the creative process of conceiving the design concept.

    The Authors want to thank Nicoletta Risi, Madison Cotteret and Hugh Greatorex for their comments, support and advice on the manuscript. Ole Richter, for the time of the manuscript creation, would like to acknowledge the financial support of the CogniGron research center and the Ubbo Emmius Funds (Univ. of Groningen), during the design and testing Ole Richter was solely affiliated to SynSense AG.

    Affiliations:

    ${ }^{1}$ SynSense AG, Thurgauerstrasse 60, 8050 Zurich, Swizerland

    ${ }^{2}$ SynSense, No. 1577, Tianfu Avenue, Chegdu, Sichuan, PR China

    3 Bio-Inspired Circuits and Systems (BICS) Lab, Zernike Institute for Advanced Materials, University of Groningen, Netherlands.

    ${ }^{4}$ Groningen Cognitive Systems and Materials Center (CogniGron), University of Groningen, Netherlands.

    * For inquiries about the publication: o.j.richter@ rug.nl, for inquiries about Speck, Samna and Sinabs: sales@synsense.ai and media@ synsense.ai

