# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models 

Chengming $\mathbf{X u}^{1} \quad$ Kai Hu $^{2} \quad$ Donghao Luo $^{1} \quad$ Jiangning Zhang $^{1} \quad \mathbf{W e i ~}^{\mathbf{L i}^{3}}$<br>Yanhao Ge ${ }^{3}$ Chengjie Wang ${ }^{1}$<br>${ }^{1}$ Youtu Lab, Tencent ${ }^{2}$ Carnegie Mellon University ${ }^{3}$ VIVO


#### Abstract

Stylized Text-to-Image Generation (STIG) aims to generate images based on text prompts and style reference images. We in this paper propose a novel framework dubbed StyleMaster for this task by leveraging pretrained Stable Diffusion (SD), which addresses previous problems such as insufficient style and inconsistent semantics. The enhancement lies in two novel modules, namely multi-source style embedder and dynamic attention adapter. In order to provide SD with better style embeddings, we propose the multi-source style embedder, which considers both global and local level visual information along with textual information, which provide both complementary style-related and semantic-related knowledge. Additionally, aiming for better balance between the adapter capacity and semantic control, the proposed dynamic attention adapter is applied to the diffusion UNet in which adaptation weights are dynamically calculated based on the style embeddings. Two objective functions are introduced to optimize the model together with denoising loss, which can further enhance semantic and style consistency. Extensive experiments demonstrate the superiority of StyleMaster over existing methods, rendering images with variable target styles while successfully maintaining the semantic information from the text prompts.


## 1 Introduction

Among all image generation problems, Stylized Image Generation (SIG), in which images with target styles are required to be generated, possesses significant scholarly merit. SIG methods can be smoothly applied to extensive regions such as artistic creation and movie editing, well facilitating real-life cases. Recently, with the surge in diffusion-based generative models [9, 17], the research focus has been transferred from traditional style transfer to the Stylized Text-to-Image Generation (STIG). In this task, one or several style reference images are given as condition, and various images are generated based on the style condition along with additional information such as text prompts. This task, while being more challenging due to mixed input condition, is more flexible and applicable, thus being of higher practical value.

As a typical STIG method, StyleAdapter [21] is proposed based upon pretrained Stable Diffusion (SD). The reference images are processed with a style embedding module and then injected into the diffusion UNet through extra cross-attention modules, which guide the model to denoise the features according to style information, resulting in stylized images. While such a framework shows promising results for handling specific styles and text prompts, we find that StyleAdapter still suffers from problems such as (1) insufficient style, in which the generated images do not fully share the same style with reference images, for complex styles, and (2) inconsistent semantics, in which semantic objects are leaked from reference images, thus affecting the semantic consistency between the generated images and the text prompts. These problems can be attributed to the design of StyleAdapter. Specifically, StyleAdapter adopts CLIP [16] as source of style information. Such patch-level representation focuses more on local patterns, while ignoring global patterns such as
![](https://cdn.mathpix.com/cropped/2024_06_04_a3ea7c26486ba64c0474g-02.jpg?height=586&width=1248&top_left_y=236&top_left_x=430)

Figure 1: Left: With our proposed StyleMaster, the diffusion-based stylized image generation process can avoid issues such as inconsistent semantics and insufficient style, leading to better generation quality. Right: Our proposed method can generated consistent and delicate stylized images with different styles. For simplicity we only present the generated stylized images here. For the style reference images please refer to the supplementary material.

which are also important style descriptors. On the other hand, the adaptation modules in StyleAdapter are applied to all attention layers. In this way, each aggregation with text prompts would be biased by the semantic conuterpart contained in the style embedding, thus resulting in inconsistent semantic.

To this end, we in this paper propose a novel framework for STIG called StyleMaster with advanced extraction and injection of style information. Our method inherits the basic pipeline of StyleAdapter, while adopting a specific design for style embedding extraction and injection respectively. For style embedding extraction, we propose a multi-source structure to compensate for the patch-level CLIP information. Specifically, we not only use the commonly-used CLIP-based patch embedding to represent image information, but also adopt global-level VGG descriptor, which is engaged in the attention process via adaptive scaling and shifting. Apart from that, we make use of the semantic knowledge of reference images contained in their image captions. This highly abstract semantic information is eliminated from style embedding through the proposed negative embedding branch. By actively merging these different latent spaces, the processed style embedding can thus contain more comprehensive knowledge about the target styles, while avoiding memorizing the semantic content. For style embedding injection, since fully adapting UNet features based on style embeddings can distort the semantic information, a simple solution would be shrinking adapter attachment only to part of UNet, e.g. its upsampling layers. However, direct shrinkage can hurt the capacity of adapters, which in turn worsens the insufficient style problem. To solve this and enhance the module capacity for thorough stylization, we propose the dynamic attention adapter, in which dynamic weights are generated from the style embeddings and then used to adapt both the self-attention and cross-attention layers in the diffusion UNet.

Moreover, we further enhance the model with objectives other than the commonly-used noise prediction loss for diffusion models. We introduce a gram consistency loss, in which we augment the reference images with two sets of transformations, resulting in one set with original style and another set with a disrupted style. Then gram matrices of estimated denoised result and these transformed reference images are calculated as their style-aware statistics. By calculating a triplet loss among them, the model is encourage to generate images with more robust and consistent styles when processing different reference images. Additionally, a semantic disentangle loss is utilized to alleviate the inconsistent semantics problem by contrasting the style embeddings against reference text embeddings, while keeping them similar to reference image embeddings.

To show the effectiveness of our proposed method, we conduct extensive experiments among various styles containing both one-shot and mult-shot settings. We show that our method can significantly outperform baseline methods including StyleAdapter, generating correct styles and avoiding inconsistent semantics. In summary, the contributions of this work are as follows:

- We propose a novel method for reference-based stylized image generation named StyleMaster, boosting Stylized Text-to-Image Generation (STIG) with a multi-source style embedder
and a dynamic attention adapter, greatly solving problems including insufficient style and inconsistent semantics.
- Two objective functions are introduced to enhance our model, including a gram stylization loss and a semantic disentangle loss, which encourage the model to decouple the style related information from reference images with other style unrelated information.
- Extensive experiment results show that our method consistently work well in both text-toimage and image-to-image tasks with different reference images, showing the practical value of such a method.


## 2 Related work

Text-to-image diffusion models. Diffusion models have been proven to be a powerful family of generative models. DDPM [9] originated to propose the framework by modeling the mapping between Gaussian distribution and image distribution with the forward diffusion and inverse denoising process. Based on that Latent Diffusion Model (LDM) [17] largely improved the practical usage by leveraging diffusion model to latent space instead of pixel space, which leads to commonly-known text-to-image diffusion models such as Stable Diffusion (SD), Midjourney and DALLE-3 [1]. Other works focus on improve the diffusion model structure. For example, DiT [14], MDT [6] and PIXART- $\alpha$ [3] utilize the transformer instead of UNet structure, which can be better scaled to larger model size. Blattmann et. al. [2] and Zhang et. al. [25] leverage ideas of Retrieval Augmented Generation (RAG) to generate images based on other retrived images which provide extra knowledge. Yang et. al. [22] propose to leverage the LLMs for planning the text-to-image problems. Our work is built on pretrained SD models, in which the attention mechanism merges conditional information from text prompts to images. Different from the previous works, we focus on designing extra attention adaptation so that the knowledge contained in the style reference images can be smoothly embedded into the denoising process, leading to stylized images.

Stylized image generation. Among all conditional image generation tasks, stylized image generation has long been a highlighted one. Most previous works focus on style transfer, i.e.transfer the style of a content image given another style refernce image. For example, Gatys et. al. [7] solved this problem by optimizing the style-related statistics. Li et. al. [12] equipped this method with explanability. MicroAST [20] proposed to speed up such framework by abandoning the complex visual encoder and utilizing a dual-modulation strategy. Yang et. al. [23] leverages diffusion models, in which the style-aware guidance is used to generate the wanted style. InST [26] realized style transfer by inverting the content image to noise and then re-generate it with the condition control of style images. Apart from these style transfer methods, StyleAdapter [21] proposes a new framework which can generate images directly from style reference images and text prompts without content images. Our work mainly follows StyleAdapter to present a generalized stylization method. Different from StyleAdapter, we analyze the role of style reference images and text prompts in the generation process. Based on that, we propose a novel module to extract more representative style embeddings, which are then injected into noise space with our proposed dynamic adapter.

## 3 Preliminary: Stable Diffusion

Diffusion models are probabilistic models that learn a data distribution $p_{\theta}\left(\mathbf{x}_{0}\right)$ by progressively denoising a standard Gaussian distribution, which can be computed by iteratively adding Gaussian noise to the clean data $\mathbf{x}_{0}$. Stable Diffusion (SD) extends such a model to text-to-image based on text prompt $p$. With pre-trained VQ-VAE [19] containing encoder $\mathcal{E}$ and decoder $\mathcal{D}$, SD allows the model to focus more on the semantic information of data and improves efficiency. A diffusion UNet is used to predict the noise, in which attention mechanism is adopted. Specifically, for the $l$-th layer, selfattention is first used to interact among spatial features: $z^{l}=$ Attention $\left(W_{Q}^{l} \cdot z^{l}, W_{K}^{l} \cdot z^{l}, W_{V}^{l} \cdot z^{l}\right)$, where Attention denotes the attention operator, $z^{l}$ denotes latent embeddings of the $l$-th layer, $W_{Q}, W_{K}, W_{V}$ denotes the projection layers of self-attention. After that the cross-attention is utilized to merge condition information such as text prompt: $\hat{z}^{l}=\operatorname{Attention}\left(\hat{W}_{Q_{t}}^{l} \cdot z^{l}, \hat{W}_{K_{t}}^{l} \cdot z_{\text {text }}, \hat{W}_{V_{t}}^{l}\right.$. $z_{\text {text }}$ ), where $z_{\text {text }}$ denotes text prompt embedding, $\hat{W}_{Q}, \hat{W}_{K}, \hat{W}_{V}$ denotes the projection layers of

![](https://cdn.mathpix.com/cropped/2024_06_04_a3ea7c26486ba64c0474g-04.jpg?height=566&width=1374&top_left_y=232&top_left_x=381)

Figure 2: Overview of our proposed StyleMaster. Concretely, the multi-source style embedder (Sec. 4.1) aggregates style patterns contained in each reference image simultaneously from global and local levels. Style embeddings are then engaged in the denoising procedure through the proposed dynamic attention adaptation (Sec. 4.2), which guides both the attentions in diffusion UNet to properly merge style and semantic information from different sources. During training augmented input images are used as style reference images, through which objectives as described in Sec. 4.3 are used to optimize the model. During inference the style reference images are achieves with manual assignment instead of using augmentation of a specific image.

![](https://cdn.mathpix.com/cropped/2024_06_04_a3ea7c26486ba64c0474g-04.jpg?height=374&width=1371&top_left_y=1149&top_left_x=385)

Figure 3: Left: multi-source style embedder. Right: dynamic attention adapter.

cross-attention. The training objective of SD is as follows:

$$
\begin{equation*}
\mathcal{L}_{\text {noise }}=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_{\theta}\left(z^{t}, t\right)\right\|_{2}^{2}\right] \tag{1}
\end{equation*}
$$

where $t$ is uniformly sampled from $\{0, \ldots, T\}, z^{t}$ denotes noisy latent at $t$-th timestep.

## 4 Methodology

We focus on reference-based Stylized Text-to-Image Generation (STIG) in this paper. Formally, a reference style image set $\mathcal{I}_{s}=\left\{\mathbf{I}_{\text {style }}^{i}\right\}_{i=1}^{N_{s}}$, where $N_{s}$ denotes the number of reference images, together with text prompt $p$ are given as condition information. $N_{s}$ can be variable among different trials to describe different style concepts. The model is required to generate image $\mathbf{I}$ that shares the same style pattern with $\mathcal{I}_{s}$ and same semantic meaning with text prompt $p$. To solve this task we present a novel framework named StyleMaster based on SD, as shown in Fig. 2, which will be introduced in this section.

### 4.1 Multi-source style embedder

Given the successful application of text-to-image based on SD, an ideal reference-based stylization should necessarily depend on a representative style embeddings. StyleAdapter [21] adopts CLIP visual encoder [16] to extract the patch-level features from style reference images, which are then processed with cross-attentions with learnable style tokens. However, StyleAdapter still suffers from problems like insufficient style and inconsistent semantics, i.e.the semantic knowledge in reference
images are generated during the denoising procedure. The former problem can be attributed to that CLIP-based patch embeddings concentrate more on local patterns. In this way, the style embeddings fail to hold the global style patterns, leading to insufficient style. Meanwhile, since CLIP image embeddings are aligned with their captions, they inevitably contain information about contents in the images. Engaging such information in style embeddings can lead to inconsistent semantics.

To solve these problems, we propose to leverage knowledge from multiple sources to complement the style embedding. Specifically, given a style image set $\mathcal{I}_{s}$, we extract three different kinds of features with pretrained models: (1) local style-aware features: Following StyleAdapter, we utilize CLIP to encode each style image $\mathbf{I}_{\text {style }}^{i}$ into latent patch tokens $z_{C L I P}^{i}$, which are then concatenated together along the token dimension, resulted as $z_{C L I P}$. (2) global style-aware features: Inspired by previous style transfer methods [7], we adopt the gram matrix to represent global-level style information. Specifically, $\mathbf{I}_{\text {style }}^{i}$ is processed with VGG-19 [18] to get the relu3_1 feature $z_{v g g}^{i}$, using which the gram matrix can be calculated as $z_{g r a m}^{i}=z_{v g g}^{i}{ }^{T} \cdot z_{v g g}^{i}$. Then gram matrices for different style images are averaged to get the final representation $z_{\text {gram }}$. (3) semantic-aware features: We adopt CLIP text encoder to extract the text embedding $z_{\text {cap }}^{i}$ of captions of $\mathbf{I}_{\text {style }}^{i}$, which is then concatenated into $z_{\text {cap }}$. During training, the captions are provided in the training set, while during inference, we adopt BLIP [11] to first annotate the style images.

Generally, each token in $z_{C L I P}$ contains local-level style features, while $z_{\text {gram }}$ describes more abstract style information. On the other hand, $z_{\text {cap }}$ contains the semantic information of $\mathcal{I}_{s}$, which should be eliminated in the final style embedding to avoid inconsistent semantics. To properly make use of these embeddings, we propose a novel dual-branch structure as shown in Fig. 3 (a). Concretely, several learnable style tokens $z_{s}$ are first attached to $z_{C L I P}$, with their replication $z_{s}^{N}$ denoted as negative semantic tokens attached to $z_{\text {cap }}$ :

$$
\begin{equation*}
\hat{z}_{C L I P}=z_{C L I P}\left\|_{t}\left(z_{s}+\delta_{t}\right), \quad \hat{z}_{\text {caption }}=z_{\text {cap }}\right\|_{t} z_{s}^{N} \tag{2}
\end{equation*}
$$

where $\cdot \|_{t}$. denotes concatenation along the token dimension, $\delta_{t}$ is the same time embedding of denoising timestep as used in diffusion UNet. $\hat{z}_{\text {caption }}$ is then individually processed with several transformer layers to aggregate the information between style tokens and text embedding. For $\hat{z}_{C L I P}$, we adopt a modified version of transformer layer. Specifically, $z_{\text {gram }}$ is first projected to scaling and shift coefficients. These coefficients are applied to the self-attention procedure in the same way as adaLN [15]. Before processing the attention result with FFN, the style token part in $\hat{z}_{\text {caption }}$ is subtracted from the counterpart in $\hat{z}_{C L I P}$.

Such design enjoys three major merits. First, the module is timestep-aware. Since the style tokens are merged with time embedding, different denoising steps can thus model different information. Second, the global information in $z_{\text {gram }}$ can guide the model to better concentrate on style-related knowledge, thus avoiding those irrelevant but repetitive local patterns shared among style images. Third, the negative semantic tokens generally contain more abstract content information rather than style. Consequently, subtracting it from $\hat{z}_{C L I P}$ can help alleviate the problem of inconsistent semantics. While some captions may describe the style of images, the model can learn to maintain these knowledge during training thanks to the training strategy described in Sec. 4.3. By using the proposed module, we can learn more representative and generalizable style embeddings which can better facilitate the stylization process described as follow.

### 4.2 Dynamic attention adaptation

The extracted style embedding $z_{s}$ from $\mathcal{I}_{s}$ as described above can then be used to adapt the pretrained SD to guide the denoising process based on style information. Thanks to the design of self-attention and cross-attention mechanism in diffusion UNet, such adaptation can be simply instantiated as an extra cross-attention module that is parallel to the original prompt-based cross-attention, as adopted in StyleAdapter. However, we empirically find that such a method is suboptimal in a large amount of cases, leading to severe semantic inconsistency. A straightforward solution is cut down the number of extra attention modules so that only upsample layers in diffusion UNet are adapted. In this way, the text prompt can dominate the cross-attention in half of the UNet, consequently resulting in better semantics in the generated images. However, this can decrease the capacity of adapters, leading to less preferable stylization. To this end, we propose to adopt a dynamic adaptation strategy which is applied to both self-attention and cross-attention (Fig. 3.b)).

Dynamic self-attention adapter. As discussed in previous works [8], the projected value tensor $W_{V}^{l} \cdot z^{l}$ in the self-attentions contributes to the texture of generated images. Therefore we introduce a dynamic self-attention adapter module based on adaIN. Formally, for the $l$-th self-attention layer, we project $z_{s}$ with a linear layer and adjust $W_{V}^{l} \cdot z^{l}$ according to statistics of $z_{s}$ :

$$
\begin{equation*}
\hat{\mathbf{V}}^{l}=\mu\left(f_{p r o j-S A}^{l}\left(z_{s}\right)\right)+\frac{\sigma\left(f_{p r o j-S A}^{l}\left(z_{s}\right)\right)}{\sigma\left(W_{V}^{l} \cdot z^{l}\right)} *\left(W_{V}^{l} \cdot z^{l}-\mu\left(W_{V}^{l} \cdot z^{l}\right)\right) \tag{3}
\end{equation*}
$$

where $f_{\text {proj-SA }}^{l}$ denotes dynamic projection layer, $\mu, \sigma$ denote mean and standard deviation. By rescaling $\mathbf{V}^{l}$, the information contained in $z_{s}$ can be directly embedded into the image feature without destroying the structure and semantic meaning of the generated image.

Dynamic cross-attention adapter. To adapt the cross-attention layers, we follow the idea of StyleAdapter to adopt the dual-path cross-attention mechanism. Basically, for the $l$-th crossattention layer, besides the original cross-attention performed between text embedding $z_{\text {text }}$ and image embedding $z^{l}$ as in Sec. 3, an extra style-aware cross-attention is added as $\tilde{z}^{l}=$ Attention $\left(\hat{W}_{Q_{t}}^{l} \cdot z^{l}, \hat{W}_{K_{s}}^{l} \cdot z_{s}, \hat{W}_{V_{s}}^{l} \cdot z_{s}\right)$ with additional learnable parameters $\hat{W}_{K_{s}}^{l}, \hat{W}_{V_{s}}^{l}$. Then $\hat{z}^{l}+\lambda \tilde{z}^{l}$ is fed into the following feed-forward networks, where $\lambda$ is learnable coefficient.

To enhance the capacity, we further propose a dynamic cross-attention adapter. Specifically, we first project the statistics of $z_{s}$ to a layer specific latent space:

$$
\begin{equation*}
z_{s}^{l}=f_{p r o j-C A}^{l}\left(\mu\left(z_{s}\right) \|_{c} \sigma\left(z_{s}\right)\right) \tag{4}
\end{equation*}
$$

where $f_{\text {proj-CA }}$ denotes a linear projection layer, $\cdot \|_{c}$. denotes concatenation along the channel dimension. Then two weight generators instantiated as linear layers are applied to $z_{s}^{l}$, resulted in two dynamic weights $W_{K_{d}}^{l}, W_{V_{d}}^{l}$ with dimension $d * d^{l}$. These two features are reshaped into linear layer weights and used to transform $z_{s}$. After that the style-aware cross-attention is modified as

$$
\begin{equation*}
\tilde{z}^{l}=\operatorname{Attention}\left(\hat{W}_{Q_{t}}^{l} \cdot z^{l},\left(\hat{W}_{K_{s}}^{l}+W_{K_{d}}^{l}\right) \cdot z_{s},\left(\hat{W}_{V_{s}}^{l}+W_{V_{d}}\right) \cdot z_{s}\right) \tag{5}
\end{equation*}
$$

In this way, the key and value projections are partially dependent on $z_{s}$, resulting in a more complex transformation of $z_{s}$ and leading to better capacity. To make this module parameter-efficient, we adopt a grouping strategy, i.e.channels of $z_{s}$ in each group share the same dynamic weight to produce $W_{K_{d}}^{l}$ and $W_{V_{d}}^{l}$, hence lighter weight generators can be used to generate dynamic weights.

### 4.3 Training objectives

To train the model such that it can generate images that are conforms to both the style information from style reference images and semantic information from text prompts, we introduce a mixed training objectives including three terms as follows.

$$
\begin{equation*}
\mathcal{L}=\mathcal{L}_{\text {noise }}+\mathcal{L}_{\text {disen }}+\mathcal{L}_{\text {style }} \tag{6}
\end{equation*}
$$

where $\mathcal{L}_{\text {noise }}$ is the noise prediction loss as in Eq. 1. $\mathcal{L}_{\text {disen }}$ denotes a semantic disentangle loss applied to style embedding $z_{s}$. To ensure that the proposed style embedding model can get rid of the semantic information contained in $\mathbf{I}_{\text {style }}$ when producing $z_{s}, \mathcal{L}_{\text {disen }}$ is designed by enlarging the similarity between $z_{s}$ and $z_{C L I P}$ while decreasing the similarity between $z_{s}$ and text embedding $z_{\text {text }}$. As discussed in Sec. 4.1, the text embedding represents more abstract semantic information than the image embedding. Therefore this loss term can help the model avoid the possibility of semantic leakage, thus leading to better style embedding. On the other hand, to enhance the style consistency, we propose to regulate the gram matrix of $\hat{x}^{0}$, which is the noisy estimation from $z^{t}$ and can be calculated as

$$
\begin{equation*}
\hat{z}^{0}=\frac{z^{t}-\sqrt{1-\bar{\alpha}^{t}} \epsilon^{t}}{\sqrt{\bar{\alpha}^{t}}}, \quad \hat{x}^{0}=\mathcal{D}\left(\hat{z}^{0}\right) \tag{7}
\end{equation*}
$$

Specifically, we apply several rigid transformations such as random rotation and cropping to $\mathcal{I}_{S}$ to get a new image $\mathbf{I}_{\text {pos }}$ that has the same style as $\hat{x}_{0}$. Then elastic transformation and color jitter are also applied to $\mathcal{I}_{S}$. The resulted $\mathbf{I}_{n e g}$, while sharing similar semantic object to $\mathbf{I}_{i n p}$, barely inherits the style from it. Then the objective is a triplet loss which can be written as

$$
\begin{align*}
\delta_{p} & =\sum\left|\mathcal{G}\left(\phi_{v g g}\left(\hat{x}_{0}\right)\right)-\mathcal{G}\left(\phi_{v g g}\left(\mathbf{I}_{\text {pos }}\right)\right)\right|  \tag{8}\\
\delta_{n} & =\sum\left|\mathcal{G}\left(\phi_{v g g}\left(\hat{x}_{0}\right)\right)-\mathcal{G}\left(\phi_{v g g}\left(\mathbf{I}_{\text {neg }}\right)\right)\right|  \tag{9}\\
\mathcal{L}_{\text {style }} & =\max \left\{\delta_{p}-\delta_{n}+0.1,0\right\} \tag{10}
\end{align*}
$$

where $\mathcal{G}$ denotes the gram matrix of features. By optimizing this loss term, the model is encouraged to learn more detailed style information, thus leading to better results.

## 5 Experiments

### 5.1 Implementation d etail

Dataset. We follow StyleAdapter to adopt LAION-Aesthetic 6.5+ as the training set, which contains about $600 \mathrm{k}$ images. For each input image during training, we use its augmented variants as the style reference images. For evaluation we adopt 50 prompts used in StyleAdapter, and select 20 styles covering color, texture and global layout. More details are presented in the supplementary material.

Experiment setting. Our experiments cover both one-shot and multi-shot settings. To make the evaluation more challenging, the shot number, i.e.number of reference images, varies from 2 to 5 among different styles in the multi-shot setting. We use all 20 styles for multi-shot experiments and 10 of them for one-shot experiments.

Training details. We adopt AdamW as optimizer with 1e-5 learning rate. Our model is trained for 200,000 iterations on 8 V100s with 8 batch size on each gpu.

Competitor. We include extensive methods as our competitor. For 1-shot experiment, MicroAST [20], StyleAlign [8], StyTR ${ }^{2}$ [4], and StyleAdapter [21] are adopted. For multi-shot experiment, InST [26], LoRA [10], Textual Inversion (TI) [5] and StyleAdapter are adopted.

### 5.2 Quantitative results

Table 1: Quantitative results for multi-shot setting. For all metrics, the larger score denotes the better model.

| 1-shot Methods | Text Sim $\uparrow$ | StyleSim $\uparrow$ |
| :---: | :---: | :---: |
| MicroAST [20] | 0.299 | 0.529 |
| StyleAlign $[8]$ | 0.276 | 0.645 |
| StyTR $^{2}[4]$ | 0.298 | 0.541 |
| StyleAdapter [21] | 0.282 | 0.668 |
| Ours | $\mathbf{0 . 2 9 9}$ | $\mathbf{0 . 7 0 8}$ |

Table 2: Quantitative results for multi-shot setting. For all metrics, the larger score denotes the better model.

| multi-shot Methods | Text Sim $\uparrow$ | StyleSim $\uparrow$ |
| :---: | :---: | :---: |
| InST [26] | 0.196 | 0.692 |
| LoRA [10] | 0.237 | 0.665 |
| TI [5] | 0.268 | 0.678 |
| StyleAdapter [21] | 0.286 | 0.682 |
| Ours | $\mathbf{0 . 2 9 1}$ | $\mathbf{0 . 7 1 9}$ |

Objective quantitative results. For quantitative evaluation we adopt CLIP to calculate the style similarity between generated images and target style reference images, and the semantic similarity between generated images and target text prompts. The results are presented in Tab. 1 and Tab. 2 for 1-shot and multi-shot respectively. Specifically, MicroAST and StyTR ${ }^{2}$ share similar text similarity to ours one-shot experiments. This is because we use pretrained SD to generate base images for them, thus the basic semantic meaning is contained in the image. However the gap in terms of style similarity is much more marginal. Our method outperforms the best baseline StyleAdapter by 0.04 . The results for multi-shot setting is consistent with one-shot, in which our method is significantly better then the competitors, thus showing the superiority of the proposed method.

Table 3: User Study for multi-shot setting. For all metrics, the larger score denotes the better model.

| 1-shot Methods | $\overline{\text { Text Sim }}$ | $\overline{\text { StyleSim }}$ |
| :---: | :---: | :---: |
| MicroAST | 3.18 | 2.84 |
| StyleAlign | 2.97 | 2.83 |
| StyTR $^{2}$ | 3.85 | 3.25 |
| StyleAdapter | 3.16 | 3.30 |
| Ours | 3.20 | 3.80 |

Table 4: User Study for multi-shot setting. For all metrics, the larger score denotes the better model.

| multi-shot Methods | Text Sim | StyleSim |
| :---: | :---: | :---: |
| InST | 2.11 | 2.68 |
| LoRA | 2.84 | 3.34 |
| TI | 2.95 | 3.03 |
| StyleAdapter | 3.12 | 3.78 |
| Ours | $\mathbf{3 . 1 4}$ | $\mathbf{3 . 8 6}$ |

Subjective quantitative results. Apart from the quantitative results in the main paper, we randomly sample 5 images for each method and each style and conduct a user study. During the study, the
participant are required to score 1-5 for each image by considering both semantic fidelity and style consistency. Then all scores are averaged. The results are shown in Tab. 3 and Tab. 4 , which are generally consistent with the objective results except that the text similarity of StyTR ${ }^{2}$ is extremely high with human scores. We think this may be attributed to the fact that our method still lacks behind the SD model used to generated base images for StyTR ${ }^{2}$ in terms of semantic fidelity. On the other hand, while StyTR ${ }^{2}$ receives good semantic fidelity, its style consistency is poor. Our method, on the contrary, can find a good balance for the trade off between semantic and style.

### 5.3 Qualitative results

![](https://cdn.mathpix.com/cropped/2024_06_04_a3ea7c26486ba64c0474g-08.jpg?height=401&width=1260&top_left_y=659&top_left_x=430)

Figure 4: One-shot qualitative comparison. Styles from top to bottom: Cezanne, flat cartoon, expressionism, ink. Prompts from left to right: A bird in a word; A boy wearing glasses, he is reading a thick book; A cherry blossom. For detailed reference images please refer to the appendix.

![](https://cdn.mathpix.com/cropped/2024_06_04_a3ea7c26486ba64c0474g-08.jpg?height=391&width=1241&top_left_y=1271&top_left_x=434)

Figure 5: Multi-shot qualitative comparison. Styles from top to bottom: pencil, watercolor, Monet, impasto. Prompts from left to right: A robot; A modern house with a pool; A lake with calm water and reflections. For detailed reference images please refer to the appendix.

We present several uncurated results for both settings in Fig. 4 and Fig. 5. For one-shot experiment, since we use pretrained SD to first generate the content images, the style transfer based methods such as StyTR ${ }^{2}$ and MicroAST can generally enjoy reasonable semantic consistency. However, such methods can only inherit the basic color information from reference images rather than the detailed style information such as shape, texture and layout, thus making them less preferable. For example, they fail to present the cubism and the curves in the second and third row. This is because these methods rely on simple representation to transfer the style-related knowledge from reference images, which leads to the problem of under-stylization. StyleAlign, on the other hand, can stylize the images better. For example the images generated by StyleAlign in the fourth row share similar style patterns with the reference image. However, we find that StyleAlign suffers from inconsistent semantics problem, which leads to inconsistent semantic meaning with the text prompt. Moreover, images generated by StyleAlign seems messy and disordered, which may be attributed to the uncontrolled shared self-attention between content and style images. The performance of StyleAdapter is much better than other competitors, while it is hard for this method to understand complex style patterns, leading to undesirable results when it comes to ink painting (fourth row). Compared with these methods, our method can learn appropriate style information from reference images, e.g. the scattered color patches in the first and fourth row, and simultaneously keep the images faithful to the prompts, thus making the best of both worlds.

![](https://cdn.mathpix.com/cropped/2024_06_04_a3ea7c26486ba64c0474g-09.jpg?height=491&width=1315&top_left_y=237&top_left_x=405)

Figure 6: Images generated by different model variants.

The multi-shot setting which is more challenging shows similar results. InST can hardly replicate the style. LoRA and TI suffer from limited style information. StyleAdapter, while utilizing a specifically-designed pipeline, shows a tendency to confuse the given styles with the photographic prior knowledge from pretrained SD. Such phenomenon is most obvious in the first row of Fig. 5 . where pencil drawings are provided as reference images, but StyleAdapter generates grayscale photos. Our method, thanks to the proposed multi-source style embedder which can extract more detailed style information and the dynamic attention adaptation, can generally generate different kinds of styles with high image quality and semantic fidelity.

### 5.4 Ablation study

To further verify the efficacy of our contributions, we conduct several ablation studies on multi-shot setting. More quantitative and qualitative ablation studies are provided in the appendix.

Design of style embedding module. We consider two variants together with the full model for the style embedding module: not using the VGG gram matrix to regulate the attention layers (w/o VGGToken), and not engaging the reference image captions in the extraction (w/o NegEmb). The results are shown in the left three columns of Fig. 6 The style of images generated by model without VGGToken is generally less mimic. Meanwhile, the model without NegEmb not only has worse style (first and second row) but also suffers from wrong semantic meaning (third row).

Design of attention adapter. We illustrate the role of different parts in the proposed dynamic attention adapter in the middle three columns of Fig. 6. When the model adopts attention adapter for all UNet attention layers instead of only the upsample ones, the images generally have problem of wrong semantic meaning. For example, the clock is missing in the first row, and the cloth color is wrong in the third row. Also, it is obvious that when only using the same cross-attention adapter as in StyleAdapter, the generated images show inconsistent and undesirable styles, which can be attributed to the limited capacity to such strategy.

Effectiveness of different objective functions. In the right three columns of Fig. 6 we inspect the efficacy of two objectives introduced in Sec. 4.3. The results directly support our claim that the gram consistency loss can enhance the style in generated images and the semantic disentangle loss can make the model tell apart semantic and style information from reference images, thus better handling contents in the text prompts.

## 6 Conclusion

We try to solve Stylized Text-to-Image Generation in this paper. A novel model is proposed to solve the problems of insufficient style and inconsistent semantics which are suffered by previous methods such as StyleAdapter. The improvement mainly comes from the multi-source style embedder, in which multiple sources and used to achieve comprehensive style embeddings and eliminate the semantic information from style reference images, and the dynamic attention adapter, in which style embeddings dynamically interact with attention layers in diffusion UNet. Extensive experiments have been conducted to show the efficacy of our proposed method as a powerful stylization method, which can be widely applied to real-life scenarios.

## References

[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.

[2] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval-augmented diffusion models. Advances in Neural Information Processing Systems, 35:15309-15324, 2022.

[3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.

[4] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11326-11336, 2022.

[5] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.

[6] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164-23173, 2023 .

[7] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2414-2423, 2016 .

[8] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133, 2023.

[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020 .

[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[11] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888-12900. PMLR, 2022.

[12] Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural style transfer. arXiv preprint arXiv:1701.01036, 2017

[13] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038-6047, 2023.

[14] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195-4205, 2023.

[15] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32,2018 .

[16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.

[17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.

[18] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.

[19] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.

[20] Zhizhong Wang, Lei Zhao, Zhiwen Zuo, Ailin Li, Haibo Chen, Wei Xing, and Dongming Lu. Microast: Towards super-fast ultra-resolution arbitrary style transfer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2742-2750, 2023.

[21] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, and Ping Luo. Styleadapter: A unified stylized image generation model without test-time fine-tuning.

[22] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708, 2024 .

[23] Serin Yang, Hyunmin Hwang, and Jong Chul Ye. Zero-shot contrastive loss for text-guided diffusion image style transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22873-22882, 2023.

[24] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836-3847, 2023.

[25] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 364-373, 2023.

[26] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10146-10156, 2023.
