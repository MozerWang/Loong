# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model 

Zezheng Song* Jiaxin Yuan*<br>Department of Mathematics<br>University of Maryland College Park<br>College Park, MD, USA<br>\{zsong001, jyuan98\}@umd.edu

Haizhao Yang<br>Department of Mathematics<br>Department of Computer Science<br>University of Maryland College Park<br>College Park, MD, USA<br>hzyang@umd.edu


#### Abstract

In this paper, we propose a pre-trained foundation model FMint (Foundation Model based on Initialization), designed to speed up large-scale simulations of various differential equations with high accuracy via error correction. Humandesigned simulation algorithms excel at capturing the fundamental physics of engineering problems, but often need to balance the trade-off between accuracy and efficiency. While deep learning methods offer innovative solutions across numerous scientific fields, they frequently fall short in domain-specific knowledge. FMint bridges these gaps through conditioning on the initial coarse solutions obtained from conventional human-designed algorithms, and trained to obtain refined solutions for various differential equations. Based on the backbone of large language models, we adapt the in-context learning scheme to learn a universal error correction method for dynamical systems from given prompted sequences of coarse solutions. The model is pre-trained on a corpus of $600 \mathrm{~K}$ ordinary differential equations (ODEs), and we conduct extensive experiments on both in-distribution and out-of-distribution tasks. FMint outperforms various baselines on large-scale simulation, and demonstrates its capability in generalization to unseen ODEs. Our approach achieves an accuracy improvement of 1 to 2 orders of magnitude over state-of-the-art dynamical system simulators, and delivers a $5 \mathrm{X}$ speedup compared to traditional numerical algorithms.


## 1 Introduction

Dynamical systems characterize the evolution of physical states over time. They are fundamental in describing the change of physical states across a wide range of disciplines, including physics [1, 2, 3], chemistry [4, 5], engineering [6, 7, 8], and finance [9, 10]. Typically, these systems are formulated as systems of ordinary differential equations (ODEs):

$$
\begin{equation*}
\frac{d \mathbf{u}(t)}{d t}=\mathbf{f}[\mathbf{u}(t)], \quad \mathbf{u}(0)=\mathbf{c}_{0} \tag{1}
\end{equation*}
$$

where $\mathbf{c}_{0}$ denotes the initial condition of the system. To solve these systems numerically, one usually employs a human-designed numerical integration algorithm such as Euler method or Runge-Kutta methods. These methods can be adapted easily to solve different types of ODEs that share the same format with guaranteed accuracy. The implementation is given as

$$
\begin{equation*}
\mathbf{u}_{n+1}=\mathbf{u}_{n}+S\left(\mathbf{f}, \mathbf{u}_{n}, \Delta t_{n}\right), \quad \mathbf{u}_{\mathbf{0}}=c_{0}, \quad n=0,1, \cdots \tag{2}
\end{equation*}
$$[^0]where $S$ represents the numerical integration scheme, $\Delta t_{n}$ is the step size at $n$-th time step, and $\mathbf{u}_{n} \in \mathbb{R}^{n}$ is the approximated solution at cumulative time $\sum_{i=0}^{n} \Delta t_{i}$.

One obstacle of these human-designed algorithm is the trade-off between accuracy and efficiency. This makes the large-scale simulation using these numerical schemes impossible. Large-scale simulation often entails the simulation of numerous trajectories, each characterized by distinct initial conditions. In fact, in many real-world scenarios, high-volume simulation that produces forecasts on a set of initial conditions simultaneously plays a significant role in various applications. For example, simulations of virus propagation during an epidemic given different circumstances are necessary for formulating health regulations; weather forecasting uses ensemble forecasting to avoid misleading single forecast [11]. In these scenarios, it is practical to standardize the time step $\Delta t:=\Delta t_{1}=\Delta t_{2}=\cdots$ across simulations, facilitating batch processing. Yet, this standardization introduces a trade-off between accuracy and efficiency: a larger time step speeds up the simulation at the cost of increased simulation error, while a smaller time step reduces the error but slows down the simulation. Therefore, the long runtime makes these traditional algorithms unsuitable for wide range simulations in many practical situations.

Recently, deep learning methods have demonstrated remarkable success across various scientific domains, including solving partial differential equations (PDEs) [12], learning operators [13], and addressing inverse problems [14, 15, 16]. Data-driven algorithms utilize large data sets and are able to compute the desired quantities efficiently with high precision. However, they typically underperform in data-scarce environments and may lack essential domain knowledge. In an effort to facilitate fast simulations using neural network, Huang et al.[11] introduced NeurVec, which is designed to compensate for integration errors that enables large time step simulation with high accuracy. Nevertheless, it faces the same obstacle as many machine learning-based solvers that for each ODE system, a separate model must be trained from scratch. It therefore demands high data and computational complexity, and cannot accommodate out-of-distribution systems, restricting its applicability in real-world simulations.

With the success of large language models such as GPT-4 [17] on numerous natural language processing (NLP) tasks, the scientific computing community has increasingly focused on developing a unified model that can be applied across various systems, especially in solving partial differential equations (PDEs) and learning neural operators [18, 13, 19, 20, 21]. For more details, please see Section 2. One particular area of focus for the community is the utilization of in-context learning. Yang et al. [22, 23, 24] introduced the framework of in-context operator learning, which trains the model to learn operators and solve PDEs using prompted data. It demonstrated great generalizability to new PDE examples without any weight updates.

Inspired by the achievement of foundation models in scientific machine learning community, and to address the trade-off between accuracy and computational efficiency of conventional numerical scheme, we introduce FMint for Foundation Model based on Initialization, a pre-trained foundation model designed to speed up large-scale simulations of dynamical systems with high accuracy via error correction. Moreover, we integrate human expertise i.e., traditional ODE solvers into modern data-driven methods. Using a decoder-only transformer architecture [25], we adapt the idea of in-context learning to obtain refined solutions based on the initialization of coarse solutions that are computed using human-designed integration method for various differential equations.

Concretely, a demo consists of the coarse solution simulated with large time step and the corresponding error to the fine-grained solution using much smaller time step. FMint is then trained on demos that are generated using the ODE equation but different initial conditions. With the errors for the query sequences masked, the model learns a universal error correction method for prompted coarse solutions. The model is pre-trained on 600,000 dynamical systems from six main ODE families. Our experiments showcase that our model outperforms baselines in delivering rapid, high-accuracy ODE solutions with $5 \mathrm{X}$ speedup in comparison to numerical integration schemes. We further demonstrate the exceptional generalization capability and data-efficiency through a series of out-of-distribution tasks.

## Summary of contributions:

(1) Introduced a pre-trained foundation model FMint that synthesizes human-designed algorithms and deep learning framework. Back-boned on the decoder-only transformer, we adapt in-context learning to a universal error correction model for ODEs, enabling the fast and accurate large-scale simulations of dynamical systems.

(2) We obtained 10 to 100 times higher accuracy than state-of-the-art dynamical system simulators, and $5 \mathrm{X}$ speedup compared to traditional numerical algorithms.

(3) We demonstrated remarkable generalization ability of our model through out-of-distribution learning tasks. FMint outperform baselines on unseen non-autonomous systems, despite being trained exclusively on autonomous systems.

## 2 Related Work

Neural network for dynamical systems. In recent years, neural network based solvers have been increasingly applied to tackle scientific problems, such as solving ordinary or partial differential equations (PDEs), operator learning, and inverse problems. One commonly employed framework parameterizes the PDE solutions with a feed-forward neural network [26, 27, 12, 28, 29, 30, 31, 32, 33]. The results are enforced to obey the physical laws through either hard or soft constraints incorporated into the network's loss function. While enforcing physical laws through hard constraints guarantees the compliance to the restriction, architecture design requires extensive domain knowledge. Soft constraints implementation enables more flexibility but still imposes physical knowledge in mathematical form. Another framework, the Finite Expression Method (FEX), considers expressing PDE solutions in computer algebra [34, 35, 36], capturing the solution's structure and thus offering high accuracy and interpretable results. Neural operator learning involves a mapping from varying parameters or initial conditions to solutions using neural networks. This method achieves discretization invariance by learning a family of parametric maps, allowing it to generalize across different parameters and conditions of PDEs [13, 37, 38, 39, 40, 41]. Both FEX and neural operator learning demand large amounts of high-quality training data and lack the generalization ability to unseen distribution.

Recently, another line of research has focused on integrating traditional numerical algorithms with deep learning-based methods to enhance the accuracy of dynamical system simulations [42, 11]. For example, NeurVec [11] employs this strategy to enable rapid simulation of ODEs with large time steps. This approach achieved decent accuracy with relatively coarse integration steps on several classic dynamical systems. However, its lack of generalization capability to out-of-distribution (OOD) systems significantly restricts its practicality for large-scale real-world simulations.

## Foundation model in scientific machine learning.

Recently, large language models such as GPT-4 [17], DALL-E [43], and Llama [44] have demonstrated significant achievements in various domains [45, 46, 47, 48, 49, 50, 51, 52], including text-to-visual generation [53, 54], information retrieval [55], and text generation [56, 57]. These models are characterized by their extensive pre-training on large datasets, then are adapted to downstream tasks through zero-shot or few-shot learning approaches, or can be fine-tuned [58, 59] to tackle specific problems, showing impressive generalization and transfer learning capabilities.

Inspired by the breakthroughs, the scientific machine learning community has experienced a marked increase in the adoption of foundation models over the past year. For instance, Subramanian et al. [18] explored the transfer learning capabilities of the Fourier Neural Operator (FNO) [13]. It is used to solve three classical PDEs and showed its applicability across various physics, scales, and data availability in downstream tasks. The Unified PDE Solver (UPS) [19] extends this approach by covering a broader range of 1D and 2D PDEs, employing a pre-trained large language model for operator learning. In addition, McCabe et al. [20] introduced a method to embed PDEs with varying physical properties into a shared embedding space, facilitating the simultaneous addressing of multiple heterogeneous PDEs. Rahman et al. [21] on the other hand, proposed an attention mechanism tailored to the codomain of PDEs, enhancing the model's ability to handle PDEs with varying dimensions.

Another burgeoning area within scientific machine learning focuses on leveraging in-context learning [60, 61, 62, 63, 64]. In this approach, models are prompted with multiple example pairs and are trained to make predictions on a new query data based on patterns recognized from the training demonstrations. A notable implementation of this is the In-context Operator Network (ICON), which Yang et al. have explored in several studies [22, 23, 24]. ICON demonstrates operator learning by using example pairs that vary in parameters of the PDE and their corresponding solutions, thus enabling the network to predict solutions for new query data points. In this paper, we employ the methodology of in-context learning and build a foundation model in enhancing simulation of ODE systems via a error correction scheme.

## 3 Methodology

In solving Equation (1) for large-scale simulations, we consider selecting a numerical integration scheme that utilizes a large time step size. This can be written in stride $k \in\{1,2, \ldots\}$ and step size $\Delta t$ that results in desired accuracy, denoted as $k \Delta t$. For illustrative purposes, we consider the Euler method, which yields the following numerical simulation scheme:

$$
\begin{equation*}
\hat{\mathbf{u}}(t+k \Delta t)=\hat{\mathbf{u}}(t)+\mathbf{f}[\hat{\mathbf{u}}(t)] \cdot k \Delta t \tag{3}
\end{equation*}
$$

However, solving the dynamical system (1) with numerical scheme (3) and large step size $k \Delta t$ unavoidably causes large simulation errors. From the Taylor expansion

$$
\begin{equation*}
\mathbf{u}(t+k \Delta t)=\underbrace{\mathbf{u}(t)+\mathbf{f}[\mathbf{u}(t)] \cdot k \Delta t}_{\text {For Euler method }}+\sum_{n=2}^{\infty} \underbrace{\frac{1}{n!} \frac{\mathrm{d}^{n}}{\mathrm{~d} t^{n}} \mathbf{u}(t) \cdot[k \Delta t]^{n}}_{\operatorname{err}_{n}(k, \Delta t, \mathbf{u}(t))} \tag{4}
\end{equation*}
$$

we see that the error term $\sum_{n=2}^{\infty} \operatorname{err}_{n}(k, \Delta t, \mathbf{u}(t))$ is non-negligible and this limits the fast simulation of real-world dynamical systems. We therefore consider building a corrector foundation model that approximates $\sum_{n=2}^{\infty} \operatorname{err}_{n}$ for various dynamical systems. We call solutions obtained by vanilla numerical integration schemes (3) with time step $k \Delta t$ as "coarse solutions". With coarse solutions as an initialization, our goal is to produce highly accurate solution with fast inference time on a diverse set of dynamical systems, i.e.,

$$
\begin{equation*}
\hat{\mathbf{u}}_{k(n+1)}=\hat{\mathbf{u}}_{k n}+S\left(\mathbf{f}, \hat{\mathbf{u}}_{k n}, k \Delta t\right)+\operatorname{FMint}\left(\hat{\mathbf{u}}_{k n} ; \Theta\right), \quad \hat{\mathbf{u}}_{0}=\mathbf{c}_{0}, \quad n=0,1, \cdots \tag{5}
\end{equation*}
$$

where $\Theta$ represents all the model parameters.

Inspired by the success of large language models in various domains and the employment of in-context learning with transformer in scientific computing [22], we designed our model using a decoder-only transformer backbone [25]. The model is trained to perform in-context learning such that it predicts the error correction term in examples based on previous demonstrations. The training is done in a similar manner to the next-token-prediction scheme.

Input tokens. We construct FMint to learn the corrector from multiple demos from the same ODE system, each consists of coarse solutions and their corresponding correction term. In details, for $i$-th ODE equation, we first simulate using fine step size $\Delta t$ and obtain ODE $\left\{\mathbf{u}_{j}^{i}\right\}_{j=1}^{k n}$ where $\mathbf{u}_{j}^{i}$ represents the fine-grained solution for $i$-th ODE system at time step $j \Delta t$. Then using coarse step size $k \Delta t$, we generate ODE results $\left\{\hat{\mathbf{u}}_{k j}^{i}\right\}_{j=1}^{n}$ where we denote $\hat{\mathbf{u}}_{k j}^{i}$ the coarse solution for $i$-th ODE equation at time step $k j \Delta t$ with predefined stride $k$. The corresponding error correction term for each coarse solutions are computed from the difference

$$
\begin{equation*}
\operatorname{err}_{\hat{\mathbf{u}}_{k j}}=\mathbf{u}_{k(j+1)}-\hat{\mathbf{u}}_{k j}-S\left(\mathbf{f}, \hat{\mathbf{u}}_{k j}, k \Delta t\right) \tag{6}
\end{equation*}
$$

One pair of coarse solutions $\hat{\mathbf{u}}^{i}=\left\{\hat{\mathbf{u}}_{k j}^{i}\right\}_{j=1}^{n}$ and error term $\operatorname{err}^{i}=\left\{\operatorname{err}_{\hat{\mathbf{u}}_{k j}^{i}}\right\}$ composes one demo. The model takes a collection of demos of size $d$, a query data sequence $\hat{\mathbf{u}}^{t}$ and outputs an error correction term err ${ }^{t}$ for the query data

$$
\begin{equation*}
\left\{\left\{\hat{\mathbf{u}}^{1}, \operatorname{err}^{1}\right\},\left\{\hat{\mathbf{u}}^{2}, \operatorname{err}^{2}\right\}, \ldots,\left\{\hat{\mathbf{u}}^{d}, \operatorname{err}^{d}\right\}, \hat{\mathbf{u}}^{t}\right\} \rightarrow \operatorname{err}^{t} \tag{7}
\end{equation*}
$$

Table 1 shows an example of demo's input tokens for a two-dimensional ODE system. Similarly to [22], the first row key contains the time steps, i.e. $t_{1}=k \Delta t, t_{2}=2 k \Delta t, \ldots, t_{n}=n k \Delta t$, which is the same for both coarse solutions and error terms. The second and third rows consist of the coarse solutions for both dimensions and their corresponding correction terms. For one-dimensional ODE systems, the last rows are populated with zeros. Each column in the table represents one token.

Model architecture. During training, each token undergoes transformation through a shared embedding layer to obtain a representative embedding vector. This vector is then concatenated with a learnable positional embedding to maintain temporal context. To preserve order invariance among key-value pairs e.g., see Table 1, all tokens of the same type within an example share the same positional encoding. The concatenated vectors are fed into the transformer model, and the transformer's output is subsequently directed to a prediction head for error prediction of the query coarse solution. The model is then updated through the mean squared error of the prediction. The architecture of FMint is shown in Figure 1 .

![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-05.jpg?height=629&width=1284&top_left_y=301&top_left_x=407)

Figure 1: FMint first prepares data through simulations of coarse and fine solutions. The input tokens are generated from coarse solutions and corrections (6). Model takes input tokens of demos and the query ODE and output the predicted error terms for the query ODE.

A major challenge in training decoder-only transformer models is the implementation of masking. Specifically, when predicting the error term for the query ODE, the model must consider the coarse solutions and correction values from preceding examples, and coarse solutions from the query ODE but not the ground truth error term. This requirement stems from the fact that all demonstration examples pertain to the identical ODE, differing only in their initial conditions. Moreover, predictions of QoI are independent and remain unaffected by the order of the tokens. To effectively manage these constraints, we employ a specialized transformer masking technique employed by Yang et al. [24]. The mask satisfies all the constraints mentioned and has shown great efficiency in computational science.

Table 1: Input tokens for a demo.

| key | Coarse solution |  |  |  | Error term |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | 0 | $t_{1}$ | $\ldots$ | $t_{n}$ | 0 | $t_{1}$ | $\ldots$ | $t_{n}$ |
| value | $\hat{u}(0)$ <br> $\hat{v}(0)$ | $\hat{u}\left(t_{1}\right)$ <br> $\hat{v}\left(t_{1}\right)$ |  | $\hat{u}\left(t_{n}\right)$ <br> $\hat{v}\left(t_{n}\right)$ | $\operatorname{err}_{\hat{u}}(0)$ <br> $\operatorname{err}_{\hat{v}}(0)$ | $\operatorname{err}_{\hat{u}}\left(t_{1}\right)$ <br> $\operatorname{err}_{\hat{v}}\left(t_{1}\right)$ |  | $\operatorname{err}_{\hat{u}}\left(t_{n}\right)$ <br> $\operatorname{erf}_{\hat{v}}\left(t_{n}\right)$ |

## 4 Experiments

In this section, we demonstrate the effectiveness of FMint through a variety of in-distribution and out-of-distribution tasks. We first compare FMint with various baselines on large-scale simulation of ODE systems. Then we investigate the generalization ability for three circumstances under zeroto few-shot or fine-tuning: (1) unseen ODE families, (2) unseen coefficients range, and (3) unseen strides. Lastly, we perform ablation studies examining the effects of various design decisions in FMint.

### 4.1 Basic set-up

Data preparation. The training data consists of $600 \mathrm{~K}$ ODEs that are commonly observed in important applications in engineering and science. To pre-train our foundation model, we initially generate time series data from key dynamical systems that are prevalent across various applications. For each specified ODE, we create 1,000 variations with different parameters, and for each variation, we produce 100 trajectories with unique initial conditions. Consequently, our dataset comprises trajectories of 100,000 ODEs for each dynamical system, differentiated by varying coefficients and initial conditions. Our model is trained on data from six dynamical systems: Newton's Law of

Table 2: Parameter setup

| Name | $k$ | $\Delta t$ | IC (1st dim) | IC (2nd dim) | Integration scheme |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Law of cooling | 10 | 0.05 | $(0,80)$ | N/A | Euler |
| Lotka-Volterra | 200 | 0.005 | $(10,20)$ | $(2,10)$ | RK4 |
| Damped Osci. | 100 | 0.001 | $(-2.0,2.0)$ | $(-0.1,0.1)$ | RK4 |
| Fitzhugh Nagumo | 100 | 0.005 | $(-1.0,1.0)$ | $(-0.5,0.5)$ | RK4 |
| Falling object | 20 | 0.01 | $(0,100)$ | $(0,2)$ | RK4 |
| Pendulum gravity | 20 | 0.01 | $\left(0, \frac{\pi}{4}\right)$ | $\left(-\frac{\pi}{4}, \frac{\pi}{4}\right)$ | RK4 |
| Exponential decay | 10 | 0.05 | $(100,200)$ | N/A | Euler |
| Driven damped pendulum | 20 | 0.01 | $\left(-\frac{\pi}{4}, \frac{\pi}{4}\right)$ | $(-0.5,0.5)$ | RK4 |

Cooling (1D), Lotka-Volterra system (2D), damped harmonic oscillator (2D), FitzHugh-Nagumo (2D), falling object (2D), damped pendulum under gravity (2D).

To test our model's performance and data-efficiency on unseen ODEs via zero-shot learning and fine-tuning, we use data prepared from the two dynamical systems: exponential decay equation (1D) and driven damped pendulum (2D).

For all ODE systems, the time step size $\Delta t$, the value of strides $k$, the range of initial conditions (IC), and the numerical integration scheme used for simulations are summarized in Table 2 For more details on the physical representations of parameters in each ODE system, see Appendix A. 1

Implementation details. As a decoder-only transformer model, FMint is configured with approximately 15.8 million parameters. The model features six heads for multi-head attention, with an input/output dimension of 256 for each layer. Demo number for training used is five. The dimension for the query, key, and value of each token is set to 256 , and the hidden dimension of the feed-forward networks is 1024. All experiments are conducted on a NVIDIA A100 GPU with $80 \mathrm{~GB}$ of memory and the pre-training takes approximately 24 hours. We use AdamW optimizer with a warmup-cosinedecay schedule, with peak learning rate 1e-4 and 60 training epochs. The Adam $\beta_{1}$ and Adam $\beta_{2}$ are 0.9 and 0.999 , respectively and the weight decay is set to be $1 \mathrm{e}-4$.

Baselines and tasks. For the task of ODE simulations, we first compute FMint's improvement of the initialization of coarse solutions. Then we compare against three baselines: Neural ODE [65], NeurVec [11], and In-Context Operator Networks (ICON-LM) [24]. Neural ODEs model continuoustime dynamics by parameterizing the derivative of the hidden state with a neural network. This approach turns the forward pass into solving an initial value problem, offering a memory-efficient way to capture temporal patterns in data. NeurVec is a deep learning-based corrector aimed to compensate for integration errors and enable larger time step sizes in simulations. Based on the decoder-only transformer architecture, ICON-LM uses in-context learning for operator learning. Since the problem setting in ICON-LM is different from ours, we adapt our inputs tokens to their format with initial values as conditions and fine-grained ODEs as QoIs. ICON-LM is a multi-task model trained on a large collection of examples while both Neural ODE and NeurVec fit one neural network per example. The configuration and training details of Neural ODE and NeurVec are provided in the Appendix A. 2 A. 3 .

Evaluation metrics. We use the mean relative errors (MRE) and root mean square errors (RMSE) compared to fine-grained ODE solutions as the evaluation metric:

$$
\begin{equation*}
\operatorname{MRE}=\frac{1}{N} \sum_{i=1}^{N} \frac{\left|\tilde{u}^{i}-u^{i}\right|}{\left|u^{i}\right|}, \text { and } \quad \text { RMSE }=\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left\|\tilde{u}^{i}-u^{i}\right\|_{2}} \tag{8}
\end{equation*}
$$

where $\tilde{u}^{i}$ is the predicted ODE solution for the i-th equation. For FMint, it can be computed via $\tilde{u}_{k}^{i}=\hat{u}_{k}^{i}+\hat{e r r}^{i} . \hat{u}_{k}^{i}$ is the coarse solution of the i-th equation, and errr ${ }^{i}$ is the model output by FMint.

### 4.2 In-distribution performance

Here we evaluate FMint on the test split of the pretraining dataset. This contains ODEs from the same ODE families with the same parameter range, but have different random parameters within the range and different initial conditions. MRE and RMSE results are shown in Table 3 for all in-distribution
![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-07.jpg?height=472&width=650&top_left_y=287&top_left_x=369)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-07.jpg?height=477&width=590&top_left_y=301&top_left_x=1147)

(b)

Figure 2: (a). Visualization of the output of FMint for Lotka-Volterra (2D) in comparison to the ground truth fine-grained solutions and coarse solutions. (b). Normalized runtime for Lotka-Volterra of reference solution (RK4-FINE) obtained by using RK4 with step size 5e-3, coarse solution (RK4COARSE) is simulated with using RK4 with time step 1 and FMint.

ODE families. Compared to the initialized coarse solutions, FMint is able to improve the accuracy of simulation with at least two order of magnitude using both metrics for all six families.

FMint in general outperforms all other baselines except for Pendulum gravity where NeurVec has slightly better precision. Noticeably that we outperform task specific baselines Neural ODE for all ODE families and NeurVec for five out of six ODE families. This shows the benefit and possibility of training a multifaceted neural network for physical systems rather than specialized ones for each example. Conditioning on the initialization of coarse solutions, FMint outperforms ICON-LM on all examples using both metrics, mostly by one order of magnitude. This illustrates the importance of utilizing results from human-designed algorithms that provide information of essential physics. As an illustration, we visualize the output of FMint on example of Lotka-Volterra in Figure 2a and include the visualization of the rest examples in Appendix A. 4

In addition, we display the runtime of FMint in comparison with fine solution generation using RK4. The test is conducted on Lotka-Volterra system with 500 equations and we report the result in Figure $2 \mathrm{~b}$ To display the runtime better, we use the runtime for obtaining coarse solutions using RK4 as one unit. FMint is able to attain results with comparable accuracy to the fine solutions (RK-FINE) using less than $20 \%$ of its time.

Table 3: Comparison with baselines for in-distribution ODEs via MRE and RMSE (lower is better). Both MRE and RMSE are averaged over 500 ODEs with different parameters and initial conditions from the same family. Number of demos is five during inference stage.

| ODEs | MRE |  |  |  |  | RMSE |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | FMint | Coarse <br> sol. | ICON- <br> LM | Neural <br> ODE | NeurVec | FMint | Coarse <br> sol. | ICON- <br> LM | Neural <br> ODE | NeurVec |
| Damped Osci. | $3.70 \mathrm{e}-2$ | 1.93 | 1.06 | $5.87 \mathrm{e}-1$ | $5.50 \mathrm{e}-1$ | $1.20 \mathrm{e}-2$ | 2.39 | $2.18 \mathrm{e}-1$ | $3.77 \mathrm{e}-1$ | $1.20 \mathrm{e}-1$ |
| Falling object | $8.39 \mathrm{e}-5$ | $3.63 \mathrm{e}-2$ | $1.22 \mathrm{e}-3$ | $5.29 \mathrm{e}-4$ | $4.57 \mathrm{e}-3$ | 7.68e-3 | 3.76 | $3.14 \mathrm{e}-1$ | $4.29 \mathrm{e}-2$ | $2.65 \mathrm{e}-1$ |
| Fitzhugh Nagumo | $3.50 \mathrm{e}-3$ | $1.80 \mathrm{e}-1$ | $1.03 \mathrm{e}-1$ | $2.77 \mathrm{e}-2$ | $2.84 \mathrm{e}-2$ | $4.10 \mathrm{e}-3$ | $1.03 \mathrm{e}-1$ | $3.08 \mathrm{e}-1$ | $1.04 \mathrm{e}-1$ | $5.61 \mathrm{e}-2$ |
| Law cooling | $3.32 \mathrm{e}-4$ | $2.37 \mathrm{e}-2$ | $1.90 \mathrm{e}-3$ | $1.08 \mathrm{e}-2$ | $1.12 \mathrm{e}-2$ | 1.45e-2 | 1.31 | $1.28 \mathrm{e}-1$ | $5.52 \mathrm{e}-1$ | $4.70 \mathrm{e}-1$ |
| Lotka-Volterra | $2.38 \mathrm{e}-3$ | $3.71 \mathrm{e}-1$ | $8.16 \mathrm{e}-3$ | $3.81 \mathrm{e}-2$ | $4.29 \mathrm{e}-3$ | 8.60e-3 | 1.54 | $6.65 \mathrm{e}-2$ | $5.58 \mathrm{e}-1$ | $4.36 \mathrm{e}-2$ |
| Pendulum gravity | $6.76 \mathrm{e}-2$ | 3.08 | $4.87 \mathrm{e}-1$ | $2.22 \mathrm{e}-1$ | $3.33 \mathrm{e}-2$ | $1.3 \mathrm{e}-3$ | $9.89 \mathrm{e}-2$ | $1.23 \mathrm{e}-2$ | $2.61 \mathrm{e}-2$ | $1.09 \mathrm{e}-3$ |

### 4.3 Out-of-distribution

Unseen ODE families. We use exponential decay (1D) and driven damped pendulum (2D) as our unseen ODE families. We evaluate the transfer performance of FMint using training trajectories of size $N \in\{0,1000,5000,10000\}$ for each example. For $N=0$, we directly assess FMint without updating any model parameter and hence reflects the transfer performance under zero-shot setting. For $N \in\{1000,5000,10000\}$, we fine-tune the model for 500 iterations on the new training data of size $N$ and report the RMSE on the test examples. As a comparison, RMSEs are computed for NeurVec and Neural ODE using training set of size $50 \mathrm{~K}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-08.jpg?height=502&width=1374&top_left_y=275&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-08.jpg?height=203&width=569&top_left_y=289&top_left_x=369)

![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-08.jpg?height=201&width=569&top_left_y=496&top_left_x=369)

(a) FMint

![](https://cdn.mathpix.com/cropped/2024_06_04_d0ac8d01bd923307fd55g-08.jpg?height=431&width=574&top_left_y=283&top_left_x=1147)

(b) NeurVec

Figure 3: We compare the simulation results by FMint and NeurVec on a driven damped pendulum. FMint shows superior accuracy on this non-autonomous system, even when pre-trained only on autonomous systems, while NeurVec fails to provide correct simulation.

The results are shown in Table 4 For both ODE families, the prediction error of FMint decreases as the training sample size increases. Even the error for zero-shot results are better than that of NeurVec and Neural ODE. FMint demonstrates superior generalization ability; unlike the autonomous systems in the training dataset, the driven damped pendulum is a non-autonomous dynamical system, which poses a significant simulation challenge. As shown in Figure 3. NeurVec and Neural ODE fail to simulate such non-autonomous dynamical systems. However, FMint still achieves superior accuracy, thanks to its innovative input token design and the in-context learning scheme. This demonstrates that FMint has great potential for large-scale simulations in many real-world scenarios where data collection are expensive and training from scratch are almost impossible.

Unseen coefficients range. We also consider ODEs from the same families in the training set but with different range of coefficients to test the generalization ability of FMint. We choose the damped harmonic oscillator system with $\zeta \sim \operatorname{Uniform}(0.02,0.04)$ and $\omega \sim \operatorname{Uniform}(7.5,12.5)$ as our test examples. In the training data, we used $\zeta \sim \operatorname{Uniform}(0.01,0.02)$ and $\omega \sim \operatorname{Uniform}(5,10)$. The modified system is more challenging due to the higher frequency oscillations. The results are shown in the last column of Table 4 The zero-shot performance of our model remains competitive with NeurVec and Neural ODE trained on 50,000 ODEs. Furthermore, when fine-tuned with more training data, the accuracy of FMint further improves. This demonstrates the robust generalization capability of our model to even more challenging out-of-distribution systems.

Table 4: Zero- and few-shot transfer performance of FMint on unseen ODEs and unseen coefficients in RMSE. Our zero-shot results outperform NeurVec and Neural ODE trained from scratch.

| Method | \# Samples | Unseen ODE |  | Unseen coeffs <br> Damped Osci. |
| :---: | :---: | :---: | :---: | :---: |
|  |  | Expo Decay | Driven damped |  |
| FMint | 0 | 1.58 | $4.04 \mathrm{e}-2$ | $5.55 \mathrm{e}-1$ |
|  | 1000 | 1.42 | $8.84 \mathrm{e}-3$ | $2.64 \mathrm{e}-1$ |
|  | 5000 | 1.39 | $8.65 \mathrm{e}-3$ | $2.61 \mathrm{e}-1$ |
|  | 10000 | 1.38 | $8.19 e-3$ | $2.38 \mathrm{e}-1$ |
| NeurVec | 50000 | 2.43 | $3.29 \mathrm{e}-1$ | $4.33 \mathrm{e}-1$ |
| Neural ODE | 50000 | 2.07 | $2.08 \mathrm{e}-1$ | $4.72 \mathrm{e}-1$ |

Unseen strides. We show here the zero-shot performance of our pre-trained model on test data generated with smaller or larger strides $k$ without further training. This examines how adaptable FMint is to handle realistic circumstances in which the coarse solution simulation varies during inference stage. The strides for each ODE families used for training are shown in Table 2. For consistency over various families, we generate test examples with new stride values proportional to the training strides: $\alpha k, \alpha=\{0.75,1.5\}$.

Table 5 reports the improvement on the accuracy of simulation through MRE and RMSE of FMint and coarse solutions. When tested on examples with smaller stride values with $\alpha=0.75$, FMint is able to decrease the error by one order of magnitude for ODEs except for the damped oscillator and pendulum gravity. For initialization generated with larger strides $\alpha=1.5$, FMint improves the

Table 5: MRE and RMSE of FMint under unseen strides.

| Name | $k$ | $\overline{M R E}$ |  | RMSE |  | $k$ | MRE |  | RMSE |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | FMint | Coarse <br> sol. | FMint | Coarse <br> sol. |  | FMint | Coarse <br> sol. | FMint | Coarse <br> sol. |
| Damped Osci. | 75 | 1.01 | 3.04 | $6.84 \mathrm{e}-1$ | 1.92 | 150 | 1.11 | 2.34 | 1.69 | 3.14 |
| Falling object | 15 | $4.09 \mathrm{e}-1$ | 1.43 | $5.89 \mathrm{e}-1$ | 2.77 | 30 | $1.58 \mathrm{e}-2$ | $6.15 \mathrm{e}-2$ | 1.23 | 5.57 |
| Fitzhugh Nagumo | 75 | $1.41 \mathrm{e}-2$ | $4.19 \mathrm{e}-2$ | $3.93 \mathrm{e}-2$ | $1.02 \mathrm{e}-1$ | 150 | $6.46 e-2$ | $2.49 \mathrm{e}-1$ | $1.09 \mathrm{e}-1$ | $1.86 \mathrm{e}-1$ |
| Law of cooling | 7 | $5.72 \mathrm{e}-3$ | $2.14 \mathrm{e}-2$ | $2.69 \mathrm{e}-1$ | 1.03 | 15 | $1.34 \mathrm{e}-2$ | $3.83 \mathrm{e}-2$ | $7.2 \mathrm{e}-1$ | 2.06 |
| Lotka-Volterra | 150 | $9.49 \mathrm{e}-2$ | $3.33 \mathrm{e}-1$ | $3.48 \mathrm{e}-1$ | 1.15 | 300 | $4.82 \mathrm{e}-1$ | 1.59 | $7.11 \mathrm{e}-1$ | 2.27 |
| Pendulum gravity | 15 | 1.90 | 5.03 | $2.64 \mathrm{e}-2$ | $7.37 \mathrm{e}-2$ | 30 | $6.38 \mathrm{e}-1$ | 2.69 | $5.58 \mathrm{e}-2$ | $1.50 \mathrm{e}-1$ |

accuracy by an order of magnitude except for the damped oscillator and falling object. This may due to the fact that in a damped oscillator, though oscillatory motion is preserved, the amplitude decreases over time.

### 4.4 Ablation studies

We further conduct an ablation study to show that for in-distribution ODEs with different parameters, one demo is enough for FMint to achieve the same level of accuracy as shown in Table 3 during the inference stage. We show it by inspecting the impact of the number of demos on the accuracy of FMint. We used five demos during training and here we compare the RMSE with respect to the number of demos $d \in\{1,2,3,4,5\}$. The results are averaged over 500 test examples in each ODE family and are shown in Figure 10

## 5 Conclusion and discussions

In this paper, we presented FMint, a novel pre-trained model that speeds up large-scale simulations of dynamical systems via error correction. Based on the architecture of decoder-only transformer, FMint incorporates the in-context learning for a universal error corrector for ODEs from given prompted sequences of coarse initialized solutions. It is pre-trained using a diverse set of ODE families in one to two-dimensional space, with various coefficients and initial conditions.

We show that FMint achieves a significant improvement in accuracy over state-of-the-art dynamical system simulators and accelerates traditional integration schemes. In comparison to direct ODE solvers, we recognize the importance of integrating the strengths of human-designed algorithms and data-driven methods for the simulation of dynamical systems. Furthermore, despite being pre-trained on autonomous dynamical systems, FMint generalizes to non-autonomous systems, a feat where both Neural ODE and NeurVec models fall short. This is likely because the design of the key-value token pairs, wherein the key encodes the temporal information of the coarse solutions. The in-context learning scheme then enables it to effectively interpolate to arbitrary time points, enhancing its versatility in handling temporal dynamics.

Currently, FMint can only handle 1D and 2D ODE systems while in real-life situations, highdimensional ODEs are prevalent. However, it is straightforward to adapt it for higher-order systems by adjusting the token length, albeit at the expense of increased computational cost. As we continue to develop and refine FMint, future research will focus on expanding its applicability to even more complex systems and exploring the potential synergies with other machine learning techniques.

## Acknowledgments and Disclosure of Funding

H. Y. and Z. S. were partially supported by the US National Science Foundation under awards DMS-2244988, DMS-2206333, and the Office of Naval Research Award N00014-23-1-2007. J. Y. was partially supported by AFOSR MURI grant FA9550-20-1-0397.

## References

[1] Roger Temam. Infinite-dimensional dynamical systems in mechanics and physics, volume 68. Springer Science \& Business Media, 2012.

[2] James D Meiss. Differential dynamical systems. SIAM, 2007.

[3] Denis L Blackmore, Valeriy Hr Samoylenko, et al. Nonlinear dynamical systems of mathematical physics: spectral and symplectic integrability analysis. World Scientific, 2011.

[4] Tamás Tél, Alessandro de Moura, Celso Grebogi, and György Károlyi. Chemical and biological activity in open flows: A dynamical system approach. Physics reports, 413(2-3):91-196, 2005.

[5] Christian Vidal and Adolphe Pacault. Non-Equilibrium Dynamics in Chemical Systems: Proceedings of the International Symposium, Bordeaux, France, September 3-7, 1984, volume 27. Springer Science \& Business Media, 2012.

[6] Vasile Marinca and Nicolae Herisanu. Nonlinear dynamical systems in engineering: Some approximate approaches. Springer Science \& Business Media, 2012.

[7] Stephen Wiggins. The dynamical systems approach to lagrangian transport in oceanic flows. Annu. Rev. Fluid Mech., 37:295-328, 2005.

[8] Rafal Goebel, Ricardo G Sanfelice, and Andrew R Teel. Hybrid dynamical systems. IEEE control systems magazine, 29(2):28-93, 2009.

[9] Dominique Guegan. Chaos in economics and finance. Annual Reviews in Control, 33(1):89-93, 2009.

[10] J Dong, D Zhang, and A Nagurney. A projected dynamical systems model of general financial equilibrium with stability analysis. Mathematical and computer Modelling, 24(2):35-44, 1996.

[11] Zhongzhan Huang, Senwei Liang, Hong Zhang, Haizhao Yang, and Liang Lin. On fast simulation of dynamical system with neural vector enhanced numerical solver. Scientific Reports, 13(1):15254, 2023.

[12] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422-440, 2021.

[13] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations (2020). arXiv preprint arXiv:2010.08895, 2010.

[14] Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett. Deep learning techniques for inverse problems in imaging. IEEE Journal on Selected Areas in Information Theory, 1(1):39-56, 2020.

[15] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. Nett: Solving inverse problems with deep neural networks. Inverse Problems, 36(6):065005, 2020.

[16] Hemant K Aggarwal, Merry P Mani, and Mathews Jacob. Modl: Model-based deep learning architecture for inverse problems. IEEE transactions on medical imaging, 38(2):394-405, 2018.

[17] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[18] Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael W Mahoney, and Amir Gholami. Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior. Advances in Neural Information Processing Systems, 36, 2024.

[19] Junhong Shen, Tanya Marwah, and Ameet Talwalkar. Ups: Towards foundation models for pde solving via cross-modal adaptation. arXiv preprint arXiv:2403.07187, 2024.

[20] Michael McCabe, Bruno Régaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, et al. Multiple physics pretraining for physical surrogate models. arXiv preprint arXiv:2310.02994, 2023.

[21] Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A Yeh, Jean Kossaifi, et al. Pretraining codomain attention neural operators for solving multiphysics pdes. arXiv preprint arXiv:2403.12553, 2024.

[22] Liu Yang, Siting Liu, Tingwei Meng, and Stanley J Osher. In-context operator learning for differential equation problems. arXiv preprint arXiv:2304.07993, 2023.

[23] Liu Yang and Stanley J Osher. Pde generalization of in-context operator networks: A study on 1d scalar nonlinear conservation laws. arXiv preprint arXiv:2401.07364, 2024.

[24] Liu Yang, Tingwei Meng, Siting Liu, and Stanley J Osher. Prompting in-context operator learning with sensor data, equations, and natural language. arXiv preprint arXiv:2308.05061, 2023.

[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[26] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):85058510,2018 .

[27] Jiequn Han, Arnulf Jentzen, et al. Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations. Communications in mathematics and statistics, 5(4):349-380, 2017.

[28] Tianqi Cui, Tom Bertalan, Nelson Ndahiro, Pratik Khare, Michael Betenbaugh, Costas Maranas, and Ioannis G Kevrekidis. Data-driven and physics informed modeling of chinese hamster ovary cell bioreactors. Computers \& Chemical Engineering, 183:108594, 2024.

[29] Mario De Florio, Ioannis G Kevrekidis, and George Em Karniadakis. Ai-lorenz: A physics-datadriven framework for black-box and gray-box identification of chaotic systems with symbolic regression. arXiv preprint arXiv:2312.14237, 2023.

[30] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339-1364, 2018.

[31] Bing Yu et al. The deep ritz method: a deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6(1):1-12, 2018.

[32] Jiaxin Yuan, Amar Shah, Channing Bentz, and Maria Cameron. Optimal control for sampling the transition path process and estimating rates. Communications in Nonlinear Science and Numerical Simulation, 129:107701, 2024.

[33] Haixin Wang, Jiaxin Li, Anubhav Dwivedi, Kentaro Hara, and Tailin Wu. Beno: Boundaryembedded neural operators for elliptic pdes. arXiv preprint arXiv:2401.09323, 2024.

[34] Senwei Liang and Haizhao Yang. Finite expression method for solving high-dimensional partial differential equations. arXiv preprint arXiv:2206.10121, 2022.

[35] Zezheng Song, Maria K Cameron, and Haizhao Yang. A finite expression method for solving high-dimensional committor problems. arXiv preprint arXiv:2306.12268, 2023.

[36] Zezheng Song, Chunmei Wang, and Haizhao Yang. Finite expression method for learning dynamics on complex networks. arXiv preprint arXiv:2401.03092, 2024.

[37] Yong Zheng Ong, Zuowei Shen, and Haizhao Yang. Iae-net: Integral autoencoders for discretization-invariant learning. arXiv preprint arXiv:2203.05142, 2022.

[38] Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing systems, 34:24924-24940, 2021.

[39] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. arXiv preprint arXiv:2205.13671, 2022.

[40] Lulu Zhang, Tao Luo, Yaoyu Zhang, Zhi-Qin John Xu, Zheng Ma, et al. Mod-net: A machine learning approach via model-operator-data network for solving pdes. arXiv preprint arXiv:2107.03673, 2021.

[41] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature machine intelligence, 3(3):218-229, 2021.

[42] Yue Guo, Felix Dietrich, Tom Bertalan, Danimir T Doncevic, Manuel Dahmen, Ioannis G Kevrekidis, and Qianxiao Li. Personalized algorithm generation: A case study in learning ode integrators. SIAM Journal on Scientific Computing, 44(4):A1911-A1933, 2022.

[43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821-8831. Pmlr, 2021.

[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[45] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930-1940, 2023.

[46] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[47] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024.

[48] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023.

[49] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.

[50] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models. arXiv preprint arXiv:2310.06201, 2023.

[51] Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, and Xueqi Cheng. Decoding by contrasting knowledge: Enhancing llms' confidence on edited facts, 2024.

[52] Li Jiang, Yusen Wu, Junwu Xiong, Jingqing Ruan, Yichuan Ding, Qingpei Guo, Zujie Wen, Jun Zhou, and Xiaotie Deng. Hummer: Towards limited competitive preference dataset. arXiv preprint arXiv:2405.11647, 2024.

[53] Pengliang Ji, Chuyang Xiao, Huilin Tai, and Mingxiao Huo. T2vbench: Benchmarking temporal dynamics for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2024.

[54] Pengliang Ji and Junchen Liu. Tltscore: Towards long-tail effects in text-to-visual evaluation with neuro-symbolic generative foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2024.

[55] Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, and Bo Li. C-rag: Certified generation risks for retrieval-augmented language models. arXiv preprint arXiv:2402.03181, 2024.

[56] Simin Chen, Xiaoning Feng, Xiaohong Han, Cong Liu, and Wei Yang. Ppm: Automated generation of diverse programming problems for benchmarking code generation models. arXiv preprint arXiv:2401.15545, 2024.

[57] Simin Chen, Zihe Song, Mirazul Haque, Cong Liu, and Wei Yang. Nicgslowdown: Evaluating the efficiency robustness of neural image caption generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15365-15374, 2022.

[58] Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Jinan Sun, Shikun Zhang, Xiao Luo, and Qi Tian. Parameter-efficient tuning of large-scale multimodal foundation model. Advances in Neural Information Processing Systems, 36, 2023.

[59] Bruce XB Yu, Jianlong Chang, Haixin Wang, Lingbo Liu, Shijie Wang, Zhiyu Wang, Junfan Lin, Lingxi Xie, Haojie Li, Zhouchen Lin, et al. Visual tuning. ACM Computing Surveys, 2023.

[60] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.

[61] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.

[62] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

[63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

[64] Xin Xu, Shizhe Diao, Can Yang, and Yang Wang. Can we verify step by step for incorrect answer detection? arXiv preprint arXiv:2402.10528, 2024.

[65] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.
