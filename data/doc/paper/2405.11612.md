# SOCIOTECHNICAL IMPLICATIONS OF GENERATIVE ARTIFICIAL INTELLIGENCE FOR INFORMATION ACCESS* 

Bhaskar Mitra<br>Microsoft Research<br>Montr√©al, Canada<br>bmitra@microsoft.com

Henriette Cramer<br>PaperMoon AI<br>San Francisco, USA<br>henriette.cramer@gmail.com

Olya Gurevich<br>PaperMoon AI<br>San Francisco, USA<br>olya.gurevich@gmail.com


#### Abstract

Robust access to trustworthy information is a critical need for society with implications for knowledge production, public health education, and promoting informed citizenry in democratic societies. Generative AI technologies may enable new ways to access information and improve effectiveness of existing information retrieval systems but we are only starting to understand and grapple with their long-term social implications. In this chapter, we present an overview of some of the systemic consequences and risks of employing generative $\mathrm{AI}$ in the context of information access. We also provide recommendations for evaluation and mitigation, and discuss challenges for future research.


## 1 Introduction

Robust access to trustworthy information is a critical need for society including implications for knowledge production, public health education, and promoting informed citizenry in democratic societies. Generative AI technologies such as large language models (LLMs) may enable new ways to access information and improve effectiveness of existing information retrieval (IR) systems. However, the long-term social implications of deploying these technologies in the context of information access are not yet well-understood. Existing research has focused on how these models may generate biased and harmful content [11, 22, 67, 78, 121, 151, 227] as well as the environmental costs [22, 30, 60, 159, 160, 232] of developing and deploying these models at scale. In the context of information access, Shah and Bender [178] have argued that certain framings of LLMs as "search engines" lack the necessary theoretical underpinnings and may constitute as a category error. In this current work, we present a broader perspective on the sociotechnical implications of generative AI for information access. Our perspective is informed by existing literature and aims to provide a summary of known challenges viewed through a systemic lens that we hope will serve as a useful resource for future critical research in this area. We present a summary of these implications next followed by recommendations for evaluation and mitigation later in this chapter.

## 2 Implications of generative AI for information access

We present a reflection on the potential sociotechnical implications of generative AI, with an emphasis on LLMs, for information access. Generative AI is still an emerging technology and our understanding of its sociotechnical impact today, and how it may evolve over time, is fairly limited. Our treatment of this topic is therefore necessarily both incomplete and speculative. We are informed by several recent works [22, 190, 225, 226] that attempts to map the landscape of risks and harms from LLMs. What distinguishes our treatment of this topic relative to this previous literature is the specific focus on information access. There has also been work on the considerations for specific applications of LLMs in IR, such as for generating direct responses to users' expressed information needs [178], which is relevant to our current discussion. However, a thorough exploration of every potential application of LLMs in IR systems is beyond the scope of our current work. Instead, we explore the implications for information access through a broader lens that encompasses considerations for content creation, content retrieval, sociopolitical power[^0]

Table 1: Overview of the consequences for information access from generative AI, the related mechanisms introduced by these AI technologies, and corresponding risks.

| Consequences | Mechanisms | Risks |
| :---: | :---: | :---: |
| Information ecosystem <br> disruption (\$2.1.1) | Content pollution $(\$ \sqrt{2.1 .1 .1}$ | Risks to society: democracy, <br> health and wellbeing, and <br> global inequity ( $\$ 2.2 .1$ |
|  | The "Game of telephone" effect $\$(2.1 .1 .2$ |  |
|  | Search engine manipulation $\$ 2.1 .1 .3)$ |  |
|  | Degrading retrieval quality (\$2.1.1.4) |  |
|  | Direct model access $(\$ 2.1 .1 .5)$ |  |
|  | The paradox of reuse $\$ 2.1 .1 .6$ |  |
| Concentration of power <br> ( 2.1 .2 | Compute and data moat (\$2.1.2.1) |  |
|  | ![](https://cdn.mathpix.com/cropped/2024_06_04_db1b7ce80a13fa9d9e97g-02.jpg?height=56&width=606&top_left_y=736&top_left_x=672) |  |
|  | AI alignment $(\$ 2.1 .2 .3)$ |  |
| Marginalization ( $\$ 2.1 .3$ ) | Appropriation of data labor (\$2.1.3.1) |  |
|  | Bias amplification $(\$ 2.1 .3 .2$ |  |
|  | AI doxing $(\$ 2.1 .3 .3)$ |  |
| Innovation decay $(\$(2.1 .4)$ | Industry capture $(\$ 2.1 .4 .1 p$ | Risks to IR research $(\$ 2.2 .2$ |
|  | Pollution of research artefacts $(\$ 2.1 .4 .2)$ |  |
| Ecological impact $(\$ 2.1 .5$ | Resource demand and waste $(\$ 2.1 .5 .1)$ | Risks to environment $(\$ 2.2 .3$ |
|  | Persuasive advertising $(\$ \sqrt{2.1 .5 .2)}$ |  |

dynamics, geopolitical inequities, crowd-work, ecology, and future of IR research. We reference relevant previous taxonomies and studies throughout this section to both support our claims and to establish meaningful connections in an attempt to present a more complete and consistent view on this topic to the reader.

We adopt the consequences-mechanisms-risks (CMR) framework proposed by Gausen et al. [74] to structure our presentation. Gausen et al. introduce the CMR framework to support designers and developers of AI (and in general any computational) systems to identify and understand (i) the systemic consequences of developing and deploying the technology under study in the real world, (ii) the mechanisms introduced by said technology responsible for these consequences, and (iii) the corresponding risks to relevant stakeholders. The framework intentionally explicates the higher-level consequences to motivate viewing the challenges through a more systemic lens. The mechanisms, in turn, focus on more low-level system behaviors and aspects of the technology development process that contribute to the consequences and risks, and therefore represent sites for more actionable mitigation. These consequences and mechanisms are mapped to relevant potential risks. Through literature survey, in this work we identify the consequences, mechanisms, and risks of generative $\mathrm{AI}$ in the context of information access, and organize them according to the CMR framework as shown in Table 1. While we acknowledge that this list of consequences-mechanisms-risks is incomplete, we hope that it provides a summary of the sociotechnical concerns already identified in existing literature and provokes new questions for critical future research.

### 2.1 Consequences and mechanisms

In the context of information access, we identify five consequences of moral and social import of generative AI, and corresponding mechanisms, that we discuss next.

### 2.1.1 Consequence: Information ecosystem disruption

To reflect on the implications of generative AI on information access, we must consider the information ecosystem as a whole, and not constrain our discussion only to the application of these emerging technologies directly in IR systems. This ecosystem includes different actors and stakeholders such as information seekers, content producers, IR systems developers, advertisers, and other sociopolitical actors. While the information ecosystem is constantly evolving, generative AI holds the potential to significantly disrupt how each of these actors operate on their own and how they relate to other actors and stakeholders. This potential for disruption spans across how content is produced, consumed, monetized, and manipulated towards specific ends. By no means do we want to imply that these plausible changes are inherently bad but the scale of these potential disruptions across the ecosystem should motivate careful
and thoughtful considerations before these technologies are deployed at scale. We discuss next some the underlying mechanisms introduced by generative AI that may contribute to these disruptions. We encourage the reader to view these mechanisms not just in isolation but to also consider how they may interact with each other and how that may impact the ecosystem over time.

### 2.1.1.1 Mechanism: Content pollution

Generative AI enables low-cost generation of derivative low-quality content at unprecedented scale. As a consequence, synthetic AI-generated content are rapidly flooding the web [102]. On Amazon.2 these AI-generated content include scammy derivatives of existing publications [118, 131, 152] and fake travel guides [122]. On YouTube ${ }^{3}$ AI-generated videos are targeting children [4, 101, 119]. We are also witnessing a proliferation of news websites on the web that are almost entirely generated by $\mathrm{AI}$ [174] which are being surfaced in search results [50] and being funded by online ads [32]. Even reputable publishers have been reported to have published AI-generated articles under fake AI-generated author profiles [61]. Beyond news, other synthetic content such as AI-generated images are starting to pollute search results [13, 62]. According to another recent study [204], a "shocking" amount of content on the web today are machine translated text. Concerns have also been raised about LLMs serving as "Misinformation Superspreaders" [31, 155] as they make it trivially easy to inundate the web with politcally-motivated firehose of falsehood 4 Hoel [101] points out that this AI pollution of our information ecosystems is a "tragedy of the commons" [96].

Pollution of our information ecosystem at such scale has critical implications for people and society. When authoring a document requires significant time and effort then the quality, style, and comprehensiveness are factors that readers may consider to decide whether and how much to trust its content. However, when the cost of writing an extensive article approaches zero, it become significantly harder for the reader to make that decision. Furthermore, the increasing adoption of these same $\mathrm{AI}$ authoring tools by reputable publishers and content producers may homogenize the language and style of content on the web making it further difficult for readers to distinguish them from low-quality AI-generated content whose sole intent is to generate ad-revenue or to disinform. This pollution of the web is also a concern for future AI models that require large web-scale datasets to train on. Including AI-generated in the training data for new AI models may have significant negative impact on model performance, what has been referred to in the current literature as "Model collapse" 136, 183], "Model Autophagy Disorder" [14], and "Habsburg AI" 5

### 2.1.1.2 Mechanism: The "Game of telephone" effect

LLMs have recently been employed in conversational search interfaces. In systems such as Bing Copilot, the LLM has access to relevant web search results that it can draw information from to generate appropriate responses to the user's expressed information needs. In this scenario, the LLM performs a complex summarization task extracting relevant information from the retrieved documents to answer the search query. In doing so, the LLM now inserts itself between the user and the retrieved web results. This shifts the responsibility of inspecting the information in the documents and assessing their relevance, trustworthiness, and surrounding context from the user to the LLM. Factual errors and inconsistencies may arise between what the LLM produces and what is in the retrieved documents. Seeing the model through an anthropomorphic lens, these errors are sometimes referred to as "hallucinations". A more technical view may see this as a noisy translation akin to the children's game of telephone ${ }^{6}$ These errors, often subtle and hard to spot, however may contribute to misinformation and reduce robustness of the information access system. While the LLMgenerated responses may cite relevant documents, it is unlikely that users diligently click these links and verify the information in the response is indeed supported by said sources. Even if the LLM reproduces exact pieces of text from the source documents without error, taking these out of the context of the document may lead to unexpected negative consequences. Such examples have previously been reported [207] in context of extracted answers that search engines display on the search result pages (SERPs) as response to the user query. These issues may become more prevalent if conversational search interfaces become a popular way to access online information.

In a more radical proposal, Metzler et al. [140] have suggested that LLMs could directly replace retrieval systems and respond directly to the user based on information in its training data. LLMs are trained to produce statistically plausible text sequences and any semblance to an information retrieval system is likely an important miscategorization[^1]of these models that we should be wary of [178]. The game of telephone effect is likely to be more intense when LLMs are expected to produce information from their training data and not just the in-context information in its input.

This interjection of the LLM between the user and the search results may have other long term effects. These interfaces may alienate users from the practice of verifying information sources and make them less skilled over time at discerning online misinformation. If users get accustomed to information being presented neatly summarized disconnected from their sources, the critical cognitive skills necessary to distinguish between trustworthy and untrustworthy information may atrophy.

### 2.1.1.3 Mechanism: Search engine manipulation

New applications of LLMs to the IR stack have exposed new attack vectors. Prompt inject attacks [87, 132, 133] that try to blur the line between instructions and data have garnered specific interest. In these types of attacks, website owners may inject what looks like instructions to the LLM. When such documents are retrieved and included in the input of the LLM as augmentation, the LLM may mistake the injected prompt in the document content and be vulnerable to manipulation.

Recently, LLMs have also found application in relevance labeling for search [203]. It is not well understood yet whether this may make the search engine vulnerable to improper ranking manipulation by website owners and search engine optimization experts. For example, one may employ the same, or similar, LLMs to reproduce the labeling scheme externally and then overfit their website content and design to achieve undue high predicted relevance against queries to rank higher on SERPs.

Other attack vectors may include using LLMs to create effective content farms to manipulate ranking of web results or even use LLMs to artificially simulate users interacting with the search system to fake clicks and other user behavior signals that search engines depend on.

### 2.1.1.4 Mechanism: Degrading retrieval quality

LLMs may negatively impact search result quality not just by contributing to new attack vectors but more worryingly in some cases the negative effect may be a result of the LLM behaving exactly as it is supposed to. For example, one potential consequence of conversational search interfaces becoming significantly more prevalent than the classic SERP is that the quality of feedback from user behavior signals on SERP may significantly degrade. Historically, users of commercial web search engines have given the systems noisy implicit feedback through clicks and other actions on SERP. These clicks are one of the key secret sauce of any modern search systems. However, conversational interfaces may discourage direct user clicks on web results and at best provide much weaker satisfaction signal that may be gleaned from the users' next utterance in the conversation. This over time may negatively impact the underlying retrieval quality making the systems worse.

In conversational search interfaces and other applications, such as Microsoft Copilot for M365 [139, 223], the LLM may conduct the search on the user's behalf. In this process, the LLM generates search queries. If these queries differ from those that are likely to be submitted by users then the underlying search system needs to optimize itself for both real user queries and LLM-generated queries. This may have consequences that are not yet well understood. Optimizing the search system directly to improve the LLMs natural language responses may also have unforeseen outcomes, especially in light of the fact that what makes for a good result set for retrieval-augmentation is not yet fully understood [55].

### 2.1.1.5 Mechanism: Direct model access

Another important consideration is the implications of open foundation models [112]. While centralized systems have their own negative implications, as we discuss in $\$ 2.1 .2$, open access generative AI models without any access moderation also pose certain challenges. For example, there are many classes of harmful intents that systems should refuse to respond to for minimizing harm. This may include search queries seeking information on methods to selfharm or cause harms to others. Publicly accessible LLMs trained on large web corpus may produce irresponsible responses to such queries in absence of content moderation. Even if the model is trained to not respond to certain classes of queries it is likely that there will be leakage. Such leakage may also happen in the context of traditional search systems. However, in the latter case, all queries are typically instrumented which allows for post-hoc analysis
and identification of critical gaps in the moderation system. Unfortunately, no such mitigation is possible once these generative AI models are released in the wild.

### 2.1.1.6 Mechanism: The paradox of reuse

Content producers and information access technologies share a critical inter-dependant relationship [138, 217]. Websites such as Wikipedia $\sqrt[7]{7}$ StackExchange 8 and Reddi $\sqrt[9]{7}$ produce critical content that are surfaced by information access platforms (e.g., web search engines) and contribute to making these platforms significantly more useful to their users. In return, these platforms have historically sent traffic back to these websites that contribute to their increased readership, subscription, and monetization. However, when these platforms stop directing traffic back to these websitese.g., by instead surfacing the relevant content directly on the search result pages (SERPs)-the relationship becomes less symbiotic towards the content producers, a phenomenon Taraborelli [198] termed the "paradox of reuse".

The application of LLMs as conversational information access interfaces is likely to significantly intensify this problem. For example, LLMs such as ChatGPT ${ }^{10}$ and Google Gemin ${ }^{11}$ may gobble up large quantities of content from these websites as part of their training data and later regurgitate the same information without any attribution back to the source websites. Even when these models summarize information from multiple online sources with attributione.g., Bing Coplio 12 , they typically deemphasize the references and reduce the likelihood of the searcher clicking through to the source websites compared to the classic ten-blue-links interface. There is evidence [56] that support that this phenomenon is already happening at scale and is jeopardizing the "grand bargain at the heart of the web" [97].

### 2.1.2 Consequence: Concentration of power

"We may have democracy, or we may have wealth concentrated in the hands of a few, but we can't have both."

$$
\begin{array}{r}
\text { - Louis Brandeis } \\
\text { As quoted by Lonergan }
\end{array}
$$

Technology shapes and is shaped by the sociopolitical power structures within which it exists. Deliberation on the social consequences of any technology must therefore include critical consideration of how the technology, and general narratives about said technology, shifts power and re-architects and codifies structures of hierarchy and control. It must also include critical reflections on the politics and values of those in power to oversee what and how technology is built, especially when they reinforce hierarchy and authoritarianism (e.g. 63, 76, 123]). The 2024 edition of the World Economic Forum's Global Risks Report [10] lists "technological power concentration" as one of the top global risks for the coming decade and as the biggest upward mover in their annual ranking of global risks compared to the previous year. A report [109] from the research institute AI Now ${ }^{13}$ similarly asserts "the concentration of economic and political power in the hands of the tech industry-Big Tech in particular" as the core challenge posed by AI. They further note that not just the technologies but the narratives (both the hype and the fear-mongering) around them questionably bolsters claims of "foundational" advancements and their unassailable equivalence with scientific progress. These concerns are complemented by the discourses within the AI community, such as the observations by Birhane et al. [26] that the prominent values expressed and operationalized in top cited AI papers generally have implications in support of centralization of power. Even if platform owners acted accountably to civil society, the concentration of power and control in their hands makes them vulnerable to other actors, such as autocratic governments, and allows that power to be potential abused for oppressive and harmful intents.

The popularization of generative AI is likely to further entrench Big Tech's concentration of power as they emerge as some of the only institutions that have the resources to develop and deploy these technologies [114]. We must interrogate how the application of these technologies for information access may contribute to further concentration and growing inequities of wealth and power. Towards that goal, we next discuss three mechanisms in the context of generative AI that may contribute to concentration of power and control.[^2]

### 2.1.2.1 Mechanism: Compute and data moat

The development of generative AI is heavily reliant on the availability of large swaths of training data and largescale computing power for training and deployment. Only a handful of institutions, largely in the private sector, own and control these necessary resources while simultaneously evangelizing AI as crucial geopolitical leverage and critical social infrastructure [109]. Increased access to these models have sometimes been touted as potential paths to mitigation [5, 187], where access may range from being heavily restricted over API to "fully open" models. The ability to download these models with their learned parameters allows others to further adapt for their own applications and open the door to more meaningful analysis and audit of these models. However, such "open access" also have severe limitations that we should recognize. The availability of the trained models does little to challenge Big Tech's predominant vision of what AI fundamentally looks like, which would require dismantling the data and compute moat itself and turning them into public infrastructure. The availability of such infrastructure would allow a broader section of society to participate in the reimagination and development of diverse approaches to AI and not merely being forced to be satisfied with critiquing and finetuning artefacts produced by these institutions. This also illustrates the importance of existing institutions such as archives, libraries and universities that have reliable, historical data. Similarly, while the research community would benefit from easier access to models for critical studies and audit, it is significantly limiting unless that access is also extended to the user-facing systems in which these technologies are deployed and corresponding instrumentation data that provides context on how these systems are used by people and potential consequences.

### 2.1.2.2 Mechanism: AI persuasion

There is an emerging recognition of the dangers of AI persuasion [35, 39, 65, 157], which Burtell and Woodside [35] define as "a process by which AI systems alter the beliefs of their users". AI systems may persuade users by appealing to their reason and argument ("rational persuasion") or by exploiting their cognitive biases and heuristics ("manipulation") [65]. El-Sayed et al. [65] identify six mechanisms of generative AI persuasion-namely (i) trust and rapport, (ii) anthropomorphism, (iii) personalisation, (iv) deception and lack of transparency, (v) manipulative strategies, and (vi) alteration of choice environment-and corresponding model features that contribute to these mechanisms. In the context of information access and advertising, these capabilities of generative AI can be powerful tools to hyper-target users and manipulate their behaviors.

Modern online information access and communication platforms monetized with targeted advertising have ushered in an age of surveillance capitalism [238, 239]. These information access systems increasingly collect detailed user behavior data that allow them to build accurate user profiles for audience targeting. There is strong evidence that people are more likely to consume information that oppose their own personal views and beliefs when the latter employs language similar to their own political leaning [234]. So, combining this treasure trove of users' private preferences and behavioral data with the capabilities of generative AI to produce persuasive language could create terrifyingly effective tools for mass behavioral manipulation. The impact of such pervasive algorithmic nudging [144] may be further pronounced over longer time periods from continuous interactions between the user and the system. Putting these capabilities in the hands of online platform owners, which typically tends to be large multinational for-profit institutions with largely hierarchical non-democratic internal governance structures, poses serious risks to functioning of democratic societies. All platforms moderate content posted or accessible through the platform [81] and in doing so they unavoidably impose their values on their users. But even without an explicit politic, these platforms by design try to affect consumer choices as part of their advertising business models. So, there is a direct incentive for these platforms to use generative AI to produce hyper-targeted highly-personalized persuasive advertisements to convince users to make certain buying decisions. When these platforms optimize for increasing user engagement out of their own commercial interests they may knowingly or unknowingly incentivize these generative AI models to be more persuasively rage-baiting [105].

### 2.1.2.3 Mechanism: AI alignment

To prevent generative AI models from producing harmful and offensive content recent research have focused on how to align these model outputs with "human values" [70, 71, 113, 172, 195]. Approaches such as reinforcement learning from human feedback (RLHF) [42, 237] have been effective in limiting certain types of problematic content from being produced. However, this approach presupposes some notions of desirable values and puts the burden of determining and enforcing them on the shoulders of platform owners. Any notions of universal values that determines what type of content these models should generate-or, not generate [206]-is highly contested [25, 108, 165, 168, 175]. Placing
these decisions in the exclusive domain of the platform owners, especially in the absence democratic and civil society oversight, further concentrates power to these platforms.

### 2.1.3 Consequence: Marginalization

Generative AI, both in its process of development and in its deployment in the context of information access, can marginalize groups and individuals by diminishing their value, power, and well-being. Next, we discuss some the mechanisms that may contribute to this.

### 2.1.3.1 Mechanism: Appropriation of data labor

Li et al. [126] define data labor as "activities that produce digital records useful for capital generation". The term encompasses both witting labor activities-as in the case of crowdwork [15], peer production [199, 200], and content moderation [81]-and unwitting activities such as user behavior data and other data generated when users interact with and participate on the platforms. Data labor also encompass the generation of artefacts by writers [44, 45], artists [211, 212], and programmers [210] among others outside of the AI development process that are nonetheless extracted from the web and fed in as training data to generative AI models. Appropriation of data labor in this context includes both (i) the uncompensated appropriation of works by writers, authors, programmers, and peer production communities like Wikipedia [17, 33, 34, 40, 41, 44, 45, 80, 135, 182, 210, 212, 213, 215, 216], and (ii) under-compensated crowdwork for data labeling that have been instrumental in the development of these technologies [15, 94, 95, 163, 196, 230, 233]. It is particularly harmful when these technologies developed on appropriated labor is then employed to displace and automate the jobs of those whose labor was already appropriated in the process [16, 51, 215]. Introduction of such automation may involve vicious cycles of skill-transfer from people to AI models whereby every profession is replaced by corresponding lesser-paid gigified equivalent as auditing and editing of model outputs [85], and the AI model capabilities continue to improve by learning from workers' inputs while the workers themselves progressively lose their economic value and power, get deskilled, and even relegated into the role of moral crumple zones [66]. This is a critical challenge in the context of information access because (i) the devaluation of writers and artists have direct implications for the quality of content on the web, and (ii) these automated content generation tools are starting to get incorporated directly in information access platforms [164]. Similar concerns of commodification and appropriation have also been raised in other information and knowledge access contexts such as in the enterprise [74].

AI for me, data labor for thee. Another pernicious aspect of AI data labor dynamics is how it mirrors and reifies racial capitalism and coloniality, employs global labor exploitation and extractive practices, and reinforces the global north and south divide [24, 49, 92, 117, 147, 154, 194]. These dynamics encompass accruing the benefits of generative AI to privileged western population who speak English or other data-rich languages while data labor is relegated to the global south where these technologies struggle to find relevance. Communities that significantly contribute to AI data labor may even find their own linguistic styles being labeled AI-ese [99] and being forced to repeatedly prove their own humanity [57, 137]. Attempts to bridge the global north-south data gap also in turn may further intensify surveillance capitalism and data extractive practices in the global south [43].

### 2.1.3.2 Mechanism: Bias amplification

LLMs and other generative models reproduce and amplify harmful biases and stereotypes from their training datasets [11, 22, 28, 29, 36, 83] which can lead to allocative and representational harms [53]. Harms may also materialize from demographic blindness [74] when the model (or, the system it is embedded in) treats different individuals and groups as alike when it in fact is unwarranted. Examples may include the handling of certain languages as one homogeneous entity without regards for sociolects or dialects [27] or holding different perspectives as equally valid without considerations for historical context or structural dynamics of power. These biases are concerning in the context of information access systems that are responsible for supporting informed citizenry and functioning democracies, health literacy, and knowledge production among other societal needs.

### 2.1.3.3 Mechanism: AI doxing

We introduce the phrase AI doxing to describe the act of leaking private information of people by an AI system. Weidinger et al. [225] note that this may be caused by models leaking private information (e.g., address and telephone number) present in their training data [38] or when these models are employed to predict people's sensitive attributes (e.g., political and sexual identities) based on what is known about them publicly [120, 156, 169, 235]. Private
information in the training data is a challenge even if the datasets have been sourced from the public web because these models may continue to regurgitate that information after it has been removed from the web or these models may bypass safety measures that would prevent such information from surfacing through web search-e.g., the information may be protected by robots.txt that blocks popular search crawlers but misses crawler bots that specifically collect data for AI model training. In many contexts, applications of these models to predict people's private information may be based on shaky scientific grounds [12, 209], to put it mildly. However, such applications of these models may contribute to serious harms and discrimination regardless of their accuracy as long as some people are convinced of its predictive power and employ it to marginalize others. AI doxing may also take other forms such as reverse-imagesearch [19], a functionality supported by some search engines, that may be abused for stalking and harassment.

### 2.1.4 Consequence: Innovation decay

Generative AI may find innovative new applications in information access. However, the excitement around these technologies and the significant investments from industry, government, and academia on corresponding research and development have broader implications for IR research. Next, we discuss some of the mechanisms associated with the research and development of generative AI that may potentially throttle innovation in information access technologies.

### 2.1.4.1 Mechanism: Industry capture

The compute and data moat that concentrates power in hands of big tech, as discussed earlier in $\$ 2.1 .2 .1$, also creates significant barriers to entry for academic research. These barriers limits academic AI research to handful of institutions that have the necessary means and connections to industry who provide access to compute and data resources to incentivize research in areas of their economic interests. Academics who want to contribute to the research on large scale AI systems or critique their sociotechnical impacts are pressured to play well with institutions holding monopolistic control over compute, data, and systems [148]. Access to "open access' models-without the compute and data necessary to build them from scratch-allows academic researchers to invest in finding more effective applications of these technologies that serve industry interests but never to reimagine them to be radically different. Students and other academics who may someday want to work in industry are shepherded into integrating themselves into this homogenized research agenda.

This industry capture [229] subjugates academic research to the narrow profit-driven sociotechnical imaginaries of big tech and Silicon Valley capitalism [143], and thwarts any research that may not be immediately monetizable or challenges the status quo of power concentration. As Mitra [143] asks: "Whose sociotechnical imaginaries are granted normative status and what myriad of radically alternative futures are we overlooking?" Narratives of the inevitability of these technologies that are hyped up to be both transformative forces for society and simultaneously posing existential risks for humanity (often purported by the same actors) only bolsters their imagined importance to accumulate increasing global investments, including from governments. Researchers who care about sociotechnical impact and ecological sustainability are kept busy enumerating the harms of constantly emerging new AI technologies and chasing potential mitigations instead of imagining and developing systems for social good. Even as the grounded risks from these technologies (such as those discussed here) gather consensus from academic communities and civil society, the option of slowing down their deployment is taken off the table citing "competitive pressures", or only framed to address imagined future harms [77] in ways that only contributes to further reinforcement of monopolistic powers of those who have already added these technologies to their arsenals. Critical research on sociotechnical harms of AI is also under risk from controversial ideologies [75, 76] that attempts to displace calls for structural changes with altruism and misdirect attention from concerns of real harms to marginalized peoples today to unsubstantiated imagined future concerns. Even the framing of "responsible AI" seems to shift the frame ever so slightly from centering the field on concerns of ethics, accountability, and transparency to one that presupposes that AI can be made responsible and that it is the technology, not the privileged institutions who wield its powers, that should be challenged. This has led some sociotechnical researchers in AI to explicitly draw attention to how these systems shift power (e.g., $[28,74,110,141])$ and to prioritize research guided by alternative visions for our sociotechnical futures grounded in universal emancipation and social justice [143].

As generative AI starts to accumulate the lion's share of research investments, it may starve out other areas of information access research. Generative AI has had exciting but limited deployments in information access systems today. There are significant open challenges to making these models broadly useful, including but not limited to concerns of potential sociotechnical harms. There is a risk that if these challenges are not mitigated in spite of the extensive resources already invested on them at present that there may be calls for even larger investments in future prompted by
sunk cost fallacy 14 It would be astute for the IR community to consciously continue to invest on research on systems and applications that our society needs beyond what these emerging AI technologies make plausible [143, 178].

### 2.1.4.2 Mechanism: Pollution of research artefacts

Risks to academic research from generative AI may also emerge through the applications of these models in IR scholarship-e.g., for authoring scientific papers and peer reviewing. There is evidence that researchers in computational sciences are already leveraging these tools [130], sometimes with hilariously terrible outcomes [161]. While the use of language models for light editing may eventually fall within the norms of socially acceptable behavior in research, their application in scholarship does raise concerns of plagiarism and scientific inaccuracies. This is an area that currently has more questions than answers and the IR community would benefit from proactively considering potential implications of this trend on future IR research.

### 2.1.5 Consequence: Ecological impact

Another important consequence of generative $\mathrm{AI}$ is its impact on our environment. In this context it is important for us to consider the direct environmental cost of developing and deploying generative AI systems at scale as well as the potential impact of these technologies on the climate change discourse online.

### 2.1.5.1 Mechanism: Resource demand and waste

The ecological cost of deep learning models has been a subject of much concern and debate in the AI community [22, 23, 30, 60, 111, 159, 160, 192, 232]. Similar concerns have also been raised within the IR community with respect to the application of these models for information access [176, 240]. By some estimates, the computing power being utilized for deep learning research has been doubling every 3.4 months since 2012 [37]. In the US, data centers consumed more than $4 \%$ of their total national electricity in 2022 , and that number is projected to grow to $6 \%$ by 2026 [91]. Another study [20] estimates that by 2040 Information and Communications Technology industry on the whole alone would account for $14 \%$ of global emissions. Beyond emissions, data center consumption of water is also raising alarm bells $[54,86,88,90,93,127,150,170]$. By 2027, global AI demand may be responsible for withdrawal of $1.1-1.7$ trillion gallons of fresh water annually [93, 127]. Serious concern also revolve around the rising levels of electronic wastes [115]. Even as we make progress in reducing the ecological cost of training and deploying our current AI models, we risk encouraging the development of even larger models and their wider deployment worsening the overall ecological impact (i.e., Jevons paradox) ${ }^{15}$

### 2.1.5.2 Mechanism: Persuasive advertising

Generative AI may not only negatively impact the environment through increasing demand for natural resources and increasing generation of waste, but it may also supercharge climate change disinformation [7, 9, 47, 59, 72, 188]. For example, the fossil-fuel industry may attempt to sway public opinion through advertising that leverages generative AI's persuasion capabilities discussed in $\$$ 2.1.2.2 Persuasive advertising may also be employed by other environmentallyunfriendly business models like fast-fashion [46]. While the direct ecological cost of generative AI justifiably garners lots of attention, its potential impact on the politics of climate change also deserves scrutiny.

### 2.2 Risks

We categorize the risks of generative AI broadly to our society, to IR research, and to the environment. We map the first three consequences discussed earlier in this section-i.e., (i) Information ecosystem disruption (\$2.1.1), (ii) concentration of power ( $\$ 2.1 .2$, and (iii) marginalization ( $\$ 2.1 .3$ - and their corresponding mechanisms as potentially contributing to the risks to society. We further map the last two consequences-i.e., (iv) Innovation decay (\$2.1.4) and (v) Ecological impact ( $\$ 2.1 .5$-to the risks to IR research and the environment, respectively.[^3]

### 2.2.1 Risks to society

Information access is a critical need of any democratic society and a necessary ingredient for social transformation [48, 82, 84, 100, 166]. It is also a social determinant of economic progress [149, 236] and health [145]. Disruptions to the information ecosystem bears potentially grave risks to most aspects of our social lives. Our world is currently facing a confluence of pandemic [68, 177, 202], rising global conflicts [201, 205], and escalating climate catastrophes 106 , 158, 167] that are pushing us towards precarity. Our information ecosystems are already struggling under the weight of misinformation and disinformation that in this critical moment is eroding public trust in online platforms, institutions, and each other. It is imperative that researchers and developers of information access systems prioritize safeguarding social interests and be vigilant in considering potential risks of disruption and ecosystem collapse when integrating generative AI technologies in the IR stack.

The risks to society are not just from potential disruptions of the information ecosystem, but also from how these technologies simultaneously concentrate power away from those at the margins of society. As institutions that develop and operate these technologies are themselves beneficiaries of this concentration, we need democratic oversights over these platforms. If these technologies further exacerbate already worsening wealth and power inequities, it may pose severe threats to democratic institutions and human rights. We must also broaden our frame of discourse beyond how these technologies may make things worse for those at the margins while accruing benefits for those already privileged. We must consider the opportunity cost of not reimagining information access in light of our boldest sociotechnical ambitions of human emancipation, culture, and knowledge production, instead of being constrained solely by what these emerging technologies make plausible and the homogenized visions put forth by elite institutions who wield these technologies [143].

### 2.2.2 Risks to IR research

There is a tenable threat to academic IR research from industry capture that may happen through the confluence of different factors including the distancing of academic researchers from the data and compute they need to perform their work and how the prevalent narratives about the inevitability of AI technologies shapes what computational research gets funded. The concentration of access to these technologies in a handful of elite institutions shapes even what is considered "foundational" or even "AI". Research on generative AI cannot be allowed to be performed only in the context of corporate economic interests while academia is hollowed out and prevented from exploring radical new methods that challenge the status quo. Instead, the IR community must be empowered with both the space and the resources necessary to explore a diversity of these visions and critique dominant narratives. Otherwise, we risk relegating all IR research to just being extensions of industrial system development.

### 2.2.3 Risks to environment

Information access provides one of the large scale application settings for generative AI. However, the impact of such wide-scale deployment of these technologies on the impending climate crisis from anthropogenic global warming should be a critical consideration. In contrast to many "existential risks" that have become a subject of popular discourse in recent years but lack adequate scientific basis, the climate impact of generative AI poses a more substantial existential risk for our species. As we discussed in $\$ 2.1 .5$, these concerns include not just the ecological cost of developing and deploying generative AI technologies but also their impact on online discourses around climate change.

## 3 Methods to evaluate risks and impact

### 3.1 Evaluating the impact of generative IR applications

Evaluating the impact of generative IR applications requires methods, as do data-informed interventions to steer that impact. Creating a LLM-based demo has become exceedingly easy. Understanding the impact of a system when it gets used in real life contexts, and getting to a high quality experience for a wide variety of users, is much harder. Standards for impact assessment have not kept up a similar pace as tech developments. Klaaf points out the need to carefully consider the differences in value alignment of the goals of a system, and safety considations, harms and risk [116]. A wide range of online, offline, and human-assisted evaluations are possible -and necessary- to get a full sense of the impact of a system.

There are a number of frameworks that can provide helpful starting points for evaluating the impact of generative IR applications, and potential quality or safety improvements. Not surprisingly however, they can measure quite different aspects of a system and its underlying models. Distinctions have to be made between evaluating a model, a system,
or a technology as a whole. For example, standards for foundation model evaluations might not take into account the impact of a system that uses such a model (or a combination of models) in a specific application context.

Measurement and interventions are possible at every stage of the development life cycle of products, and their underlying models and data. In this regard, general insights around for example harm mitigation interventions being possible throughout the Machine Learning life cycle [193] also apply to generative IR. To improve quality and safety, we need to be able to operationalize and measure the impact of potential interventions. This includes evaluations on aspects of that might be both system performance issues, but are also of societal importance, e.g., harmful/toxic output, hallucination, and differing model performance across languages/demographics.

### 3.2 Threat identification, assessment, and modeling

When the emergence of a new technology or application becomes apparent, the assessment of whether this poses risks or opportunities within specific domains poses a challenge. Before development of a system, threats and opportunities can be identified. As Kapoor et al. [112] point out, it's crucial to not evaluate the risks and impact of new systems in isolation, but rather in comparison with existing technologies. For example, the impact of usage of foundation models in search should be compared to existing web search. For this purpose, Kapoor et al., present an evaluation framework focus on marginal risks, applied to Open Foundation Models. Their framework is based on threat identification work from cybersecurity and consists of six steps necessary to demonstrate such marginal risk. These steps are: 1) threat identification, 2) evaluating existing risk absent open foundation models, 3) considering existing defenses absent open foundation models , 4) evidence of marginal risk of open foundation models, 5) ease of defending against new risks, and 6) outlining uncertainty and assumptions. Note that this framework does not set exact assessment criteria, but rather defines the steps to get to such evaluations.

In practical settings, this might mean having to select standards for the development process (e.g. emerging standards from organizations such as NIST [8] or ISO [1], company-specific standards such as Microsoft's Responsible AI Standard v2 General Requirements [2], or following new (local) legal requirements). However, mapping out potential consequences and identifying mechanisms that introduce risks in the specific context of a system needs to go much further. How to disrupt potential negative mechanisms in order to mitigate those risks requires gauging a wide range of consumer-side impacts [64], but also wider societal impacts. That includes framework focused on worker consequences [74], or practical methods focused on reducing the (legal) risks of using certain types of copyrighted or restricted training data vs. expected performance gains [142].

### 3.3 Evaluation during model development

### 3.3.1 Model benchmarks vs. actual system context

LLM benchmarks are widely used to compare the quality and safety progress made by new model releases, resulting in model leaderboards on different scenarios. The Stanford HELM [191] leaderboard for example shows the performance of different LLM models on benchmarks, and these benchmarks include societal impact and bias-related measures. Their HELM ('holistic framework for evaluating foundation models') framework [129] uses scenarios, and measures seven metrics. Those are accuracy, calibration, robustness, efficiency, but also more social impact-oriented fairness, bias, and toxicity. Each scenario focuses on one use cases, and consists of a dataset of instances, such as the LegalBench set of legal reasoning tasks [89], or medical board exam problem sets [107]. The larger BIG-bench ("Beyond the Imitation Game benchmark") [21] consists of 200+ tasks, contributed by hundreds of authors at a variety of institutes. More specific benchmarks for trustworthiness such as DecodingTrust, in turn focus on subsets such as toxicity, stereotyping, adversarial and out-of distribution robustness, privacy, machine ethics, and fairness [222],

Paradoxically, while these benchmarks include aspects of societal impacts, bias and as toxicity, these benchmarks do not necessarily cover the aspects that matter most in a specific application context. Benchmarks are generally geared towards structured comparisons between models, not towards evaluating end-user applications in practice. This means that they may not be particularly suitable for a specific application and the people involved in its usage. In addition, using such large benchmarks can be quite resource intensive, making 'lite' versions necessary that are less comprehensive. Both Helm and BIG-Bench are also implemented as Lite versions. However, the evaluation differences that arise from specific, lighter implementations of benchmarks can significantly impact model comparison results [189]. This makes it necessary to go beyond these benchmarks, and ensure suitable evaluations for the application at hand to avoid deriving conclusions about safety or responsibility devoid from actual application concerns.

### 3.3.2 Combining IR and generative AI evaluation metrics

It is challenging that societal impact measure standards, including for bias, fairness and other types of impact are yet scarce for IR product settings. For example, Smith et al. [186] provide an overview of different metrics available for recommendation bias and fairness evaluation and the challenges practitioners face when choosing between them. In some cases, it may be more appropriate to for example focus on 'traditional' performance and accuracy metrics, but understand the performance and subsequent quality of experiences for different groups of people by segmenting/slicing results between groups. This then assumes the possibility of being able to define groups, or relies on more advanced methods to find clusters that may - or may not- be actionably different in performance.

Specific methods might also be necessary to match new techniques. For example, Retrieval Augmented Generation (RAG) might be used to include more reliable information in a specific domain, and reduce hallucination in a LLM setting. However, RAG does not necessarily fully solve every hallucination related issue. Specific frameworks that fit an application context are still necessary to evaluate these techniques and their actual impact on aspects such as factuality within that context. One example is Saad-Falcon et al. [173], who present an evaluation framework, ARES, for RAG-assisted Question \& Answering settings. This framework uses three evaluation scores: context relevance of the retrieved information, answer faithfulness - the answer's grounding in the retrieved context, and answer relevance to the question asked. These are similar to IR-evaluations, but might need adjustment to the setting at hand, and used datasets need to reflect actual needs in current circumstances.

### 3.3.3 LLMs to evaluate LLM

Beyond specific metrics, ongoing research is investigating the efficacy of LLMs to evaluate LLMs [181, 224]. For example, [224] et al. use an LLM to rate the factuality of a long-form response to prompts, while additionally also using Google Search. While promising, such more complex evaluation constellations also lead to additional complexity in understanding what is being evaluated, and changes therein as the evaluator LLM changes. This leads to having to validate the validation in itself [181]. While a human-and-LLM agent collaboration can help in this validation (as in e.g. [181]'s EvalGen approach), the evaluation criteria cannot be fully separated from observation of model outputs, resulting in a feedback loop from output to adjusted evaluation criteria.

### 3.4 Evaluation pre/post system release

### 3.4.1 Online evaluation using actual user behavior vs. offline evaluation

Whether evaluations are done online or offline can deeply impact results. Offline evaluations - even when using thoughtful standards - might not reflect what actual end-users do in real-life settings, or system performance over time. Online evaluations similarly are limited to what is in scope in for example included metrics, but include actual interactions. It involves field testing; getting an IR system online and out to actual users and analyzing their interactions with the system. It can include methods such as controlled experiments or extended A/B testing, and analysis of interactions; Hoffmann provides an overview of most common techniques used in IR settings [104].

### 3.4.2 Stress testing, red-teaming and qualitative end-user evaluations

Other types of stress testing, and assessment of security issues, as well as evaluation of experiences of actual users is necessary. A helpful discussion is provided by Khlaaf, who points out the need for careful consideration what methods and terminology are appropriate for evaluations that probe for vulnerabilities of a system towards the outside world, including for example red teaming for safety issues that can arise from using a system [116].

Beyond metrics and quantitative analysis oriented methods, it is crucial to apply a combination of safety/securityinspired methods, user design and UX research methods to understand the actual reactions of users. For example, red team approaches in which "a group of people authorized and organized to emulate a potential adversary's attack or exploitation capabilities against an enterprise's security posture" (following the definition from the National Institute of Standards and Technology , NIST) in themselves also lead to design questions. For example, using LLMs as red teamers themselves [162] by generating risky prompts at scale, or using large-scale human red teaming efforts with thousands of participants who need access points might yield different results. Similarly, model characteristics might have consequences on red teaming results. Ganguli et al. [73], for instance find that RLHF models are increasingly difficult to red team as they scale, while they don't find similar challenges for other models. Interestingly this means that techniques such RLHF (reinforcement learning from human feedback) which are explicitly meant to help align agents with human preferences could also result in challenges in evaluating the systems that use them.

Table 2: Different types of existing evaluation frameworks relevant for generative IR impact \& safety. Note this is not an exhaustive overview, but rather a quick peek at the variety of methods evaluators can (and have to) choose from

| Evaluation focus | Examples |
| :--- | :--- |
| Marginal system impact, e.g. release decisions in <br> comparison with existing technology | Kapoor et al., risk framework based on cybersecurity <br> $[112]$ |
| Comparison benchmarks between LLM models that <br> include fairness, bias, toxicity-type aspects | Benchmarks used in leaderboards, e.g., HELM [191], <br> BIG-bench [21], or trustworthiness benchmarks <br> $[222]$ |
| Online or offline IR metrics, including accuracy or <br> quality across groups | Online IR-evaluation methods [103], im- <br> pact/fairness/bias metrics in recommendation <br> systems [185] |
| Evaluation metrics using automated evaluation for <br> specific LLM techniques or risks | E.g., LLMs as agents evaluating factuality of other <br> LLMs' statements [181, 224] |
| Qualitative evaluation including human adversarial <br> testing | E.g., red teaming[73, 162], and UX evaluation |

### 3.5 Societal impact of a system beyond its direct implementation and use

The impact of system usage can have impact much beyond the direct usage context. For example, the increasing demand for data and compute power of LLMs has environmental impact. Such impact can be hard to calculate, even while methods have been developed in both the IR and LLM communities around reducing environmental harm[176] and sustainability industry teams exist to ensure more energy efficient data centers for both environmental as well as monetary reasons. Others in turn try to assess whether LLMs could help in generating more green code, and develop metrics to assess the code's 'green capacity' based on earlier sustainability metrics [208].

Similarly, a plethora of work points out the potential of amplifying and entrenching power structures through the usage of generative AI methods, or changing market conditions through releasing new models for free [171], de-facto changing standards to the model that gets used most in practice. However, IR and ML evaluation methods are not generally suitable for the analysis of such impact that a particular technique or system might have. Methods from political analysis and behavioral economics might be more suitable, but are generally not shared in IR or ML venues. Challenging in the evaluation of systems is a deeper understanding of the long-term incentives that are created, and the resulting 'rational' use of LLMs in undesirable ways. A compounding challenge is that new incentives are also necessary to ensure that interventions from actual practice can be shared. Trust $\&$ Safety teams might be doing scenario planning or prepare for incidents and crises.

### 3.6 Sharing evaluation methods

From the above selection of methods, which is no mean is comprehensive, it is clear that practitioners have to carefully pick and choose which methods work for them. However, different organizations come from different evaluation traditions.

Incentives to share methods and results might not align with practical product team incentives and pressures. Metrics and standards for evaluations from actual practice are often not shared in scientific literature. Security communitystyled (external) red and (internal) blue teams, Trust \& Safety incident monitoring approaches, IR-communities' existing offline and online user feedback methods, or UX product testing approaches might be more (or less) top of mind depending on the organization and prior expertise. This means there is a gap in our generative IR literature in terms of shared understanding of actual practices and efficacy of methods [52]. If we as a community are to properly address the social risks as outlined in 2.2.1, it is imperative we find fast and effective ways to share these methods and align them with practical needs. Especially with the increasing speed of the field, the variety of fields involved, and volume of new techniques.

## 4 Actors, incentives and ways of getting organized

### 4.1 Incentives towards misuse of AI

Emerging AI capabilities and their consequences (good or bad) are a hot topic of discussion. But it is just as important to talk about incentives, or why individuals or organizations might choose to use AI in certain ways.

Below are some examples of types of actors and their possible incentives that can lead to harmful uses of AI, along with ways in which some of them can be shifted in a more positive direction. AI can be transformative for human experience and quality of life, but only if incentives (both short-term and long-term) for its use are aligned with the benefits to humanity.

Actor: State actors and ideological groups.

Incentive: Geopolitical influence in favor or against something. This includes the use of extra-persuasive [231], micro-targeted content and deepfakes to sow malicious narratives [184], undermine support and trust in democratic institutions [146], weaken social cohesion, etc.

Modification: The most effective way to modify this behavior is by making it prohibitively expensive or inconvenient to use AI for these purposes, through harsh legal consequences, content moderation, or counter-speech. The burden of implementing countermeasures falls on governments, content platforms, and community organizations.

Actor: Criminal or unscrupulous organizations.

Incentive: Financial gains from scams, ad-monetized website traffic, or product sales. This includes more legitlooking phishing content [221] and "Nigerian prince" letters; or gaming search engines via AI-generated SEO-friendly content [153].

Modification: The incentives for financial gain are always going to exist and be exploited; protection against them can take the form of better (AI-enhanced) cybersecurity and anti-spam tools, implemented and deployed by most consumer-facing web surfaces.

Actor: Commercial enterprises.

Incentive: Economic competitive advantage and increased shareholder value. Taken to its worst extreme, this incentive can lead to deceptive or discriminatory business practices, hasty deployment of cheaply developed AI to customers [69], premature restructuring of teams [124], etc. In the case of social media platforms, the high engagement on polarizing or sensationalist content can lead the platforms to tolerate, encourage, and algorithmically amplify it.

Modification: The same drive for competitive advantage can also be a force for good, particularly when it is aligned with public opinion or customer sentiment. The best-case scenario is when trustworthy and safe AI makes products more usable, attracting more customers (akin to Apple's "it just works" aesthetic that has no shortage of fans despite being more expensive than the competition). Government-led compliance requirements can also create positive incentives, like for food or car safety. And in some cases, a punitive legal strategy also works, like in the suing of tobacco companies or opiate producers, creating incentives for surviving companies to behave better.

Actor: Individuals.

Incentives: Faster completion of work tasks, improved social status, revenge against perceived slights, or exploitation of the vulnerable. At worst, these can lead to cheating, misrepresentation of one's identity of accomplishments, slander, deep-fake pornography, or AI-enhanced grooming.

Modifications: While some of these behaviors are illegal or fundamentally antisocial (and should be prosecuted as dictated by law), the urge to improve one's work performance or social status can be a good thing. If AI tools are designed to enhance human productivity while rewarding our creative impulses, and feel fun, joyful, and satisfying to use, people will be more likely to employ them to good ends.

### 4.2 Who can shift incentives, and how

In the broadest sense, it will take a whole-of-society approach to ensure that technological advances will align with the best interests of humans impacted by them (see Fig. 1). Technology builders (company and individual), governments, academia, and civil society all bear responsibility for ensuring that technological advances in information access align with societal interests. The rest of this section focuses on what can be done at the intersection of these groups or actors, since inter-group coordination is most often where things go awry.

### 4.2.1 Organizational Factors

While most of the literature and education in computer science by definition focuses on technical approaches, the impact of generative IR techniques can be influenced in other ways as well.

Changing work processes within organizations can have a direct impact on the expectations set on teams. This includes policies, explicit Go/No-Go procedures, roles and responsibilities to monitor systems, algorithmic impact assessments and model cards or other types of documentation. In different organizations, the responsibility for different measurement and mitigation might look very different. In one organization, a Machine Learning team may be expected to look at the energy consumption of their system design choices, whereas other organizations might have a technical sustainability team. In another organization, a Trust \& Safety or Integrity team might deliver evaluations of system

![](https://cdn.mathpix.com/cropped/2024_06_04_db1b7ce80a13fa9d9e97g-15.jpg?height=716&width=707&top_left_y=298&top_left_x=728)

Figure 1: Primary actors responsible for aligning technology with societal interests

output toxicity, whereas in another organization a separate Data Science team, or Product teams themselves, might have to do this work. In any case, if this responsibility is unclear, it is much harder to get this work done.

External engagement can help address internal deficiencies. Especially for audiences working on generative IR systems, some of these might not necessarily be familiar routes. Examples include:

- External advice and safety boards. increasingly created by companies to provide external advice for more complex safety or content moderation questions. This includes Facebook's Oversight Board ${ }^{16}$, which provides independent rulings on content moderation questions; parent company Meta's Safety Advisory Council ${ }^{17}$; or Spotify's Safety Advisory Board ${ }^{18}$. These do not necessarily have decision making power, but provide a more formalized way to advise external organizations and researchers.
- Regulatory advisory groups and expert consultations. Organizations such as the UN, EU, various regions and countries working on future AI policy have all formed advisory boards (e.g., the UN AI advisory board 19 . the Nordic AI advisory board). Apart from such official avenues, individual lawmakers and legal firms often consult experts. While regulatory capture is a very real concern [228], this also allows for actually implementable regulation. This means owever that considering the potential overlap between advisory boards, as well as perhaps a lack of overlap with more specific AI experts, not all relevant expertise will be represented.
- Professional organizations. Organizations such as ACM, IEEE, AAAI, the Trust \& Safety Professional Association allow for formal and informal exchange of best practices. A major challenge is ensuring that best practices in fast moving areas are also gathered and exchanged between organizations and to the public at large.

For the above arrangements, getting to collections of concrete examples of what has worked in the past is increasingly important. AI developments are speeding up, and increasingly diverse professional communities are both being impacted and getting involved. This makes efficient and effective coordination even more important. For policy makers, governmental agencies and journalists it may be hard to get an overview of which professional communities can provide actionable advice - especially with new AI developments being 'louder' than, for example, long-standing IR communities. Inside of companies, in order to benefit from external advice or research, tech teams still have to navigate how to best work with external organizations. Researchers and non-governmental organizations in turn have[^4]to know where to invest their time and expertise most effectively, and how to offer actionable advice to appropriate individuals or teams in tech companies. This includes big picture scenario planning of where to best invest, and how to create incentives that truly will have a positive impact. Implicit hierarchies of the value of different types of produced knowledge (e.g. 'being the first' or 'more technically complex'), but also a simple lack of knowledge about how certain processes work, can stand in the way of sharing of paved paths towards desired results, and of sharing these in accessible ways. It can also involve very pragmatic on-the-ground work, such as knowing how to set up contractual arrangements that work for all parties (not a skill commonly taught in IR or AI-related programs).

### 4.2.2 Data-focused methods

While a complete overview of all different mechanisms to positively affect AI development is outside the scope of this paper, one area does provide ample inspiration. Extensive literature exists on data labor and the need to understand how to effectively advocate for that labor's value [18, 79, 126, 213, 215]. Especially in the realm of training data concerns, multiple practical routes already exist, including:

- Business and partnership model development, including developing new types of licensing and new types of business partnerships [3, 6], along with ways to get funding to data creators. There is also also research on the efficacy of suggested mechanisms, such as data dividends that are suggested as a means of AI profit sharing [219].
- Collective action. When new business models do not work out, coordinated action is imperative. These can be focused on data through data strikes [218], as well as large-scale labor organizing and strikes focused on treatment of data workers. More recently the Hollywood strikes illustrated how those particularly impacted by the ways their work and likeness can be used as data, can effectively organize, lay out clear demands and succeed through both technical and organizational competence. This included understanding what incentives are at play and what leverage data producers have [214]. Methods include data strikes to withhold data [197, 218], data poisoning [58] techniques such as NightShade [98, 180], Glaze [179] and Mist [128]. Ways to empower end-users and the wider public in their relationship with tech companies are important [220], as is understanding their potential leverage and means for protest through adjusted usage [125].

For effective action, however, it is crucial that generative IR researchers who care about mitigating negative societal impact have access to ways to learn how to effectively organize and navigate organizational and political structures. Implicit hierarchies in what knowledge is appreciated in generative IR circles can become a hurdle in effectively identifying and addressing the risks outlined in earlier sections, $\$ 2.2 .1$, $\$ 2.2 .2$, and $\$ 2.2 .3$. A critical factor is knowing which concrete situations matter, what to ask for in those situations and how to assess whether impacts and risks are successfully steered.

## 5 Conclusion

In this chapter we have presented a discussion on the sociotechnical implications of generative AI for information access. Our deliberations are grounded in how these emerging technologies are currently being applied in IR applications as well as their future applications as being envisioned by practitioners and researchers. It is important to recognize that our sociotechnical visions of what information access should look like in the future is not just shaped by what emerging technologies like generative AI make plausible, but our collective visions for the future of information access should in turn shape AI technologies themselves. Mitra [143] proposed the hierarchy of IR stakeholder needs shown in 2 and argued that IR research and system development require a fundamental shift towards re-centering societal needs and that we should reimagine information access as a vehicle for radically new emancipatory futures for our society. When contemplating the implications of emerging technologies, we risk of falling in the trap of limiting ourselves to how the technology (and its process of development) is today, rather than how it can be or should be in the future. Neither generative AI nor its application in the context of information access is predetermined. So, while it is important that we consider potential harms of contemporary applications of generative $\mathrm{AI}$ in the context of information access, we close with some open question for the reader: If not this status quo, then what and how? What is the future of information access that we want to imagine for our collective wellbeing and liberation, and how can generative AI be another tool in the toolbox towards that transformation?

## References

[1] Artificial intelligence (ai) standards. URL https://www.iso.org/sectors/it-technologies/ai

Producer needs

brand exposure, traffic, content monetization

Consumer needs

relevance, efficiency, personalization
