# Crafting Interpretable Embeddings by Asking LLMs Questions 

Vinamra Benara*<br>UC Berkeley

Ion Stoica

UC Berkeley

Chandan Singh*<br>Microsoft Research

John X. Morris<br>Cornell University

## Richard Antonello <br> UT Austin

Alexander G. Huth

UT Austin

*Equal contribution

Jianfeng Gao<br>Microsoft Research


#### Abstract

Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.

We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks. ${ }^{1}$


## 1 Introduction

Text embeddings are critical to many applications, including information retrieval, semantic clustering, retrieval-augmented generation, and language neuroscience. Traditionally, text embeddings leveraged interpretable representations such as bag-of-words or BM-25 [1]. Modern methods often replace these embeddings with representations from large language models (LLMs), which may better capture nuanced contexts and interactions [2-7]. However, these embeddings are essentially black-box representations, making it difficult to understand the predictive models built on top of them (as well as why they judge different texts to be similar in a retrieval context). This opaqueness is detrimental in scientific fields, such as neuroscience [8] or social science [9], where trustworthy interpretation itself is the end goal. Moreover, this opaqueness has debilitated the use of LLM embeddings (for prediction or retrieval) in high-stakes applications such as medicine [10], and raised issues related to regulatory pressure, safety, and alignment [11-14].

To ameliorate these issues, we introduce question-answering embeddings (QA-Emb), a method that builds an interpretable embedding by repeatedly querying a pre-trained autoregressive LLM with a set of questions that are selected for a problem (Fig. 1). Each element of the embedding represents[^0]

![](https://cdn.mathpix.com/cropped/2024_05_29_9a564bc6117c4f3a3e39g-02.jpg?height=453&width=1372&top_left_y=256&top_left_x=363)

Figure 1: QA-Emb produces an embedding for an input text by prompting an LLM with a series of yes/no questions. This embedding can then be used in downstream tasks such as fMRI response prediction or information retrieval.

the answer to a different question asked to an LLM, making the embedding human-inspectable. For example, the first element may be the answer to the question Does the input mention time? and the output would map yes/no to 1/0. Training QA-Emb requires only black-box access to the LLM (it does not require access to the LLM internals) and modifies only natural-language prompts, rather than LLM parameters. The learning problem is similar to the optimization faced in natural-language autoprompting $[15,16]$ or single-neuron explanation $[17,18]$, but seeks a set of questions rather than an individual prompt.

We focus on a single neuroscience problem in close collaboration with neuroscientists. Grounding in a neuroscience context allows us to avoid common pitfalls in evaluating interpretation methods [19, 20] that seek to test "interpretability" generally. Additionally, this focus allows to more realistically integrate domain knowledge to select and evaluate the questions needed for QA-Emb, one of its core strengths. Nevertheless, QA-Emb may be generally applicable in other domains where it is important to meaningfully interpret text embeddings.

In our neuroscience setting, we build QA-Emb representations from natural-language questions that can predict human brain responses measured by fMRI to natural-language stimuli. This allows for converting informal verbal hypotheses about the semantic selectivity of the brain into quantitative models, a pressing challenge in fields such as psychology [21]. We find that predictive models built on top of QA-Embs are quite accurate, providing a $26 \%$ improvement over an established interpretable baseline [22] and even slightly outperforming a black-box BERT baseline [23]. Additionally, QAEmb yields concise embeddings, outperforming the interpretable baseline (that consists of 985 features) with only 29 questions.

We investigate two major limitations of QA-Emb in Sec. 5. First, with regards to computational efficiency, we find that we can drastically reduce the computational cost of QA-Emb by distilling it into a model that computes the answers to all selected questions in a single feedforward pass by using many classification heads. Second, we evaluate the accuracy of modern LLMs at reliably answering diverse yes/no questions. Finally, Sec. 6 explores broader applications for QA-Emb in a simple information retrieval setting and text-clustering setting.

## 2 Methods

QA-Emb is an intuitive method to generate text embeddings from a pre-trained autoregressive LLM (Fig. 1). Given a text input, QA-Emb builds an interpretable embedding by querying the LLM with a set of questions about the input. Each element of the embedding represents the answer to a different question asked to an LLM. This procedure allows QA-Emb to capture nuanced and relevant details in the input while staying interpretable.

Learning a set of yes/no questions QA-Emb requires specifying a set of yes/no questions $Q \in$ $\mathcal{Q}_{\text {yes/no }}$ that yield a binary embedding $v_{Q}(x) \in\{0,1\}^{d}$ for an input string $x$. The questions are chosen to yield embeddings that are suitable for a downstream task. In our fMRI prediction task, we optimize for supervised linear regression: given a list of $n$ input strings $X$ and a multi-dimensional continuous output $Y \in \mathbb{R}^{n x d}$, we seek embeddings that allow for learning effective ridge regression models:

$$
\begin{equation*}
Q=\underset{Q \in \mathcal{Q}_{\text {yesson }}}{\operatorname{argmin}}\left[\min _{\theta \in \mathbb{R}^{d}} \sum_{i}^{n}\left\|Y^{(i)}-\theta^{T} v_{Q}\left(X^{(i)}\right)\right\|+\lambda\|\theta\|_{2}\right] \tag{1}
\end{equation*}
$$

where $\theta$ is a learned coefficient vector for predicting the fMRI responses and $\lambda$ is the ridge regularization parameter.

Directly optimizing over the space of yes/no questions is difficult, as it requires searching over a discrete space with a constraint set $\mathcal{Q}_{\text {yes/no }}$ that is hard to specify. Instead, we heuristically optimize the set of questions $Q$, by prompting a highly capable LLM (e.g. GPT-4 [24]) to generate questions relevant to our task, e.g. Generate a bulleted list of questions with yes/no answers that is relevant for \{\{task description\}\}. Customizing the task description helps yield relevant questions. The prompt can flexibly specify more prior information when available. For example, it can include examples from the input dataset to help the LLM identify data-relevant questions. Taking this a step further, questions can be generated sequentially (similar to gradient boosting) by having the LLM summarize input examples that incur high prediction error to generate new questions focused on those examples. While we focus on optimizing embeddings for fMRI ridge regression in Eq. (1), different downstream tasks may require different inner optimization procedures, e.g. maximizing the similarity of relevant documents for retrieval.

Post-hoc pruning of $Q$. The set of learned questions $Q$ can be easily pruned to be made compact and useful in different settings. For example, in our fMRI regression setting, a feature-selection procedure such as Elastic net [25] can be used to remove redundant/uninformative questions from the specified set of questions $Q$. Alternatively, an LLM can be used to directly adapt $Q$ to yield taskspecific embeddings. Since the questions are all in natural language, they can be listed in a prompt, and an LLM can be asked to filter the task-relevant ones, e.g. Here is a list of questions:\{\{question list\}\} List the subset of these questions that are relevant for \{\{task description\}\}.

Limitations: computational cost and LLM inaccuracies. While effective, the QA-Emb pipeline described here has two major limitations. First, QA-Emb is computationally intensive, requiring $d$ LLM calls to compute an embedding. This is often prohibitively expensive, but may be worthwhile in high-value applications (such as our fMRI setting) and will likely become more tenable as LLM inference costs continue to rapidly decrease. We find that we can dramatically reduce this cost by distilling the QA-Emb model into a single LLM model with many classification heads in Sec. 5.1. Otherwise, LLM inference costs are partially mitigated by the ability to reuse the KV-cache for each question and the need to only generate a single token for each question. While computing embeddings with QA-Emb is expensive, searching embeddings is made faster by the fact that the resulting embeddings are binary and often relatively compact.

Second, QA-Emb requires that the pre-trained LLM can faithfully answer the given yes-no questions. If an LLM is unable to accurately answer the questions, it hurts explanation's faithfulness. Thus, QA-Emb requires the use of fairly strong LLMs and the set of chosen questions should be accurately answered by these LLMs (Sec. 5.2 provides analysis on the question-answering accuracy of different LLMs).

Hyperparameter settings For answering questions, we average the answers from Mistral-7B [26] (mistralai/Mistral-7B-Instruct-v0.2) and LLaMA-3 8B [27] (meta-llama/Meta-Llama-3-8B-Instruct) with two prompts. All perform similarly and averaging their answers yields a small performance improvement (Table A2). For generating questions, we prompt GPT-4 [24] (gpt-4-0125-preview). Experiments were run using 64 AMD MI210 GPUs, each with 64 gigabytes of memory, and reproducing all experiments in the paper requires approximately 4 days (initial explorations required roughly 5 times this amount of compute). All prompts used and generated questions are given in the appendix or on Github.

## 3 Related work

Text embeddings Text embeddings models, which produce vector representations of document inputs, have been foundational to NLP. Recently, transformer-based models have been trained to yield embeddings in a variety of ways [2-7], including producing embeddings that are sparse [28] or have variable lengths [29]. Recent works have also leveraged autoregressive LLMs to build embeddings, e.g. by repeating embeddings [30], generating synthetic data [6,31], or using the lasttoken distribution of an autoregressive LLM as an embedding [32]. Similar to QA-Emb, various works have used LLM answers to multiple prompts for different purposes, e.g. text classification [33, 34] or data exploration [35].

Interpreting representations A few works have focused on building intrinsically interpretable text representations, e.g. word or ngram-based embeddings such as word2vec [36], Glove [37], and LLM word embeddings. Although their dimensions are not natively interpretable, for some tasks, such as classification, they can be projected into a space that is interpretable [38], i.e. a word-level representation. Note that it is difficult to learn a sparse interpretable model from these dense embeddings, as standard techniques (e.g. Elastic net) cannot be directly applied.

When instead using black-box representations, there are many post-hoc methods to interpret embeddings, e.g. probing [39, 40], categorizing elements into categories [41-44], categorizing directions in representation space [45-47], or connecting multimodal embeddings with text embeddings/text concepts [48-52]. For a single pair of text embeddings, prediction-level methods can be applied to approximately explain why the two embeddings are similar [53, 54].

Natural language representations in fMRI Using LLM representations to help predict brain responses to natural language has recently become popular among neuroscientists studying language processing [55-60] (see [61, 62] for reviews). This paradigm of using "encoding models" [63] to better understand how the brain processes language has been applied to help understand the cortical organization of language timescales [64, 65], examine the relationship between visual and semantic information in the brain [66], and explore to what extent syntax, semantics, or discourse drives brain activity $[22,67-73,18]$. The approach here extends these works to build an increasingly flexible, interpretable feature space for modeling fMRI responses to text data.

## 4 Main results: fMRI interpretation

A central challenge in neuroscience is understanding how and where semantic concepts are represented in the brain. To meet this challenge, we extend the line of study that fits models to predict the response of different brain voxels (i.e. small regions in the brain) to natural language stimuli. Using QA-Emb, we seek to bridge models that are interpretable [1,22] with more recent LLM models that are accurate but opaque $[55-57]$.

## 4.1 fMRI experimental setup

Dataset We analyze data from two recent studies $[74,75]$ (released under the MIT license), which contain fMRI responses for 3 human subjects listening to $20+$ hours of narrative stories from podcasts. We extract text embeddings from the story that each subject hears and fit a ridge regression to predict the fMRI responses (Eq. (1)). Each subject listens to either 79 or 82 stories (consisting of 27,449 time points) and 2 test stories ( 639 time points); Each subject's fMRI data consists of approximately 100,000 voxels; we preprocess it by running principal component analysis (PCA) and extracting the coefficients of the top 100 components.

Regression modeling We fit ridge regression models to predict these 100 coefficients and evaluate the models in the original voxel space (by applying the inverse PCA mapping and measuring the correlation between the response and prediction for each voxel). We deal with temporal sampling following [22, 57]; an embedding is produced at the timepoint for each word in the input story and these embeddings are interpolated using Lanczos resampling. Embeddings at each timepoint are produced from the ngram consisting of the 10 words preceding the current timepoint. We select the best-performing hyperparameters via cross-validation on 5 time-stratified bootstrap samples of the training set. We select the best ridge parameters from 12 logarithmically spaced values between 10
and 10,000 . To model temporal delays in the fMRI signal, we also select between adding 4,8 , or 12 time-lagged duplicates of the stimulus features.

Generating QA-Emb questions To generate the questions underlying QA-Emb, we prompt GPT-4 with 6 prompts that aim to elicit knowledge useful for predicting fMRI responses (precise prompts in Appendix A.3). This includes directly asking the LLM to use its knowledge of neuroscience, to brainstorm semantic properties of narrative sentences, to summarize examples from the input data, and to generate questions similar to single-voxel explanations found in a prior work [18]. This process yields 674 questions (Fig. 1 and Table A1 show examples, see all questions on Github). We perform feature selection by running multi-task Elastic net with 20 logarithmically spaced regularization parameters ranging from $10^{-3}$ to 1 and then fit a Ridge regression to the selected features. ${ }^{2}$ See extended details on the fMRI experimental setup in Appendix A. 1 and all prompts in Appendix A.3.

Baselines We compare QA-Emb to Eng1000, an interpretable baseline developed in the neuroscience literature specifically for the task of predicting fMRI responses from narrative stories [22]. Each element in an Eng1000 embedding corresponds to a cooccurence statistic with a different word, allowing full interpretation of the underlying representation in terms of related words. We additionally compare to embeddings from BERT [23] (bert-base-uncased) and LLaMA models [77, 27]. For each subject, we sweep over 5 layers from LLaMA-2 7B (meta-llama/Llama-2-7b-hf, layers 6, 12, 18, 24, 30), LLaMA-2 70B (meta-llama/Llama-2-70b-hf, layers 12, 24, 36, 48, 60), and LLaMA-3 8B (meta-llama/Meta-Llama-3-8B, layers 6, 12, 18, 24, 30), then report the test performance for the model that yields the best cross-validated accuracy (see breakdown in Table A3).

## 4.2 fMRI predictive performance

We find that QA-Emb predicts fMRI responses fairly well across subjects (Fig. 2A), achieving an average test correlation of 0.116 . QA-Emb significantly outperforms the interpretable baseline Eng1000 (26\% average improvement). Comparing to the two transformer-based baselines (which do not yield straightforward interpretations), we find that QA-Emb slightly outperforms BERT (5\% improvement) and worse than the best cross-validated LLaMA-based model ( $7 \%$ decrease). Trends are consistent across all 3 subjects.

To yield a compact and interpretable model, Fig. 2B further investigates the compressibility of the two interpretable methods (through Elastic net regularization). Compared to Eng 1000, QA-Emb improves performance very quickly as a function of the number of features included, even outperforming the final Eng 1000 performance with only 29 questions (mean test correlation 0.122 versus 0.118 ). Table A1 shows the 29 selected questions, which constitute a human-readable description of the entire model.

Fig. 2C-D further break down the predictive performance across different brain regions for a particular subject (S03). The regions that are well-predicted by QA-Emb (Fig. 2C) align with language-specific areas that are seen in the literature $[56,78]$. They do not show any major diversions from transformerbased encoding models (Fig. 2D), with the distribution of differences being inconsistent across subjects (see Fig. A1).

### 4.3 Interpreting the fitted representation from QA-Emb

The QA-Emb representation enables not only identifying which questions are important for fMRI prediction, but also mapping their selectivity across the cortex. We analyze the QA-Emb model which uses 29 questions and visualize the learned regression weights for different questions. Fig. 3 shows example flatmaps of the regression coefficients for 3 of the questions across the 2 best-predicted subjects (S02 and S03). Learned feature weights for the example questions capture known selectivity and are highly consistent across subjects. In particular, the weights for the question "Does the sentence involve a description of a physical environment or setting?" captures classical place areas including occipital place area [79] and retrosplenial complex [80], as well as intraparietal sulcus [81]. The weights for the question "Is the sentence grammatically complex?" bear striking similarity to the language network [78, 82], which is itself localized from a contrast between sentences and nonwords. Other questions, such as "Does the sentence describe a physical action?", which has strong right[^1]

![](https://cdn.mathpix.com/cropped/2024_05_29_9a564bc6117c4f3a3e39g-06.jpg?height=948&width=1376&top_left_y=206&top_left_x=382)

A

![](https://cdn.mathpix.com/cropped/2024_05_29_9a564bc6117c4f3a3e39g-06.jpg?height=466&width=677&top_left_y=252&top_left_x=385)

B

![](https://cdn.mathpix.com/cropped/2024_05_29_9a564bc6117c4f3a3e39g-06.jpg?height=467&width=675&top_left_y=249&top_left_x=1061)

D

Figure 2: Predictive performance for QA-Emb compared to baselines. (A) Test correlation for QAEmb outperforms the interpretable Eng 1000 baseline, is on par with the black-box BERT baseline, and is worse than the best-performing LLaMA model. (B) Test correlation for method quickly grows as a function of the number of included questions. (C) Test correlation per voxel for QA-Emb. (D) Difference in the test correlation per voxel for subject between QA-Emb and BERT. Error bars for (A) and (B) (standard error of the mean) are within the points (all are below 0.001). (B), (C), and (D) show results for subject $\mathrm{S} 03$.

Table 1: Mean test correlation when comparing QA-Emb computed via many LLM calls to QA-Emb computed via a single distilled model. Distillation does not significantly degrade performance. All standard errors of the mean are below $10^{-3}$.

|  | QA-Emb | QA-Emb (distill, binary) | QA-Emb (distill, probabilistic) | Eng1000 |
| :--- | ---: | ---: | ---: | ---: |
| UTS01 | 0.081 | 0.083 | 0.080 | 0.077 |
| UTS02 | 0.124 | 0.118 | 0.118 | 0.096 |
| UTS03 | 0.136 | 0.132 | 0.142 | 0.117 |
| AVG | $\mathbf{0 . 1 1 4}$ | $\mathbf{0 . 1 1 1}$ | $\mathbf{0 . 1 1 3}$ | $\mathbf{0 . 0 9 7}$ |

laterality, do not have a strong basis in prior literature. These questions point to potentially new insights into poorly understood cortical regions.

## 5 Evaluating the limitations of QA-Emb

### 5.1 Improving computational efficiency via model distillation

To reduce the computational cost of running inference with QA-Emb, we explore distilling the many LLM calls needed to compute QA-Emb into a single model with many classification heads. Specifically, we finetune a RoBERTa model [83] (roberta-base) with 674 classification heads to predict all answers required for QA-Emb in a single feedforward pass. We finetune the model on answers from LLaMA-3 8B with a few-shot prompt for $80 \%$ of the 10 -grams in the 82 fMRI training stories ( 123,203 examples), use the remaining $20 \%$ as a validation set for early stopping ( 30,801

![](https://cdn.mathpix.com/cropped/2024_05_29_9a564bc6117c4f3a3e39g-07.jpg?height=764&width=1640&top_left_y=239&top_left_x=253)

Figure 3: Learned feature weights for 3 example questions capture known selectivity and are consistent across subjects. All feature weights are jointly rescaled to the range $(-1,1)$ for visualization. Abbreviations: $\mathrm{Pr}=$ precuneus, $\mathrm{pTemp}=$ posterior temporal cortex, $\mathrm{PFC}=$ prefrontal cortex, $\mathrm{IPS}=$ intraparietal sulcus, $\mathrm{RSC}=$ retrosplenial complex, $\mathrm{OPA}=$ occipital place area, $\mathrm{PPA}=$ parahippocampal place area, Broca $=$ Broca's area, $\mathrm{sPMv}=$ superior premotor ventral speech area, $\mathrm{AC}=$ auditory cortex.

examples), and evaluate on all 10-grams in the 2 testing stories (4,594 examples). We finetune using AdamW [84] with a learning rate of $5 \cdot 10^{-5}$.

When evaluated on the fMRI prediction task, the distilled model (QA-Emb (distill, binary) in Table 1) yields a performance only slightly below the original model. If we relax the restriction that the finetuned model yields binary embeddings and instead use the predicted probability for yes, the performance rises slightly to nearly match the original model ( 0.113 instead of 0.114 average test correlation) and maintains a significant improvement over the Eng1000 baseline. Note that the distilled model achieves an $88.5 \%$ match for yes/no answers on 10 -grams for the test set. Nevertheless, the fMRI prediction for any given timepoint is computed from many questions and ngrams, mitigating the effect of individual errors in answering a question.

### 5.2 Evaluating question-answering faithfulness

We evaluate the faithfulness of our question-answering models on a recent diverse collection of 54 binary classification datasets [85, 86] (see data details in Table A4). These datasets are difficult, as they are intended to encompass a wider-ranging and more realistic list of questions than traditional NLP datasets.

Fig. 4 shows the classification accuracy for the 3 LLMs used previously along with GPT-3.5 (gpt-3.5-turbo-0125). On average, each of the LLMs answers these questions with fairly high accuracy, with GPT-4 slightly outperforming the other models. However, we observe poor performance on some tasks, which we attribute to the task difficulty and the lack of task-specific prompt engineering. For example, the dataset yielding the lowest accuracy asks the question Is the input about math research? While this may seem like a fairly simple question for an LLM to answer, the examples in the negative class consist of texts from other quantitative fields (e.g. chemistry) that usually contain numbers, math notation, and statistical analysis. Thus the LLMs answer yes to most examples and achieve accuracy near chance ( $50 \%$ ). Note that these tasks are more difficult than the relatively simple questions we answer in the fMRI experiments, especially since the fMRI input lengths are each 10 words, whereas the input lengths for these datasets are over 50 words on average (with some inputs spanning over 1,000 words).

![](https://cdn.mathpix.com/cropped/2024_05_29_9a564bc6117c4f3a3e39g-08.jpg?height=523&width=962&top_left_y=248&top_left_x=576)

Figure 4: Performance of question-answering for underlying LLMs on the D3 collection of binary classification datasets. Each point shows an individual dataset and error bars show the $95 \%$ confidence interval.

Table 2: Information retrieval results for different interpretable embedding models. QA-Emb in combination with BM-25 achieves a slight improvement over the interpretable baselines. QA-Emb additionally yields reasonably strong performance compared to its embedding size. ${ }^{\dagger}$ Note that QA-Emb embeddings are binary, so the raw number of dimensions overrepresents the embedding's size relative to other methods. Error bars show standard error of the mean.

|  | Mean reciprocal rank | Recall@ 1 | Recall@5 | Size |
| :--- | ---: | ---: | ---: | ---: |
| Bag of words | $0.37 \pm .01$ | $0.28 \pm .02$ | $0.42 \pm .02$ | 27,677 |
| Bag of bigrams | $0.39 \pm .01$ | $0.30 \pm .02$ | $0.44 \pm .02$ | 197,924 |
| Bag of trigrams | $0.39 \pm .02$ | $0.30 \pm .02$ | $0.44 \pm .02$ | 444,403 |
| QA-Emb | $0.45 \pm .01$ | $0.34 \pm .01$ | $0.50 \pm .01$ | $\dagger \mathbf{2 , 0 0 0}$ |
| BM-25 | $0.77 \pm .01$ | $0.69 \pm .01$ | $0.82 \pm .01$ | 27,677 |
| BM-25 + QA-Emb | $\mathbf{0 . 8 0} \pm . \mathbf{0 1}$ | $\mathbf{0 . 7 1} \pm . \mathbf{0 1}$ | $\mathbf{0 . 8 4} \pm . \mathbf{0 1}$ | 29,677 |

## 6 Secondary results: evaluating QA-Emb in simple NLP tasks

### 6.1 Benchmarking QA-Emb for information retrieval

In this section, we investigate applying QA-Emb to a simplified information retrieval task. We take a random subset of 4,000 queries from the MSMarco dataset ([87], Creative Commons License) and their corresponding groundtruth documents, resulting in 5,210 documents. We use $25 \%$ of the queries to build a training set and keep the remaining $75 \%$ for testing. For evaluation, we calculate the cosine similarity match between the embeddings for each query and its groundtruth documents using mean reciprocal rank and recall.

To compute QA-Emb, we first generate 2,000 questions through prompting GPT-4 based on its knowledge of queries in information retrieval (see prompts in the Github). We use a regex to slightly rewrite the resulting questions for queries to apply to documents (e.g. Is this query related to a specific timeframe? $\rightarrow$ Is this text related to a specific timeframe?). We then answer the questions both for each query and for each corpus document, again using LLaMA-3 8B. Rather than fitting a ridge regression as in Eq. (1), we use the training set to learn a scalar for each question that multiplies its binary output to change both its sign and magnitude in the embedding (optimization details in Appendix A.4).

Table 2 shows the information retrieval results. Combining BM-25 with QA-Emb achieves a small but significant improvement over the interpretable baselines. QA-Emb on its own achieves modest performance, slightly improving slightly over a bag-of-words representation, but significantly underperforming BM-25. Nevertheless, its size is considerably smaller than the other interpretable baselines making it quicker to interpret and to use for retrieval.

Table 3: Clustering scores before and after zero-shot adaptation (higher is better). Errors give standard error of the mean.

|  | Rotten tomatoes | AG News | Emotion | Financial phrasebank | AVG | Embedding <br> size (AVG) |
| :--- | :---: | :---: | :---: | :---: | :---: | ---: |
| Original | $0.126 \pm 0.011$ | $0.124 \pm 0.007$ | $0.046 \pm 0.007$ | $0.084 \pm 0.008$ | 0.095 | 100 |
| Adapted | $\mathbf{0 . 2 4 8} \pm \mathbf{0 . 0 1 6}$ | $\mathbf{0 . 1 6 6} \pm \mathbf{0 . 0 1 2}$ | $\mathbf{0 . 0 5 7} \pm \mathbf{0 . 0 1 0}$ | $\mathbf{0 . 2 9 2} \pm \mathbf{0 . 0 1 7}$ | $\mathbf{0 . 1 9 1}$ | $\mathbf{2 5 . 7 5} \pm \mathbf{0 . 9 5}$ |

### 6.2 Zero-shot adaptation in text clustering

We now investigate QA-Emb in a simplified text clustering setting. To do so, we study 4 textclassification datasets: Financial phrasebank ([88], creative commons license), Emotion [89] (CC BY-SA 4.0 license), AGNews [90], and Rotten tomatoes [91]. For each dataset, we treat each class as a cluster and evaluate the clustering score, defined as the difference between the average inter-class embedding distance and the average intra-class embedding distance (embedding distance is measured via Euclidean distance). A larger clustering score suggests that embeddings are well-clustered within each class.

In our experiment, we build a 100-dimensional embedding by prompting GPT- 4 to generate 25 yes/no questions related to the semantic content of each dataset (e.g. for Rotten tomatoes, Generate 25 yes/no questions related to movie reviews). We then concatenate the answers for all 100 questions to form our embedding. These general embeddings do not yield particularly strong clustering scores (Table 3 top), as the questions are diverse and not particularly selective for each dataset.

However, simply through prompting, we can adapt these general embeddings to each individual dataset. We call GPT-4 with a prompt that includes the full list of questions and ask it to select a subset of questions that are relevant to each task. The result embeddings (Table 3 bottom) yield higher clustering scores, suggesting that QA-Emb can be adapted to each task in a zero-shot manner (in this simplified setting). Moreover, the resulting task-specific embeddings are now considerably smaller.

## 7 Discussion

We find that QA-Emb can effectively produce interpretable and high-performing text embeddings. While we focus on a language fMRI setting, QA-Emb may be able to help flexibly build an interpretable text feature space in a variety of domains, such as social science [9], medicine [10], or economics [92], where meaningful properties of text can help discover something about an underlying phenomenon or build trust in high-stakes settings. Alternatively, it could be used in mechanistic interpretability, to help improve post-hoc explanations of learned LLM representations.

As LLMs improve in both efficiency and capability, QA-Emb can be incorporated into a variety of common NLP applications as well, such as RAG or information retrieval. For example, in RAG systems such as RAPTOR [93] or Graph-RAG [94], explanations may help an LLM not only retrieve relevant texts, but also specify why they are relevant and how they may be helpful.

Learning text questions rather than model weights is a challenging research area, furthering work in automatic prompt engineering [15, 16]. Our approach takes a heuristic first step at solving this problem, but future work could explore more directly optimizing the set of learned questions $Q$ in Eq. (1) via improved discrete optimization approaches and constraints. One possible approach may involve having LLMs themselves identify the errors the current model is making and improving based on these errors, similar to general trends in LLM self-improvement and autoprompting [95-98]. Another approach may involve improving the explanation capabilities of LLMs to help extract more questions more faithfully from data $[99,100]$.

Broader Impacts QA-Emb seeks to advance the field of LLM interpretation, a crucial step toward addressing the challenges posed by these often opaque models. Although LLMs have gained widespread use, their lack of transparency can lead to significant harm, underscoring the importance of interpretable AI. There are many potential positive societal consequences of this form of interpretability, e.g., facilitating a better understanding of scientific data and models, along with a better understanding of LLMs and how to use them safely. Nevertheless, as is the case with most ML research, the interpretations could be used to interpret and potentially improve an LLM or dataset
that is being used for nefarious purposes. Moreover, QA-Emb requires substantial computational resources, contributing to increased concerns over sustainability.

## References

[1] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009.

[2] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.

[3] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, New York, NY, USA, 2020. Association for Computing Machinery.

[4] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.

[5] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022.

[6] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.

[7] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023.

[8] Shailee Jain, Vy A Vo, Leila Wehbe, and Alexander G Huth. Computational language modeling and the promise of in silico experimentation. Neurobiology of Language, 5(1):80-106, 2024.

[9] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? arXiv preprint arXiv:2305.03514, 2023.

[10] Xiao Zhang, Dejing Dou, and Ji Wu. Learning conceptual-contextual embeddings for medical text. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9579-9586, 2020.

[11] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a" right to explanation". arXiv preprint arXiv:1606.08813, 2016.

[12] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

[13] Iason Gabriel. Artificial intelligence, values, and alignment. Minds and machines, 30(3):411-437, 2020.

[14] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. Rethinking interpretability in the era of large language models. arXiv preprint arXiv:2402.01761, 2024.

[15] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.

[16] Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng Gao. Explaining patterns in data with language models via interpretable autoprompting. arXiv preprint arXiv:2210.01848, 2022.

[17] Steven Bills, Nick Cammarata, Dan Mossing, William Saunders, Jeff Wu, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, and Jan Leike. Language models can explain neurons in language models. https: //openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html, 2023.

[18] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863, 2023.

[19] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pages 9505-9515, 2018.

[20] Finale Doshi-Velez and Been Kim. A roadmap for a rigorous science of interpretability. arXiv preprint arXiv:1702.08608, 2017.

[21] Tal Yarkoni. The generalizability crisis. Behavioral and Brain Sciences, 45:e1, 2022.

[22] Alexander G Huth, Wendy A De Heer, Thomas L Griffiths, Frédéric E Theunissen, and Jack L Gallant. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600):453-458, 2016

[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[24] OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023.

[25] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301-320, 2005.

[26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ@e Lacroix, and William El Sayed. Mistral 7b, 2023.

[27] AI@Meta. Llama 3 model card. 2024.

[28] Kyoung-Rok Jang, Junmo Kang, Giwon Hong, Sung-Hyon Myaeng, Joohee Park, Taewon Yoon, and Heecheol Seo. Ultra-high dimensional sparse representations with binarization for efficient text retrieval. arXiv preprint arXiv:2104.07198, 2021.

[29] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233-30249, 2022.

[30] Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. arXiv preprint arXiv:2402.15449, 2024.

[31] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. Gecko: Versatile text embeddings distilled from large language models, 2024.

[32] Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Promptreps: Prompting large language models to generate dense and sparse representations for zero-shot document retrieval, 2024.

[33] Chandan Singh, John Morris, Alexander M Rush, Jianfeng Gao, and Yuntian Deng. Tree prompting: Efficient task adaptation without fine-tuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6253-6267, 2023.

[34] Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, and Chris Callison-Burch. Interpretable-by-design text classification with iteratively generated concept bottleneck. arXiv preprint arXiv:2310.19660, 2023.

[35] Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, and Jingbo Shang. Answer is all you need: Instruction-following text embedding via answering the question. arXiv preprint arXiv:2402.09642, 2024.

[36] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[37] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.

[38] Chandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with large language models during training. Nature Communications, 14(1):7913, 2023.

[39] Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070, 2018.

[40] Frederick Liu and Besim Avci. Incorporating priors with feature attribution on text classification. arXiv preprint arXiv:1906.08286, 2019.

[41] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6541-6549, 2017.

[42] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences, 117(48):30071-30078, 2020.

[43] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with sparse probing, 2023.

[44] Nicholas Bai, Rahul A Iyer, Tuomas Oikarinen, and Tsui-Wei Weng. Describe-and-dissect: Interpreting neurons in vision networks with language models. arXiv preprint arXiv:2403.13771, 2024.

[45] Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, and Antonio Torralba. Toward a visual concept vocabulary for gan latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6804-6812, 2021.

[46] Ruochen Zhao, Shafiq Joty, Yongjie Wang, and Tan Wang. Explaining language models' predictions with high-impact concepts. arXiv preprint arXiv:2305.02160, 2023.

[47] Yibo Jiang, Bryon Aragam, and Victor Veitch. Uncovering meanings of embeddings via partial orthogonality. Advances in Neural Information Processing Systems, 36, 2024.

[48] Tuomas Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description of neuron representations in deep vision networks. arXiv preprint arXiv:2204.10965, 2022.

[49] Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and Tsui-Wei Weng. Label-free concept bottleneck models. arXiv preprint arXiv:2304.06129, 2023.

[50] Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P Calmon, and Himabindu Lakkaraju. Interpreting clip with sparse linear concept embeddings (splice). arXiv preprint arXiv:2402.10376, 2024.

[51] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1918719197, 2023.

[52] Aya Abdelsalam Ismail, Julius Adebayo, Hector Corrada Bravo, Stephen Ra, and Kyunghyun Cho. Concept bottleneck generative models. In The Twelfth International Conference on Learning Representations, 2023.

[53] Ruoyu Chen, Jingzhi Li, Hua Zhang, Changchong Sheng, Li Liu, and Xiaochun Cao. Sim2word: Explaining similarity with representative attribute words via counterfactual explanations. ACM Trans. Multimedia Comput. Commun. Appl., 19(6), jul 2023.

[54] Karthikeyan Natesan Ramamurthy, Amit Dhurandhar, Dennis Wei, and Zaid Bin Tariq. Analogies and feature attributions for model agnostic explanation of similarity learners. arXiv preprint arXiv:2202.01153, 2022.

[55] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sciences, 118(45):e2105646118, 2021.

[56] Richard Antonello, Aditya Vaidya, and Alexander Huth. Scaling laws for language encoding models in fmri. Advances in Neural Information Processing Systems, 36, 2024.

[57] Shailee Jain and Alexander Huth. Incorporating context into language encoding models for fmri. Advances in neural information processing systems, 31, 2018.

[58] Leila Wehbe, Ashish Vaswani, Kevin Knight, and Tom Mitchell. Aligning context-based statistical models of language with brain activity during reading. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233-243, Doha, Qatar, October 2014. Association for Computational Linguistics.

[59] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain). In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

[60] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A. Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren Jansen, Harshvardhan Gazula, Gina Choe, Aditi Rao, Catherine Kim, Colton Casto, Lora Fanda, Werner Doyle, Daniel Friedman, Patricia Dugan, Lucia Melloni, Roi Reichart, Sasha Devore, Adeen Flinker, Liat Hasenfratz, Omer Levy, Avinatan Hassidim, Michael Brenner, Yossi Matias, Kenneth A. Norman, Orrin Devinsky, and Uri Hasson. Shared computational principles for language processing in humans and deep language models. Nature Neuroscience, 25(3):369380, March 2022. Number: 3 Publisher: Nature Publishing Group.

[61] John T. Hale, Luca Campanelli, Jixing Li, Shohini Bhattasali, Christophe Pallier, and Jonathan R. Brennan. Neurocomputational models of language processing. Annual Review of Linguistics, 8(1):427-446, 2022.

[62] Shailee Jain, Vy A. Vo, Leila Wehbe, and Alexander G. Huth. Computational Language Modeling and the Promise of in Silico Experimentation. Neurobiology of Language, pages 1-27, March 2023.

[63] Michael C.-K. Wu, Stephen V. David, and Jack L. Gallant. Complete functional characterization of sensory neurons by system identification. Annual Review of Neuroscience, 29:477-505, 2006.

[64] Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S Turek, and Alexander Huth. Interpretable multi-timescale models for predicting fmri responses to continuous natural speech. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 13738-13749. Curran Associates, Inc., 2020.

[65] Catherine Chen, Tom Dupré la Tour, Jack Gallant, Daniel Klein, and Fatma Deniz. The cortical representation of language timescales is shared between reading and listening. bioRxiv, pages 2023-01, 2023.

[66] Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma Deniz, James S Gao, Anwar O NunezElizalde, and Jack L Gallant. Visual and linguistic semantic representations are aligned at the border of human visual cortex. Nature neuroscience, 24(11):1628-1636, 2021.

[67] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Remi King. Disentangling syntax and semantics in the brain with deep networks. In Proceedings of the 38th International Conference on Machine Learning, pages 1336-1348. PMLR, July 2021. ISSN: 2640-3498.

[68] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas, and Evelina Fedorenko. Lexical semantic content, not syntactic structure, is the main contributor to ann-brain similarity of fmri responses in the language network. bioRxiv, pages 2023-05, 2023.

[69] Aniketh Janardhan Reddy and Leila Wehbe. Can fMRI reveal the representation of syntactic structure in the brain? preprint, Neuroscience, June 2020.

[70] Alexandre Pasquiou, Yair Lakretz, Bertrand Thirion, and Christophe Pallier. Information-Restricted Neural Language Models Reveal Different Brain Regions' Sensitivity to Semantics, Syntax and Context, February 2023. arXiv:2302.14389 [cs].

[71] Khai Loong Aw and Mariya Toneva. Training language models for deeper understanding improves brain alignment, December 2022. arXiv:2212.10898 [cs, q-bio].

[72] Sreejan Kumar, Theodore R. Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A. Norman, Thomas L. Griffiths, Robert D. Hawkins, and Samuel A. Nastase. Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model. Technical report, bioRxiv, June 2022. Section: New Results Type: article.

[73] Subba Reddy Oota, Manish Gupta, and Mariya Toneva. Joint processing of linguistic properties in brains and language models, December 2022. arXiv:2212.08094 [cs, q-bio].

[74] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin Gupta, Allyson Morgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth. A natural language fmri dataset for voxelwise encoding models. bioRxiv, pages 2022-09, 2022.

[75] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. Nature Neuroscience, pages 1-9, 2023.

[76] Fabian Pedregosa, Ga ë 1 Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12(Oct):2825-2830, 2011.

[77] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[78] Evelina Fedorenko, Anna A Ivanova, and Tamar I Regev. The language network as a natural kind within the broader landscape of the human brain. Nature Reviews Neuroscience, pages 1-24, 2024.

[79] Joshua B Julian, Jack Ryan, Roy H Hamilton, and Russell A Epstein. The occipital place area is causally involved in representing environmental boundaries during navigation. Current Biology, 26(8):1104-1109, 2016.

[80] Anna S Mitchell, Rafal Czajkowski, Ningyu Zhang, Kate Jeffery, and Andrew JD Nelson. Retrosplenial cortex and its role in spatial cognition. Brain and neuroscience advances, 2:2398212818757098, 2018.

[81] Ilenia Salsano, Valerio Santangelo, and Emiliano Macaluso. The lateral intraparietal sulcus takes viewpoint changes into account during memory-guided attention in natural scenes. Brain Structure and Function, 226(4):989-1006, 2021.

[82] Saima Malik-Moraleda, Dima Ayyash, Jeanne Gallée, Josef Affourtit, Malte Hoffmann, Zachary Mineroff, Olessia Jouravlev, and Evelina Fedorenko. An investigation across 45 languages and 12 language families reveals a universal language network. Nature Neuroscience, 25(8):1014-1019, 2022.

[83] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[84] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

[85] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.

[86] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing differences between text distributions with natural language. In International Conference on Machine Learning, pages 2709927116. PMLR, 2022.

[87] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human-generated machine reading comprehension dataset. 2016.

[88] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65, 2014.

[89] Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687-3697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.

[90] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.

[91] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 115-124, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.

[92] Anton Korinek. Language models and cognitive automation for economic research. Technical report, National Bureau of Economic Research, 2023.

[93] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059, 2024.

[94] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.

[95] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233, 2023.

[96] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.

[97] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427, 2023.

[98] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.

[99] Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. Deductive closure training of language models for coherence, accuracy, and updatability. arXiv preprint arXiv:2401.08574, 2024.

[100] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He, and Jianfeng Gao. Towards consistent natural-language explanations via explanation-consistency finetuning. arXiv preprint arXiv:2401.13986, 2024.

[101] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
