# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM 

Abdur Rahman, Rajat Chawla, Muskaan Kumar, Arkajit Datta, Adarsh Jha,<br>Mukunda NS, and Ishaan Bhola<br>SuperAGI Research<br>abdur75648, rcrajatchawla\{@gmail.com\}, muskaan, arkajit, adarsh, mukunda,<br>ishaan \{@superagi.com\}


#### Abstract

In the rapidly evolving landscape of AI research and application, Multimodal Large Language Models (MLLMs) have emerged as a transformative force, adept at interpreting and integrating information from diverse modalities such as text, images, and Graphical User Interfaces (GUIs). Despite these advancements, the nuanced interaction and understanding of GUIs pose a significant challenge, limiting the potential of existing models to enhance automation levels. To bridge this gap, this paper presents V-Zen, an innovative Multimodal Large Language Model (MLLM) meticulously crafted to revolutionise the domain of GUI understanding and grounding. Equipped with dual-resolution image encoders, V-Zen establishes new benchmarks in efficient grounding and next-action prediction, thereby laying the groundwork for self-operating computer systems. Complementing V-Zen is the GUIDE dataset, an extensive collection of real-world GUI elements and task-based sequences, serving as a catalyst for specialised fine-tuning. The successful integration of V-Zen and GUIDE marks the dawn of a new era in multimodal AI research, opening the door to intelligent, autonomous computing experiences. This paper extends an invitation to the research community to join this exciting journey, shaping the future of GUI automation. In the spirit of open science, our code, data, and model will be made publicly available, paving the way for multimodal dialogue scenarios with intricate and precise interactions. Repo Link: SuperAGI/vzen


Keywords: LLM $\cdot$ Multimodal $\cdot$ automation $\cdot$ GUI Understanding

## 1 Introduction

Introduction In the vibrant and ever-evolving field of artificial intelligence, Multimodal Large Language Models (MLLMs)[36,33] have emerged as a transformative force, bridging the gap between diverse data representations and their comprehension. These models, adept at integrating information from multiple modalities such as text and images, have significantly expanded the scope of research and practical applications. A critical area of focus within this domain is the automation of tasks involving Graphical User Interfaces (GUIs)[13]. The

![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-02.jpg?height=664&width=1200&top_left_y=394&top_left_x=468)

Task: Reply with a thank you message

Next Action 1: Click on the reply button [x1,y1,x2,y2]

Next Action 2: Type "Thank You" in the text box

[x1,y1,x2,y2]

And so on..

Fig. 1: A Sample Case of GUI Automation Difficulty. In order to build intelligent systems capable of interacting seamlessly with various applications, identifying relevant UI components is crucial. As shown in this Gmail example, specifying tasks and their logical continuations requires a precise understanding of underlying GUI structures, predicting the next action, and precisely performing the grounding task. Our approach addresses these challenges effectively.

automation of these tasks holds immense potential for enhancing efficiency and productivity across a wide range of applications.

However, a significant portion of existing models and benchmarks in this field have been primarily centred on text-based tasks. This approach overlooks the vast potential of multimodal agents that can effectively process and integrate visual information for problem resolution. The main thrust of our research is the application of these models, with a particular emphasis on the concept of grounding, especially in the context of GUI images. Grounding, in the realm of MLLMs, refers to the process of associating words or phrases in a language with corresponding entities in other modalities. For instance, in a text-image pair, the term "apple" would be grounded in the image of an apple. This capability of MLLMs to efficiently and precisely perform grounding is particularly crucial for automating GUI tasks $[14,12]$.

However, grounding in MLLMs presents a unique set of challenges. A primary concern is the alignment of modalities, i.e., ensuring the model accurately correlates entities across different modalities. Several multimodal LLMs have recently

![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-03.jpg?height=339&width=1206&top_left_y=405&top_left_x=470)

Fig. 2: A timeline of SOTA MLLMs

addressed this issue by employing projection layers to convert one embedding to another. Despite these advancements, the coordinates of the bounding boxes provided by these models in the form of LLM text responses often lack precision. This issue becomes particularly pronounced when dealing with GUIs, where the accuracy of object detection and localization is critical. Existing LLMs rely on textual descriptions of visual content or the HTML context of web pages, but essential details like icons, images, diagrams, and spatial relationships may be overlooked or misrepresented during conversion to text embeddings [9,19]. Many GUIs do not offer direct textual interfaces for automation, highlighting the need for a multimodal LLM that can directly process visual GUI signals. The precision in detecting and interacting with GUI elements is of paramount importance in this context. The ability to accurately identify and interact with GUI elements not only enhances the functionality of these agents but also significantly augments their utility in real-world applications. The primary objective of this research is to push the boundaries of multimodal agent-based GUI task automation by developing a Multimodal Large Language Model (MLLM) that can effectively navigate, understand, and interact with GUI elements with high precision.

Our proposed model, V-Zen, is specifically designed to address these challenges. V-Zen is not just another MLLM but a sophisticated GUI Agent that can accurately process image-text inputs, interpret natural language instructions, precisely identify GUI elements, and execute actions on websites to accomplish user-defined objectives. V-Zen integrates a visual grounding module that harnesses the capabilities of the DINO detector, equipping it to effectively handle multimodal grounding tasks. In addition to the text response by LLM, the coordinates of grounding are provided separately by the grounding module, replacing a typical object detection module, thereby ensuring precise coordinates. The model's performance is further augmented by a High Cross-Resolution Module (HRCM), which enables the model to process high-resolution features and comprehend text within images. In conjunction with the development of the novel model, we have also created a dataset for this task named GUIDE (Graphical User Interface Data for Execution) [5], a cutting-edge benchmark dataset that includes bounding box annotations and textual descriptions with chain of thought
collected across various GUI platforms. GUIDE aids in advancing agentive research, ultimately leading to the development of more agile, responsive, and human-like agents across a multitude of fields.

Our key contributions in this paper are:

1. We propose V-Zen, a novel GUI Agent that leverages the power of MLLMs for efficient GUI understanding and task prediction, forming a self-operating system for various GUI tasks.
2. We introduce a visual grounding module that leverages the DINO detector's capabilities, enabling it to handle multimodal grounding tasks effectively.
3. We design a unique architecture that processes an input image in parallel at two different resolutions, allowing for efficient GUI understanding and task prediction.
4. We curate and publicly release GUIDE, a state-of-the-art benchmark dataset for executing tasks on diverse GUI platforms.

In addition to our key contributions outlined above, we conduct a thorough comparative analysis of state-of-the-art (SOTA) Grounding MLLM models under similar experimental setups. We also examine the contributions of individual modules in our model towards accuracy as an ablation study (Table 3). Finally, we discuss the remaining limitations and potential avenues for future research in the field. The remainder of the paper is structured as follows: Section 2 offers a comprehensive review of related work in the field of MLLMs and grounding. Section 3 delineates the architecture of our proposed model, V-Zen. Section 4 introduces the GUIDE dataset and its construction. Section 5 discusses the experiments conducted and the results obtained. Finally, Section 6 concludes the paper and outlines future work. This research aims to contribute significantly to the field of AI, pushing the boundaries of what is possible in GUI automation.

## 2 Related Work

The field of Natural Language Processing (NLP) has witnessed a significant transformation with the advent of Large Language Models (LLMs[21,20]). GPT3 [2], one of the pioneering LLMs, marked a milestone by significantly scaling up the model size and training data size, showcasing exceptional performance in numerous NLP tasks and setting a trend for subsequent advancements in the field. Several models such as GPTs [18], PaLM [8], BLOOM [32], and LLaMA [29], have since emerged, each pushing the boundaries of LLMs. These models have demonstrated remarkable abilities in learning from in-context examples, reasoning, following instructions, and operating over long-context sequences. Recent endeavours in the field have concentrated on refining LLMs to better align with human instructions and feedback, with models like InstructGPT [23], ChatGPT [2], and GPT4 [22] standing out as exemplars in this regard.

In the context of building web agents, these LLMs have been leveraged extensively. However, they are primarily text-based and lack the capability to handle
images or other modalities. This limitation has led to the development of Multimodal Large Language Models (MLLMs). MLLMs extend the capabilities of LLMs to understand and integrate information from multiple modalities, such as vision and audio [36]. In the context of GUI automation, our primary focus is on MLLMs, where the input modalities include text and image, and the output is a corresponding text response. The architecture and functioning of MLLMs can vary, but they generally follow a similar pattern: An encoder for each data modality generates the embeddings for data of that modality, an embedding layer aligns embeddings of different modalities into the same multimodal embedding space, and then a LLM generates text responses. Models like Flamingo [1], Kosmos-1 [11], BLIP-2 [15], and PaLM-E [8] exemplify this approach. Over time, the inherent reasoning and decision-making capabilities of MLLMs have improved, enabling them for more intricate tasks like image retrieval, image generation, and visual question answering.

The application of MLLMs in grounding tasks has been a significant area of research. Works such as Kosmos-2 [24] and Shikra [7] have enabled MLLMs to perform fine-grained image comprehension and open-world grounding. Additional works in this direction include GPT4ROI [37], PVIT [6], BuboGPT [38], VisionLLM [31], Ferret [34], Veagle [4] and CogVLM [30]. While these works improve the grounding capabilities of the model through architectural improvements or training strategy improvements, they all have a few limitations, which our work aims to address. Firstly, they produce bounding boxes in the form of pure text output, which, even if it points to the correct object, is not highly accurate. This is particularly relevant for GUI automation tasks, where there are several small elements in GUIs that need to be accurately grounded for some tasks. Secondly, most of them commonly use a $224 \times 224$ resolution image input, which makes the tiny icons and texts in GUI screenshots difficult to recognize.

Our proposed model, V-Zen, addresses these challenges by introducing a novel architecture for efficient GUI understanding and precise grounding. For accurate grounding of GUI elements, we introduce a separate grounding module on top of the LLM in the style of an open-set object detection model, and we also enable a high-resolution $1120 \times 1120$ image input through a cross-attention branch inspired by CogVLM. Additionally, we meticulously curate an extensive instruction-tuning dataset for executing tasks on diverse GUI platforms and finetune our model on it. As a result, V-Zen exhibits superior performance compared to previous works when it comes to executing tasks on diverse GUI platforms.

## 3 Proposed Architecture

The architecture of V-Zen, our proposed multimodal Large Language Model (LLM), is a sophisticated ensemble of interconnected components meticulously designed for efficient GUI understanding and precise grounding. The architecture is composed of five major modules: Low-Resolution Visual Feature Extractor (LRVFE), Multimodal Projection Adapter (MPA), Pretrained Language

![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-06.jpg?height=640&width=1198&top_left_y=409&top_left_x=472)

Fig. 3: Proposed Architecture Of V-Zen.

Note: This is just a representational architecture. The final architecture diagram is yet to be drawn - It should contain the module's names as per the equations in the paper

Model with Visual Expert (PLMVE), High-Resolution Cross Visual Module (HRCVM), and High-Precision Grounding Module (HPGM).

### 3.1 Low-Resolution Visual Feature Extractor

The journey of input through the architecture commences with the LRVFE, a low-resolution encoder (EVA-2-CLIP [26,25]) that processes the input image at a resolution of $224 \times 224$. This module is responsible for extracting meaningful features from the image, which are then used for further processing. Given an input image $I_{i n}$ and text prompt $T_{i n}$, the LRVFE generates low-resolution image features as:

$$
F_{L R}=L R V F E\left(I_{i n}\right)
$$

### 3.2 Multimodal Projection Adapter

The features extracted by the LRVFE are transformed by the MPA into a format that is suitable for processing by the LLM backbone of our architecture [16]. The MPA plays a pivotal role in aligning the modalities, ensuring that the image features match the input format of the LLM. The transformation can be represented as:

$$
F_{T}=M P A\left(F_{L R}\right)
$$

, where $F_{T}$ are the transformed features.

### 3.3 Pretrained Language Model with Visual Expert

The PLMVE, which adopts Vicuna-7B [27] as the base language model, is tasked with generating text outputs based on the processed image features and any textual input provided. Given an input $X_{i n}^{(i)}$ to the $i$ th attention layer of the PLMVE, it's split into $X_{i m g}^{(i)}$ and $X_{t x t}^{(i)}$. Then, $Q_{i m g}^{(i)}$ is obtained as:

$$
Q_{i m g}^{(i)}=V E L\left(X_{i m g}^{(i)}\right)
$$

, and $Q_{t x t}^{(i)}$ is obtained as:

$$
Q_{t x t}^{(i)}=O L L\left(X_{t x t}^{(i)}\right)
$$

The overall output can be represented as:

This can be overall represented as:

, where VEL represents the Visual Expert Layers, OLL represents the original LLM Layers, and MHSVE represents the process of multi-head self-attention with the visual expert.

### 3.4 High-Resolution Cross Visual Module

The HRCVM, inspired by CogAgent [10], is designed for higher-resolution input, accepting images of size $1120 \times 1120$ pixels. It employs a smaller EVA2-CLIP vision encoder and cross-attention of a small hidden size to fuse high-resolution image features with every layer of the PLMVE. This can be represented as

$$
X_{h i}=H R C V M\left(I_{H R}\right)
$$

, where $I_{H R}$ is the high-resolution input image, and $X_{h i}$ is the high-resolution output of the HRCVM. Each layer's attention procedure with the residual connection can be formulated as

$$
X_{o u t}^{(i)}=M H S V E\left(X_{i n}^{(i)}\right)+X_{i n}^{(i)}
$$

And then final output features with residual connection can be formulated as:

$$
Y_{\text {out }}^{(i)}=M H C A\left(X_{\text {out }}^{(i)}, X_{h i}\right)+X_{\text {out }}^{(i)}
$$

, where MHCA represents multi-head cross-attention.

| Method | Accuracy |
| :--- | :--- |
| Base Model with LRVFE and Vicuna | 87.5 |
| *+HRCVM | 89.6 |
| *+Grounding DINO | 90.3 |
| *+Projection Layer | 92.9 |
| *+Mistral LLM | 93.2 |

Table 1: Ablation Study wrt Next Task Prediction.

| Method | Accuracy |
| :--- | :--- |
| Base Model with LRVFE and Vicuna | 74.5 |
| *+HRCVM | 76.2 |
| *+Grounding DINO | 89.1 |
| *+Projection Layer | 89.7 |
| *+Mistral LLM | 89.7 |

Table 2: Ablation Study wrt Grounding.

### 3.5 High-Precision Grounding Module

The HPGM takes the hidden states extracted from the PLMVE and uses them to perform precise grounding tasks [28,35]. Unlike typical MLLM modules that provide grounding bounding boxes as part of the LLM's text output, our HPGM outputs bounding box coordinates separately, ensuring precision. The module follows an enhanced DETR [3] object detector named DINO [17]. PLMVE's last hidden state is used as the query of visual grounding to query the multi-scale feature set for visual grounding, denoted as $q_{l l m \_g n d}$. The multi-scale feature set, denoted as fms_img, is obtained using a Swin Transformer-based backbone. It takes $q_{l l m \_g n d}$ and $f m s \_i m g$ and produces the bounding boxes for precise grounding. This way, the HPGM module can precisely ground the GUI elements based on the processed image and text features.

In conclusion, the architecture of V-Zen, our proposed multimodal Large Language Model (LLM), represents a sophisticated orchestration of several interconnected components. Each module within this architecture is meticulously designed and plays a pivotal role in achieving the overarching goal of efficient GUI understanding and precise grounding. The design of these modules and their intricate interconnections is a testament to the detailed planning and innovative thinking that has gone into the development of V-Zen. This complex yet efficient assembly of components not only enhances the functionality of the system but also significantly augments its utility in real-world applications. The architecture, therefore, stands as a robust framework that pushes the boundaries of what is possible in GUI automation, ultimately contributing significantly to the field of artificial intelligence.

|  | Next Task Prediction Grounding |  |
| :--- | :--- | :--- |
| GPT-4V | 94 | 28 |
| Gemini-Pro | 92 | 21 |
| Chatter-Box | 91.3 | 87.9 |
| CogAgent | 92.4 | 86.3 |
| V-Zen | $\mathbf{9 3 . 2}$ | $\mathbf{8 9 . 7}$ |

Table 3: Performance of the proposed model.

## 4 Experiments and Results: To Be Re-Written Later

### 4.1 Training Procedure

Following the CogAgent [10] pre-training strategy, the model undergoes a twostage training procedure consisting of pre-training and specialised fine-tuning (SFT). During pre-training, the focus lies on enhancing the model's ability to grasp high-resolution images and adapt them for GUI applications by emphasising text recognition, visual grounding, and understanding GUI imagery. Various public datasets serve as pre-training resources, covering synthetic renderings, academic documents, and optical character recognition (OCR) images. After completing the pre-training stage, SFT uses the GUIDE dataset, a specially curated collection of real-world GUI elements and task-based sequences. Through fine-tuning, V-Zen learns from complex workflows, action histories, and negative samples, gaining proficiency in making accurate inferences and performing pertinent actions on previously unencountered GUIs. Training benefits from NVIDIA's $8 *$ A100 platform and utilises the DeepSpeed library for optimal speed while applying the Adam optimiser, a learning rate of 0.00001 , a batch size of 8 , and a gradient accumulation step of 1 to maintain steady learning progression.

### 4.2 GUIDE Dataset

The GUIDE (Graphical User Interface Data for Execution) [5] dataset is a largescale, meticulously curated dataset developed specifically to enhance the applications of Multimodal Large Language Models (MLLMs), with a particular focus on Robotic Process Automation (RPA) use cases. The dataset, which comprises 124,000 data points, authentically represents user interactions within various GUI environments and covers a diverse array of fields, online platforms, and activities. It includes data from popular GUI platforms such as Apollo.io, Contlo, Gmail, Google Calendar, and Canva. Each data entry in GUIDE consists of an image, a task description, the last action taken, and the next action to be performed, along with grounding information indicating where the action needs to be executed. Furthermore, the dataset incorporates a Chain of Thought (CoT),

## Prompt

Task: Write an email to Kevin at

kevin@gmail.com asking him about the update on the multimodal model.

Previous Action: TYPE: Type the email content in the content box button Give me the next action?

![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-10.jpg?height=293&width=523&top_left_y=580&top_left_x=473)

Response

Reasoning: After writing the content of the mail, the next action is to click on the send button.

CLICK: Click on send $[0.80,0.66,0.96,0.98]$ button

![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-10.jpg?height=477&width=545&top_left_y=409&top_left_x=1126)

Response

Reasoning: After Double clicking on the bounded text box tab, the next action is to type WealthWise in the bounded text box tab. TYPE: Type WealthWise in the bounded text box $[0.52,0.63,0.77,0.69]$ tab.

Fig. 4: Some samples of the GUIDE dataset: Notice how the next action is predicted along with the bounding box locations, demonstrating the dataset's utility in guiding Multimodal Large Language Models for GUI automation tasks.

preserving historical records of earlier actions and promoting contextual reasoning during model operation. The dataset was collected using the authors' in-house advanced annotation tool, NEXTAG (Next Action Grounding and Annotation Tool), and adapted for multiple operating systems, browsers, and display types. It was collated by multiple annotators to capture the variation of design and the way a person uses a website. GUIDE supports investigations into cross-interface automated tasks and encourages the development of multiplatform LLMs for practical applications in automation and natural language understanding. In essence, GUIDE is about predicting the next task on a given GUI image and performing the corresponding grounding task for correctly interacting with GUI elements like boxes, buttons, icons, etc., across a diverse range of platforms.

### 4.3 Results And Discussion

In this section, we delve into the empirical evaluation of our proposed model, VZen, and its performance on the GUIDE dataset. The evaluation focuses on two pivotal tasks: Next Task Prediction and Grounding. For the Next Task Prediction, we assess the model's ability to predict the next action accurately. Specifically, we compare the predicted action with the ground-truth action in terms of semantic meaning. To measure accuracy, we consider an action prediction correct if it aligns with the intended task progression. For grounding, we focus on bounding box localization accuracy. The F1 score, commonly used in object detection tasks, serves as our primary evaluation metric for grounding accuracy. We juxtapose the performance of V-Zen with other state-of-the-art models, namely

CogAgent, GPT-4V, Chatterbox, and Gemini-Pro, under similar experimental conditions to ensure a fair comparison. As delineated in Table 1, V-Zen exhibits superior performance in the Next Task Prediction task, achieving an accuracy of $93.2 \%$. This metric is indicative of V-Zen's proficiency in accurately predicting the subsequent task in a GUI environment, thereby demonstrating its potential in real-world applications. In the context of the Grounding task, V-Zen continues to outperform the other models, as evidenced in Table 3. With a next-task prediction accuracy of $93.2 \%$ and grounding accuracy of $89.7 \%$, V-Zen demonstrates its capability to precisely ground GUI elements, a critical aspect in GUI automation tasks.

These empirical results underscore the efficacy of V-Zen in both tasks, thereby attesting to its robustness and versatility. The success of V-Zen can be attributed to its innovative architecture, which seamlessly integrates low-resolution and high-resolution visual modules, a multimodal projection adapter, and a highprecision grounding module. This intricate design enables V-Zen to effectively process and integrate visual and textual information, thereby enhancing its GUI understanding and grounding capabilities. Furthermore, the use of the GUIDE dataset for specialised fine-tuning has significantly bolstered V-Zen's proficiency in handling real-world GUI elements and task-based sequences. The GUIDE dataset, with its diverse array of GUI environments and task-based sequences, provides a rich resource for training, thereby enabling V-Zen to learn from complex workflows, action histories, and negative samples. In conclusion, the experimental results substantiate the effectiveness of V-Zen in automating GUI tasks, thereby setting a new benchmark in the realm of multimodal large language models for GUI automation. The results presented herein provide a promising direction for future research in this domain. Future work will focus on further enhancing the performance of V-Zen and expanding its applicability to a wider range of GUI platforms.

## 5 Conclusion

In conclusion, this research paper has presented V-Zen, a groundbreaking Multimodal Large Language Model (MLLM) specifically engineered to revolutionise the realm of Graphical User Interface (GUI) understanding and grounding. V-Zen, with its innovative dual-resolution encoding mechanism and dedicated grounding module, has successfully transcended traditional limitations in GUI interaction and interpretation, thereby marking a significant advancement in GUI-centric AI solutions. Our rigorous evaluations have unequivocally demonstrated V-Zen's superior performance over competing models in next-action prediction and grounding tasks, thereby establishing it as a pioneering force in the domain of self-operating computer systems.

Simultaneously, we have introduced GUIDE, a state-of-the-art benchmark dataset meticulously compiled to catalyze advancements in MLLMs, with a particular emphasis on Robotic Process Automation (RPA) applications. GUIDE, with its comprehensive collection of GUI grounding-oriented dialogues and real-
![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-12.jpg?height=444&width=344&top_left_y=458&top_left_x=494)
"GreenTTumb Online Nursery," an e-commerce elaltorm offering a
wide variety of indoor and outdoor plants for sale. Download in JPG text box tab. Give me the next action?

![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-12.jpg?height=38&width=339&top_left_y=954&top_left_x=499)
Uurser" in the text box, the next action is to click on the share butto
proceed with the task of creating the log. CuIck C Click on the to proceed with the task of creating the
share $[0.933,0.099,0.994,0.144]$ button.
![](https://cdn.mathpix.com/cropped/2024_06_04_506ce845a1e4f23aca25g-12.jpg?height=576&width=790&top_left_y=462&top_left_x=858)

Fig. 5: Qualitative Results on GUIDE Samples Using V-Zen. Demonstrates the effectiveness of our developed model in predicting the next actions and bounding box locations for achieving a given task.

istic spatial relationship quandaries, serves as a powerful catalyst propelling the field towards innovative breakthroughs in multimodal AI modeling.

The introduction of V-Zen and GUIDE marks a significant advancement in $\mathrm{AI}$, setting the stage for future developments in this dynamic field. Our contributions aim to inspire future MLLMs, providing them with the tools needed to master GUI automation. We foresee continuous refinement of V-Zen, accommodating a wider range of GUI platforms and real-life complexities. Concurrently, we expect GUIDE to evolve, embracing complex and diverse scenarios to meet the growing demands of the field. Ultimately, we aspire to foster an ecosystem where AI can effectively tackle real-world problems, delivering value and contributing to societal betterment. The successful synthesis of V-Zen and GUIDE opens a new chapter in multimodal AI research, unlocking possibilities for intelligent, autonomous computing experiences. We invite fellow researchers to join us in shaping this exciting frontier, anticipating a future where AI not only enhances human capabilities but also enriches human experiences.

## References

1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., Simonyan, K.: Flamingo: a visual language model for few-shot learning (2022)
2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020)
3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endto-end object detection with transformers (2020)
4. Chawla, R., Datta, A., Verma, T., Jha, A., Gautam, A., Vatsal, A., Chaterjee, S., NS, M., Bhola, I.: Veagle: Advancements in multimodal representation learning (2024)
5. Chawla, R., Jha, A., Kumar, M., NS, M., Bhola, I.: Guide: Graphical user interface data for execution (2024)
6. Chen, C., Qin, R., Luo, F., Mi, X., Li, P., Sun, M., Liu, Y.: Position-enhanced visual instruction tuning for multimodal large language models (2023)
7. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm's referential dialogue magic (2023)
8. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: Palm: Scaling language modeling with pathways (2022)
9. Gur, I., Nachum, O., Miao, Y., Safdari, M., Huang, A., Chowdhery, A., Narang, S., Fiedel, N., Faust, A.: Understanding html with large language models (2023)
10. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Zhang, Y., Li, J., Xu, B., Dong, Y., Ding, M., Tang, J.: Cogagent: A visual language model for gui agents (2023)
11. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, F.: Language is not all you need: Aligning perception with language models (2023)
12. Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.: Mdetr modulated detection for end-to-end multi-modal understanding (2021)
13. Koh, J.Y., Lo, R., Jang, L., Duvvur, V., Lim, M.C., Huang, P.Y., Neubig, G., Zhou, S., Salakhutdinov, R., Fried, D.: Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. ArXiv abs/2401.13649 (2024), https:// api.semanticscholar.org/CorpusID:267199749
14. Koh, J.Y., Salakhutdinov, R., Fried, D.: Grounding language models to images for multimodal inputs and outputs (2023)
15. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models (2023)
16. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)
17. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., Zhang, L.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection (2023)
18. Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., Wu, Z., Zhao, L., Zhu, D., Li, X., Qiang, N., Shen, D., Liu, T., Ge, B.: Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology 1(2), 100017 (Sep 2023). https://doi.org/10.1016/j.metrad.2023.100017, http://dx.doi.org/ 10.1016/j.metrad.2023.100017
19. Ma, X., Zhang, Z., Zhao, H.: Comprehensive cognitive llm agent for smartphone gui automation (2024)
20. Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., Gao, J.: Large language models: A survey (2024)
21. Naveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., Mian, A.: A comprehensive overview of large language models (2024)
22. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H.W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W., Kim, C., Kim, Y., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C.M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O'Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H.P., Michael, Pokorny, Pokrass, M., Pong, V.H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M.B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.F.C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J.J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A.,

Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., Zoph, B.: Gpt-4 technical report (2024)

23. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback (2022)
24. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world (2023)
25. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision (2021)
26. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training techniques for clip at scale (2023)
27. Team, T.V.: Vicuna: An open-source chatbot impressing gpt-4 with $90 \%^{*}$ chatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna/ (2023), accessed: 2024$05-20$
28. Tian, Y., Ma, T., Xie, L., Qiu, J., Tang, X., Zhang, Y., Jiao, J., Tian, Q., Ye, Q.: Chatterbox: Multi-round multimodal referring and grounding (2024)
29. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient foundation language models (2023)
30. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual expert for pretrained language models (2024)
31. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., Dai, J.: Visionllm: Large language model is also an open-ended decoder for vision-centric tasks (2023)
32. Workshop, B., :, Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A.S., Yvon, F., Gallé, M., Tow, J., Rush, A.M., Biderman, S., Webson, A., Ammanamanchi, P.S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A.V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P.O., Sanh, V., Laurençon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A.F., Alfassy, A., Rogers, A., Nitzav, A.K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., van Strien, D., Adelani, D.I., Radev, D., Ponferrada, E.G., Levkovizh, E., Kim, E., Natan, E.B., Toni, F.D., Dupont, G., Kruszewski, G., Pistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., Johnson, I., Gonzalez-Dios, I., de la Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., Werra, L.V., Weber, L., Phan, L., allal, L.B., Tanguy, L., Dey, M., Muñoz, M.R., Masoud, M., Grandury, M., Šaško, M., Huang, M., Coavoux, M., Singh, M., Jiang, M.T.J., Vu, M.C., Jauhar, M.A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O., de Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amuok, P., Lhoest, Q., Harliman, R., Bommasani, R., López, R.L., Ribeiro, R., Osei, S., Pyysalo, S., Nagel, S., Bose, S., Muhammad, S.H., Sharma, S., Longpre, S., Nikpoor, S., Silberberg, S., Pai, S., Zink, S., Torrent, T.T., Schick, T., Thrush, T., Danchev, V., Nikoulina, V., Laippala, V., Lepercq, V., Prabhu, V., Alyafeai, Z., Talat, Z., Raja, A., Heinzerling, B., Si, C., Taşar, D.E., Salesky, E., Mielke, S.J.,

Lee, W.Y., Sharma, A., Santilli, A., Chaffin, A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H., Pandey, H., Strobelt, H., Fries, J.A., Rozen, J., Gao, L., Sutawika, L., Bari, M.S., Al-shaibani, M.S., Manica, M., Nayak, N., Teehan, R., Albanie, S., Shen, S., Ben-David, S., Bach, S.H., Kim, T., Bers, T., Fevry, T., Neeraj, T., Thakker, U., Raunak, V., Tang, X., Yong, Z.X., Sun, Z., Brody, S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H.W., Tae, J., Phang, J., Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J., Ryabinin, M., Mishra, M., Zhang, M., Shoeybi, M., Peyrounette, M., Patry, N., Tazi, N., Sanseviero, O., von Platen, P., Cornette, P., Lavallée, P.F., Lacroix, R., Rajbhandari, S., Gandhi, S., Smith, S., Requena, S., Patil, S., Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A., Ligozat, A.L., Subramonian, A., Névéol, A., Lovering, C., Garrette, D., Tunuguntla, D., Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G.I., Schoelkopf, H., Kalo, J.C., Novikova, J., Forde, J.Z., Clive, J., Kasai, J., Kawamura, K., Hazan, L., Carpuat, M., Clinciu, M., Kim, N., Cheng, N., Serikov, O., Antverg, O., van der Wal, O., Zhang, R., Zhang, R., Gehrmann, S., Mirkin, S., Pais, S., Shavrina, T., Scialom, T., Yun, T., Limisiewicz, T., Rieser, V., Protasov, V., Mikhailov, V., Pruksachatkun, Y., Belinkov, Y., Bamberger, Z., Kasner, Z., Rueda, A., Pestana, A., Feizpour, A., Khan, A., Faranak, A., Santos, A., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi, A., Tammour, A., HajiHosseini, A., Behroozi, B., Ajibade, B., Saxena, B., Ferrandis, C.M., McDuff, D., Contractor, D., Lansky, D., David, D., Kiela, D., Nguyen, D.A., Tan, E., Baylor, E., Ozoani, E., Mirza, F., Ononiwu, F., Rezanejad, H., Jones, H., Bhattacharya, I., Solaiman, I., Sedenko, I., Nejadgholi, I., Passmore, J., Seltzer, J., Sanz, J.B., Dutra, L., Samagaio, M., Elbadri, M., Mieskes, M., Gerchick, M., Akinlolu, M., McKenna, M., Qiu, M., Ghauri, M., Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy, N., Samuel, O., An, R., Kromann, R., Hao, R., Alizadeh, S., Shubber, S., Wang, S., Roy, S., Viguier, S., Le, T., Oyebade, T., Le, T., Yang, Y., Nguyen, Z., Kashyap, A.R., Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh, A., Beilharz, B., Wang, B., Brito, C., Zhou, C., Jain, C., Xu, C., Fourrier, C., Periñán, D.L., Molano, D., Yu, D., Manjavacas, E., Barth, F., Fuhrimann, F., Altay, G., Bayrak, G., Burns, G., Vrabec, H.U., Bello, I., Dash, I., Kang, J., Giorgi, J., Golde, J., Posada, J.D., Sivaraman, K.R., Bulchandani, L., Liu, L., Shinzato, L., de Bykhovetz, M.H., Takeuchi, M., Pàmies, M., Castillo, M.A., Nezhurina, M., Sänger, M., Samwald, M., Cullan, M., Weinberg, M., Wolf, M.D., Mihaljcic, M., Liu, M., Freidank, M., Kang, M., Seelam, N., Dahlberg, N., Broad, N.M., Muellner, N., Fung, P., Haller, P., Chandrasekhar, R., Eisenberg, R., Martin, R., Canalli, R., Su, R., Su, R., Cahyawijaya, S., Garda, S., Deshmukh, S.S., Mishra, S., Kiblawi, S., Ott, S., Sang-aroonsiri, S., Kumar, S., Schweter, S., Bharati, S., Laud, T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y., Bajaj, Y.S., Venkatraman, Y., Xu, Y., Xu, Y., Xu, Y., Tan, Z., Xie, Z., Ye, Z., Bras, M., Belkada, Y., Wolf, T.: Bloom: A 176b-parameter open-access multilingual language model (2023)

33. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., Chen, E.: A survey on multimodal large language models (2024)
34. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity (2023)
35. Zang, Y., Li, W., Han, J., Zhou, K., Loy, C.C.: Contextual object detection with multimodal large language models (2023)
36. Zhang, D., Yu, Y., Li, C., Dong, J., Su, D., Chu, C., Yu, D.: Mm-llms: Recent advances in multimodal large language models (2024)
37. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Liu, Y., Chen, K., Luo, P.: Gpt4roi: Instruction tuning large language model on region-of-interest (2023)
38. Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., Kang, B.: Bubogpt: Enabling visual grounding in multi-modal llms (2023)
