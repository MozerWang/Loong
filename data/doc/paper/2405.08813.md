# CinePile: A Long Video Question Answering Dataset and Benchmark 

Ruchit Rawal * Khalid Saifullah * Ronen Basri * David Jacobs<br>Gowthami Somepalli ${ }^{\star}$ Tom Goldstein ${ }^{\star}$<br>University of Maryland, College Park Weizmann Institute of Science

https://hf.co/datasets/tomg-group-umd/cinepile

![](https://cdn.mathpix.com/cropped/2024_06_04_0ee5f3ea4d433ed90c9dg-01.jpg?height=667&width=1385&top_left_y=976&top_left_x=370)

Figure 1: A sample clip (from here) and corresponding MCQs from CinePile.


#### Abstract

Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data. Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene. Additionally, we evaluate recent video-centric LLMs, both open-source and proprietary, on the test split of our dataset. The findings reveal that even state-of-the-art video-centric LLMs significantly lag behind human performance in these tasks, highlighting the complexity and challenge inherent in video understanding.


[^0]
## 1 Introduction

Large multi-modal models offer the potential to analyze and understand long, complex videos. However, training and evaluating models on video data offers difficult challenges. Most videos contain dialogue and pixel data and complete scene understanding requires both. Furthermore, most existing vision-language models are pre-trained primarily on still frames, while understanding long videos requires the ability to identify interactions and plot progressions in the temporal dimension.

In this paper, we introduce CinePile, a large-scale dataset consisting of $\sim 305 \mathrm{k}$ question-answer pairs from 9396 videos, split into train and test sets. Our dataset emphasizes question diversity, and topics span temporal understanding, perceptual analysis, complex reasoning, and more. It also emphasizes question difficulty, with humans exceeding the best commercial vision/omni models by approximately $26 \%$, and exceeding open source video understanding models by $70 \%$.

We present a scene and a few question-answer pairs from our dataset in Fig. 1. Consider the first question, How does Gru's emotional state transition throughout the scene? For a model to answer this correctly, it needs to understand both the visual and temporal aspects, and even reason about the plot progression of the scene. To answer the second question, What are the objects poking out of the book cover and what is their purpose, the model must localize an object in time and space, and use its world knowledge to reason about their purpose.

CinePile addresses several weaknesses of existing video understanding datasets. First, the large size of CinePile enables it to serve as both an instruction-tuning dataset and an evaluation benchmark. We believe the ability to do instruction tuning for video at a large scale can bridge the gap between the open-source and commercial video understanding models. Also, the question diversity in CinePile makes it a more comprehensive measure of model performance than existing benchmarks. Unlike existing metrics (see Section 6 for detailed comparisons), CinePile does not over-emphasize on purely visual questions (e.g., What color is the car?), or on classification questions (e.g., What genre is the video?) that do not require temporal understanding. Rather, CinePile is comprehensive with diverse questions about vision, temporal, and narrative reasoning with a breakdown of question types to help developers identify blind spots in their models.

The large size of CinePile is made possible by our novel pipeline for automated question generation and verification using large language models. Our method leverages large existing sets of audio descriptions that have been created to assist the vision impaired. We transcribe these audio descriptions and align them with publicly available movie video clips from YouTube. Using this detailed human description of scenes, powerful LLMs are able to create complex and difficult questions about the whole video without using explicit video input. At test time, video-centric models must answer these questions from only the dialogue and raw video, and will not have access to the hand-written descriptions used to build the questions. Dataset and artifacts are available at this https://ruchitrawal.github.io/cinepile.

## 2 Creating a long video understanding benchmark

Our dataset curation process has four primary components 1) Collection of raw video and related data. 2) Generation of question templates. 3) Automated construction of the question-answer dataset using video and templates, and 4) Application of a filtering pipeline to remove malformed questions.

### 2.1 Data collection and consolidation

We obtain clips from English-language films from the YouTube channel MovieClips ${ }^{1}$. This channel hosts self-contained clips, each encapsulating a major plot point, facilitating the creation of a dataset focused on understanding and reasoning. Next, we collected Audio Descriptions from AudioVault ${ }^{2}$. Lastly, we collect movie information for each scene, such as genre, actors, and main plot from IMDB ${ }^{3}$.[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_0ee5f3ea4d433ed90c9dg-03.jpg?height=393&width=1399&top_left_y=237&top_left_x=360)

Figure 2: Question template generation pipeline: We begin by substituting the first names in human-written source questions and then cluster them. We then feed a selection of questions from each cluster into GPT-4 which in turn outputs "question templates" which are used in the next stage of dataset creation. See Section 2.2 for more details.

Getting visual descriptions of video for free. Audio descriptions (ADs) are audio tracks for movies that feature a narrator who explains the visual elements crucial to the story during pauses in dialogue. They have been created for many movies to assist the vision impaired. The key distinction between conventional video caption datasets and ADs lies in the contextual nature of the latter. In ADs, humans emphasize the important visual elements in their narrations, unlike other video caption datasets, which tend to be overly descriptive. We use the audio descriptions as a proxy for visual annotation in the videos for our dataset creation.

Scene localization in AD. The video clips we have gathered are typically 2-3 minutes long, while Audio Descriptions (ADs) cover entire movies. To align descriptions with video, we transcribe the audio from both the movie clip and the whole movie AD file using an Automatic Speech Recognition (ASR) system WhisperX [4], an enhanced version of Whisper [22] designed to offer quicker inference and more precise word-level timestamps. We then embed the first 3 and last 3 lines of the text transcription of a YouTube movie clip using a sentence embedding model, WhereIsAI/UAE-Large-V1. We similarly embed all the sentences in the corresponding movie AD file. We then localize the YouTube clip within the $\mathrm{AD}$ file via the rolling window algorithm. We then extract all $\mathrm{AD}$ data that lies between the matched start and end of the movie clip embeddings. This localized text contains both the visual elements and the dialogue for the given YouTube clip. This serves as a base text for creating the QA dataset which we will discuss in the rest of the paper.

For the rest of the paper, we will refer to the human-written description of the scene as "visual description" and the speaking or dialogue part of the video as "dialogue". When combined, we will refer to both data sources as "scene-text-annotation".

Sentence classification. When we transcribe an $\mathrm{AD}$ file, the text contains a human's visual descriptions and the movie's dialogue. However, the transcription model does not label whether a given sentence belongs to a visual description or a dialogue. Since we planned to create a few questions solely on the visual components of the video, the distinction is important to us. To categorize each sentence as either visual or dialogue, we fine-tuned a BERT-Base model [7] using annotations from the MAD dataset [25], which contains labels indicating whether a sentence is a dialogue or a visual description. We applied a binary classification head for this task. For training the classification model, we split the MAD dataset annotations into an 80-20 training-evaluation split. The model achieves $96 \%$ accuracy on eval split after 3 epoch training. Qualitatively, we observed that the model accurately classifies sentences in the data we curated, distinguishing effectively between dialogue and visual description content.

### 2.2 Automated Question Templates

Many prominent video question-answering benchmarks were written by human annotators. The question-answer pairs are typically curated in one of two ways: 1) Human annotators are given complete freedom to ask questions about a given scene [28] 2) They are asked to focus on specific aspects and are trained or provided with examples of questions, encouraging them to write more questions in a similar style [11, 13, 21, 37]. For instance, in the Perception Test Benchmark [21], annotators are

```
Question Template Automation
    Generate a few templates based on the following questions
    [Question 1] Where did the couple meet?
    [Question 2] What holiday is coming up when they meet?
    [Question 3] What made him break with her?
    Response Based on the nature of these questions, I can identify a few underlying templates that encompass most of them:
    [Template 1] Catalytic Actions
    [Proto Question 1] What event acts as a catalyst for the character's next major decision?
    [Template 2] Setting and Context
    [Proto Question 2] Where does this interaction take place, and how does the location impact the conversation?
$\cdots$
```

Figure 3: Extracting templates from human-generated questions. We share 10 samples from each question cluster, and prompt an LLM to create a few templates and a prototypical question. See Fig. 2 and Section 2.2 for more details.

directed to concentrate on temporal or spatial aspects, while for the Next-QA dataset [37], annotators mainly focused on temporal and causal action reasoning questions.

During early experimentation, we found that providing a range of templates along with scene-textannotation to an LLM helped it create more detailed, diverse, and well-formed questions, so we decided to use a template-based approach for question generation. Rather than confining questions to a few hand-curated themes, we propose a pipeline to create question templates naturally on top of human-generated questions. The automated question template generation pipeline is illustrated in Fig. 2 .

Our starting point is approximately 30,000 human-curated questions from the MovieQA [28], TVQA [11], and Perception Test [21] datasets. We cluster these questions, select a few representatives per cluster, and then use GPT-4 to discern the underlying themes and write a prompt. First, we preprocess the questions by replacing first names and entities with pronouns, as BERT [24] embeddings over-index on proper nouns, hence the resultant clusters end up with shared names rather than themes. For instance, 'Why is Rachel hiding in the bedroom?' is altered to 'Why is she hiding in the bedroom?'. We used GPT-3.5 to do this replacement, as it handled noun replacement better than many open-source and commercial alternatives. The modified questions are then embedded using WhereIsAI/UAE-Large-V1, a semantic textual similarity model which is a top performer on the MTEB leaderboard ${ }^{4}$. When the first names were replaced, we observed significant repetition among questions, which prompted us to duplicate them, ultimately resulting in 17,575 unique questions. We then perform k-means clustering to categorize the questions into distinct clusters. We experimented with different values of $k=10,50,100$. Qualitatively, we found $k=50$ to be an optimal number of clusters where the clusters are diverse and at the same time clusters are not too specific. For example, we see a 'high-school dance' specific question cluster when $k=100$, and these questions are merged into an 'event' cluster when we reduce the number of clusters to 50. The Perception Test questions are less diverse as human annotators were restricted to creating questions based on a small number of themes, so we used $k=20$ for this set.

The number of questions in each cluster varied, with counts ranging from 60 to 450 . We selected 10 random questions from each, and used them to prompt GPT-4 to create relevant question templates, as illustrated in Fig. 3. We also ablated the scenario of taking the closest 10 questions to the cluster center, we however qualitatively observed that random questions produced more general/ higher quality templates.

We generate four templates for each question cluster, resulting in around 300 templates across three datasets. We then manually reviewed all 300 templates, eliminating those that were overly specific and merging similar ones. Overly specific templates and their proto-questions looked like "Prewedding Dilemmas: What complicates character Z's plans to propose marriage to their partner?" and "Crime and Consequence: What is the consequence of the character's criminal actions?". The authors also added a few templates that are compli-[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_0ee5f3ea4d433ed90c9dg-05.jpg?height=352&width=1390&top_left_y=244&top_left_x=365)

Figure 4: Automated QA Generation and Filtering Our process begins with a set of automated templates and scenes. Initially, we filter out the templates relevant to each scene. Next, we pass these templates along with the annotated-scene-text to GPT-4, which is then used to create multiple-choice questions (MCQs). The generated MCQs are then subjected to numerous filters to curate the final dataset. For more detailed information, refer to Section 2.3 and Section 2.4

mentary to the auto-generated ones. This process resulted in 86 unique templates. Following that, we manually binned these into four high-level categories: Character and Relationship Dynamics, Narrative and Plot Analysis, Thematic Exploration and Setting, and Technical Analysis. For a detailed discussion on each of these question template categories, please refer to the Appendix. We present two sample question templates per category and their prototypical questions in Table 1.

Table 1: Sample templates and prototypical questions from each of the categories

| Category | Question template | Prototypical question |
| :--- | :--- | :--- |
| Character and Relationship <br> Dynamics (CRD) | Interpersonal Dynamics | What changes occur in the relationship <br> between person A and person B follow- <br> ing a shared experience or actions? |
| Character and Relationship <br> Dynamics (CRD) | Decision Justification | What reasons did the character give for <br> making their decision? |
| Narrative and Plot Analysis <br> (NPA) | Crisis Event | What major event leads to the character's <br> drastic action? |
| Narrative and Plot Analysis <br> (NPA) | Mysteries Unveiled | What secret does character A reveal <br> about event B? |
| Setting and Technical Analy- <br> sis (STA) | Physical Possessions | What is [Character Name] holding? <br> look like [during/at] [specific time/- <br> place/event]? |
| Setting and Technical Analy- <br> sis (STA) | Environmental Details | Are there any symbols or motifs intro- <br> duced in Scene A that reappear or evolve <br> in Scene B, and what do they signify? |
| Thematic Exploration (TH) | Symbolism and Motif Track- <br> ing | What does the chaos in the scene parallel <br> in terms of the movie's themes? |
| Thematic Exploration (TH) | Thematic Parallels |  |

### 2.3 Automated QA generation with LLMs

The pipeline for generating questions is shown in Fig. 4. While the question templates are general, they might not be relevant to all the movie clips. Hence for a given scene, we choose a few relevant question templates. To do this, we provide Gemini with the scene-text-annotation of the scene, and ask the model to shortlist the 20 most relevant templates to that scene, out of which we randomly select 5-6 templates.

We then provide a commercial language model with (i) the scene-text-annotation, which includes both visual descriptions and dialogue, (ii) the selected question template names (e.g. 'Physical Possession'), (iii) the prototypical questions for the templates (e.g. "What is [Character Name] holding"), and (iv) a system prompt asking it to write questions about the scene. Through rigorous experimentation, we devised a system prompt that makes the model attentive to the entire scene and
is capable of generating deeper, longer-term questions as opposed to mere surface-level perceptual queries. We observed that providing the prototypical example prevents GPT-4 from hallucination, and also leads to more plausible multiple-choice question (MCQ) distractors. We also found that asking the model to provide rationale for its answer enhances the quality of the questions. Additionally, we found that including timestamps for dialogues and visual descriptions augments the quality of generated temporal questions. Through this method, we were able to generate $\approx 32$ questions for each video in the dataset.

After experimenting with this pipeline, we analyzed the generated QA pairs and noticed a consistent trend: most questions are focused on reasoning or understanding. For diversity, we also wanted to include purely perceptual questions. To achieve this, we introduced additional hand-crafted prompt templates for perceptual questions and also templates for temporal questions. While GPT-4 performs well across all question templates, we found that Gemini excels particularly with perceptual templates. Therefore, we utilized Gemini to generate a segment of perceptual questions in the dataset, while using GPT-4 for reasoning templates. Our experiments with open-source models indicated subpar question quality, despite extensive prompt tuning. We present example questions generated by GPT- 4 and Gemini, along with their templates, in the Appendix.

A small proportion of questions produced by this pipeline can be answered without referring to the clip, such as What's the color of the blonde woman's hair?. We implemented a few checks to eliminate such trivial or poorly framed questions. We discuss these checks and a few axes of evaluating the question-answering dataset in the following section.

### 2.4 Testing the quality of the dataset

While the process above consistently produces well-formed and answerable questions, we observed that some questions are either trivial, with answers embedded within the question itself, or pertaining to basic world concepts that do not require viewing the clip. To prune these, we evaluated our dataset with the help of a few LLMs on the following axes and we either removed the questions from the dataset or computed a metric that the users can use in the downstream tasks.

Degeneracy. A question is considered degenerate if the answer is implicit in the question itself, e.g., What is the color of the pink house?. In our dataset, these types of questions constitute only a small fraction. Manually reviewing all questions being impractical, we employed three distinct language models (LMs) to automate this process: Gemini [29], GPT-3.5 [1], and Phi-1.5 [14]. These models vary in their underlying training data and sizes. We presented only the questions and choices to these models, omitting any context, and calculated the accuracy of each question across the multiple models. If all models correctly answer a question, it is likely to be degenerate. We excluded degenerate questions from the evaluation split of our dataset but retained them in the training set.

Vision Reliance. When generating the multiple-choice questions (MCQs), we considered the entire scene without differentiating between visual text and dialogue. Consequently, some questions in the dataset might be answerable solely based on dialogue, without the necessity of the video component. For this analysis, we utilized the Gemini model. The model was provided with only the dialogue, excluding any visual descriptions, to assess its performance. If the model correctly answers a question, it is assigned a score of 0 for the visual dependence metric; if it fails, the score is set at 1 . In later sections, we present the distribution of the visual dependence scores across different MCQ categories.

Hardness. We developed a metric to gauge the difficulty of questions for the models, even when provided with full context. For this purpose, we selected the Gemini model, given its status as one of the larger and more capable models. This metric differs from accuracy; for accuracy computation, the models are only supplied with videos and dialogue information, excluding visual descriptions. However, in calculating the hardness metric, we include visual descriptions as part of the context given to the model.

In addition to this, authors regularly verified the quality of questions across multiple scenes and fixed any systemic errors that arise in the pipeline. Next, we discuss a human study we conducted on our dataset and some of the findings.

Distractor similarity

Q1. What is the state of Snake's vehicle during the scene? Answer: it's exploding

Problem: there's another option that could also be correct in the context of the scene -- "it's damaged"

Q2. What does Sean ask his mother to do for him? Answer key: To act like a normal, loving parent.

Problem: It's hard to answer since another option "To stop acting like a lunatic." might seem plausible on surface, but really isn't if you watch the scene carefully

## Confusing Characters

Q3. What happens immediately after Antonio tells Kathy that he loves her?

Answer: Kathy tells Antonio that she loves him too.

Problem: Actually Kathy says I love you and Anotonio says I love you too. The subtitles doesn't have speaker information: <subtitle> 4400.3984400 .938 I love you.

<subtitle> 4400.9584402 .899 I love you, too.

Q4. What happens after the character mentions that her child, Kimi, is almost two years old?

Answer key: She says that her child is not a girl

Problem: Another character says that their child is not a girl

Figure 5: Sample failure cases from human study: We conducted a human study to check the quality of questions and we found a few systemic issues. We fixed all systemic issues in the final version of the dataset. The movie clip for Q1 can be found here; for Q2, here; for Q3, here; and for Q4, here.

Table 2: We compare our dataset, CinePile against the pre-existing video-QA datasets. Our dataset is both large and diverse. Multimodal refers to whether both the video and audio data is used for question creation and answering. For understanding different QA types, refer to Section 2.3

| Dataset | Annotation | Num QA | Avg sec | Multimodal | QA Type |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | Temporal | Attribute | Narrative | Theme |
| TGIF-QA [10] | Auto | 165,165 | 3 | $x$ | $\checkmark$ | $x$ | $x$ | $x$ |
| MSRVTT-QA [39] | Auto | 243,690 | 15 | $x$ | $x$ | $\checkmark$ | $x$ | $\hat{x}$ |
| How2QA [13] | Human | 44,007 | 60 | $x$ | $\checkmark$ | $\checkmark$ | $x$ | $x$ |
| NExT-QA [37] | Human | 52,044 | 44 | $x$ | $\checkmark$ | $\checkmark$ | $x$ | $x$ |
| EgoSchema [19] | Auto | 5,000 | 180 | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ |
| MovieQA [28] | Human | 6,462 | 203 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ |
| TVQA [11] | Human | 152,545 | 76 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ |
| Perception Test [21] | Human | 44,000 | 23 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ | $x$ |
| MoVQA [44] | Human | 21,953 | 992 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ |
| CinePile (Ours) | Human + Auto | 303,828 | 160 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |

## 3 Human Study

Once we finalized the pipeline and generated a reasonable number of questions, we conducted a small human study to help us find weaknesses or systematic problems with the dataset. Each participant (graduate student volunteers) answered 10 questions about two different randomly chosen videos. We randomly sampled the 10 questions from the list of all the questions we generated for the scene. We had 25 participants excluding authors. We also interviewed each participant after the survey to ask if they found any systematic issues in any of the questions they were asked to answer about the video. Later, a panel of authors audited all questions where humans got the answer wrong. We noticed that most of the time when a human got a question wrong it was likely due to one of the following reasons (i) due to their inability to attend over the entire clip at once, (ii) due to their inability to understand the dialogue or understand cultural references (iii) carelessness in answering, as the correct answer was indeed present in the video. We did notice some problematic patterns with a small subset of questions. The main issue is distractor similarity, where humans found two plausible answers and they chose one randomly. We present a few such examples in Fig. 5. We removed the questions from the test set for which we found ambiguous answers.

We again conducted a second human study on the test set's final version, and the human accuracy is $73 \%$. The authors have independently taken the survey, and the corresponding accuracy is $86 \%$. Once again, a careful investigation by a team of authors indicates that even most of these wrong answers are due to human error and confusion over the many events in a scene. We conclude from this study that many of the questions are answerable but difficult. We present the question category-level performance in Section 5.

## 4 A look at the dataset

In the initial phase of our dataset collection, we collected $\sim 15,000$ movie clips from channels like MovieClips on YouTube. We filtered out clips that did not have corresponding recordings from
![](https://cdn.mathpix.com/cropped/2024_06_04_0ee5f3ea4d433ed90c9dg-08.jpg?height=370&width=1392&top_left_y=240&top_left_x=366)

Figure 6: Test split statistics. Left: Question category composition in the dataset. Middle: Percentage of vision-reliant questions across categories. Right: Percentage of hard questions per question category type. TEMP - Temporal, CRD - Character and Relationship Dynamics, NPA - Narrative and Plot Analysis, STA Setting and Technical Analysis, TH - Thematic Exploration. The colors correspond to the same categories across the plots. Refer to the Appendix for corresponding plots of train split.

Audiovault, as our question generation methodology relies on the integration of visual and auditory cues-interleaved dialogues and descriptive audio-to construct meaningful questions. We also excluded clips with low alignment scores when comparing the YouTube clip's transcription with the localized scene's transcription in the Audio Description (AD) file as discussed in Section 2.1. This process resulted in a refined dataset of 9396 movie clips. The average video length in our dataset is ã€œ160 sec, significantly longer than many other VideoQA datasets and benchmarks.

We split 9396 videos into train and test splits of 9248 and 148 videos each. We made sure both the splits and the sampling preserved the dataset's diversity in terms of movie genres and release years. We follow the question-answer generation and filtering pipeline which was thoroughly outlined in Section 2. We ended up with 298,888 training points and 4,940 test-set points with around 32 questions per video scene. Each MCQ contains a question, answer, and four distractors. As a post hoc step, we randomized the position of the correct answer among the distractors for every question, thus eliminating any positional bias. We filtered out the degenerate questions from the test split, however, we left them in the train set, since those questions are harmless and might even teach smaller models some helpful biases the larger multimodal models like Gemini might inherently possess.

Our dataset is large and varied because we used a wide variety of movie clips and different prompting strategies about diverse question types. Each strategy zeroes in on particular aspects of the movie content. We present a scene and example MCQs from different question templates in Fig. 1. Please refer to the Appendix for other examples. In Fig. 6 (Left), we provide a visual breakdown of the various categories of questions generated in our dataset. A significant portion of the questions falls under "Character Relationship Dynamics". This is attributed to the fact that a large number of our automated question templates, which were derived from human-written questions belonged to this category. This is followed by "Setting and Technical Analysis" questions, which predominantly require visual interpretation.

We display the metrics for vision reliance and question hardness, as discussed in Section 2.4, at the category level in Fig. 6 (Middle, Right). As anticipated, questions in the "Setting and Technical Analysis" category exhibit the highest dependency on visual elements, followed by those in "Character Relationship Dynamics", and "Temporal" categories. In terms of the hardness metric, the "Temporal" category contains the most challenging questions, with "Thematic Exploration" following closely behind.

Finally, we compare our dataset with other existing datasets in this field in Table 2, showing its superiority in both the number of questions and average video length compared to its counterparts.

Test split. As mentioned previously, our test split comprises 148 video clips, derived from movies distinct between training and testing to avoid information leakage. Additionally, we have eliminated all degenerate questions from this split, which constituted $4.5 \%$ of the generated questions. Following several rounds of manual cleanup and thorough testing, our final count stands at 4940 questions. Of all the test questions, $33.21 \%$ are reliant on visual information.
Human errors
Q1. What is
the scene?

Answer: Sean confronts his mother about her past choice Participant Response: Sean asks his mother for help with his colleg application

Plausible reason for error: Sean does ask help with college application much later during the scene, maybe the participants have a recency bias, or they didn't pay attention to the operative word "initial" in the question.

Q2. What is the first thing Antonio does after revealing the content of the letter from his mother?

Answer key: He hangs his head

Participant Response: He gazes out at the water

Plausible reason for error: For the vast majority of the scene, Antonio is indeed gazing at the water. But after he finishes the relevant content of the letter, the scene cuts to Antonio hanging his head.

GPT-4 errors

Q3. What is the sequence of events that Antonio narrates to Parker while they sit on the dock?

Answer: Antonio's father told him about a letter, Antonio refused to see it, and then his father threw it away. Model Response: Antonio found a letter from his mother, read it, and then his father threw it away

Plausible reason for error: The wording of Answer and Model Response may seem the same, but there's key difference that makes the model response incorrect.

Q4. What does the chaos caused by the fiery beast parallel in terms of the movie's themes?

Answer: The unpredictability of scientific experiments Model Response: The recklessness of youth

Plausible reason for error: The model gets influenced by a slightly related scene that talks about being an "adult".

Figure 7: Hard questions according to humans and GPT-4 V: After conducting the human study, we looked at the questions which human got wrong and the questions which GPT-4 got wrong. Some of these questions are difficult and can only be answered by paying careful attention to the video. The movie clip for Q1 can be found here; for Q2 and Q3, here; and for Q4, here.

## 5 Model evaluation

In this section we discuss the evaluations of various closed and open source video LLMs on our dataset, some challenges, and the model performance trends. Given that our dataset is of type multiple-choice question answers (MCQs), we evaluate a given model's performance on our benchmark questions by measuring its ability to accurately select the right answer from a set of multiple-choice options, containing only one correct answer and four distractors. One key challenge in this process is reliably parsing the model's response to extract its chosen answer, and mapping it to one of the predefined answer choices. Model's responses may vary in format, including additional markers, or may only contain the option letter, or have a combination of the option letter and its corresponding text, etc. Such variations necessitate a robust post-processing step to accurately extract and match the model's response to the correct option.

Table 3: Model Evaluations. We present the accuracy of various video LLMs on the CinePile's test split. We also present Human performance for comparison. We also ablate the accuracies across the question categories we discussed earlier. TEMP - Temporal, CRD - Character and Relationship Dynamics, NPA - Narrative and Plot Analysis, STA - Setting and Technical Analysis, TH - Thematic Exploration.

| Model | Average | CRD | NPA | STA | TEMP | TH |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Human | 73.21 | 82.92 | 75.00 | $\mathbf{7 3 . 0 0}$ | 75.52 | 64.93 |
| Human (authors) | $\mathbf{8 6 . 0 0}$ | $\mathbf{9 2 . 0 0}$ | $\mathbf{8 7 . 5}$ | $\underline{71.20}$ | $\mathbf{1 0 0}$ | $\mathbf{7 5 . 0 0}$ |
| GPT-4o [20] | 59.65 | 66.54 | 77.22 | 52.76 | 42.39 | 62.33 |
| GPT-4 Vision [1] | 58.04 | 65.37 | 80.97 | 47.42 | 42.01 | 70.37 |
| Gemini 1.5 Pro [23] | 59.08 | 63.44 | 63.88 | 59.52 | 37.50 | 68.42 |
| Gemini Pro Vision [29] | 50.10 | 55.52 | 66.11 | 45.02 | 34.05 | 53.24 |
| Claude 3 (Opus) [3] | 44.72 | 49.64 | 61.11 | 38.86 | 32.60 | 44.87 |
| mPLUG-Owl [40] | 10.72 | 10.95 | 10.98 | 10.66 | 10.35 | 9.12 |
| Video-ChatGPT [17] | 15.44 | 17.31 | 15.05 | 15.79 | 7.14 | 23.38 |
| MovieChat [26] | 4.61 | 4.95 | 4.29 | 5.23 | 2.48 | 4.21 |

Our evaluation method incorporates a two-stage process to address these variations. In the first stage, we employ a normalization function to parse the model's response, extracting the option letter (A-E) and the accompanying option text if present. This normalization handles various response formats, such as direct letter responses (e.g., "A") or more verbose forms (e.g., "Answer: D, The Eiffel Tower"), ensuring that both the option letter and text are accurately identified, if present. Following normalization, the process enters its second stage, which entails comparing the normalized model response with the correct answer key. This comparison involves checking for both the option letter and text in the model response. If both elements are present and match the answer key, a score of one is awarded. However, if only the option letter or text appears, the comparison is limited to the relevant part, and the score is assigned accordingly.

We evaluate various commercial and open-source LLM models and we present their performance in Table 3. We also present human numbers (author and non-author) for comparison. This distinction is important because the authors carefully watched the video (go back and rewatch the video if necessary) while answering the questions. This removes the carelessness errors from the human study. On average, VLM models both commercial and OSS, are behind human performance on our dataset. While commercial VLMs perform reasonably well, the OSS models perform quite poorly showing the gap in their capabilities. We present a few QA's which humans got wrong and GPT-4 got wrong and the plausible reason for errors in Fig. 7.

Among the various commercial VLMs analyzed, GPT-4o, GPT-4 Vision and Gemini 1.5 Pro (Video) emerge as top performers, each achieving around $60 \%$ average performance. ${ }^{5}$ Gemini 1.5 Pro particularly outperforms the GPT-4 models in the "Setting and Technical Analysis" category that is dominated by visually reliant questions focusing on the environmental and surroundings of a movie scene, and its impact on the characters. On the contrary, we note that GPT-4 models performs substantially better on other question categories such as "Narrative and Plot Analysis" that revolve around the core storylines, and interaction between the key characters. It's important to note that Gemini 1.5 Pro is designed to handle long multimodal contexts, and while GPT-4o is also a natively multimodal model, GPT-4V only handles 10 frames per video in our evaluation setup. When compared under the same conditions of 10 frames, Gemini Pro Vision, another model in the Gemini series, performs $10 \%$ lower compared to GPT-4V. Meanwhile, Claude 3 (Opus) ranks as the least effective model commercial VLM, trailing Gemini Pro Vision by $\approx 5 \%$. Additionally, as discussed in Sec. 2.4, we provide a "hard split" in the test set consisting of particularly challenging questions. In Fig. 8, we compare the performance of various models on both the average and the hard splits of our dataset. We note that while each model suffers a performance decline of $15 \%-20 \%$ on the hard split; however, the relative ranking among the models remains unchanged, with the exception of Gemini 1.5 Pro outperforming the GPT-4 models. While a noticeable gap persists between human performance and that of state-of-the-art VLMs, OSS models trail significantly to both. To better understand these trends, we conducted a qualitative analysis of the raw responses generated by the OSS models. This investigation aimed to determine whether the deficiencies of these models were as significant as presumed, or if there are other factors that might be influencing the overall score.

Why are OSS models so far behind? We observed that a key reason for the subpar performance of OSS models is their inability to adhere to the given instructions strictly. For instance, in our evaluation prompts, we explicitly instruct the models to only output the option marker in their responses. However, the OSS models frequently deviated from this instruction, generating excessive and sometimes irrelevant text e.g., including unnecessary personal opinions, rephrasing the answer options, mentioning choices not listed in the provided options, and producing completely unrelated text such as repeating lines from subtitles or attempting to complete scenes from movies. We present a few examples in Tab. 4. We solve some of these issues by implementing a substring matching module within our evaluation pipeline, which searches for the correct answer key within the model's response, only considering those responses that mention a single choice. However, this modification is unable to reliably evaluate scenarios where the model outputs a rephrased version of the text. Hence, we also computed traditional video-caption evaluation metrics like BertScore [45], CIDEr [31], ROUGE-L [15], and SPICE [2] for open-source models. We present these results in the Appendix, and find that the relative ranking between the models remains unchanged. We hope training the OSS models on our training data can help bridge the gap in the performance between OSS models and commercial ones in the future.

## 6 Related Work

### 6.1 Video understanding and question answering.

LVU [36], despite being one of the early datasets proposed for long video understanding, barely addresses the problem of video understanding as the main tasks addressed in this dataset are year, genre classification or predicting the like ratio for the video. A single frame might suffice to answer[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_0ee5f3ea4d433ed90c9dg-11.jpg?height=458&width=1391&top_left_y=243&top_left_x=367)

Figure 8: Models' performance on CinePile test split, all questions vs hard questions.

Table 4: Analyzing raw responses generated by OSS models, scores assigned by our evaluation pipeline, and corresponding failure modes for the movie clip: Area 51 (2015) - Sneaking Onto the Base Scene (4/10)

| Question | Answer <br> Key | Model <br> Response | Assigned <br> Score | Failure <br> Mode |
| :---: | :---: | :---: | :---: | :---: |
| How does the emotional tone transi- <br> tion during the scene? <br> - A) From despair to hope <br> - B) From fear to acceptance <br> - C) From confusion to understand- <br> ing <br> - D) From tension to panic <br> - E) From anxiety to excitement | From anx- <br> iety to ex- <br> citement | "5, 4, 3, 2, <br> 1.\n All right, <br> I think I got <br> it, guys. \n You <br> did it? $\ldots$ n . . . <br> $\ldots .$. <br> This is the <br> base." | 0 | The model doesn't <br> choose any of the op- <br> tions, rather just re- <br> peats all the subtitles. |
| What action does Darren take after <br> observing the broken sensor? <br> - A) Calls for help <br> - B) Repairs it <br> - C) Panics <br> - D) Ignores it <br> - E) Suggests next steps | Suggests <br> next steps | "\n ) Darren <br> calls for help." | 0 | While we are able <br> to reliably extract <br> the choice present, <br> it doesn't match the <br> answer key. |
| Where is Jelena in relation to the <br> helicopter at the end of the scene? <br> - A) Inside <br> - B) In front <br> - C) To the side <br> - D) Directly underneath <br> - E) On it | Directly <br> under- <br> neath | Jelena is <br> standing in <br> front of the <br> helicopter at <br> the end of the <br> scene. | 0 | While we are able <br> to reliably extract <br> the choice present, <br> it doesn't match the <br> answer key. |
| How do Darren and Reed first inter- <br> act with each other in the scene? <br> - A) They argue <br> - B) They fight <br> - C) They exchange information <br> - D) They greet each other <br> - E) They ignore each other | They <br> exchange <br> informa- <br> tion | In the scene, <br> Darren and Reed <br> are seen walking <br> together, and <br> they exchange <br> information with <br> each other. | 1 | We are able to extract <br> the response, and it <br> matches the answer <br> key. |

the questions and these tasks cannot be considered quite as "understanding" tasks. MovieQA [28] is one of the first attempts to create a truly understanding QA dataset, where the questions are based on entire plot the movie but not localized to a single scene. On closer examination, very few questions are vision focused and most of them can be answered just based on dialogue. EgoSchema [19] is one of the recent benchmarks, focused on video understanding which requires processing long enough segments in the video to be able to answer the questions. However, the videos are based on egocentric videos and hence the questions mostly require perceptual knowledge, rather than multimodal reasoning. Another recent benchmark, Perception Test [21], focuses on core perception skills, such as memory and abstraction, across various reasoning abilities (e.g., descriptive, predictive, etc) for short-form videos. MAD [25] dataset contains dialogue and visual descriptions for full-length movies and is typically used in scene captioning task rather than understanding. Another issue is this dataset does not provide raw visual data, they share only [CLS] token embeddings, which makes it hard to use. TVQA [11] is QA dataset based on short 1-min clips from famous TV shows. The annotators are instructed to ask What/How/Why sort of questions combining two or more events in the video. MoVQA [44] manually curates questions across levels multiple levels-single scene,
multiple scenes, full movie- by guiding annotators to develop queries in predefined categories like Information Processing, Temporal Perception, etc. TGIF-QA [10] is heuristic based questions, like how many times cat nodded its head, or what happened after event A? It is partially manually annotated with some automation for counting types of questions. Next-QA [37] comprises manually curated causal, temporal, and descriptive questions about daily life videos. Long video understanding datasets, such as EpicKitchens [6], tend to concentrate heavily on tasks related to the memory of visual representations, rather than on reasoning skills. While these benchmarks are valuable for gauging the extent of visual representation captured by a model, they fall short in providing insights into video understanding. These datasets mainly test a model's ability to recall and recognize visual elements but do not adequately assess its capability to reason and interpret the context and narrative of videos.

CinePile differs from all the above datasets that have much longer videos and we ask many questions per video to capture the perceptual, temporal, and reasoning aspects of the video. And it is truly multimodal where the person has to watch the video as well as audio/dialogue to answer many questions. Unlike the previous datasets with fixed templates, we automated this process on previously human-generated questions, this let us capture many more question categories compared to previous works. Lastly, our approach to dataset generation is scalable and hence it is easy for anyone to extend our dataset.

### 6.2 Synthetic data with human in the loop.

Training models on synthetic data is a popular paradigm in recent times. We have seen many advances in generation as well as usage on synthetic data in recent times, both in vision $[5,9,30,35]$ and language [14, 18, 27, 34, 42]. For instance, Self-Instruct [33] proposes a pipeline to create an instruction dataset based on a few instruction examples and categories defined by humans. We mainly derived inspiration and the fact that modern LLMs are quite good at understanding long text and creating question-answer pairs. UltraChat [8] is another synthetic language dataset which is created by using separate LLMs to iteratively generate opening dialogue lines, simulate user queries, and provide responses. This allows constructing large-scale multi-turn dialogue data without directly using existing internet data as prompts. Additionally, Evol-Instruct [38], automatically generates a diverse corpus of open-domain instructions of varying complexities by prompting an LLM and applying iterative evolution operations like in-depth evolving (adding constraints, deepening, etc.) and in-breadth evolving (generating new instructions). To our knowledge, we are among the first to apply automated template generation and question synthesis techniques to vision and video modalities using LLMs.

## 7 Discussion and Conclusion

In this paper, we introduced CinePile, a unique long video understanding dataset and benchmark, featuring $\sim 300 \mathrm{k}$ questions in the training set and $\sim 5000$ in the test split. We detailed a novel method for curating and filtering this dataset, which is both scalable and cost-effective. Additionally, we benchmarked various recent commercial video-centric LLMs and conducted a human study to gauge the achievable performance on this dataset. To our knowledge, CinePile is the only largescale dataset that focuses on multi-modal understanding, as opposed to the purely visual reasoning addressed in previous datasets. The dataset is publicly available at https://hf .co/datasets/ tomg-group-umd/cinepile. Additionally, we plan to set up a leaderboard for the test set, providing a platform for new video LLMs to assess and benchmark their performance on CinePile.

Despite its strengths, there are still a few areas for improvement in our dataset, such as the incorporation of character grounding in time. While we believe our dataset's quality is comparable to or even better than that of a Mechanical Turk annotator, we acknowledge that a motivated human, given sufficient time, can create more challenging questions than those currently generated by an LLM. Our goal is to narrow this gap in future iterations of CinePile.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 382-398. Springer, 2016.

[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. URL https://www. anthropic. com/claude.

[4] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. Whisperx: Time-accurate speech transcription of long-form audio. INTERSPEECH 2023, 2023.

[5] Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, and Ari Morcos. Pug: Photorealistic and semantically controllable synthetic data for representation learning. Advances in Neural Information Processing Systems, 36, 2024.

[6] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision (ECCV), 2018.

[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[8] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.

[9] Reyhane Askari Hemmat, Mohammad Pezeshki, Florian Bordes, Michal Drozdzal, and Adriana Romero-Soriano. Feedback-guided data synthesis for imbalanced classification. arXiv preprint arXiv:2310.00158, 2023.

[10] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758-2766, 2017.

[11] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018.

[12] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.

[13] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. arXiv preprint arXiv:2005.00200, 2020.

[14] Yuanzhi Li, SÃ©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.

[15] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004.

[16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.

[17] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv:2306.05424, 2023.

[18] Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: A recipe for compute and data-efficient language modeling. arXiv preprint arXiv:2401.16380, 2024.

[19] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2024.

[20] OpenAI. Gpt-4o release, 2024. URL https://openai.com/index/hello-gpt-4o/.

[21] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: A diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024.

[22] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492-28518. PMLR, 2023.

[23] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.

[24] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019.

[25] Mattia Soldan, Alejandro Pardo, Juan LeÃ³n AlcÃ¡zar, Fabian Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem. Mad: A scalable dataset for language grounding in videos from movie audio descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5026-5035, 2022.

[26] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023.

[27] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[28] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46314640,2016

[29] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

[30] Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola. Learning vision from models rivals learning vision from data. arXiv preprint arXiv:2312.17742, 2023 .

[31] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575, 2015.

[32] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023.

[33] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

[34] Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023.

[35] Erroll Wood, Tadas BaltruÅ¡aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3681-3691, 2021.

[36] Chao-Yuan Wu and Philipp KrÃ¤henbÃ¼hl. Towards Long-Form Video Understanding. In CVPR, 2021.

[37] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9777-9786, 2021.

[38] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.

[39] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645-1653, 2017.

[40] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023.

[41] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.

[42] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.

[43] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. A simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023.

[44] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: A benchmark of versatile question-answering for long-form movie understanding. arXiv preprint arXiv:2312.04817, 2023.

[45] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.

[46] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.
