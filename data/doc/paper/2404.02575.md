# Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models 

Hyungjoo Chae ${ }^{1}$, Yeonghyeon Kim ${ }^{1}$, Seungone Kim ${ }^{2}$, Kai Tzu-iunn Ong ${ }^{1}$,<br>Beong-woo Kwak ${ }^{1}$, Moohyeon Kim $^{1}$, Seonghwan Kim ${ }^{1}$, Taeyoon Kwon ${ }^{1}$,<br>Jiwan Chung ${ }^{1}$, Youngjae $\mathrm{Yu}^{1}$, Jinyoung Yeo ${ }^{1}$<br>${ }^{1}$ Yonsei University ${ }^{2}$ KAIST AI<br>\{mapoout, jinyeo\}@yonsei.ac.kr


#### Abstract

Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Programof-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents THINK-AND-EXECUTE, a novel framework that decomposes the reasoning process of language models into two steps. (1) In THINK, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In EXECUTE, we further tailor the generated pseudocode to each instance and simulate the execution of the code. With extensive experiments on seven algorithmic reasoning tasks, we demonstrate the effectiveness of THINK-AND-EXECUTE. Our approach better improves LMs' reasoning compared to several strong baselines performing instance-specific reasoning (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic. Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.


## 1 Introduction

Reasoning in large language models (LLMs) typically entails analyzing the logical structure underlying a problem and realizing the logic into a sequence of reasoning steps to derive the final answer (Zhou et al. 2022a b: Hao et al. 2023). In particular, algorithmic reasoning has long been a formidable challenge for LLMs, as it requires to scrutinize a complicated reasoning pattern and to translate it into a long sequence of reasoning steps (Suzgun et al. 2022: Valmeekam et al. 2022: Pan et al. 2023).

To improve the reasoning capabilities of LLMs, prior works have primarily pursued two directions. The first direction includes enhancing the reasoning execution step by generating a rationale in natural language (e.g., Chain-of-Thought (Wei et al., 2022; Kojima et al., 2022)) or a piece of code (e.g., Program-of-Thought (Chen et al. 2023), Program-Aided LMs (Gao et al. 2023)). However, such approaches perform step-by-step reasoning on-the-fly, without a dedicated phase for planning. This necessitates that the LLM analyze the logic and execute it within a single inference call, which constrains its expressiveness. Moreover,

![](https://cdn.mathpix.com/cropped/2024_06_04_93a447fa4897bc84e4c2g-02.jpg?height=613&width=1317&top_left_y=276&top_left_x=401)

Figure 1: An illustration of THINK-AND-EXECUTE, compared with Zero-shot Chain-ofThought (Kojima et al., 2022) and Program-of-Thoughts (Chen et al., 2023).

when encountering a similar problem, the LLM should solve it without being able to reuse the logic previously understood.

The second direction involves explicitly generating a plan described in natural language with LLMs. The plan describes the logic of the task and the LLM would subsequently concretize it into a sequence of reasoning steps (e.g., Least-to-Most (Zhou et al., 2022b), Planand-Solve (Wang et al. 2023)). Yet, as prior works have mentioned, during our preliminary experiments, we find that natural language might not be the optimal medium to describe the logic of the problem ( $\mathrm{Li}$ et al., 2023). In addition, prior works mostly rely on generating a plan by observing a single instance, which hinders analyzing the core reasoning pattern shared across similar instances within a single task (Zhou et al. 2024).

To address these issues, we introduce THINK-AND-EXECUTE, an algorithmic framework that discovers a logic that reflects the shared reasoning pattern behind a given task, and conducts reasoning by tailoring the logic into each instance. THINK-AND-EXECUTE consists of three distinctive steps; We first ask an LLM to THINK about common reasoning patterns of a task by providing it with a few example questions. Then, the LLM translates the natural language description of the logic in a pseudocode format. The pseudocode format allows more flexibility in applying the logic to each instance compared to programming language such as Python. Finally, in EXECUTE step, the LLM simulates the execution of the task-level pseudocode to follow the logic in it and predicts the output result of the pseudocode.

Through extensive experiments on 7 algorithmic reasoning tasks from Big-Bench Hard (Suzgun et al. 2022), we show the effectiveness of THINK-AND-EXECUTE over the challenging baselines. The superior performance of THINK-AND-EXECUTE over PoT suggests that discovering the common logic for a given task and applying it to each instance would be more helpful than writing instance-specific code for every instance. Noteworthily, simulating the execution of pseudocode is shown to improve $\mathrm{LMs}^{\prime}$ reasoning more than planning with natural language (NL), even though they are trained to follow NL instructions. Furthermore, we empirically show that the pseudocode prompt discovered by an LLM can be applied to small LMs (SLMs), such as CodeLlama-7B, to boost their reasoning ability. This indicates the efficiency of THINK-AND-EXECUTE over other code prompting methods that require the LLM to generate instance-specific code every time (e.g., PoT).

To summarize, our contributions are as follows:

- We introduce THINK-AND-EXECUTE, a framework that performs reasoning with a pseudocode that contains the common logical structure of a given task.

![](https://cdn.mathpix.com/cropped/2024_06_04_93a447fa4897bc84e4c2g-03.jpg?height=859&width=1263&top_left_y=275&top_left_x=428)

Figure 2: An overview of THINK-AND-ExECUTE. In THINK (Top), an LLM analyzes the given task provided in the meta prompt and generates a pseudocode prompt that describes the necessary logic for solving the task. Then, in EXECUTE (Bottom), the LLM conducts reasoning for each instance by simulating the execution of the pseudocode prompt.

- We show that THINK-AND-EXECUTE achieves notable improvements over strong baselines, including Chain-of-Thought and Program-of-Thought prompting, across various algorithmic tasks in Big-Bench Hard.
- We demonstrate that the pseudocode written by an LLM can be transferred to SLMs, showing the efficiency of our approach.


## 2 THINK-AND-EXECUTE

In this section, we introduce THINK-AND-EXECUTE and provide a detailed explanation of how LLMs perform reasoning with it. We incorporate an Instructor LM $\mathcal{I}$ and a Reasoner LM $\mathcal{R}$, for THINK and EXECUTE, respectively. Figure 2 shows the overview of our framework.

### 2.1 THINK: Describing the Underlying Logic of a Task in a Pseudocode Format

The goal for the Instructor $\mathrm{LM} \mathcal{I}$ in this phase is to discover the underlying logic for solving a given task $t$, and generate a prompt describing the logic, which will be further applied to all instances of the task (in EXECUTE). This prompt is constructed with pseudocode rather than natural language, which is used in prior work to guide the LM to perform step-by-step reasoning (Kojima et al. 2022: Wang et al. 2023).

Step 1: Constructing a meta prompt. To prompt the Instructor $\mathrm{LM} \mathcal{I}$ to generate a tasklevel pseudocode for the given target task $t$, we provide $\mathcal{P}$ of other tasks as demonstrations in a meta prompt 1 In practice, we construct the meta prompt with 3 randomly sampled tasks (3 example questions, analysis, and $\mathcal{P}$ for each task) from $\mathcal{T}$ as demonstrations and the target task $t$ ( 3 example questions without the answers). ${ }^{2}$[^0]

Step 2: Analyzing the target task. Given the meta prompt, $\mathcal{I}$ generates an analysis containing key reasoning logic that is required to solve the target task regardless of the instances (questions). For example, in Figure 2 (Top), the generated analysis points out that building a truthfulness map and updating it by processing statements are needed to solve the task, i.e., Web of Lies. This step guides $\mathcal{I}$ to focus on the reasoning process shared among all the instances, which would be crucial in making a task-level prompt.

Step 3: Generating a pseudocode prompt based on the analysis. Next, based on the analysis, $\mathcal{I}$ writes a prompt $\mathcal{P}$ in the form of pseudocode, which breaks down the necessary reasoning steps for solving the target task. We choose to use the pseudocode format over the form of natural language plan (Kojima et al. 2022; Wang et al. 2023) for two main reasons: (1) the efficiency of it in describing the logic behind a task (e.g., avoid using repetitive instructions via for loop), and (2) the guidance of what and when to generate rationales via the argument in print() statement and the location within the execution of code. For example, in Figure 2, the $\mathcal{P}$ contains the statement, print(f" \{person1\} says \{person2\} \{action\}. \{person1\} tells the truth: \{truth_dict[person1]\}"), which instructs the Reasoner LM to generate a rationale that is helpful in keep tracking of the truth map containing the truthfulness of each person, during the execution of $\mathcal{P}$. We provide more examples of meta prompt, analysis, and pseudocode prompt in Appendix G

### 2.2 EXECUTE: Simulating the Execution of Pseudocode Prompt for an Instance

The reasoner LM $\mathcal{R}$ then conducts reasoning with the generated pseudocode prompt $\mathcal{P}$, tailoring the logic in $\mathcal{P}$ for the given instance. Following Wei et al. (2022), we aim to maximize the reasoning abilities of the LM by instructing them to explicitly generate intermediate reasoning steps, known as chain-of-thought (CoT) reasoning. $\mathcal{R}$ is instructed to predict not only the final output result of the code, but also the intermediate execution outputs as rationales. Specifically, $\mathcal{R}$ predicts a list of outputs $O=\left\{o_{1}, o_{2}, \ldots, o_{k}\right\}$ of the pseudocode by simulating the execution process of $\mathcal{P}$, where $o_{i}$ denotes the $i$-th system output from print() statements, and $\left\{o_{1}\right\}_{1}^{k-1}$ are CoT rationales toward the final answer $o_{k}$. We assume that tracking intermediate execution results would benefit $\mathcal{R}$ to keep track of the state of variables while they change over the execution of the code. We enable $\mathcal{R}$ to mimic the behavior of a compiler with a system message "Generate the expected outputs (from all print() functions) of the code.". The final answer for a given question is outputted with "print("Final answer:\{answer\}")" command as the last system output $o_{k}$.

## 3 Experimental Setup

### 3.1 Datasets

We curate seven algorithmic reasoning tasks from Big-Bench Hard (Suzgun et al., 2022), including: dyck languages; geometric shapes; navigate; reasoning about colored objects; temporal sequence;tracking shuffled objectives; web of lies. These are specifically designed to measure the step-by-step reasoning capability of LLMs. Model performance on evaluated in zero-shot settings, where we do not provide demonstrations in the prompt. We provide detailed explanations in Appendix A.4.

### 3.2 Baselines

We consider the following baselines: (1) Direct prompting: Directly predicting the answer without generating any rationales. (2) Zero-shot CoT (Kojima et al. 2022): A setting where LLMs are evoked to generate the reasoning steps with "Let's think step by step", before the answer. (3) Zero-shot PoT (Chen et al., 2023): A setting where an LLM generates an instancespecific Python code that can be executed with a Python interpreter. Then, the execution result is used as the final answer. (4) NL planning: A variation of THINK-AND-EXECUTE, where the task-level instruction is generated in natural language, instead of pseudocode.

| Reasoner/Method | DL | GS | Nav | CO | TS | SO | WL | Avg |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CodeLlama-7B |  |  |  |  |  |  |  |  |
| Direct Prompting | 0.0 | 9.0 | 39.0 | $\underline{24.4}$ | 4.4 | 11.2 | $\underline{47.6}$ | 19.4 |
| Zero-shot CoT | 0.0 | $\mathbf{1 6 . 8}$ | 26.0 | 10.8 | $\mathbf{2 0 . 0}$ | 10.4 | $\underline{44.8}$ | 18.4 |
| NL Planning | 0.0 | 10.0 | $\underline{52.0}$ | 0.4 | 7.6 | $\underline{18.8}$ | $\mathbf{5 0 . 4}$ | $\underline{19.9}$ |
| Zero-shot PoT | 0.0 | 10.0 | $\mathbf{4 7 . 2}$ | 23.6 | 4.4 | 3.2 | 45.2 | 19.1 |
| THINK-AND-EXECUTE | $\mathbf{2 . 0}$ | $\underline{13.2}$ | $\mathbf{7 0 . 8}$ | $\mathbf{4 9 . 6}$ | $\underline{19.2}$ | $\mathbf{2 2 . 0}$ | 38.8 | 30.8 |
| CodeLlama-13B |  |  |  |  |  |  |  |  |
| Direct prompting | 0.0 | 3.2 | 39.0 | 28.8 | 0.0 | 6.8 | 37.2 | 16.4 |
| Zero-shot CoT | 0.0 | $\mathbf{2 4 . 8}$ | $\underline{62.4}$ | 28.0 | $\underline{21.6}$ | 15.6 | 44.8 | $\underline{28.2}$ |
| NL Planning | $\underline{1.2}$ | 8.8 | 24.8 | 28.8 | 7.2 | 17.6 | 53.6 | 2.3 |
| Zero-shot PoT | $\underline{1.2}$ | 16.4 | 45.6 | $\underline{38.8}$ | 10.8 | 35.6 | 20.4 | 24.1 |
| THINK-AND-EXECUTE | $\mathbf{8 . 0}$ | $\underline{18.4}$ | $\mathbf{7 0 . 4}$ | $\underline{50.4}$ | $\mathbf{2 5 . 2}$ | $\underline{32.4}$ | $\underline{49.6}$ | 36.3 |
| GPT-3.5-Turbo |  |  |  |  |  |  |  |  |
| Direct prompting | 1.0 | 33.0 | 57.0 | $\underline{52.4}$ | 41.2 | 20.0 | 54.0 | 36.9 |
| Zero-shot CoT | $\underline{4.4}$ | $\mathbf{4 6 . 8}$ | 73.2 | 7.4 | $\underline{44.4}$ | 37.6 | $\underline{59.2}$ | $\underline{48.0}$ |
| NL Planning | 1.2 | 35.6 | 58.8 | 46.8 | 32.0 | $\underline{40.0}$ | 50.4 | 37.8 |
| Zero-shot PoT | 0.4 | 21.2 | $\underline{77.2}$ | 45.6 | 0.4 | 28.0 | 54.0 | 32.4 |
| THINK-AND-EXECUTE | $\mathbf{6 . 0}$ | $\underline{41.6}$ | $\mathbf{9 6 . 8}$ | $\mathbf{7 2 . 0}$ | $\mathbf{6 8 . 0}$ | $\mathbf{6 5 . 6}$ | $\mathbf{7 2 . 8}$ | $\mathbf{6 0 . 4}$ |

Table 1: Zero-shot performance of THINK-AND-EXECUTE and baselines on seven algorithmic reasoning tasks, including Dyck Languages (DL), Geometric Shapes (GS), Navigate (Nav), Reasoning about Colored Objects (CO), Temporal Sequences (TS), Tracking Shuffled Objectives (SO), and Web of Lies (WL) from Big-Bench Hard (Suzgun et al., 2022).

### 3.3 Models

For the Reasoner LM $\mathcal{R}$, we adopt GPT-3.5-Turbo (OpenAI, 2023), which shows strong performance in various reasoning benchmarks and code generation tasks (Zellers et al. 2019: Cobbe et al., 2021, Muennighoff et al. 2024), as well as the 7B and 13B versions of CodeLlama (Roziere et al. 2023), which are trained on both code and natural language corpora and further fine-tuned to follow natural language instructions. As for the Instructor LM $\mathcal{I}$, we choose GPT-3.5-Turbo.

## 4 Results

### 4.1 THINK-AND-EXECUTE Improves Algorithmic Reasoning

We start by comparing our framework with direct prompting and zero-shot CoT Kojima et al. (2022) in Table 1. We find that zero-shot CoT performs better than direct prompting with average improvements of $11.1 \%$ with GPT-3.5-Turbo, respectively, suggesting zero-shot CoT to be a strong baseline. Our THINK-AND-EXECUTE, however, further outperforms both of them significantly regardless of model sizes, which indicates that explicitly generating a plan is an effective way to improve the LLM's reasoning capabilities than simply encouraging LLMs to generate their intermediate reasoning steps.

4.2 Task-level Pseudocode Prompts Benefits a Wider Range of Algorithmic Reasoning Tasks than Instance-specific Python Code

In Table1. PoT shows performance gains in some tasks over direct prompting (e.g., Navigate; Tracking Shuffled Objects) with Python code generated specifically for each instance and the corresponding interpreter output as the answer. However, such improvement is difficult to generalize to all tasks, e.g., $0.4 \%$ accuracy in both Dyck Language and Temporal Sequences, with GPT-3.5-Turbo. By contrast, THINK-AND-EXECUTE outperforms PoT and direct prompting in all tasks with GPT-3.5-Turbo. This suggests that making the task-level

![](https://cdn.mathpix.com/cropped/2024_06_04_93a447fa4897bc84e4c2g-06.jpg?height=431&width=1087&top_left_y=294&top_left_x=519)

Figure 3: Ablation study of the components of pseudocode prompt using GPT-3.5-Turbo.

| Method | Avg |
| :--- | :--- |
| w/o Analysis | 21.8 |
| THINK-AND-EXECUTE | $\mathbf{6 0 . 4}$ |

Table 2: Ablation on Step2 of THINK phase.

strategy with pseudocode and applying it to each instance can benefit LLM's reasoning in a wider range of algorithmic reasoning tasks than generating instance-specific Python codes.

### 4.3 The Logic Discovered by an LLM can be Transferred to SLMs

We further explore if the pseudocode prompt written by an LLM (i.e., GPT-3.5-Turbo as the instructor) can be applied to smaller LMs: the CodeLlama family in Table 1 . When applying the pseudocode prompts generated by GPT-3.5-Turbo, CodeLlama-7B and -13B significantly outperform direct prompting. Moreover, THINK-AND-EXECUTE with CodeLlama-13B shows comparable performance with GPT-3.5-Turbo with PoT and direct prompting.

### 4.4 Pseudocode Better Describes the Logic for Solving a Task than Natural Language

We also compare our approach with NL planning, a variant of ours that utilizes natural language to write the task-level instruction, instead of pseudocode. In practice, we provide human-written NL plans that contain a similar amount of information to $\mathcal{P}$ in the meta prompt and use it to generate the task-level NL plan for the given task. Surprisingly, although the LMs are fine-tuned to follow natural language instructions, we find that task-level pseudocode prompts can boost their performance more than NL plans (Table 1).

### 4.5 Ablation Studies

Components of the pseudocode prompt. We conduct an ablation study on each component of the pseudocode prompt. For that, we prepare four types of pseudocode prompts: (1) Human-written pseudocode; (2) Human-written prompt w/o comments and semantics by removing the comments that explain the code and replacing variable names with meaningless alphabets, such as X, Y, and Z; (3) Human-written prompt w/ for loop and (4) w/ intermediate print() statements. The results are in Figure 3. Model performance decreases significantly when applying prompts w/o comments and semantics, especially in Temporal Sequences. This implies that semantics play an important role in guiding the LLMs to apply the discovered logic and reasoning with it accordingly. Also, we find that printing out the intermediate execution steps with print() is crucial in reasoning, which is consistent with the finding from Wei et al. (2022).

Generating the analysis before the pseudocode prompt. Table 2 shows a notable decrease in model performance when generating pseudocode prompts without conducting the

| Method | Avg |  |  |  |
| :--- | :--- | :--- | :--- | :--- |
| Chain-of-Code | 28.1 |  | Method | Avg |
| Plan-and-Solve | 50.3 |  | Self-Discover w/ GPT-4 | 77.9 |
| THINK-AND-EXECUTE | $\mathbf{6 0 . 4}$ |  | THINK-AND-EXECUTE w / GPT-4 | $\mathbf{8 1 . 7}$ |

Table 3: Left: Comparison of THINK-AND-EXECUTE, Chain-of-Code (Li et al., 2023), and Plan-and-Solve (Wang et al. 2023) using GPT-3.5-Turbo. Right: Comparison of THINK-ANDEXECUTE and Self-Discover (Zhou et al. 2024) using GPT-4. The results of Self-Discover are obtained from the original paper, as the code and prompts are not provided.

analysis first. This suggests that explicitly generating analysis on the task can elicit a better pseudocode prompt that contains the necessary logic for solving the task.

### 4.6 Comparison with other Baselines

We further compare THINK-AND-EXECUTE with another three baselines: (1) Plan-andSolve (Wang et al. 2023), where an LLM sequentially generates a natural language plan for solving the given instance, step-by-step reasoning according to the plan, and the final answer; (2) Chain-of-Code (Li et al. 2023), where Python code is generated as a part of intermediate reasoning steps specitically for a given instance; (3) Self-Discover (Zhou et al., 2024), a concurrent work that devises a task-level reasoning structure in a JSON tormat before inferencing the instance. First, as presented in Table 3 (Left), we find THINK-AND-EXECUTE largely outperforms Plan-and-Solve and Chain-of-Code by 10.9 and 32.3 percentage points in terms of accuracy, respectively. Second, while Self-Discover also incorporate task-level instruction, in Table 3 (Right), our THINK-AND-EXECUTE with pseudocode prompts shows better performance when using GPT-4 (Achiam et al. 2023) 3. These findings indicate that generating (1) task-level instruction with (2) pseudocode can better represent the necessary logic for solving a task and benefit LLM's algorithmic ability.

## 5 Analysis

We conduct experiments to address the following research questions:

- RQ1: Is task-level pseudocode more helpful than instance-specific pseudocode?
- RQ2: Does pre-training on code corpora improve reasoning?
- RQ3: How is the quality of the logic discovered by THINK-AND-EXECUTE compared to human-written logic?


### 5.1 Implementing the Underlying Logic is more Effective than Instance-specific Logic in Pseudocode (RQ1)

We conduct an analysis to check if the improvement of THINK-AND-EXECUTE is contributed by our chosen format for the task-level instruction, i.e., pseudocode. We compare THINKAND-EXECUTE with a concurrent work, Chain-of-Code (CoC) (Li et al., 2023). In Table 3 . THINK-AND-EXECUTE outperforms $\mathrm{CoC}$, showing about $2 x$ improvement in the average score. The main difference between THINK-AND-EXECUTE and CoC is that we use pseudocodes which are generated to express logic shared among the tasks instances, while CoC incorporates pseudocode as part of the intermediate reasoning steps towards the solution of a given instance. Hence, the results indicate the advantages of applying pseudocode for the generation of task-level instruction over solely using them as a part of rationales.[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_93a447fa4897bc84e4c2g-08.jpg?height=382&width=1330&top_left_y=297&top_left_x=403)

Figure 4: Analysis on the effect of code pre-training on the reasoning capability in applying THINK-AND-EXECUTE. Without pre-training on code corpora the accuracies drop notably.

| Reasoner/Method | $\mathrm{DL}$ | GS | $\mathrm{Nav}$ | $\mathrm{CO}$ | TS | $\mathrm{SO}$ | $\mathrm{WL}$ | $\mathrm{Avg}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CodeLlama-7B |  |  |  |  |  |  |  |  |
| Human-written $\mathcal{P}$ | 2.4 | 0.0 | 40.4 | 29.6 | 12.0 | 18.0 | 52.8 | $22.2 \quad$ |
| THINK-AND-EXECUTE | 2.0 | 13.2 | 70.8 | 49.6 | 19.2 | 22.0 | 38.8 | $30.8 \quad$ |
| CodeLlama-13B |  |  |  |  |  |  |  |  |
| Human-written $\mathcal{P}$ | 2.8 | 14.8 | 72.8 | 40.4 | 16.8 | 15.6 | 49.6 | $30.4 \quad$ |
| THINK-AND-EXECUTE | 8.0 | 18.4 | 70.4 | 50.4 | 25.2 | 32.4 | 49.6 | 36.3 |
| GPT-3.5-Turbo |  |  |  |  |  |  |  |  |
| Human-written $\mathcal{P}$ | 12.4 | 50.0 | 86.0 | 50.8 | 84.0 | 32.4 | 74.4 | 55.7 |
| THINK-AND-EXECUTE | 6.0 | 41.6 | 96.8 | 72.0 | 68.0 | 65.6 | 72.8 | 60.4 |

Table 4: Comparison between THINK-AND-EXECUTE and Human-written $\mathcal{P}$.

### 5.2 THINK-AND-EXECUTE Requires Knowledge in Code (RQ2)

To understand whether SLMs acquire the ability to understand the task-level logic written in pseudocode during pre-training on code corpora, we compare the performance of CodeLlama-13B with Llama-13B using THINK-AND-EXECUTE. In Figure 4. CodeLlama-13B shows better reasoning capabilities compared to Llama-13B in all tasks. These results suggest that the improvement from using THINK-AND-EXECUTE could depend on the knowledge of code, which is usually obtained by pre-training with code corpora. Writing code usually involves understanding the logic behind the given problem and expecting the execution results of a code, which resemble the same reasoning process of THINK-AND-EXECUTE.

### 5.3 THINK-AND-EXECUTE can Generate a Logic Comparable to Human's (RQ3)

To gauge LLMs' capabilities in discerning the underlying logic of a task, we compare THINKAND-EXECUTE (using GPT-3.5-Turbo as the Instructor) with human-written pseudocode prompts. The results are shown in Table4. Using the GPT-3.5-Turbo the Reasoner, THINKAND-EXECUTE scores $60.4 \%$ in terms of accuracy, which is superior to the human-written $\mathcal{P}$ (with an accuracy of $55.7 \%$ ). Especially, in the tasks of Navigate and Tracking Shuffled Objectives, pseudocode prompts generated by THINK-AND-EXECUTE elicit better performance. This also holds true when adopting CodeLlama-7B and -13B as the Reasoner, further suggesting the effectiveness of our THINK step over human writers.

### 5.4 Impact of LLMs' Capability on THINK-AND-EXECUTE

In examining the impact of LLMs' capabilities within our framework, we investigate the influence of both the Reasoner and Instructor components on performance, as depicted in Table5. Notably, higher accuracy scores are observed when utilizing GPT-3.5-Turbo as Reasoners compared to CodeLlama-13B and CodeLlama-34B. Additionally, the effectiveness

| Reasoner | Instructor |  |  |
| :--- | :---: | :---: | :---: |
|  | CodeLlama-13B | CodeLlama-34B | GPT-3.5-Turbo |
| CodeLlama-13B | 30.9 | 33.0 | 36.4 |
| CodeLlama-34B | 32.5 | 34.2 | 39.1 |
| GPT-3.5-Turbo | 33.9 | 35.9 | 60.4 |

Table 5: Analysis of the effect of the capability of Reasoner and Instructor on the performance. We report the average performance on the 7 tasks.

of the Instructor also plays a crucial role, with GPT-3.5-Turbo exhibiting the highest accuracy scores across all configurations. These results underscore the significance of both the Reasoner and Instructor components in enhancing the performance of THINK-AND-EXECUTE.

## 6 Related Work

Chain-of-Thought prompting. Chain-of-thought (CoT) prompting evokes LMs to generate intermediate reasoning steps that guide and explain the solution (Wei et al. 2022, Wang et al., 2022. Wu et al., 2023). One common paradigm of this is zero-shot Col prompting (Kojima et al. 2022). Without specifically designed question-explanation-answer triplets as demonstrations, zero-shot CoT prompting elicits a plausible reasoning path towards the final answer with simple instruction, such as "Let's think step-by-step", eliciting better model performance in tasks that require multi-step reasoning.

In the context of improving zero-shot CoT, Wang et al. (2023) propose to first generate a plan breaking down the target task into smaller subtasks, and then solve each subtask according to the plan. Similar to our approach, a concurrent work (Zhou et al., 2024) devises a task-level reasoning structure that can be applied to each instance (question) of the target task. The most significant distinction between these prior studies and ours is that our THINK-AND-EXECUTE adopts pseudocode (as opposed to natural language) to express the necessary logic for solving the task. We demonstrate that our task-level pseudocode prompt empowers LMs with better ability of zero-shot reasoning than natural language plans under various settings in Section 5

Incorporation of code in reasoning. With unambiguous syntaxe and strict structure, programming languages such as Python have been applied to LLM-based systems to improve system performance in solving tasks. For instance, Gao et al. (2023) and (Chen et al. 2023) use LLMs to generate Python code for given mathematical questions, and run the generated code on external compilers to obtain/calculate the answers. Concurrently with our work, Li et al. (2023) present chain-of-code (CoC), where pseudocode is also incorporated along with the Python code for solving a given question (instance). While this approach generates instance-specific code as intermediate reasoning steps for each individual instance, our THINK-AND-EXECUTE, by contrast, focus on the task-level pseudocode prompt that can be applied to all instances. We compare CoC and THINK-AND-EXECUTE in Section 4 .

## 7 Limitations and Discussion

A possible limitation of our approach is that we focus on algorithmic reasoning, as we believe it is the best setting to assess LLMs' capabilities in understanding a complex logic and carrying out a sequence of reasoning step, following the logic. However, we believe that THINK-AND-EXECUTE can be applied to other domains of reasoning that require following a long sequence of reasoning steps, such as multi-hop reasoning (Ji et al., 2020) and symbolic reasoning (Madaan \& Yazdanbakhsh, 2022).

## 8 Conclusion

In this paper, we present THINK-AND-EXECUTE, an algorithmic reasoning framework that generates a logic for solving the given task into a pseudocode and performs reasoning by simulating the execution of the pseudocode with language models. Through extensive experiments, we show the effectiveness of THINK-AND-EXECUTE, over the strong baselines. These results underscore not only the usefulness of pseudocode in eliciting language models' reasoning capabilities but also the efficiency of our framework in discovering the highquality logic behind a given task.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. ArXiv, abs/2305.14992, 2023. URL https://api.semanticscholar.org/CorpusID:258865812.

Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, and Minlie Huang. Language generation with multi-hop reasoning on commonsense knowledge graph. In Conference on Empirical Methods in Natural Language Processing, 2020. URL https: //api.semanticscholar.org/CorpusID:221879025

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.

Chengshu Li, Jacky Liang, Fei Xia, Andy Zeng, Sergey Levine, Dorsa Sadigh, Karol Hausman, Xinyun Chen, Li Fei-Fei, and brian ichter. Chain of code: Reasoning with a language model-augmented code interpreter. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URLhttps://openreview.net/forum?id=tlRUbIOYf3

Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.

Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mw1PWNSWZP

OpenAI. Chatgpt, 2023. https://openai.com/blog/chatgpt

Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. ArXiv, abs/2305.12295, 2023. URL https://api.semanticscholar.org/CorpusID:258833332.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. URL https://openreview.net/forum?id=wUU-7XTL5XO

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2609-2634, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023 acl-long. 147 .

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id $=\_V j Q 1 M e S B \_J$

Dingjun Wu, Jing Zhang, and Xinmei Huang. Chain of thought prompting elicits knowledge augmentation. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6519-6534, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.408

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https: //aclanthology.org/P19-1472

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022a.

Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022b.

Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose reasoning structures. arXiv preprint arXiv:2402.03620, 2024.
