# Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network 

Hyeongjin Kim ${ }^{1 ~ *}$, Sangwon Kim ${ }^{1 \text { * }}$, Jong Taek Lee ${ }^{2}$, Byoung Chul Ko ${ }^{1 \dagger}$<br>${ }^{1}$ Dept. of Computer Engineering, Keimyung University<br>${ }^{2}$ School of Computer Science and Engineering, Kyungpook National University<br>\{henryjoshuakim, eddiesangwonkim\}@gmail.com, jongtaeklee@knu.ac.kr, niceko@kmu.ac.kr


#### Abstract

Along with generative AI, interest in scene graph generation (SGG), which comprehensively captures the relationships and interactions between objects in an image and creates a structured graph-based representation, has significantly increased in recent years. However, relying on object-centric and dichotomous relationships, existing SGG methods have a limited ability to accurately predict detailed relationships. To solve these problems, a new approach to the modeling multiobject relationships, called edge dual scene graph generation (EdgeSGG), is proposed herein. EdgeSGG is based on a edge dual scene graph and Dual Message Passing Neural Network (DualMPNN), which can capture rich contextual interactions between unconstrained objects. To facilitate the learning of edge dual scene graphs with a symmetric graph structure, the proposed DualMPNN learns both object- and relation-centric features for more accurately predicting relation-aware contexts and allows fine-grained relational updates between objects. A comparative experiment with state-of-the-art (SoTA) methods was conducted using two public datasets for SGG operations and six metrics for three subtasks. Compared with SoTA approaches, the proposed model exhibited substantial performance improvements across all SGG subtasks. Furthermore, experiment on long-tail distributions revealed that incorporating the relationships between objects effectively mitigates existing long-tail problems.


## Introduction

Recent advancements in deep neural networks (DNNs) have resulted in an unprecedented performance in visual recognition tasks (Ghosh et al. 2019; Yang et al. 2019; Ahn et al. 2023). There has consequently been a continuous flow of research aimed at illustrating how DNNs aggregate and utilize visual information. An active area of research is scene graph generation (SGG), which uses DNNs to automatically map images onto semantically structured scene graphs. To comprehensively capture the relationships and interactions between objects within an image, SGG primarily focuses on generating structured representations, frequently in the form of graphs. Therefore, SGG requires the correct labeling of detected objects and their relationships.[^0]

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-1.jpg?height=325&width=485&top_left_y=737&top_left_x=1275)

(a) Input image

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-1.jpg?height=193&width=269&top_left_y=1107&top_left_x=1226)

(b) Primitive graph $(\mathcal{G})$

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-1.jpg?height=201&width=354&top_left_y=1106&top_left_x=1493)

(c) Edge dual graph $(\hat{\mathcal{G}})$
Figure 1: Process of generating the proposed edge dual scene graph. (a) Result of object detector from the input image. (b) A scene graph is formed using the objects as nodes and their relationship as edges extracted through (a). (c) The edge dual scene graph is transformed from the scene graph (b). The key idea of edge dual scene graph generation is to convert nodes into edges and edges into nodes.

SGG approaches generally involve two primary steps: object detection and relationship modeling. Object detection is responsible for identifying and localizing individual objects within an image, resulting in a collection of bounding boxes. The subsequent step, relationship modeling, aims to comprehend the interactions among these objects and capture their contextual dependencies. Several methods (Tang et al. 2019; Zellers et al. 2018) have been proposed to address the challenges to SGG, encompassing both rule- and learning-based approaches. Rule-based methods commonly employ predefined templates or heuristics to establish relationships between objects. Conversely, learning-based methods use DNNs to learn relationships from extensively annotated datasets. Graph neural networks (GNNs) are often employed to model structured representations and leverage graph-based operations for reasoning and prediction within an SGG framework.

Common design approaches (Li et al. 2021; Xu et al.
2017a; Yoon et al. 2023) in relationship modeling using GNNs aim to uncover the dependencies between objects by predicting graph edges in an object-centric manner. However, despite the need to construct a scene graph that considers the complex dependencies among various objects in a scene, most approaches in this field have focused on exploring the relationship between pairs of individual objects. For example, consider a scenario in which a person is riding a bike in a park. Existing methods typically concentrate on dichotomous pairs such as person-bike, bike-park, and person-park. Although an object in the real-world will often interact with multiple other objects, research to date has overlooked this aspect. Therefore, rather than solely considering simple object pairs, analyzing the mutual relationships among multiple objects can lead to an improved performance. Several methods (Guo et al. 2021; Shit et al. 2022; Jung et al. 2023) have attempted to effectively model object interactions by analyzing the relationships among all graph edges. However, such exhaustive approaches incur an excessive number of computations owing to their comprehensive nature. HetSGG (Yoon et al. 2023) attempted to address the relationships among objects by proposing heterogeneous graphs for capturing the relationships in more detail. Nevertheless, the precise prediction of detailed relationships is limited because they rely on an object-centric approach. To overcome the problems of dichotomous relationships and object-centric approaches, we propose edge dual scene graph generation (EdgeSGG), a novel relationcentric approach to modeling multiple object relationships for dependency prediction. As shown in Fig. 1, our method is based on the concept of an edge dual scene graph that allows the capture of rich contextual interactions between unconstrained objects. We demonstrate that EdgeSGG (i) facilitates fine-grained scene reasoning even in scenarios with complex objects, (ii) exhibits higher accuracy in predicting relationships than existing methods, and (iii) alleviates the long-tail problem. Finally, we illustrate how EdgeSGG can be utilized to enhance the understanding of DNNs by uncovering relationships using a reconstructed edge dual scene graph.

## Related Work

Scene Graph Generation. Visual relationship (Lu et al. 2016) pioneered the more challenging task of detecting visual relations in the wild by modeling relationships in an independent manner. They adapted a separate relationship prediction scheme (i.e., training only the relationship modeling function) to account for the infrequent nature of most relationships.

SGG based on Contextual Information. Many studies (Jung et al. 2023; Zheng et al. 2023; Yoon et al. 2023; Li et al. 2017; Zellers et al. 2018; Xu et al. 2017 a) have been subsequently introduced to address the issue of ambiguity and to predict missing relationships by leveraging contextual information. Such studies are aimed at improving the accuracy of SGG in terms of relationship prediction by considering the interactions between objects in an image. IMP (Xu et al. 2017a) first incorporated contextual information to enhance the relation modeling by utilizing an iterative mes- sage passing structure, which refining the object and relation features. A few recent studies have continued to explore prototyping (Zheng et al. 2023) or transformer-based methods (Jung et al. 2023), while also introducing approaches (Yoon et al. 2023) to generating diverse graphs for the synthesizing of rich contextual information from heterogeneous sources. However, most studies based on contextual information have continued to adopt object-centric approaches and have failed to adequately consider the unbalanced relationship distribution.

SGG based on Long-tail Solving. Knowledg-embedded routing network (Chen et al. 2019) aimed to tackle the unbalanced distribution issue in wild scenes by incorporating statistical correlations between object pairs and their relationships. Following the study by (Chen et al. 2019), several approaches (Li et al. 2022a, 2021; Tang et al. 2020a) have attempted to solve the problem of an imbalanced class distribution. Although most of these studies have improved the SGG prediction performance of the tail classes in a longtail data distribution (LTD), they have failed to significantly improve the overall performance, including the head. To the best of our knowledge, the present study represents the first attempt to reconstruct a scene graph as an edge dual scene graph that effectively captures contextual information and employ a relation-centric approach to addressing the LTD problem.

## Method

## Preliminaries

SGG Learning. To be specific, within the independence paradigm, SGG learns a mapping from samples $x \in X$ to parsed scene graph $(\mathcal{O}, \mathcal{R}) \in \mathcal{T}$ by means of the following: (i) An object detection function $d: X \rightarrow(\mathcal{O}, \mathcal{B}, L) \in \mathbb{R}^{k}$ maps a sample from the input space $x \in X \subseteq \mathbb{R}^{n}$ to the intermediate spaces $o \in \mathcal{O} \subseteq \mathbb{R}^{d_{o}}, b \in \mathcal{B} \subseteq[0,1]^{4}$, and $l \in L \subseteq \mathbb{R}^{|Y|}$ formed by $k$ objects and their bounding boxes and classes. A symbol $d_{o}$ means the object feature dimension. (ii) A primitive graph building function $\psi:(\mathcal{O}, \mathcal{B}) \rightarrow \mathcal{G}$ maps samples from the object space $(o, b) \in(\mathcal{O}, \mathcal{B})$ to a primitive graph $(\mathcal{N}, \mathcal{E}) \in \mathcal{G}$. Finally, (iii) a relationship modeling function $f:(\mathcal{N}, \mathcal{E}) \rightarrow \mathcal{R}$ maps samples from the object space $(o, l) \in(\mathcal{O}, \mathcal{L})$ to a relationship space $r \in \mathcal{R} \subseteq \mathbb{R}^{d_{r}}$. Here, $d_{r}$ indicates relation feature dimension. During training, the SGG is encouraged to align $\hat{r}=f(d(x))$ with the predicted $(o, y)=d(x)$ to the corresponding ground-truth relation $r$ of $x$. This can be achieved by (i) employing the widely used ResNeXt-101-FPN (Xie et al. 2017), Faster R-CNN detector (Ren et al. 2015) and (ii) using its output to train the relationship-modeling function.

Dual Graph. In graph theory, the dual graph of a planar graph $(\mathcal{N}, \mathcal{E}) \in \mathcal{G}$ has each face of $\mathcal{G}$ represented as a node. More precisely, given a graph $\mathcal{G}$ in which the edges do not overlap, it is possible to generate a dual graph that exhibits symmetry to $\mathcal{G}$ by utilizing a mapping function $p: \mathcal{G} \rightarrow \hat{\mathcal{G}}$. However, in the case of a parsed scene graph $\mathcal{T}$, where the edges may overlap depending on the interacting objects, it is impossible to directly generate a traditional dual graph.
(a)
![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-3.jpg?height=730&width=438&top_left_y=238&top_left_x=344)

(b)

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-3.jpg?height=756&width=995&top_left_y=240&top_left_x=771)

Figure 2: (a) The object detection results are fed into DualMPNN. (b) The proposed DualMPNN consists of three parts: (1) object-centric MPNN, (2) relation-centric MPNN, and (3) feature aggregation. A primitive graph is formed to improve the object-centric learning from the results of the object detector, and at the same time, a symmetric edge dual scene graph is generated to enable improved relation-centric learning; the output of two MPNNs are then aggregated and fed to (c) the scene graph prediction module.

Therefore, inspired by (Bogdanov et al. 2017), we propose a novel approach, an edge dual scene graph. The original edge dual graph is a symmetric graph that preserves the primal graph structure while inverting the roles of each node and edge. By expanding original approach to image, it becomes possible to conduct relation-centric learning on different scenes, allowing for the learning of various object interactions and contextual information.

## EdgeSGG Overview

We present an overview of our proposed framework for scene graph generation based on an edge dual scene graph with the dual message passing neural network (DualMPNN). As shown in Fig. 2, the proposed EdgeSGG comprises (a) an object detector, (b) a DualMPNN to enable object- and relation-centric learning with an edge dual scene graph generator, and (c) a scene graph prediction module.

Object Detection Function. Within a framework, the object detector (Fig. 2 (a)) identifies the objects of interest in a scene. Following previous studies, we employ Faster RCNN (Ren et al. 2015) as the object detector and Glove (Pennington, Socher, and Manning 2014) as word embedding. As a result, the input image $x \in X$ is mapped to $(o, b, l) \in(\mathcal{O}, \mathcal{B}, L)$. We then construct an initial primitive graph $(\mathcal{U}, \mathcal{E}, \mathcal{V}) \in \mathcal{G}$ from the detected objects with function $\psi:(\mathcal{O}, \mathcal{B}) \rightarrow \mathcal{G}$. Primitive graph $\mathcal{G}$ is defined by three components: $u \in \mathcal{U} \subset \mathbb{R}^{d_{o}}$ representing the subjects, $v \in \mathcal{V} \subset \mathbb{R}^{d_{o}}$ representing the objects, and $e_{<u, v>} \in \mathcal{E}$ representing the relationships from subject to object. The feature vector of the relationship between $u$ and $v$ is extracted from the bounding box positions of $u$ and $v$, and their corresponding union boxes. Here, $\mathcal{U}$ and $\mathcal{V}$ are sets of object pairs derived from $\mathcal{O}$, and $\mathcal{V} \subseteq \mathcal{U}$ which $\mathcal{V}$ is a subset of $\mathcal{U}$ that does not contain itself.

Building Edge Dual Scene Graph. As previously mentioned, a conventional dual graph is useful for synthesizing rich contextual information; however, its applicability is limited to only planar graphs with non-overlapping edges. In particular, the primitive graph $\mathcal{G}$ contains complex relationships among multiple objects within a scene, making it a challenge to guarantee it to be a planar graph. In this study, we propose a new edge dual scene graph to facilitating the concept-oriented understanding of a dynamic scene structure. Edge dual scene graph allows us to capture the complex relationships among multiple objects within a scene, thereby ensuring the incorporation of abundant contextual information. Our proposed edge dual scene graph forms dual nodes from existing edges and edge duals from the existing nodes. The edge dual scene graph $\left(\hat{\mathcal{U}}, \hat{\mathcal{E}}, \hat{\mathcal{U}}_{j}\right) \in \hat{\mathcal{G}}$ is defined as follows:

$$
\begin{gather*}
\hat{\mathcal{U}}=\left\{e_{i}\left|e_{i} \in \mathcal{E}, i=1,2 \ldots\right| \mathcal{E} \mid\right\}  \tag{1}\\
\hat{\mathcal{E}}=\left\{u_{i}=\left(e_{i}, e_{j}\right) \mid e_{i} \cap e_{j}=u_{i} \in \mathcal{U}, i \neq j, u_{i} \neq \varnothing\right\} \tag{2}
\end{gather*}
$$

To facilitate the calculation of the edge ordered pair $u_{i}$, the properties $\left(e_{i}, e_{j}\right)=\left\{\left\{e_{i}\right\},\left\{e_{i}, e_{j}\right\}\right\}$ are converted into a collective structure and utilized. The edge dual scene graph $\hat{\mathcal{G}}$ produced using Eqs. 1 and 2 consistently transforms the existing nodes of graph $\mathcal{G}$ into edges, and the edges into
nodes. An edge dual scene graph transformation is presented as follows: Using the objects and relationships extracted from the object detector, as shown in Fig. 2 (b), we construct a complete graph comprising $|N|$ nodes and $|E|=|N|(|N|-1) / 2$ edges. The corresponding graph is transformed into an edge dual scene graph $\hat{\mathcal{G}}$ consisting of $|E|$ nodes and $\left|E_{\text {dual }}\right|=|E|(|N|-2)$ edges. Our example graph (Fig. 2 (b)) has 4 nodes and 6 edges. In the edge dual scene graph, we obtain 6 nodes and 12 edges. In this transformation, the originally adjacent edges become nodes in a dual graph and the two nodes are connected by an edge. This transformation ensures that the subgraphs of the connected nodes in the dual graph correspond to those in the original functional network. Furthermore, because $\hat{\mathcal{G}}$ reflects the relationships among relationships, it facilitates message passing. A detailed description of the MPNN method using graphs $\mathcal{G}$ and $\hat{\mathcal{G}}$ is presented in the following section.

Dual Message Passing Neural Network. In this study, we generated an edge dual scene graph that is symmetric to the primitive graph, enabling improved relation-centric learning. To facilitate the learning of two symmetric graphs, we propose the novel DualMPNN. The proposed DualMPNN differs from existing message passing as it learns both object-centric (Li et al. 2021) and relation-centric features, enabling a more accurate prediction of relation-aware contexts and fine-grained relationship updates between objects. A DualMPNN comprises the following three parts: (1) an object-centric MPNN, (2) a relation-centric MPNN, and (3) feature aggregation.

Part 1) Object-centric MPNN. To update the object-centric features using the primitive graph $\mathcal{G}$, which has nodes $\mathcal{U}$ and $\mathcal{V}$, and edge $\mathcal{E}$, the feature update process applied is as follows:

$$
\begin{align*}
& e_{<u, v>}^{h+1}=e_{<u, v>}^{h}+\sigma\left(\alpha(u, v) e_{<u, v>}^{h} W_{u}+\right. \\
& \left.\quad(1-\alpha(u, v)) e_{<v, u>}^{h} W_{v}\right) \tag{3}
\end{align*}
$$

where $e_{<u, v>}^{h+1} \in \mathbb{R}^{d_{r}}$ is the $(h+1)$ th $\in \mathbb{N}_{H}$ relation feature (i.e., $H$ is the total number of layers), and $W_{u}$ and $W_{v} \in \mathbb{R}^{d_{r} \times d_{r}}$ are the weight matrices. The activation function $\sigma$ is the ReLU, and $\alpha(u, v)$ is an attention score operation computed using the weight matrix $W_{\text {att }} \in \mathbb{R}^{d_{o}}$ as follows:

$$
\begin{equation*}
\alpha(u, v)=\frac{\exp \left(W_{a t t}^{\top} u\right)}{\exp \left(W_{a t t}^{\top} u\right)+\exp \left(W_{a t t}^{\top} v\right)} \tag{4}
\end{equation*}
$$

Part 2) Relation-centric MPNN. To update the relation features, an object-centric MPNN considers only the dependencies between objects $\mathcal{U}$ and $\mathcal{V}$, which limits its ability to capture the contextual information of the neighborhood. To address this limitation, we introduce a relation-centric MPNN that enables an update of the edge features by considering such relative neighborhood relationships. Using the edge dual graph $\hat{\mathcal{G}}$ constructed through Eqs. 1 and 2, the relation-centric MPNN is applied as follows:

$$
\begin{equation*}
z_{\left\langle e_{i}, e_{j}\right\rangle}^{0}=u_{i} W_{o 2 e}, \text { with } \quad W_{o 2 e} \in \mathbb{R}^{d_{o} \times d_{r}} \tag{5}
\end{equation*}
$$

$$
\begin{array}{r}
z_{\left\langle e_{i}, e_{j}\right\rangle}^{h+1}=z_{\left\langle e_{i}, e_{j}\right\rangle}^{h}+\sigma\left(\sum_{e_{j} \in \mathcal{N}\left(e_{i}\right)} \alpha\left(e_{i}, e_{j}\right) z_{\left\langle e_{i}, e_{j}\right\rangle}^{h} W_{i}+\right. \\
\left.\alpha\left(e_{j}, e_{i}\right) z_{\left\langle e_{j}, e_{i}\right\rangle}^{h} W_{j}\right) \tag{6}
\end{array}
$$

where $z_{\left\langle e_{i}, e_{j}\right\rangle}^{h+1} \in \mathbb{R}^{d_{r}}$ is a relation-centric edge feature of the $(h+1)$ th MPNN layer, $W_{i}, W_{j} \in \mathbb{R}^{d_{r} \times d_{r}}$ and $W_{o 2 e}$ are the weight metrics. The relation-centric MPNN can incorporate not only simple $(u, v)$ pairs but also edge features with contextual neighborhood information, facilitating finegrained scene graph generation. The effectiveness of this method is demonstrated through the long-tail solution shown in Fig. 3.

Part 3) Feature Aggregation. The relation features $e^{H}$ and $z^{H}$, generated using the two MPNN methods were combined via a concatenation operation. Subsequently, the concatenated feature is fed into a fully connected layer to derive the final feature vector $p_{r}$, which encompasses both objectcentric and relation-centric features.

$$
\begin{equation*}
p_{r}=\sigma\left(F C\left(\left[e^{H} \| z^{H}\right]\right)\right) \tag{7}
\end{equation*}
$$

Here, $\|$ indicates the concatenation operation, and $F C$ denotes a linear layer defined as $F C: \mathbb{R}^{\left(d_{r}+d_{r}\right)} \rightarrow \mathbb{R}^{d_{r}}$. For simplicity, normalization and bias are omitted from Eq. 7.

Scene Graph Prediction and Training. The prediction of the relationship label is inferred through the object feature $u$ extracted from a Faster R-CNN and a simple linear classifier, i.e., the feature vector $p_{r}$ passing through the DualMPNN module. The object features $u$ and $p_{r}$ were applied to the linear layer using softmax.

$$
\begin{equation*}
\hat{u}=\operatorname{softmax}\left(u W_{o b i}\right), \quad \hat{p_{r}}=\operatorname{softmax}\left(p_{r} W_{r e l}\right) \tag{8}
\end{equation*}
$$

Here, $W_{o b j} \in \mathbb{R}^{d_{o} \times\left|Y_{o b j}\right|}$ and $W_{r e l} \in \mathbb{R}^{d_{r} \times\left|Y_{r e l}\right|}$ are the weight matrix of the linear classifier, and to simplify the equation, the bias vector of the two layers is omitted. Finally, $\hat{u}$ indicates $l \in L$ (i.e., $\hat{u}$ is a feature for the object classification). $Y_{o b j}$ and $Y_{r e l}$ are object and relation labels set, respectively. The object loss $\mathcal{L}_{o b j}$ and relation loss $\mathcal{L}_{r e l}$ are learned to converge in the direction in which the joint loss $\mathcal{L}$ is minimized.

$$
\begin{equation*}
\mathcal{L}_{o b j}=\frac{1}{|\mathcal{V}|} \sum_{i=1}^{|\mathcal{V}|} \mathcal{L}_{C E}\left(y_{i}, \hat{u}_{i}\right), \quad \mathcal{L}_{r e l}=\frac{1}{|\mathcal{E}|} \sum_{i=1}^{|\mathcal{E}|} \mathcal{L}_{C E}\left(s_{i}, \hat{p_{r i}}\right) \tag{9}
\end{equation*}
$$

$$
\begin{equation*}
\mathcal{L}=\mathcal{L}_{o b j}+\mathcal{L}_{r e l} \tag{10}
\end{equation*}
$$

Here, $y_{i} \in Y_{o b j}$ and $s_{i} \in Y_{\text {rel }}$ indicate the ground-truth vectors of the object and relation labels, respectively. $\mathcal{L}_{C E}$ is the cross-entropy loss.

## Experiments

Datasets. The proposed EdgeSGG enables relation-centric learning through edge dual graphs in public datasets for SGG tasks. Model training and evaluation were conducted using two types of datasets.

- Visual Genome (VG) dataset (Xu et al. 2017b) contains 108k images, with detailed annotations of objects

| Method | PredCls |  | SGCls |  | SGGen |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | mR@ 50/100 | $\mathbf{R @} @ 50 / 100$ | mR@ 50/100 | R@ $50 / 100$ | mR@50/100 | R@ $50 / 100$ |
| "Motifis (Zellers et al. 2018) | ![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-5.jpg?height=43&width=210&top_left_y=265&top_left_x=667) | $666.0 / 67.9$ | ![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-5.jpg?height=43&width=210&top_left_y=265&top_left_x=1092) | ![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-5.jpg?height=43&width=195&top_left_y=265&top_left_x=1315) | ![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-5.jpg?height=43&width=210&top_left_y=265&top_left_x=1516) | ![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-5.jpg?height=43&width=203&top_left_y=265&top_left_x=1739) |
| VCTree (Tang et al. 2019) | 15.4 / 16.6 | $65.5 / 67.4$ | 7.4 / 7.9 | $38.9 / 39.8$ | $6.6 / 7.7$ | 31.8 / 36.1 |
| G-RCNN (Yang et al. 2018) | 16.4 / 17.2 | $65.4 / 67.2$ | $9.0 / 9.5$ | $37.0 / 38.5$ | 5.8 / 6.6 | 29.7 / 32.8 |
| MSDN (Li et al. 2017) | $15.9 / 17.5$ | 64.6 / 66.6 | 9.3 / 9.7 | 38.4 / 39.8 | $6.1 / 7.2$ | $31.9 / 36.6$ |
| Unbiased (Tang et al. 2020b) | $25.4 / 28.7$ | 47.2 / 51.6 | $12.2 / 14.0$ | $25.4 / 27.9$ | $9.3 / 11.1$ | 19.4 / 23.2 |
| GPS-Net (Lin et al. 2020) | $15.2 / 16.6$ | 65.2 / 67.1 | 8.5 / 9.1 | $37.8 / 39.2$ | 6.7 / 8.6 | $31.1 / 35.9$ |
| RU-Net (Lin et al. 2022) | $-/ 24.2$ | $67.7 / 69.6$ | $-/ 14.6$ | 42.4 / 43.3 | $-/ 10.8$ | $32.9 / 37.5$ |
| R-CAGCN (Yang et al. 2021) | $18.3 / 19.9$ | 66.6 / 68.3 | 10.2 / 11.1 | $38.3 / 39.0$ | 7.9 / 8.8 | $28.1 / 31.3$ |
| Nice-Motif (Li et al. 2022a) | $29.9 / 32.3$ | $55.1 / 57.2$ | $16.6 / 17.9$ | $33.1 / 34.0$ | $12.2 / 14.4$ | $27.8 / 31.8 \quad-$ |
| PPDL (Li et al. 2022b) | $32.2 / 33.3$ | $47.2 / 47.6$ | 17.5 / 18.2 | 28.4 / 29.3 | 11.4 / 13.5 | $21.2 / 23.9$ |
| BGNN (Li et al. 2021) | $30.4 / 32.9$ | $59.2 / 61.3$ | $\overline{14.3} / 16.5$ | $37.4 / 38.5$ | $10.7 / 12.6$ | $31.0 / 35.8$ |
| IS-GGT (Kundu and Aakur 2023) | $26.4 / 31.9$ | $-1-$ | 15.8 / 18.9 | $-1-$ | $9.1 / 11.3$ | $-1-$ |
| HetSGG (Yoon et al. 2023) | $31.6 / 33.5$ | 57.8 / 58.9 | 17.2 / 18.7 | 37.6 / 38.7 | $12.2 / 14.4$ | 30.0 / 34.6 |
| HetSGG++ (Yoon et al. 2023) | $32.3 / 34.5$ | 57.1 / 59.4 | 15.8 / 17.7 | $37.6 / 38.5$ | 11.5 / 13.5 | $30.2 / 34.5$ |
| PENET (Zheng et al. 2023) | $31.5 / 33.8$ | $68.2 / 70.1$ | 17.8 / 18.9 | 39.4 / 40.7 | $\underline{12.4} / \underline{14.5}$ | $30.7 / 35.2$ |
| EdgeSGG (ours) | 34.7 / 36.9 | $60.1 / 61.8$ | $\mathbf{1 7 . 8}$ / 18.8 | $39.1 / 40.1$ | 13.6 / 15.8 | $29.7 / 34.0$ |

Table 1: Performance comparison with the SoTA SGG Methods on VG dataset. ++ denotes HetSGGplus (Yoon et al. 2023) model. Bold means best score, underline means second-highest score

| Method | mR@50 | R@ 50 | wmAP $_{\text {rel }}$ | wmAP $_{\text {phr }}$ | score $_{\text {wtd }}$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| RelDN (Zhang et al. 2019) | 37.2 | 75.3 | 32.2 | 33.4 | 42.0 |
| VCTree (Tang et al. 2019) | 33.9 | 74.1 | 34.2 | 33.1 | 40.2 |
| G-RCNN (Yang et al. 2018) | 34.0 | 74.5 | 33.2 | 34.2 | 41.8 |
| Motifs (Zellers et al. 2018) | 32.7 | 71.6 | 29.9 | 31.6 | 38.9 |
| Unbiased (Tang et al. 2020b) | 35.5 | 69.3 | 30.7 | 32.8 | 39.3 |
| GPS-Net (Lin et al. 2020) | 38.9 | 74.7 | 32.8 | 33.9 | 41.6 |
| BGNN (Li et al. 2021) | 40.5 | 75.0 | 33.5 | 34.1 | 42.1 |
| HetSGG (Yoon et al. 2023) | 42.7 | 76.8 | 34.56 | 35.5 | 43.3 |
| HetSGG++ (Yoon et al. 2023) | 43.2 | 74.8 | 33.5 | 34.5 | 42.2 |
| PENET (Zheng et al. 2023) | - | 76.5 | $\mathbf{3 6 . 6}$ | $\mathbf{3 7 . 4}$ | $\mathbf{4 4 . 9}$ |
| EdgeSGG (ours) | $\mathbf{4 3 . 3}$ | $\mathbf{7 7 . 1}$ | 36.4 | $\mathbf{3 7 . 4}$ | $\mathbf{4 4 . 9}$ |

Table 2: Performance comparison with the SoTA methods on OI dataset.

and their relationships. Each image was annotated with bounding boxes and class labels for an average of 150 objects and 50 relationship labels for object pairs. The dataset presents a challenge owing to its long-tailed distribution of relationships. For a fair evaluation, the VG dataset is commonly split into training, validation, and testing sets.

- OpenImages V6 (OI) dataset (Kuznetsova et al. 2020) is a large-scale dataset commonly used for SSG tasks. It contains a diverse collection of over 133k images with 126,368 training, 1,813 validation, and 5,322 testing images. This dataset covers a wide range of real-world scenarios. The OI provides object-level annotations for each image, including bounding boxes and 301 object categories. In addition, it includes 31 relationship annotations that describe the interactions and connections between pairs of objects within a scene.

Subtasks. The SGG task can be divided into four main subtasks:

- Predicate Classification (PredCls) focuses on classifying the relationships between two objects in each image. The input to this task was an image along with object proposals that included ground-truth bounding boxes and labels.
- Scene Graph Classification (SGCls) uses image and object proposals as inputs, similar to PredCls. However, in contrast to PredCls, object proposals contain only ground truth bounding boxes without labels.
- Scene Graph Generation (SGGen) is the most challenging subtask because it simulates real-world conditions. It requires the model to detect objects (i.e., determine the bounding boxes and labels) in an image and predict the relationships between them.

Metrics. In our experiments, we employed two key evaluation metrics (i.e., recall@K and Mean Recall@K) to assess the performance of the proposed EdgeSGG and compare it with previous SoTA approaches. For all of the experiments, we calculated the metrics for $\mathrm{K} \in\{50,100\}$.

- Recall@K $(\mathbf{R} @ \mathbf{K})$ is a commonly used metric in SGG tasks that measures the ability of a model to predict at least one correct relationship among the top K predicted relationships for each object in a given scene.
- Mean Recall@K $(\mathbf{m R} @ K)$ is the average of $\mathrm{R} @ \mathrm{~K}$ scores across all objects in a scene. This provides a more comprehensive evaluation of the ability of the model to predict relationships for different objects in a scene.

In addition to these two metrics, we employed the following three additional metrics to provide a more comprehensive assessment of the SGG methods:

- Weighted Mean Average Precision for Relationships ( $\mathbf{w m A P}_{\text {rel }}$ ) evaluates the performance of the model in predicting the relationships between object pairs. It calculates the mean AP for each relationship category, weighted by the number of ground-truth instances of that relationship in a dataset. It provides a more balanced evaluation by considering the varying frequencies of different relationship types in scene graphs.
- Weighted Mean Average Precison for Phrases ( $\mathbf{w m A P} \mathbf{p h r}$ ) assesses the ability of the model to predict

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-6.jpg?height=547&width=1781&top_left_y=179&top_left_x=169)

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-6.jpg?height=474&width=875&top_left_y=197&top_left_x=186)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-6.jpg?height=472&width=876&top_left_y=195&top_left_x=1058)

(b)

Figure 3: Predicate prediction changes in VG dataset according to learning progress. (a) is the predicate result of iteration $=$ 1500 and (b) is the predicate result of iteration $=49500$. The red bar represents head, the green bar represents body, and the blue bar represents tail label. The dot represents the recall of each label.

| Learning <br> Object |  | RGGGen |  |  |
| :---: | :---: | :---: | :---: | :---: |
| $\checkmark$ |  | 9.3 | 12.2 | 14.0 |
| mR@ | mRo |  |  |  |
|  | $\checkmark$ | 9.9 | 12.7 | 14.7 |
| $\checkmark$ | $\checkmark$ | $\mathbf{1 0 . 4}$ | $\mathbf{1 3 . 6}$ | $\mathbf{1 5 . 8}$ |

Table 3: Performance impact of the proposed edge dual scene graph of DualMPNN.

relationship phrases involving both object categories and their corresponding relationships.

- Weighted Score (score ${ }_{w t d}$ ) is a comprehensive evaluation metric that combines the performance of the model with both the relationship and phrase predictions, considering their relative importance in scene graphs. This is the weighted sum of $\mathbf{w m A} \mathbf{P}_{r e l}$ and $\mathbf{w m} \mathbf{A} \mathbf{P}_{p h r}$, where the weights are determined based on the significance of the relationships and phrases in a dataset. score ${ }_{w t d}$ was calculated as: $\mathbf{s c o r e}_{w t d}=0.2 \times \mathrm{R} @ 50+0.4 \times \mathbf{w m A P}_{r e l}$ $+0.4 \times \mathbf{w m A P}_{p h r}$.

Setup. All experiments were conducted on a private machine equipped with two $\operatorname{Intel}(\mathrm{R}) \mathrm{Xeon}(\mathrm{R})$ CPUs, i.e., Gold 6230R CPU @ 2.10GHz; 128GB RAM, and four NVIDIA RTX 3090 GPUs. We used the SGD optimizer, the detailed settings and hyperparameters of which can be found in Appendix $A$.

## Quantitative Experiments

Performance comparison on VG dataset. Table 1 shows the measured performances using four key metrics, $\mathrm{mR} @ 50 / 100$ and $\mathrm{R} @ 50 / 100$, for each subtask. Our proposed EdgeSGG demonstrated an outstanding performance across all subtasks and $\mathrm{mR} @ \mathrm{~K}$ metrics when compared with the SoTA approaches. Because the VG dataset has an imbalanced data distribution, $\mathrm{mR} @ \mathrm{~K}$, which prefers tail predicates, can be said to be more reliable than $\mathrm{R} @ \mathrm{~K}$ metrics that focus on common predictions with abundant samples. Although EdgeSGG has slightly lower scores for R@50 and 100 than other SoTA methods, its relatively high score

| Variants | $\mathbf{m R} @ 20$ | SGGen <br> mR@50 | mR@100 |
| :---: | :---: | :---: | :---: |
| $\overline{\text { Mean }}$ | $\overline{99.7}$ | $\overline{12.7}$ | $\overline{15.0}$ |
| Multiple | 10.1 | 12.9 | 15.1 |
| Concat | 10.4 | 13.6 | 15.8 |

Table 4: Performance changes according to the proposed feature aggregation function type.

for $\mathrm{mR} @ \mathrm{~K}$ indicates that it is also robust to imbalanced data distributions. For PredCls, EdgeSGG achieved 2.4\% higher mR@ $00 / 100$ scores than the second-highest method, HetSGG, indicating its effectiveness and generic capture of more relevant predicates within the top-50 and top-100 predictions, respectively. Similarly, for SGCls, our EdgeSGG showed the highest score for PENET at mR@50/100. The ability of EdgeSGG to capture fine-grained scene graph classifications is evident from its superior performance among all metrics. In the case of SGGen, the proposed EdgeSGG outperformed all the other methods in terms of $\mathrm{mR} @ 50 / 100$. The results indicate the robustness of our method in accurately and consistently detecting scene graph relationships. More importantly, when comparing (Lin et al. 2020; Li et al. 2021; Yoon et al. 2023) which are the same paradigms as the MPNN methods, we observed that the proposed EdgeSGG is superior among all subtasks. This indicates that for effective scene graph prediction, the consideration of not only objectcentric MPNNs, but also relation-centric MPNNs, helps improve the performance. The consistent superiority of our method in capturing fine-grained relationships and contextual information validates its potential in advancing SoTA approaches used in SGG on the VG dataset.

Analysis on long-tail distribution. In Fig. 3, we present a performance analysis of the proposed EdgeSGG on the long-tail distribution. Figure 3 (a) shows a performance bar chart for each class during an early training iteration, and Fig. 3 (b) displays a performance bar chart for a late training iteration. The bars in the charts represent the number of accurately classified objects for each class, providing in-

![](https://cdn.mathpix.com/cropped/2024_05_26_4b65af044149a259cef2g-7.jpg?height=674&width=1440&top_left_y=164&top_left_x=342)

Image
BGNN
HetSGG++
EdgeSGG

Figure 4: Quantitative performance comparison for VG dataset. We confirm that the relation-centric MPNN of EdgeSGG enables a more fine-grained scene graph prediction than the comparison methods. The assessment was conducted on the PredCls subtask. (Red: incorrect predictions, Blue: correct predictions which more fine-grained)

sight into the performance of the model for different classes. In addition, the dotted charts placed above each bar indicate the recall for each class, representing the ability of the model to correctly predict relationships for objects within that class. Analyzing Fig. 3 (a), we can observe that classes with fewer instances have lower bars, indicating that the model faces challenges in accurately predicting these less frequent classes during the early training phase. However, as the training progresses, Fig. 3 (b) shows a significant improvement, with higher bars for the previously underrepresented classes. This improvement was not limited to the tail, but was also found in the head and body parts, confirming that the proposed EdgeSGG provides an overall performance improvement. Moreover, the dot charts above each bar in Fig. 3 (b) show increasing recall scores across classes, further highlighting the ability of the model to generalize and predict relationships for objects in the long-tail classes.

## Ablation Studies

The effect of edge dual scene graph. In this section, we describe an ablation study conducted to investigate the performance impact of the proposed edge dual scene graph in the SGDet subtask. Table 3 presents the evaluation results, with each row corresponding to two settings: object- and relationcentric learning of DualMPNN. Each column represents the evaluation metric mR@20/50/100 for the SGDet subtask. In object-centric learning, we train the entire architecture without utilizing the proposed edge dual scene graph in the DualMPNN, whereas relation-centric learning uses an edge dual scene graph without object-centric learning. Table 3 clearly shows that when we used object- and relation-centric learning together, the performance was consistently better than when we used only object- or relation-centric learning for all metrics. This result highlights the significant performance improvement achieved by integrating the objectand relation-centric aspects of the edge dual scene graph in DualMPNN. This ablation study demonstrated that the edge dual scene graph plays a vital role in boosting the performance of EdgeSGG. By leveraging relation-centric learning through the edge dual scene graph, the proposed EdgeSGG outperforms the object-centric MPNN.

Feature aggregation strategies. To determine the optimal method for aggregating objects and relation-centric features, we conducted an ablation study using a feature combination method. Table 4 presents a performance comparison of three methods, i.e., mean, multiplication, and cat, for the SGGen task. The mean and multiplication methods were $0.7 \%, 0.9 \%$ and $0.8 \%$, and $0.3 \%, 0.7 \%$ and $0.7 \%$ lower for $\mathrm{mR} @ 20 / 50 / 100$ in comparison to the Concat method, respectively. This can be attributed to the inherent nature of the mean and multiplication methods, which mix rather than preserve the features. By contrast, the Concat method demonstrated a superior performance because it was able to construct a feature vector that effectively reflects the characteristics of each individual feature.

## Visualization

Compare with SoTA method. To obtain more convincing quantitative results, we compared the predicted scene graph results of EdgeSGG with those of BGNN, HetSGG, an MPNN paradigm for SoTA models. As illustrated in Fig. 4 , the proposed method updates the feature by considering the relationships between relationships, making it possible to predict a finer-grained scene graph. In particular, in relation of the man-board in Fig 4 (b), we can see that the predicate "carrying" is more detailed than the previous methods.

## Conclusions

We proposed EdgeSGG, an MPNN based on an edge dual scene graph, and a novel method for scene graph generation. We demonstrate that the proposed method outperforms the SoTA SGG models on the benchmark datasets. The proposed relation-centric MPNN method is applicable to various graph and SGG tasks. However, the computational cost
of generating an additional edge dual scene and message passing process is considered a limitation of the proposed method. Therefore, in a future study, we plan to develop a more efficient edge dual scene graph that reduces the computational cost of message passing and can be applied to various scene interpretation tasks.

## References

Ahn, D.; Kim, S.; Hong, H.; and Ko, B. C. 2023. STARTransformer: a spatio-temporal cross attention transformer for human action recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 3330-3339.

Bogdanov, P.; Dereli, N.; Dang, X.-H.; Bassett, D. S.; Wymbs, N. F.; Grafton, S. T.; and Singh, A. K. 2017. Learning about learning: Mining human brain sub-network biomarkers from fMRI data. PLoS one, 12(10): 1-22.

Chen, T.; Yu, W.; Chen, R.; and Lin, L. 2019. Knowledgeembedded routing network for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6163-6171.

Ghosh, S.; Burachas, G.; Ray, A.; and Ziskind, A. 2019. Generating natural language explanations for visual question answering using scene graphs and visual attention. arXiv preprint arXiv:1902.05715.

Guo, Y.; Gao, L.; Wang, X.; Hu, Y.; Xu, X.; Lu, X.; Shen, H. T.; and Song, J. 2021. From general to specific: Informative scene graph generation via balance adjustment. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), 16383-16392.

Jung, D.; Kim, S.; Kim, W. H.; and Cho, M. 2023. Devil's on the Edges: Selective Quad Attention for Scene Graph Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1866418674.

Kundu, S.; and Aakur, S. N. 2023. IS-GGT: Iterative Scene Graph Generation With Generative Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6292-6301.

Kuznetsova, A.; Rom, H.; Alldrin, N.; Uijlings, J.; Krasin, I.; Pont-Tuset, J.; Kamali, S.; Popov, S.; Malloci, M.; Kolesnikov, A.; et al. 2020. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 128(7): 1956-1981.

Li, L.; Chen, L.; Huang, Y.; Zhang, Z.; Zhang, S.; and Xiao, J. 2022a. The devil is in the labels: Noisy label correction for robust scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 18869-18878.

Li, R.; Zhang, S.; Wan, B.; and He, X. 2021. Bipartite graph network with adaptive message passing for unbiased scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), $11109-11119$.

Li, W.; Zhang, H.; Bai, Q.; Zhao, G.; Jiang, N.; and Yuan, X. 2022b. Ppdl: Predicate probability distribution based loss for unbiased scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 19447-19456.

Li, Y.; Ouyang, W.; Zhou, B.; Wang, K.; and Wang, X. 2017. Scene graph generation from objects, phrases and region captions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 1261-1270.

Lin, X.; Ding, C.; Zeng, J.; and Tao, D. 2020. Gps-net: Graph property sensing network for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3746-3753.

Lin, X.; Ding, C.; Zhang, J.; Zhan, Y.; and Tao, D. 2022. Ru-net: Regularized unrolling network for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1945719466.

Lu, C.; Krishna, R.; Bernstein, M.; and Fei-Fei, L. 2016. Visual relationship detection with language priors. In Proceedings of the European Conference on Computer Vision (ECCV), 852-869.

Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP), 1532-1543.

Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems (NeurIPS).

Shit, S.; Koner, R.; Wittmann, B.; Paetzold, J.; Ezhov, I.; Li, H.; Pan, J.; Sharifzadeh, S.; Kaissis, G.; Tresp, V.; et al. 2022. Relationformer: A unified framework for image-tograph generation. In Proceedings of the European Conference on Computer Cision (ECCV), 422-439.

Tang, K.; Niu, Y.; Huang, J.; Shi, J.; and Zhang, H. 2020a. Unbiased scene graph generation from biased training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3716-3725.

Tang, K.; Niu, Y.; Huang, J.; Shi, J.; and Zhang, H. 2020b. Unbiased scene graph generation from biased training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3716-3725.

Tang, K.; Zhang, H.; Wu, B.; Luo, W.; and Liu, W. 2019. Learning to compose dynamic tree structures for visual contexts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6619-6628.

Xie, S.; Girshick, R.; Doll√°r, P.; Tu, Z.; and He, K. 2017. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1492-1500.

Xu, D.; Zhu, Y.; Choy, C. B.; and Fei-Fei, L. 2017a. Scene graph generation by iterative message passing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5410-5419.

Xu, D.; Zhu, Y.; Choy, C. B.; and Fei-Fei, L. 2017b. Scene graph generation by iterative message passing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5410-5419.

Yang, G.; Zhang, J.; Zhang, Y.; Wu, B.; and Yang, Y. 2021. Probabilistic modeling of semantic ambiguity for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12527-12536.

Yang, J.; Lu, J.; Lee, S.; Batra, D.; and Parikh, D. 2018. Graph r-cnn for scene graph generation. In Proceedings of the European conference on computer vision (ECCV), 670685.

Yang, X.; Tang, K.; Zhang, H.; and Cai, J. 2019. Autoencoding scene graphs for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10685-10694.

Yoon, K.; Kim, K.; Moon, J.; and Park, C. 2023. Unbiased Heterogeneous Scene Graph Generation with RelationAware Message Passing Neural Network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 3285-3294.

Zellers, R.; Yatskar, M.; Thomson, S.; and Choi, Y. 2018. Neural motifs: Scene graph parsing with global context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5831-5840.

Zhang, J.; Shih, K. J.; Elgammal, A.; Tao, A.; and Catanzaro, B. 2019. Graphical contrastive losses for scene graph parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1153511543.

Zheng, C.; Lyu, X.; Gao, L.; Dai, B.; and Song, J. 2023. Prototype-based Embedding Network for Scene Graph Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2278322792.
