# InversionView: A General-Purpose Method for Reading Information from Neural Activations 

Xinting Huang ${ }^{1}$, Madhur Panwar ${ }^{2}$, Navin Goyal ${ }^{2}$, Michael Hahn ${ }^{1}$<br>${ }^{1}$ Saarland University, Germany, \{xhuang, mhahn\}@lst.uni-saarland.de<br>${ }^{2}$ Microsoft Research India, \{t-mpanwar, navingo\}@microsoft.com


#### Abstract

The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present three case studies where we investigate models ranging from small transformers to GPT2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.


## 1 Introduction

Despite their huge success, neural networks are still widely considered black boxes. One of the most important reasons is that the continuous vector representations in these models pose a significant challenge for interpretation. If we could understand what information is encoded in the activations of a neural model, significant progress might be achieved in fully deciphering the inner workings of neural networks, which would make modern AI systems safer and more controllable. Toward this goal, various methods have been proposed for understanding the inner activations of neural language models. They range from supervised probes [2, 5, 4, 40] to projecting to model's vocabulary space [30, 7] to causal intervention [16, 39, 20, 11] on model's inner states. However, to this date, decoding the information present in neural network activations in human-understandable form remains a major challenge. Supervised probing classifiers require the researcher to decide which specific information to probe for, and does not scale when the space of possible outputs is very large. Projecting to the vocabulary space is restricted in scope, as it only produces individual tokens. Causal interventions uncover information flow, but do not provide direct insight into the information present in activations.

Here, we introduce InversionView as a principled general-purpose method for generating hypotheses about the information present in activations in neural models on language and discrete sequences, which in turn helps us put together the algorithm implemented by the model. InversionView aims at providing a direct way of reading out the information encoded in an activation. The technique starts from the intuition that the information encoded in an activation can be formalized as its preimage, the set of inputs giving rise to this particular activation under the given model. In order to explore this preimage, given an activation, we train a decoder to sample from this preimage. Inspection of the preimage, across different inputs, makes it easy to identify which information is passed along, and which information is forgotten. It accounts for the geometry of the representation, and can identify which information is reinforced or downweighted at different model components. InversionView facilitates the interpretation workflow, and provides output that is in principle amenable to automated interpretation via LLMs (we present a proof of concept in Section 4).

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-02.jpg?height=241&width=984&top_left_y=237&top_left_x=560)

Figure 1: Illustration of the geometry at two different activation sites, encoding different information about the input. On the left, the semantics of being on leave are encoded. On the right, the information that the subject of the input sentence is John is encoded.

We showcase the usefulness of the method in three case studies: a character counting task, Indirect Object Identification, and 3-digit addition. The character counting task illustrates how the method uncovers how information is processed and forgotten in a small transformer. In Indirect Object Identification in GPT2-Small [39], we use InversionView to easily interpret the information encoded in the components identified by [39], substantially simplifying the interpretability workflow. For 3-digit addition, we use InversionView to provide for the first time a fully verified circuit sufficient for performing the task. Across the case studies, InversionView allows us to rapidly generate hypotheses about the information encoded in each activation site. Coupled with attention patterns or patching methods, we reverse-engineer the flow of information, which we verify using causal interventions.

## 2 Methodology

### 2.1 Interpretation Framework

What information is encoded by an activation in a neural network? InversionView answers this in terms of the inputs that give rise to a given activation (Figure 1). For instance, if a certain activation encodes solely that "the subject is John" (Figure 1, right), then it will remain unchanged when other parts in the sentence change while preserving this aspect (e.g., "John is on leave today." $\Rightarrow$ "John has a cute dog."). Building on this intuition, given an activation, InversionView aims to find those inputs that give rise to the same activation, and examine what's common among them to infer what information it encodes. In realistic networks, different inputs will rarely give rise to exactly the same activation. Rather, different changes to an input will change the activation to different degrees. The sensitivity of an activation to different changes reflects the representational geometry: larger changes make it easier for downstream components to read out information than very small changes. This motivates a threshold-based definition of preimages, where we consider information as present in an activation when the activation is sufficiently sensitive to it. Formally speaking, given a space $\mathcal{X}$ of valid inputs, a query input $\mathbf{x}^{q} \in \mathcal{X}$, a function $f$ that represents the activation of interest as a function of the input, and a query activation $\mathbf{z}^{q}=f\left(\mathbf{x}^{q}\right)$, define the $\epsilon$-preimage:

$$
\begin{equation*}
B_{\mathbf{z}^{q}, f, \epsilon}=\left\{\mathbf{x} \in \mathcal{X}: D\left(f(\mathbf{x}), \mathbf{z}^{q}\right) \leq \epsilon\right\} \tag{1}
\end{equation*}
$$

where $\epsilon>0$ is a threshold and $D(\cdot, \cdot)$ is a distance metric. Both $\epsilon$ and $D(\cdot, \cdot)$ are chosen by the researcher based on representation geometry; we will define these later in case studies. In practice, in all our three case studies, we vary $\epsilon$ and set it so we can read out coherent concepts from the $\epsilon$-preimage (Appendix A.4). With a threshold-based definition, we consider only those pieces of information that have substantial impact on the activation. See more discussion in Appendix A. 1

### 2.2 Conditional Decoder Model

Directly enumerating $B_{\mathbf{z}^{q}, f, \epsilon}$ is in general not scalable, as the input space grows exponentially with the sequence length. To efficiently inspect $B_{\mathbf{z}^{q}, f, \epsilon}$, we train a conditional decoder model that takes as input the activation $\mathbf{z}^{q}$ and generates inputs giving rise to similar activations in the model under investigation. In the following, we refer to the original model that we are interpreting as the probed model, the conditional decoder as the decoder, the place in the probed model from which we take the activation as the activation site (e.g., the output of $i$ th layer), the inputs generated by the decoder as samples, and the index of a token in the sequence as position.

We implement the decoder as an autoregressive language model conditioned on $\mathbf{z}^{q}$, decoding input samples $\mathbf{x}$. As the decoder's training objective corresponds to recovering $\mathbf{x}$ exactly, sampling
at temperature 1 will typically not cover the full $\epsilon$-preimage. We increase diversity by drawing samples at higher temperatures and with noise added to $\mathbf{z}^{q}$ (details in Appendix A.2). We then evaluate $D\left(f(\mathbf{x}), \mathbf{z}^{q}\right)$ at each position in each sample $\mathbf{x}$, select the position minimizing $D$ determine membership in $B_{\mathbf{z}^{q}, f, \epsilon}$, and subsample in- $\epsilon$-preimage and out-of- $\epsilon$-preimage samples for inspection.

An important question is whether this method, relying on a black-box decoder, produces valid $\epsilon$ preimages. As we evaluate $D\left(f(\mathbf{x}), \mathbf{z}^{q}\right)$ for each sample $\mathbf{x}$, correctness (are all generated samples in the $\epsilon$-preimage?) is ensured by design. We explicitly verify completeness (are all elements of the $\epsilon$-preimage generated at reasonable probability?) by enumerating inputs in one of our case studies (Appendix B). Another, more scalable, approach is to design counter-examples $\mathbf{x}$ not satisfying a hypothesis about the content of $B_{\mathbf{z}^{q}, f, \epsilon}$, and checking if they in fact are outside of it. In our experiments, we found that these examples were always outside of $B_{\mathbf{z}^{q}, f, \epsilon}$.

## 3 Discovering the Underlying Algorithm by InversionView

### 3.1 Setup and Background

Notation. Using notation based on [14] and [29], we denote the residual stream [14] as $x^{i,\{\text { pre,mid,post }\}} \in \mathbb{R}^{N \times d}$, where $i$ is the layer (an attention (sub)layer + an MLP (sub)layer) index, $N$ is the number of input tokens, $d$ is the model dimension, pre, mid, post stand for the residual stream before the attention layer, between attention and MLP layer, and after the MLP layer. For example, $x^{0, \text { pre }}$ is the sum of token and position embedding, $x^{0, \text { mid }}$ is the sum of the output of the first attention layer and $x^{0, \text { pre }}$, and $x^{0, \text { post }}$ is the sum of the output of the first MLP layer and $x^{0, \text { mid }}$. Note that $x^{i, \text { post }}=x^{i+1, \text { pre }}$. We use subscript $t$ to refer to the activation at token position $t$, e.g., $x_{t}^{i, \text { mid }} \in \mathbb{R}^{d}$. The attention layer output decomposes into outputs of individual heads $h^{i, j}(\cdot)$, i.e., $x^{i, \text { mid }}=x^{i, \text { pre }}+\sum_{j} h^{i, j}\left(\mathrm{LN}\left(x^{i, \text { pre }}\right)\right)$, where $\mathrm{LN}(\cdot)$ represents layer normalization (GPT style/pre-layer-norm). We denote the attention head's output as $a^{i, j}$, i.e., $a^{i, j}=h^{i, j}\left(\operatorname{LN}\left(x^{i, \operatorname{pre}}\right)\right)$.

Decoder Architecture. We train a single two-layer transformer decoder across all activation sites of interest. The query activation $\mathbf{z}^{q}$ is concatenated with an activation site embedding $\mathbf{e}$, a learned embedding layer indicating where the activation comes from, passed through multiple MLP layers with residual connections, and then made available to the attention heads in each layer of the decoder, alongside the already present tokens from the input, so that each attention head can also attend to the post-processed query activation in addition to the context tokens. Appendix Chas technical details. Each training example is a triple consisting of an activation vector $\in \mathbb{R}^{d}$, the activation site index, and the input, on which the decoder is trained with a language modeling objective.

### 3.2 Character Counting

We train a transformer (2 layers, 1 head) on inputs such as "vvzccvczvvvzvcvc|v:8" to predict the last token " 8 ", the frequency of the target character (here, " $v$ ") before the separator " $\mid$ ". For each input, three distinct characters are sampled from the set of lowercase characters, and each character's frequency is sampled uniformly from 1-9. The input length varies between 7 and 31 . We created $1.56 \mathrm{M}$ instances and applied a $75 \%-25 \%$ train-test split; test set accuracy is $99.53 \%$. See details in Appendix D. For InversionView, we use $D\left(\mathbf{z}, \mathbf{z}^{q}\right)=\frac{\left\|\mathbf{z}-\mathbf{z}^{q}\right\|_{2}}{\left\|\mathbf{z}^{q}\right\|_{2}}$ (i.e., normalized euclidean distance), as the magnitude of activations varies between layers, and the threshold $\epsilon=0.1$.

Interpreting via InversionView and attention. In layer 0 , the target character consistently attends to the same character in the previous context, suggesting that counting happens here. In Figure 2a. we show the $\epsilon$-preimage of $x_{t c}^{0 \text {,mid }}$ and $x_{t c}^{0 \text {,post }}$, where the subscript $t c$ denotes the target character. We show $\approx 10$ random samples at a single query input, but our hypotheses are based on-and easily confirmed by-rapid visual inspection of dozens of inputs across different query inputs. ${ }_{2}^{2}$ On the left (before the MLP), the activation encodes the target character, as all samples have " $\mathrm{g}$ " as the target character. Count information is not sharply encoded: while the closest activation corresponds to[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-04.jpg?height=520&width=1394&top_left_y=239&top_left_x=362)

(a)

(b)

Figure 2: InversionView on Character Counting Task. The model counts how often the target character (after ' $l$ ') occurs in the prefix (before ' $l$ '). B and $E$ denote the beginning and end of sequence tokens. The query activation conditions the decoder to generate samples that capture its information content. We show non-cherrypicked samples inside and outside the $\epsilon$-preimage $(\epsilon=0.1)$ at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. True count indicates the correct count of the target character in the samples (decoder may generate an incorrect count). (a) MLP layer amplifies count information. By comparing the distances before (left) and after (right) the MLP, we can see samples with diverging counts become much more distant from the query activation. (b) In the next layer (when the information is copied from residual stream of target character to the residual stream of ":" via attention, since ":" exclusively attends to target character), the count is retained but the identity of the target character is no longer encoded ("c", "m", etc. instead of " $\mathrm{g}$ "), as it is no longer relevant for the predicting the count. Therefore, observing the generations corresponding to activation sites informs us of their content and how it changes across activation sites.

" $\mathrm{g}$ " occurring 3 times, two activations corresponding to a count-4 input (" $\mathrm{g}$ " occurring 4 times) are also close, even closer than a count-3 input. On the other hand, on the right (after the MLP), only count- 3 inputs are inside the $\epsilon$-preimage, and count-4 inputs become much more distant than before. Comparing the $\epsilon$-preimage before and after the MLP in layer 0, we find that the MLP makes the count information more prominent in the representational geometry of the activation. The examples are not cherry-picked; count information is generally reinforced by the MLP across query inputs.

In the next layer, the colon consistently attends to the target character, and InversionView confirms that count information is moved to the colon's residual stream (Figure 2b). More importantly, this illustrates how information is abstracted: We previously found that $x_{t c}^{0, p o s t}$ encodes identity and frequency of the target character. However, the colon obtains only an abstracted version of the information, in which count information remains while the target character is largely (though not completely) removed. InversionView makes this process visible, by showing that the target character becomes interchangeable with little change to the activation. See more examples in Appendix D. 2 . Overall, with InversionView, we have found a simple algorithm by which the model makes the right prediction: In layer 0 , the target character attends to all its occurrences and obtains the counts. In layer 1, the colon moves the results from the target character to its residual stream and then produces the correct prediction. Accounting for other activation sites, we find that the model implements a somewhat more nuanced algorithm, investigated in Appendix D. 4 Overall, InversionView shows how certain information is amplified, providing an understanding of the effect of the MLP layer. More importantly, InversionView also shows how information is abstracted or forgotten.

Quantitative verification. We causally verified our hypothesis using activation patching [38, 16] on (position, head output) pairs. As the attention head in layer 1 attends almost entirely to the target character, only heads $a_{t c}^{0,0}, a_{:}^{0,0}$, and $a_{:}^{1,0}$ can possibly play a role in routing count information. We patch their outputs with activations from a contrast example flipping a single character before " $\mid$ ". To verify dependencies, we patch cumulatively, starting either at the lowest or highest layer, with
![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-05.jpg?height=358&width=354&top_left_y=282&top_left_x=387)
<|endoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin
0.000;<lendoftext|After Erin and Justin went to the house, Erin gave a ring(to) Justin
Generated Samples
0.024;<<endoftext|\The station Sara and Justin went to had a kiss. Sara gave it( to) Justin[EOS]
0.024;<<endoftext>When Paul and Justin got a kiss at the school, Paul decided to give it(to) Justin[EOS
<lendoftext|Then, Alicia and Justin had a long argument. Afterwards Alicia said( to) Justin[EOS]
0.034;<lendoftext\Then, Justin and Erin went to the garden. Erin gave a basketball( to) Justin[EOS
0.037;<lendoftext|After the lunch in the afternoon, Justin and Kristen went to the station. Kristen gave a kiss(to) Justin[EOS]
0.039; <lendoftext|AAter taking a long break Kimberly and Justin went to the house, Kimberly gave a bone(to) Justin(EOS
* 0.043;<endoftext|WWhile spending time together Justin and Alicia were working at the garden, Alicia gave a kiss (to) Justin[EOS]
0.056;<\endoftext\Then, Justin and Kristen went to the school. Kristen gave a bone(to) Justin[EOS]
O 0.506; ; \endoftext|Priends separated at birth Kristen and Justin found a snack at the garden. Justin gave it(to) Kristen[EOS]

- 0.598;<lendoftext| / While spending time together Michelle and Joshua were commuting to the restaurant, Alexander gave a ring(to) Michelle[EOS)

```

(a) (b)

Figure 3: (a) Character Counting. Activation patching results show that $a_{t c}^{0,0}$ and $a_{:}^{1,0}$ play crucial roles in prediction, as hypothesized based on Figure 2 and Sec. 3.4. In contrast examples, only one character differs. Top: We patch activations cumulatively from left to right. We can see patching $a_{t c}^{0,0}$ accounts for the whole effect, and when $a_{t c}^{0,0}$ is already patched, patching $a_{:}^{1,0}$ has almost no effect. Bottom: On the other hand, if we patch cumulatively from right to left, $a_{:}^{1,0}$ accounts for the whole effect while patching $a_{t c}^{0,0}$ has no effect if $a_{:}^{1,0}$ has been patched. So we verified that $a_{:}^{1,0}$ solely relies on $a_{t c}^{0,0}$ and this path is the one by which the model performs precise counting. The patching effect is averaged across the whole test set. (b) IOI. InversionView applied to Name Mover Head 9.9 at "to"; we fix the compared position to "to". Throughout the $\epsilon$-preimage, "Justin" appears as the IO, revealing that the head encodes this name. This interpretation is confirmed across query inputs.

some fixed ordering within each layer. By the end of patching, the model prediction is on the contrast example by the end of patching. When patching a head, we attribute to it the increment in the difference of $L D$ before and after patching, where $L D$ denotes the logit difference between original count and the count in the contrast example. Cumulative patching allows us to verify that all paths are accounted for; unaccounted paths through a layer would manifest in an increment at a point where all hypothesized connected activations in the other layer have already been patched. Results (Figure 3a) show that patching either of the heads in the hypothesized path $\left(a_{t c}^{0,0}\right.$ and $\left.a_{:}^{1,0}\right)$ is sufficient to absorb the entire effect on logit differences, confirming the hypothesis. See Appendix D. 3 for further details and D. 4 for further experiments.

\subsection*{3.3 IOI circuit in GPT-2 small}

To test the applicability of InversionView to transformers pretrained on real-world data, we apply our method to the activations in the indirect object identification (IOI) circuit in GPT-2 small [34] discovered by Wang et al. [39]. We apply InversionView to the components of the circuit, read out the information, and compare it with the information or function that Wang et al. [39] had ingeniously inferred using a variety of tailored methods, such as patching and investigating effects on logits and attention. We show that InversionView unveils the information contained in the activation sites, with results agreeing with those of Wang et al. [39], while avoiding the need for tailored methods.

The IOI task consists of examples such as "When Mary and John went to the store, John gave a drink to", which should be completed with "Mary". We use S for the subject "John" in the main clause, IO for the indirect object "Mary" introduced in the initial subclause, S1 and S2 for the first and second occurrences of the subject, and END for the "to" after which IO should be predicted. To facilitate comparison, we denote attention heads as in Wang et al. [39] with i.j denoting $h^{i, j}$. Wang et al. [39] discover a circuit of 26 attention heads in GPT-2 small and categorize them by their function. In short, GPT-2 small makes correct predictions by copying the name that occurs only once in the previous context. For InversionView, we train the decoder on the IOI examples (See details in E.1). Despite the size of the probed model, we find the same 2-layer decoder architecture as in Section 3.2 to be sufficient. We use $D\left(\mathbf{z}, \mathbf{z}^{q}\right)=1-\frac{\mathbf{z} \cdot \mathbf{z}^{q}}{\|\mathbf{z}\| \cdot\left\|\mathbf{z}^{q}\right\| \|}$ (i.e., cosine distance), and $\epsilon=0.1$. Euclidean distance leads to similar results, but cosine distance is a better choice for this case (Appendix E.4.

We start with the Name Mover Head 9.9, which Wang et al. [39] found moves the IO name to the residual stream of END. 3b shows the $\epsilon$-preimage at "to". The samples in the $\epsilon$-preimage share the name "Justin" as the IO. The head also shows similar activity at some other positions (Appendix A.3).

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-06.jpg?height=604&width=1131&top_left_y=240&top_left_x=497)

(a)

(b)

(c)

Figure 4: InversionView applied to 3-digit addition: Visually inspecting sample inputs inside and outside the $\epsilon$-preimage of the query allows us to understand what information is contained in an activation. The color on each token in generated samples denotes the difference in the token's likelihood between a conditional or unconditional decoder (Appendix G). The shade thus denotes how much the generation of the token is caused by the query activation (darker shade means a stronger dependence). In ( $\mathrm{a}-\mathrm{c}$ ), the colored tokens are most relevant to the interpretation. We interpret two attention heads (a,b) and the output of the corresponding residual stream after attention (c). In (a), what's common throughout the $\epsilon$-preimage is that the digits in the hundreds places are 6 and 8 . Inputs outside the $\epsilon$-preimage don't have this property. In (b), what's common is that the digits in tens places are 1,6 , or numerically close. Hence, we can infer that the activation sites $a^{0,0}$ and $a^{0,3}$ encode hundreds and tens place in the input operands respectively; the latter is needed to provide carry to A1. Also, the samples show that the activations encode commutativity since the digits at hundreds and tens place are swapped between the two operands. In (c), the output of the attention layer after residual connection combining information from the sites in (a) and (b) encodes " 6 " and " 8 " in hundreds place, and the carry from tens place. Note that $a^{0,1}$ and $a^{0,2}$ contains similar information as $a^{0,0}$. These observations are confirmed across inputs. Taken together, InversionView reveals how information is aggregated and passed on by different model components.

Results are consistent across query inputs. Therefore, the result of InversionView is consistent with the conclusions of Wang et al. [39] on head 9.9. Applying the same analysis to other heads (Table 2), we recovered information in high agreement with the information that [39] had ingeniously inferred using a variety of tailored methods. Among the 26 attention heads they identified, InversionView indicates a different interpretation in only 3 cases; these $(0.1,0.10,5.9)$ were challenging for the methods used before (Appendix E.3). In summary, InversionView scales to larger models.

\subsection*{3.4 3-Digit Addition}

We next applied InversionView to the problem of adding 3-digit numbers, between 100 and 999 . Input strings have the form "B362+405=767E" or "B824+692=1516E", and are tokenized at the character level. We use F1, F2, F3 to denote the three digits of the first operand and S1, S2, S3 for the digits of the second operand, and A1, A2, A3, A4 (if it exists) for the three or four digits of the answer, and $\mathrm{C} 2, \mathrm{C} 3$ for the carry from tens place and ones place (i.e., $\mathrm{C} 2$ : whether $\mathrm{F} 2+\mathrm{S} 2 \geq 10, \mathrm{C} 3$ : whether $\mathrm{F} 3+\mathrm{S} 3 \geq 10$ ). Unlike Quirke and Barez [33], we do not pad numbers to have all the same length; hence, positional information is insufficient to determine the place value of each digit.

The probed model is a decoder-only transformer (2 layers, 4 attention heads, dimension 32). We set attention dropout to 0 . Other aspects are identical to GPT-2. The model is trained for autoregressive next-token prediction on the full input, in analogy to real-world language models. In testing, the model receives the tokens up to and including " $=$ ", and greedily generates up to "E". The prediction counts as correct if all generated tokens match the ground truth. The same train-test ratio as in Section 3.2 is used. The test accuracy is $98.01 \%$. For other training details see Appendix F.1.

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-07.jpg?height=456&width=352&top_left_y=241&top_left_x=472)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-07.jpg?height=415&width=829&top_left_y=275&top_left_x=862)

(b)

Figure 5: 3-Digit Addition Task: (a) Information flow diagram for predicting A1 inferred via InversionView. The colors denote which places are routed; alternating colors indicate two places are routed. This is a subfigure of Figure 29 (b) Validation of (a) via activation patching for the prediction of A1. Like Figure $3 \mathrm{a}, \rightarrow(\leftarrow)$ means cumulatively patching activation from left to right (right to left) on the horizontal axis. Left: Patching with activation containing modified F1 and S1 information. Right: Patching with activation containing modified F2 and S2 information. As we can see, components from (a) show a substantial increment if and only if they have a not-yet-patched connection to output (when patching right to left) or input (patching left to right), verifying that (a) causally describes the flow of information. Therefore, InversionView helps us uncover both information flow and content of activations.

Interpreting via InversionView and attention. As Section 3.2 we use normalized Euclidean distance for $D(\cdot, \cdot)$ and the threshold $\epsilon=0.1$. We first trace how the model generates the first answer digit, A1, by understanding the activations at the preceding token, " $=$ ". We first examine the attention heads at " $=$ " in the 0 -th layer (Figure 4). As for the first head $\left(a^{0,0}\right)$, only F1 and S1 matter in the samples - indeed, changing other digits, or swapping their order, has a negligible effect on the activation (Figure 4). Across different inputs, each of the three heads $a^{0,0}, a^{0,1}, a^{0,2}$ encode either one or both of F1 and S1 (Figure 24); taken together, they always encode both. This is in agreement with attention focusing on these tokens. The fourth and remaining head in layer $0\left(a^{0,3}\right)$ encodes F2 and S2, which provide the carry from the tens place to the hundreds place. Combining the information from these four heads, $x^{0, \text { mid }}$ consistently encodes $\mathrm{F} 1$ and $\mathrm{S} 1$; and approximately represents F2, S2-only the carry to A1 (whether F2+S2 $\geq 10$ ) matters here (Figure 4c). Other examples are in Figure 25. We can summarize the function of layer 0 at " $=$ ": Three heads route F1 and S1 to the residual stream of " $=" x_{=}$. The fourth head routes the carry resulting from F2 and S2. Layer 1 mainly forwards information already obtained in layer 0 , and does not consistently add further information for A1. See more examples in Appendix F. 2 .

Figure 5 a shows the circuit predicting A1. InversionView allows us to diagnose an important deficiency of this circuit: even though the ones place sometimes receives some attention in layer 1 , the circuit does not consistently provide the carry from the ones place to the hundreds place, which matters on certain inputs-we find that this deficiency in the circuit accounts for all mistakes made by the model (Appendix F.3). Taken together, we have provided a circuit allowing the model to predict A1 while also understanding its occasional failure in doing so correctly. Corresponding findings for A2, A3, and A4 are in Table 3 and Figure 29. From A2 onwards, InversionView allows us to uncover how the model exhibits two different algorithms depending on whether the resulting output will have 3 or 4 digits. In particular, when predicting A3, the layer 0 circuit is the same across both cases, while the layer 1 circuit varies, since this determines whether A3 will be a tens place or ones place. Beyond figures in the Appendix, we also encourage readers to verify our claims in our interactive web application.

Quantitative verification. We used causal interventions to verify that information about the digits in hundreds and tens place is routed to the prediction of A1 only through the paths determined in Figure 5 a, and none else. Like before, we cumulatively patch the head output on " $=$ " preceding the target token A1, with an activation produced at the same activation site by a contrast example changing both digits in a certain place. Results shown in Figure 5 strongly support our previous
conclusions. For example, $a^{0,3}$ and $a^{1,2}$ are not relevant to F1 and S1. Important heads detected by activation patching, $a^{0,0}, a^{0,1}, a^{0,2}, a^{1,1}$, all contain $\mathrm{F} 1$ and $\mathrm{S} 1$ according to Figure $5 \mathrm{a}$ Furthermore, we can also confirm that $a^{1,1}$ relies on the output of layer 0 as depicted in sub-figure (a): When heads in layer 0 are already patched, patching $a^{1.1}$ has no further effect (value corresponding to $\rightarrow$ is zero), but it has an effect when patching in the opposite direction. On the contrary, $a^{1,0}$ shows little dependence on layer 0 , consistent with Figure $5 \mathrm{a}$ On the right of Figure $5 \mathrm{~b}$, we can confirm that $a^{0,3}$ is important for routing F2 and S2, and the downstream heads in layer 1 rely on it. Findings for other answer digits are similar (See Appendix F.5). Overall, the full algorithm obtained by InversionView is well-supported by causal interventions.

\section*{InversionView reveals granularity} of information. Heads often read from both digits of the same place. Only the sum matters for addition raising the question whether the digits are represented separately, or only as their sum. Unlike traditional probing, InversionView answers this question without designing any tailored probing tasks. In Figure 6 (left), $a^{0,2}$ exactly represents F2 and S2 (here, 2 and 5). Other inputs where $\mathrm{F} 1+\mathrm{S} 1=5+2$ have high $D$. In contrast, on the right, F2 and S2 are represented only by their sum: throughout the $\epsilon$-preimage, $\mathrm{F} 2+\mathrm{S} 2=9$. In fact, we find such sumonly encoding only when F2+S2=9a special case where the ones place of operands affects the hundreds place of the answer via cascading carry. We hypothesize that the model encodes them similarly because these inputs require special treatment. Therefore, even though encoding number pairs by their sum is a good strategy for the addition task from a human perspective, the model only does it as needed. We also observe intermediate cases (Figure 27).

\section*{4 Discussion}

Traditional probing [e.g. 2, 6] trains a supervised classifier to assess how much information about a variable of interest is encoded in an activation site. It requires a hypothesis in advance and is thus inherently limited to hypotheses conceived a priori by the researcher. InversionView, on the other hand, helps researchers form hypotheses without any need for prior guesses, and allows finegrained per-activation interpretation. Inspecting attention patterns [e.g. 10] is a traditional approach to inferring information flow, and we have drawn on it in our analyses. More recently, path patching [39. 20] causally identifies paths along which information flows. In contrast to these approaches, InversionView aims at aiding generation of hypotheses about what information is contained in activations and passed along. While the information flow provides an upper bound on the information passed along by tracing back to the input token, it is insufficient for determining how information is processed and abstracted. For instance, in Section 3.2, occurrences of the target character are causally connected to $a_{t c}^{0,0}$, which then connects to $a_{:}^{1,0}$ (direct or mediated by MLP layer 0 ). Without looking at encoded information, we only know that the information in these paths is related to the occurrences of the target character, but not whether it is their identity, positions, count, etc. More generally, when a component reads a component that itself has read from multiple components, connectivity does not tell us which pieces of information are passed on. We provide further discussion in Appendix $\mathrm{H}$

The samples produced by InversionView can be easily fed into LLMs for automated interpretation. We show a proof of concept by using Claude 3 to interpret the model trained for 3-digit addition.

See results in Table 4 Overall, we find that the interpretation given by the LLM reflects the main information in almost all cases of the addition task. Despite some flaws, the outcome is informative in general, suggesting this as a promising direction for further speeding up hypothesis generation.

InversionView offers distinctive advantages and makes analyses feasible that are otherwise very hard to do with other methods. It can also improve the interpretability workflow in coordination with other methods. For example, one may first use methods such as path patching or attribution [15] to localize activity to specific components, and then understand the function of these components using InversionView. In sum, InversionView is worth adding to the toolbox of interpretability research.

Limitations. InversionView relies on a black-box decoder, which needs to be trained using relevant inputs and whose completeness needs to be validated by counter-examples. Also, InversionView, while easing the human's task, is still not automated, and interpretation can be laborious when there are many activation sites. We focus on models up to $125 \mathrm{M}$ parameters; scaling the technique to models with billions of parameters is an interesting problem for future work, which will likely require advances in localizing behavior to a tractable number of components of interest. Fourth, interpretation uses a metric $D(\cdot, \cdot)$. The geometry, however, in general could be nonisotropic and more complex than the $L^{2}$-based metrics we used. We leave the exploration of this to future work.

\subsection*{4.1 Related Work}

Trained probing classifiers are arguably the most common method for uncovering information from activations [e.g. 2, 5, 4, 40, 37, 24, 25]. More recent methods obtain insights by projecting representations into the vocabulary space [30, 7, 31, 22, 18], patching them into an LLM [19], or by passing them through sparse feature vectors [9, 36, 12]. Dar et al. [13] directly interprets model parameters by projecting them into the embedding space. Another important area of research is uncovering information flow. Geva et al. [18] intervene on attention weights to study information flow. Activation patching, a causal intervention method, can be used to study the causal effect of an activation on the output, and can help localize where information is stored [28, 35], or find alignment between a high-level causal model and inner states of a neural model [16, 17, 41]. While activation patching affects all downstream computation in the model, path patching [39, 20] restricts effects for downstream components in order to identify the path along which information flows [39, 11, 21, 26]. Recent work has started using LLMs to generate interpretations [8, 9].

Related to Section 3.4, [32] interpret the algorithm implemented by a 1-layer 3-head transformer for $n$-digit addition ( $n \in\{5,10,15\}$ ), finding that the model implements the usual addition algorithm with restrictions on carry propagation. In their one-layer setup, attention patterns are sufficient for generating hypotheses. Lengths of operands and results are fixed by prepending 0 . Our results, in contrast, elucidate a more complex algorithm computed by a two-layer transformer on a more realistic version without padding, which requires the model to determine which place it is predicting. We also contribute by providing a detailed interpretation, including how digits are represented in activations.

\section*{5 Conclusion}

We present InversionView, an effective method for decoding information from neural activations. By applying it in three case studies-character counting, IOI, and 3-digit addition-we showcase how it can reveal various types of information, thus facilitating reverse-engineering of algorithm implemented by neural networks. Moreover, we compare it with other interpretability methods and show its unique advantages. We also show that the results given by InversionView can in principle be interpreted automatically by LLMs, which opens up possibilities for a more automated workflow. This paper only explores a fraction of the opportunities this method offers. Future work could apply it to subspaces of residual stream (which can probably produce more insights), to larger models, or to different modalities such as vision.

\section*{6 Acknowledgements}

Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 232722074 - SFB 1102

\section*{References}

[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.

[3] Anthropic. Introducing the next generation of claude, 2024. https://www.anthropic.com/ news/claude-3-family

[4] Y. Belinkov. Probing classifiers: Promises, shortcomings, and advances. Comput. Linguistics, 48(1):207-219, 2022. doi: 10.1162/COLI\A\_00422. URL https://doi.org/10.1162/ coli_a_00422.

[5] Y. Belinkov and J. Glass. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72, 2019.

[6] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, and J. Glass. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861-872, 2017.

[7] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman, and J. Steinhardt. Eliciting latent predictions from transformers with the tuned lens, 2023.

[8] S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.

[9] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, page 2, 2023.

[10] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of bert's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276-286, 2019.

[11] A. Conmy, A. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems, 36:16318-16352, 2023.

[12] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023.

[13] G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.

[14] N. Elhage, N. Nanda, C. O. T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits, 2021. https: //transformer-circuits.pub/2021/framework/index.html.

[15] J. Ferrando and E. Voita. Information flow routes: Automatically interpreting language models at scale, 2024.

[16] A. Geiger, H. Lu, T. Icard, and C. Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 9574-9586, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 4f5c422f4d49a5a807eda27434231040-Abstract.html.

[17] A. Geiger, C. Potts, and T. Icard. Causal abstraction for faithful model interpretation. arXiv preprint arXiv:2301.04709, 2023.

[18] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216-12235, 2023.

[19] A. Ghandeharioun, A. Caciularu, A. Pearce, L. Dixon, and M. Geva. Patchscope: A unifying framework for inspecting hidden representations of language models. arXiv preprint arXiv:2401.06102, 2024.

[20] N. Goldowsky-Dill, C. MacLeod, L. Sato, and A. Arora. Localizing model behavior with path patching. arXiv preprint arXiv:2304.05969, 2023.

[21] M. Hanna, O. Liu, and A. Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Advances in Neural Information Processing Systems, 36, 2024.

[22] S. Katz and Y. Belinkov. Visit: Visualizing and interpreting the semantic information flow of transformers. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14094-14113, 2023.

[23] G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Attention is not only a weight: Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7057-7075, 2020.

[24] B. Z. Li, M. Nye, and J. Andreas. Implicit representations of meaning in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1813-1827, 2021.

[25] K. Li, A. K. Hopkins, D. Bau, F. Viégas, H. Pfister, and M. Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations, 2022.

[26] T. Lieberum, M. Rahtz, J. Kramár, G. Irving, R. Shah, and V. Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv preprint arXiv:2307.09458, 2023.

[27] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.

[28] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.

[29] N. Nanda and J. Bloom. Transformerlens. https://github.com/TransformerLensOrg/ TransformerLens, 2022.

[30] nostalgebraist. interpreting gpt: the logit lens. LESSWRONG, 2020. https://www. lesswrong com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.

[31] K. Pal, J. Sun, A. Yuan, B. C. Wallace, and D. Bau. Future lens: Anticipating subsequent tokens from a single hidden state. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 548-560, 2023.

[32] P. Quirke and F. Barez. Understanding addition in transformers. In The Twelfth International Conference on Learning Representations, 2023.

[33] P. Quirke and F. Barez. Understanding addition in transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? $i d=r I x 1 Y X V W Z b$

[34] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[35] A. Stolfo, Y. Belinkov, and M. Sachan. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7035-7052, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.emnlp-main.435. URL https://aclanthology.org/2023.emnlp-main 435

[36] A. Tamkin, M. Taufeeque, and N. D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks. arXiv preprint arXiv:2310.17230, 2023.

[37] I. Tenney, D. Das, and E. Pavlick. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593-4601, 2019 .

[38] J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388-12401, 2020.

[39] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. 2023. URL https: //openreview.net/forum?id=NpsVSN6o4ul.

[40] Z. Wang, A. Ku, J. M. Baldridge, T. L. Griffiths, and B. Kim. Gaussian process probes (gpp) for uncertainty-aware probing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[41] Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. Interpretability at scale: Identifying causal mechanisms in alpaca. Advances in Neural Information Processing Systems, 36, 2024.

[42] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.

\section*{Contents}
1 Introduction ..... 1
2 Methodology ..... 2
2.1 Interpretation Framework ..... 2
2.2 Conditional Decoder Model ..... 2
3 Discovering the Underlying Algorithm by InversionView ..... 3
3.1 Setup and Background ..... 3
3.2 Character Counting ..... 3
3.3 IOI circuit in GPT-2 small ..... 5
3.4 3-Digit Addition. ..... 6
4 Discussion ..... 8
4.1 Related Work ..... 9
5 Conclusion ..... 9
6 Acknowledgements ..... 9
A Practical Guidelines ..... 14
A. 1 Observing Larger Neighborhoods is Important ..... 14
A. 2 Sampling with Decoder Model ..... 14
A. 3 Selecting Position in Samples. ..... 15
A. 4 Threshold-Dependence of Claims about Activations ..... 16
B Experimental Verification of Completeness ..... 18
C Decoder Model Architecture ..... 18
D Character Counting: More Details and Examples ..... 20
D. 1 Implementation Details ..... 20
D. 2 More Examples of InversionView ..... 21
D. 3 Causal Intervention Details ..... 21
D. 4 Extended Algorithm with Positional Cues ..... 22
E IOI Task: Details and Qualitative Results ..... 24
E. 1 Implementation Details ..... 24
E. 2 More Examples of InversionView ..... 24
E. 3 Qualitative Examination Results ..... 24
E. 4 Choice of Distance Metric in IOI ..... 26
F 3-Digit Addition: More Details and Examples ..... 26
F. 1 Implementation Details ..... 26
F. 2 More Examples of InversionView ..... 29
F. 3 Model Deficiency ..... 29
F. 4 Qualitative Examination Results ..... 30
F. 5 Causal Intervention: Details and Full Results ..... 33
G Decoder Likelihood Difference ..... 34
H Notes on Attention and Path Patching ..... 37
H. 1 InversionView Reveals Information from Un-attended Tokens. ..... 37
H. 2 Additional Discussion about Path Patching ..... 37
I Automated Interpretability ..... 38
J Compute Resources ..... 44

\section*{A Practical Guidelines}

\section*{A. 1 Observing Larger Neighborhoods is Important}

Here, we illustrate the importance of inspecting $\epsilon$-preimages up to the threshold $\epsilon$, rather than just top- $k$ nearest neighbors of the query activation. In Figure 7, an initial glance at the samples on the left may suggest that the residual stream of "+" encodes F1 and F2. However, observing a broader neighborhood (as depicted on the right) reveals that this conclusion is not even robust to tiny perturbations of the activation. Indeed, after a more comprehensive calculation over all possible $x_{+}^{0, \text { post }}$, we find that the maximum possible metric value between any pair of $x_{+}^{0, \text { post }}$ is 0.0184 . So for any $\epsilon \geq 0.0184$ the $\epsilon$-preimage covers the entire input space. Hence, the activation is unlikely to contain usable information.

We further prove this by causal intervention. We found that $x_{+}^{0, \text { post }}$ has no effect on the model's output. Concretely we patch $x_{+}^{0, \text { post }}$ with its mean on the test set (mean ablation [39]) and for each prediction target (A1, A2 etc.), we compare 1) the KL divergence between the distribution before and after patching. 2) logit decrement ratio, which is the difference between the maximum logit value before patching and the logit value of the same target token after patching, divided by the former. E.g., 1.0 means the logit is reduced to zero (assuming it is originally positive). The results are shown in Table 1 . We can see the effect of $x_{+}^{0, \text { post }}$ is negligible.

\begin{tabular}{c||c|c|c|c}
\hline & $\mathrm{A} 1$ & $\mathrm{~A} 2$ & $\mathrm{~A} 3$ & $\mathrm{~A} 4 / \mathrm{E}$ \\
\hline KL divergence & $8.1 \times 10^{-8}$ & $5.4 \times 10^{-7}$ & $2.5 \times 10^{-8}$ & $3.6 \times 10^{-9}$ \\
Logit decrement ratio & $-2.3 \times 10^{-5}$ & $8.7 \times 10^{-7}$ & $1.4 \times 10^{-5}$ & $2.8 \times 10^{-7}$ \\
\hline
\end{tabular}

Table 1: Activation patching results for $x_{+}^{0, \text { post }}$.

\section*{A. 2 Sampling with Decoder Model}

In Section 2.2. we mentioned that the distribution $p\left(\mathbf{x} \mid \mathbf{z}^{q}\right)$ is modeled by the decoder. Strictly speaking, $p\left(\mathbf{x} \mid \mathbf{z}^{q}\right)$ represents the data distribution in the $\epsilon$-preimage defined by $\epsilon=0$. For example, when the probed model is using causal masking, and a certain activation is relevant to all previous context (by non-zero attention weights), then $p\left(\mathbf{x} \mid \mathbf{z}^{q}\right)$ is the distribution over those inputs that share the same previous context (i.e., they have same prefix). This requires that the decoder can distinguish any tiny difference in activation and decode the full information (imagine a token attended with 0.0001 attention weight). Such a decoder must be very powerful and perhaps trained without any regularization. But in practice, the decoder is a continuous function of activation and tiny changes

\begin{tabular}{|c|c|}
\hline \multicolumn{2}{|l|}{ Activation Site: $x^{0, p o s t}$} \\
\hline \multicolumn{2}{|l|}{ Query Input } \\
\hline \multicolumn{2}{|c|}{0.000 ; B $982(+) 347=1329$} \\
\hline Generated Samples & \\
\hline $0.000 ;$ В $986(+) \cdots$ & 0.005 ; В $013(+)$ \\
\hline $0.000 ;$ В $986(+) \ldots$ & 0.005 ; B 180 (+) \\
\hline $0.000 ;$ В $985(+) \ldots$ & 0.005 ; В 888 \\
\hline $0.000 ;$ В $985(+) \cdots$ & 0.006 ; B $109(+)$ \\
\hline $0.000 ;$ В $989(+) \cdots$ & $0.006 ;$ B 202 (+) \\
\hline $0.000 ;$ В $986(+) \cdots$ & 0.006 ; B 488 (+) \\
\hline $0.000 ;$ В $987(+) \ldots$ & 0.007 ; B 402 (+) \\
\hline $0.000 ;$ В $982(+) \ldots$ & 0.007 ; B $140(+)$ \\
\hline $0.003 ;$ B $890(+) \cdots$ & 0.007 ; B $125(+)$ \\
\hline $0.003 ;$ B $891(+) \cdots$ & $0.008 ;$ В 791 \\
\hline
\end{tabular}

Figure 7: Addition Task: Inspecting $\epsilon$-preimage avoids pitfall of inspecting simple top-k similar activations. Generation based on query activation $x_{+}^{0, \text { post }}$ of a random example. Contents after " + " is omitted since they do not affect the activation due to causal masking.

in activation are not perceivable by the decoder. We observe that the decoder rarely generates the sample that lies at the same point (producing the same activation) as the query input in vector space, instead it usually generates samples that are in the neighborhood of the query input. Because we need to observe the whole neighborhood of the query input and prevent samples from being too concentrated, we adjust the sampling temperature to control how concentrated they are. Importantly, even if the decoder is too powerful and can always recover the same activation, we can still obtain the neighborhood by adding random noise to the query activation before giving it to the decoder. This motivates decoding with temperature and noise, as described in the next paragraph.

Increasing Coverage by Temperature and Noise. In our experiments, we use both ways to control the generation, i.e., by adjusting the temperature and adding random noise to the query activation. We denote temperature as $\tau$ and noise coefficient as $\eta$. The noise vector consists of independent random variables sampled from the standard normal distribution and then multiplied by $\operatorname{std}\left(\mathbf{z}^{q}\right) \cdot \eta$ where $\operatorname{std}(\cdot)$ stands for standard deviation. In our web application, we provide multiple sampling configurations: four configurations in which $\tau=\{0.5,1.0,2.0,4.0\}$ and $\eta=0.0$; one figuration named "Auto" which is sampled by following procedure: we iterate over a few predefined $\tau$ (ranging from 0.5 to 2.0 ) and $\eta$ ( 0.0 or 0.1 ) and sample a certain amount of inputs (e.g., 250) for each parameter combination. We then calculate the metric value for all inputs collected from different sampling configurations. We then randomly choose a small part of them (100) with different probability for in- $\epsilon$-preimage inputs and out-of- $\epsilon$-preimage inputs. We dynamically adjust the probability such that the in- $\epsilon$-preimage inputs account for $60 \%-80 \%$ of the chosen set of inputs (when this is possible).

When inspecting the samples, we choose a configuration for which the distances $D(\cdot, \cdot)$ to the query activation best cover the interval $[0, \epsilon]$. The choice is usually specific to the activation site that we are inspecting and can be performed manually in the web application.

\section*{A. 3 Selecting Position in Samples}

As the decoder outputs an input but not the position of the activation, we then assign the position minimizing $D(\cdot, \cdot)$ to the query activation. Usually, there is only one position with a small $D(\cdot, \cdot)$, matching the structural position (not necessarily the absolute position) of the position the query activation was taken from (e.g., the target character in Figure 2a). In certain cases, we visualize $D(\cdot, \cdot)$ for an activation from a position not minimizing $D(\cdot, \cdot)$ for expository purposes. For example, in Figure $2 \mathrm{~b}$, because the target character exclusively attends to itself in layer 1 , resulting $a_{t c}^{1,0} \approx a_{:}^{1,0}$,

```

<|endoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin
Query Input
0.000;<|endoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin
Generated Samples
0.024;<|endoftext|>The station Sara and Justin went to had a kiss. Sara gave it( to) Justin[EOS]
0.024;<|endoftext|>When Paul and Justin got a kiss at the school, Paul decided to give it( to) Justin[EOS]
0.025;<<endoftext|>Then, Alicia and Justin had a long argument. Afterwards Alicia said( to) Justin[EOS]
0.030;<<endoftext|>Then, Justin and Erin went to the garden. Erin( gave) a basketball to Justin[EOS]
0.037;<|endoftext|>After the lunch in the afternoon, Justin and Kristen went to the station. Kristen gave a kiss( to) Justin[EOS]
|.039; <<endoftext|>After taking a long break Kimberly and Justin went to the house, Kimberly gave a bone( to) Justin[EOS]
|.042;<<endoftext|>While spending time together Justin and Alicia were working at the garden, Alicia( gave) a kiss to Justin[EOS]
1 0.048;<<endoftext|>Then, Justin and Kristen went to the school. Kristen( gave) a bone to Justin[EOS]
- 0.198;<<endoftext|>Friends separated at birth Kristen and Justin found a snack at the garden(.) Justin gave it to Kristen[EOS]

- 0.579;<<endoftext|>While spending time together Michelle and Joshua were commuting to the restaurant(,) Alexander gave a ring to Michelle[EOS]

```

Figure 8: IOI: InversionView applied to Name Mover Head 9.9 at "to"; Unlike Figure 3b, here the position minimizing $D(\cdot, \cdot)$ is in parentheses. The head also copies the name "Justin" in other circumstances, e.g., at "gave". The name "Justin" is always contained

so sometimes the metric value of $a_{t c}^{1,0}$ is smaller than $a^{1,0}$. Throughout the appendix and our web application, we use italic font and rounded bars to visualize $D(\cdot, \cdot)$ in such cases.

We also find that selecting the position minimizing $D(\cdot, \cdot)$ can reveal that components are active in similar ways at other positions than the one originally investigated. For example, in Figure 8, we can see sometimes "gave" is selected. This is reasonable, because the IO is also likely to appear right after "gave" and the head needs to move the IO name for this prediction. We can see that activation at the period "." can also be somewhat similar to the query activation, this is not surprising. Because the model needs to predict the subject for the next sentence and copying a name from the previous context is helpful. In summary, the copying mechanism can be triggered in circumstances different from IOI, selecting position minimizing $D(\cdot, \cdot)$ reveals more information about this.

\section*{A. 4 Threshold-Dependence of Claims about Activations}

One question people may have is whether our conclusion about the information in activation depends significantly on the threshold we choose. To address this potential concern, we show more details about the geometry of the vector space in Figure 9 On the one hand, we can see that with different thresholds $\epsilon$ we can make different conclusions about the query activation. On the other hand, the conclusions made with different thresholds are "in alignment". In other words, the conclusions do not differ fundamentally, instead, the difference between them is about granularity or the amount of details being ignored.

Specifically, in Figure $9 \mathrm{a}, \epsilon_{1}$ results in the conclusion that the count is 5 , the target character is either 't' or 'm', and also approximate sequence length is retained. $\epsilon_{2}$ results in a conclusion only about the count and the sequence length. In Figure 9 b if we set the threshold to $\epsilon_{1}$ (i.e., a value between 0.000 and 0.009 ), the obtained information will be $\mathrm{F} 1=5, \mathrm{~S} 1=7$. If we set the threshold to $\epsilon_{2}$, the information will be 5 and 7 are in the hundreds place. If we set the threshold to $\epsilon_{3}$ the information will be " 5 is in the hundreds place". In Figure $9 \mathrm{c}, \epsilon_{1}$ results in conclusion that 9,8 are in hundreds place and 2, 7 are in tens place; $\epsilon_{2}$ results in conclusion that 9,8 are in hundreds place and $\mathrm{F} 2+\mathrm{S} 2=9$; $\epsilon_{3}$ results in conclusion that $\mathrm{F}+\mathrm{S} 1 \approx 17$ and $\mathrm{F} 2+\mathrm{S} 2 \approx 9$. Therefore, changing the threshold value will not lead us in a different direction, because the $\epsilon$-preimage is based on the same underlying geometry.

In practice, rather than selecting a threshold first and treating inputs in a black-and-white manner, we first observe the geometry of the vector space and obtain a broad understanding of the encoded information, then choose a reasonable threshold that best summarizes our findings. In other words, the threshold value is used to simplify our findings so that we can focus more on the big picture of the model's overall algorithm, and it should also be set according to the difference that is likely to be readable for the model. As the interpretation progresses, one can see if the chosen threshold leads to a plausible algorithm and can adjust it if necessary. Finally, verification experiments are conducted to verify the hypothesis.

\section*{B m u a u m m m $\mid \mathbf{m}$ (:) 5}

Query Input

0.000 ; B m u a u m m m m $/ \mathrm{m}$ (:) 5

Generated Samples

0.012 ; \#: $5 ; B \mathrm{~m} \mathrm{t} \mathrm{q} \mathrm{m} \mathrm{m} \mathrm{m} \mathrm{m} / \mathrm{m}$ (:) $5 \mathrm{E}$ 0.016;\#:5; B ○ o p m m m m | m (:) $5 \mathrm{E}$ 0.015 ;\#: 5 ; Bpttptptptpppsp|t (:) $5 \mathrm{E}$ 0.016 ;\#:5; Bptptppaptptptp|t (:) $5 \mathrm{E}$ 0.017 ; \#: 5 ; B m a m a m m o m | m (:) $5 \mathrm{E}$ 0.020 ; \#: 5 ; B q t t q j q q t q q q|t (:) $5 \mathrm{E}$ 0.021 ;\#:5; Bhcottttooootoo|t (:) $5 \mathrm{E}$ 0.021 ;\#: $5 ; \mathrm{B} \mathrm{m} \mathrm{m} \mathrm{m} \mathrm{m} \mathrm{o} \mathrm{a} \mathrm{a} \mid \mathrm{m}$ (:) $5 \mathrm{E}$ 0.023;\#: 5 ; B m t q m m q tm q t q | (:) $5 \mathrm{E}$ 0.024 ;\#:5; B m m m m a h h | m (:) $5 \mathrm{E}$ 0.024 ;\#: 5 ; Bytvytyttty y y $\mid \mathrm{t}$ (:) $5 \mathrm{E}$ 0.025 ; \#: 5; Btttthzhhthzzzh|t (:) $5 \mathrm{E}$ 0.024;\#:5; Bohththotoooot t|t (:) $5 \mathrm{E}$ 0.027;\#: 5 B d t q q t q t t q q q t q | t (:) $5 \mathrm{E}$ 0.033;\#:5; B tt ottpoooooo|t (:) $5 \mathrm{E}$ 0.035 ; \#: 5 ; B m o y o momomom m (:) $5 \mathrm{E}$ $\boldsymbol{\varepsilon}_{1} 037$;\#: 5 ; Bhttzhzhhzhzzz|h (:) $5 \mathrm{E}$ 0.037;\#: 5 ; $\mathrm{m} \mathrm{m} \mathrm{m} \mathrm{m} \mathrm{m} \mathrm{o} \mathrm{○} \mathrm{o} \mathrm{o} \mathrm{|} \mathrm{m}$ (:) $5 \mathrm{E}$ 0.039 ;\#: 5 ; B m d x m m d d m m | m (:) $5 \mathrm{E}$ 0.042 ; \#: 5 ; Btftotffttfffo O|t (:) $5 \mathrm{E}$ 0.041 ;\#: 5 ; B z g z g zg z zg z|g (:) $5 \mathrm{E}$ 0.041 ;\#: 5 ; B u m m m p m $\mid \mathrm{m}$ (:) $5 \mathrm{E}$ 0.043 ;\#:5; B h h n m h h d h | h (:) $5 \mathrm{E}$ 0.044 ; \#: $5 ; B \mathrm{~mm} \mathrm{~m} \mathrm{~m} \mathrm{mot|} \mathrm{m} \mathrm{(:)} 5 \mathrm{E}$ 0.045 ;\#: 5 ; B m m c m c c m c c c m (:) $5 \mathrm{E}$ 0.049;\#:5; Bohhomoooh h h | h (:) $5 \mathrm{E}$ 0.052 ;\#: 5 ; B c c a d c d c c d d d c (:) $5 \mathrm{E}$ 0.062 ;\#:5; B rzzzzz c|z (:) $5 \mathrm{E}$ 0.068 ;\#:5; B m m m m m a a a m a | a (:) 5 E 0.075 ; \#:5; B m m m m m a a a a a $\mid$ a (:) $5 \mathrm{E}$

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-17.jpg?height=49&width=374&top_left_y=1637&top_left_x=431)
0.129 ;\#:6; B $\circ \mathrm{m} \circ \mathrm{m} \mathrm{m} \mathrm{m} \mathrm{o} \mathrm{m} \mathrm{o} \mathrm{m} \mid \mathrm{m}$ (:) $5 \mathrm{E}$ 0.136;\#:6; B m m m m m th|m (:) $5 \mathrm{E}$ $0.142 ; \#: 6 ; B \operatorname{ttttt}$ q q q q q q q g|t (:) $5 \mathrm{E}$ 0.158;\#:6; Bttrttqtrrqtqq q q|t (:) $6 \mathrm{E}$ 0.148 ;\#:6; B m m m m m g m g $\mid \mathrm{m}$ (:) $5 \mathrm{E}$ 0.180 ; \#:3; B i ig e i b b begggg|i (:) $4 \mathrm{E}$ 0.307;\#:3; B a rg a a a g g a r ra|r (:) $5 \mathrm{E}$ 0.316;\#:3; B a km mum a | m (:) $5 \mathrm{E}$ 0.618;\#:0; Bmznnntttnznbnznztttnz|p (:) $4 E$ 0.341 ;\#:3; B s vg nvssnnggsn|g(:) $4 \mathrm{E}$ 0.431;\#: 1; B pg pg p p p b i p q|i (:) $1 \mathrm{E}$ $0.375 ; \#: 2 ; B$ tyty|t (:) $4 \mathrm{E}$

0.441 ;\#:0; B x b b p xh b bh h bh h xh h b |w (:) $2 \mathrm{E}$

\section*{B $50+737$ (=) 1287}

Query Input

0.000 ; B $550+737 \Leftrightarrow 1287$ Generated Samples

0.000 ; В $539+709 \Leftrightarrow 1248 \mathrm{E}$ $0.000 ;$ B $560+794 \Leftrightarrow 1354 \mathrm{E}$ $0.000 ;$ B $548+788 \Leftrightarrow 1336 \mathrm{E}$ 0.000 ; В $521+702 \Leftrightarrow 1223 \mathrm{E}$ $0.000 ;$ B $519+784 \Leftrightarrow 1303 \mathrm{E}$ 0.000 ; B $556+729 \Leftrightarrow 1285 \mathrm{E}$ 0.000 ; B $541+719 \Leftrightarrow 1260 \mathrm{E}$ 0.000 ; B $546+780 \Leftrightarrow 1326 \mathrm{E}$ $0.000 ;$ B $517+792 \Leftrightarrow 1309 \mathrm{E}$ $0.000 ;$ B $523+772 \Leftrightarrow 1295 \mathrm{E}$ 0.000 ; В $550+741 \Leftrightarrow 1291 \mathrm{E}$ 0.000 ; B $530+786 \Leftrightarrow 1316 \mathrm{E}$ $0.000 ;$ B $565+799 \Leftrightarrow 1364 \mathrm{E}$ 0.000 ; B $554+798 \Leftrightarrow 1352 \mathrm{E}$ ع.000; B $507+792 \Leftrightarrow 1299 \mathrm{E}$ $\boldsymbol{\varepsilon}_{01009 ; \text { B } 710+571 \Leftrightarrow 1281 \mathrm{E}}$ 0.009 ; В $774+507$ (=) $1281 \mathrm{E}$ $0.009 ; B 757+510 \Leftrightarrow 1267 \mathrm{E}$ $0.009 ; B 729+552 \Leftrightarrow 1281 \mathrm{E}$ 0.009 ; B $703+577 \Leftrightarrow 1280 \mathrm{E}$ $0.009 ;$ В $794+598 \Leftrightarrow 1392 \mathrm{E}$ 0.009 ; B $750+501 \Leftrightarrow 1251 \mathrm{E}$ $0.009 ; B 712+526 \Leftrightarrow 1238 \mathrm{E}$ $0.009 ;$ В $710+520 \Leftrightarrow 1230 \mathrm{E}$ $0.009 ;$ В $754+530 \Leftrightarrow 1284 E$ 0.009 В $760+522 \Leftrightarrow 1282 \mathrm{E}$ $\boldsymbol{\varepsilon}_{0.073 ; B 570+691 \Leftrightarrow 1261 \mathrm{E}}$ $0.073 ;$ B $660+512 \Leftrightarrow 1172 \mathrm{E}$ $0.073 ;$ B $603+529 \Leftrightarrow 1132 \mathrm{E}$ $0.073 ; B 558+699 \Leftrightarrow 1257 \mathrm{E}$ $0.102 ;$ B $805+507 \Leftrightarrow 1312 \mathrm{E}$ 0.102 ; B $860+542$ (=) $1402 \mathrm{E}$ $0.106 ;$ B $516+865 \Leftrightarrow 1381 \mathrm{E}$ $0.106 ; B 507+880 \Leftrightarrow 1387 \mathrm{E}$ $0.106 ; B 509+887 \Leftrightarrow 1396 \mathrm{E}$ 0.106 ; B $562+898 \Leftrightarrow 1460 \mathrm{E}$ $0.106 ;$ B $513+897 \Leftrightarrow 1410 \mathrm{E}$ $0.126 ;$ B $179+501 \Leftrightarrow 680 \mathrm{E}$ $0.129 ; B 957+511 \Leftrightarrow 1468 \mathrm{E}$ $0.130 ;$ B $551+199 \Leftrightarrow 750 \mathrm{E}$ ع.146; В $586+584=$ (1) $170 \mathrm{E}$ $\boldsymbol{\varepsilon}_{\mathbf{3}} 383$; в 6 (5) 5+817=1472E 0.436 ; B $619+457=$ (1) $076 \mathrm{E}$ 0.477 ; В $691+408=$ (1) $099 \mathrm{E}$ 0.509 ; B $676+185 \Leftrightarrow 861 \mathrm{E}$

(a)

(b)

\section*{B $920+878=$ (1) 798}

Query Input

0.000 ; B $920+878=$ (1) 798

Generated Samples

0.003 ; В $928+870=$ (1) $798 \mathrm{E}$ $0.004 ;$ B $920+873=$ (1) $793 \mathrm{E}$ $0.005 ;$ B $820+974=$ (1) $794 \mathrm{E}$ 0.012 ; B $921+874=$ (1) $795 \mathrm{E}$ 0.015 ; B $874+927=$ (1) $801 \mathrm{E}$ 0.015 ; B $874+927=$ (1) $801 \mathrm{E}$ $0.015 ; B 923+879=$ (1) $802 \mathrm{E}$ 0.016 ; B $924+876=$ (1) $800 \mathrm{E}$ $\mathcal{E}_{\text {G. } 030 ; B 96} 96+831=$ (1) $797 \mathrm{E}$ $0.040 ; B 962+837=$ (1) $799 \mathrm{E}$ 0.041 ; B $964+832=$ (1) $796 \mathrm{E}$ 0.047 ; B $964+837=$ (1) $801 \mathrm{E}$ $0.051 ;$ B $935+864=$ (1) $799 \mathrm{E}$ $0.058 ; B 942+851=$ (1) $793 \mathrm{E}$ 0.059 ; B $839+964=$ (1) $803 \mathrm{E}$ 0.060 ; B $982+818=$ (1) $800 \mathrm{E}$ 0.062 ; B $911+884=$ (1) $795 \mathrm{E}$ $0.063 ;$ B $911+885=$ (1) $796 \mathrm{E}$ 0.063 ; B $887+918=$ (1) $805 \mathrm{E}$ $0.064 ; B 956+841=$ (1) $797 \mathrm{E}$ 0.065 B $946+851=$ (1) $797 \mathrm{E}$ $0.066 ; B 816+980=$ (1) $796 \mathrm{E}$ 0.067 ; B $914+883=$ (1) $797 \mathrm{E}$ 0.067 ; B $915+887=$ (1) $802 \mathrm{E}$ 0.069 ; B $914+884=$ (1) $798 \mathrm{E}$ 0.082 ; B $957+842=$ (1) $799 \mathrm{E}$ 0.099 ; B $844+958=$ (1) $802 \mathrm{E}$ $0.120 ;$ В $805+996=$ (1) $801 \mathrm{E}$ 0.124 ; B $995+809=$ (1) $804 \mathrm{E}$ $\boldsymbol{\varepsilon}_{2}^{0.124}$; В $995+809=$ (1) $804 \mathrm{E}$ $\boldsymbol{\varepsilon}_{\mathbf{2}} .211$; в $901+794=$ (1) $695 \mathrm{E}$ $0.226 ;$ B $858+849=$ (1) $707 \mathrm{E}$ $0.242 ;$ В $741+950=$ (1) $691 \mathrm{E}$ 0.249 ; B $927+777=$ (1) $704 \mathrm{E}$ 0.251 ; B $854+857=$ (1) $711 \mathrm{E}$ $0.255 ;$ B $866+842=$ (1) $708 \mathrm{E}$ $0.260 ;$ B $818+884=$ (1) $702 \mathrm{E}$ 0.287 ; B $858+842=$ (1) $700 \mathrm{E}$ $0.289 ;$ В $860+834=$ (1) $694 \mathrm{E}$ 0.289 ; В $860+837=$ (1) $697 \mathrm{E}$ $0.294 ; B 847+850=$ (1) $697 \mathrm{E}$ 0.296 ; B $844+851=$ (1) $695 \mathrm{E}$ $0.328 ;$ B $852+754=$ (1) $606 \mathrm{E}$ 0.406 ; В $834+977=$ (1) $811 \mathrm{E}$ $\boldsymbol{\varepsilon}_{3}^{0.417}$; В $926+889=$ (1) $815 \mathrm{E}$

(c)

Figure 9: (a) Activation site $a^{1,0}$. (b) Activation site $a^{0,2}$. (c) Activation site $a^{1,3}$. In all three cases, we use normalized Euclidean distance as the distance metric. We use $\epsilon_{1}, \epsilon_{2}, \cdots$ to mark varying threshold values by which different interpretations will be made.

\section*{B Experimental Verification of Completeness}

In Section 2.2, we described that an ideal strategy for obtaining samples in the $\epsilon$-preimage satisfies two desiderata: it only provides samples that are indeed within the $\epsilon$-preimage (Correctness), and it provides all such samples at reasonable probability (Completeness). As further described there, we can directly ensure Correctness by evaluating $D(\cdot)$ for every sample. Ensuring completeness is more challenging, due to the exponential size of the input space; the most general approach is to design counterexamples not satisfying a hypothesis about the content of the $\epsilon$-preimage, and verifying that $D(\cdot)$ is indeed large.

Here, we provide a direct test of completeness in one domain (3-digit addition). The primary concern with completeness is that, if some groups of inputs in $B_{\mathbf{z}^{q}, f, \epsilon}$ are systematically missing from the generated samples, one may overestimate the information contained in activations. To see if may happen in reality, we plotted log-probability against distance, each of which includes all inputs in 3 digit addition task, as shown in Figure 10 We next evaluated the sampling probability for different sampling configurations, as described in Appendix A.2 When adding noise, we calculate probability of an input using a Monte Carlo estimate: Concretely, because the probability of inputs is conditioned on the noise vector added to the query activation, we randomly sample 500 noise vectors from the normal distribution (with the standard deviation described in Appendix A.2) and calculate input probability given these noise vectors, then average to obtain the estimated probability, and then compute the logarithm.

Across setups, we can see that there is a triangular blank area in the bottom left corner, i.e., the bottom left frontier stretches from the upper left towards the lower right. In all sub-figures, not a single input close to the query input is assigned disproportionately low probability. All inputs in the $\epsilon$-preimage (the dots on the left of the red vertical line) are reasonably likely to be sampled from the decoder, with probability decreasing as the input becomes more distant, alleviating concerns about completeness for these query activations. On the other hand, some inputs distant from the query input are also likely to appear in samples, but this is not a problem for our approach, as we can easily tell that they are not in the $\epsilon$-preimage by calculating the distance (correctness is ensured).

We can see that sometimes the distribution of the decoder itself (at temperature 1 and no noise) is quite sharp, and in- $\epsilon$-preimage inputs can have low probability as they are near the boundary of preimage. By comparing the sub-figures, we can see both increasing the temperature and adding noise substantially smooth the distribution within the $\epsilon$-preimage, lowering the difference of the probability of inputs that are at similar distance.

\section*{C Decoder Model Architecture}

The decoder is a decoder-only transformer. In order to condition the decoder on the query activation, the query activation is first passed through a stack of MLP layers to decode information depending on the activation sites and then made available to each attention layer of the decoder transformer.

Preprocessing Query Activation. The query activation $\mathbf{z}^{q} \in \mathbb{R}^{d}$ is first concatenated with a trainable activation site embedding $\mathbf{e}_{a c t} \in \mathbb{R}^{d_{s i t e}}$, producing the intermediate representation $\mathbf{z}^{(0)}=$ $\left[\mathbf{z}^{q} ; \mathbf{e}_{a c t}\right]$. We chose $d_{\text {site }}$ to be the number of possible activation sites in the training set. The result $\mathbf{z}^{(0)}$ is then fed through 6 MLP layers with residual connections:

$$
\begin{equation*}
\mathbf{z}^{(i+1)}=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}^{(i)}\right)\right)+\mathbf{z}^{(i)} \tag{2}
\end{equation*}
$$

where LN represents layer normalization. For each MLP layer, the input, hidden and output dimensions are $d+d_{\text {site }}, d$, and $d+d_{\text {site }}$, respectively. The activation function is ReLU. There is also a final layer normalization, $\mathbf{z}^{(f n)}=\mathrm{LN}\left(\mathbf{z}^{(6)}\right)$.

Providing Query Activation to Decoder. As we want to make the query activation available to each attention layer of the decoder, we separately customize it to the needs of each layer using a linear layer. That is, for each layer of the decoder transformer (indexed by $\ell$ ), we define a linear layer $\mathrm{L}^{(\ell)}: \mathbb{R}^{d+d_{\text {site }}} \rightarrow \mathbb{R}^{d_{\text {decoder }}}$ and a layer normalization $\mathrm{LN}^{(\ell)}$, where $d_{\text {decoder }}$ is the model dimension of the decoder model:

$$
\begin{equation*}
\mathbf{z}^{(l)}=\mathrm{LN}^{(\ell)}\left(\mathrm{L}^{(\ell)}\left(\mathbf{z}^{(f n)}\right)\right) \tag{3}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-19.jpg?height=432&width=1332&top_left_y=304&top_left_x=404)

(a)
![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-19.jpg?height=418&width=1330&top_left_y=828&top_left_x=405)

(b)
![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-19.jpg?height=412&width=1324&top_left_y=1346&top_left_x=411)

(c)

Figure 10: Addition Task: Exhaustive verification of the decoder's completeness for 8 random query activation. Failure of completeness would mean that some inputs result in an activation very close to the query activation but nonetheless are assigned very small probability. Here, we show that this does not happen, by verifying that all inputs within the $\epsilon$-preimage are assigned higher probability by the decoder than most other inputs. We also show that by increasing the temperature and adding random noise, we can increase the probability of inputs near the boundary of $\epsilon$-preimage. Each sub-figure - (a), (b), (c) - contains 8 scatter plots, each of which contains 810000 dots representing all input sequences in the 3-digit addition task. The y-axis of scatter plots is the log-probability of the input sequence given by the decoder (which reads the query activation), the $x$-axis is the distance between the query input and the input sequence. As before, distance is measured by the normalized Euclidean distance between the query activation (the activation site, query input, and selected position are shown in the scatter plot title) and the most similar activation along the sequence axis. In addition, the red vertical line represents the threshold $\epsilon$, which is 0.1 in the case study. (a) Temperature $\tau=1.0$, no noise is added. (b) Temperature $\tau=2.0$, no noise is added. (c) Temperature $\tau=1.0$, noise coefficient $\eta=0.1$ (See Appendix A.2 for explanation of $\eta$ ).

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-20.jpg?height=515&width=697&top_left_y=244&top_left_x=714)

Figure 11: Training loss of the Character Counting task. Each data point is the averaged loss over an epoch.

In the $\ell$-th layer $(\ell=1,2, \ldots)$ of the decoder, $\mathbf{z}^{(\ell)}$ is concatenated with the input of the attention layer along the length axis before computing keys and values, so that each attention head can also attend to $\mathbf{z}^{(\ell)}$ in addition to the context tokens. This means that each head in the $\ell$-th layer of the decoder, instead of attending to $x_{1}^{(\ell)}, x_{2}^{(\ell)}, \ldots$, now computes its attention distribution over $\mathbf{z}^{(l)}, x_{1}^{(\ell)}, x_{2}^{(\ell)}, \ldots$ Here $x_{t}^{(\ell)}$ is the residual stream corresponding to the $t$-th token input into the $\ell$-th layer. We note that there are other possible choices for conditioning the generation on the activation, and we didn't optimize this choice thoroughly.

Decoder Hyperparameters. The decoder transformer has 2 layers, 4 heads per layer, and a model dimension of 256. The attention dropout rate is 0 . Other settings are the same as the GPT-2. We use the same architecture for all 3 tasks in the paper.

\section*{D Character Counting: More Details and Examples}

\section*{D. 1 Implementation Details}

To construct the dataset, we enumerate all the 3-combinations from the set of lowercase characters defined in ASCII. For each combination, we generate 600 distinct data points by varying the occurrence of each character and the order of the string. The occurrences are sampled uniformly from 1-9 (both inclusive). So the length of the part before the pipe symbol ("1") lies in [3, 27] (not considering "B"). Like 3-digit addition, we split the dataset into train and test sets, which account for $75 \%$ and $25 \%$ of all data respectively. The input is tokenized on the character level.

The model is a two-layer transformer with one head in each layer, the model dimension is 64. All dropout rates are set to 0 . The model is trained with cross-entropy loss on the last token, the answer of the counting task. The model is trained with a batch size of 128 for 100 epochs, using a constant learning rate of 0.0005 , weight decay of 0.01 , and AdamW [27] optimizer. The training loss is shown in Figure 11, we can see the stair-like pattern. An interesting future direction is to investigate what happens when the loss rapidly decreases using InversionView.

With regard to the decoder model, the architecture is described in C We select $x^{0, \text { pre }}, x^{i, \text { mid }}, x^{i, \text { post }}, a^{i, j}, m^{i}$ as the set of activation sites we are interested in, where $i \in\{0,1\}, j \in$ $\{0\}$, and $m$ denotes MLP layer output. The query activation is sampled from those activations corresponding to only the target character and colon. By default, we sample query activation with equal probability of activation sites, as is done when training the decoder for the other two case studies. During training, we test the generation quality quantified by the ratio of samples (temperature $=1$ ) that are inside the $\epsilon$-preimage $(\epsilon=0.1)$. For those activation sites for which the decoder has a low generation quality, we increase their probability of being sampled in the training data.

\section*{Activation Site: $x^{0, \text { mid }}$}

Query Input

0.000; B q e q q q a q a e q a a a q e q q a a | (a) : 7

Generated Samples

$$
\begin{aligned}
& 0.009 ; \#: 7 ; \text { B a a q q q q q a q a q q a a q e e a e| (a) }: 8 \text { E } \\
& 0.013 ; \#: 7 \text { B a a q q a q q q a q a q q a q e e a e| (a) }: 8 \text { E } \\
& \| 0.049 ; \#: 7 ; \text { B q e a q a e a q a m a e q q a q e q e a | (a) : } 8 \text { E } \\
& \| 0.054 ; \#: 7 ; \text { B q q a q q q a a q a q a q q a r a r q | (a) }: 8 \text { E } \\
& \| 0.065 ; \#: 8 ; \text { B a q q a q f a q q q q f a q a q a a a } \mid \text { (a) : } 9 \text { E } \\
& \text { ॥ } 0.065 ; \#: 7 ; \text { B a a q q a q a a g g q q q q q a q a q | (a) }: 8 \mathrm{E} \\
& \text { 10.066;\#: 8; B a a q a q a q q q a q q q q a q g a a | (a) : } 9 \text { E } \\
& 0.110 ; \#: 6 ; \text { B a o o a e e e a a a q q o q e e e q e a } \mid \text { (a) : } 7 \mathrm{E} \\
& \text { ■.117;\#: 7; B a a h c c q c q e a c a h a a h c q q q a | (a): } 8 \text { E } \\
& \text { - 0.122;\#:6; B g a g a a q g q q a q a h q q q q q a | (a): } 7 \text { E }
\end{aligned}
$$

Activation Site: $x^{0, \text { post }}$

Query Input

$0.000 ;$ B q e q q q a q a e q a a a q e q q a a $\mid$ (a) : 7

Generated Samples

\begin{abstract}
0.006;\#: 7 ; B q q q q q a q q q a a a a a e e q e a | (a) : 7 E 1 0.010;\#: 7; B a q a a q q q a q a e q e a q q e a q | (a) : 7 E 0.011; \#: 7; B q a q q q q q a q a q a q a a q a e e| (a) : 7 E 10.016;\#: 7 ; B q q q q a q q q q a q a a a e e a e a | (a) : 7 E 0.016;\#: 7 ; B a q a a a e a q q e e e q q q q q a a | (a): 7 E - 0.158;\#: 8; B q q n a a q a a q q a g a a a q q q q | (a) : 7 E - 0.180 ;\#: 8 ; B q a a q q q q q a i q a a a q q q a a | (a) : 7 E - 0.193;\#:6; B e e pque a a q e pe e a a q p a a p q | (a): 7 E - 0.207;\#: 6; B q q a e e e a q e a e g a d a q q q a | (a) : 7 E - 0.215; \#: 6 ; B q t q q q q a s s a t a a q q q a t a | (a) : 7 E
\end{abstract}

Figure 12: $\epsilon$-preimage showing function of MLP layer 0

Activation Site: $a^{1,0}$

Bwwnncwnnncwnncwncw|c(:) 4

Query Input

0.000 ; B w w n n c w n n ncwnncwncw|c (:) 4

Generated Samples

```

0.021;\#:4; B c c c c i i i i c v i ivc c v iv i|v (:) 4 E
l 0.044;\#:4; B c c r w g w r w w w c r w r rg cw t r r rw | c(:) 5 E
1 0.048;\#:4; B cjajjcjcjajcccaa|a(:) 4E
0.057;\#:4; B q j q e q j q n j j q n j j n q j n | n(:) 4E
1 0.057;\#:4; Bcjjccnnccjcjcjjnjjnc|n(:) 4E
0 0.090;\#:4; B j j c j j c c c j r c c c j j c r j r r | r(:) 4 E

- 0.097;\#:4; B c c cccjjjjjccccrrrjjjr|r(:) 4 E
    - 0.140;\#:5; B ttootw wwtwoothhtojt| o (:) 4 E
    - 0.250;\#:6; B b a b d b d a d d a b d a a d b d d d a b b | a (:) 5 E
0.298;\#:2; B e e vs n e ve v e s v v ev \ s (:) 3 E

```

Activation Site: $a^{1,0}$

\section*{B и 000 j $0 \mathbf{j}$ j 000 U (:) 1}

Query Input

0.000 ; B u o o o jojjo o o | u (:) 1

Generated Samples

$$
\begin{aligned}
& 0.081 \text {; \#: 1; B w w w w w w w w o e o d e (:) } 1 \text { E } \\
& \text { 0.055;\#: 1; B xt t tktknxxx|n(:) } 1 \mathrm{E} \\
& \text { 0.069;\#: 1; B i n in p n i i i n i | p (:) } 1 \text { E } \\
& 0.075 ; \#: 1 ; B \times x \times d x \times x \times x d a \mid a(:) 1 E
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-21.jpg?height=35&width=458&top_left_y=1365&top_left_x=1208)

$$
\begin{aligned}
& 0.111 ; \#: 1 ; B \mathrm{pd} \times \times \mathrm{r} \times \mathrm{d} \times \times \times \times \mathrm{p} \times \mathrm{d} \mid \mathrm{r} \text { (:) } 1 \mathrm{E} \\
& 0.323 ; \#: 0 ; B \vee k \mathrm{mkk} \mathrm{m} \mathrm{k} \mathrm{m} \mathrm{k} \mathrm{k} \mathrm{|} \mathrm{n} \mathrm{(:)} 1 \mathrm{E} \\
& 0.114 \text {; \#: 1; B p x p } 4 \text { p } \times \times \times \times \times n \mid n(:) 1 E \\
& 0.263 ; \#: 2 ; B|n| s|| w|| n \mid n(:) 2 E \\
& 0.430 ; \#: 3 ; B \mid \mathrm{m} \mathrm{m} v \times 1 \mathrm{~m} \text { w III|m (:) } 3 \mathrm{E}
\end{aligned}
$$

Figure 13: $\epsilon$-preimage of $a_{:}^{1,0}$. As we mentioned, we hypothesize that the attention head is reading the subspace where the count information is stored. One can presumably find this "count subspace" by optimizing a projection matrix such that after projecting the activation there is only pure count information in the $\epsilon$-preimage, and compare it with the subspace read by the value matrix of the attention head. Therefore, InversionView can be potentially useful for subspace study.

\section*{D. 2 More Examples of InversionView}

See Figures 12 and 13 Here, we show results similar to Figure 2 for other query inputs.

\section*{D. 3 Causal Intervention Details}

For an input example $\mathrm{x}_{\text {orig }}$ with $t_{\text {orig }}$ as the count (final token), we construct a contrast example $\mathrm{x}_{\text {con }}$ with a different count $t_{\text {con }}$ by changing a random character before "l". The contrast example is a valid input (the count token is the count of the target character). We also ensure that the contrast example is within the dataset distribution (the count is in the range [1-9] and there are 3 distinct characters in the input).

We run three forward passes. 1) The model takes as input $\mathrm{x}_{\text {orig }}$ and produces logit values for count prediction, we record the logit difference $L D_{\text {orig }}$ between $t_{\text {orig }}$ and $t_{\text {con }}$ (former minus latter). 2 ) We feed the model with $\mathrm{x}_{c o n}$ and store all activations. 3) We run a forward pass using $\mathrm{x}_{\text {orig }}$, replacing the interested activations (e.g, $\left\{a_{a}^{0,0}, a_{t c}^{0,0}\right\}$ ) with the stored activation in the same position and activation

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-22.jpg?height=653&width=567&top_left_y=237&top_left_x=779)

$\square$ Increment of ( $\left.L D_{\text {orig }}-L D_{p c h}\right)$

$-=L D_{\text {orig }}=L D_{\text {pch }}$

Patching Effect -->

Patching Effect $<--$

Patching Effect <--

Figure 14: Results of activation patching for model trained on character counting task. Same figure as 3a with intermediate steps of calculation shown using line plot. Note that the gray lines correspond to the y-axis on the right. In contrast examples, only one character differs. $L D$ stands for logit difference between the original count and the count in the contrast example. $L D_{\text {pch }}$ and $L D_{\text {orig }}$ correspond to the $L D$ with and without patching, respectively. Top: We patch activations cumulatively from left to right, flipping the sign of $L D$. The "none" on the left end of $x$-axis denotes the starting point, i.e., nothing is patched. Bottom: We patch from right to left. Similarly, "none" on the right end of x-axis denotes the starting point.

sites, and record the new logit difference $L D_{p c h}$. Because the model can make the right prediction in most cases, we can see that average $L D_{p c h}$ changes from positive to negative values as we patch more and more activations. We do the same for all inputs in the test set and report the average results.

Figure 14 shows the $L D_{\text {orig }}$ and $L D_{\text {pch }}$. We cumulatively patch the activations we study. For example, on the top of the figure, we patch $\left\{a_{:}^{0,0}\right\},\left\{a_{:}^{0,0}, a_{t c}^{0,0}\right\},\left\{a_{:}^{0,0}, a_{t c}^{0,0}, a_{:}^{1,0}\right\}$ respectively. Patching more activation results in increases of $L \dot{D}_{\text {orig }}-L D_{\text {pch }}$, we attribute the increment to the newly patched activation. Hence, the causal effect of each activation is measured conditioned on some activations already being patched.

We sort the activations according to their layer indices and show the results of patching from bottom to top $(\rightarrow)$ and from top to bottom $(\leftarrow)$. In this way, we can verify the dependence between activations in the top and bottom layer. For example, at the top of Figure 14, when $a_{t c}^{0,0}$ is already patched, patching $a_{:}^{1,0}$ has almost no effect. On the bottom, we also see patching $a_{t c}^{0,0}$ has no effect if $a_{:}^{1,0}$ has been patched. So we verified that $a_{t c}^{0,0}$ is the only upstream activation that $a_{:}^{1,0}$ relies on, and $a_{:}^{1,0}$ is the only downstream activation that reads $a_{t c}^{0,0}$.

\section*{D. 4 Extended Algorithm with Positional Cues}

In Section 3.2, we verified the information flow by an activation patching experiment in which the contrast example only differs by one character. These experiments verified that the algorithm we described is complete in distinguishing between such minimally different contrast examples. We now show that the model implements a somewhat more complex algorithm that combines this algorithm with position-based cues, which become visible once we consider contrast example that differ in more than one character, in particular, those that differ in length.

To show this, we conduct another activation patching experiment in which the contrast example is a random example in the dataset with a different count. In other words, everything can be different in contrast examples, including the sequence length and the target character. Thus, we cumulatively patch four places:

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-23.jpg?height=567&width=504&top_left_y=256&top_left_x=561)

Patched Activation

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-23.jpg?height=610&width=1028&top_left_y=253&top_left_x=554)

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-23.jpg?height=566&width=510&top_left_y=259&top_left_x=1057)

Patched Activation

Figure 15: Results of activation patching for model trained on character counting task. $\rightarrow$ and $\leftarrow$ means the same as previously. Left: Patching with activation from examples with different counts. Right: Patching with activation from examples in which only one character differs.

Activation Site: $a^{1,0}$

Buuuuubb muubmuub $\mathbf{m}$ (:) 2

Query Input

0.000 ; Buuuuubbmuubmuub $\mid$ m (:) 2

Generated Samples

```

1 0.038;\#:2; B k ekkkekeeeeeezzkkk|z (:) 2 E
1 0.058;\#:2; B a b a a bh b a b a a a b a h b b|h (:) 2 E
0.059;\#:2; Buujuuua auujjujjju|a (:) 2E
0.069;\#:2; B ajh ajjjh a a ajajja aj|h(:) 2 E
0 0.071;\#:2; B v n Ivvn | va vbvvv|n (:) 3 E
0 0.086;\#:2; B b b b a n a a a a n b a a b a a b b | n (:) 2 E
0.092;\#:2; B m j m a m j m m m j j m m j a m | a (:) 2 E
- 0.126;\#:2; B e b a e e e be a e e e be| a (:) 2 E
0.230;\#:3; B s spok os s skzksksksko|O (:) 2E
0.617;\#: 7; Bitvvttvtvvvititjvvtv|t(:) 3 E

```

$$
\text { Activation Site: } a^{1,0}
$$

\section*{Btttetnnttntxt|t (:) 9}

Query Input

$0.000 ; B t t t t n n n t \operatorname{cost} \mid \mathrm{t}() 9$

Generated Samples

```

0.022;\#:9; Btttttttttszzzz|t(:) 9E
0.025;\#:9;B t t m t t It t m m tm t t|t(:) 9E
0.028;\#:9;Btttttftfttffft|t(:) 9E
1 0.034;\#:9;Btttttttthrrrrt|t(:) 9 E
0.063;\#:9; Bviccvjjccccvcvcc|c(:) 9 E
- 0.100;\#:8; Btosstoctcttlttt|t(:) 9 E
- 0.111;\#:8;Bttttttptptpppe|t(:) 9E
- 0.159;\#:8;B rm cm crcrcrrrrrc|r(:) 9E
- 0.170;\#:8;B p faqqqp fqfqqdqq|q(:) 9E
- 0.458;\#:0; B m n\veenb\veevbBbb\veebb\veemmb\veev|q(:) 7E

```

Figure 16: $\epsilon$-preimage of $a_{:}^{1,0}$ to show the position information is also encoded and is independent of count information.
1. $e_{t c}^{p o s}$ and $e^{p o s}$, where $e^{p o s}$ stands for position embedding, because the final count correlates with positional signal, so the model may utilize it. They are patched together because the attention pattern of the colon in layer 1 relies on their adjacency.
2. $a_{t c}^{0,0}$ (as before) and $e_{t c}^{t k n}$, where $e^{t k n}$ stands for token embedding. They are patched together because patching only one of them would result in a conflict between character information in the patched and the un-patched activation;

3. 3) $a_{:}^{0,0}$; (as before)
4. $a_{:}^{1,0}$ (as before).

The result is shown on the left of Figure 15. We can see that, besides the components we had detected previously based on minimal contrast examples $\left(a_{t c}^{0,0}, a_{:}^{1,0}\right)$, some other signal also contributes notably to the final logits. We compare with patching experiments for the same set of activations on contrast examples that differ in one character, shown on the right of Figure 15

Overall, besides the algorithm identified in Section 3.2, we find other 3 sources of information influencing the model's output. 1) The position embedding, $e_{t c}^{p o s}$ and $e_{:}^{p o s}$. This is observable on the
left of Figure 15, from which we can also know $a_{:}^{1,0}$ contains the position information (because the bars of $e_{t c}^{\text {pos }}$ and $e_{:}^{\text {pos }}$ are not symmetric). This is confirmed by InversionView. As shown in Figure 16 . we see the inputs in $\epsilon$-preimage roughly follow the query input length, being independent of the count. Therefore, the model is also utilizing the correlation between the count and the sequence length. 2) Attention output of colon, $a_{:}^{0,0}$, which attends to all previous token equally. From InversionView, we observe it contains fuzzy information about the length (same as position signal), and the characters that occur in the context, as well as their approximate count. Our causal experiment also shows that it does not contain a precise count. Therefore, it contributes to the model's prediction in manner similar to the position signal. 3) Attention output of the pipe sign, $a_{\mid}^{0,0}$. From the attention pattern we observe sometimes in layer 0 , pipe sign attends selectively to one type of character, e.g. " $x$ ", " $k$ ", or " $\mathrm{j}$ ". InversionView shows that it indeed contains the approximate count in that case (though the decoder has not been trained on activation corresponding to pipe sign). In next layer, the colon also attends to the pipe sign if target character is the same as the character attended by pipe sign in layer 0 . This explains why we can observe nonzero effect of patching $a_{:}^{1,0}$ when other activation is already patched (the red bar corresponding to $a_{:}^{1,0}$ on both sub-figures of Figure 15.

Whereas patching with minimally different contrast examples allowed us to extract an algorithm sufficient for solving the task in Section 3.2 patching with arbitrarily different contrast examples allowed us to uncover that the model combines this algorithm with position-based cues. The model performs precise counting using the algorithm we found earlier in Section 3.2, while it also makes use of simple mechanisms such as correlation to obtain a coarse-grained distribution over counts. Overall, we have found the full algorithm by alternating between different methods - InversionView, traditional inspection of attention patterns, and causal interventions, and confirming results from one with others.

\section*{E IOI Task: Details and Qualitative Results}

\section*{E. 1 Implementation Details}

In order to train a decoder model, we construct a dataset that consists of IOI examples. We used the templates of IOI examples from ACDC [11] implementation. For example, "Then, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to $[\mathrm{A}]$ ", in which "[B]" and "[A]" will be replaced by two random names (one token name), "[PLACE]" and "[OBJECT]" will also be replaced by random item from the predefined set. Besides "BABA" template (i.e., $\mathrm{S}$ is before $\mathrm{IO}$ ) we also use "ABBA" templates (S is after IO) by swapping the first "[B]" and "[A]". We generate $250 \mathrm{k}$ data points.

The architecture of the decoder model is the same as before, as described in Appendix C. The set of activation sites the decoder is trained on consists of the output of all attention heads and MLP layers (no residual stream). Note that when producing query activation using GPT-2, we always add the " $<$ endoftext $\mid>$ " token as the BOS token. We do so as during the training of GPT-2 this or multiple such tokens that usually appear in the previous context can be used as a BOS token, which is possibly important to the model's functioning. We use a new token "[EOS]" as the EOS token when training the decoder. The query activation is sampled uniformly from all positions excluding EOS and padding tokens, and uniformly from all activation sites the decoder is trained for.

\section*{E. 2 More Examples of InversionView}

See Figures $17,18,19$

\section*{E. 3 Qualitative Examination Results}

The qualitative examination results is shown in Table 2. We summarize the description in [39] of each head category to facilitate comparison. Figure 20 shows the IOI circuit in GPT-2 small, which is taken from their paper. For more details, please refer to [39].

There are some heads for which InversionView indicates a different interpretation. First, Head 0.1 and 0.10: [39] only shows that they usually attend to the previous occurrence of a duplicate token and validates the attention pattern on different datasets. However, there is no evidence for the information moved by these heads. Thus they only hypothesize that the position of previous occurrence is copied. Second, Head 5.9: The path patching experiments in [39] show that head 5.9 influences the final

```

<|endoftext|>Then in the morning, Stephanie and Nicole were thinking about going to the hospital. Stephanie wanted to give a ring( to) Nicole
Query Input
0.000;<<endoftext|>Then in the morning, Stephanie and Nicole were thinking about going to the hospital. Stephanie wanted to give a ring( to) Nicole
Generated Samples
0.020;<lendoftext|>Then in the morning, Erin and Sarah were thinking about going to the house. Erin wanted to give a ring( to) Sarah[EOS]
0.027;<<endoftext|>Then in the morning, Sarah and Sarah were thinking about going to the station. Sarah wanted to give a ring(to) Sarah[EOS]
0.039;<lendoftext|>Then in the morning, Lindsey and Emily were thinking about going to the station. Lindsey wanted to give a ring( to) Emily[EOS]
1 0.055;<|endoftext|>Then, Lindsey and Anthony were thinking about going to the house. Lindsey wanted to give a necklace( to) Anthony[EOS]
1 0.062;<<endoftext|\Then in the morning, Kelly and Richard were thinking about going to the office. Kelly wanted to give a ring(to) Richard[EOS]
1 0.075;<<endoftext|Then, Kelly and Patrick were thinking about going to the office. Kelly wanted to give a ring(to) Patrick[EOS]
\ 0.086;<lendoftext|>Then, Brian and Katie were thinking about going to the school. Brian wanted to give a ring( to) Katie[EOS]
_ 0.170;<|endoftext|>Then, Michelle and David were thinking about going to the restaurant. David wanted to give a ring( to) Michelle[EOS
- 0.179;<<endoftext|>Then in the morning, John and Amber were thinking about going to the office. Amber wanted to give a necklace( to) John[EOS]
- 0.317;<<endoftext|>Then in the morning, Elizabeth and Tyler were thinking about going to the station. Tyler wanted to give a necklace( to) Elizabeth[EOS]

```

Figure 17: $\epsilon$-preimage of S-Inhibition Head 7.3. The relative position of $\mathrm{S} 1-$ but not its identity, is contained in the head output. That means, in the samples within the $\epsilon$-preimage, S1 always appears before the IO. While the relative position is encoded, the absolute position can vary, as can the identities of the names.

```

<|endoftext|>Friends separated at birth Bradley and Cody found a computer at the store.( Cody) gave it to Bradley
Query Input
0.000 ; <|endoftext|>Friends separated at birth Bradley and Cody found a computer at the store. ( Cody) gave it to Bradley
Generated Samples
$0.000 ;<$ endoftext|>Then in the morning afterwards, Cody and Jacob had a long argument. Afterwards Jacob said to( Cody)[EOS]
$0.000 ;<|e n d o f t e x t|>$ Then in the morning, Cody and Scott had a lot of fun at the school.( Cody) gave a bone to Scott[EOS]
$0.000 ;<\mid$ endoftext|>Then in the morning afterwards, Cody and Ryan had a long argument. Afterwards( Cody) said to Ryan[EOS]
$0.000 ;<\mid$ endoftext|>After taking a long break Cody and Lauren went to the station,( Cody) gave a computer to Lauren[EOS]
$0.001 ;<$ endoftext|>Afterwards, Cody and Danielle went to the restaurant.( Cody) gave a basketball to Danielle[EOS]
$0.001 ;<$ endoftext|>Then, Cody and Jonathan had a lot of fun at the house. Jonathan gave a bone to( Cody)[EOS]
$0.001 ;<$ endoftext|>Then, Paul and Cody had a long argument. Afterwards( Cody) said to Paul[EOS]
$0.392 ;<$ endoftext|>Then in the morning afterwards, Courtney and Dustin had a long argument. Afterwards( Dustin) said to Courtney[EOS]
- 0.392 ; <|endoftext|>Friends separated at birth Alicia and Dustin found a snack at the school.( Dustin) gave it to Alicia[EOS]
- 0.395 ; <|endoftext|>When soon afterwards Jason and Nicholas got a snack at the hospital, Nicholas decided to give the snack to( Jason)[EOS]

```

Figure 18: $\epsilon$-preimage of Duplicate Token Head 0.1. S name is contained in head output.

```

<|endoftext|>After taking a long break Bradley and Rebecca went to the store,(Bradley) gave a basketball to Rebecca
Query Input
0.000;<<endoftext|>After taking a long break Bradley and Rebecca went to the store,( Bradley) gave a basketball to Rebecca
Generated Samples
0.002;<<endoftext|>After taking a long break Bradley and Gregory went to the station,( Bradley) gave a computer to Gregory[EOS]
0.014;<|endoftext|>After taking a long break Kelly and Aaron went to the restaurant,(Kelly) gave a bone to Aaron[EOS]
0.015;<<endoftext|>After taking a long break Kelly and Aaron went to the hospital,(Kelly) gave a necklace to Aaron[EOS]
0.019;<|endoftext|>After taking a long break Kyle and Paul went to the hospital,(Kyle) gave a snack to Paul[EOS]
0.020;<<endoftext|>After taking a long break Travis and Andrea went to the restaurant, Andrea gave a kiss to( Travis)[EOS]
0.022;<<endoftext|>After taking a long break Megan and Benjamin went to the restaurant, Benjamin gave a ring to( Megan)[EOS]
0.024;<<endoftext|>After taking a long break Lauren and Joshua went to the garden, Joshua gave a necklace to( Lauren)[EOS]
0.028;<<endoftext|>After taking a long break Michelle and Amanda went to the restaurant,(Michelle) gave a kiss to Amanda[EOS]
0.032;<|endoftext|>While spending time together Dustin and Sarah were working at the station, Sarah gave a snack to( Dustin)[EOS]
- 0.370;<<endoftext|>After taking a long break Bradley and Andrea went to the restaurant, Patrick gave a computer to(Andrea)[EOS]

```

Figure 19: $\epsilon$-preimage of Induction Head 5.5. Position - but not identity - of the current token (token in parenthesis)'s last occurrence is contained in head output

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-26.jpg?height=464&width=1114&top_left_y=253&top_left_x=495)

Figure 20: IOI circuit in GPT-2 small. Figure 2 from [39]

logits notably via S-Inhibition Heads' keys. But there are no further experiments to explain the concrete function of this head. While the authors refer to it as a Fuzzy Induction Head, the induction score (measured by the attention weight from a token $T$ to the token after $T$ 's last occurrence) of this head shows a very weak induction pattern. Even if such pattern occurs, it cannot tell us what information is captured by this head. Interpretation with InversionView suggests that the head barely contains any information, within the input space of IOI-like patterns. One possibility is that head 5.9 recognizes the IOI pattern (i.e., there are two names and one is duplicated in the previous context), so that if an IOI-like pattern exists, S2 should be attended to by S-Inhibition heads. As the decoder model is trained on IOI examples and generates mostly IOI examples - that is, the input space $\mathcal{X}$ in (11) consists of IOI-like inputs, this information is by definition not visible. Expanding the input space to arbitrary language modeling would allow capturing such information; we leave this to future work

\section*{E. 4 Choice of Distance Metric in IOI}

As described in Section 3.3, we used cosine distance for the IOI task, while for the other two tasks, we use normalized Euclidean distance. In this section, we show that both distance metrics produce similar interpretations while cosine distance makes the meaningful patterns easier to identify.

In Figure 21 and 22 we show two examples of using normalized Euclidean distance as the distance metric. Readers can see more examples on our web application. We see that the samples generated by the decoder tend to have larger distances under normalized Euclidean distance. Nonetheless, we can still see the top-ranked samples show meaningful commonalities - indeed if we set $\epsilon=0.4$, we obtain the same interpretation as the one we obtain based on Figure $3 \mathrm{~b}$ and 17

While normalized Euclidean distance can produce a similar interpretation, we have to set a much larger $\epsilon$, which we find less intuitive. We believe this to be because of the differences in the activation dimensionality, which is 768 in the IOI task, much larger than in character counting (64) and addition (32) tasks: Under Euclidean distance, the ratio of all close samples to all possible samples becomes lower and lower when the dimension becomes higher. In other words, the volume of the $\epsilon$-preimage accounts for a very tiny proportion of the whole space in the high-dimensional case. By using cosine similarity, we allow more input samples to lie in a close distance with the query input, as cosine similarity ignores the magnitude difference. This suggests cosine similarity may overall be more suitable when applying InversionView in high-dimensional activations.

\section*{F 3-Digit Addition: More Details and Examples}

\section*{F. 1 Implementation Details}

We constructed 810,000 instances and applied a random $75 \%-25 \%$ train-test split. The probed model is trained with a constant learning rate of 0.0001 , a batch size of 512 , for 50 epochs ( 59350 steps). We save the last checkpoint as the trained model, and test it on the test set. We use the AdamW [27] optimizer with weight decay of 0.01 . The training loss is shown in Figure 23 .

\begin{tabular}{|c|c|c|c|c|}
\hline \begin{tabular}{l} 
Head \\
Category
\end{tabular} & \begin{tabular}{l} 
Function \\
According to 39
\end{tabular} & Position & \begin{tabular}{l} 
Observation \\
from InversionView
\end{tabular} & \begin{tabular}{l} 
Whether \\
Consis- \\
tent
\end{tabular} \\
\hline \begin{tabular}{l} 
Name Mover \\
Heads
\end{tabular} & \begin{tabular}{l} 
Copy whatever it attends to (in- \\
crease its logit). It will always \\
attend to IO because of previous \\
components.
\end{tabular} & $\mathrm{END}$ & 9.99 .6 10.0: IO name & Yes \\
\hline \begin{tabular}{l} 
Negative Name \\
Mover Heads
\end{tabular} & \begin{tabular}{l} 
Copy whatever it attends to (de- \\
crease its logit), it attends to IO
\end{tabular} & $\mathrm{END}$ & 10.7 11.10: IO name & Yes \\
\hline \begin{tabular}{l} 
S-Inhibition \\
Heads
\end{tabular} & \begin{tabular}{l} 
Move info about S and cause \\
Name Mover Heads to attend \\
less to S. They attend to S2. \\
They adds two signals to resid- \\
ual stream, one is token value \\
of S, the other is position of S1, \\
and position signal has a greater \\
effect.
\end{tabular} & $\mathrm{END}$ & \begin{tabular}{l} 
7.3 8.6: Position of S1 (relative \\
position to IO, same for the fol- \\
lowing part when we say posi- \\
tion) \\
7.9: S name; Position of $\mathrm{S} 1$ \\
8.10: S name; Position of S1 \\
(most of the time)
\end{tabular} & Yes \\
\hline \begin{tabular}{l} 
Duplicate To- \\
ken Heads
\end{tabular} & \begin{tabular}{l} 
Attend from S2 to S1, more gen- \\
erally, attend to previous dupli- \\
cate token, and copy the position \\
of this previous occurrence.
\end{tabular} & $\mathrm{S} 2$ & \begin{tabular}{l}
$0.10 .10:$ S name (instead of \\
position) \\
3.0: Position of the duplicated \\
name
\end{tabular} & Partly \\
\hline \begin{tabular}{l} 
Previous Token \\
Heads
\end{tabular} & \begin{tabular}{l} 
Attend the previous token, move \\
the token (name) to $S 1+1$, then \\
this information is used as key \\
of $S 1+1$ in Induction Heads
\end{tabular} & $\mathrm{S} 1+1$ & \begin{tabular}{l} 
2.2: S name (most of the time, \\
sometimes attend to "and" when \\
S1 is after IO and make S name \\
less important) \\
4.11: S name; Position of the \\
previous token (relative to the \\
other name)
\end{tabular} & Yes \\
\hline \begin{tabular}{l} 
Induction \\
Heads
\end{tabular} & \begin{tabular}{l} 
Attend to $\mathrm{S} 1+1$ and move posi- \\
tion signal (similar to the func- \\
tion of Duplicate Token Heads, \\
while different from normal in- \\
duction heads)
\end{tabular} & $\mathrm{S} 2$ & \begin{tabular}{l}
5.56 .9 : Position of the current \\
token's last occurrence \\
5.8: Position of the current to- \\
ken's last occurrence (most of \\
the time) \\
5.9: Almost no info, possibly \\
some info about the template.
\end{tabular} & Partly \\
\hline \begin{tabular}{l} 
Backup Name \\
Mover Heads
\end{tabular} & \begin{tabular}{l} 
Do not move IO normally, but \\
act as Name Mover Heads when \\
they are knocked out. \\
There are 4 categories: \\
$9.0,10.1,10.10,10.6$ : similar to \\
Name Mover \\
10.2, 11.9: attend to both $\mathrm{S} 1$ and \\
IO, and move both \\
11.2: attends to S1 and move S \\
9.7 : attends to S2 and writes \\
negatively
\end{tabular} & END & \begin{tabular}{l}
$9.010 .110 .10:$ IO name \\
$10.6: \mathrm{S}$ and/or IO name (Most \\
of the time: IO, sometimes: IO \\
and $\mathrm{S}$, occasionally: $\mathrm{S}$ ) \\
$10.2: \mathrm{S}$ and IO name (most of \\
the time), $\mathrm{S}$ or IO name (some- \\
times) \\
11.9: S and/or IO name (no ob- \\
vious difference in frequency) \\
11.2: S and IO name (most of \\
the time), $\mathrm{S}$ name (sometimes) \\
$9.7: \mathrm{S}$ name
\end{tabular} & Yes \\
\hline
\end{tabular}

Table 2: Column "Position" means the query activation is taken from that position. "S1+1" means the token right after S1. Rows are ordered according to the narration in the original paper. When we say "S name", it means the the name of $\mathrm{S}$ in the query input, but the name is not necessarily $\mathrm{S}$ in the samples. This also applies to "IO name". The information learned by InversionView which is different from the information suggested by Wang et al. [39] is in bold.
<endoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin
Query Input
$0.000 ;<\mid$ endoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin
Generated Samples
- 0.208 ; <|endoftext|>While spending time together Erin and Justin were working at the house, Erin gave a computer( to) Justin[EOS]
- $0.243 ;<$ endoftext|>While Justin and Erin were working at the hospital, Erin gave a kiss( to) Justin[EOS
- 0.259 ; <|endoftext|>Then in the morning, Timothy and Justin went to the garden. Timothy gave a snack(to) Justin[EOS
- 0.266 ; <|endoftext|>Then in the morning, Benjamin and Justin had a long argument, and afterwards Benjamin said( to) Justin[EOS]
- 0.268 ; $<\mid$ endoftext|>While spending time together Eric and Justin were commuting to the hospital, Eric gave a ring( to) Justin[EOS]
- 0.273 ; <|endoftext|>Friends separated at birth Patrick and Justin found a computer at the hospital. Patrick gave it(to) Justin[EOS
- 0.275 ; <|endoftext|>While Daniel and Justin were commuting to the office, Daniel gave a kiss( to) Justin[EOS]
- $0.302 ;<\mid$ endoftext|>While spending time together Thomas and Justin were working at the school, Thomas gave a ring(to) Justin[EOS
- $0.311 ;<\mid$ endoftext|>While spending time together Sean and Justin were working at the hospital, Sean gave a computer( to) Justin[EOS]
- $0.374 ;<$ endoftext|>Then, Justin and Kristen went to the hospital. Kristen ( gave) a bone to Justin[EOS]
- $0.444 ;<\mid$ endoftext|>While spending time together Justin and Kristen were commuting to the garden, Kristen( gave) a bone to Justin[EOS]
- $0.491 ;<$ endoftext|>Then in the morning, Justin and Kristen had a lot of fun at the hospital(.) Justin gave a basketball to Kristen[EOS]
- 0.541 ;<|endoftext|>Then in the morning, Justin and Kristen were working at the office(.) Justin decided to give a bone to Kristen[EOS]
- $0.783 ;<\mid$ endoftext $\mid>$ While spending time together Joshua and Justin were commuting to the station(,) Justin gave a ring to Joshua[EOS]
- $0.940 ;<\mid$ endoftext|>While James and Kristen were commuting to the school, James ( gave) a snack to Kristen[EOS

Figure 21: $\epsilon$-preimage of the same activation as Figure $3 \mathrm{~b}$ using normalized Euclidean distance instead of cosine similarity (Appendix E.4). The line shown in the figure still represents threshold of 0.1 . While at $\epsilon=0.1$, some query inputs do result in a substantial number of in- $\epsilon$-preimage samples, many cases result in very small or even empty (as here) sample sets, suggesting that the $\epsilon$-preimage at 0.1 - at least as accessible to the decoder - is extremely small in this model, which we speculate is related to the models higher dimensionality compared to the other tasks ( 768 vs 32/64), which tends to make Euclidean distances large except for extremely similar vectors (see Appendix E. 4 for more discussion). We find it more convenient and natural to use similar thresholds $(\epsilon=0.1)$ across tasks and account for the different geometries using different distance metrics. What is key, however, is that for an appropriately higher $\epsilon$ (e.g., $\epsilon=0.4$ ) we again always obtain a substantial number of samples, and - most importantly - these samples lead to the same interpretation as we obtained in our main experiments with the cosine similarity. Here, for example, we can obtain the same interpretation by setting $\epsilon=0.4$, i.e., the IO name is encoded in the activation.

```

<|endoftext|>Then in the morning, Stephanie and Nicole were thinking about going to the hospital. Stephanie wanted to give a ring( to) Nicole
Query Input
0.000;<<endoftext|Then in the morning, Stephanie and Nicole were thinking about going to the hospital. Stephanie wanted to give a ring( to) Nicole
Generated Samples
= 0.186;<<endoftext|TThen, Michelle and Patrick were thinking about going to the hospital. Michelle wanted to give a ring(to) Patrick[EOS]
E- 0.195;<|endoftext|>Then in the morning, Sarah and Jason were thinking about going to the station. Sarah wanted to give a ring(to) Jason[EOS]
= 0.239;<<endoftext|>Then in the morning, Lindsey and Joseph were thinking about going to the garden. Lindsey wanted to give a ring( to) Joseph[EOS]
- 0.277;<lendoftext|>Then, Shannon and Erin were thinking about going to the office. Shannon wanted to give a ring(to) Erin[EOS
= 0.277;<lendoftext|>Then in the morning, Katherine and William were thinking about going to the store. Katherine wanted to give a ring(to) William[EOS]
- 0.285;<<endoftext|>Then, Kelly and Rachel were thinking about going to the house. Kelly wanted to give a necklace( to) Rachel[EOS]
= 0.302;<lendoftext|Then in the morning, Kelly and Courtney were thinking about going to the house. Kelly wanted to give a ring( to) Courtney[EOS
- 0.317;<<endoftext|TThen in the morning, Jamie and Robert were thinking about going to the garden. Jamie wanted to give a necklace(to) Robert[EOS]
- 0.323;<<endoftext|>Then in the morning, Lindsey and Bryan were thinking about going to the station. Lindsey wanted to give a necklace (to) Bryan[EOS]
= 0.331;<<endoftext|>Then, Lindsey and Jessica were thinking about going to the office. Lindsey wanted to give a necklace( to) Jessica[EOS]
- 0.331;<<endoftext|>Then in the morning, Allison and William were thinking about going to the house. Allison wanted to give a basketball( to) William[EOS]

- 0.476;<<endoftext|Then in the morning, John and Amy were thinking about going to the garden. Amy wanted to give a ring(to) John[EOS]
- 0.484; < endoftext|>Then in the morning, Richard and Scott were thinking about going to the garden. Scott wanted to give a ring( to) Richard[EOS
- 0.512;<<endoftext|Then, Jamie and Melissa were thinking about going to the store. Melissa wanted to give a ring( to) Jamie[EOS
- 0.539;<<endoftext|Then in the morning, Lindsey and Christina were thinking about going to the garden. Christina wanted to give a bone(to) Lindsey[EOS]

```

Figure 22: $\epsilon$-preimage of the same activation as Figure 17 using normalized Euclidean distance. The line shown in the figure still represents the threshold of 0.1. As explained in Figure 21, due to the representation geometry, normalized Euclidean distance tends to require a much higher threshold to obtain a sufficient sample size for interpretation. Importantly, as also explained there, we still obtain the same interpretation as in our main experiments if we use normalized Euclidean distance but take a higher threshold (e..g, $\epsilon=0.4$ ): here, the relative position of $\mathrm{S} 1$ is encoded in the activation.

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-29.jpg?height=531&width=697&top_left_y=241&top_left_x=711)

Figure 23: Training loss of the 3-digit addition task. Each data point is the averaged loss over an epoch. The final loss is still big since the two operands of the addition is unpredictable.

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-29.jpg?height=710&width=902&top_left_y=916&top_left_x=384)

Activation Site: $a^{0,1}$

B $615+861$ (=) 1476

\section*{Query Input}

0.000 ; В $615+861$ (=) 1476

Generated Samples

$0.000 ;$ В $632+829$ (=) $1461 \mathrm{E}$

0.000 ; B $668+825$ (=) $1493 \mathrm{E}$

$0.000 ;$ B $612+829 \Leftrightarrow 1441 \mathrm{E}$

0.000 ; В $646+868$ (=) $1514 \mathrm{E}$

$0.000 ;$ В $653+834 \Leftrightarrow 1487 \mathrm{E}$

$0.022 ;$ B $821+643$ (=) $1464 \mathrm{E}$

0.022 ; B $827+634$ (=) $1461 \mathrm{E}$

0.022 ; B $869+618$ (=) $1487 \mathrm{E}$

■ 0.173 ; B $582+885$ (=) $1467 \mathrm{E}$

$0.936 ;$ В $609+729 \Leftrightarrow 1338 \mathrm{E}$
Activation Site: $a^{0,2}$

B $615+861$ (=) 1476

Query Input

$0.000 ;$ В $615+861 \Leftrightarrow 1476$

Generated Samples

0.000 ; В $696+820$ (=) $1516 \mathrm{E}$

0.001 ; B $666+824 \Leftrightarrow 1490 \mathrm{E}$

0.010 ; B $893+633 \Leftrightarrow 1526 \mathrm{E}$

0.010 ; B $837+634$ H $1471 \mathrm{E}$

0.010 ; В $896+692$ (=) $1588 \mathrm{E}$

0.010 ; B $809+619$ (=) $1428 \mathrm{E}$

0.010 ; B $880+654$ (=) $1534 \mathrm{E}$

$10.043 ;$ B $924+686 \Leftrightarrow 1610 \mathrm{E}$

$0.364 ;$ B $572+756=$ (1) $328 \mathrm{E}$
Activation Site: $x^{0, p o s t}$

Query Input

0.000 ; B $615+861 \Leftrightarrow 1476$

Generated Samples

$$
\begin{aligned}
& 0.001 ; \text { B } 612+868 \Leftrightarrow 1480 \mathrm{E} \\
& 0.011 ; \text { B } 816+660 \Leftrightarrow 1476 E \\
& 0.045 ; \text { B } 648+810 \Leftrightarrow 1458 \mathrm{E} \\
& 10.046 ; \text { B } 843+612 \Leftrightarrow 1455 \mathrm{E} \\
& \text { 1 } 0.048 ; \text { B } 817+649 \Leftrightarrow 1466 \mathrm{E} \\
& 10.049 ; \text { B } 833+616 \Leftrightarrow 1449 E \\
& 0.055 ; \text { B } 822+618 \Leftrightarrow 1440 \mathrm{E} \\
& 10.068 ; \text { В } 812+608 \Leftrightarrow 1420 \mathrm{E} \\
& 0.408 ; \text { B } 765+614 \text { (=) } 1379 E \\
& 1.272 ; \text { B } 147+812 \Leftrightarrow 959 E
\end{aligned}
$$

Figure 24: The $\epsilon$-preimage of $a \xlongequal{0,1}, a_{=}^{0,2}$ and $x_{=}^{0, \text { post }}$ for the same query input as Figure 4 .

With regard to the decoder model, we select $x^{0, \text { pre }}, x^{i, \text { mid }}, x^{i, \text { post }} a^{i, j}$ as the set of activation sites we are interested, where $i \in\{0,1\}, j \in\{0,1,2,3\}$. When sampling training data, we select an activation site and a token position uniformly at random.

\section*{F. 2 More Examples of InversionView}

See Figures 24 to $27{ }^{3}$

\section*{F. 3 Model Deficiency}

In Section 3.4, we mention that there is no firm and clear path of obtaining the carry from ones to tens, so the model may make wrong prediction. We examine those instances for which the model makes wrong prediction and find they all satisfy one condition: F2+S2=9. In other words, it fails to make the right prediction because the ones place matters. This is consistent with our interpretation of the model. Furthermore, we check the model's accuracy on this special subset where F2+S2=9, and find that it is significantly higher than chance level. The accuracy on training subset (training
\footnotetext{
${ }^{3}$ We have also tried value-weighted attention pattern [23]; it makes a negligible difference, so we always show the original attention pattern in our paper.
}

$$
\text { Activation Site: } x^{0, m i d}
$$

Query Input

$0.000 ;$ B $271+829 \Leftrightarrow 1100$

Generated Samples

0.014 ; B $879+224 \Leftrightarrow 1103 \mathrm{E}$ $0.015 ; B 821+278 \Leftrightarrow 1099 E$ $0.015 ; B 825+276 \Leftrightarrow 1101 E$ $0.016 ; B 820+279 \Leftrightarrow 1099 \mathrm{E}$ 0.016 ; B $879+229 \Leftrightarrow 1108 \mathrm{E}$ $0.016 ;$ B $823+271 \Leftrightarrow 1094 E$ $10.083 ;$ B $816+284 \Leftrightarrow 1100 \mathrm{E}$ $0.342 ;$ B $835+273 \Leftrightarrow 1108 \mathrm{E}$ $0.465 ;$ B $721+278 \Leftrightarrow 999 \mathrm{E}$

$0.465 ;$ B $721+274 \Leftrightarrow 995 \mathrm{E}$
Activation Site: $x^{0, \text { mid }}$

Query Input

$0.000 ;$ B $835+141$ (=) 976

Generated Samples

$$
\begin{aligned}
& 0.003 ; \text { B } 844+131 \Leftrightarrow 975 \mathrm{E} \\
& 0.008 ; \text { В } 843+138 \Leftrightarrow 981 \mathrm{E} \\
& 0.013 ; \text { B } 137+843 \Leftrightarrow 980 \mathrm{E} \\
& 0.021 ; \text { B } 836+139 \Leftrightarrow 975 \mathrm{E} \\
& 0.023 ; \text { B } 133+839 \Leftrightarrow 972 \mathrm{E} \\
& 0.029 ; \text { B } 830+139 \Leftrightarrow 969 \mathrm{E} \\
& 0.086 ; \text { B } 126+853 \Leftrightarrow 979 \mathrm{E} \\
& 0.086 ; \text { B } 149+821 \Leftrightarrow 970 \mathrm{E} \\
& \hline 0.157 ; \text { B } 145+848 \Leftrightarrow 993 \mathrm{E} \\
& 0.880 ; \text { B } 855+830 \Leftrightarrow 1685 \mathrm{E}
\end{aligned}
$$

Activation Site: $x^{0, \text { mid }}$

Query Input

$0.000 ;$ B $638+152$ (=) 790

Generated Samples

\begin{tabular}{l}
$0.018 ;$ В $152+631$ (=) $783 \mathrm{E}$ \\
$0.027 ;$ В $132+650$ (=) $782 \mathrm{E}$ \\
$0.033 ;$ В $638+159$ (=) $797 \mathrm{E}$ \\
$\| 0.076 ;$ В $149+642$ (=) $791 \mathrm{E}$ \\
$\| 0.078 ;$ В $648+148$ (=) $796 \mathrm{E}$ \\
$\square 0.089 ;$ В $142+646$ (=) $788 \mathrm{E}$ \\
\hline $0.154 ;$ В $137+644$ (=) $781 \mathrm{E}$ \\
$0.465 ;$ В $736+156$ (=) $892 \mathrm{E}$ \\
$0.575 ;$ В $658+157$ (=) $815 \mathrm{E}$ \\
$0.590 ;$ В $172+630$ (=) $802 \mathrm{E}$
\end{tabular}

Figure 25: The $\epsilon$-preimage of $x \stackrel{0, \text { mid }}{=}$ of different examples.

Activation Site: $a^{1,1}$

\section*{B $562+119=68$ (1)}

```

Query Input
0.000; B 562+119=68 (1)
Generated Samples
| 0.029; B 482+299=78 (1) E
। 0.033; B 372+509=88 (1) E
। 0.033; B 372+509=88 (1) E
I 0.046; B 142+759=90 (1) E
I 0.047; B 2 92+41 9= 71 (1) E
| 0.048; B 4 12+449=86 (1) E
| 0.053; B 1 62+769=93 (1) E
\.114; B 4 31+239=67}\mathrm{ (0)E
- 0.261; В 791+668=145 (9) E
= 0.271; B 772+419=119(1) E

```

Activation Site: $a^{1,3}$

\section*{B268+899=1 (1) 67}

Query Input

$0.000 ;$ B $268+899=1$ (1) 67

Generated Samples

$$
\begin{aligned}
& 0.010 ; \text { B } 269+898=1 \text { (1) } 67 \mathrm{E} \\
& 0.011 ; \text { B } 869+298=1 \text { (1) } 67 \mathrm{E} \\
& 0.020 ; \text { B } 867+299=1 \text { (1) } 66 \mathrm{E} \\
& 0.067 ; \text { B } 868+298=1 \text { (1) } 66 \mathrm{E} \\
& \| 0.071 ; \text { B } 162+999=1 \text { (1) } 61 \mathrm{E} \\
& 0.073 ; \text { В } 865+299=1 \text { (1) } 64 \mathrm{E} \\
& 0.079 ; \text { B } 666+499=1 \text { (1) } 65 \mathrm{E} \\
& \hline 0.141 ; \text { В } 969+196=1 \text { (1) } 65 \mathrm{E} \\
& 0.260 ; \text { B } 779+996=1 \text { (7) } 75 \mathrm{E} \\
& 0.291 ; \text { B } 364+197=\text { (5) } 61 \mathrm{E}
\end{aligned}
$$

Activation Site: $a^{1,3}$

\section*{B $107+559=6$ (6) 6}

Query Input

0.000 ; B $107+559=6$ (6) 6

Generated Samples

0.000 ; B $307+689=9$ (9) $6 E$

0.000 ; B $207+829=10$ (3) $6 E$

0.001 ; B $377+959=13$ (3) $6 \mathrm{E}$

0.004 ; B $397+809=12$ (0) $6 \mathrm{E}$

0.004 ; В $247+799=10$ (4) $6 \mathrm{E}$

0.005 ; B $767+649=14$ (1) $6 E$

$0.005 ;$ B $347+349=6$ (9) $6 E$

। 0.045 ; B $167+738=90$ (5) E

$0.628 ; B 736+728=146$ (4) E

$0.706 ;$ В $836+869=170$ (5) E

Figure 26: $\epsilon$-preimage of more examples

data where $\mathrm{F} 2+\mathrm{S} 2=9$ ) is $80.45 \%$, and on test subset is $80.06 \%$, while chance level is $50 \%$. So, we can infer that the probed model obtains some information about the ones place by means other than memorization. Indeed, we observe fuzzy information about ones place in $a^{1,0}$ and $a^{1,1}$ occasionally (See Figure 28).

\section*{F. 4 Qualitative Examination Results}

We present our overall qualitative results in Figure 29 and Table 3. We have found that the model obtains required digits by attention, and primary digits are assigned more heads than secondary digits, e.g., $a^{0,0}, a^{0,1}, a^{0,2}$ for hundreds and $a^{0,3}$ for tens when predicting A1. More importantly, the primary digits are encoded precisely while the secondary digits are encoded approximately in the residual stream. In addition, the model routes the information differently based on whether $\mathrm{A} 1=1$, i.e., the length of the answer is 3 or 4 . When predicting A2, this information is known before layer 0 , thus paths differ from the start. On the contrary, when predicting A3, the information is obtained in layer 0 , thus paths differ only in layer 1. Furthermore, in Figure 29 sub-figure (c) and (d) are very similar, indicating model uses almost the same algorithm to predict digit in tens place. While sub-figure (e) shares the layer 0 with (d), its layer 1 is similar to (f).

Activation Site: $a^{1,0}$

B $271+829=(1) 100$

Query Input

$0.000 ;$ B $271+829=(1) 100$

Generated Samples

$0.009 ;$ В $874+229=$ (1) $103 \mathrm{E}$
$0.019 ;$ В $265+834=$ (1) $099 \mathrm{E}$
$0.029 ;$ В $260+834=$ (1) $094 \mathrm{E}$
$0.048 ;$ В $254+846=$ (1) $100 \mathrm{E}$
$0.085 ;$ B $240+852=$ (1) $092 \mathrm{E}$
$0.098 ;$ В $762+339=$ (1) $101 \mathrm{E}$
0.120 ; В $359+743=$ (1) $102 \mathrm{E}$
$0.258 ;$ B $457+658=$ (1) $115 \mathrm{E}$
$0.403 ;$ B $920+277=$ (1) $197 \mathrm{E}$
0.670 ; В $951+257=$ (1) $208 \mathrm{E}$

Activation Site: $a^{1,0}$

\section*{B $808+294=$ (1) 102}

Query Input

$0.000 ;$ В $808+294=(1) 102$

Generated Samples

\begin{tabular}{l}
$0.029 ;$ В $890+202=(1) 092 \mathrm{E}$ \\
$10.039 ;$ В $278+824=$ (1) $102 \mathrm{E}$ \\
10.044 В $876+223=$ (1) $099 \mathrm{E}$ \\
$\| 0.081 ;$ В $386+714=$ (1) $100 \mathrm{E}$ \\
$\| 0.086 ;$ В $856+246=$ (1) $102 \mathrm{E}$ \\
$\| 0.091$ В $858+246=$ (1) $104 \mathrm{E}$ \\
\hline 0.109 В $249+858=$ (1) $107 \mathrm{E}$ \\
0.386 В $649+437=$ (1) $086 \mathrm{E}$ \\
$0.436 ;$ В $900+291=$ (1) $191 \mathrm{E}$ \\
0.437 В $900+292=$ (1) $192 \mathrm{E}$
\end{tabular}

Figure 27: Some examples where we can see intermediate states between representing digits separately and representing digits as their sum. In these examples, we see in the $\epsilon$-preimage the digits in hundreds place are either $(2,8)$ or $(3,7)$, while the digits in tens place are mostly encoded as their sum.

Activation Site: $a^{1,0}$

B $573+269$ (=) 842

Query Input

0.000 ; B $573+269 \Leftrightarrow 842$

Generated Samples

$10.032 ;$ B $562+279 \Leftrightarrow 841 \mathrm{E}$

I 0.048 ; B $559+262$ (=) $821 \mathrm{E}$

$10.054 ;$ B $569+283 \Leftrightarrow 852 \mathrm{E}$

॥ 0.090 ; B $584+279 \Leftrightarrow 863 \mathrm{E}$

II $0.094 ;$ B $569+254 \Leftrightarrow 823 \mathrm{E}$

॥ 0.094 ; B $569+254$ ( $823 \mathrm{E}$

. 0.098 ; B $569+263 \Leftrightarrow 832 E$
- $0.206 ;$ В $535+283 \Leftrightarrow 818 \mathrm{E}$

$0.427 ;$ В $550+258 \Leftrightarrow 808 E$

$0.927 ;$ В $370+649 \Leftrightarrow 1019 \mathrm{E}$
Activation Site: $a^{1,1}$

\section*{B $352+106$ (=) 458}

Query Input

$0.000 ;$ В $352+106 \Leftrightarrow 458$

Generated Samples

$10.055 ;$ B $135+302$ ( $437 \mathrm{E}$

10.059 ; В $364+103$ H $467 \mathrm{E}$

॥ 0.071 ; B $124+355 \Leftrightarrow 479 \mathrm{E}$

I 0.074 ; B $135+301 \Leftrightarrow 436 \mathrm{E}$

II 0.075 ; В $335+101 \Leftrightarrow 436 \mathrm{E}$

I $0.076 ;$ B $141+307 \Leftrightarrow 448 \mathrm{E}$

. 0.098 ; В $345+113 \Leftrightarrow 458 \mathrm{E}$
- $0.105 ;$ B $142+301 \Leftrightarrow 443 \mathrm{E}$

0.299 ; B $944+141=1$ (0) $85 E$

$0.305 ;$ В $307+110 \Leftrightarrow 417 \mathrm{E}$
Activation Site: $a^{1,1}$

B $534+220$ (=) 754

Query Input

0.000 ; B $534+220 \Leftrightarrow 754$

Generated Samples
- 0.082 ; B $240+524$ (=) $764 \mathrm{E}$

I 0.083 ; B $624+134$ (=) $758 \mathrm{E}$

\| 0.088 ; В $213+535$ (=) $748 \mathrm{E}$

$0.162 ;$ B $851+28(4)=1135 \mathrm{E}$
- $0.184 ;$ B $435+22(4)=659 \mathrm{E}$
- $0.218 ;$ B $430+274$ ( $704 \mathrm{E}$
- $0.284 ;$ B $470+33(4)=804 \mathrm{E}$

-10.321;B $682+704=$ (1) $386 \mathrm{E}$

$0.446 ;$ B $583+36(5)=948 \mathrm{E}$

$0.482 ;$ В $932+580=1$ (5) $12 \mathrm{E}$

Figure 28: Some examples where information about ones place is also encoded.

\begin{tabular}{|c|c|c|c|c|}
\hline \begin{tabular}{l} 
Pred \\
Target
\end{tabular} & A1 & $\mathbf{A} 2$ & $\mathbf{A 3}$ & A4 / E \\
\hline$a^{0,0}$ & \multirow{3}{*}{\begin{tabular}{l} 
1-2 digits from $F 1$ \\
and $\mathrm{S} 1$
\end{tabular}} & whether $\mathrm{A} 1=1$ & \begin{tabular}{l} 
whether $\mathrm{A} 1=1$, so the \\
model knows A3 is in \\
tens or ones place
\end{tabular} & \begin{tabular}{l} 
whether $\mathrm{A} 1=1$, so the \\
model knows the next \\
token is $\mathrm{A} 4$ or $\mathrm{E}$
\end{tabular} \\
\hline$a^{0,1}$ & & \begin{tabular}{l} 
If $\mathrm{A} 1=1:$ Almost no \\
info \\
Else: $1-2$ digits from \\
F2 and S2
\end{tabular} & \multirow[t]{2}{*}{\begin{tabular}{l}
$1-2$ digits from $\mathrm{F} 2$ \\
and $\mathrm{S} 2$
\end{tabular}} & \multirow[t]{3}{*}{\begin{tabular}{l} 
1-2 digits from F3 \\
and S3
\end{tabular}} \\
\hline$a^{0,2}$ & & \begin{tabular}{l} 
If $\mathrm{A}=1$ : both $\mathrm{F} 1$ and \\
S1 \\
Else: $1-2$ digits from \\
$\mathrm{F} 2$ and $\mathrm{S} 2$
\end{tabular} & & \\
\hline$a^{0,3}$ & \begin{tabular}{l}
$1-2$ digits from $\mathrm{F} 2$ \\
and $\mathrm{S} 2 ; \mathrm{C} 2$
\end{tabular} & \begin{tabular}{l} 
If $A=1: \quad 1-2$ digits \\
from $\mathrm{F} 1$ and $\mathrm{S} 1$ \\
Else: $1-2$ digits from \\
F3 and S3; C3
\end{tabular} & \begin{tabular}{l}
1 digit from $\mathrm{F} 3$ and \\
$\mathrm{S} 3(2$ when $\mathrm{F} 3=\mathrm{S} 3)$ \\
$\mathrm{C} 3$
\end{tabular} & \\
\hline$x^{0, \mathrm{mid}}$ & $\mathrm{F} 1$ and $\mathrm{S} 1 ; \mathrm{C} 2$ & \begin{tabular}{l} 
A1 \\
If $\mathrm{A} 1=1: \mathrm{F} 1$ and $\mathrm{S} 1$ \\
Else: $\mathrm{F} 2$ and $\mathrm{S} 2 ; \mathrm{C} 3$
\end{tabular} & \begin{tabular}{l}
$\mathrm{A} 2 ; \mathrm{F} 2$ and $\mathrm{S} 2 ; \mathrm{C} 3$ \\
whether $\mathrm{A} 1=1$
\end{tabular} & \begin{tabular}{l}
$\mathrm{A} 3 ; \quad \mathrm{F} 3$ and $\mathrm{S} 3 ;$ \\
whether $\mathrm{A} 1=1$
\end{tabular} \\
\hline$x^{0, \text { post }}$ & same as $x^{0, \text { mid }}$ & same as $x^{0, \text { mid }}$ & same as $x^{0, \text { mid }}$ & same as $x^{0, \text { mid }}$ \\
\hline$a^{1,0}$ & \begin{tabular}{l} 
Fuzzy info about $\mathrm{F} 1$, \\
$\mathrm{S} 1$ and $\mathrm{C} 2$; \\
Fuzzy info about $\mathrm{F} 3$ \\
and S3 (sometimes)
\end{tabular} & \begin{tabular}{l} 
If A1=1: F1 and S1 \\
(sometimes fuzzy); \\
C2 (sometimes) \\
Else: $1-2$ digits from \\
F2 and S2;
\end{tabular} & \multirow{4}{*}{\begin{tabular}{l} 
If $\mathrm{A} 1=1: 1-2$ digits \\
from $\mathrm{F} 2$ and $\mathrm{S} 2 ; \mathrm{C} 3$ \\
(sometimes); \\
For $a^{1,2}$, also $1-2$ \\
digits from F1 and S1 \\
(fuzzy); \\
Else: $1-2$ digits from \\
F3 and S3
\end{tabular}} & \multirow{4}{*}{\begin{tabular}{l} 
1-2 digits from $\mathrm{F} 3$ \\
and $\mathrm{S} 3$; \\
For $a^{1,0}$ and $a^{1,1}$ \\
they also contain info \\
about whether next \\
token should be $\mathrm{E}$
\end{tabular}} \\
\hline$a^{1,1}$ & \begin{tabular}{l} 
If A1=1 (likely to be): \\
1-2 digit from F1 and \\
S1 (sometimes their \\
sum); C2; \\
Else: Fuzzy info, \\
including some info \\
about F3 and S3 \\
(sometimes)
\end{tabular} & \begin{tabular}{l} 
If A1=1: Uncertain. \\
F1 and S1 (some- \\
times); 1 digit from \\
F3 and S3 (some- \\
times) \\
Else: F2 and S2
\end{tabular} & & \\
\hline$a^{1,2}$ & \begin{tabular}{l} 
Fuzzy info about F2 \\
and S2 (sometimes)
\end{tabular} & \begin{tabular}{l} 
1-2 digits from F1 \\
and S1 (sometimes \\
fuzzy); 1-2 digits \\
from F2 and S2 \\
(sometimes fuzzy);
\end{tabular} & & \\
\hline$a^{1,3}$ & \begin{tabular}{l}
$1-2$ digits from $\mathrm{F} 1$ \\
and $\mathrm{S} 1$ (sometimes \\
fuzzy)
\end{tabular} & \begin{tabular}{l} 
If A1=1: F1 and S1 \\
(sometimes fuzzy) \\
C2 \\
Else: F2 and S2
\end{tabular} & & \\
\hline$x^{1, \text { mid }}$ & same as $x^{0, \text { mid }}$ & \begin{tabular}{l} 
If $\mathrm{A} 1=1$ : info from \\
$x^{0, \text { mid }}+\mathrm{C} 2$ \\
Else: same as $x^{0, \mathrm{mid}}$
\end{tabular} & \begin{tabular}{l} 
If $\mathrm{A} 1=1:$ same as \\
$x^{0, \text { mid }}$ \\
Else: $\quad$ info from \\
$x^{0, \text { mid }}+\mathrm{F} 3$ and $\mathrm{S} 3$
\end{tabular} & same as $x^{0, \text { mid }}$ \\
\hline$x^{1, \text { post }}$ & same as $x^{0, \text { mid }}$ & \begin{tabular}{l} 
same as $x^{1, \text { mid }}$ except \\
that current input to- \\
ken $A 1$ is less impor- \\
tant
\end{tabular} & \begin{tabular}{l} 
same as $x^{1, \text { mid }}$ except \\
that current input to- \\
ken A2 is less impor- \\
tant
\end{tabular} & \begin{tabular}{l} 
same as $x^{0, \text { mid }}$ except \\
that current input to- \\
ken $A 3$ is blurred
\end{tabular} \\
\hline
\end{tabular}

Table 3: Summary of our observations for each activation site and position. "same as" denotes that there is no obvious difference between the two sites for indicated position.

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-33.jpg?height=721&width=1390&top_left_y=239&top_left_x=365)

Figure 29: The information flow diagrams for predicting the digits in answer. F1 and S1 are aligned, F2 and S2 are aligned, and so forth. Color of the lines represents the information being routed, and alternating color represents a mixture of information. The computation is done from left to right (or simultaneously during training), and from bottom to top in each sub-figure. Note that the figure represents what information we find in activation, rather than the information being used by the model. Also note that the graphs are based on our qualitative examination using InversionView and attention pattern, and are an approximate representation of reality. We keep those stable paths that almost always occur. Inconsistently present paths such as routing the ones place when predicting A1 are not shown.

\section*{F. 5 Causal Intervention: Details and Full Results}

We use similar activation patching method as character counting task, described in D.3. In Figure 29. we split the overall algorithm into individual ones for digits in the answer, under different condition. Each algorithm predict target token based on multiple types of information (digits in hundreds/tens/ones place).

In order to verify the paths responsible for routing each type of information, we construct contrast examples as follows: Given a prediction target (e.g., A1), a set of tokens that can be changed $T_{\text {chg }}$ (corresponds to a certain type information, e.g., F1 and S1), we construct a contrast example $\mathrm{x}_{\text {con }}$ that contains a different token $t_{c o n}$ as prediction target by changing tokens in $T_{c h g}$. Note that the contrast example still follows the rule that the answer is the sum of the two operands.

We now give a detailed explanation for Figure $5 \mathrm{~b}$ shown in the main paper, in which prediction target is A1 and we patch head output corresponding to the preceding token " $=$ ". On the left of Figure $5 \mathrm{~b}, T_{c h g}=\{\mathrm{F} 1, \mathrm{~S} 1\}$. So in the contrast examples the $\mathrm{F} 1$ and $\mathrm{S} 1$ are changed and other digits in operands remains the same. In the third run where we calculate $L D_{p c h}$, activations are replaced by new activations from contrast example, so the new activations contain modified F1 and S1 information. Therefore, for activations that contributes to routing F1 and S1 (e.g., $a_{=}^{0,0}$ ), patching them with new activations can effect model's prediction. On the contrary, patching $a_{=}^{0,3}$ has no effect because it contains information about $\mathrm{F} 2$ and $\mathrm{S} 2$, which are the same in contrast examples. On the right of Figure $5 \mathrm{~b}, T_{c h g}=\{\mathrm{F} 2, \mathrm{~S} 2\}$, so we are verifying the activations that plays a role in routing $\mathrm{F} 2$ and S2. Note that we exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{F} 1+\mathrm{S} 1 \geq 10$, because changing F2 and S2 cannot change A1 in those cases.

Activation patching results for other cases are shown in Figures 30 to 34 .

Overall, among all the intervention experiments and their corresponding information flow diagrams in Figure 29 , the activation with the highest increment not included in the information flow diagram is $a_{A 1}^{1,1}$ in Figure 30 (right), accounting for only $5.92 \%$ of the cumulative increment. In this sense, the

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-34.jpg?height=545&width=1089&top_left_y=253&top_left_x=518)

Figure 30: Causal verification results for the information flow in sub-figure (b) in Figure 29 predicting $\mathrm{A} 2$ when $\mathrm{A} 1=1$. We only consider data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{A} 1=1$. The constructed contrast data $\mathrm{x}_{\text {con }}$ also satisfies this constraint. Left: $T_{\text {chg }}=\{\mathrm{F} 1, \mathrm{~S} 1\}$. Right: $T_{c h g}=\{\mathrm{F} 2, \mathrm{~S} 2\}$. Note that the included data from $\mathrm{x}_{\text {orig }}$ all satisfy $\mathrm{F} 1+\mathrm{S} 1 \geq 10$, because, if $\mathrm{F} 1+\mathrm{S} 1=9$ and $\mathrm{A} 1=1$, no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 29 p.

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-34.jpg?height=545&width=1089&top_left_y=1142&top_left_x=518)

Figure 31: Causal verification results for the information flow in sub-figure (c) in Figure 29 predicting $\mathrm{A} 2$ when $A 1 \neq 1$. We exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{A} 1=1$. The constructed contrast data $\mathrm{x}_{\text {con }}$ also satisfies this constraint. Left: $T_{c h g}=\{\mathrm{F} 2, \mathrm{~S} 2\}$. Right: $T_{c h g}=\{\mathrm{F} 3, \mathrm{~S} 3\}$. We further exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{F} 1+\mathrm{S} 1=9$ and $\mathrm{F} 2+\mathrm{S} 2=9$ because we cannot find a contrast example in those cases. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 29.

information flow diagrams coupled with interpretations present an almost exhaustive characterization of the algorithm used by the model to predict the answer digits.

\section*{G Decoder Likelihood Difference}

In figures of addition task, we also show the decoder likelihood difference for each token in generated samples. It indicates what might be relevant to the activation. It is calculated as follows: During generation, we sample the next token from the distribution produced by the decoder model, and we record the probability of the sampled token in that distribution. We denote it as $p_{a c t}$. Then, we run the decoder again with the same input tokens, but this time it is fed with a "blank" activation (the activation corresponding to BOS token from the same activation site: because of the causal masking of the probed model, this activation does not contain any information). Therefore, it produces a different

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-35.jpg?height=548&width=1093&top_left_y=252&top_left_x=516)

Figure 32: Causal verification results for the information flow in sub-figure (d) in Figure 29 predicting $\mathrm{A} 3$ when $A 1=1$. We exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{A} 1 \neq 1$. The constructed contrast data $\mathrm{x}_{\text {con }}$ also satisfies this constraint. Left: $T_{c h g}=\{\mathrm{F} 2, \mathrm{~S} 2\}$. Right: $T_{c h g}=\{\mathrm{F} 3, \mathrm{~S} 3\}$. We further exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{F} 1+\mathrm{S} 1=9$ and $\mathrm{F} 2+\mathrm{S} 2=9$ because we cannot find a contrast example in those cases.

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-35.jpg?height=524&width=533&top_left_y=1077&top_left_x=796)

Figure 33: Causal verification results for the information flow in sub-figure (e) in Figure 29 predicting $\mathrm{A} 3$ when $A 1 \neq 1 . T_{\text {chg }}=\{\mathrm{F} 3, \mathrm{~S} 3\}$. We exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{A} 1=1$. The constructed contrast data $\mathrm{x}_{\text {con }}$ also satisfies this constraint.

distribution. The probability of the same token (the token already sampled in the normal run) in the new distribution is $p_{\text {blank }}$. The difference $p_{a c t}-p_{\text {blank }}$ indicates if the decoder model can be more confident about a token when it receives the information from the activation. If $p_{\text {act }}-p_{\text {blank }}>0$, the color is blue, and if $p_{\text {act }}-p_{\text {blank }}<0$ it is red. The depth of color is proportional to the magnitude of the value. Therefore, it highlights what can be learned from the query activation, in addition to what is in the context.

Note that the decoder has learned how to handle the activation of BOS, since it also appears in the training set because we sample uniformly at random among all tokens in the input (as in IOI and addition task).

Importantly, unlike the distance metric value, the decoder likelihood difference depends on the capability of the decoder model, and we should bear in mind that it might be inaccurate. We caution that this difference does not necessarily highlight the directly relevant part, complicating its interpretation. As shown in Figure 35, the color highlights digits in the second operand, which does not reflect the actual flow of information. In these examples the query activation corresponds to digits in answer. For example, in the left most sub-figure, $x^{0, \text { pre }}$ contains the information " 6 is at the position of A1", but the color does not highlight A1. This is because, on one hand, decoder predicts S1 conditioned on F1, so knowing their sum will significantly increase the confidence of

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-36.jpg?height=545&width=1093&top_left_y=253&top_left_x=516)

Figure 34: Causal verification results for the information flow in sub-figure (f) in Figure 29 predicting A4/E. Left: $T_{\text {chg }}=\{\mathrm{F} 3, \mathrm{~S} 3\}$. We exclude those data in $\mathrm{x}_{\text {orig }}$ where $\mathrm{A} 1 \neq 1$, since in that case the prediction the target position is almost always E (end of the text). Changing F3 and S3 will not change E. Even when it does, i.e., when $\mathrm{F} 1+\mathrm{S} 1=9$ and $\mathrm{F} 2+\mathrm{S} 2=9$ and $\mathrm{F} 3+\mathrm{S} 3<9$, changing $\mathrm{F} 3$ and $\mathrm{S} 3$ will cause $\mathrm{A} 1$ to change. But we need to keep other variables the same. Based on the same reason, the contrast examples should also satisfy the constraint $\mathrm{A} 1=1$. Right: $T_{c h g}=\{\mathrm{F} 1, \mathrm{~S} 1\}$. We change $\mathrm{F} 1$ and $\mathrm{S} 1$ in order to change A1, thus changing $\mathrm{A} 4$ to $\mathrm{E}$ or vise versa. There is no constraint in this case, since we can always find contrast examples.

```

Activation Site: }\mp@subsup{x}{}{0,pre
Query Input
0.000; В 304+309=(6) 13
Generated Samples
0.000; B 196+434=(6) 30 E
0.000; B 308+309=(6) 17E
0.000; В 308+304=(6) 12E
0.000; В 535+152=(6) 87E
0.000; B 297+375= (6) 72E
0.000; B 325+350= (6) 75E
0.000; B 251+416=(6) 67E
0.000; B 157+531=(6) 8 8E
0.000; В 362+263=(6) 25E
0.603; B 441+105=(5) 46E

```

Activation Site: $x^{0, p r e}$

Query Input

$0.000 ;$ В $931+591=1$ (5) 22

Generated Samples

\begin{tabular}{l}
$0.000 ;$ В $231+222=4$ (5) $3 \mathrm{E}$ \\
$0.000 ;$ В $771+762=1$ (5) $33 \mathrm{E}$ \\
$0.000 ;$ В $843+713=1$ (5) $56 \mathrm{E}$ \\
$0.000 ;$ В $899+696=1$ (5) $95 \mathrm{E}$ \\
$0.000 ;$ В $899+672=1$ (5) $71 \mathrm{E}$ \\
$0.000 ;$ В $297+255=5$ (5) $2 \mathrm{E}$ \\
$0.000 ;$ В $767+826=1$ (5) $93 \mathrm{E}$ \\
$0.000 ;$ В $833+763=1$ (5) $96 \mathrm{E}$ \\
\hline $0.568 ;$ В $278+671=9$ (4) $9 \mathrm{E}$ \\
$0.852 ;$ В 3 (5) $4+821=1175 \mathrm{E}$
\end{tabular}

Activation Site: $x^{0, p r e}$

Query Input

$0.000 ;$ В $982+347=13$ (2) 9

Generated Samples

$$
\begin{aligned}
& 0.000 ; \text { B } 168+374=54 \text { (2) E } \\
& 0.000 ; \text { B } 575+307=88 \text { (2) E } \\
& 0.000 ; \text { B } 298+344=64 \text { (2) E } \\
& 0.000 ; \text { B } 347+879=12 \text { (2) } 6 \mathrm{E} \\
& 0.000 ; \text { B } 712+180=89 \text { (2) E } \\
& 0.000 ; \text { B } 389+523=91 \text { (2) E } \\
& 0.000 ; \text { B } 397+445=84 \text { (2) E } \\
& 0.000 ; \text { B } 782+841=16 \text { (2) } 3 \text { E } \\
& \hline 0.405 ; \text { B } 148+982=11 \text { (3) } 0 E \\
& 0.689 ; \text { B } 549+527=10 \text { (7) } 6 \mathrm{E}
\end{aligned}
$$

Figure 35: The $\epsilon$-preimage of $x^{0, \text { pre }}$. Here the information contained is A1, A2, A3 respectively, while the decoder likelihood difference highlights digits in operands. Because given the first operand and the final sum, the digits in second operand can be inferred.

$\mathrm{S} 1$, and $\mathrm{S} 1$ is highlighted. On the other hand, when predicting $\mathrm{A} 1$, the previous digits can already determine the answer. There is a high confidence even without knowing A1, thus it is not highlighted. In essence, the information contained in a query activation may manifest itself early, and the decoder likelihood difference does not necessarily align with the part of the input from which the information has actually been obtained.

An interesting direction for future work could be developing decoders that generate samples in a permuted order, and generate most confident tokens first, possibly based on architectures like XLNet [42].

![](https://cdn.mathpix.com/cropped/2024_06_04_07e32d21b47ea7e28932g-37.jpg?height=621&width=1158&top_left_y=237&top_left_x=489)

B 556+280 (=) 836

Query Input

0.000; B 5 56+280\Leftrightarrow &36

Generated Samples

Generated Samples

Manual Samples

0.000; В 357+780 ()1137E

0.022; В 284+551\Leftrightarrow835E

- 0.128; B 5 36+280 & 8 8 16E

0.001; В 750+688 | 1438E |0.026; B 682+558\Leftrightarrow1240E

0.008; B 248+986\Leftrightarrow1234E | 0.026; B 782+757\Leftrightarrow1539E

0.009; В 942+985 (=)1927E

0.027; B 871+381 (=) 1252E

0.029; В 579+882\Leftrightarrow1461E

0.029; B 283+969\Leftrightarrow)1252 E

0.030; B 588+670 =) 1258E

0.031; В 981+683\Leftrightarrow1664E

0.031; B 189+589\Leftrightarrow778E

0.034; В 885+687(=)1572E

(a) 0.452 ; B $526+280 \Leftrightarrow 806 \mathrm{E}$ 1.111 ; B $516+280 \Leftrightarrow 796 \mathrm{E}$ $1.717 ;$ B $506+280 \Leftrightarrow 786 \mathrm{E}$

(b)

B $526+280 \Leftrightarrow 806$

B $516+280 \Leftrightarrow 796$

B $506+280$ (=) 786

(c)

Figure 36: For activation site $a^{0,3}$, InversionView reveals how activations can encode information without an attention edge: (a) Even though, on this input, $h^{0,3}$ attends only to one tens place digit, it also encodes the approximate identity (range 4-8) of another tens place digit. It encodes that the sum of tens places is greater than ten. (b) We verify our hypothesis by manually create some samples and calculate $D$. (c) Attention patterns for manually created inputs outside of the $\epsilon$-preimage. The attention pattern differs from that of the query input. In the query input, the attention head infers information about the second tens place digit from the absence of an attention edge.

\section*{H Notes on Attention and Path Patching}

\section*{H. 1 InversionView Reveals Information from Un-attended Tokens}

In Section 4, we mention that attention pattern is not sufficient to form hypothesis when the model has more than one layer. Because unlike in layer 0 each residual stream contains information only about the corresponding token, in higher layers each residual stream contains a mixture of tokens from the context, making it difficult to determine what information is routed by attention. Besides this point, we also find that sometimes attention pattern can be misleading even in layer 0 .

InversionView reveals how components can know more than what they attend to. At the top of Figure 36, we show the attention weights of head $h^{0,3}$. Here, " $=$ " attends almost solely to S2, so the head output $a^{0,3}$ should only contain information that there is an " 8 " in tens place. The generated $\epsilon$-preimage, however, shows that it contains information about F2: The number in tens place other than " 8 " is always in a certain range $(\geq 4)$, resulting in a carry to the hundreds place. To verify this, we manually constructed examples (rightmost column in Figure 36) where the other number is outside of the range, and found that, for these, the activation distance is indeed very large, confirming the information suggested by the decoder. In layer 0 , how does the model obtain information about a token without attending it? At the bottom of Figure 36, we show the attention weights of those manually inserted examples. So the answer is: a different attention pattern would arise if F2 is not in that range. Information can be passed not only by values, but also by queries and keys. InversionView successfully shows this hidden information, even without comparing across different examples.

\section*{H. 2 Additional Discussion about Path Patching}

Besides our argument in Section 4, another important aspect of circuit discovery methods is that, in many tasks (including our character counting task), the computational nodes do not correspond to fixed positions, and directly applying path patching is problematic. It's not really clear how to apply path patching when varying input positions matter, as the literature on circuit discovery defines circuits in terms of components, not in terms of input positions. In the case of Character Counting Task, such an interpretation would just define a circuit linking the embeddings, attention heads, and MLPs, without capturing the role of different positions, and the fact that characters from varying positions feed into the computation. Such a view would not provide any nontrivial information about the mechanics of the implemented algorithm. This reflects a more general conceptual challenge of
circuit discovery: When different input positions are relevant on different inputs, as in the Character Counting Task, one could either define a single circuit across inputs in which every input position is connected to a single node that performs a potentially complex computation, or define per-input circuits where the wiring is input-dependent; however, per-sample path patching is not very scalable, and resulting per-input circuits would require further interpretation to understand how they are formed across inputs.

\section*{I Automated Interpretability}

We further explore whether the process of obtaining the common information from a collection of inputs can be automated by LLMs. We use Claude $3^{4}$ [3] In preliminary experiments we also try GPT-4 [1] but we find Claude 3 works better in our case. In the prompt given to Claude 3, we first describe the task it needs to perform, the terminology we are using (e.g., F1, F2, etc.), the rules (e.g., the pattern it finds should be applicable to each inputs in the $\epsilon$-preimage), input and output form, and the crucial steps it should follow. In addition, we also provide it with 3 demonstrating examples in conjunction with the correct answers. Each example corresponds to a specific activation site and token position (e.g., $a^{0,0}, \mathrm{~A} 1$ ). In each example, there are 2-3 specific query inputs, each query input is accompanied with 20 (sometimes less) samples that are inside the $\epsilon$-preimage. Claude 3 needs to find the pattern for each query input, and summarize its findings across several query inputs, which is the information contained in general in that activation. In addition to the content described above (the common part shared between prompts), we give it a questioning example, which is the content we would like it to interpret. The questioning example shares the same form as the demonstrating example, except that it contains 5 query inputs and their corresponding samples. In addition, when two separate interpretations are needed based on different $\mathrm{A} 1$ value, we run the generation twice with examples of different A1 value, instead of giving the model a mixture of two cases and resorting to its own capacity.

We think the following findings from our experiments are worth mentioning: 1) It's hard for the model to align digits of the same place (e.g., comparing all F1 digits), because the samples are presented as a single flattened string instead of a 2-dimensional table. We find that explicitly adding the variable name can largely mitigate this problem, they may serve as certain kind of keys. For example, "7(F1) 1 (F2) 1 (F3) +9 (S1) $9(\mathrm{~S} 2)$ 4(S3)". 2) The generated interpretation is sometimes not consistent. The model may generate different conclusion even with the same prompt, but this usually only happens to less important information. 3) The model does not strictly follow the rule, i.e., the common pattern should match all inputs, even though we state this repeatedly in different ways in the prompt. The model will say "always" even when there is a counterexample. We should keep 2) and 3) in mind when reading the results.

We run the generation for each entry in Table 3 once, using the samples generated from the corresponding activation. The results are shown in Table 4, accompanied with human interpretation for comparison. The interpretation given by Claude 3 reflects the main information in almost all cases. Even when the information becomes more complex in layer 1, the interpretation quality does not significantly decline. This implies that automated interpretation by LLM is promising. On the other hand, we can also see there are some problems: 1) Some of the model's claims are spurious, these claims are usually ranked low by the model, indicating they are not very obvious. 2) The model sometimes does not explain in a desirable manner. For example, for the entry " $x^{0, \text { mid }}, \mathrm{A} 3$ ", the information includes $\mathrm{A} 2$ and whether $\mathrm{A} 1=1$, which means when $\mathrm{A} 1=1$, the sum of $\mathrm{F} 1$ and $\mathrm{S} 1$ is known. Thus Claude 3 concludes that (F1, S1) is approximately represented. Though this is somewhat true, it is not a fundamental piece of information, and there is a more concise summary. 3) The interpretation tends to be verbose, even though we ask the model to be succinct and provide it with some examples. Despite the problems, we think the automatically generated interpretation is by and large satisfactory and informative, and we think some of the problems can probably be solved or mitigated by engineering better prompts.
\footnotetext{
${ }^{4}$ Version: claude-3-opus-20240229
}

\begin{tabular}{|c|c|c|}
\hline \multicolumn{3}{|r|}{ Start of Table 4} \\
\hline \begin{tabular}{l} 
Entry in \\
Table 3
\end{tabular} & \begin{tabular}{l} 
Interpretation \\
from Table 3
\end{tabular} & Interpretation by Claude 3 \\
\hline$a^{0,0}, \mathrm{~A} 1$ & \begin{tabular}{l} 
1-2 digits from \\
F1 and S1
\end{tabular} & *used as example in the prompt \\
\hline$a^{0,1}, \mathrm{~A} 1$ & \begin{tabular}{l} 
1-2 digits from \\
F1 and S1
\end{tabular} & \begin{tabular}{l} 
F1 and S1 (sometimes exactly and sometimes approximately. Their order \\
can sometimes vary)
\end{tabular} \\
\hline$a^{0,2}, A 1$ & \begin{tabular}{l} 
1-2 digits from \\
F1 and S1
\end{tabular} & \begin{tabular}{l} 
the digits in the hundreds place (F1 and S1): - They tend to be large single \\
digits (7,8,9) that add up to 10 or more - Sometimes one or both are fixed \\
to a certain single digit value - Sometimes their values can swap positions \\
between F1 and S1 There are no strong consistent patterns observed for \\
the digits in the tens or ones places (F2, S2, F3, S3). No other types of \\
patterns (e.g. on A1 or carries) are consistently observed.
\end{tabular} \\
\hline$a^{0,3}, \mathrm{~A} 1$ & \begin{tabular}{l} 
1-2 digits from \\
F2 and S2; C2 \\
\end{tabular} & \begin{tabular}{l} 
F2 and S2 (fuzzy, interchangeable); F3 (sometimes fuzzy); S2 (sometimes \\
fuzzy)
\end{tabular} \\
\hline$x^{0, \text { mid }}, \mathrm{A} 1$ & $\mathrm{~F} 1$ and $\mathrm{S} 1 ; \mathrm{C} 2$ & \begin{tabular}{l} 
1) F1 and S1 are either the same digit, or a pair of digits that always \\
appear together in the hundreds place (sometimes with order swapped). \\
2) Whether F2+S2 produces a carry (the value of C2) is often consistent \\
in each example, either always 0 or always 1. 3) There is no consistent \\
pattern found for the ones digits F3 and S3.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{0, \text { post }}$ \\
$\mathrm{A} 1$
\end{tabular} & $\mathrm{~F} 1$ and $\mathrm{S} 1 ; \mathrm{C} 2$ & \begin{tabular}{l} 
1) F1 and S1, either exactly or an approximate version, with order some- \\
times varying 2) S2, usually 3) C2, occasionally F3, S3 and other parts \\
of the addition rarely contain any mutual information between the query \\
and generated strings.
\end{tabular} \\
\hline$a^{1,0}, \mathrm{~A} 1$ & \begin{tabular}{l} 
Fuzzy info about \\
F1, S1 and C2; \\
Fuzzy info about \\
F3 and S3 (some- \\
times)
\end{tabular} & \begin{tabular}{l} 
1) F1 and S1 digits (sometimes exact match, sometimes approximate \\
match). 2) Whether the sum of F2 and S2 is greater than or equal to 10 \\
(i.e. the value of C2). 3) F3 and/or S3 digits are often preserved from the \\
query to the generated strings.
\end{tabular} \\
\hline$a^{1,1}, \mathrm{~A} 1$ & \begin{tabular}{l} 
If A1=1 (likely \\
to be): 1-2 digit \\
from F1 and S1 \\
(sometimes their \\
sum); C2; \\
Else: Fuzzy info, \\
including some \\
info about F3 \\
and S3 (some- \\
times)
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1 . F1 and S1 digits from the query string appear in the generated \\
strings, but their order may be swapped. 2 . Occasionally some other \\
information is shared, such as the carry digit C2, or a specific digit like \\
F3, but this is not consistent across all examples. In summary, the key \\
common information is the two digits that appear in the hundreds place \\
of the query and generated strings, while their specific positions as F1 or \\
S1 may vary. \\
Else: The amount of information shared between the query and generated \\
strings varies by example. In some cases, there are clear patterns like \\
certain digits being consistently shared (e.g. S1, S3). Other digits may \\
share approximate rather than exact values (e.g. F1). Some digits tend to \\
vary more (e.g. F2, S2). Occasionally, the carry C2 is also shared. When \\
there are too few generated strings, it is hard to conclusively identify \\
patterns. More generated strings allow more confidence in the patterns \\
found.
\end{tabular} \\
\hline$a^{1,2}, \mathrm{~A} 1$ & \begin{tabular}{l} 
Fuzzy info about \\
F2 and S2 (some- \\
times)
\end{tabular} & \begin{tabular}{l} 
1. F1 and S1 (sometimes fuzzy representation, their order can vary) 2. \\
F2 and/or S2 (sometimes only one of them, sometimes partially match) 3 . \\
Occasionally F3 and/or S3 (partially match, their order can vary)
\end{tabular} \\
\hline$a^{1,3}, \mathrm{~A} 1$ & \begin{tabular}{l} 
1-2 digits from \\
F1 and S1 (some- \\
times fuzzy)
\end{tabular} & \begin{tabular}{l} 
1) F1 and S1 values, either exactly or approximately. Their order can \\
sometimes be swapped. 2) To a lesser extent, F2 and S2 values, either \\
exactly or approximately. Patterns for F3/S3 are rarely seen. Other \\
aspects like C2 are occasionally but not consistently shared. The level \\
of correspondence varies a lot between examples. Examples with many \\
missing values are hard to find patterns in.
\end{tabular} \\
\hline$x^{1, \mathrm{mid}}, \mathrm{A} 1$ & F1 and S1; C2 & \begin{tabular}{l} 
1) The values of F1 and S1, either the exact values or their sum 2) The \\
value or presence/absence of S2 3) The relationship between F2 and S2 \\
(which is greater, or whether there is a carry) 4) Which digits are missing \\
in the result (A1 or A2)
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{1, \text { post }}$ \\
$\mathrm{A} 1$
\end{tabular} & $\mathrm{~F} 1$ and $\mathrm{S} 1 ; \mathrm{C} 2$ & \begin{tabular}{l} 
the F1 and S1 values from the query can be found in either order (F1, \\
S1) or (S1, F1) in all generated strings for that query. In some cases, C2 \\
(carry from tens place) is also consistent between query and generated \\
strings, being either 0 or 1 in all strings for that query. F3 and S3 values \\
do not seem to have any limitations or correspondence between query \\
and generated strings.
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|}
\hline \multicolumn{3}{|r|}{ Continuation of Table 4} \\
\hline \begin{tabular}{l} 
Entry in \\
Table 3
\end{tabular} & \begin{tabular}{l} 
Interpretation \\
from Table 3
\end{tabular} & Interpretation by Claude 3 \\
\hline$a^{0,0}, \mathrm{~A} 2$ & whether $\mathrm{A} 1=1$ & \begin{tabular}{l} 
- Sometimes A1 is consistent (always the same value) between query and \\
generated strings - Sometimes the exact or approximate F3 and S3 values \\
from the query appear together frequently in the generated strings - In \\
some examples, there is no clear common information The values of (F1, \\
S1) and (F2, S2) do not seem to be consistently shared between query \\
and generated strings in these examples.
\end{tabular} \\
\hline$a^{0,1}, \mathrm{~A} 2$ & \begin{tabular}{l} 
If A1=1: Almost \\
no info \\
Else: $1-2$ digits \\
from F2 and S2
\end{tabular} & \begin{tabular}{l} 
If A1=1: F2 (sometimes fuzzy); A1 \\
Else: 1) The exact values or range of values for F2 and/or S2 2) The \\
value of C2 (whether F2+S2 10) 3) Occasionally F1/S1 or F3/S3, but \\
not as common as F2/S2 4) A1 is usually not shared The most commonly \\
shared information is the exact values or ranges for F2 and S2, as well as \\
the resulting C2. F1/S1 and F3/S3 are less commonly shared.
\end{tabular} \\
\hline$a^{0,2}, \mathrm{~A} 2$ & \begin{tabular}{l} 
If $\mathrm{A}=1$ : both $\mathrm{F} 1$ \\
and $\mathrm{S} 1$ \\
Else: $1-2$ digits \\
from $\mathrm{F} 2$ and $\mathrm{S} 2$
\end{tabular} & \begin{tabular}{l} 
If A=1: 1. F1 and S1 from the query string always appear in generated \\
strings in the hundreds place, but their order may be swapped. 2. A1 is \\
always the same in query and generated strings. \\
Else: 1) F2 and S2 (sometimes S2 is fuzzy) 2) Number of digits in the \\
final sum (related to whether A1 is shown in query) Less commonly, F1 \\
and S1 are shared in a fuzzy way.
\end{tabular} \\
\hline$a^{0,3}, \mathrm{~A} 2$ & \begin{tabular}{l} 
If $A=1: 1-2$ dig- \\
its from F1 and \\
S1 \\
Else: $1-2$ digits \\
from F3 and S3; \\
C3
\end{tabular} & \begin{tabular}{l} 
If A=1: 1) F1 and S1 are often the same pair of digits (sometimes with a \\
bit of variation). The order of the two digits may vary. 2) A1 is always \\
the same digit. 3) In some cases, the presence or absence of a carry from \\
the tens place (C2) is consistent. The digits in the ones place (F3 and S3) \\
are usually not shared between the query and generated strings. \\
Else: 1) F3 and/or S3 often have specific shared values between the query \\
and generated strings. The shared digit is usually in the range 2-9. 2) \\
Occasionally other patterns exist in generated strings (e.g. ones place of \\
answer) but are not truly shared with the query. 3) Besides the ones place \\
operands, no other consistent patterns were found across F1/S1, F2/S2 or \\
the answer digits.
\end{tabular} \\
\hline$x^{0, \text { mid }}, \mathrm{A} 2$ & \begin{tabular}{l} 
A1 \\
If A1=1: F1 and \\
S1 \\
Else: F2 and S2; \\
C3
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1) F1 and S1 have fixed values in each example, although the \\
values differ between examples. Their order can vary. 2) A1 is always \\
"1" in all examples. 3) In some examples, there is a consistent carry or \\
lack of carry from the sum of F2 and S2 (C2). But this is not true for all \\
examples. 4) There are no consistent patterns in the values of F2, S2, F3, \\
S3 across all examples. \\
Else: A1 is always exactly shared, the middle digits F2 and S2 are often \\
approximately shared, and usually one of the first digits (F1/S1) and \\
one of the last digits (F3/S3) from the query appears frequently in the \\
generated strings.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{0, \text { post }}$ \\
A2
\end{tabular} & \begin{tabular}{l} 
A1 \\
If A1=1: F1 and \\
S1 \\
Else: F2 and S2; \\
C3
\end{tabular} & \begin{tabular}{l} 
If A1=1: - F1 and S1 always match, although their order can be swapped. \\
In some cases, the carry C2 (whether F2+S2 is $\geq 10$ ) is also shared \\
between the query and generated strings. \\
Else: 1) The digits in the tens place (F2, S2), either the exact digits or \\
their sum (carry C2). 2) The ones digit of the answer (A1). 3) Sometimes \\
the digits in the ones place (F3, S3), either the exact digits or their sum \\
(carry C3).
\end{tabular} \\
\hline$a^{1,0}, \mathrm{~A} 2$ & \begin{tabular}{l} 
If $\mathrm{A} 1=1: \quad \mathrm{F} 1$ \\
and $\mathrm{S} 1$ (some- \\
times fuzzy); C2 \\
(sometimes) \\
Else: $1-2$ digits \\
from $\mathrm{F} 2$ and $\mathrm{S} 2$;
\end{tabular} & \begin{tabular}{l} 
If A1=1: * used as example in the prompt \\
Else: - F2 and S2 often have fixed values that are shared between the \\
query and generated strings - A1 and A2 (when present) are consistently \\
single digits in both query and generated strings
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|}
\hline \multicolumn{3}{|r|}{ Continuation of Table 4} \\
\hline \begin{tabular}{l} 
Entry in \\
Table 3
\end{tabular} & \begin{tabular}{l} 
Interpretation \\
from Table 3
\end{tabular} & Interpretation by Claude 3 \\
\hline$a^{1,1}, \mathrm{~A} 2$ & \begin{tabular}{l} 
If A1=1: Un- \\
certain. F1 and \\
S1 (sometimes); \\
1 digit from F3 \\
and S3 (some- \\
times) \\
Else: F2 and S2
\end{tabular} & \begin{tabular}{l} 
If A1=1: - Some or all digits from the hundreds place (F1 and S1), \\
sometimes exactly, sometimes approximately - The carry to the hundreds \\
place (C2), in some examples - The first digit of the answer (A1) The \\
digits in the tens and ones place (F2, S2, F3, S3) usually vary and are \\
not consistently shared. When no generated strings are provided, the \\
common information is uncertain. \\
Else: - F2 and S2 are often fixed to specific values in the query and \\
generated strings of each example. - The sum of F2 and S2 determines the \\
value of C2, which is consistent between the query and generated strings \\
within each example. - A1 is usually a single digit but can sometimes \\
expand to 2 digits if the sum reaches or exceeds 100 in the generated \\
strings. The specific digits of A1 are not always shared.
\end{tabular} \\
\hline$a^{1,2}, \mathrm{~A} 2$ & \begin{tabular}{l}
$1-2$ digits from \\
F1 and S1 (some- \\
times fuzzy); 1- \\
2 digits from F2 \\
and S2 (some- \\
times fuzzy); \\
\end{tabular} & \begin{tabular}{l} 
- F1 and S1 often approximately match between the query and generated \\
strings, but their exact values and order can vary. - In some cases, F2 and \\
S2 also approximately match, but less frequently and consistently than F1 \\
and S1. - A1 always exactly matches between the query and generated \\
strings.
\end{tabular} \\
\hline$a^{1,3}, \mathrm{~A} 2$ & \begin{tabular}{l} 
If $\mathrm{A} 1=1: \quad \mathrm{F} 1$ \\
and $\mathrm{S} 1$ (some- \\
times fuzzy); C2 \\
Else: $\mathrm{F} 2$ and $\mathrm{S} 2$
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1) F1 and S1, their order can vary 2) Sometimes F2 and S2, \\
either the exact values or approximate range of values 3) A1 is often the \\
same F3 and S3 usually don't exhibit obvious patterns. The amount of \\
shared information varies in different examples, with some having more \\
(e.g. exact F2 and S2 values) while others have less. \\
Else: 1) One or both digits in the tens place (F2 and/or S2) 2) One or \\
more digits in the answer (A1 and/or A2) Less frequently, there are also \\
patterns shared in the ones place (F3 and S3). The hundreds place (F1 \\
and S1) least often contain common patterns.
\end{tabular} \\
\hline$x^{1, \text { mid }}, \mathrm{A} 2$ & \begin{tabular}{l} 
A1 \\
If $\mathrm{A} 1=1: \mathrm{F} 1$ and \\
S1; C2 \\
Else: F2 and S2; \\
C3
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1. F1 and S1 are always the same two digits but their order can \\
vary. 2. The sum of F2 and S2 is always the same, implying the carry C2 \\
is consistent. 3. A1 is always the same digit "1". There is no consistent \\
pattern found for F3 and S3. \\
Else: 1. F1 and S1: their specific values in query often show up in \\
generated strings in an approximate way, and their order can sometimes \\
vary. 2. F2 and S2: the specific values or a small range of values are \\
often shared. 3. F3 and S3: a fuzzy version is often shared, meaning the \\
specific values may vary but are close to those in query. 4. A1: often \\
exactly the same between query and generated strings.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{1, \text { post }}$ \\
A2
\end{tabular} & \begin{tabular}{l} 
If $\mathrm{A} 1=1: \mathrm{F} 1$ and \\
S1; C2 \\
Else: F2 and S2; \\
C3
\end{tabular} & \begin{tabular}{l} 
If A1=1: - F1 and S1 have two possible values that are swapped between \\
F1 and S1. In other words, (F1, S1) have two possible combinations that \\
are the reverse of each other. - A1 is always the same in the query and \\
all generated strings in each example. Sometimes the carry C2 (whether \\
F2+S2 210 ) is also shared between query and generated strings. \\
Else: 1) Specific digits are often fixed or highly common in certain places \\
(e.g. F2, S2) across query and equations. 2) The answer digit A1 is often \\
the same or highly consistent across query and equations. 3) The carry \\
C2 from tens place is sometimes consistent (0 or 1) across examples. 4) \\
Other than above, the digits and relationships seem to vary substantially \\
between examples.
\end{tabular} \\
\hline$a^{0,0}, \mathrm{~A} 3$ & whether $\mathrm{A} 1=1$ & \begin{tabular}{l} 
1. Digits in the answer part (A1, A2, etc.), either the exact digit or a \\
range. This is observed in all 5 examples. 2 . The range that F1 and S1 \\
can take, and sometimes they can swap positions. This is observed in \\
2 out of 5 examples. Digits in F2, S2, F3, S3 do not show consistent \\
patterns across examples.
\end{tabular} \\
\hline$a^{0,1}, \mathrm{~A} 3$ & \begin{tabular}{l} 
1-2 digits from \\
F2 and S2
\end{tabular} & \begin{tabular}{l} 
- When the sum is a 2-digit number, A1 is sometimes shared between \\
query and generated strings - When the sum is a 3-digit number, A2 is \\
often (but not always) shared - If there is a shared A2, its value often \\
matches the query - If both A1 and A2 are present, A1A2 is between \\
10-19 There are no consistent patterns in the operand digits F1/S1, F2/S2, \\
F3/S3 or their sums.
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|}
\hline \multicolumn{3}{|r|}{ Continuation of Table 4} \\
\hline \begin{tabular}{l} 
Entry in \\
Table 3
\end{tabular} & \begin{tabular}{l} 
Interpretation \\
from Table 3
\end{tabular} & Interpretation by Claude 3 \\
\hline$a^{0,2}, \mathrm{~A} 3$ & \begin{tabular}{l}
$1-2$ digits from \\
F2 and S2
\end{tabular} & \begin{tabular}{l} 
1) S2 or F2 sometimes takes on a consistent value (like $0,9,7$ ) in the \\
query and many generated strings. 2) The equations are often incomplete, \\
with A1 and/or A2 missing in many generated strings. 3) When present. \\
A2 sometimes has a consistent value (like $1,2,6)$ in the query and \\
generated strings, but not always. 4) No consistent patterns were found \\
for the (F1, S1) and (F3, S3) digit pairs.
\end{tabular} \\
\hline$a^{0,3}, \mathrm{~A} 3$ & \begin{tabular}{l}
1 digit from F3 \\
and S3 (2 when \\
F3=S3); C3
\end{tabular} & \begin{tabular}{l} 
1) Certain digits from F3 and/or S3, although the order of F3 and S3 may \\
vary in generated strings compared to the query. 2) The existence of A1 \\
and A2 in complete equations, but not always. Other patterns like digits \\
from F1, S1, F2, S2 are rarely shared.
\end{tabular} \\
\hline$x^{0, \text { mid }}, \mathrm{A} 3$ & \begin{tabular}{l} 
A2; F2 and S2; \\
C3; whether \\
A1=1
\end{tabular} & \begin{tabular}{l} 
1. An exact or approximate representation of (F1, S1), sometimes with \\
order varying 2. Specific values or range of values for F2 and S2, some- \\
times along with their sum (C2) 3. Sometimes specific values for F3 \\
and/or S3 4. Often an exact or approximate representation of (A1, A2) \\
The amount and type of information shared varies between examples \\
But in general, there are usually some constraints on the digits in each \\
place value shared by the query and generated strings.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{0, \text { post }}$ \\
A3
\end{tabular} & \begin{tabular}{l} 
A2; F2 and S2; \\
C3; whether \\
A1=1
\end{tabular} & \begin{tabular}{l} 
1. F1 and S1: exact match or fuzzy match, order of F1 and S1 may vary \\
2. F2 and S2: exact match or fuzzy match, order of F2 and S2 may vary. \\
sometimes have size relationship (like F2 $\leq$ S2) 3. F3: exact match or \\
fuzzy match 4. S3: exact match or fuzzy match 5. A1: exact match 6. A2 \\
exact match Not every example has all the above common information.
\end{tabular} \\
\hline$a^{1,0}, \mathrm{~A} 3$ & \begin{tabular}{l} 
If A1=1: $1-2$ dig- \\
its from F2 and \\
S2; C3 (some- \\
times); \\
Else: 1-2 digits \\
from F3 and S3
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1. F2 and/or S2 (sometimes their roles as F2 or S2 can vary) \\
A pattern about A1 and A2: when A1 is a specific value (usually “1"), \\
A2 is provided; otherwise A2 is not provided 3. Sometimes A1 4. Rarely \\
A2 when A1 is a specific value \\
Else: - Fuzzy/approximate matching of F3 and S3 values - Exact match-h \\
ing of F3, S2, S3 values in some cases - Partial matching of A2 (last digit) \\
in some cases Overall, ones place digits (F3, S3) and occasionally other \\
digits (S2, A2) tend to be shared in a fuzzy or exact manner. Hundreds \\
place digits (F1, S1, A1) are rarely shared.
\end{tabular} \\
\hline$a^{1,1}, \mathrm{~A} 3$ & \begin{tabular}{l} 
If A1=1: $1-2$ dig- \\
its from F2 and \\
S2; C3 (some- \\
times); \\
Else: $1-2$ digits \\
from F3 and S3
\end{tabular} & \begin{tabular}{l} 
If A1=1: F2 and S2 are always certain fixed digits in each example, \\
shared by the query and all generated strings in that example. Sometimes \\
the first answer digit A1 is also shared between query and some generated \\
strings, but this is not fully consistent. The last answer digit shows no \\
consistency between query and generated strings. \\
Else: 1) S3 (sometimes F3) 2) Fuzzy representation of (A1, A2) in some \\
cases The ones place digit (F3 or S3) tends to be exactly preserved, \\
while the answer (A1, A2) is sometimes preserved approximately but not \\
exactly. Other digits and the carry do not show consistent patterns.
\end{tabular} \\
\hline$a^{1,2}, \mathrm{~A} 3$ & \begin{tabular}{l} 
If A1=1: $1-2$ dig- \\
its from F2 and \\
S2; C3 (some- \\
times); 1-2 digits \\
from F1 and S1 \\
(fuzzy); \\
Else: 1-2 digits \\
from F3 and S3
\end{tabular} & \begin{tabular}{l} 
If A1 1: 1. F1 and S1 (order may vary, values may be fuzzy or exact \\
match) 2. F2 (may be exact or fuzzy match) 3. S2 (may be exact or fuzzy \\
match, may appear in F2 or S2) 4. A1 (always exact match) 5 . A2 (may \\
be exact match or not present) The amount of shared information varies \\
across examples. F3, S3 are not shared. \\
Else: Across the examples, the most common pattern is that either F3 or \\
S3 has a fixed single digit value shared by the query and all generated \\
strings in that example. The specific digit varies between examples but is \\
consistent within each example.
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|}
\hline \multicolumn{3}{|r|}{ Continuation of Table 4} \\
\hline \begin{tabular}{l} 
Entry in \\
Table 3
\end{tabular} & \begin{tabular}{l} 
Interpretation \\
from Table 3
\end{tabular} & Interpretation by Claude 3 \\
\hline$a^{1,3}, \mathrm{~A} 3$ & \begin{tabular}{l} 
If A1=1: $1-2$ dig- \\
its from F2 and \\
S2; C3 (some- \\
times); \\
Else: 1-2 digits \\
from F3 and S3
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1. (F1, S1), but usually in an approximate or fuzzy way. Their \\
exact values and order may vary in generated strings. 2. (F2, S2), usually \\
exactly the same as in query string. In a few cases, they are shared in an \\
approximate way. 3. A1, sometimes shared between query and generated \\
strings. 4. S3, occasionally shared if it is present. The amount and type \\
of shared information seem to vary from example to example. Some only \\
share (F2, S2), while others share (F1, S1) and A1 as well, although in \\
an approximate manner. \\
Else: - The last digit of F (F3) is often shared between query and gen- \\
erated strings - The last digit of the answer (A2 for 2-digit answer, A3 \\
for 3-digit answer) is very frequently shared - Sometimes S3 or other \\
answer digits are also shared, but less reliably In general, the ones place \\
digits of the operands and answer tend to be shared between the query \\
and generated strings in each example.
\end{tabular} \\
\hline$x^{1, \text { mid }}, \mathrm{A} 3$ & \begin{tabular}{l} 
If A1=1: A2; \\
F2 and S2; C3; \\
whether A1=1 \\
Else: A2; F2 and \\
S2; F3 and S3; \\
whether A1=1 \\
\end{tabular} & \begin{tabular}{l} 
If A1=1: F1 and S1 (sometimes fuzzy values or ranges); F2 and S2 \\
(sometimes unordered); F3 (rarely); A1; A2 \\
Else: F3 and S3, sometimes only appearing in an approximate manner \\
and sometimes their relative order can vary; A2, always precisely shared \\
between the query string and generated strings.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{1, \text { post }}$ \\
$\mathrm{A} 3$
\end{tabular} & \begin{tabular}{l} 
If A1=1: A2; \\
F2 and S2; C3; \\
whether A1=1 \\
Else: A2; F2 and \\
S2; F3 and S3; \\
whether A1=1
\end{tabular} & \begin{tabular}{l} 
If A1=1: 1. Approximate or exact values of F1 and S1, their order can \\
vary 2. Exact values or a couple possible values of F2 and S2, their order \\
can vary 3. Exact or approximate values of A1 and A2 The information \\
about F3 and S3 does not seem to be shared. \\
Else: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be \\
swapped) - The last digit of the answer (A2) - Whether there is a carry \\
from tens place to hundreds place (C2)
\end{tabular} \\
\hline$a^{0,0}, \mathrm{~A} 4 / \mathrm{E}$ & whether $\mathrm{A} 1=1$ & \begin{tabular}{l} 
1) The value of A1 (in 4 out of 5 examples) 2) An approximate or exact \\
match of (F1, S1) pair (in 2 out of 5 examples) 3) The sum of F3 and \\
S3 (in 1 out of 5 examples) Other occasional commonalities include the \\
value of A2 and whether A3 is complete.
\end{tabular} \\
\hline$a^{0,1}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l} 
1-2 digits from \\
F3 and S3
\end{tabular} & \begin{tabular}{l} 
Across the examples, the most consistent commonality is in the ones \\
place (F3 and S3). They often have the same or very similar values in the \\
query and generated strings. \\
Sometimes a digit in a certain place of the final answer is also shared \\
between query and many/all generated strings, like the first digit (A1) or \\
last digit (A3). But this is not always the case. \\
The hundreds place (F1/S1) and tens place (F2/S2) digits usually vary \\
without a clear mutual pattern between the query and generated strings.
\end{tabular} \\
\hline$a^{0,2}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l}
$1-2$ digits from \\
F3 and S3
\end{tabular} & \begin{tabular}{l} 
1) F3 and S3 digits (sometimes approximately). Present in 4 out of 5 \\
examples. 2) A1 and/or A2 digits (only in example 0 ).
\end{tabular} \\
\hline$a^{0,3}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l}
$1-2$ digits from \\
F3 and S3
\end{tabular} & \begin{tabular}{l} 
- Certain digit positions (often F3 or S3) having the same value in the \\
query and all/most generated strings - When sums are shown, certain \\
digits (often A1 or A3) being restricted to a small set of values or range \\
The specific digit positions and values/ranges vary between examples, \\
but the general pattern of certain positions being fixed or constrained is \\
consistent.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{0, \mathrm{mid}}$ \\
$\mathrm{A} 4 / \mathrm{E}$
\end{tabular} & \begin{tabular}{l} 
A3; F3 and S3; \\
whether A1=1
\end{tabular} & \begin{tabular}{l} 
- F1 and S1 sometimes appear in generated strings with their order \\
swapped - F3 and S3 sometimes appear in generated strings with their \\
order swapped - F3, S3 and A3 of the query string often appear unchanged \\
in the same position in generated strings
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{0, \text { post }}$ \\
$\mathrm{A} 4 / \mathrm{E}$
\end{tabular} & \begin{tabular}{l} 
A3; F3 and S3; \\
whether A1=1
\end{tabular} & \begin{tabular}{l} 
- F3 is often shared between the query string and generated strings - S3 is \\
sometimes shared between the query string and generated strings - A3 is \\
consistently the same value in the query string and all generated strings \\
for each example There is no strong pattern for F1, S1, F2 or S2.
\end{tabular} \\
\hline$a^{1,0}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l}
$1-2$ digits from \\
F3 and S3; \\
whether next \\
token is E
\end{tabular} & \begin{tabular}{l} 
1. Specific digits or narrow ranges in F3 and S3 2. The value or a narrow \\
range of A1 3. The ending digits or a narrow range of ending digits in \\
A3 4. Sometimes the carry C1 or C2 The digits in F1, S1, F2, S2 and the \\
full value of A2, A3 are usually not shared.
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|}
\hline \multicolumn{3}{|r|}{ Continuation of Table 4} \\
\hline \begin{tabular}{l} 
Entry in \\
Table 3
\end{tabular} & \begin{tabular}{l} 
Interpretation \\
from Table 3
\end{tabular} & Interpretation by Claude 3 \\
\hline$a^{1,1}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l}
$1-2$ digits from \\
F3 and S3; \\
whether next \\
token is E
\end{tabular} & \begin{tabular}{l} 
1) F3 (always the same or frequently appears) 2) S3 (always the same or \\
frequently appears) 3) One or more answer digits, especially the last one \\
A3 (always the same or frequently appears)
\end{tabular} \\
\hline$a^{1,2}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l} 
1-2 digits from \\
F3 and S3
\end{tabular} & \begin{tabular}{l} 
The ones digit (F3) of the query string consistently shows up in the ones \\
place (either F3 or S3) of the generated strings. Sometimes the other \\
ones digit in generated strings has a specific value when query F3 is in a \\
certain place. Occasionally, the carry (C3) from the ones place addition \\
is also shared between the query and generated strings.
\end{tabular} \\
\hline$a^{1,3}, \mathrm{~A} 4 / \mathrm{E}$ & \begin{tabular}{l}
$1-2$ digits from \\
F3 and S3
\end{tabular} & \begin{tabular}{l} 
only the ones digits (F3 and sometimes S3) are consistently shared, while \\
the other parts of the addition problems vary between the query and \\
generated strings in each example.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{1, \mathrm{mid}}$ \\
$\mathrm{A} 4 / \mathrm{E}$
\end{tabular} & \begin{tabular}{l} 
A3; F3 and S3; \\
whether A1=1
\end{tabular} & \begin{tabular}{l} 
- F3 and S3 have some fixed values (varying by example) that always \\
sum to the same total. The order of the two digits doesn't matter. - As a \\
result, A3 is always a fixed value for each example. - There is sometimes \\
a consistent carry over amount from the tens to hundreds place, resulting \\
in a fixed A1 value.
\end{tabular} \\
\hline \begin{tabular}{l}
$x^{1, \text { post }}$ \\
$\mathrm{A} 4 / \mathrm{E}$
\end{tabular} & \begin{tabular}{l} 
F3 and S3; \\
whether A1=1
\end{tabular} & \begin{tabular}{l} 
- Specific digits in the answer (A3) - Specific digits in the operands (F3, \\
S3) - Whether there is a carry in a certain place (C2, C3) The shared \\
information varies across examples, but usually relates to the ones or tens \\
place digits and carries.
\end{tabular} \\
\hline
\end{tabular}

Table 4: Interpretation for 3 digit addition produced by Claude 3, compared with human interpretation from Table 3 . In general, the automated information is very informative, and the human interpretations 3 is contained in almost all cases, though the output tends to be more verbose. The LLM outputs, with some human post-checking, can thus further speed up interpretation.

\section*{J Compute Resources}

We ran all experiments on an NVIDIA A100 card. Training time for the decoder is between 4 and 6 hours in each of the three case studies, without exhaustively tuning efficiency of the implementation, which we believe could further speed up training. Generation of $\epsilon$-preimage samples, and patching experiments run more quickly, as they require just one pass over the test data. For the largest model (IOI), generation takes $1-2$ hours.```


[^0]:    ${ }^{1}$ In some cases, including Figure $2 \mathrm{~b} 3 \mathrm{~b}$, we fix a particular position for interpretation. See Appendix A. 3

    ${ }^{2}$ Besides representative examples given in the Appendix, the reader can check generations conveniently at https://inversion-view.streamlit.app.

