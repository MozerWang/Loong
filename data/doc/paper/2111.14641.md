# Randomized block Gram-Schmidt process for the solution of linear systems and eigenvalue problems. 

Oleg Balabanov* and Laura Grigori ${ }^{\dagger \ddagger}$


#### Abstract

This article introduces randomized block Gram-Schmidt process (RBGS) for QR decomposition. RBGS extends the single-vector randomized Gram-Schmidt (RGS) algorithm and inherits its key characteristics such as being more efficient and having at least as much stability as any deterministic (block) Gram-Schmidt algorithm.

Block algorithms offer superior performance as they are based on BLAS3 matrix-wise operations and reduce communication cost when executed in parallel. Notably, our low-synchronization variant of RBGS can be implemented in a parallel environment using only one global reduction operation between processors per block. Moreover, the block Gram-Schmidt orthogonalization is the key element in the block Arnoldi procedure for the construction of a Krylov basis, which in turn is used in GMRES, FOM and Rayleigh-Ritz methods for the solution of linear systems and clustered eigenvalue problems. In this article, we develop randomized versions of these methods, based on RBGS, and validate them on nontrivial numerical examples.


Key words - Gram-Schmidt, QR factorization, randomization, sketching, numerical stability, rounding errors, loss of orthogonality, multi-precision arithmetic, block Krylov subspace methods, Arnoldi iteration, Petrov-Galerkin, RayleighRitz, generalized minimal-residual method.

## 1 Introduction

Let $\mathbf{W} \in \mathbb{R}^{n \times m}$ be a matrix with a moderately large number of columns, so that $m \ll n$. We consider column-oriented block Gram-Schmidt (BGS) algorithms for computing a QR factorization of $\mathbf{W}$ :

$$
\mathbf{W}=\mathbf{Q R}
$$

where $\mathbf{Q} \in \mathbb{R}^{n \times m}$ has $\ell_{2}$-orthonormal or very well-conditioned columns such that $\operatorname{range}(\mathbf{Q})=\operatorname{range}(\mathbf{W})$, and $\mathbf{R} \in$ $\mathbb{R}^{m \times m}$ is upper triangular with positive diagonal entries. Block algorithms are usually based on matrix-wise BLAS3 operations allowing proper exploitation of modern cache-based and high-performance computational architectures. The BGS orthogonalization forms a skeleton for block Krylov subspace methods for solving clustered eigenvalue problems as well as linear systems with multiple right-hand sides. It is also used in $s$-step, enlarged and other communication-avoiding Krylov subspace methods $[16,18]$. Please see [11] and the references therein for an extensive overview of BGS variants, and $[2,23,24,25,30]$ for the underlying block Krylov methods.

The randomized Gram-Schmidt (RGS) process for QR decomposition, based on the random sketching technique (see $[20,27,28]$ and the references therein), was introduced in [4, 5]. It requires nearly half as many flops and data passes as the classical and modified Gram-Schmidt (CGS and MGS) processes. Moreover, in a parallel architecture, RGS maintains the number of synchronization points of CGS, and therefore reduces them by a factor of $\mathcal{O}(m)$ compared to MGS. Notably, RGS is significantly more stable than CGS and at least as stable as MGS, as it yields a well-conditioned Q factor even when cond $(\mathbf{W})=\mathcal{O}\left(u^{-1}\right)$, with $u$ representing the unit roundoff. While the RGS algorithm can be already beneficial[^0]under unique precision computations, there are even more benefits that can be gained by working in two precisions: using a coarse unit roundoff for expensive high-dimensional operations and a fine unit roundoff elsewhere. In this case the stability of the RGS algorithm can be guaranteed for the coarse roundoff independent of the dominant dimension $n$ of the matrix. This property can be particularity useful for large-scale computations performed on low-precision arithmetic architectures. Another advantage of RGS is the ability of efficient a posteriori certification of the factorization without having to estimate the condition number of a large-scale matrix, or to perform any other expensive large-scale computations.

The essential feature of RGS algorithm is orthonormalization of a random sketch $\Theta \mathbf{Q}$ of $\mathbf{Q}$ rather than the full factor, where $\Theta \in \mathbb{R}^{k \times n}$ is a carefully chosen random matrix with typically $k=\mathcal{O}(m)$ rows that can be efficiently applied to vectors within the given architecture. The sketching matrix is designed to be an approximate isometry, or a $\varepsilon$-embedding, for range $(\mathbf{Q})$ with high probability. The authors then applied RGS to Krylov methods for solving linear systems $\mathbf{A x}=\mathbf{b}$ and eigenvalue problems $\mathbf{A x}=\lambda \mathbf{b}$. They obtained randomized Arnoldi iteration providing a sketch-orthonormal Krylov basis $\mathbf{Q}=\left[\mathbf{q}_{1}, \ldots, \mathbf{q}_{m}\right]$ that satisfies the Arnoldi identity $\mathbf{A}\left[\mathbf{q}_{1}, \ldots, \mathbf{q}_{m-1}\right]=\mathbf{Q H}$, and the associated GMRES method providing a solution $\mathbf{x}_{m-1}$ that minimizes the sketched residual $\left\|\boldsymbol{\Theta}\left(\mathbf{A} \mathbf{x}_{m-1}-\mathbf{b}\right)\right\|_{2}$ over the Krylov subspace. The approximate orthogonality of $\mathbf{Q}$ and the accuracy of $\mathbf{x}_{m-1}$ compared to the classical GMRES solution directly follow from the fact that $\Theta$ is a $\varepsilon$-embedding for the computed Krylov subspace.

In this paper we propose a block generalization (RBGS) of the RGS process. It has similar stability guarantees as its single-vector counterpart, and similar flops count (nearly half of that of block CGS). At the same time, thanks to the block paradigm, it is better suited to modern computational architectures. In particular, the major operations in the RBGS algorithm can be implemented using cache-efficient BLAS3 subroutines and a reduced number of synchronizations between distributed processors. Notably, we introduce a low-synchronization variant of RBGS, which stands out for its requirement of only one global synchronization between distributed processors per block. In addition, RBGS is only weakly sensitive to the accuracy of inter-block orthogonalization, and inherits the ability of efficient certification of factorization. Similarly to RGS, the RBGS has the advantage of performing dominant operations with a unit roundoff independent of the matrix's primary dimension.

Furthermore, we address the application of the RBGS algorithm to block Krylov methods. We introduce a block generalization of the randomized Arnoldi algorithm from [4, 5], which is subsequently used to develop randomized block GMRES, full orthogonalization method (FOM), and Rayleigh-Ritz (RR) method for solving linear systems and eigenvalue problems. In exact arithmetic, these methods can be interpreted as minimization of a sketched residual norm or imposing a sketched Galerkin orthogonality condition on the residuals. Furthermore, we also discuss an application of RBGS to Krylov $s$-step methods.

It is noticed that our RBGS algorithm can be augmented with a Cholesky QR step to provide a QR factorization with a $\mathrm{Q}$ factor that is not just well-conditioned but $\ell_{2}$-orthogonal to machine precision. A detailed presentation of Cholesky QR and its properties can be found in $[15,29]$. Such augmented procedure can be readily used to facilitate the classical GMRES, FOM and RR approximations, or in any other applications.

In addition to $[4,5]$ and the present article, there is another notable work by Nakatsukasa and Tropp [21] on randomized Krylov methods, which was conducted independently and made available slightly earlier than the present work. It is also worth noting that the sketched minimal-residual condition used in randomized GMRES and the sketched Galerkin orthogonality condition used in randomized FOM and RR were proposed in $[6,7]$, where they were applied to compute approximate solutions of linear parametric systems in a low-dimensional subspace.

This article is organized as follows. The basic notations are explained in Section 1.1. We introduce a general BGS process and particularize it to a few classical variants in Section 1.2. In Section 1.3 we present the basic ingredients of the random sketching technique and also extend the results from [4] concerning the effect of sketching on rounding errors in a matrix-matrix product. In Section 2, we present novel RBGS algorithms, including the low synchronization variant which relies on randomized CholeskyQR, a matrix based version of a vector oriented algorithm presented in $[4,5]$. Randomized CholeskyQR is highly related to [22]. The stability of RBGS is analyzed in Section 3. Section 4 discusses the application of the methodology to solving clustered eigenvalue problems and linear systems, possibly with multiple right-hand sides. For this we develop the randomized block Arnoldi iteration and the associated GMRES, FOM, and RR methods. Nontrivial numerical experiments in Section 5 demonstrate great potential of our methodology. The proofs of theorems and propositions from the stability analysis are deferred to Section 6. In Section 7 we conclude the work and provide an avenue of future research.

As supplementary material, we provide a discussion on the efficient solution of sketched least-squares problems, which
is a component of the RGS and RBGS algorithms, and provide an analysis of the accuracy of the randomized RR approximation from the paper.

### 1.1 Preliminaries

Throughout this work, we use the following notations, which is an adaptation of the notations from [4] to block linear algebra. As in [4], we denote vectors $\mathbf{x}$ by bold lowercase letters. A matrix composed of several column vectors $\mathbf{x}_{1}, \ldots, \mathbf{x}_{k}$ is denoted with the associated bold capital letter and a subscript specifying the number of columns, i.e., $\mathbf{X}_{k}$. If $k$ is constant, this notation can be simplified to $\mathbf{X}$. The $(i, j)$-th block of a block matrix $\mathbf{X}$ is denoted by $\mathbf{X}_{(i, j)}$ i.e, we have

$$
\mathbf{X}=\left[\begin{array}{cccc}
\mathbf{X}_{(1,1)} & \mathbf{X}_{(1,2)} & \cdots & \mathbf{X}_{(1, p)} \\
\mathbf{X}_{(2,1)} & \mathbf{X}_{(2,2)} & \cdots & \mathbf{X}_{(2, p)} \\
\vdots & \vdots & \ddots & \\
\mathbf{X}_{(l, 1)} & \mathbf{X}_{(l, 2)} & & \mathbf{X}_{(l, p)}
\end{array}\right]
$$

for some $p$ and $l$. When $l=1$ (i.e., matrix $\mathbf{X}$ is partitioned column-wise), the notation $\mathbf{X}_{(1, j)}$ can be simplified to $\mathbf{X}_{(j)}$. The sub-block of $\mathbf{X}$ composed of blocks $\mathbf{X}_{(i, j)}$ with $N_{1} \leq i \leq N_{2}$ and $M_{1} \leq j \leq M_{2}$, is denoted by $\mathbf{X}_{\left(N_{1}: N_{2}, M_{1}: M_{2}\right)}$. We denote the matrix $\mathbf{X}_{\left(N_{1}: N_{2}, M_{1}: M_{1}\right)}$ by simply $\mathbf{X}_{\left(N_{1}: N_{2}, M_{1}\right)}$. Moreover, if $l=1$, then matrix $\mathbf{X}_{\left(1: 1, M_{1}: M_{2}\right)}$ is denoted by $\mathbf{X}_{\left(M_{1}: M_{2}\right)}$, and matrix $\mathbf{X}_{\left(M_{1}: M_{1}\right)}$ by $\mathbf{X}_{\left(M_{1}\right)}$. The minimal and the maximal singular values of $\mathbf{X}$ are denoted by $\sigma_{\min }(\mathbf{X})$ and $\sigma_{\max }(\mathbf{X})$, and the condition number by cond $(\mathbf{X})$. We let $\langle\cdot, \cdot\rangle$ and $\|\cdot\|=\sigma_{\max }(\cdot)$ be the $\ell_{2}$-inner product and $\ell_{2}$-norm, respectively. $\|\cdot\|_{F}$ denotes the Frobenius norm. For two matrices (or vectors) $\mathbf{X}$ and $\mathbf{Y}$, the relation $\mathbf{X} \leq \mathbf{Y}$ indicates that the entries of $\mathbf{X}$ satisfy $x_{i, j} \leq y_{i, j}$. Furthermore, for a matrix (or a vector) $\mathbf{X}$, we denote by $|\mathbf{X}|$ the matrix $\mathbf{Y}$ with entries $y_{i, j}=\left|x_{i, j}\right|$. We also let $\mathbf{X}^{\mathrm{T}}$ and $\mathbf{X}^{\dagger}$ respectively indicate the transpose and the Moore-Penrose inverse of $\mathbf{X}$. Finally, we let $\mathbf{I}$ be the identity matrix of size appropriate to the expression where this notation is used.

A quantity or an arithmetic expression $X$ computed with finite precision arithmetic is denoted by $\mathrm{fl}(X)$ or $\hat{X}$. In addition, we also use the following "plus-minus" notation. The $\pm$ symbol indicates that the left hand side is bounded from above by the right hand side with $\pm$ replaced by a plus, and from below with $\pm$ replaced by a minus. Moreover, $\mp$ symbol is used in conjunction with $\pm$.

### 1.2 BGS process

Let matrix $\mathbf{W} \in \mathbb{R}^{n \times m}$ be partitioned into $p$ blocks $\mathbf{W}_{(i)} \in \mathbb{R}^{n \times m_{p}}$, with $1 \leq i \leq p, m=m_{p} p$ :

$$
\mathbf{W}=\mathbf{W}_{(1: p)}=\left[\mathbf{W}_{(1)} \mathbf{W}_{(2)} \ldots \mathbf{W}_{(p)}\right]
$$

The BGS process for computing a QR factorization of $\mathbf{W}$ proceeds recursively, at iteration $i$, selecting a new block matrix $\mathbf{W}_{(i)}$ and orthogonalizing it with the previously orthogonalized blocks yielding a matrix $\mathbf{Q}_{(i)}^{\prime}$, followed by orthogonalization of $\mathbf{Q}_{(i)}^{\prime}$ itself. This procedure is summarized in Algorithm 1.1.

```
Algorithm 1.1 BGS process
    Given: $n \times m$ block matrix $\mathbf{W}=\mathbf{W}_{(1: p)}, m \leq n$
    Output: $n \times m$ factor $\mathbf{Q}=\mathbf{Q}_{(1 ; p)}$ and $m \times m$ upper triangular factor $\mathbf{R}=\mathbf{R}_{(1: p, 1: p)}$.
    for $i=1: p$ do
        1. Compute a projection $\mathbf{Q}_{(i)}^{\prime}=\boldsymbol{\Pi}^{(i-1)} \mathbf{W}_{(i)}$ (also yielding $\mathbf{R}_{(1: i-1, i)}$ ).
        2. Compute $\mathrm{QR}$ factorization $\mathbf{Q}_{(i)} \mathbf{R}_{(i, i)}=\mathbf{Q}_{(i)}^{\prime}$ with suitable efficient routine.
    end for
```

For standard methods, the projector $\boldsymbol{\Pi}^{(i-1)}$ in Algorithm 1.1 is taken as an approximation to $\ell_{2}$-orthogonal projector onto range $\left(\mathbf{Q}_{(1: i-1)}\right)^{\perp}$. For the classical BGS process (BCGS), one chooses $\boldsymbol{\Pi}^{(i-1)}$ as

$$
\boldsymbol{\Pi}^{(i-1)}=\mathbf{I}-\mathbf{Q}_{(1: i-1)} \mathbf{Q}_{(1: i-1)}^{\mathrm{T}}
$$

Whereas, for the modified BGS (BMGS) process, we have

$$
\begin{equation*}
\boldsymbol{\Pi}^{(i-1)}=\left(\mathbf{I}-\mathbf{Q}_{(i-1)}\left(\mathbf{Q}_{(i-1)}\right)^{\mathrm{T}}\right)\left(\mathbf{I}-\mathbf{Q}_{(i-2)}\left(\mathbf{Q}_{(i-2)}\right)^{\mathrm{T}}\right) \ldots\left(\mathbf{I}-\mathbf{Q}_{(1)}\left(\mathbf{Q}_{(1)}\right)^{\mathrm{T}}\right) \tag{1.1}
\end{equation*}
$$

In infinite precision arithmetic, these two projectors are equivalent, and are exactly equal to the $\ell_{2}$-orthogonal projector onto range $\left(\mathbf{Q}_{(1: i-1)}\right)^{\perp}$, since, by construction, $\mathbf{Q}_{(1: i-1)}$ is an orthogonal matrix. However, in the presence of rounding errors these projectors may cause instability. In the first case, the condition number of $\mathbf{Q}$ can grow as cond $(\mathbf{W})^{2}$ or even worse and thus requires special treatment [10]. While in the second case, cond( $\mathbf{Q})$ can grow as cond $(\mathbf{W}) \max _{1 \leq j \leq p} \operatorname{cond}\left(\mathbf{W}_{(j)}\right)$ unless step 2 is unconditionally stable $[8,11]$. The stability of these processes can be improved by re-orthogonalization, i.e., by running the inner loop twice. In particular, it can be shown that the BCGS process with re-orthogonalization, here denoted by BCGS2, yields an almost orthogonal $\mathrm{Q}$ factor as long as the matrix $\mathbf{W}$ is numerically full rank [9].

Inter-block orthogonalization is an important step in BGS algorithm. For standard versions, this step can be performed with any suitable efficient and stable routine for $\left(\ell_{2}-\right) \mathrm{QR}$ factorization of tall and skinny matrices, such as TSQR from [14], a Gram-Schmidt QR applied $l$ times, Cholesky QR applied $l$ times, and others. Typically, such QR factorization takes only a fraction of the overall computational cost.

### 1.3 Random sketching

In this subsection we recall the basic notions of the random sketching technique. Let $\Theta \in \mathbb{R}^{k \times n}$ with $k \leq n$ be a sketching matrix such that the associated sketched product $\langle\boldsymbol{\Theta} \cdot \boldsymbol{\Theta} \cdot\rangle$ approximates well the $\ell_{2}$-inner product between any two vectors in the subspace (or subspaces) of interest $V \subset \mathbb{R}^{n}$. Such matrix $\Theta$ is referred to as a $\varepsilon$-embedding for $V$, as defined below.

Definition 1.1. For $\varepsilon<1$, we say that the sketching matrix $\boldsymbol{\Theta}$ is a $\varepsilon$-embedding for subspace $V$ (or matrix $\mathbf{V}$ spanning $V)$, if it satisfies the following relation:

$$
\begin{equation*}
\forall \mathbf{x}, \mathbf{y} \in V, \quad|\langle\mathbf{x}, \mathbf{y}\rangle-\langle\boldsymbol{\Theta} \mathbf{x}, \boldsymbol{\Theta} \mathbf{y}\rangle| \leq \varepsilon\|\mathbf{x}\|\|\mathbf{y}\| \tag{1.2}
\end{equation*}
$$

Furthermore, we assume that $\boldsymbol{\Theta}$ is chosen at random from a certain distribution, such that it satisfies (1.2) for any fixed $d$-dimensional subspace with high probability (see Definition 1.2).

Definition 1.2. A random sketching matrix $\boldsymbol{\Theta}$ is called $a(\varepsilon, \delta, d)$ oblivious $\ell_{2}$-subspace embedding, if for any fixed $V \subset \mathbb{R}^{n}$ of dimension $d$, it satisfies

$$
\mathbb{P}(\boldsymbol{\Theta} \text { is a } \varepsilon \text {-embedding for } V) \geq 1-\delta
$$

Corollary 1.3. If $\Theta \in \mathbb{R}^{k \times n}$ is a $(\varepsilon, \delta / n, 1)$ oblivious $\ell_{2}$-subspace embedding, then with probability at least $1-\delta$, we have

$$
\|\boldsymbol{\Theta}\|_{\mathrm{F}} \leq \sqrt{(1+\varepsilon) n}
$$

Corollary 1.4. If $\boldsymbol{\Theta} \in \mathbb{R}^{k \times n}$ is an $\varepsilon$-embedding for $\mathbf{V}$, then the singular values of $\mathbf{V}$ are bounded by

$$
(1+\varepsilon)^{-1 / 2} \sigma_{\min }(\boldsymbol{\Theta} \mathbf{V}) \leq \sigma_{\min }(\mathbf{V}) \leq \sigma_{\max }(\mathbf{V}) \leq(1-\varepsilon)^{-1 / 2} \sigma_{\max }(\boldsymbol{\Theta} \mathbf{V})
$$

The proofs for Corollaries 1.3 and 1.4 can be found for instance in [4].

In recent years, several distributions of $\Theta$ have been proposed that satisfy Definition 1.2 and have a small first dimension $k$ that depends at most logarithmically on the dimension $n$ and probability of failure $\delta$. Among them, the most suitable distribution should be selected depending on the problem and computational architecture. The potential of random sketching is here realized on (rescaled) Rademacher matrices and Subsampled Randomized Hadamard Transform (SRHT). The entries of a Rademacher matrix are i.i.d. random variables satisfying $\mathbb{P}\left(\theta_{i, j}=1 / \sqrt{k}\right)=\mathbb{P}\left(\theta_{i, j}=-1 / \sqrt{k}\right)=1 / 2$. Rademacher matrices can be efficiently multiplied by vectors and matrices through the proper exploitation of computational resources such as cache or distributed machines. For $n$, which is a power of 2 , SRHT is defined as a product of a diagonal matrix of random signs with a Walsh-Hadamard matrix, followed by an uniform sub-sampling matrix scaled by $1 / \sqrt{k}$. For a general $n$, the SRHT has to be combined with zero padding to make the dimension a power of 2 . Random sketching with SRHT can reduce the complexity of an algorithm. Products of SRHT matrices with vectors require only $n \log _{2} n$ flops using the fast Walsh-Hadamard transform or $2 n \log _{2}(k+1)$ flops following the methods in [1]. Furthermore, for
both distributions, the usage of a seeded random number generator can allow efficient storage and application of $\Theta$. It follows from [26,28] that the rescaled Rademacher distribution, and SRHT (possibly with zero padding) respectively are $(\varepsilon, \delta, d)$ oblivious $\ell_{2}$-subspace embeddings, if they have a sufficiently large first dimension: if $k \geq 7.87 \varepsilon^{-2}(6.9 d+\log (1 / \delta))$ for Rademacher matrices, or if $k \geq 2\left(\varepsilon^{2}-\varepsilon^{3} / 3\right)^{-1}(\sqrt{d}+\sqrt{8 \log (6 n / \delta)})^{2} \log (3 d / \delta)$ for SRHT. A complete proof of this fact can be found for instance in $[6]$.

### 1.4 Effect of random sketching on rounding errors

Let us now discuss an important result from [4, Section 2.2] characterizing the rounding errors in a sketched matrix-vector product, which can be easily extended to matrix-matrix products. In short, this result states that multiplying $\Theta$ by a matrix-vector product $\widehat{\mathbf{x}}=\mathrm{f}(\mathbf{Y} \mathbf{z})$ does not increase the rounding error bound of $\widehat{\mathbf{x}}$ by more than a small factor. This property is essentially a sketched version of the standard "rule of thumb" of rounding analysis [17]. It was rigorously proven in [4] for the probabilistic rounding model, [13, Model 4.7]. The extension of this result to matrix-matrix products is provided below.

Let us fix a realization of an oblivious $\ell_{2}$-subspace embedding $\Theta \in \mathbb{R}^{k \times n}$ of sufficiently large size, and consider a matrix-matrix product $\mathbf{X}=\mathbf{Y Z}$, with $\mathbf{Y} \in \mathbb{R}^{n \times m}, \mathbf{Z} \in \mathbb{R}^{m \times l}$, computed in finite precision arithmetic with unit roundoff $u<0.01 / \mathrm{m}$. The following results can be derived directly from [4, Section 2.2] with the observation that each column of $\mathbf{X}$ is a matrix-vector product: $\mathbf{x}_{i}=\mathbf{Y} \mathbf{z}_{i}, 1 \leq i \leq l$.

We have,

$$
\begin{equation*}
|\mathbf{X}-\widehat{\mathbf{X}}| \leq \mathbf{U} \tag{1.3}
\end{equation*}
$$

for some matrix $\mathbf{U}$ describing the "worst-case scenario" rounding error. In general, the standard analysis (e.g., see [17]) gives the bound

$$
\begin{equation*}
|\mathbf{X}-\widehat{\mathbf{X}}| \leq \frac{m u}{1-m u}|\mathbf{Y}||\mathbf{Z}| \leq 1.02 m u|\mathbf{Y}||\mathbf{Z}| \tag{1.4}
\end{equation*}
$$

which means that $\mathbf{U}=1.02 \mathrm{mu}|\mathbf{Y}||\mathbf{Z}|$ satisfies (1.3). Furthermore, as in [4], if $\mathbf{Y Z}$ represents a sum of a matrix-matrix product with a matrix, i.e., $\mathbf{Y} \mathbf{Z}=\mathbf{Y}^{\prime} \mathbf{Z}^{\prime}+\mathbf{H}$, then one can take

$$
\mathbf{U}=1.02 u\left(|\mathbf{H}|+m\left|\mathbf{Y}^{\prime} \|\right| \mathbf{Z}^{\prime} \mid\right)
$$

Let us now address bounding the norm of the rounding error after sketching $\widehat{\mathbf{X}}$. We have the following "worst-case scenario" bound

$$
\begin{equation*}
\|\boldsymbol{\Theta}(\mathbf{X}-\widehat{\mathbf{X}})\|_{\mathrm{F}} \leq\|\boldsymbol{\Theta}\|\|\mathbf{X}-\widehat{\mathbf{X}}\|_{F} \leq\|\boldsymbol{\Theta}\|\|\mathbf{U}\|_{\mathrm{F}} \tag{1.5}
\end{equation*}
$$

which, combined with Corollary 1.3, implies that if $\Theta$ is $(\varepsilon, \delta / n, 1)$ oblivious $\ell_{2}$-subspace embedding, then $\|\Theta(\mathbf{X}-\widehat{\mathbf{X}})\|_{\mathrm{F}} \leq$ $\sqrt{1+\varepsilon} \sqrt{n}\|\mathbf{U}\|_{F}$ holds with probability at least $1-\delta$. The next important point is that, as was argued in [4], this bound is pessimistic and can be improved by a factor of $\mathcal{O}(\sqrt{n})$ by exploiting statistical properties of rounding errors. For instance one can rely on the assumption that the rounding errors in elementary arithmetic operations are mean-independent random variables [13]. We summarize the results from [4, Theorem 2.5 and Corollary 2.6] below.

Corollary 1.5. Consider a matrix-matrix product $\mathbf{X}=\mathbf{Y Z}$, with $\mathbf{Y} \in \mathbb{R}^{n \times m}, \mathbf{Z} \in \mathbb{R}^{m \times l}$, computed under probabilistic rounding model, where the rounding errors due to elementary arithmetic operations are mean-independent random variables with zero mean. Furthermore assume that the errors are bounded so that it holds,

$$
|\mathbf{X}-\widehat{\mathbf{X}}| \leq \mathbf{U}
$$

where $\mathbf{U}$ is a deterministic matrix representing the worst-case scenario rounding error. If $\boldsymbol{\Theta}$ is a $\left(\varepsilon / 4, l^{-1}\binom{n}{d}^{-1} \delta, d\right)$ oblivious $\ell_{2}$-subspace embedding, with $d=4.2 c^{-1} \log (4 / \delta)$, where $c \leq 1$ is some universal constant, then

$$
\begin{equation*}
\|\Theta(\mathbf{X}-\widehat{\mathbf{X}})\|_{\mathrm{F}} \leq \sqrt{1+\varepsilon}\|\mathbf{U}\|_{\mathrm{F}} \tag{1.6}
\end{equation*}
$$

holds with probability at least $1-2 \delta$.

By using the bounds from Section 1.3 we deduce that for $l \leq n$ the relation (1.6) is satisfied with probability at least $1-2 \delta$ if $\boldsymbol{\Theta}$ is a Rademacher matrix with $\mathcal{O}(\log (n) \log (1 / \delta))$ rows or SRHT matrix with $\mathcal{O}\left(\log ^{2}(n) \log ^{2}(1 / \delta)\right)$ rows.

The fact that the bound (1.6) is independent of (the high) dimension $n$ implies that, in practice, products of matrices in randomized algorithms can be performed with a unit roundoff that is independent of $n$.

## 2 RBGS process

Consider BGS algorithms with projectors $\boldsymbol{\Pi}^{(i-1)}$ that respect the relation

$$
\boldsymbol{\Pi}^{(i-1)} \mathbf{W}_{(i)}=\mathbf{W}_{(i)}-\mathbf{Q}_{(1: i-1)} \mathbf{X}
$$

where $\mathbf{X}=\mathbf{R}_{(1: i-1, i)}$ is computed from $\mathbf{W}^{(i)}$ and $\mathbf{Q}_{(1: i-1)}$. Standard algorithms take $\mathbf{X}$ as an approximate solution to the following minimization problem:

$$
\begin{equation*}
\min _{\mathbf{Y}}\left\|\mathbf{Q}_{(1: i-1)} \mathbf{Y}-\mathbf{W}_{(i)}\right\|_{\mathbf{F}} \tag{2.1}
\end{equation*}
$$

For instance, BCGS algorithm approximates the exact solution to (2.1) by $\mathbf{Q}_{(1: i-1)}^{\mathrm{T}} \mathbf{W}_{(i)}$, whereas BMGS projector (1.1) improves this approximation under finite precision arithmetic, though it may cause a computational overhead in terms of inter-processor communication and operation with cache/RAM.

As proposed in [4], a reduction of computational cost and/or improvement of the stability of the Gram-Schmidt process can be obtained with a projector that gives a $\mathrm{Q}$ factor orthonormal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$ instead of the $\ell_{2}$-inner product as in standard methods. In our case, this corresponds to taking $\mathbf{X}$ as an approximate solution to

$$
\begin{equation*}
\min _{\mathbf{Y}}\left\|\boldsymbol{\Theta} \mathbf{Q}_{(1: i-1)} \mathbf{Y}-\boldsymbol{\Theta} \mathbf{W}_{(i)}\right\|_{\mathbf{F}} \tag{2.2}
\end{equation*}
$$

which is a $k$-dimensional block least-squares problem. Since the sketching dimension satisfies $k \ll n$, a very accurate solution to (2.2) should be more efficient to compute than even the cheapest (i.e, BCGS) solution to (2.1). Furthermore, with the right choice of random sketching matrices, the precomputation of the sketches of $\mathbf{Q}_{(1: i-1)}$ and $\mathbf{W}_{(i)}$ should also have only a minor cost compared to that of standard high-dimensional operations. If (2.2) is very small, its solution can be obtained with a direct solver based, for example, on Householder or Givens QR factorization, requiring a cubic complexity. If the problem has a moderate size, so that each direct solution has a considerable computational cost, it can be beneficial to recycle the $\mathrm{QR}$ factorization of $\Theta \mathrm{Q}_{(1: i-2)}$ computed at iteration $i-1$ to get the $\mathrm{QR}$ factorization of $\Theta \mathbf{Q}_{(1: i-1)}$. Alternatively, we may use the fact that the matrix $\Theta \mathbf{Q}_{(1: i-1)}$ is almost orthonormal, which implies applicability of iterative solvers running in quadratic complexity. Several such solvers are discussed in Section 2.1.

To produce a $\mathrm{QR}$ factorization of $\mathbf{W}$ with respect to $\langle\boldsymbol{\Theta} \cdot \boldsymbol{\Theta} \cdot\rangle$, the inter-block orthogonalization of $\mathbf{Q}_{(i)}^{\prime}$ in step 2 of Algorithm 1.1 has to provide a $\mathrm{Q}$ factor $\mathbf{Q}_{(i)}$ orthonormal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$. An efficient procedure for this task can be based on sketched Cholesky QR factorization (RCholeskyQR), which consists in obtaining the $\mathrm{R}$ factor $\mathbf{R}_{(i)}$ by a regular $\mathrm{QR}$ of the sketch $\Theta \mathbf{Q}_{(i)}^{\prime}$, and then retrieving $\mathbf{Q}_{(i)}$ with forward substitution: $\mathbf{Q}_{(i)}=\mathbf{Q}_{(i)}^{\prime} \mathbf{R}_{(i)}^{-1}$. In this case, it can be beneficial to obtain the sketch of $\mathbf{Q}_{(i)}^{\prime}$ from $\Theta \mathbf{Q}_{(i)}^{\prime}=\Theta \mathbf{W}_{(i)}-\Theta \mathbf{Q}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}$ rather than by multiplying $\boldsymbol{\Theta}$ with $\mathbf{Q}_{(i)}^{\prime}$, as this saves a global synchronization between processors. Another option would be to simply perform the inter-block orthogonalization with a single-vector RGS algorithm. See Section 2.2 for more details.

The RBGS process using the random sketching projector is depicted in Algorithm 2.1.

At the iteration $i=1$ in Algorithm 2.1 we used the notation that $\mathbf{R}_{(1: i-1, i)}$ is a $0 \times m_{p}$ matrix and $\mathbf{Q}_{(1: i-1)}$ is a $n \times 0$ matrix, so that $\mathbf{Q}_{(i)}^{\prime}=\mathbf{W}_{(i)}$. Algorithm 2.1 is presented under multi-precision arithmetic using two unit roundoffs, like its single-vector counterpart from [4]. The working precision is represented by a coarse roundoff $u_{c r s}$. It is used for standard high-dimensional operations in step 3, which determine the overall computational cost. All other (inexpensive) operations in Algorithm 2.1 are computed with a fine unit roundoff $u_{\text {fine }}, u_{\text {fine }} \leq u_{\text {crs }}$. It is shown in Section 3 that Algorithm 2.1 is stable if $u_{c r s} \leq \mathcal{O}\left(\operatorname{cond}(\mathbf{W}) m^{-2}\right)$, which is a very mild condition on $u_{c r s}$. The fact that this bound is independent of the high dimension $n$ explains potential of our methodology for large-scale problems computed on low-precision arithmetic architectures. Furthermore, according to our numerical experiments, the RBGS algorithm can be sufficiently stable even when $u_{\text {crs }}$ is larger than $\mathcal{O}\left(\operatorname{cond}(\mathbf{W}) m^{-2}\right)$. In such cases, the stability of the algorithm can be certified by a posteriori

```
Algorithm 2.1 RBGS algorithm (RBGS)
    Given: $n \times m$ block matrix $\mathbf{W}=\mathbf{W}_{(1: p)}$, and $k \times n$ matrix $\boldsymbol{\Theta}, m \leq k \ll n$.
    Output: $n \times m$ factor $\mathbf{Q}=\mathbf{Q}_{(1 ; p)}$ and $m \times m$ upper triangular factor $\mathbf{R}=\mathbf{R}_{(1: p, 1 ; p)}$.
    for $i=1: p$ do
        1. Sketch $\mathbf{W}_{(i)}: \mathbf{P}_{(i)}=\boldsymbol{\Theta} \mathbf{W}_{(i)}$. \# macheps: $u_{\text {fine }}$
        2. Solve small block least-squares problem:
            $\mathbf{R}_{(1: i-1, i)}=\arg \min \mathbf{Y}\left\|\mathbf{S}_{(1: i-1)} \mathbf{Y}-\mathbf{P}_{(i)}\right\|_{\mathrm{F}} . \quad$ \# macheps: $u_{\text {fine }}$
        3. Compute projection of $\mathbf{W}_{(i)}: \mathbf{Q}_{(i)}^{\prime}=\mathbf{W}_{(i)}-\mathbf{Q}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}$. \# macheps: $u_{c r s}$
        4-5. Compute $\mathrm{QR}$ fact. $\mathbf{Q}_{(i)} \mathbf{R}_{(i, i)}=\mathbf{Q}_{(i)}^{\prime}$ with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$. \# macheps: $u_{\text {fine }}$
        Compute sketch of $\mathbf{Q}_{(i)}: \mathbf{S}_{(i)}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}$.
    end for
    \# macheps: $u_{\text {fine }}$
    6. (Optional) compute $\Delta^{(p)}=\left\|\mathbf{I}-\mathbf{S}^{\mathrm{T}} \mathbf{S}\right\|_{\mathrm{F}}$ and $\tilde{\Delta}^{(p)}=\frac{\|\mathbf{P}-\mathbf{S R}\|_{\mathrm{F}}}{\|\mathbf{P}\|_{F}}$.
    \# macheps: $u_{\text {fine }}$
```

bounds given by the quantities $\Delta^{(p)}$ and $\tilde{\Delta}^{(p)}$, computed in (optional) step 6 . These bounds provide stability certification if $u_{c r s}=\mathcal{O}\left(m^{-3 / 2}\right)$, which is a milder condition than the one for a priori guarantees, and is independent not only of $n$ but also of $\operatorname{cond}(\mathbf{W})$.

The stability guarantees of Algorithm 2.1 executed in unique precision can be obtained directly from the analysis of the multi-precision algorithm by taking $u_{\text {fine }}=u$ and $u_{c r s}=F(m, n) u$, where $F(m, n)$ is some polynomial of low degree.

In terms of performance, the SRHT-based RBGS algorithm requires about half the flops and data passes of the cheapest standard BGS algorithm, which is BCGS. Its computational cost is defined by $p$ well-parallelizable BLAS3 operations. Furthermore, with a suitable choice of inter-block QR factorization in steps 4-5, RBGS requires only one global reduction operation per block (see Section 2.2.3 for details). This version of RBGS is particularly noteworthy in the realm of "one-synchronization" block Gram-Schmidt algorithms $[12,19]$ in particular due to its provable stability characteristics.

Remark 2.1. If necessary, the output of the RBGS algorithm can be post-processed with Cholesky $Q R$ to provide a $Q R$ factorization with a $Q$ factor, which is not only well-conditioned but is $\ell_{2}$-orthonormal up to machine precision. More specifically, we can compute an upper-triangular matrix $\mathbf{R}^{\prime}$ such that $\mathbf{R}^{\prime \mathrm{T}} \mathbf{R}^{\prime}=\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ with a Cholesky decomposition, and consider

$$
\mathbf{Q} \leftarrow \mathbf{Q R}^{\prime-1} \text { and } \mathbf{R} \leftarrow \mathbf{R}^{\prime} \mathbf{R}
$$

The computational cost of this procedure is dominated by the computation of $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ and, possibly, $\mathbf{Q} \mathbf{R}^{\prime-1}$. In terms of flops, the computational cost of computing $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ is similar to the cost of the entire RBGS algorithm. Though as a single BLAS3 operation, it is better suited for cache-based and parallel computing. The computation of $\mathbf{Q R}^{\prime-1}$ can be omitted when the application allows operating with the $Q$ factor in an implicit form, for instance, in the Arnoldi iteration or $R R$ algorithm. Otherwise, this product can be computed by well-parallelizable forward substitution, or even by direct inversion, allowing a second BLAS3 multiplication. The stability guarantees of the $Q R$ factorization obtained after the Cholesky $Q R$ step follow directly from the fact that the RBGS algorithm produces a well-conditioned $Q$ factor along with the standard numerical stability guarantees of Cholesky $Q R$.

### 2.1 Solution of block least-squares problem in step 2

As already pointed out, the stability of Algorithm 2.1 strongly depends on the stability of the least-squares solver used in step 2. In particular, in our analysis in Section 3 we require the solution $\widehat{\mathbf{X}}=\widehat{\mathbf{R}}_{(1: i-1, i)}$ to satisfy the following backward-stability condition.

Assumptions 2.2 (Backward-stability of step 2 of RBGS). For each $1 \leq j \leq m_{p}$, the $j$-th column $\widehat{\mathbf{x}}$ of $\widehat{\mathbf{X}}$ and the $j$-th column $\widehat{\mathbf{p}}$ of $\widehat{\mathbf{P}}_{(i)}$ satisfy

$$
\begin{equation*}
\widehat{\mathbf{x}}=\arg \min _{\mathbf{y}}\left\|\left(\widehat{\mathbf{S}}_{(1: i-1)}+\boldsymbol{\Delta} \mathbf{S}\right) \mathbf{y}-(\widehat{\mathbf{p}}+\boldsymbol{\Delta} \mathbf{p})\right\| \tag{2.3}
\end{equation*}
$$

where the perturbations $\boldsymbol{\Delta} \mathbf{S}$ and $\boldsymbol{\Delta} \mathbf{p}$ are such that

$$
\|\boldsymbol{\Delta} \mathbf{S}\|_{\mathrm{F}} \leq 0.01 u_{c r s}\left\|\widehat{\mathbf{S}}_{(1: i-1)}\right\|, \quad\|\boldsymbol{\Delta} \mathbf{p}\| \leq 0.01 u_{c r s}\|\widehat{\mathbf{p}}\|
$$

Note that $\boldsymbol{\Delta} \mathbf{S}$ and $\boldsymbol{\Delta} \mathbf{p}$ may depend on $(i, j)$.

The condition in Assumptions 2.2 can be met by standard direct solvers based on Householder transformation or Givens rotations and a sufficiently large gap between $u_{\text {crs }}$ and $u_{\text {fine }}$, which follows from [17, Theorems 8.5, 19.10 and 20.3] and their proofs. However, direct solvers require a cubic complexity and can become too expensive even for relatively small $m$ and $k$. A remedy can be to re-use the $\mathrm{QR}$ factorization of $\mathbf{S}_{(1: i-2)}$ computed at iteration $i-1$ to get the $\mathrm{QR}$ factorization of $\mathbf{S}_{(1: i-1)}$. Another option is to appeal to iterative methods that can exploit the approximate orthogonality of $\mathbf{S}_{(1: i-1)}$ to speedup the computations such as the Richardson iterations $\mathbf{X} \leftarrow \mathbf{X}+\mathbf{S}_{(1: i-1)}^{\mathrm{T}}\left(\mathbf{P}_{(i)}-\mathbf{S}_{(1: i-1)} \mathbf{X}\right)$ (that can be viewed as CGS or BCGS reorthogonalizations), MGS or BMGS reorthogonalizations, Conjugate Gradient or GMRES methods applied to the normal system of equations $\left(\mathbf{S}_{(1: i-1)}^{\mathrm{T}} \mathbf{S}_{(1: i-1)}\right) \mathbf{X}=\mathbf{S}_{(1: i-1)}^{\mathrm{T}} \mathbf{P}_{(i)}$. A discussion of these methods can be found in the supplement to the article.

### 2.2 Inter-block QR factorization in steps 4-5

Let us now provide four ways of efficient and stable inter-block orthogonalization in steps $4-5$ of the RBGS algorithm.

### 2.2.1 Single-vector RGS algorithm

First, the inter-block orthogonalization can be readily performed with the single-vector RGS algorithm from [4] computed with unique roundoff $u=u_{\text {fine }}=F(m, n) u_{c r s}$, where $F(m, n)$ is a low-degree polynomial. This algorithm provides a QR factorization that satisfies (2.4a), which follows directly by construction. Furthermore, it should also satisfy (2.4b), since $\|\boldsymbol{\Theta}\|_{\mathrm{F}} \leq \sqrt{2 n}$ holds with high probability (according to Corollary 1.3).

$$
\begin{align*}
& \left|\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right| \leq 0.1 u_{c r s}\left|\widehat{\mathbf{Q}}_{(i)}\right|\left|\widehat{\mathbf{R}}_{(i, i)}\right|  \tag{2.4a}\\
& \left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right)\right\|_{\mathrm{F}} \leq 0.1 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\|\left\|\widehat{\mathbf{R}}_{(i, i)}\right\| \tag{2.4b}
\end{align*}
$$

As shown in [4], if $\boldsymbol{\Theta}$ is $\varepsilon$-embedding for $\widehat{\mathbf{Q}}_{(i)}^{\prime}$, then the RGS algorithm satisfies

$$
\begin{equation*}
1-0.1 u_{c r s} \operatorname{cond}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}\right) \leq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right) \leq \sigma_{\max }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right) \leq 1+0.1 u_{c r s} \operatorname{cond}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}\right) \tag{2.5}
\end{equation*}
$$

Moreover, in this case $\boldsymbol{\Theta}$ is guaranteed to be a $\varepsilon^{\prime}$-embedding for $\widehat{\mathbf{Q}}_{(i)}$, with $\varepsilon^{\prime}=2 \varepsilon+u_{c r s}$. Though, in practice, this property holds for smaller values $\varepsilon^{\prime}$, say, $\varepsilon+u_{c r s}$.

According to [4], the single-vector RGS algorithm applied to an inter-block has the cost of $2 m_{p}^{2} n$ flops and $m_{p} n$ memory units, which can be considered negligible compared to the complexity and memory consumption of other computations such as the standard high-dimensional operations in Algorithm 2.1. The RGS algorithm, however, requires $m_{p}$ global synchronizations between distributed processors, which can dominate the computational costs in parallel architectures. In such cases, it is necessary to appeal to other approaches for inter-block $\mathrm{QR}$ factorization, some of which are described below.

### 2.2.2 RCholeskyQR factorization

Another way to perform inter-block $\mathrm{QR}$ factorization of matrix $\mathbf{Q}_{(i)}^{\prime}$ with respect to the sketched inner product is to appeal to the following sketched version of Cholesky QR, called RCholeskyQR. We can first compute the $\mathrm{R}$ factor $\mathbf{R}_{(i, i)}$ by performing a $\left(\ell_{2^{-}}\right) \mathrm{QR}$ factorization of a small matrix $\mathbf{S}_{(i)}^{\prime}=\Theta \mathbf{Q}_{(i)}^{\prime}$ with any suitable stable routine. Then the $\mathrm{Q}$ factor is retrieved by computing $\mathbf{Q}_{(i)}=\mathbf{Q}_{(i)}^{\prime}\left(\mathbf{R}_{(i, i)}\right)^{-1}$ with forward substitution (see Algorithm 2.3). We note that the vector oriented version of RCholeskyQR factorization, where a $\mathrm{QR}$ of $\mathbf{S}_{(i)}^{\prime}$ and the forward substitution $\mathbf{Q}_{(i)}^{\prime}\left(\mathbf{R}_{(i, i)}\right)^{-1}$ are performed column by column, was introduced first in [4, 5, Remark 2.10]. For completeness, we present this version in Algorithm 2.2, particularly useful when the vectors become available one at a time. As notation, the sub-block $\mathbf{X}\left[N_{1}: N_{2}, M_{1}: M 2\right]$ is formed by the elements in rows $N_{1}$ to $N_{2}$ and columns $M_{1}$ to $M_{2}$ of $\mathbf{X}$, while the sub-block $\mathbf{X}\left[:, M_{1}: M 2\right]$ is formed by all the rows and columns $M_{1}$ to $M_{2}$ of $\mathbf{X}$. We refer to this algorithm as vRCholeskyQR. The
difference between vRCholeskyQR and RGS lies in step 4 of Algorithm 2.2, where in RGS the sketch is computed directly from the columns of $\mathbf{Q}_{(i)}$ before scaling, as $\widetilde{\mathbf{s}}_{j}^{\prime}=\boldsymbol{\Theta} \widetilde{\mathbf{q}}_{j}^{\prime}$.

In exact arithmetic RCholeskyQR provides the same output as the RGS algorithm from [4, 5]. This in particular implies that the $\mathrm{Q}$ factor $\mathbf{Q}_{(i)}$ obtained by RCholeskyQR is very well conditioned with a high probability. Moreover, it was shown more recently in [3] that RCholeskyQR has finite-precision stability guarantees similar to those of RGS given by (2.4) and (2.5). However, in practice RCholeskyQR may provide less numerical stability than RGS, as is indicated in $[4,5$, Remark 2.10].

RCholeskyQR algorithm has a high relation to the preconditioning technique for overdetermined least-squares problems developed in [22]. The difference is that in [22], $\mathbf{Q}_{(i)}^{\prime}\left(\mathbf{R}_{(i, i)}\right)^{-1}$ is not computed explicitly, but rather operated with as a function providing products with vectors, and that $\mathbf{R}_{(i, i)}$ is not computed using a regular $\mathrm{QR}$ of $\mathbf{S}_{(i)}^{\prime}$ but rather a $\mathrm{QR}$ with column-pivoting. RCholeskyQR is also considered in [21] to approximately orthogonalize a basis, where it is referred to as "basis whitening".

```
Algorithm 2.2 Vector oriented RCholeskyQR (vRCholeskyQR)
    Given: $\mathbf{Q}_{(i)}^{\prime}, \Theta$
    Output: $\mathrm{QR}$ fact. $\mathbf{Q}_{(i)}^{\prime}=\mathbf{Q}_{(i)} \mathbf{R}_{(i, i)}$, where $\mathbf{R}_{(i, i)}$ is upper triangular and $\mathbf{Q}_{(i)}$ is orthonormal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$; a sketch
    $\mathbf{S}_{(i)}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}$. For clarity, let $\mathbf{V}:=\mathbf{Q}_{(i)}^{\prime}=\left[\mathbf{v}_{1}, \ldots, \mathbf{v}_{m_{p}}\right], \widetilde{\mathbf{Q}}:=\mathbf{Q}_{(i)}=\left[\widetilde{\mathbf{q}}_{1}, \ldots, \widetilde{\mathbf{q}}_{m_{p}}\right], \widetilde{\mathbf{R}}:=\mathbf{R}_{(i, i)}$, and $\widetilde{\mathbf{S}}:=\mathbf{S}_{(i)}=\left[\widetilde{\mathbf{s}}_{1}, \ldots, \widetilde{\mathbf{s}}_{m_{p}}\right]$
    for $j=1: m_{p}$ do
        1. Sketch $\mathbf{v}_{j}: \mathbf{p}_{j}=\boldsymbol{\Theta} \mathbf{v}_{j}$. \# macheps: $u_{\text {fine }}$
        2. Solve least-squares problem:
            $\widetilde{\mathbf{R}}[1: j-1, j]=\arg \min _{\mathbf{y}}\left\|\widetilde{\mathbf{S}}[:, 1: j-1] \cdot \mathbf{y}-\mathbf{p}_{j}\right\| . \quad$ \# macheps: $u_{\text {fine }}$
        3. Compute projection of $\mathbf{v}_{j}$ :
            $\widetilde{\mathbf{q}}_{j}^{\prime}=\mathbf{v}_{j}-\widetilde{\mathbf{Q}}[:, 1: j-1] \cdot \widetilde{\mathbf{R}}[1: j-1, j] . \quad$ \# macheps: $u_{c r s}$
        4. Sketch $\widetilde{\mathbf{s}}_{j}^{\prime}=\mathbf{p}_{j}-\widetilde{\mathbf{S}}[:, 1: j-1] \cdot \widetilde{\mathbf{R}}[1: j-1, j] \quad$ \# macheps: $u_{\text {fine }}$
        5. Compute the sketched norm $\widetilde{\mathbf{R}}[j, j]=\left\|\widetilde{\mathbf{s}}_{j}^{\prime}\right\|$. \# macheps: $u_{\text {fine }}$
        6. Scale vector $\widetilde{\mathbf{s}}_{j}=\widetilde{\mathbf{s}}_{j}^{\prime} / \widetilde{\mathbf{R}}[j, j]$. \# macheps: $u_{\text {fine }}$
        7. Scale vector $\widetilde{\mathbf{q}}_{j}=\widetilde{\mathbf{q}}_{j}^{\prime} / \widetilde{\mathbf{R}}[j, j]$. \# macheps: $u_{\text {fine }}$
    end for
```

```
Algorithm 2.3 Steps 4-5 of RBGS: RCholeskyQR
    Given: $\mathbf{Q}_{(i)}^{\prime}, \Theta$.
    Output: QR fact. $\mathbf{Q}_{(i)}^{\prime}=\mathbf{Q}_{(i)} \mathbf{R}_{(i, i)}$, where $\mathbf{R}_{(i, i)}$ is upper triangular and $\mathbf{Q}_{(i)}$ is orthonormal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$; a sketch
    $\mathbf{S}_{(i)}=\Theta \mathbf{Q}_{(i)}$.
        4. Compute $\mathbf{S}_{(i)}^{\prime}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}^{\prime}$.
            Compute $\mathbf{R}_{(i, i)}$ as the $\mathrm{R}$ factor of $\ell_{2}$-QR factorization of $\mathbf{S}_{(i)}^{\prime}$.
            Compute $\mathbf{Q}_{(i)}=\mathbf{Q}_{(i)}^{\prime}\left(\mathbf{R}_{(i, i)}\right)^{-1}$ with forward substitution.
        5. Calculate the sketch of $\mathbf{Q}_{(i)}: \mathbf{S}_{(i)}=\Theta \mathbf{Q}_{(i)}$.
```


### 2.2.3 Implicit RCholeskyQR factorization

The efficiency of Algorithm 2.3 on distributed architectures can be improved by using the knowledge on how $\mathbf{Q}_{(i)}^{\prime}$ was computed in the RBGS algorithm. This in particular can help to overcome (or to postpone) one multiplication with $\boldsymbol{\Theta}$, as described by Algorithm 2.4. The numerical stability of such steps 4 and 5 of RBGS follows by induction from the stability of the regular RCholeskyQR, as explained in Section 3. It is then noticed that in RBGS that utilizes Algorithm 2.4 the sketch of $\mathbf{Q}_{(i)}$ in step 5 of iteration $i$ can be computed together with the sketch of $\mathbf{W}_{(i+1)}$ in step 1 of iteration $i+1$ with just one global synchronization between distributed processors. In this way, the resulting RBGS process would require only one global synchronization per iteration and therefore belongs to the "one-synchronization" family of BGS algorithms.

Remark 2.3 (Relation between RBGS of $\mathbf{W}$ and RCholeskyQR of $\mathbf{W}$ ). Notice that the RBGS algorithm utilizing Algorithm 2.4 in steps 4-5 computes the $\mathbf{Q}$ factor as $\mathbf{W R}^{-1}$ with forward substitution. This computation implies a strong

```
Algorithm 2.4 Steps 4-5 of RBGS: RCholeskyQR with postponed sketching step
    Given: $\mathbf{Q}_{(i)}^{\prime}, \boldsymbol{\Theta}$, and quantities $\mathbf{S}_{(1: i-1)}, \mathbf{R}_{(1: i-1, i)}, \mathbf{P}_{(i)}$.
    Output: QR fact. $\mathbf{Q}_{(i)}^{\prime}=\mathbf{Q}_{(i)} \mathbf{R}_{(i, i)}$, where $\mathbf{R}_{(i, i)}$ is upper triangular and $\mathbf{Q}_{(i)}$ is orthonormal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$; a sketch
    $\mathbf{S}_{(i)}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}$.
        4. Compute $\mathbf{S}_{(i)}^{\prime}=\mathbf{P}_{(i)}-\mathbf{S}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}$.
        Compute $\mathbf{R}_{(i, i)}$ as the $\mathrm{R}$ factor of $\ell_{2}$-QR factorization of $\mathbf{S}_{(i)}^{\prime}$.
        Compute $\mathbf{Q}_{(i)}=\mathbf{Q}_{(i)}^{\prime}\left(\mathbf{R}_{(i, i)}\right)^{-1}$ with forward substitution.
    5. Calculate the sketch of $\mathbf{Q}_{(i)}: \mathbf{S}_{(i)}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}$.
```

connection between RBGS of $\mathbf{W}$ and RCholeskyQR of $\mathbf{W}$. The difference is that in $R B G S$, the $R$ factor depends on the computed columns of the $Q$ factor, while in RCholeskyQR it is computed solely from the sketch of $\mathbf{W}$.

Moreover, as noticed in [3], RBGS would become numerically equivalent to RCholeskyQR if in step 5 of Algorithm 2.4 the sketch of $\mathbf{Q}_{(i)}$ would be obtained as $\mathbf{S}_{(i)}=\mathbf{S}_{(i)}^{\prime} \mathbf{R}_{(i, i)}^{-1}$ instead of $\mathbf{S}_{(i)}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}$. Indeed, according to [3], the RBGS algorithm with such a modified step 5 can be seen as a RCholeskyQR where the $\mathbf{R}_{(i, i)}$ factor is computed by BGS orthogonalization of $\mathbf{S}_{(i)}^{\prime}$, and the multiplication by $\mathbf{R}_{(i, i)}^{-1}$ is performed block columnwise. Such RCholeskyQR and RBGS have similar computational costs in terms of flops, memory, and parallelization. However, RBGS has two advantages. First, it should be more stable in practice as it provides an $R$ factor that accounts for errors made during the computation of $\mathbf{Q}$. Second, RBGS allows for efficient certification of the solution at each iteration of the algorithm. This certification can serve, for instance, as a criterion for re-orthogonalization or a restart of a Krylov solver.

### 2.2.4 $\quad \ell_{2}$-QR+RCholeskyQR factorization

Although the RCholeskyQR and RGS for inter-block orthogonalization already can provide great stability of RBGS algorithm (as shown in Section 3), this stability can be improved even more by running Algorithm 2.3 twice, or by combining it with modern efficient routines for $\ell_{2}$-orthogonalization as described next. Note that this should have only a minor impact on the overall cost of RBGS in standard sequential architecture but not in parallel.

The idea here is to improve the stability of RCholeskyQR by pre-processing the matrix $\mathbf{Q}_{(i)}^{\prime}$ with classical routines for $\ell_{2}$-orthogonalization, as shown in Algorithm 2.5. In principle, the pre-processing step can be done with any suitable routine for $\ell_{2}$-orthogonalization of tall-and-skinny matrices such as the classical or modified Gram-Schmidt with $l$ reorthogonalizations, Householder algorithm, the tall-and-skinny QR from [14] (TSQR) or any others. In each particular situation, the most suitable routine should be selected depending on the computational architecture and programming environment. For instance, because of their popularity and reliability, the Gram-Schmidt and Householder algorithms are often available in scientific libraries as greatly optimized high-level routines. On the other hand, TSQR is favorable in massively parallel environments, since it reduces the amount of messages/synchronizations between processors.

```
Algorithm 2.5 Steps 4-5 of RBGS: $\ell_{2}-\mathrm{QR}+$ RCholeskyQR factorization of $\mathbf{Q}_{(i)}^{\prime}$
    Given: $\mathbf{Q}_{(i)}^{\prime}, \Theta$.
    Output: QR fact. $\mathbf{Q}_{(i)}^{\prime}=\mathbf{Q}_{(i)} \mathbf{R}_{(i, i)}$, where $\mathbf{R}_{(i, i)}$ is upper triangular and $\mathbf{Q}_{(i)}$ is orthonormal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$; a sketch
    $\mathbf{S}_{(i)}=\Theta \mathbf{Q}_{(i)}$.
        4. Compute $\ell_{2}$ - $\mathrm{QR}$ fact. of $\mathbf{Q}_{(i)}^{\prime}: \mathbf{Q}_{(i)}^{\prime}=\mathbf{Q}^{*} \mathbf{R}^{\prime}$.
            Use Algorithm 2.3, providing $\mathbf{Q}^{*}$ as $\mathbf{Q}_{(i)}^{\prime}$, to compute RCholeskyQR $\mathbf{Q}^{*}=\mathbf{Q}_{(i)} \mathbf{R}^{\prime \prime}$.
            Calculate $\mathbf{R}_{(i, i)}=\mathbf{R}^{\prime \prime} \mathbf{R}^{\prime}$.
    5. Calculate the sketch of $\mathbf{Q}_{(i)}: \mathbf{S}_{(i)}=\boldsymbol{\Theta} \mathbf{Q}_{(i)}$.
```

It can be shown that Algorithm 2.5 produces a stable QR factorization satisfying (2.4) and (2.5), if it is computed with roundoff $u=u_{\text {fine }}=F(m, n) u_{c r s}$, where $F(m, n)$ is some low-degree polynomial.

## 3 Stability analysis

In this section we provide a rigorous stability analysis of the RBGS algorithm. In particular, we show that the RBGS algorithm has similar a priori as well as a posteriori stability guarantees as its single-vector counterpart (see [4, Section $3]$ ).

### 3.1 Assumptions

Our analysis will be based on the following assumptions. We first assume that $\Theta$ has a bounded norm as in (3.2). This condition is satisfied with probability at least $1-\delta$, if $\Theta$ is $(1 / 2, \delta / n, 1)$ oblivious subspace embedding (see Corollary 1.3). Let the matrix $\Delta \mathbf{Q}_{(i)}^{\prime}$ define the rounding error in step 3:

$$
\begin{equation*}
\boldsymbol{\Delta} \mathbf{Q}_{(i)}^{\prime}=\widehat{\mathbf{Q}}_{(i)}^{\prime}-\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \tag{3.1}
\end{equation*}
$$

Then, the standard worst-case scenario rounding analysis gives (3.3a). Furthermore, we assume that $\Theta$ also satisfies (3.3b) which, according to Corollary 1.5, holds under the probabilistic rounding model with probability at least $1-4 \delta$, if $\Theta$ is $\left(1 / 8, m^{-1}\binom{n}{d}^{-1} \delta, d\right)$ oblivious subspace embedding, with $d=\mathcal{O}(\log (m / \delta))$. In its turn this condition is met by Rademacher matrices with $k=\mathcal{O}(\log (n) \log (m / \delta))$ rows or SRHT with $k=\mathcal{O}\left(\log ^{2}(n) \log ^{2}(m / \delta)\right)$ rows.

Finally, it is assumed that in steps $4-5$, the QR factorization and the sketch of the Q factor satisfies (3.4) and (3.5) that can be attained by the algorithms from Section 2.2. The only exception is the implicit RCholeskyQR factorization from Section 2.2.3, which does not directly satisfy (3.5) and therefore requires slightly modified stability analysis of RBGS than other cases. Fortunately the stability guarantees of the version of RBGS that uses the implicit RCholeskyQR follow directly from the guarantees of the version of RBGS that uses the regular RCholeskyQR, as is shown next. We first notice that perturbing $\widehat{\mathbf{Q}}_{(i)}^{\prime}$ by some matrix that has (sketched) Frobenius norm $<u_{c r s} F(m)\|\mathbf{W}\|_{\mathrm{F}}$ will not change the stability guarantees from Section 3.2 and their proofs up to constants. Therefore in the RBGS based on the regular RCholeskyQR we can perturb $\widehat{\mathbf{Q}}_{(i)}^{\prime}$ to $\widehat{\mathbf{Q}}_{(i)}^{\prime \prime}$ so that this matrix has the sketch $\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime \prime}=\widehat{\mathbf{P}}_{(i)}-\widehat{\mathbf{S}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}$. Then it is noticed that at the last line of step 4 of the RCholeskyQR we can perturb the matrix $\widehat{\mathbf{Q}}_{(i)}^{\prime \prime}$ back to $\widehat{\mathbf{Q}}_{(i)}^{\prime}$ as this can increase $\operatorname{cond}\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right)$ only by $\mathcal{O}\left(u_{c r s} F(m)\|\mathbf{W}\|_{\mathrm{F}}\left\|\mathbf{R}_{(i, i)}\right\|_{\mathrm{F}}^{-1}\right)=\mathcal{O}\left(u_{c r s} \operatorname{cond}(\mathbf{W}) F(m)\right)$, which can be shown straightforwardly by following the proof of stability of RCholeskyQR in [3]. The argument is finished by noticing that the implicit RCholeskyQR can be viewed exactly as the regular RCholeskyQR with such two perturbations.

Assumptions 3.1. It is assumed that for some $\varepsilon \leq 1 / 2$,

$$
\begin{equation*}
\|\Theta\|_{F} \leq \sqrt{1+\varepsilon} \sqrt{n} \tag{3.2}
\end{equation*}
$$

Furthermore, we assume that

$$
\begin{align*}
\left|\Delta \mathbf{Q}_{(i)}^{\prime}\right| & \leq 1.02 u_{c r s}\left(\left|\widehat{\mathbf{W}}_{(i)}\right|+i m_{p}\left|\widehat{\mathbf{Q}}_{(1: i-1)}\right|\left|\widehat{\mathbf{R}}_{(1: i-1, i)}\right|\right)  \tag{3.3a}\\
\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \mathbf{Q}_{(i)}^{\prime}\right\|_{\mathrm{F}} & \leq 1.02 u_{c r s} \sqrt{1+\varepsilon}\left\|\left|\widehat{\mathbf{W}}_{(i)}\right|+i m_{p}\left|\widehat{\mathbf{Q}}_{(1: i-1)}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)} \mid\right\|_{\mathrm{F}}\right.\right. \tag{3.3b}
\end{align*}
$$

with $1 \leq i \leq p$ and $1 \leq t \leq m_{p}$. Finally, it is assumed that in steps 4-5 of Algorithm 2.1, we have

$$
\begin{align*}
& \left|\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right| \leq u_{\text {fine }} m\left(\left|\widehat{\mathbf{Q}}_{(i)}^{\prime}\right|+\left|\widehat{\mathbf{Q}}_{(i)}\right|\left|\widehat{\mathbf{R}}_{(i, i)}\right|\right) \leq 0.1 u_{c r s}\left|\widehat{\mathbf{Q}}_{(i)}\right|\left|\widehat{\mathbf{R}}_{(i, i)}\right|  \tag{3.4a}\\
& \left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right)\right\|_{\mathrm{F}} \leq u_{\text {fine }} m\|\boldsymbol{\Theta}\|_{\mathrm{F}}\left\|\left|\widehat{\mathbf{Q}}_{(i)}^{\prime}\right|+\left|\widehat{\mathbf{Q}}_{(i)}\left\|\widehat{\mathbf{R}}_{(i, i)} \mid\right\|\right.\right.  \tag{3.4b}\\
& \leq 0.1 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\|\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|
\end{align*}
$$

and, if $\boldsymbol{\Theta}$ is a $\varepsilon^{\prime}$-embedding for $\widehat{\mathbf{Q}}_{(i)}^{\prime}$,

$$
\begin{equation*}
1-0.1 u_{c r s} \operatorname{cond}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}\right) \leq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right) \leq \sigma_{\max }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right) \leq 1+0.1 u_{c r s} \operatorname{cond}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}\right) \tag{3.5}
\end{equation*}
$$

and $\left\|\widehat{\mathbf{Q}}_{(i)}\right\| \leq\left(1-\varepsilon^{\prime}\right)^{-1 / 2}\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right\|$.

### 3.2 Stability guarantees of RBGS algorithm

Our stability analysis will rely on the condition that $\boldsymbol{\Theta}$ satisfies the $\varepsilon$-embedding property for $\widehat{\mathbf{W}}$ and $\widehat{\mathbf{Q}}$. See Section 3.3 for a characterization of this property.

### 3.2.1 A posteriori analysis of RBGS algorithm

Let us first give an a posteriori characterization of the RBGS algorithm. Such characterization can be performed by measuring or bounding coefficients $\Delta^{(p)}=\left\|\mathbf{I}-\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}\right\|_{\mathrm{F}}$ and $\tilde{\Delta}^{(p)}=\frac{\|\widehat{\mathbf{P}}-\widehat{\mathbf{S}} \widehat{\mathbf{R}}\|_{\mathrm{F}}}{\| \text { 都 }}$, as done in $[4]$. We have the following result, which is an analogue of $[4$, Theorem 3.2] for RBGS.

Theorem 3.2. Consider Algorithm 2.1. Assume that

$$
100 m^{1 / 2} n^{3 / 2} u_{f i n e} \leq u_{c r s} \leq 0.01
$$

along with Assumptions 3.1, possibly excluding (3.5).

If $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\widehat{\mathbf{Q}}$ and $\widehat{\mathbf{W}}$, with $\varepsilon \leq 1 / 2$, and if $\Delta^{(p)}, \tilde{\Delta}^{(p)} \leq 0.1$, then the following inequalities hold:

$$
\begin{gathered}
(1+\varepsilon)^{-1 / 2}\left(1-\Delta^{(p)}-0.1 u_{c r s}\right) \leq \sigma_{\min }(\widehat{\mathbf{Q}}) \leq \sigma_{\max }(\widehat{\mathbf{Q}}) \leq(1-\varepsilon)^{-1 / 2}\left(1+\Delta^{(p)}+0.1 u_{c r s}\right) \\
\|\widehat{\mathbf{W}}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}}\|_{\mathrm{F}} \leq 4 u_{c r s} m^{3 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}
\end{gathered}
$$

Proof. See Section 6 .

Remark 3.3. In Theorem 3.2, we also have

$$
\|\boldsymbol{\Theta}(\widehat{\mathbf{W}}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}})\|_{\mathrm{F}} \leq 5 u_{c r s} m^{3 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}
$$

Theorem 3.2 implies the numerical stability of the RBGS algorithm if $\Delta^{(p)}$ and $\tilde{\Delta}^{(p)}$ are $\leq 0.1$. These coefficients can be efficiently computed a posteriori from the sketches $\widehat{\mathbf{S}}$ and $\widehat{\mathbf{P}}$ and the $\mathrm{R}$ factor $\widehat{\mathbf{R}}$, thus providing a way for the certification of the solution. Such certification, in particular, does not involve any assumptions on cond $(\widehat{\mathbf{W}})$, the accuracy of the least-squares solution in step 2, and the stability of inter-block orthogonalization in steps $4-5$. Furthermore, we would like to highlight the very mild condition $u_{c r s}=\mathcal{O}\left(m^{-3 / 2}\right)$ on the working (coarse) unit roundoff to guarantee the accuracy of the algorithm, which is in particular independent of the high-dimension $n$.

### 3.2.2 A priori analysis of RBGS algorithm

Clearly, to get a priori bounds for $\Delta^{(p)}=\left\|\mathbf{I}-\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}\right\|_{\mathrm{F}}$ and $\tilde{\Delta}^{(p)}=\frac{\|\widehat{\mathbf{P}}-\widehat{\mathbf{S}} \widehat{\mathbf{R}}\|_{\mathrm{F}}}{\|\widehat{\mathbf{P}}\|_{\mathbf{F}}}$ we need more assumptions than those stated in Theorem 3.2. In particular, it is necessary to impose a stability condition on the least squares solver used in step 2 . We also need $\widehat{\mathbf{W}}$ to be numerically full rank, i.e., to satisfy $u_{c r s} \leq \mathcal{O}\left(\operatorname{cond}(\widehat{\mathbf{W}})^{-1}\right)$.

The following a priori guarantee of stability of the RBGS algorithm is, in fact, an analogue of [4, Theorem 3.3] for the single-vector RGS algorithm.

Theorem 3.4. Consider Algorithm 2.1 with a backward-stable solver (e.g., based on Richardson iterations) satisfying Assumptions 2.2.

Under Assumptions 3.1, assume that $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\widehat{\mathbf{Q}}_{(1: p-1)}$ and $\widehat{\mathbf{W}}$, with $\varepsilon \leq 1 / 2$. If

$$
u_{\text {crs }} \leq 10^{-3} \operatorname{cond}(\widehat{\mathbf{W}})^{-1} m^{-2} \text { and } u_{\text {fine }} \leq 10^{-2} m^{-1 / 2} n^{-3 / 2} u_{c r s}
$$

then $\Delta^{(p)}$ and $\tilde{\Delta}^{(p)}$ are bounded by

$$
\begin{align*}
& \tilde{\Delta}^{(p)} \leq 4.2 u_{c r s} m^{3 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}} /\|\widehat{\mathbf{P}}\|_{\mathrm{F}} \leq 6 u_{c r s} m^{3 / 2}  \tag{3.7}\\
& \Delta^{(p)} \leq 20 u_{c r s} m^{2} \operatorname{cond}(\widehat{\mathbf{W}}) \tag{3.8}
\end{align*}
$$

Proof. See Section 6 .

Theorem 3.4 states that the RBGS algorithm is stable unless the input matrix $\widehat{\mathbf{W}}$ is numerically rank-deficient. This stability guarantee is seen in other stable deterministic algorithms such as MGS, CGS2, BCGS2, and others. Furthermore, the stability is proven for the working unit roundoff independent of the high dimension $n$. This unique feature of randomized algorithms can be especially interesting for large-scale problems solved on low-precision arithmetic architectures.

### 3.3 Epsilon embedding property

The stability analysis in Section 3 holds if $\boldsymbol{\Theta}$ satisfies the $\varepsilon$-embedding property for $\widehat{\mathbf{Q}}$ and $\widehat{\mathbf{W}}$. In this section we analyze this property.

We consider the case when $\widehat{\mathbf{W}}$ and $\boldsymbol{\Theta}$ are independent of each other. Then, if $\boldsymbol{\Theta}$ is a $(\varepsilon, \delta, m)$ oblivious $\ell_{2}$-subspace embedding, it satisfies the $\varepsilon$-embedding property for $\widehat{\mathbf{W}}$ with high probability. Below, we show that, in this case $\Theta$ will also satisfy the $\varepsilon$-embedding property for $\widehat{\mathbf{Q}}$ with moderately increased value of $\varepsilon$. This result is basically the RBGS counterpart of $[4$, Proposition 3.6].

Proposition 3.5. Consider Algorithm 2.1 with a backward-stable solver satisfying Assumptions 2.2,

$$
u_{\text {crs }} \leq 10^{-3} \operatorname{cond}(\widehat{\mathbf{W}})^{-1} m^{-2} \text { and } u_{\text {fine }} \leq 10^{-2} m^{-1 / 2} n^{-3 / 2} u_{\text {crs }}
$$

Under Assumptions 3.1, if $\boldsymbol{\Theta}$ is a $\varepsilon$-embedding for $\widehat{\mathbf{W}}$, with $\varepsilon \leq 1 / 4$, then it satisfies the $\varepsilon^{\prime}$-embedding property for $\widehat{\mathbf{Q}}$ with $\varepsilon^{\prime}=2 \varepsilon+180 u_{c r s} m^{2} \operatorname{cond}(\widehat{\mathbf{W}})$.

Proof. See Section 6 .

The $\varepsilon$-embedding property will likely hold even when matrix $\widehat{\mathbf{W}}$ depends on $\Theta$. In such a case the quality of $\Theta$ can be certified a posteriori by computing additional sketches $\boldsymbol{\Phi} \widehat{\mathbf{Q}}$ and $\boldsymbol{\Phi} \widehat{\mathbf{W}}$, associated with a new sketching matrix $\boldsymbol{\Phi}$, in addition to the sketches $\Theta \widehat{\mathbf{Q}}$ and $\Theta \widehat{\mathbf{W}}$. Then one may characterize the quality of $\Theta$ by measuring the orthogonality of $(\boldsymbol{\Theta} \widehat{\mathbf{W}}) \mathbf{X}$ and $(\boldsymbol{\Theta} \widehat{\mathbf{Q}}) \mathbf{Y}$, where $\mathbf{X}$ and $\mathbf{Y}$ are inverses of $\mathrm{R}$ factors of $\boldsymbol{\Phi} \widehat{\mathbf{W}}$ and $\boldsymbol{\Phi} \widehat{\mathbf{Q}}$, as is described in [4, Propositions 3.6-3.8] extrapolated from [7]. For efficiency in terms of cache or communication, at each iteration, the products $\boldsymbol{\Phi} \widehat{\mathbf{W}}_{(i)}$ and $\boldsymbol{\Phi} \widehat{\mathbf{Q}}_{(i)}$ can be computed together with $\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(i)}$ in step 2 , and $\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}$ in steps $4-5$, respectively.

## 4 Randomized block Krylov methods

In this section we discuss practical applications of the methodology. We particularly focus on improving the efficiency of popular block Krylov methods, such as the block GMRES and FOM, and the RR method, for solving block linear systems of the form $\mathbf{A X}=\mathbf{B}$ and eigenvalue problems of the form $\mathbf{A X}=\mathbf{X} \boldsymbol{\Lambda}$, where $\mathbf{A}$ is large, possibly non-symmetric $n \times n$ matrix, $\mathbf{B}$ is $n \times m_{p}$ matrix, and $\boldsymbol{\Lambda}$ is $m_{p} \times m_{p}$ diagonal matrix of the extreme eigenvalues of $\mathbf{A}$.

The block Krylov methods proceed with approximation of $\mathbf{X}$ by a projection $\mathbf{X}^{(j)}$ onto the Krylov space $\mathcal{K}^{(j)}(\mathbf{A}, \mathbf{B})$, which is defined as

$$
\mathcal{K}^{(j)}(\mathbf{A}, \mathbf{B}):=\operatorname{span}\left\{\mathbf{B}, \mathbf{A B}, \ldots, \mathbf{A}^{j-1} \mathbf{B}\right\}
$$

with $j$ being the order of the subspace.

The GMRES method computes $\mathbf{X}^{(j)}$ that minimizes the Frobenius norm of the residual, while the FOM and RR methods seek $\mathbf{X}^{(j)}$ that is optimal in the Galerkin sense. Both kinds of methods first construct an orthonormal basis for $\mathcal{K}^{(p)}(\mathbf{A}, \mathbf{B})$ with GS orthogonalization, called Arnoldi iteration, and then determine the coordinates of the columns of $\mathbf{X}^{(j)}$ in the computed basis.

### 4.1 Krylov basis computation: randomized block Arnoldi iteration

The Arnoldi algorithm produces orthonormal matrix $\mathbf{Q}=\mathbf{Q}_{(1: p)}$ satisfying the Arnoldi identity $\mathbf{A Q}_{(1: p-1)}=\mathbf{Q}_{(1: p)} \mathbf{H}$, where $\mathbf{H}=\mathbf{H}_{(1: p, 1: p-1)}$ is block upper Hessenberg matrix. The Arnoldi algorithm can be viewed as a block-wise QR factorization of matrix $\left[\mathbf{B}, \mathbf{A Q}_{(1: p-1)}\right]$. In this context, the Arnoldi matrix $\mathbf{H}$ can be seen as the $\mathbf{R}$ factor $\mathbf{R}=\mathbf{R}_{(1: p, 1: p)}$ without the first column of subblocks, that is, as $\mathbf{R}_{(1: p, 2: p)}$.

Below, we propose a randomized Arnoldi process based on the RBGS algorithm. Note that unlike standard methods, Algorithm 4.1 produces a Krylov basis orthonormal with respect to the sketched product $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$.

```
Algorithm 4.1 RBGS-Arnoldi algorithm
    Given: $n \times n$ matrix $\mathbf{A}, n \times m_{p}$ matrix $\mathbf{B}, k \times n$ matrix $\boldsymbol{\Theta}$ with $k \ll n$, parameter $p$.
    Output: $n \times m$ factor $\mathbf{Q}=\mathbf{Q}_{(1: p)}$ and $m \times m$ upper triangular factor $\mathbf{R}=\mathbf{R}_{(1: p, 1: p)}$.
    1. Set $\mathbf{W}_{(1)}=\mathbf{B}$ and perform 1-st iteration of the RBGS algorithm.
    for $i=2: p$ do
        2. Compute $\mathbf{W}_{(i)}=\mathbf{A Q}_{(i-1)}$.
        3. Perform $i$-th iteration of the RBGS algorithm.
    end for
    4. (Optional) compute $\Delta^{(p)}$ and $\tilde{\Delta}^{(p)}$. Use Theorem 3.2 to certify the output.
```

In step 2 of Algorithm 4.1, the computation of the matrix-vector product can be executed either with roundoff $u_{\text {fine }}$ or $u_{\text {crs }}$ depending on the situation. The stability guarantees of Algorithm 4.1 can be obtained directly from Theorems 3.2 and 3.4 with standard stability analysis similar to that from [4, Section 4.1]. In particular, it can be shown that, under the stability conditions of RBGS, the computed $\widehat{\mathbf{Q}}$ and $\widehat{\mathbf{H}}$ satisfy

$$
\begin{equation*}
(\mathbf{A}+\Delta \mathbf{A}) \widehat{\mathbf{Q}}_{(1: p-1)}=\widehat{\mathbf{Q}}_{(1: p)} \widehat{\mathbf{H}} \tag{4.1}
\end{equation*}
$$

with $\|\boldsymbol{\Delta} \mathbf{A}\|$ close to machine precision, and $\operatorname{cond}(\widehat{\mathbf{Q}})=\mathcal{O}(1)$. We leave the precise analysis of this fact outside of the scope of this manuscript.

### 4.2 Linear systems: randomized block GMRES method

Let us now discuss solving block linear systems $\mathbf{A X}=\mathbf{B}$ with GMRES.

The GMRES method computes the approximate solution $\mathbf{X}^{(p-1)}=\mathbf{U}=\left[\mathbf{u}_{1}, \ldots, \mathbf{u}_{m_{p}}\right]$ :

$$
\begin{equation*}
\mathbf{u}_{i}=\widehat{\mathbf{Q}}_{(1: p-1)} \arg \min _{\mathbf{z}}\left\|\widehat{\mathbf{H}} \mathbf{z}-\widehat{\mathbf{R}}_{(1: p, 1)} \mathbf{e}_{i}\right\| \tag{4.2}
\end{equation*}
$$

performed in sufficient precision, where $\mathbf{e}_{i}$ is the $i$-th column of the identity matrix. Let us now characterize quasioptimality of such a projection when $\widehat{\mathbf{Q}}$ and $\widehat{\mathbf{H}}$ were obtained with the RBGS-Arnoldi algorithm. Under the stability conditions of RBGS, the Arnoldi identity (4.1) implies that

$$
\left\|(\mathbf{A}+\boldsymbol{\Delta} \mathbf{A}) \widehat{\mathbf{Q}}_{(1: p-1)} \mathbf{z}-\mathbf{b}_{i}\right\|=\left\|\widehat{\mathbf{Q}}_{(1: p)}\left(\widehat{\mathbf{H}} \mathbf{z}-\widehat{\mathbf{R}}_{(1: p, 1)} \mathbf{e}_{i}\right)\right\| \leq\|\widehat{\mathbf{Q}}\|\left\|\widehat{\mathbf{H}} \mathbf{z}-\widehat{\mathbf{R}}_{(1: p, 1)} \mathbf{e}_{i}\right\|
$$

and, similarly,

$$
\left\|(\mathbf{A}+\mathbf{\Delta} \mathbf{A}) \widehat{\mathbf{Q}}_{(1: p-1)} \mathbf{z}-\mathbf{b}_{i}\right\| \geq \sigma_{\min }(\widehat{\mathbf{Q}})\left\|\widehat{\mathbf{H}} \mathbf{z}-\widehat{\mathbf{R}}_{(1: p, 1)} \mathbf{e}_{i}\right\|
$$

These two relations imply that

$$
\left\|(\mathbf{A}+\Delta \mathbf{A}) \mathbf{u}_{i}-\mathbf{b}_{i}\right\| \leq \operatorname{cond}(\widehat{\mathbf{Q}}) \min _{\mathbf{v} \in Q_{p-1}}\left\|(\mathbf{A}+\Delta \mathbf{A}) \mathbf{v}-\mathbf{b}_{i}\right\|
$$

with $Q_{p-1}=\operatorname{range}\left(\widehat{\mathbf{Q}}_{(1: p-1)}\right)=\mathcal{K}_{p-1}(\mathbf{A}+\boldsymbol{\Delta} \mathbf{A}, \mathbf{B})$. Consequently, the randomized version of GMRES provides a solution which minimizes the norm of the residual associated with a slightly perturbed matrix, up to a factor of order 1 .

### 4.3 Linear systems: randomized block FOM method

In contrast to GMRES, the FOM method obtains solution $\mathbf{X}^{(p-1)}=\mathbf{U}=\left[\mathbf{u}_{1}, \ldots, \mathbf{u}_{m_{p}}\right]$ by imposing a Galerkin orthogonality condition on the residuals (in exact arithmetic):

$$
\begin{equation*}
\left\langle\mathbf{v}, \mathbf{r}\left(\mathbf{u}_{i}, \mathbf{b}_{i}\right)\right\rangle=0, \quad \forall \mathbf{v} \in Q_{p-1}, 1 \leq i \leq m_{p} \tag{4.3}
\end{equation*}
$$

where $Q_{p-1}=\operatorname{range}\left(\mathbf{Q}_{(1: p-1)}\right)$ and $\mathbf{r}\left(\mathbf{u}_{i}, \mathbf{b}_{i}\right)=\mathbf{A} \mathbf{u}_{i}-\mathbf{b}_{i}$ is the residual associated with the $i$-th right-hand-side. The Galerkin projection may be a more appropriate choice than the minimal-residual projection provided by GMRES, when the quality of the solution is measured with an energy error rather than the residual error. When $\mathbf{Q}_{(1: p-1)}$ is obtained with the traditional Arnoldi method, the solution to (4.3) is given by

$$
\begin{equation*}
\mathbf{U}=\mathbf{Q}_{(1: p-1)}\left(\mathbf{H}_{(1: p-1,1: p-1)}\right)^{-1} \mathbf{R}_{(1: p-1,1)} \tag{4.4}
\end{equation*}
$$

Suppose now that $\mathbf{Q}_{(1: p-1)}$ was obtained by the randomized Arnoldi method. Then, by noticing that

$$
\mathbf{H}_{(1: p-1,1: p-1)}=\left[\mathbf{I} \mathbf{0}_{(1: p-1, p)}\right] \mathbf{H}=\left(\boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)}\right)^{\mathrm{T}} \boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)} \mathbf{H}=\left(\boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)}\right)^{\mathrm{T}} \boldsymbol{\Theta} \mathbf{A} \mathbf{Q}_{(1: p-1)}
$$

where $\mathbf{0}_{(1: p-1, p)}$ is a null matrix of size of $\mathbf{H}_{(p, 1: p-1)}^{\mathrm{T}}$, we deduce that the solution (4.4) satisfies

$$
\begin{equation*}
\left\langle\boldsymbol{\Theta} \mathbf{v}, \boldsymbol{\Theta r}\left(\mathbf{u}_{i}, \mathbf{b}_{i}\right)\right\rangle=0, \quad \forall \mathbf{v} \in Q_{p-1}, 1 \leq i \leq m_{p} \tag{4.5}
\end{equation*}
$$

The relation (4.5) can be viewed as the sketched version of the Galerkin orthogonality condition. To our knowledge, it was first used in model order reduction community to obtain a reduced-basis solution of parametric linear systems [6]. By using similar considerations as in [6], one can show that (4.5) preserves the quality of the classical Galerkin projection when $\Theta$ is a $\varepsilon$-embedding for $Q_{p}$ with $\varepsilon \operatorname{cond}(\mathbf{A})<1$, though, in practice, this condition can be too pessimistic [6]. We leave the further development of the randomized FOM method for future research.

Remark 4.1. It should be noted that the reasoning of Sections 4.2 and 4.3 could be reversed. We could first derive the sketched minimal-residual projection $\mathbf{u}_{i}=\arg \min _{\mathbf{v} \in Q_{p-1}}\left\|\Theta \mathbf{r}\left(\mathbf{v}, \mathbf{b}_{i}\right)\right\|$ and sketched Galerkin orthogonality condition (4.5) by replacing the $\ell_{2}$-inner products and $\ell_{2}$-norms in the standard minimal-residual equation $\mathbf{u}_{i}=\arg \min _{\mathbf{v} \in Q_{p-1}}\left\|\mathbf{r}\left(\mathbf{v}, \mathbf{b}_{i}\right)\right\|$ and the standard Galerkin orthogonality condition (4.3) by sketched ones, similarly as was done in [6, 7]. After that, it could be realized that the solution to the sketched minimal-residual and Galerkin equations can be obtained by orthogonalizing the Krylov basis with respect to $\langle\boldsymbol{\Theta} \cdot \boldsymbol{\Theta} \cdot\rangle$ and using the classical identities (4.2) and (4.4).

### 4.4 Eigenvalue problems: randomized RR method

Next, we consider the computation of the extreme eigenpairs of A. Note that the methodology proposed below can be readily used for finding eigenpairs in the desired region by introducing a shift to the matrix. In addition, our methodology can be extended to inverse methods by replacing $\mathbf{A}$ with $\mathbf{A}^{-1}$. In this case, the application of the inverse can be readily performed with (possibly preconditioned) randomized block GMRES method from Section 4.2 or other efficient methods.

To simplify the presentation, the analysis of the RR method and its randomized version will be provided only in infinite precision arithmetic. The stability of these approaches follows directly from the stability of the Arnoldi iteration.

Let $\mathbf{Q}=\mathbf{Q}_{(1: p)}$ be a Krylov basis generated with the standard or randomized block-Arnoldi algorithm. The standard RR method, based on the Arnoldi iteration, approximates the extreme eigenpairs $(\lambda, \mathbf{x})$ of $\mathbf{A}$ by

$$
\begin{equation*}
(\lambda, \mathbf{x}) \approx(\mu, \mathbf{u})=\left(\mu, \mathbf{Q}_{(1: p-1)} \mathbf{y}\right) \tag{4.6}
\end{equation*}
$$

where $(\mu, \mathbf{y})$ are the corresponding extreme eigenpairs of $\mathbf{H}_{(1: p-1,1: p-1)}$. Furthermore the residual error of an approximate eigenpair $(\mu, \mathbf{u})$ is estimated by $\left\|\mathbf{H}_{(p, 1: p-1)} \mathbf{y}\right\|$.

For the standard block-Arnoldi algorithm, we have

$$
\mathbf{H}_{(1: p-1,1: p-1)}=\left[\mathbf{I} \mathbf{0}_{(1: p-1, p)}\right] \mathbf{H}=\left(\mathbf{Q}_{(1: p-1)}\right)^{\mathrm{T}} \mathbf{Q} \mathbf{H}=\left(\mathbf{Q}_{(1: p-1)}\right)^{\mathrm{T}} \mathbf{A} \mathbf{Q}_{(1: p-1)}
$$

and therefore

$$
\begin{equation*}
\langle\mathbf{v}, \mathbf{r}(\mathbf{u}, \mu \mathbf{u})\rangle=0, \quad \forall \mathbf{v} \in Q_{p-1} \tag{4.7}
\end{equation*}
$$

where $Q_{p-1}:=\operatorname{range}\left(\mathbf{Q}_{(1: p-1)}\right)$ and $\mathbf{r}(\mathbf{u}, \mu \mathbf{u})=\mathbf{A u}-\mu \mathbf{u}$ is the residual associated with $(\mu, \mathbf{u})$. The relation (4.7) is known as the Galerkin orthogonality condition. At the same time, we have

$$
\|\mathbf{r}(\mu, \mathbf{u})\| \geq \sigma_{\min }(\mathbf{Q})\left\|\mathbf{H y}-\mu\left[\begin{array}{l}
\mathbf{y}  \tag{4.8a}\\
\mathbf{0}
\end{array}\right]\right\|=\sigma_{\min }(\mathbf{Q})\left\|\mathbf{H}_{(p, 1: p-1)} \mathbf{y}\right\|
$$

and, similarly,

$$
\begin{equation*}
\|\mathbf{r}(\mu, \mathbf{u})\| \leq \sigma_{\max }(\mathbf{Q})\left\|\mathbf{H}_{(p, 1: p-1)} \mathbf{y}\right\| \tag{4.8b}
\end{equation*}
$$

Since, the classical Arnoldi iteration produces an $\ell_{2}$-orthogonal $Q$ factor, hence in this case, the quantity $\left\|\mathbf{H}_{(p, 1: p-1)} \mathbf{y}\right\|$ represents exactly the residual error of $(\mu, \mathbf{u})$.

On the other hand, the randomized block-Arnoldi algorithm produces a matrix $\mathbf{Q}$ orthogonal with respect to $\langle\boldsymbol{\Theta} \cdot, \boldsymbol{\Theta} \cdot\rangle$. This implies that

$$
\mathbf{H}_{(1: p-1,1: p-1)}=\left[\mathbf{I} \mathbf{0}_{(1: p-1, p)}\right] \mathbf{H}=\left(\boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)}\right)^{\mathrm{T}}\left(\boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)}\right) \mathbf{H}=\left(\boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)}\right)^{\mathrm{T}} \boldsymbol{\Theta} \mathbf{A} \mathbf{Q}_{(1: p-1)}
$$

or equivalently, that $(\mu, \mathbf{u})$ satisfies the following sketched version of the Galerkin orthogonality condition:

$$
\begin{equation*}
\langle\boldsymbol{\Theta} \mathbf{v}, \boldsymbol{\Theta} \mathbf{r}(\mathbf{u}, \mu \mathbf{u})\rangle=0, \quad \forall \mathbf{v} \in Q_{p-1} \tag{4.9}
\end{equation*}
$$

similar to the sketched Galerkin condition for linear systems in Section 4.3. Unlike for the sketched minimal-residual projection in GMRES, the optimality for the sketched Galerkin projection unfortunately does not trivially follow from the $\varepsilon$-embedding property of $\Theta$. A characterization of the accuracy of this projection should be derived by reformulating the methodology in terms of projection operators similarly to state-of-the-art analysis provided, for instance, in [23, Section 4.3]. In particular, we notice that the Galerkin orthogonality condition (4.7) can be expressed as

$$
\boldsymbol{\Pi}_{Q_{p-1}} \mathbf{A} \Pi_{Q_{p-1}} \mathbf{u}=\mu \mathbf{u}
$$

where $\Pi_{Q_{p-1}}$ is the $\ell_{2}$-orthogonal projector onto $Q_{p-1}$. In other words, the classical $\mathrm{RR}$ method can be interpreted as approximation of eigenpairs of $\mathbf{A}$ by the eigenpairs of approximate operator $\boldsymbol{\Pi}_{Q_{p-1}} \mathbf{A} \Pi_{Q_{p-1}}$. Similarly, the sketched Galerkin orthogonality condition (4.9) can be expressed as

$$
\boldsymbol{\Pi}_{Q_{p-1}}^{\Theta} \mathbf{A} \Pi_{Q_{p-1}}^{\Theta} \mathbf{u}=\mu \mathbf{u}
$$

where $\boldsymbol{\Pi}_{Q_{p-1}}^{\Theta}=\mathbf{Q}_{(1: p-1)}\left(\boldsymbol{\Theta} \mathbf{Q}_{(1: p-1)}\right)^{\dagger} \boldsymbol{\Theta}$ is an orthogonal projector onto $Q_{p-1}$ with respect to the sketched inner product $\langle\boldsymbol{\Theta}, \boldsymbol{\Theta} \cdot\rangle$. We see that the randomized RR method corresponds to taking the approximate operator as $\boldsymbol{\Pi}_{Q_{p-1}}^{\Theta} \mathbf{A} \boldsymbol{\Pi}_{Q_{p-1}}^{\Theta}$ instead of classical $\Pi_{Q_{p-1}} \mathbf{A} \Pi_{Q_{p-1}}$. This connection can be used to extrapolate the results from [23, Section 4.3], such as Theorem 4.3 that bounds the residual error of the exact eigenpair with respect to the approximate operator, from classical methods to their sketched variants. For a more detailed discussion, please refer to the supplementary materials.

Note that, independently of this work, the sketched Galerkin orthogonality condition was also used by Nakatsukasa and Tropp in their recent paper $[21]$.

Since the $\varepsilon$-embedding property of $\Theta$ implies that $\operatorname{cond}(\mathbf{Q})=1+\mathcal{O}(\varepsilon)$, hence according to (4.8), the quantity $\left\|\mathbf{H}_{(p, 1: p-1)} \mathbf{y}\right\|$ estimates well the residual error.

The RR algorithm based on RBGS-Arnoldi with restarting is depicted in Algorithm 4.2.

Remark 4.2. When the classical Galerkin projection is preferable to its sketched variant, say, because of its strong optimality properties for symmetric (or Hermitian) operators, the RBGS-Arnoldi algorithm can be used to obtain this projection instead of the sketched one. For this the $\ell_{2}$-orthogonal Krylov basis matrix $\mathbf{Q}$ and the Arnoldi matrix $\mathbf{H}$ in (4.6) can be obtained with the RBGS-Arnoldi algorithm followed by an additional Cholesky QR step, as is depicted in Remark 2.1. In particular, this situation can be accounted for in Algorithm 4.2 by adding the following line between step 1 and step 2: "Compute $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ and obtain its Cholesky factor $\mathbf{R}^{\prime}$. Set $\mathbf{Q} \leftarrow \mathbf{Q R}^{\prime-1}, \mathbf{R} \leftarrow \mathbf{R}^{\prime} \mathbf{R}$." The cost of this additional step is dominated by the computation of product $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ (since we can omit computing $\mathbf{Q R}^{\prime-1}$ ), which requires similar number of flops as the entire RBGS-Arnoldi algorithm. Nevertheless, as a single BLAS3 operation, it is very well suited to modern computational architectures.

```
Algorithm 4.2 Rand. RR algorithm for extreme eigenpairs with restarting
    Given: $n \times n, n \times m_{p}, k \times n$ matrices $\mathbf{A}, \mathbf{B}$, and $\boldsymbol{\Theta}$ with $m \leq k \ll n$, param. $p$ and $N_{\text {iter }}$.
    Output: $\boldsymbol{\Lambda}$ and $\mathbf{X}$.
    for $i=1: N_{\text {iter }}$ do
        1. Perform RBGS-Arnoldi Algorithm 4.1 returning $\mathbf{Q}$ and $\mathbf{R}$. Set $\mathbf{H}=\mathbf{R}_{(1: p, 2: p)}$.
        2. Compute diagonal matrix $\boldsymbol{\Lambda}$ with $m_{p}$ extreme eigenvalues of $\mathbf{H}_{(1: p-1,1: p-1)}$
    on the diagonal, and the matrix of associated eigenvectors $\mathbf{Y}$.
        3. (Optional) Compute $\left\|\mathbf{H}_{(p, 1: p-1)} \mathbf{y}\right\|$ to characterize the approximation error.
        4. Compute $\mathbf{B}=\mathbf{Q}_{(1: p-1)} \mathbf{Y}$.
    end for
    5. Normalize $\mathbf{B}$ with respect to $\ell_{2}$-norm and return it as $\mathbf{X}$.
```


### 4.5 Further applications

### 4.5.1 $s$-step Krylov methods

The proposed RBGS algorithm can be used to improve the $s$-step Krylov methods [18]. For simplicity, consider the case of a linear system with only one right-hand side or an eigenvalue problem approximated by a Krylov space associated with only one generating vector, i.e., taking $\mathbf{B}=\mathbf{b}$. Then the goal is to compute a basis for $\mathcal{K}^{(m)}(\mathbf{A}, \mathbf{b})$ satisfying the Arnoldi identity. With this basis, an approximate solution of a linear system or an eigenvalue problem can be obtained using the identities from Sections 4.2 to 4.4 , setting $\mathbf{B}=\mathbf{b}$.

The $s$-step Krylov methods use efficient matrix power kernels that can output a matrix $F_{s+1}(\mathbf{v})$ of basis vectors for $\mathcal{K}^{(s+1)}(\mathbf{A}, \mathbf{v})$, of the form

$$
F_{s+1}(\mathbf{v})=\left[p_{0}(\mathbf{A}) \mathbf{v}, p_{1}(\mathbf{A}) \mathbf{v}, \ldots, p_{s}(\mathbf{A}) \mathbf{v}\right]
$$

where $\mathbf{v}$ is some input vector, $p_{0}(\mathbf{A})$ is usually taken as $\mathbf{I}$, and $p_{1}(\mathbf{A}), \ldots p_{s}(\mathbf{A})$ are some suitable polynomials that aim to make $F_{s+1}(\mathbf{v})$ not too badly conditioned. Popular options for $p_{0}(\mathbf{A}), \ldots p_{s}(\mathbf{A})$ are the monomials and Newton, or Chebyshev polynomials. A relatively large $s$ (say $s=10$ or 30 [18]) may lead to stability problems even when a polynomial basis is used. Consequently, to obtain a basis for $\mathcal{K}^{(m)}(\mathbf{A}, \mathbf{b})$ of dimension suitable for practical applications, it becomes necessary to proceed with a block-wise generation, at each iteration, computing a small block of a Krylov basis by using the matrix power kernel and subsequently orthogonalizing it against the previously computed vectors with a BGS approach. In more concrete terms, we have to perform a block-wise $\mathrm{QR}$ factorization of the matrix $\mathbf{W}=\mathbf{W}_{(1: p)}$ generated recursively as

$$
\mathbf{W}_{(i)}=\left\{\begin{array}{cc}
\mathbf{b} & \text { if } i=1 \\
F_{s+1}^{*}\left(\mathbf{q}_{s(i-2)+1}\right) & \text { if } i=2, \ldots, p
\end{array}\right.
$$

where $F_{s+1}^{*}(\mathbf{v})$ corresponds to the matrix $F_{s+1}(\mathbf{v})$ without the first column, and $\mathbf{q}_{s(i-2)+1}$ is the last Krylov basis vector computed at iteration $i-1$. Such block-wise orthogonalization can readily be done with the RBGS algorithm, as is depicted in Algorithm 4.3, with all the computational benefits of this algorithm.

```
Algorithm $4.3 s$-step RBGS-Arnoldi algorithm
    Given: $n \times n$ matrix $\mathbf{A}, n \times 1$ vector $\mathbf{b}, k \times n$ matrix $\boldsymbol{\Theta}$ with $k \ll n$, parameters $p$ and $s$.
    Output: $n \times m$ Krylov basis matrix $\mathbf{Q}=\mathbf{Q}_{(1: p)}$ and Arnoldi matrix $\mathbf{H}$.
    1. Set $\mathbf{W}_{(1)}=\mathbf{b}$ and perform 1-st iteration of the RBGS algorithm.
    for $i=2: p$ do
        2. Compute $\mathbf{W}_{(i)}=F_{s+1}^{*}\left(\mathbf{q}_{s(i-2)+1}\right)$ and perform $i$-th iteration of the RBGS algorithm.
    end for
    3. Obtain $\mathbf{H}=\mathbf{S}^{\dagger}\left(\boldsymbol{\Theta} \mathbf{A Q} \mathbf{Q}_{(1 ; p-1)}\right)$ by $\mathrm{QR}$ or SVD of $\mathbf{S}$, or solving a small least-squares problem. (Optional) set the below-
    subdiagonal elements of $\mathbf{H}$ to zeros.
    4. (Optional) compute $\Delta^{(p)}$ and $\tilde{\Delta}^{(p)}$. Use Theorem 3.2 to certify the output.
```

To obtain the Arnoldi matrix $\mathbf{H}$ in step 3, Algorithm 4.3 utilizes the sketch $\mathbf{C}:=\boldsymbol{\Theta} \mathbf{A Q}_{(1: p-1)}$. Computing $\mathbf{C}$ should incur only minor costs in terms of flops and communication. It can be beneficial to compute $\boldsymbol{\Theta}\left(\mathbf{A Q} \mathbf{Q}_{(i-1)}\right)$ together with
$\boldsymbol{\Theta} \mathbf{W}_{(i)}$ at the $i$-th iteration of the RBGS algorithm. Alternatively, $\mathbf{C}$ can be obtained through a recursive procedure without any high-dimensional operations. At iteration $i \geq 2$, we can first compute $\mathbf{c}_{s(i-2)+1}$ and $\boldsymbol{\Theta} \mathbf{A} \overline{\mathbf{W}_{(i)}}$, where $\overline{\mathbf{W}_{(i)}}$ represents $\mathbf{W}_{(i)}$ without the last column, by using the following relation: $\left[\mathbf{c}_{s(i-2)+1} \boldsymbol{\Theta} \mathbf{A} \overline{\mathbf{W}_{(i)}}\right]=\left[\mathbf{s}_{s(i-2)+1} \mathbf{P}_{(i)}\right] \mathbf{T}_{s+1}$. Here, $\mathbf{P}_{(i)}$ and $\mathbf{s}_{s(i-2)+1}$ denote the sketches of $\mathbf{W}_{(i)}$ and $\mathbf{q}_{s(i-2)+1}$, respectively, computed during the $i$-th iteration of the RBGS algorithm, and $\mathbf{T}_{s+1}$ is the Arnoldi factor associated with the matrix power kernel $F_{s+1}(\mathbf{v})$, which satisfies $\mathbf{A} F_{s}(\mathbf{v})=F_{s+1}(\mathbf{v}) \mathbf{T}_{s+1}$. Then, the fact that $\boldsymbol{\Theta} \mathbf{A} \mathbf{Q}_{(i)}=\boldsymbol{\Theta}\left(\mathbf{W}_{(i)}-\mathbf{Q}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}\right) \mathbf{R}_{(i, i)}^{-1}$ enables us to obtain the next $s-1$ columns of $\mathbf{C}$ as follows:

$$
\left[\mathbf{c}_{s(i-2)+2}, \mathbf{c}_{s(i-2)+3}, \ldots, \mathbf{c}_{s(i-2)+s}\right]=\left(\boldsymbol{\Theta} \mathbf{A} \overline{\mathbf{W}_{(i)}}-\left[\mathbf{c}_{1}, \mathbf{c}_{2}, \ldots, \mathbf{c}_{s(i-2)+1}\right] \overline{\mathbf{R}_{(1: i-1, i)}}\right)\left(\overline{\overline{\mathbf{R}_{(i, i)}}}\right)^{-1}
$$

Here, the matrix $\overline{\mathbf{R}_{(1: i-1, i)}}$ represents $\mathbf{R}_{(1: i-1, i)}$ without the last column, and the matrix $\overline{\overline{\mathbf{R}_{(i, i)}}}$ represents $\mathbf{R}_{(i, i)}$ without the last row and column. Thus, the random sketching technique not only allows for the effective construction of a wellconditioned Krylov basis, but also can facilitate the determination of the Arnoldi matrix $\mathbf{H}$.

## 5 Numerical experiments

We test the methodology on three numerical examples: QR factorization of a synthetically generated matrix from $[4$, Section 5.1], the solution of a linear system with block GMRES, and the solution of an eigenvalue problem with the RR method. The numerical analysis of the $s$-step RBGS-Arnoldi algorithm and the randomized FOM method is outside the scope of this manuscript. The RBGS process is validated by comparison with standard methods such as BCGS, BMGS and BCGS2. Depending on the example, finite precision arithmetic is performed in float32 or float64 format. To ensure proper comparison, we decided to perform the inter-block orthogonalization routines in float64, even in the unique float32 precision algorithms. Furthermore, for standard methods we perform inter-block orthogonalization with an efficient Householder QR routine, while for RBGS, this routine is combined with RCholeskyQR according to Algorithm 2.5.

Several solvers are considered for the sketched least-squares problems depending on the experiment, but in all of them this step has negligible complexity and memory requirements. Finally, in all the numerical examples, SRHT and Rademacher matrices give very similar results, so we present the results for SRHT only.

### 5.1 Orthogonalization of a numerically rank-deficient matrix

Take $[\mathbf{W}]_{i, j}=f_{\mu_{j}}\left(x_{i}\right), 1 \leq i \leq n, 1 \leq j \leq m$, where

$$
f_{\mu}(x)=\frac{\sin (10(\mu+x))}{\cos (100(\mu-x))+1.1}
$$

and $x_{j}$ and $\mu_{j}$ are chosen as, respectively, $n=10^{6}$ and $m=300$ equally distanced points with $x_{0}=\mu_{0}=0$ and $x_{10^{6}}=\mu_{300}=1$. The matrix $\mathbf{W}$ is partitioned into blocks of columns of size $m_{p}=10$. Then a block-wise $\mathrm{QR}$ factorization of $\mathbf{W}$ is performed with RBGS process (Algorithm 2.1), executed either in unique float32 precision or in multi-precision, using $\Theta$ with $k=3000$ rows. The least-squares solver in step 2 of Algorithm 2.1 is chosen either as the Householder solver, or as 20 iterations of CG applied to the normal equation. Thereafter, the stability behavior of RBGS is compared to that of the standard BCGS, BMGS, and BCGS2 algorithms executed in float32 arithmetic.

We observe a similar picture as in [4, Section 5.1], comparing single-vector Gram-Schmidt algorithms. According to Figure 1a, the BCGS and BCGS2 methods become dramatically unstable respectively at iterations $i \geq 8$, and $i \geq 17$, with the latter iterations corresponding to $\mathbf{W}_{(1: i)}$ being numerically rank-deficient. The BMGS algorithm exhibits better stability than the other two standard approaches. However, it still yields a Q factor with condition number of two orders of magnitude. The unique precision RBGS algorithm, using Householder least-squares solver in step 2, has a similar stability profile as the BMGS algorithm. On the other hand, the usage of 20 iterations of CG does not provide sufficient accuracy in step 2 and, as a consequence, reduces the stability of the unique precision RBGS. In contrast to all tested methods, the multi-precision RBGS algorithm remains perfectly stable, even at iterations where $\mathbf{W}_{(1: i)}$ is numerically rank-deficient, and outputs a $\mathrm{Q}$ factor with condition number close to $\mathcal{O}(1)$. Unlike in the unique precision algorithm, here the CG solver in step 2 turns out to be sufficiently accurate. Moreover, Figure 1b examines the behavior of the approximation
error $\left\|\mathbf{W}_{(1: i)}-\mathbf{Q}_{(1: i)} \mathbf{R}_{(1: i, 1: i)}\right\| /\left\|\mathbf{W}_{(1: i)}\right\|$. We see that for all tested algorithms, besides BCGS2, this error remains close to machine precision at all iterations.

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-19.jpg?height=596&width=1594&top_left_y=434&top_left_x=271)

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-19.jpg?height=518&width=662&top_left_y=440&top_left_x=279)

(a) Condition number of $\mathbf{Q}$

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-19.jpg?height=504&width=674&top_left_y=453&top_left_x=1170)

(b) Error $\|\mathbf{W}-\mathbf{Q R}\| /\|\mathbf{W}\|$

Figure 1: Block QR factorization of the matrix from [4, Section 5.1]. In the plots, "u.p. CG-RBGS" and "m.p. CGRBGS" refer to the unique-precision and multi-precision RBGS algorithms, respectively, both of which perform step 2 with 20 iterations of CG. On the other hand, "u.p. HH-RBGS" refers to the unique precision RBGS that uses a Householder solver.

### 5.2 Solution of a linear system with block GMRES

Consider the linear system

$$
\begin{equation*}
\left(\mathbf{A}_{G a}+\alpha \mathbf{I}\right) \mathbf{X}_{G a}=\mathbf{B} \tag{5.1}
\end{equation*}
$$

where the matrix $\mathbf{A}_{G a}$ is taken as the "Ga41As41H72" matrix of dimension $n=268096$ from the SuiteSparse matrix collection, $\mathbf{I}$ is the $n \times n$ identity matrix, and $\alpha=0.2$ introduced to improve conditioning of $\mathbf{A}_{G a}$. The right-hand-side matrix $\mathbf{B}$ is taken as $n \times m_{p}$ random matrix with entries being i.i.d normal random variables, and $m_{p}=100$. Solving such a system could be part of the inverse subspace iteration for computing the eigenvalues of $\mathbf{A}_{G a}$ of the smallest magnitude. The shifted "Ga41As41H72" matrix has many clustered, possibly negative, eigenvalues that can bring stability issues to the solvers.

Furthermore, the system (5.1) is preconditioned from the right by the incomplete $\mathrm{LU}$ factorization $\mathbf{P}_{G a}$ of $\mathbf{A}_{G a}+\alpha \mathbf{I}$ with zero level of fill-in and symmetric reverse Cuthill-McKee reordering. With this preconditioner, the final system of equations has the form $\mathbf{A X}=\mathbf{B}$, where $\mathbf{A}=\left(\mathbf{A}_{G a}+\alpha \mathbf{I}\right) \mathbf{P}_{G a}$ and $\mathbf{X}_{G a}=\mathbf{P}_{G a} \mathbf{X}$. Here, the matrix $\mathbf{A}$ is not computed explicitly but is considered as an implicit map outputting a product with vectors. This system is approximately solved with the GMRES method based on different versions of BGS process. We restart the GMRES method every 30 iterations, i.e., when the dimension of the Krylov space becomes $m=3100$.

Here we examine the behavior of BCGS, BCGS2, BMGS and the unique precision RBGS under float32 arithmetic. There is no need to test the multi-precision RBGS as the unique precision algorithms are already able to provide a nearly optimal solution. The products with matrix $\mathbf{A}$ and solutions of reduced GMRES least-squares problems (4.2) are computed in float64 format. The BGS iterations and other operations are performed in float32 format. In RBGS, the sketched least-squares problems are solved with 5 Richardson iterations, as explained in Section 2.1.

Figure 3a provides the convergence of the residual error $\max _{j=1, \ldots, 100}\left\|\mathbf{A} \mathbf{u}_{j}-\mathbf{b}_{j}\right\| /\left\|\mathbf{b}_{j}\right\|$. The condition number of the computed Krylov basis $\mathbf{Q}_{(1: i-1)}$ at each iteration $i$ is given in Figure 3b. We see that already starting from the first iterations, the BCGS algorithm entails dramatic instability and stagnation of the residual error. In contrast, the BCGS2 algorithm remains stable at all iterations, providing a $\mathrm{Q}$ factor that is orthonormal up to machine precision. The BMGS algorithm shows partial instability, resulting in deteriorated convergence of the error. Finally, the RBGS algorithm is as stable as the BCGS2 algorithm. It provides a well-conditioned Q factor and optimal convergence of the residual error
for all tested sizes of the sketching matrix. At the same time, RBGS requires only half the flops and $\mathcal{O}(p)$ fewer global reductions than BMGS, and a quarter of the flops and at most half the global reductions of BCGS2.

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-20.jpg?height=518&width=664&top_left_y=435&top_left_x=275)

(a) Res. error $\max _{j}\left\|\mathbf{A} \mathbf{u}_{j}-\mathbf{b}_{j}\right\| /\left\|\mathbf{b}_{j}\right\|$

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-20.jpg?height=599&width=1578&top_left_y=432&top_left_x=271)

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-20.jpg?height=504&width=674&top_left_y=453&top_left_x=1170)

(b) Condition number of $\mathbf{Q}$

Figure 2: Solution of a linear system with GMRES.

### 5.3 Solution of an eigenvalue problem with randomized RR

In this test, we seek the smallest (negative) eigenpairs of the "Ga41As41H72" matrix $\mathbf{A}_{G a}$ from Section 5.2. For this we first transform the eigenproblem to the following positive-definite one:

$$
\begin{equation*}
\left(\alpha \mathbf{I}-\mathbf{A}_{G a}\right) \mathbf{x}=\lambda \mathbf{x} \tag{5.2}
\end{equation*}
$$

where $\alpha=1500 \approx\left\|\mathbf{A}_{G a}\right\|$, and seek the dominant eigenpairs of $\mathbf{A}=\alpha \mathbf{I}-\mathbf{A}_{G a}$. Then one can use the fact that $\mathbf{A}_{G a}$ and $\mathbf{A}$ have the same eigenvectors and the associated eigenvalues are related as $\lambda_{G a}=\alpha-\lambda$. In this case the first 50 eigenvalues of $\mathbf{A}$ are clustered inside [1500.9 1501.3]. To compute the dominant eigenpairs of $\mathbf{A}$ we either apply a subspace iteration or the RR method based on different BGS algorithms. The initial guess matrix $\mathbf{B}$ is taken as an $n \times m_{p}$ Gaussian matrix of dimension $m_{p}=50$. The Arnoldi algorithm is restarted every 50 iterations (with columns of $\mathbf{B}$ chosen as the dominant Ritz vectors) which corresponds to the Krylov space of dimension $m=2550$. In this numerical example all arithmetic operations are performed in float64. The step 2 of the RBGS algorithm is again performed with 5 Richardson iterations, as in the previous example.

We measure the approximation error by the maximum relative residual of the first $80 \%$ of computed eigenpairs (i.e., 40 eigenpairs out of 50 ). The convergence of the approximation error $\max _{j=1, \ldots, 40}\left\|\mathbf{A} \mathbf{u}_{j}-\mu_{j} \mathbf{u}_{j}\right\| /\left\|\mu_{j} \mathbf{u}_{j}\right\|$ is depicted in Figure 3a. The condition number of the computed Krylov basis is depicted in Figure 3b. It is revealed that the subspace iteration method yields early stagnation of the error and is unsuitable for this numerical example. On the other hand, the RR method, if stable, provides convergence of the error to machine precision.

It is revealed that the BCGS-based Arnoldi method becomes unstable already at early iterations and produces an ill-conditioned Krylov basis with condition number close to $\mathcal{O}\left(u^{-1}\right)$. Therefore this method needs to be restarted, say every 5 iterations. Figure 3a plots the error also for this case. It is seen that early restarting does not help as it causes a dramatic effect on the convergence of the error. In contrast to BCGS, the BCGS2 and RBGS algorithms show perfect stability and yield an approximation that converges to machine precision in 200 iterations. The BMGS method is unstable in the sense that it outputs an ill-conditioned Krylov basis. Despite this, it yields an approximation error that converges to machine precision, though with somewhat deteriorated rates compared to BCGS2 and RBGS.

## 6 Proofs of propositions and theorems

This section contains the proofs for the stability guarantees from Section 3.

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-21.jpg?height=604&width=1592&top_left_y=308&top_left_x=256)

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-21.jpg?height=512&width=661&top_left_y=324&top_left_x=274)

(a) Res. error $\max _{j}\left\|\mathbf{A} \mathbf{u}_{j}-\mu_{j} \mathbf{u}_{j}\right\| /\left\|\mu_{j} \mathbf{u}_{j}\right\|$

![](https://cdn.mathpix.com/cropped/2024_06_04_a913679eb513b3eade56g-21.jpg?height=515&width=674&top_left_y=323&top_left_x=1170)

(b) Condition number of $\mathbf{Q}$

Figure 3: Solution of an eigenvalue problem with the RR method. In the plots, "BCGS-restarted" refers to the BCGSArnoldi algorithm that restarts every 5 iterations.

Proof of Theorem 3.2. We have,

$$
\begin{equation*}
\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}-\widehat{\mathbf{S}}\|_{\mathrm{F}} \leq 1.02 u_{\text {fine }} n\|\boldsymbol{\Theta}\|_{\mathrm{F}}\|\widehat{\mathbf{Q}}\|_{\mathrm{F}} \leq 1.02 \sqrt{1+\varepsilon} u_{\text {fine }} n^{3 / 2}\|\widehat{\mathbf{Q}}\|_{\mathrm{F}} \leq 0.02 u_{c r s}\|\widehat{\mathbf{Q}}\|:=F_{1} \tag{6.1}
\end{equation*}
$$

which implies that

$$
\sigma_{\min }(\widehat{\mathbf{S}})-F_{1} \leq \sigma_{\min }(\boldsymbol{\Theta} \widehat{\mathbf{Q}}) \leq \sigma_{\max }(\boldsymbol{\Theta} \widehat{\mathbf{Q}}) \leq \sigma_{\max }(\widehat{\mathbf{S}})+F_{1}
$$

By noticing that

$$
1-\Delta^{(p)} \leq \sqrt{1-\Delta^{(p)}} \leq \sigma_{\min }(\widehat{\mathbf{S}}) \leq \sigma_{\max }(\widehat{\mathbf{S}}) \leq \sqrt{1+\Delta^{(p)}} \leq 1+\Delta^{(p)}
$$

and Corollary 1.3 we deduce that $\|\widehat{\mathbf{Q}}\| \leq 1.57$ and that

$$
(1+\varepsilon)^{-1 / 2}\left(1-\Delta^{(p)}-F_{1}\right) \leq \sigma_{\min }(\widehat{\mathbf{Q}}) \leq \sigma_{\max }(\widehat{\mathbf{Q}}) \leq(1-\varepsilon)^{-1 / 2}\left(1+\Delta^{(p)}+F_{1}\right)
$$

Let us now prove the second statement of the theorem. The statement clearly holds for $p=1$. For $p \geq 2$ notice that

$$
\begin{align*}
\|\widehat{\mathbf{R}}\|_{\mathrm{F}} & \leq \sigma_{\min }(\widehat{\mathbf{S}})^{-1}\|\widehat{\mathbf{S}} \widehat{\mathbf{R}}\|_{\mathrm{F}} \leq 1.12\|\widehat{\mathbf{S}} \widehat{\mathbf{R}}\|_{\mathrm{F}} \leq 1.12\left(\|\widehat{\mathbf{P}}\|_{\mathrm{F}}+\|\widehat{\mathbf{P}}-\widehat{\mathbf{S}} \widehat{\mathbf{R}}\|_{\mathrm{F}}\right)  \tag{6.2}\\
& \leq 1.12\left(1+\tilde{\Delta}^{(p)}\right)\|\widehat{\mathbf{P}}\|_{\mathrm{F}} \leq 1.4\left(1+\tilde{\Delta}^{(p)}\right)\|\widehat{\mathbf{W}}\|_{\mathrm{F}} \leq 1.6\|\widehat{\mathbf{W}}\|_{\mathrm{F}}
\end{align*}
$$

where we used the fact that $\|\widehat{\mathbf{P}}\|_{\mathrm{F}} \leq 1.02\|\boldsymbol{\Theta} \widehat{\mathbf{W}}\|_{\mathrm{F}} \leq 1.25\|\widehat{\mathbf{W}}\|_{\mathrm{F}}$. We also have, for $1 \leq i \leq p$,

$$
\widehat{\mathbf{W}}_{(i)}=\widehat{\mathbf{Q}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, i)}+\Delta \mathbf{W}_{(i)}
$$

with

$$
\begin{aligned}
\left\|\Delta \mathbf{W}_{(i)}\right\|_{\mathrm{F}} & \leq\left\|\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right\|_{\mathrm{F}}+1.02 u_{c r s}\left(\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+i m_{p}\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right) \\
& \leq 0.1 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|_{\mathrm{F}}+1.02 u_{c r s}\left(\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+1.57 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right) \\
& \leq 1.02 u_{c r s}\left(\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+0.16 m_{p}^{1 / 2}\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|_{\mathrm{F}}+1.57 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right) \\
& \leq 1.02 u_{c r s}\left(1.26 m_{p}^{1 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}+1.57 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right)
\end{aligned}
$$

Consequently,

$$
\begin{aligned}
& \|\boldsymbol{\Delta} \mathbf{W}\|_{\mathrm{F}}^{2} \leq 1.02^{2} u_{c r s}^{2} \sum_{1 \leq i \leq p} 2\left(1.26^{2} m_{p}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}+1.57^{2} i^{3} m_{p}^{3}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}^{2}\right) \\
& \leq 1.02^{2} u_{c r s}^{2} 2\left(1.26^{2} m\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}+1.57^{2} m^{3}\|\widehat{\mathbf{R}}\|_{\mathrm{F}}^{2}\right) \\
& \leq 1.02^{2} u_{c r s}^{2} 2\left(1.26^{2} m\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}+2.52^{2} m^{3}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}\right) \leq\left(4 u_{c r s} m^{3 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}\right)^{2}
\end{aligned}
$$

Proof of Remark 3.3. We have,

$$
\widehat{\mathbf{W}}_{(i)}=\widehat{\mathbf{Q}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, i)}+\Delta \mathbf{W}_{(i)}
$$

with

$$
\begin{aligned}
\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \mathbf{W}_{(i)}\right\|_{\mathrm{F}} & \leq\left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right)\right\|_{\mathrm{F}}+1.02 \sqrt{1+\varepsilon} u_{c r s}\left(\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+i m_{p}\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right) \\
& \leq 1.02 u_{c r s}\left(1.25\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+0.16 m_{p}^{1 / 2}\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|_{\mathrm{F}}+2 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right) \\
& \leq 1.02 u_{c r s}\left(1.51 m_{p}^{1 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}+2 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right)
\end{aligned}
$$

Consequently,

$$
\begin{aligned}
\|\boldsymbol{\Theta} \boldsymbol{\Delta} \mathbf{W}\|_{\mathrm{F}}^{2} & \leq 1.02^{2} u_{c r s}^{2} 2\left(1.51^{2} m\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}+2^{2} m^{3}\|\widehat{\mathbf{R}}\|_{\mathrm{F}}^{2}\right) \\
& \leq 1.02^{2} u_{c r s}^{2} 2\left(1.51^{2} m\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}+3.2^{2} m^{3}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}^{2}\right) \leq\left(5 u_{c r s} m^{3 / 2}\|\widehat{\mathbf{W}}\|_{\mathrm{F}}\right)^{2}
\end{aligned}
$$

Proof of Theorem 3.4. The proof is done by induction on $p$. Assume that Theorem 3.4 holds for $p=i-1 \geq 1$. Below, we show that the statement of the theorem then also holds for $p=i$. Clearly,

$$
\begin{align*}
&\left\|\widehat{\mathbf{S}}_{(1: i-1)}\right\|_{\mathrm{F}} \leq \sqrt{(i-1) m_{p}} \sqrt{1+\Delta^{(i-1)}} \leq 1.01 \sqrt{(i-1) m_{p}}  \tag{6.3a}\\
&\left\|\widehat{\mathbf{P}}_{(i)}\right\| \leq 1.02\left\|\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(i)}\right\| \leq 1.25\left\|\widehat{\mathbf{W}}_{(i)}\right\| \tag{6.3b}
\end{align*}
$$

Moreover, we have for $1 \leq j \leq i$,

$$
\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(j)}-\widehat{\mathbf{S}}_{(j)}\right\|_{\mathrm{F}} \leq 1.02 u_{\text {fine }} \sqrt{1+\varepsilon} n^{3 / 2}\left\|\widehat{\mathbf{Q}}_{(j)}\right\|_{\mathrm{F}} \leq 0.02 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(j)}\right\|
$$

From the fact that $\sigma_{\min }\left(\widehat{\mathbf{S}}_{(1: i-1)}\right) \geq \sqrt{1-\Delta^{(i-1)}} \geq 0.989$, Assumptions 2.2 and (6.3), notice that for each $(i-1) m_{p}<$ $j \leq i m_{p}$, the matrix $\widehat{\mathbf{S}}_{(1: i-1)}+\Delta \mathbf{S}$ associated with the $j$-th column of $\widehat{\mathbf{P}}$ satisfies

$$
\begin{equation*}
\sigma_{\min }\left(\widehat{\mathbf{S}}_{(1: i-1)}+\boldsymbol{\Delta} \mathbf{S}\right) \geq \sigma_{\min }\left(\widehat{\mathbf{S}}_{(1: i-1)}\right)-\|\boldsymbol{\Delta} \mathbf{S}\| \geq 0.98 \tag{6.4}
\end{equation*}
$$

Consequently, for each $(i-1) m_{p}<j \leq i m_{p}$, the $j$-th column of $\widehat{\mathbf{R}}_{(1: i-1,1: i)}$ is bounded by $\left\|\widehat{\mathbf{p}}_{j}+\boldsymbol{\Delta} \mathbf{p}\right\| / \sigma_{\min }\left(\widehat{\mathbf{S}}_{(1: i-1)}+\boldsymbol{\Delta} \mathbf{S}\right) \leq$ $1.4\left\|\widehat{\mathbf{W}}_{j}\right\|$ implying that

$$
\begin{equation*}
\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}} \leq 1.4\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \tag{6.5}
\end{equation*}
$$

This fact combined with the fact (which holds by the induction hypothesis: $\Delta^{(i-1)} \leq 0.02$ and Theorem 3.2) that

$$
\begin{equation*}
\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\| \leq 1.5 \text { and }\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\|_{\mathrm{F}} \leq 1.5(i-1)^{1 / 2} m_{p}^{1 / 2} \tag{6.6}
\end{equation*}
$$

and (3.3a), leads to the following result:

$$
\begin{align*}
\left\|\Delta \mathbf{Q}_{(i)}^{\prime}\right\|_{\mathrm{F}}= & \left\|\widehat{\mathbf{Q}}_{(i)}^{\prime}-\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right)\right\|_{\mathrm{F}} \leq 1.02 u_{c r s}\left\|\left|\widehat{\mathbf{W}}_{(i)}\right|+i m_{p}\left|\widehat{\mathbf{Q}}_{(1: i-1)}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)} \mid\right\|_{\mathrm{F}}\right.\right.  \tag{6.7}\\
\leq & 1.02 u_{c r s}\left(\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+i m_{p}\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}\right) \leq 3.2 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}, \text { and } \\
& \left\|\widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|_{\mathrm{F}} \leq\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\|\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}+\left\|\Delta \mathbf{Q}_{(i)}^{\prime}\right\|_{\mathrm{F}} \leq 3.2\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} . \tag{6.8}
\end{align*}
$$

Then by $(3.3 \mathrm{~b})$,

$$
\begin{align*}
\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \mathbf{Q}_{(i)}^{\prime}\right\|_{\mathrm{F}} & \leq 3.2 \sqrt{1+\varepsilon} u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \leq 4 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \\
\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|_{\mathrm{F}} & \leq\left\|\boldsymbol{\Theta} \widehat{\mathbf{W}}^{(i)}\right\|_{\mathrm{F}}+\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(1: i-1)}\right\|\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\|_{\mathrm{F}}+\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \mathbf{Q}_{(i)}^{\prime}\right\|_{\mathrm{F}} \leq 3.2\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} . \tag{6.9}
\end{align*}
$$

Denote residual matrix $\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}-\widehat{\mathbf{W}}_{(1: i-1)}$ by $\widehat{\mathbf{B}}^{(i-1)}$. Then, we have

$$
\begin{align*}
&\left\|\widehat{\mathbf{B}}^{(i-1)}\right\| \leq 4 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i-1)}\right\|_{\mathrm{F}} \leq 0.01 \sigma_{\min }(\widehat{\mathbf{W}}), \text { and } \\
&\left\|\boldsymbol{\Theta} \widehat{\mathbf{B}}^{(i-1)}\right\| \leq 5 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i-1)}\right\|_{\mathrm{F}} \leq 0.01 \sigma_{\min }(\widehat{\mathbf{W}}) \tag{6.10}
\end{align*}
$$

Now we are all set to derive the $\varepsilon^{\prime}$-embedding property of $\boldsymbol{\Theta}$ for $\widehat{\mathbf{Q}}_{(i)}^{\prime}$, which is needed to characterize the quality of the inter-block orthogonalization step. Let us first notice that,

$$
\begin{align*}
& \sigma_{\min }\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}\right)=\sigma_{\min }\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}+\boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right) \\
& \geq \sigma_{\min }\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right)-\left\|\boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\| \\
& \geq \sigma_{\min }\left(\widehat{\mathbf{W}}_{(i)}-\left(\widehat{\mathbf{W}}_{(1: i-1)}+\widehat{\mathbf{B}}^{(i-1)}\right) \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)}\right)-\left\|\boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\| \\
& =\sigma_{\min }\left(\left[\widehat{\mathbf{W}}_{(1: i-1)}+\widehat{\mathbf{B}}^{(i-1)}, \widehat{\mathbf{W}}_{(i)}\right]\left[-\widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)}\right]\right)-\left\|\Delta \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|  \tag{6.11}\\
& \geq \sigma_{\min }\left(\left[\widehat{\mathbf{W}}_{(1: i-1)}+\widehat{\mathbf{B}}^{(i-1)}, \widehat{\mathbf{W}}_{(i)}\right]\right)-\left\|\Delta \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\| \\
& \geq \sigma_{\min }(\widehat{\mathbf{W}})-\left\|\widehat{\mathbf{B}}^{(i-1)}\right\|-\left\|\Delta \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\| \\
& \geq 0.98 \sigma_{\min }(\widehat{\mathbf{W}})
\end{align*}
$$

Consequently, $\operatorname{cond}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}\right) \leq 5 m_{p}^{1 / 2} \operatorname{cond}(\widehat{\mathbf{W}})$. For any $\mathbf{a} \in \mathbb{R}^{m_{p}}$, it holds

$$
\begin{aligned}
& \left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\|=\left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\| \pm\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\| \\
& =\left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{W}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\| \pm\left\|\boldsymbol{\Theta} \widehat{\mathbf{B}}^{(i-1)}\right\|\left\|\widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)} \mathbf{a}\right\| \pm\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\| \\
& =\left(1 \pm \sigma_{\min }(\boldsymbol{\Theta} \widehat{\mathbf{W}})^{-1}\left(\left\|\boldsymbol{\Theta} \widehat{\mathbf{B}}^{(i-1)}\right\|+\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|\right)\left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{W}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\|\right. \\
& =(1 \pm 0.015)\left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{W}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\| \\
& =(1 \pm 0.015) \sqrt{1 \pm \varepsilon}\left\|\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{W}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\| \\
& =(1 \pm 0.015) \sqrt{1 \pm \varepsilon}\left(\left\|\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\| \pm\left\|\widehat{\mathbf{B}}^{(i-1)}\right\|\left\|\widehat{\mathbf{R}}_{(1: i-1,1: i-1)}^{-1} \widehat{\mathbf{R}}_{(1: i-1, i)} \mathbf{a}\right\|\right) \\
& =(1 \pm 0.015)(1 \pm 0.01) \sqrt{1 \pm \varepsilon}\left\|\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right) \mathbf{a}\right\| \\
& =(1 \pm 0.015)(1 \pm 0.01) \sqrt{1 \pm \varepsilon}\left(\left\|\widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\|+\left\|\boldsymbol{\Delta} \widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\|\right) \\
& =(1 \pm 0.015)(1 \pm 0.01)(1 \pm 0.01) \sqrt{1 \pm \varepsilon}\left\|\widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\| \\
& =(1 \pm 0.036) \sqrt{1 \pm \varepsilon}\left\|\widehat{\mathbf{Q}}_{(i)}^{\prime} \mathbf{a}\right\|
\end{aligned}
$$

which, combined with the parallelogram identity, implies that $\boldsymbol{\Theta}$ is a $\varepsilon^{\prime}$-embedding for $\widehat{\mathbf{Q}}_{(i)}^{\prime}$ with $\varepsilon^{\prime} \leq 1.1 \varepsilon+0.1 \leq 0.65$. By combining this fact with Assumptions 3.1, we obtain:

$$
1-u_{c r s} m_{p}^{1 / 2} \operatorname{cond}(\widehat{\mathbf{W}}) \leq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right) \leq \sigma_{\max }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right) \leq 1+u_{c r s} m_{p}^{1 / 2} \operatorname{cond}(\widehat{\mathbf{W}})
$$

and $\left\|\widehat{\mathbf{Q}}_{(i)}\right\| \leq 1.8$.

Since $\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}-\widehat{\mathbf{S}}_{(i)}\right\|_{\mathrm{F}} \leq 0.02 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\|$, we have

$$
1-1.05 u_{c r s} m_{p}^{1 / 2} \operatorname{cond}(\widehat{\mathbf{W}}) \leq \sigma_{\min }\left(\widehat{\mathbf{S}}_{(i)}\right) \leq \sigma_{\max }\left(\widehat{\mathbf{S}}_{(i)}\right) \leq 1+1.05 u_{c r s} m_{p}^{1 / 2} \operatorname{cond}(\widehat{\mathbf{W}})
$$

Consequently,

$$
\left\|\mathbf{I}-\widehat{\mathbf{S}}_{(i)}^{\mathrm{T}} \widehat{\mathbf{S}}_{(i)}\right\|_{\mathrm{F}} \leq m_{p}^{1 / 2}\left\|\mathbf{I}-\widehat{\mathbf{S}}_{(i)}^{\mathrm{T}} \widehat{\mathbf{S}}_{(i)}\right\| \leq m_{p}^{1 / 2} \max _{\mathbf{x}} \frac{\left|\|\mathbf{x}\|^{2}-\left\|\widehat{\mathbf{S}}_{(i)} \mathbf{x}\right\|^{2}\right|}{\|\mathbf{x}\|^{2}} \leq 3 u_{c r s} m_{p} \operatorname{cond}(\widehat{\mathbf{W}})
$$

We also have $\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|_{\mathrm{F}} \leq 1.02\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|_{\mathrm{F}} \leq 3.3\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}$, and

$$
\begin{aligned}
\sigma_{\min }\left(\widehat{\mathbf{R}}_{(i, i)}\right) & \geq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right) /\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}\right\| \geq 0.99\left(\sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right)-0.1 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\|\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|\right) \\
& \geq 0.58 \sigma_{\min }(\widehat{\mathbf{W}})
\end{aligned}
$$

and

$$
\begin{aligned}
\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}} \widehat{\mathbf{S}}_{(i)}\right\|_{\mathrm{F}} & =\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}}\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}+\boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime}\right) \widehat{\mathbf{R}}_{(i, i)}^{-1}+\boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime \prime}\right\|_{\mathrm{F}} \\
& \left.\leq\left(\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}} \boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|_{\mathrm{F}}+1.01 \| \boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime}\right) \|_{\mathrm{F}}\right) / 0.58 \sigma_{\min }(\widehat{\mathbf{W}})+\left\|\boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime \prime}\right\|_{\mathrm{F}}
\end{aligned}
$$

where $\boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime}=\boldsymbol{\Theta}\left(\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}-\widehat{\mathbf{Q}}_{(i)}^{\prime}\right)$ and $\boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime \prime}=\widehat{\mathbf{S}}_{(i)}-\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}$.

By noticing that,

$$
\begin{aligned}
\left\|\Delta \widehat{\mathbf{S}}_{(i)}^{\prime}\right\|_{\mathrm{F}} & \leq 0.1 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\|\left\|\widehat{\mathbf{R}}_{(i, i)}\right\| \leq 0.6 u_{c r s}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \\
\left\|\boldsymbol{\Delta} \widehat{\mathbf{S}}_{(i)}^{\prime \prime}\right\|_{\mathrm{F}} & \leq 0.02 u_{c r s}\left\|\widehat{\mathbf{Q}}_{(i)}\right\| \leq 0.04 u_{c r s} \\
\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}} \boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|_{\mathrm{F}} & \leq\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}} \boldsymbol{\Theta}\left(\widehat{\mathbf{W}}_{(i)}-\widehat{\mathbf{Q}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right)\right\|_{\mathrm{F}}+4.04 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \\
& \leq\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}}\left(\widehat{\mathbf{P}}_{(i)}-\widehat{\mathbf{S}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right)\right\|_{\mathrm{F}}+4.2 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \\
& \leq 5 u_{c r s} i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}},
\end{aligned}
$$

we deduce that $\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}} \widehat{\mathbf{S}}_{(i)}\right\|_{\mathrm{F}} \leq 10 u_{\text {crs }} i^{3 / 2} m_{p}^{2} \operatorname{cond}(\widehat{\mathbf{W}})$.

Consequently, for $i \geq 2$,

$$
\begin{aligned}
\left(\Delta^{(i)}\right)^{2} & =\left\|\mathbf{I}-\left(\widehat{\mathbf{S}}_{(i)}\right)^{\mathrm{T}} \widehat{\mathbf{S}}_{(i)}\right\|_{\mathbf{F}}^{2}+2\left\|\widehat{\mathbf{S}}_{(1: i-1)}^{\mathrm{T}} \widehat{\mathbf{S}}_{(i)}\right\|_{\mathrm{F}}^{2}+\left(\Delta^{(i-1)}\right)^{2} \\
& \leq\left(3 u_{c r s} m_{p} \operatorname{cond}(\widehat{\mathbf{W}})\right)^{2}+2\left(10 u_{c r s} i^{3 / 2} m_{p}^{2} \operatorname{cond}(\widehat{\mathbf{W}})\right)^{2}+\left(20 u_{c r s}(i-1)^{2} m_{p}^{2} \operatorname{cond}(\widehat{\mathbf{W}})\right)^{2} \\
& \leq\left(9+200 i^{3}+400(i-1)^{4}\right)\left(m_{p}^{2} u_{c r s} \operatorname{cond}(\widehat{\mathbf{W}})\right)^{2} \\
& \leq\left(400 i^{4}-1400 i^{3}+2400 i^{2}-1600 i+409\right)\left(m_{p}^{2} u_{c r s} \operatorname{cond}(\widehat{\mathbf{W}})\right)^{2} \\
& \leq\left(20 i^{2} m_{p}^{2} u_{c r s} \operatorname{cond}(\widehat{\mathbf{W}})\right)^{2}
\end{aligned}
$$

and,

$$
\begin{aligned}
& \left\|\widehat{\mathbf{S}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, i)}-\widehat{\mathbf{P}}_{(i)}\right\|_{\mathrm{F}} \leq\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}-\left(\widehat{\mathbf{P}}_{(i)}-\widehat{\mathbf{S}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1, i)}\right)\right\|_{\mathbf{F}}+\left\|\widehat{\mathbf{S}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}-\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}^{\prime}\right\|_{\mathrm{F}} \\
& \leq\left\|\boldsymbol{\Theta} \boldsymbol{\Delta} \mathbf{Q}_{(i)}^{\prime}\right\|_{\mathrm{F}}+\left\|\widehat{\mathbf{P}}_{(i)}-\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}+\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(1: i-1)}-\widehat{\mathbf{S}}_{(1: i-1)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: i-1, i)}\right\| \\
& \quad+\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(i)}-\widehat{\mathbf{S}}_{(i)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(i, i)}\right\|+\left\|\boldsymbol{\Theta}\left(\widehat{\mathbf{Q}}_{(i)}^{\prime}-\widehat{\mathbf{Q}}_{(i)} \widehat{\mathbf{R}}_{(i, i)}\right)\right\|_{\mathrm{F}} \\
& \leq u_{c r s}\left(3.2 \sqrt{1+\varepsilon i^{3 / 2}} m_{p}^{3 / 2}+0.02+0.02 \cdot 1.5 \cdot 1.4+0.04 \cdot 3.3+0.6\right)\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}} \\
& \leq 4.2 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathbf{F}} .
\end{aligned}
$$

Consequently,

$$
\begin{aligned}
\left(\Delta^{(i)}\right)^{2} & =\left\|\widehat{\mathbf{S}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}-\widehat{\mathbf{P}}_{(1: i)}\right\|_{\mathrm{F}}^{2} /\left\|\widehat{\mathbf{P}}_{(1: i)}\right\|_{\mathrm{F}}^{2} \\
& =\left(\left\|\widehat{\mathbf{S}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, i)}-\widehat{\mathbf{P}}_{(i)}\right\|_{\mathrm{F}}^{2}+\left\|\widehat{\mathbf{S}}_{(1: i-1)} \widehat{\mathbf{R}}_{(1: i-1,1: i-1)}-\widehat{\mathbf{P}}_{(1: i-1)}\right\|_{\mathrm{F}}^{2}\right) /\left\|\widehat{\mathbf{P}}_{(1: i)}\right\|_{\mathrm{F}}^{2} \\
& \leq\left(\left(4.2 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(i)}\right\|_{\mathrm{F}}\right)^{2}+\left(4.2 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i-1)}\right\|_{\mathrm{F}}\right)^{2}\right) /\left\|\widehat{\mathbf{P}}_{(1: i)}\right\|_{\mathrm{F}}^{2} \\
& =\left(4.2 i^{3 / 2} m_{p}^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}}\right)^{2} /\left\|\widehat{\mathbf{P}}_{(1: i)}\right\|_{\mathrm{F}}^{2}
\end{aligned}
$$

The proof is finished by noting that $\left\|\widehat{\mathbf{P}}_{(1: i)}\right\|_{\mathrm{F}} \geq(\sqrt{1-\varepsilon}-0.001)\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}} \geq 0.7\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}}$.

Proof of Proposition 3.5. We proceed with an induction on $p$. Clearly, the statement of the proposition holds for $p=1$. Assume that $\boldsymbol{\Theta}$ is an $\varepsilon^{\prime}$-embedding for $\widehat{\mathbf{Q}}_{(1: p)}, p=i-1 \geq 1$. This condition is sufficient for the following results in Theorem 3.4 and its proof to hold for $p=i$ :

$$
\begin{align*}
& \left\|\widehat{\mathbf{P}}_{(1: i)}-\widehat{\mathbf{S}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i: 1: i)}\right\|_{\mathrm{F}} \leq 4.2 u_{c r s} m_{p}^{3 / 2} i^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}},  \tag{6.12}\\
& \Delta^{(i)}=\left\|\mathbf{I}-\widehat{\mathbf{S}}_{(1: i)}^{\mathrm{T}} \widehat{\mathbf{S}}_{(1: i)}\right\|_{\mathrm{F}} \leq 20 u_{c r s} m_{p}^{2} i^{2} \operatorname{cond}\left(\widehat{\mathbf{W}}_{(1: i)}\right) \leq 0.02 \tag{6.13}
\end{align*}
$$

In addition, we have $\left\|\widehat{\mathbf{R}}_{(1: i, 1: 1)}\right\|_{\mathrm{F}} \leq 1.2\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}},\left\|\widehat{\boldsymbol{Q}}_{(1: i-1)}\right\| \leq 1.9,\left\|\widehat{\mathbf{Q}}_{(i)}\right\| \leq 1.3$, and

$$
\begin{align*}
& \left\|\widehat{\mathbf{W}}_{(1: i)}-\widehat{\mathbf{Q}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}\right\|_{\mathrm{F}} \leq\left\|\boldsymbol{\Delta} \mathbf{Q}_{(1: i)}^{\prime}\right\|_{\mathrm{F}}+\left(\sum_{1 \leq j \leq i}\left\|\widehat{\mathbf{Q}}_{(j)}^{\prime}-\widehat{\mathbf{Q}}_{(j)} \widehat{\mathbf{R}}_{(j, j)}\right\|_{\mathbf{F}}^{2}\right)^{1 / 2} \\
& \leq\left\|\boldsymbol{\Delta} \mathbf{Q}_{(1: i)}^{\prime}\right\|_{\mathrm{F}}+0.1 u_{c r s}\left(\sum_{1 \leq j \leq i}\left\|\widehat{\mathbf{Q}}_{(j)}\right\|^{2}\left\|\widehat{\mathbf{R}}_{(j, j)}\right\|^{2}\right)^{1 / 2}  \tag{6.14}\\
& \leq\left\|\boldsymbol{\Delta} \mathbf{Q}_{(1: i)}^{\prime}\right\|_{\mathrm{F}}+0.1 u_{c r s}\left(\left\|\widehat{\mathbf{Q}}_{(i)}\right\|+\left\|\widehat{\mathbf{Q}}_{(1: i-1)}\right\|_{\mathrm{F}}\right)\left\|\widehat{\mathbf{R}}_{(1: i, 1: i)}\right\|_{\mathrm{F}} \\
& \leq 3.5 u_{c r s}^{3 / 2} m_{p}^{3 / 2} i^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathbf{F}}
\end{align*}
$$

which can be proven directly like (6.2) and (6.7) in the proofs of Theorems 3.2 and 3.4. Finally, the following holds

$$
\begin{align*}
\left\|\widehat{\mathbf{S}}_{(1: i)}-\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(1: i)}\right\|_{\mathrm{F}} \leq 1.02 u_{\text {fine }} \sqrt{1+\varepsilon} n^{3 / 2}\left\|\widehat{\mathbf{Q}}_{(1: i)}\right\|_{\mathrm{F}} \leq u_{c r s} 0.02\left\|\widehat{\mathbf{Q}}_{(1: i)}\right\| \\
\left\|\widehat{\mathbf{P}}_{(1: i)}-\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}} \leq 1.02 u_{\text {fine }} \sqrt{1+\varepsilon} n^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}} \leq u_{c r s} 0.02\left\|\widehat{\mathbf{W}}_{(1: i)}\right\| \tag{6.15}
\end{align*}
$$

By using the fact that $\left\|\widehat{\mathbf{S}}_{(1: i)}\right\| \leq 1.02$ and $\sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: i)}\right) \geq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)}\right)-0.02 u_{c r s}\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|$, along with (6.12) and the $\varepsilon$-embedding property of $\boldsymbol{\Theta}$, we get

$$
\begin{equation*}
\sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: i, 1: i)}\right) \geq \frac{1}{\left\|\widehat{\mathbf{S}}_{(1: i)}\right\|}\left(\sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: i)}\right)-\left\|\widehat{\mathbf{P}}_{(1: i)}-\widehat{\mathbf{S}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}\right\|_{\mathrm{F}}\right) \geq 0.7 \sigma_{\min }\left(\widehat{\mathbf{W}}_{(1: i)}\right) \tag{6.16}
\end{equation*}
$$

Properties (6.12), (6.15) and (6.16) imply that

$$
\begin{aligned}
\left\|\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}-\widehat{\mathbf{S}}_{(1: i)}\right\|_{\mathrm{F}} & \leq\left\|\widehat{\mathbf{P}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}-\widehat{\mathbf{S}}_{(1: i)}\right\|_{\mathrm{F}}+\left\|\left(\widehat{\mathbf{P}}_{(1: i)}-\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)}\right) \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right\|_{\mathrm{F}} \\
& \leq\left(\left\|\widehat{\mathbf{P}}_{(1: i)}-\widehat{\mathbf{S}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}\right\|_{\mathrm{F}}+\left\|\widehat{\mathbf{P}}_{(1: i)}-\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}}\right)\left\|\widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right\| \\
& \leq 6.2 u_{c r s} m_{p}^{2} i^{2} \operatorname{cond}\left(\widehat{\mathbf{W}}_{(1: i)}\right)=: F_{1}
\end{aligned}
$$

Furthermore,

$$
1-\Delta^{(i)}-F_{1} \leq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right) \leq \sigma_{\max }\left(\boldsymbol{\Theta} \widehat{\mathbf{W}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right) \leq 1+\Delta^{(i)}+F_{1}
$$

Due to the fact that $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\widehat{\mathbf{W}}_{(1: i)}$, we deduce that

$$
\begin{equation*}
(1+\varepsilon)^{-1 / 2}\left(1-\Delta^{(i)}-F_{1}\right) \leq \sigma_{\min }\left(\widehat{\mathbf{W}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right) \leq \sigma_{\max }\left(\widehat{\mathbf{W}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right) \leq(1-\varepsilon)^{-1 / 2}\left(1+\Delta^{(i)}+F_{1}\right) \tag{6.17}
\end{equation*}
$$

We also have from (6.14) and (6.16),

$$
\left\|\widehat{\mathbf{W}}_{(1: i)} \widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}-\widehat{\mathbf{Q}}_{(1: i)}\right\|_{\mathrm{F}} \leq 3.5 u_{c r s} m_{p}^{3 / 2} i^{3 / 2}\left\|\widehat{\mathbf{W}}_{(1: i)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: i, 1: i)}^{-1}\right\| \leq 5 u_{c r s} i^{2} m_{p}^{2} \operatorname{cond}\left(\widehat{\mathbf{W}}_{(1: i)}\right)=: F_{2}
$$

By combining this property with (6.17), we get

$$
(1+\varepsilon)^{-1 / 2}\left(1-\Delta_{m}-F_{1}\right)-F_{2} \leq \sigma_{\min }\left(\widehat{\mathbf{Q}}_{(1: i)}\right) \leq \sigma_{\max }\left(\widehat{\mathbf{Q}}_{(1: i)}\right) \leq(1-\varepsilon)^{-1 / 2}\left(1+\Delta_{m}+F_{1}\right)+F_{2}
$$

We conclude that

$$
\begin{equation*}
(1+\varepsilon)^{-1 / 2}\left(1-F_{3}\right) \leq \sigma_{\min }\left(\widehat{\mathbf{Q}}_{(1: i)}\right) \leq \sigma_{\max }\left(\widehat{\mathbf{Q}}_{(1: i)}\right) \leq(1-\varepsilon)^{-1 / 2}\left(1+F_{3}\right) \tag{6.18}
\end{equation*}
$$

where $F_{3}:=\Delta_{m}+F_{1}+\sqrt{5 / 4} F_{2} \leq 32 u_{c r s} i^{2} m_{p}^{2} \operatorname{cond}\left(\widehat{\mathbf{W}}_{(1: i)}\right)$, which in particular implies that $\left\|\widehat{\mathbf{Q}}_{(1: i)}\right\| \leq 1.6$ and $\sigma_{\min }\left(\widehat{\mathbf{Q}}_{(1: i)}\right) \geq 0.86$. Furthermore, from (6.15), we get

$$
\begin{equation*}
1-F_{4} \leq \sigma_{\min }\left(\widehat{\mathbf{S}}_{(1: i)}\right)-0.1 u_{c r s} \leq \sigma_{\min }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(1: i)}\right) \leq \sigma_{\max }\left(\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(1: i)}\right) \leq \sigma_{\max }\left(\widehat{\mathbf{S}}_{(1: i)}\right)+0.1 u_{c r s} \leq 1+F_{4} \tag{6.19}
\end{equation*}
$$

where $F_{4}:=\Delta_{m}+0.1 u_{\text {crs }} \leq 20.1 u_{\text {crs }} m_{p}^{2} i^{2} \operatorname{cond}\left(\widehat{\mathbf{W}}_{(1: i)}\right)$.

From (6.18) and (6.19) it is deduced that for any vector $\mathbf{a} \in \mathbb{R}^{m}$,

$$
\begin{aligned}
& \left|\left\|\widehat{\mathbf{Q}}_{(1: i)} \mathbf{a}\right\|^{2}-\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}_{(1: i)} \mathbf{a}\right\|^{2}\right| \\
& \leq\|\mathbf{a}\|^{2} \max \left\{(1-\varepsilon)^{-1}\left(1+F_{3}\right)^{2}-\left(1-F_{4}\right)^{2},\left(1+F_{4}\right)^{2}-(1+\varepsilon)^{-1}\left(1-F_{3}\right)^{2}\right\} \\
& \leq\|\mathbf{a}\|^{2} \max \left\{1.45 \varepsilon+2.9 F_{3}+2 F_{4}, 1.34 \varepsilon+2.1 F_{4}+2.1 F_{3}\right\} \\
& \leq 0.74 \varepsilon^{\prime}\|\mathbf{a}\|^{2} \leq \varepsilon^{\prime}\left\|\widehat{\mathbf{Q}}_{(1: i)} \mathbf{a}\right\|^{2}
\end{aligned}
$$

where $\varepsilon^{\prime}=2 \varepsilon+180 u_{c r s} m_{p}^{2} i^{2} \operatorname{cond}\left(\widehat{\mathbf{W}}_{(1: i)}\right)$. By using the parallelogram identity this relation can be brought to form (1.2) where $V=\operatorname{range}\left(\widehat{\mathbf{Q}}_{(1: i)}\right)$ and $\varepsilon=\varepsilon^{\prime}$. It is deduced that $\boldsymbol{\Theta}$ is a $\varepsilon^{\prime}$-embedding for $\widehat{\mathbf{Q}}_{(1: i)}$.

## 7 Concluding remarks

In this work we developed a block generalization of the RGS process, called RBGS, to compute a QR factorization of a large-scale matrix. It was shown that this algorithm inherits the main properties of its single-vector analogue from [4], and in particular that it is at least as stable as the single-vector MGS process, and requires nearly half the cost of the classical BGS process in terms of flops and data passes. At the same time, RBGS is well suited for cache-based and highly parallel computational architectures because it mainly relies on matrix-matrix BLAS3 operations, in contrast to its single vector counterpart relying on matrix-vector BLAS2 operations. Like the single-vector RGS algorithm, it can be implemented using multi-precision arithmetic allowing to perform the dominant large-scale operations in precision independent of the dimension of the problem. This can be especially useful for simulations on low-precision arithmetic architectures.

Different strategies for treating the inter-block orthogonalization step, as well as solution of the sketched least-squares problems, have been proposed. Special care has been taken to ensure that the computational cost of these steps is negligible compared to other steps in the RBGS process.

The stability of our algorithms was verified in numerical experiments. In particular, it was seen that the multiprecision RBGS can provide a stable QR factorization of a numerically rank-deficient matrix, where even the most stable standard algorithms, including BCGS2, do not work. Furthermore, the randomized Arnoldi algorithm based on RBGS showed excellent stability in the context of GMRES and RR approximation in numerical examples where BMGS showed instability and convergence degradation. These factors indicate the robustness of the RBGS algorithm.

Our next goal is to combine RBGS with model order reduction and compression techniques for even more, possibly asymptotic, reduction of the cost of Gram-Schmidt orthogonalization and the associated Krylov methods. Another direction is the application of random sketching to increase the robustness and/or efficiency of Krylov methods that use short recurrences such as CG, BCG, Lanczos, and others. Furthermore, we want to improve not only the Krylov methods but also other methods that involve an orthogonalization of the approximation basis, such as block LOBPCG. As for the theoretical analysis, besides the characterization of the sketched Galerkin projection in the randomized RR method, we also plan to investigate the $\varepsilon$-embedding property of the sketching matrix for the computed Krylov space in the presence of rounding errors. Despite the fact that this property has been thoroughly verified in numerical experiments, it still remains unproven for the randomized Arnoldi algorithm.

## 8 Acknowledgments

This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement No 810367).

## References

[1] N. Ailon and E. Liberty. "Fast dimension reduction using Rademacher series on dual BCH codes". Discrete E8 Computational Geometry 42.4 (2009), p. 615 (cit. on p. 4 ).

[2] A. H. Baker, J. M. Dennis, and E. R. Jessup. "On improving linear solver performance: A block variant of GMRES". SIAM Journal on Scientific Computing 27.5 (2006), pp. 1608-1626 (cit. on p. 1).

[3] O. Balabanov. "Randomized Cholesky QR factorizations". arXiv preprint arXiv:2210.09953 (2022) (cit. on pp. 9, 10,11 ).

[4] O. Balabanov and L. Grigori. "Randomized Gram-Schmidt Process with Application to GMRES". SIAM Journal on Scientific Computing 44.3 (2022), A1450-A1474 (cit. on pp. 1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 18, 19, 26).

[5] O. Balabanov and L. Grigori. "Randomized Gram-Schmidt process with application to GMRES". arXiv preprint arXiv:2011.05090v1 (2020) (cit. on pp. 1, 2, 8, 9).

[6] O. Balabanov and A. Nouy. "Randomized linear algebra for model reduction. Part I: Galerkin methods and error estimation". Advances in Computational Mathematics 45.5-6 (2019), pp. 2969-3019 (cit. on pp. 2, 5, 15).

[7] O. Balabanov and A. Nouy. "Randomized linear algebra for model reduction. Part II: minimal residual methods and dictionary-based approximation". Advances in Computational Mathematics 47.2 (2021), pp. 1-54 (cit. on pp. 2, $13,15)$.

[8] J. L. Barlow. "Block Modified Gram-Schmidt Algorithms and Their Analysis". SIAM Journal on Matrix Analysis and Applications 40.4 (2019), pp. 1257-1290 (cit. on p. 4).

[9] J. L. Barlow and A. Smoktunowicz. "Reorthogonalized block classical Gram-Schmidt". Numerische Mathematik 123.3 (2013), pp. 395-423 (cit. on p. 4).

[10] E. Carson, K. Lund, and M. Rozložník. "The Stability of Block Variants of Classical Gram-Schmidt". SIAM Journal on Matrix Analysis and Applications 42.3 (2021), pp. 1365-1380 (cit. on p. 4).

[11] E. Carson, K. Lund, M. Rozložník, and S. Thomas. "An overview of block Gram-Schmidt methods and their stability properties". arXiv preprint arXiv:2010.12058 (2020) (cit. on pp. 1, 4).

[12] E. Carson, K. Lund, M. Rozložník, and S. Thomas. "Block Gram-Schmidt algorithms and their stability properties". Linear Algebra and its Applications 638 (2022), pp. 150-195 (cit. on p. 7).

[13] M. P. Connolly, N. J. Higham, and T. Mary. "Stochastic Rounding and its Probabilistic Backward Error Analysis" (2020) (cit. on p. 5).

[14] J. Demmel, L. Grigori, M. Hoemmen, and J. Langou. "Communication-optimal parallel and sequential QR and LU factorizations". SIAM Journal on Scientific Computing 34.1 (2012), A206-A239 (cit. on pp. 4, 10).

[15] T. Fukaya, Y. Nakatsukasa, Y. Yanagisawa, and Y. Yamamoto. "CholeskyQR2: a simple and communication-avoiding algorithm for computing a tall-skinny QR factorization on a large-scale parallel system". 2014 5th workshop on latest advances in scalable algorithms for large-scale systems. IEEE. 2014, pp. 31-38 (cit. on p. 2).

[16] L. Grigori, S. Moufawad, and F. Nataf. "Enlarged Krylov subspace conjugate gradient methods for reducing communication". SIAM Journal on Matrix Analysis and Applications 37.2 (2016), pp. 744-773 (cit. on p. 1).

[17] N. J. Higham. Accuracy and stability of numerical algorithms. 2nd ed. SIAM Publications, Philadelphia, PA, USA, 2002 (cit. on pp. 5, 8).

[18] M. Hoemmen. Communication-avoiding Krylov subspace methods. University of California, Berkeley, 2010 (cit. on pp. 1, 17).

[19] K. Lund. "Adaptively restarted block Krylov subspace methods with low-synchronization skeletons". Numerical Algorithms (2022), pp. 1-34 (cit. on p. 7).

[20] P.-G. Martinsson and J. A. Tropp. "Randomized numerical linear algebra: Foundations and algorithms". Acta Numerica 29 (2020), pp. 403-572 (cit. on p. 1).

[21] Y. Nakatsukasa and J. A. Tropp. "Fast \& Accurate Randomized Algorithms for Linear Systems and Eigenvalue Problems". arXiv preprint arXiv:2111.00113 (2021) (cit. on pp. 2, 9, 16).

[22] V. Rokhlin and M. Tygert. "A fast randomized algorithm for overdetermined linear least-squares regression". Proceedings of the National Academy of Sciences 105.36 (2008), pp. 13212-13217 (cit. on pp. 2, 9).

[23] Y. Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011 (cit. on pp. 1, 16).

[24] G. W. Stewart. "A Krylov-Schur algorithm for large eigenproblems". SIAM Journal on Matrix Analysis and Applications 23.3 (2002), pp. 601-614 (cit. on p. 1).

[25] K. Świrydowicz, J. Langou, S. Ananthan, U. Yang, and S. Thomas. "Low synchronization Gram-Schmidt and generalized minimal residual algorithms". Numerical Linear Algebra with Applications 28.2 (2021), e2343 (cit. on p. 1).

[26] J. A. Tropp. "Improved analysis of the subsampled randomized Hadamard transform". Advances in Adaptive Data Analysis 3.01n02 (2011), pp. 115-126 (cit. on p. 5).

[27] R. Vershynin. High-dimensional probability: An introduction with applications in data science. Vol. 47. Cambridge university press, 2018 (cit. on p. 1).

[28] D. P. Woodruff et al. "Sketching as a tool for numerical linear algebra". Foundations and Trends $B$ in Theoretical Computer Science 10.1-2 (2014), pp. 1-157 (cit. on pp. 1, 5).

[29] Y. Yamamoto, Y. Nakatsukasa, Y. Yanagisawa, and T. Fukaya. "Roundoff error analysis of the CholeskyQR2 algorithm". Electron. Trans. Numer. Anal 44.01 (2015), pp. 306-326 (cit. on p. 2).

[30] Q. Zou. "GMRES algorithms over 35 years". arXiv preprint arXiv:2110.04017 (2021) (cit. on p. 1).
