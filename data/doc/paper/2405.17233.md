# CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for $\mathbf{L L M s}$ 

Haoyu Wang<br>Shanghai Jiao Tong University<br>Shanghai, China<br>fayuge@sjtu.edu.cn

Bei Liu*<br>Shanghai Jiao Tong University<br>Shanghai, China<br>beiliu@sjtu.edu.cn

Hang Shao<br>Shanghai Jiao Tong University<br>Shanghai, China<br>hangshao99@sjtu.edu.cn

Bo Xiao<br>Meituan<br>Beijing, China<br>xiaobo09@meituan.com

Ke Zeng<br>Meituan<br>Beijing, China<br>zengke02@meituan.com

Guanglu Wan

Meituan

Beijing, China

wanguanglu@meituan.com

Yanmin Qian ${ }^{\dagger}$<br>Shanghai Jiao Tong University<br>Shanghai, China<br>yanminqian@sjtu.edu.cn


#### Abstract

Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-theart results across different bit settings, especially in extremely low-bit scenarios. Code will be released soon.


## 1 Introduction

Recent advancements in Large Language Models (LLMs) have yielded impressive accomplishments across a variety of tasks. Building on the pioneering work of GPT-3 [3], a series of influential LLMs such as OPT [47], BLOOM [23], LLaMA [37], Mixtral [21] and Yi [45] have consistently demonstrated that expanding model size predictably promotes modeling accuracy (a.k.a. the scaling law [17]). The pursue of large (e.g. hundreds of billions of parameters), and even larger language models is often considered as a promising avenue towards Artificial General Intelligence (AGI) [4].[^0]

Nevertheless, the significant computational and memory demands of LLMs pose unprecedented challenges. For example, deploying the GPT-3 model requires a staggering 350GB of memory merely to accommodate its parameters in half-precision format (FP16), with a minimum of five high-end A100-80G GPUs. Such immense memory demands, coupled with the enormous runtime communication overhead, hinder the integration of LLMs into practical applications. To tackle this problem, 8-bit Post-Training Quantization (PTQ) methods for both model weights and activations have been proposed $[40,8]$, which have led to reduced model sizes in memory and negligible declines in accuracy. Unfortunately, when PTQ methods [5, 29, 22] are attempted in quantizing the parameters into 4 bits or less, notable decreases of performance have been observed, making further reduction of model sizes a challenging task. Additionally, several quantization-aware training (QAT) approaches $[30,9,27]$ have been introduced to enhance the model performance. Nonetheless, these algorithms integrate a training phase within the quantization process, thus imposing substantial demands for computational resources.

In this paper, we introduce a novel and effective training-free quantization framework for LLMs, namely Column-Level Adaptive weight Quantization (CLAQ), which utilizes three distinct columnlevel strategies to enhance the performance of low-bit (i.e., 2-bit and 3-bit) quantization. Specifically, we initially apply a K-Means clustering based quantization that enables the adaptive generation of quantization centroids for each column of a weight matrix. Then, a simple yet powerful quantization sensitivity metric (Outlier Order) is described, which aims to identify parameter columns that are highly sensitive to quantization. Leveraging this metric, we further proposed two strategies to enhance quantization performance, including Column-Level Adaptive Precision (AP) quantization and Column-Level Adaptive Outlier Reservation (OR). Both schemes can significantly boost the performance of the low-bit quantized LLMs with only a slight increase in memory footprint. Notably, our fusion model (AP+OR) yields the best results in extremely low-bit scenarios. Extensive experiments on mainstream public LLMs and benchmarks demonstrate that our method achieves the state-of-the-art performance across various quantization bit-widths.

The main contributions of our work can be summarized as follows:

1. We introduce a novel centroid selection strategy for quantization: K-Means based weight quantization. The architecture of K-Means based quantization is illustrated in Figure 1. Compared to existing methods, K-Means based quantization excels in more accurately approximating the parameter distribution of pre-trained models and dynamically generating quantization centroids. Our proposed quantization scheme surpasses all current 3-bit and 4-bit PTQ methods under equivalent or larger quantization group sizes.
2. We present a column-wise outlier ratio based quantization sensitivity metric, namely Outlier Order, to assess the importance of parameters in LLMs. Based on this metric, Adaptive Precision (AP) quantization and Outlier Reservation (OR) are proposed to allocate higher budget to columns identified as more sensitive to quantization errors. Notably, this sensitivity metric emerges as an extremely efficient guiding principle, which can be calculated once easily and directly applied to both AP and OR.
3. We implement Adaptive Precision quantization with the guidance of Outlier Order. Different levels of precision are adaptively assigned within each weight matrix. By allocating higher quantization budget to columns with a greater concentration of outliers, we achieve a substantial reduction in quantization errors. Furthermore, adaptive precision quantization allows arbitrary equivalent bit-width from 2-bit to 4-bit, facilitating the deployment of LLMs across a variety of end-devices with different memory constraints.
4. We develop a novel Outliers Reservation (OR) strategy to further enhance the performance with only a slight increase in memory footprint. Based on Outlier Order, OR involves reserving FP16 precision outliers for columns with a higher concentration of outliers to preserve model performance, whereas columns with a lower outlier ratio are allocated less reservation budget. Moreover, the OR strategy can be combined with AP quantization, which exhibits the best results in low-bit quantization.

## 2 LLM Quantization

Post-Training Quantization Post-training quantization methods [38, 20, 28, 32] quantize LLMs without fine-tuning the parameters, while compensating the quantization error through specially

![](https://cdn.mathpix.com/cropped/2024_05_29_5fbab072f08f83e7e1ecg-03.jpg?height=634&width=1071&top_left_y=233&top_left_x=516)

Figure 1: The K-Means clustering based quantization. Elements in weight matrix column are input of K-Means clustering and the quantization centroids are derived as the output of clustering algorithm. Then the pre-trained weights are quantized to the nearest K-Means class center.

designed techniques. Previous work [40, 43, 8, 33, 11] have utilized the vanilla Round To Nearest quantization. Based on the OBS approach [13], GPTQ [15] adjusted the remaining weights while quantizing, offering an effective PTQ paradigm. A series of work have [29, 40, 24, 10, 44, 18, 39] noticed the significance of outlier treatment in LLM quantization. By preserving outliers, the performance of quantized models can be further improved. PTQ approach alleviates the computational burden associated with extensive re-training, thus facilitating the efficient deployment of LLMs in resource-constrained environments.

Quantization-Aware Training Quantization-Aware Training (QAT) incorporates a training phase within the quantization workflow. Owing to the model's ability to adapt to the constraints imposed by quantization, QAT typically leads to superior model performance. [27, 9, 41] integrated quantization with the basis of LoRA [19]. [42] performed extreme quantization with 1-bit parameters. In [36], instead of training the quantized parameters, a learnable equivalent transformation is trained for quantization. By learning the quantization error, QAT can mitigate the information loss brought by quantization schemes. Nonetheless, the advantage of QAT comes with the inclusion of specialized training phases, which not only requires additional time but also increases the computational overhead and complexity of the quantization task. In practical applications, it may also couple with model fine-tuning processes that complicates the workflow. Therefore, in this work, we focus on PTQ approach first, and leave the study of QAT approach as a future work.

## 3 CLAQ

### 3.1 K-Means Based Quantization

In general, prior quantization techniques on LLMs have typically adopted uniform quantization levels (i.e., quantization centroids are equally spaced), such as SmoothQuant [40] and GPTQ [15]. Alternative methods adopt strategies that sample quantization centroids from a normal distribution, as demonstrated by QLoRA [9]. Nonetheless, all of the above approaches fail to accurately capture the true distribution of a group of model parameters, given the heterogeneous nature of parameter magnitudes and distributions across various network layers.

To tackle this issue, we first introduce a straightforward yet effective strategy: employing K-Means clustering for the selection of quantization centroids (Figure 1). The clustering samples (i.e., quantization groups) are the columns of a parameter matrix from either self-attention or MLP structures in the LLMs.

Specifically, when quantizing an parameter matrix $W$ with shape $i \times j$, the formation of the quantization codebook $C_{W_{j}}$ for its $j$-th column is derived as follows:

$$
\begin{equation*}
C_{W_{j}}=K M e a n s\left(W_{j}\right) \tag{1}
\end{equation*}
$$

Where KMeans denotes the clustering operation of K-Means algorithm, where the number of cluster centroids $K$ is determined by the target precision of the quantization, i.e., $2^{\text {bit }}$ centroids in total. For instance, in 4-bit quantization, the codebook size is $2^{4}=16$, implying the need of 16 distinct quantization target values. Once these cluster centroids are obtained, they are employed as the discrete quantization target values. Each parameter is quantized to its nearest centroid in the codebook, and is replaced by the codebook index in storage. For de-quantization, full-precision parameters can be retrieved from the codebook according to these indices. Specifically, for any parameter $W_{i j}$ in the model, the quantization operation $Q$ can be formalized as:

$$
\begin{equation*}
q \in\left\{0,1,2, \cdots, 2^{b i t}-1\right\}, \quad Q\left(W_{i j}\right)=\underset{q}{\arg \min }\left|C_{W_{j}}(q)-W_{i j}\right| \tag{2}
\end{equation*}
$$

Inspired by [15], our approach quantizes the parameter matrix $W$ per column through clustering and quantization. We adopt the same approach as GPTQ [15] for updating the remaining parameters. The advantage of K-Means based quantization is three-fold: Firstly, the centroids generated by K-Means clustering adaptively conform to the distribution of the LLM's parameters. The better matching with the underlying parameter distribution can minimize information loss and mitigate quantization errors. Secondly, our K-Means quantization reduces storage requirements by storing a single vector of K-Means centroids as codebook per column, yet not losing the performance. Previous approaches have employed smaller quantization groups to boost low-bit quantization performance. However, smaller groups incur additional overhead of codebook storage. We found that with equivalent or even smaller model sizes, our algorithm demonstrates superior performance compared to prior works, illustrating the effectiveness of K-Means based quantization. Lastly, our approach is simple and efficient. Previous studies such as OmniQuant [36] and LLM-QAT [30] trained auxiliary network components to ensure quantization efficacy. In comparison, our method avoids the necessity of additional training.

### 3.2 Outlier Ratio Based Quantization Sensitivity Metric

Previous studies [40, 29] revealed that outliers in LLMs play a pivotal role in determining the performance of quantized models. Inspired by [44], we hypothesize that the number of outliers within the columns of the weight matrix can act as an indicator of the importance (or sensitivity) of each column for quantization. Therefore, we adopt outlier ratio as the key guiding metric for LLM quantization.

In this work, we introduce a novel, computationally efficient, and inference-agnostic adaptive precision guidance metric, termed Outlier Order. It can be utilized to allocate adaptive precision (AP) or apply Outlier Reservation (OR) according to the proportion of outliers within each column of the model weight. Columns with a high proportion of outliers, which are considered more important and sensitive to quantization, are allocated higher precision or more outlier reservation budget to preserve model capability, while columns with a lower outlier ratio are assigned lower precision or less reservation budget.

Specifically, for an $i \times j$ weight matrix $W$, the ratio of outliers $R_{j}$ of the $j$-th column $W_{j}$ is defined as:

$$
\begin{equation*}
R_{j}=\frac{\operatorname{Card}\left(\left|W_{j}\right|>|\bar{W}| \times S\right)}{i} \tag{3}
\end{equation*}
$$

where Card is the counting function, and the numerator represents the number of elements in $W_{j}$ which are greater than the $S$ times of the mean of $W . S$ is a scale coefficient for outlier selection, (e.g. typically $3,5,7,11$, etc.). The outlier rate $R_{j}$ is thus the proportion of parameters which are greater than multiple times of the mean value in the entire matrix $W$. In other words, the larger scale $S$, the greater absolute value required to become an outlier, and the fewer number of outliers picked Ablation study about the choice of $S$ is conducted in Appendix B. With the help of $R_{j}$, one can rank matrix columns according to their quantization sensitivities, which facilitates the adaptive selection strategies described below. We name the ranking of $R_{j}$ as Outlier Order.

![](https://cdn.mathpix.com/cropped/2024_05_29_5fbab072f08f83e7e1ecg-05.jpg?height=1203&width=1225&top_left_y=247&top_left_x=426)

Figure 2: The overall structure of CLAQ: quantized models are obtained from different quantization approaches. The single-precision K-Means based quantization runs without sensitivity calculation. We leverage outlier ratio based quantization sensitivity metric (a) to provide the guidance of the column-level adaptive outlier reservation (OR, b) and column-level adaptive precision (AP, c). The AP and OR strategies are orthogonal to each other.

### 3.3 Column-Level Adaptive Precision Quantization

Mixed Precision quantization (MP) involves the integration of multiple quantization levels with varying precisions to mitigate the performance degradation in highly compressed models. Mixedprecision has been applied to LLMs in previous studies [26, 25]. [26] leveraged gradient magnitudes of parameters as a guiding principle for determining quantization precision levels. [25] adopted a conventional criterion based on the relative magnitude of parameters concerning the input. Nonetheless, their metrics of precision allocation can not accurately reflect the importance of parameters and the performances of mixed-precision quantization were unsatisfactory.

To further improve the performance of low-bit quantization, we present a Column-Level Adaptive Precision (AP) strategy directed by Outlier Order, which avoids the heavy computational overhead brought by metric calculation. Additionally, experimental results reveal the AP approach guided by Outlier Order outperforms previous methods with other mixed-precision metrics. To perform the adaptive precision quantization, firstly, the parameters of the matrix are sorted per column based on their outlier ratio as described in section 3.2. Secondly, the bit allocation scheme is determined according to the target compression ratio. For example, a 2.2 -bit quantized model is derived by allocating top $10 \%$ outlier-concentrated columns to 4 -bit, and allocating 2 -bit to the rest. Through AP quantization, the columns with higher sensitivity to quantization are allocated higher precision. Let $B$ be the candidate set of bit-widths; for simplicity, we only describe applying two precision levels in
the model (e.g., $B=\{2,4\}$ or $B=\{3,4\}$ ). The quantization precision for the $j$-th column $P_{j}$ of the model is denoted as follows:

$$
B=\left\{p_{1}, p_{2}\right\}, \quad p_{1}>p_{2}, \quad \begin{cases}P_{j}=p_{1}, & \text { if } R_{j}>T_{A P}  \tag{4}\\ P_{j}=p_{2}, & \text { Otherwise }\end{cases}
$$

where the threshold $T_{A P}$ is calculated based on the target model size. $T_{A P}$ is a key criterion to ensure that the size of the generated model is close to the desired size limit. For instance of 2.2-bit adaptive quantization, the threshold $T_{A P}$ should be calibrated to the value at the top $10 \%$ of column-wise outlier ratios. With this approach, AP quantization models can be systematically derived to fit any particular size requirement in the range of $B$. Detailed analysis on various bit-width candidates in adaptive precision schemes can be found in Appendix D. Experiments on assigning different bits to different parameter matrices are described in Appendix G.

### 3.4 Column-Level Adaptive Outlier Reservation

As $[29,24,22,1]$ have revealed, the preservation of an appropriate fraction of outliers can significantly improve the performance of quantized models. However, previous approaches have simply retained outliers based on the entire parameter matrix and resulted in sub-optimal performances.

In this work, we propose a Column-Level Adaptive Outlier Reservation (OR) strategy to adaptively preserve full-precision outliers across different columns. We leverage the outlier ratio defined in Sec 3.2 to guide their retention strategy. The analysis of outlier distribution in weight (see Appendix A) illustrates that $90 \%$ of outliers are concentrated within the top $10 \%$ of columns, leading us to adopt a retention policy that preserves a greater number of outliers in the initial $10 \%$ of columns, while conserving fewer outliers in the remaining $90 \%$.

To apply the OR strategy, firstly, we pick top $10 \%$ quantization-sensitive columns with Outlier Order metric. Then we preserve a higher proportion $o_{1}$ of outliers unquantized in top $10 \%$ columns with higher outlier ratios, and maintain a lower outlier retention ratio $o_{2}$ in the rest. In each column, the same number of the largest and smallest parameters are reserved as outliers. The total number of outliers reserved is calculated by the target size of compression. The choice of $o_{1}$ and $o_{2}$ decides the proportion of outliers retained across columns. Grid search on $o_{1}$ and $o_{2}$ are discussed in Appendix C. Column-Level Adaptive Outlier Reservation (OR) ratio $O_{j}$ for the $j$-th column is presented below:

$$
O R=\left\{o_{1}, o_{2}\right\}, \quad o_{1}>o_{2}, \quad \begin{cases}O_{j}=o_{1}, & \text { if } R_{j}>T_{O R}  \tag{5}\\ O_{j}=o_{2}, & \text { Otherwise }\end{cases}
$$

where $O R$ denotes the set of retained outlier ratios. $T_{O R}$ represents the threshold of choosing the top $10 \%$ columns of higher outlier ratios. Columns with higher outlier ratio will be assigned more FP16 outlier reservation. Same proportion of outliers is reserved in all matrices to control the size of the compressed model. The advantages of this approach are threefold: First, by retaining more full-precision parameters in quantization-susceptible columns, model performance is better preserved in quantization. Second, the metrics employed for outlier reservation align with those used for generating adaptive precision quantization, avoiding redundant computational overhead. Thirdly, the implementation of outlier reservation and adaptive precision quantization are capable of being employed concurrently. Our fusion model (AP+OR) outperforms other PTQ methods with similar memory footprints.

## 4 Experiments and Results

### 4.1 Experimental Settings

Models The experiments are conducted on popular public LLMs. We adopted LLaMA [37] and Yi [45] as the baseline models, known for their superior performances in English and Chinese language tasks, respectively. The pre-trained base versions are employed in our experiments.

Datasets We employ widely-used datasets C4 [34] and WiKiText2 [31] as perplexity test sets. To evaluate downstream task capabilities, we tested on a diverse range of zero-shot benchmarks,

Table 1: Results of perplexity on WikiText2 [31] and C4 [34] of our proposed CLAQ and other methods. CLAQ models with * are fusion models employing column-level adaptive precision and outlier reservation techniques. The detail of fusion model setups is described in Appendix F. $g=128$ means group size of quantization is 128 , followed by the equivalent bit-width in italics. The results of LLaMA-2 series and Yi-34B models can be found in Appendix E.

| Method | \# Bits | LLaMA1-7B <br> WikiText $\downarrow \downarrow /$ C4 $\downarrow$ | LLaMA1-13B <br> WikiText $2 \downarrow /$ C4 $\downarrow$ | LLaMA1-30B <br> WikiText $2 \downarrow /$ C4 $\downarrow$ | LLaMA1-65B <br> WikiText $2 \downarrow /$ C4 $\downarrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| - | 16 | $5.63 / 7.08$ | $5.02 / 6.61$ | $4.04 / 5.97$ | $3.49 / 5.61$ |
| RTN | 4 | $6.43 / 7.93$ | 5.55/6.98 | $4.57 / 6.34$ | $3.87 / 5.85$ |
| GPTQ[15] | 4 | $5.99 / 7.43$ | $5.26 / 6.83$ | $4.35 / 6.20$ | $3.77 / 5.79$ |
| LLM-QAT[30] | 4 | $10.9 / 7.4$ | $9.6 / 6.5$ | $7.3 / 6.5$ | - |
| AWQ[29] | 4 | $6.08 / 7.52$ | $5.34 / 6.86$ | $4.39 / 6.17$ | $3.76 / 5.77$ |
| OmniQuant[36] | 4 | $5.86 / 7.34$ | $5.21 / 6.76$ | $4.25 / 6.11$ | $3.71 / 5.73$ |
| SqueezeLLM[22] | 4.05 | $5.79 / 7.21$ | $5.18 / 6.71$ | $4.22 / 6.06$ | $3.76 / 5.69$ |
| SpQR | $3.94 / 3.96 / 3.89 / 3.90$ | $5.87 / 7.28$ | $5.22 / 6.72$ | $4.25 / 6.08$ | $3.68 / 5.70$ |
| CLAQ | 4 | $5.78 / 7.21$ | $5.15 / 6.70$ | 4.17/6.06 | $3.62 / 5.69$ |
| GPTQ[15] | 3 | 8.0/9.54 | $6.54 / 8.15$ | $5.59 / 7.29$ | 4.94/6.69 |
| AWQ[29] | 3 | $11.88 / 13.26$ | 7.45/9.13 | $10.07 / 12.67$ | $5.21 / 7.11$ |
| OmniQuant[36] | 3 | $6.49 / 8.19$ | $5.68 / 7.32$ | 4.74/6.57 | 4.04/6.07 |
| CLAQ | 3 | 6.47/7.87 | 5.61/7.14 | $4.79 / 6.49$ | $4.11 / 6.00$ |
| $\mathrm{AWQ}[29]$ | $3(g=128,3.15)$ | $6.46 / 7.92$ | $5.51 / 7.07$ | $4.63 / 6.37$ | $3.99 / 5.94$ |
| OmniQuant[36] | $3(g=128,3.15)$ | $6.15 / 7.75$ | $5.44 / 7.05$ | $4.56 / 6.37$ | $3.94 / 5.93$ |
| SqueezeLLM[36] | 3.24 | $6.13 / 7.56$ | $5.45 / 6.92$ | $4.44 / 6.23$ | $3.88 / 5.84$ |
| CLAQ* | 3.12 | 5.97/7.40 | $5.27 / 6.83$ | 4.35/6.18 | $3.75 / 5.78$ |
| CLAQ* | 3.23 | $5.95 / 7.37$ | $5.24 / 6.81$ | 4.33/6.16 | $3.74 / 5.76$ |
| GPTQ[15] | 2 | $2148.66 / 691.25$ | $8730.02 / 1467.84$ | $508.02 / 188.73$ | 58.34/39.57 |
| CLAQ | 2 | 27.64/24.37 | 21.05/19.69 | 15.37/13.43 | $97.16 / 48.95$ |
| $\mathrm{AWQ[29]}$ | $2(g=64,2.28)$ | $2.5 \mathrm{e} 5 / 2.8 \mathrm{e} 5$ | $2.7 \mathrm{e} 5 / 2.2 \mathrm{e} 5$ | $2.3 \mathrm{e} 5 / 2.3 \mathrm{e} 5$ | $7.4 \mathrm{e} 4 / 7.4 \mathrm{e} 4$ |
| OmniQuant[36] | $2(q=64,2.28)$ | $8.90 / 11.78$ | $7.34 / 9.75$ | $6.59 / 8.65$ | $5.65 / 7.60$ |
| decoupleQ[16] | $2(g=64,2.28)$ | 8.18/- | 6.96/- | 5.81/- | $5.07 /-$ |
| CLAQ* | 2.12 | 7.57/8.87 | 6.41/7.92 | $5.40 / 7.05$ | 4.70/6.46 |
| CLAQ* | 2.24 | 6.93/8.35 | 5.99/7.52 | 5.03/6.74 | 4.41/6.22 |

including WinoGrande [35] , PiQA [2] , HellaSwag [46], ARC (comprising ARC-easy and ARCchallenge) [7] and BoolQ [6]. Additionally, $\mathrm{C} 4$ was chosen as the calibration dataset for tuning our quantization process. It is noteworthy to emphasize that the choice of calibration data might have an impact on experimental outcomes, as detailed in Appendix H.

Implementation Details To implement CLAQ, we leveraged the accelerated K-Means clustering algorithm provided by the scikit-learn-intelex library ${ }^{3}$. Our experiments were built upon the GPTQ framework ${ }^{4}$. For benchmarking purposes, the LM-Evaluation-Harness toolkit ${ }^{5}$ served as the assessment platform. All experiments were conducted on a server with 8 NVIDIA A100 GPUs, each with $80 \mathrm{~GB}$ capacity. Notably, a single NVIDIA A100 GPU is sufficient for the majority of our experiments except evaluating the LLaMA-65B model on zero-shot tasks, where two GPUs were used to accommodate its memory demands. Our contribution was mainly confined to the quantization of model weights, with activations left unquantized. Unless otherwise specified within the experimental results table, a 16-bit precision was maintained for activations. Comprehensive details regarding the hyper-parameters employed in our experiments can be found in Appendix F.

### 4.2 Main Results

The main results of our experiments are presented in Table 1 and Table 2. Table 1 shows the evaluations on perplexity, while the performance of zero-shot tasks is given in Table 2. It can be clearly observed that CLAQ consistently outperforms existing methods in various scenarios. Notably, in case of 3-bit and 4-bit single-precision quantization with a comparable codebook size configuration, our method generally outperforms the baselines examined.[^1]

Table 2: The evaluation result of Zero-Shot accuracy of our proposed CLAQ and other methods. CLAQ models with * are fusion models employing column-level adaptive precision and outlier reservation techniques. The results of LLaMA-2 series, Yi-34B model and experiments on other bit-width can be found in Appendix E.

| Model | Method | \# Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande | $\overline{\operatorname{Avg} \uparrow}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA1-7B | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |
|  | LLM-QAT[30] | 4 | 78.3 | 70.0 | 45.0 | 75.5 | 74.0 | 69.0 | 68.63 |
|  | $\mathrm{GPTQ}[15]$ | 4 | 78.67 | 73.23 | 42.83 | 74.16 | 75.07 | 70.48 | 69.07 |
|  | OmniQuant[36] | W6A6 | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17 |
|  | $\mathrm{SpQR}[10]$ | 4.63 | 78.45 | 67.13 | 38.23 | - | 56.01 | 67.48 | 61.46 |
|  | CLAQ | 4 | 78.29 | 75.29 | 43.25 | 74.70 | 75.26 | 70.32 | 69.52 |
|  | GPTQ[15] | 2 | 52.50 | 25.76 | 26.96 | 41.87 | 25.77 | 52.49 | 37.56 |
|  | CLAQ* | 2.12 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 | 64.29 |
| LLaMA1-13B | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |
|  | LLM-QAT[30] | 4 | 79.4 | 72.8 | 52.0 | 77.7 | 77.7 | 71.5 | 71.85 |
|  | GPTQ[15] | 4 | 79.60 | 76.60 | 47.87 | 76.45 | 77.89 | 71.90 | 71.72 |
|  | OmniQuant[36] | W6A6 | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95 |
|  | $\mathrm{SpQR}[10]$ | 4.63 | 78.94 | 74.37 | 43.17 | - | 59.02 | 69.77 | 65.05 |
|  | CLAQ | 4 | 79.65 | 76.94 | 48.04 | 77.74 | 78.58 | 72.85 | 72.30 |
|  | GPTQ[15] | 2 | 52.18 | 25.88 | 28.41 | 41.35 | 25.67 | 49.64 | 37.19 |
|  | CLAQ* | 2.12 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14 | 66.94 |
| LLaMA1-30B | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |
|  | LLM-QAT[30] | 4 | 81.0 | 79.4 | 56.8 | 81.8 | 81.8 | 75.1 | 75.98 |
|  | $\mathrm{GPTQ}[15]$ | 4 | 81.12 | 76.81 | 50.00 | 79.36 | 81.43 | 74.66 | 73.90 |
|  | OmniQuant[36] | W6A6 | 79.81 | 58.79 | 45.22 | 68.38 | 78.95 | 72.21 | 67.23 |
|  | SpQR[10] | 4.69 | 81.01 | 76.05 | 47.18 | - | 62.50 | 72.93 | 67.93 |
|  | CLAQ | 4 | 81.18 | 80.39 | 54.10 | 82.17 | 81.91 | 75.85 | 75.93 |
|  | GPTQ[15] | 2 | 53.16 | 27.74 | 26.79 | 43.46 | 27.10 | 48.46 | 37.79 |
|  | CLAQ* | 2.12 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64 | 72.26 |
| LLaMA1-65B | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |
|  | GPTQ[15] | 4 | 82.48 | 80.89 | 54.01 | 83.55 | 83.47 | 76.68 | 76.85 |
|  | OmniQuant[36] | W6A6 | 81.01 | 58.12 | 46.33 | 80.64 | 79.91 | 75.69 | 70.28 |
|  | SpQR[10] | 4.71 | 81.56 | 75.25 | 46.93 | - | 63.76 | 76.95 | 68.89 |
|  | CLAQ | 4 | 82.43 | 81.52 | 54.69 | 83.85 | 83.56 | 76.40 | 77.08 |
|  | GPTQ[15] | 2 | 53.59 | 28.62 | 27.22 | 54.80 | 30.09 | 51.62 | 40.99 |
|  | CLAQ* | 2.12 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30 | 73.86 |

For lower bit-width, we employ the fusion approach of AP and OR. Results show that with an additional 0.1-bit memory footprint, model accuracy is significantly boosted. Especially for compression around 2-bit (2.1 bits in our case), our method exhibits a consistent and substantial lead over all existing methods, achieving the state-of-the-art (SOTA) performance (significant difference at 0.01 confidence level).

Also, the zero-shot evaluation results demonstrate the superiority of our method. CLAQ outperforms other baselines in most cases. Specifically, in the case of 4-bit quantization, our solution closely matches the performance of full-precision models. For low-bit scenarios, our method significantly improve the accuracy of the quantized model in zero-shot tasks, and pushing the limits of large language model quantization under low-bit conditions. More zero-shot task results on LLaMA-2 series and Yi-34B models can be found in Appendix E.

### 4.3 Ablation Study

### 4.3.1 Adaptive Precision Quantization

Table 3 presents a comparison of different Adaptive Precision (AP) quantization methods across various precision levels. Inspired by [14], we developed a magnitude-to-parameter method to guide the mixed-precision allocation. Experimental results demonstrate that our proposed column-level adaptive precision quantization strategy significantly outperforms other mixed-precision methods under comparable size. The improvements can be attributed to the superiority of our Outlier Order metric. According to the outlier distribution analysis (see Appendix A), a minority of columns exhibit
higher sensitivity to quantization. Consequently, allocating additional precision on preserving these critical columns is an effective way to improve the performance of quantized models.

### 4.3.2 Outlier Reservation

Table 4 lists the results of the fixed and Outlier Reservation (OR) strategy, highlighting its superiority over static outlier preservation approaches. Remarkably, with an equal amount of increase in model sizes, the performance gain from retaining more outliers exceeds that from adaptive precision strategy. This result suggests the importance of outlier retention, indicating that directly preserving outliers at full precision is superior to quantizing them with a higher precision.

Table 3: Ablation study results for our proposed column-level adaptive precision quantization. $\dagger$ denotes a column-wise mixed-precision quantization with the metric of activation-to-weight ratio proposed in [14].

| Model | \# Bits | WikiText2 $\downarrow$ | C4 $\downarrow$ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande | Avg $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |
| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 | 67.35 |
| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85 | 43.65 |
| +MP $\dagger$ | 2.5 | 8.63 | 9.98 | 75.35 | 65.15 | 36.51 | 69.08 | 62.96 | 64.71 | $\mathbf{6 2 . 2 9}$ |
| +AP(ours) | 2.5 | $\mathbf{8 . 4 3}$ | $\mathbf{9 . 7 8}$ | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40 | 62.26 |
| +MP $\dagger$ | 2.2 | 11.60 | 12.70 | 71.00 | 57.91 | 29.43 | 65.22 | 51.07 | 59.03 | 55.61 |
| +AP(ours) | 2.2 | $\mathbf{1 0 . 1 4}$ | $\mathbf{1 1 . 4 6}$ | 73.39 | 64.10 | 31.91 | 63.52 | 60.08 | 62.83 | $\mathbf{5 9 . 3 1}$ |
| +MP $\dagger$ | 2.1 | 16.42 | 16.57 | 66.86 | 44.02 | 26.53 | 62.56 | 44.03 | 53.59 | 49.60 |
| +AP(ours) | 2.1 | $\mathbf{1 1 . 0 6}$ | $\mathbf{1 2 . 3 4}$ | 72.42 | 63.17 | 31.06 | 64.92 | 58.33 | 61.25 | $\mathbf{5 8 . 5 3}$ |

Table 4: Ablation study results for our proposed column-level adaptive outlier reservation. Outlier fix refers to keeping a fixed proportion of outliers in each column.

| Model | \# Bits | WikiText2 $\downarrow$ | C4 $\downarrow$ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande | Avg $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |
| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85 | 43.65 |
| +Outlier fix | 2.28 | 7.49 | 8.87 | 74.70 | 68.56 | 37.88 | 71.52 | 67.03 | 66.69 | 64.40 |
| +OR(ours) | 2.28 | $\mathbf{6 . 8 8}$ | $\mathbf{8 . 3 5}$ | 77.75 | 69.36 | 39.76 | 74.40 | 70.62 | 67.09 | $\mathbf{6 6 . 5 0}$ |
| +Outlier fix | 2.14 | 8.31 | 9.61 | 75.19 | 66.41 | 35.32 | 71.40 | 63.12 | 66.14 | 62.93 |
| +OR(ours) | 2.14 | $\mathbf{7 . 2 2}$ | $\mathbf{8 . 6 4}$ | 76.28 | 69.02 | 37.88 | 73.70 | 68.14 | 68.11 | $\mathbf{6 5 . 5 2}$ |

## 5 Limitations and Future Works

The main limitations of our work lie in the simplification of outlier retention and adaptive precision search strategies. We adopt two-level precision in AP for the convenience of CUDA kernel development. However, when the assignable precision bit-width exceeds the equivalent of 0.5 bit (i.e., $>2.5$ bit, $>3.5$ bit), the adaptive precision strategy outlined in our method may not represent the optimal approach. The potential for generating superior adaptive precision configurations is discussed in Appendix G, where we propose a heuristic adaptive precision search algorithm. Our future work is twofold: (1) We will continue to investigate the accomplishment of superior adaptive precision quantization configurations through efficient search algorithms. (2) We are developing customized CUDA kernels to support efficient computation of the CLAQ approach on GPUs. Compared with the AP quantization, dynamically preserving outliers per column is more challenging in deployment. We are actively refining the underlying algorithms, and the code will be released soon in the future.

## 6 Conclusion

In this paper, we have introduced a novel Column-Level Adaptive weight Quantization (CLAQ) framework, which integrates column-level adaptive precision quantization and outlier reservation techniques. Guided by the column-wise outlier proportion metric, the CLAQ method achieves efficient compression of LLMs while maximizing the preservation of model accuracy. Extensive evaluations on public LLM models confirm the superiority of CLAQ in enhancing performance across
varied bit-widths, notably in ultra-low bit cases. The results lend help to making LLMs more efficient and accessible in resource-constrained applications.

## References

[1] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023.

[2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439, 2020.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901, 2020.

[4] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.

[5] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024.

[6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.

[7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai 2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318-30332, 2022.

[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.

[10] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless 1lm weight compression. arXiv preprint arXiv:2306.03078, 2023.

[11] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, 2022.

[12] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawqv2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems, 33:18518-18529, 2020.

[13] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:4475-4488, 2022.

[14] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323-10337. PMLR, 2023.

[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[16] Yi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping Cai, Yang Zhang, and Shouda Liu. decoupleq: Towards 2-bit post-training uniform quantization via decoupling parameters into integer and floating points. arXiv preprint arXiv:2404.12759, 2024.

[17] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.

[18] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023.

[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[20] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning, pages 44664475. PMLR, 2021.

[21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

[22] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.

[23] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. 2023.

[24] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13355-13364, 2024.

[25] Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, and Guohao Dai. Enabling fast 2-bit llm on gpus: Memory alignment, sparse outlier, and asynchronous dequantization. arXiv preprint arXiv:2311.16442, 2023.

[26] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization for efficient llm deployment.

[27] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. arXiv preprint arXiv:2310.08659, 2023.

[28] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.

[29] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.

[30] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.

[31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[32] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.

[33] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.

[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.

[35] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

[36] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023.

[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[38] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In International Conference on Machine Learning, pages 98479856. PMLR, 2020.

[39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402-17414, 2022.

[40] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087-38099. PMLR, 2023.

[41] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.

[42] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che. Onebit: Towards extremely low-bit large language models. arXiv preprint arXiv:2402.11295, 2024.

[43] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168-27183, 2022.

[44] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175, 2023.

[45] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.

[46] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

[47] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
