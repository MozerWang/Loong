# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models 

Ronak Pradeep*, Sahel Sharifymoghaddam<br>David R. Cheriton School of Computer Science,<br>University of Waterloo, Canada<br>\{rpradeep, sahel.sharifymoghaddam, jimmylin\}@uwaterloo.ca


#### Abstract

Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing highquality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with $\mathrm{GPT}_{3.5}$ with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with $\mathrm{GPT}_{4}$. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at https: //github.com/castorini/rank_llm.


## 1 Introduction

The widespread availability of instruction finetuned large language models (LLMs) has led to an explosion of applications in various natural language processing and information retrieval tasks. In the context of text retrieval, we have seen multiple efforts focused on zero-shot listwise reranking using LLMs (Sun et al., 2023; Ma et al., 2023), but unfortunately, to date, they have all relied on proprietary models. While such models support rapid prototyping, particularly when exposed as API endpoints, the reproducibility of experimental results that build on them is suspect-both from the normative perspective of what is "good science" and the practical perspective of obtaining reliable and deterministic measurements of experimental[^0]

results. It would, of course, be desirable for the community to have access to a fully open-source LLM and associated code infrastructure capable of performing high-quality reranking.

RankVicuna provides exactly this: To our knowledge, we present the first open-source large language model for zero-shot listwise document reranking. Experimental validation on test collections from the TREC 2019 and 2020 Deep Learning Tracks (Craswell et al., 2020, 2021) shows that the effectiveness of our model is on par with zero-shot reranking using $\mathrm{GPT}_{3.5}$, but slightly worse than reranking with $\mathrm{GPT}_{4}$. However, we can achieve these results with a much smaller model with only 7B parameters while still constrained to a $\mathrm{GPT}_{3.5}$ teacher. We share our model checkpoints and associated code, providing a valuable resource for the research community.

During the process of building RankVicuna, we have gained several important insights that we share: First, we confirm that proprietary LLMs are indeed effective at reranking in a zero-shot manner (Sun et al., 2023; Ma et al., 2023), although they exhibit several shortcomings. Beyond the obvious issue of non-reproducibility, results from these models are also non-deterministic, which makes them unreliable for rigorous scientific research. Additionally, proprietary LLMs occasionally fail to follow the requested format in their responses. In contrast, RankVicuna is open-source, deterministic, and always generates well-formed responses.

Second, we examine the impact of first-stage retrieval methods on downstream reranking effectiveness and find that RankVicuna consistently improves over the baseline retrieved results. We also find that with an effective first-stage retriever, even a single pass with reranking only the top 20 candidates brings an improvement similar to reranking the top 100 candidates.

Finally, our experiments shed some light on the importance of training strategies that involve data
augmentation to ensure model robustness against shuffled candidates or variations in initial retrieval quality. However, we note that data augmentation techniques affect the quality of model outputs under "ideal" conditions, and thus we face an effectiveness-robustness tradeoff.

Our work lays a solid foundation for future research. By making our models and infrastructure available to the public, we hope to stimulate further exploration and innovation in reranking. We anticipate that our findings will guide researchers in developing more effective and efficient reranking models. As the demand for accurate and reliable information retrieval systems continues to grow in this age of retrieval-augmented LLMs, we expect our work to contribute to future advances.

## 2 Background and Related Work

Given a corpus $\mathcal{C}=\left\{D_{1}, D_{2}, \ldots, D_{n}\right\}$ containing a collection of documents and an information need expressed as a query $q$, the task of a retriever is to efficiently return a list of $k$ documents from $\mathcal{C}$ that are most relevant to the query $q$ according to some metric such as nDCG or average precision, where $k \ll|\mathcal{C}|$. The task of a reranker is to further improve the quality of the ranked list produced by the retriever or another upstream reranker, according to either the same or a different metric.

Retrievers and rerankers together form multistage ranking pipelines for text ranking, which have been studied in the context of transformer models (Nogueira et al., 2019; Gao et al., 2021) but date back well over a decade (Matveeva et al., 2006; Cambazoglu et al., 2010; Wang et al., 2011). Nogueira and Cho (2019) were the first to demonstrate the use of (encoder-only) transformer models for reranking (using BERT) with a simple crossencoder architecture they called monoBERT. While neural rerankers had been explored extensively by researchers prior to the advent of BERT, the monoBERT model represented a significant advance in effectiveness; see Lin et al. (2021b) for a historical overview.

Following monoBERT, other researchers have explored reranking using decoder-only transformer models (Nogueira dos Santos et al., 2020) and full encoder-decoder models (Nogueira et al., 2020; Zhuang et al., 2022). These approaches are effective but require copious amounts of training data in the form of (query, relevant passage) pairs; often, the MS MARCO dataset (Bajaj et al., 2016) is used for such purposes. Most of the early work on reranking with transformers can be characterized as a pointwise approach, where the relevance of a particular candidate document is estimated independently of others.

More recently, however, researchers have addressed this shortcoming by incorporating pairwise and listwise losses in their cross-encoder approaches (Gao et al., 2021; Pradeep et al., 2022b; Zhuang et al., 2022). Using hard negatives in combination with such losses yields systems that are better at reranking in high-precision settings and that align more closely to the first-stage retriever.

In contrast, our work focuses on the zero-shot setting, where the model is not provided any taskspecific supervised training (e.g., relevant querypassage pairs). We build on a recent thread of work (Sun et al., 2023; Ma et al., 2023; Qin et al., 2023) that directly uses LLMs as rerankers in a multi-stage ranking pipeline, primarily focusing on prompt engineering to accomplish the reranking task. We coin the term "prompt-decoders" (in contrast to BERT-style cross-encoders) to characterize this class of rerankers. Furthermore, since these models are not fine-tuned or benefit from in-context learning, we might describe this type of reranking model as a zero-shot prompt-decoder. To use an open-source LLM as a prompt-decoder, Qin et al. (2023) adopted a pairwise approach since FLANUL2 is not capable of reordering a list of input documents. We find the same shortcoming to be also true for Vicuna, but we address this by using $\operatorname{RankGPT}_{3.5}$ as its teacher.

Rerankers depend on an upstream source to supply candidate documents, which can be a first-stage retriever or another reranker. In all our experiments, we rely on a first-stage retriever to generate a candidate list of documents from the corpus. Researchers have explored a variety of sparse, dense, and hybrid retrieval techniques, but these are not the focus of our study. We refer interested readers to Lin (2021) and Lin et al. (2021b) for an overview of such models.

In another relevant thread, recent work such as InPars (Bonifacio et al., 2022; Boytsov et al., 2023) and Promptagator (Dai et al., 2022) explored using LLMs to generate synthetic queries for documents to craft relevant query-document pairs as training data for retrievers or rerankers. Similarly, HyDE (Gao et al., 2023) used LLMs to augment queries by generating hypothetical documents for
unsupervised dense retrieval. Related, Sachan et al. (2023) proposed ART, a novel approach to training a dense passage retriever starting only with questions, which outperforms the standard reference dense retrieval model DPR (Karpukhin et al., 2020). In the emerging paradigm of generative retrieval, Pradeep et al. (2023) explored different document representation strategies and found synthetic queries to be necessary for effectiveness as the corpus size increases. However, all these approaches take advantage of large language models indirectly.

Finally, we note that rerankers have gained additional prominence in recent months with the introduction of commercially available API endpoints. Examples include Cohere's Rerank API ${ }^{1}$ and Microsoft's Semantic Search API in Azure Cognitive Search. ${ }^{2}$ The existence of these production services suggests that reranking models have attained maturity beyond explorations in research laboratories, and that rerankers address a real-world problem.

## 3 Methods

### 3.1 Prompt Design

Recent work (Ma et al., 2023) has shown that zeroshot listwise LLM-based rerankers outperform their pointwise counterparts since the former can attend to multiple documents simultaneously to determine their relative positions in a relevance ranking. We build on this finding and define our ranking problem as follows: Given a user query $q$ and candidate documents $\left\{D_{1}, \ldots, D_{n}\right\}$ from the previous stage, the task is to return a reordered list of the input document identifiers that improves a retrieval metric such as nDCG.

Our prompt template for zero-shot listwise reranking is similar to the RankGPT prompt (Sun et al., 2023), but accounts for differences between Vicuna and GPT; specifically, we use the default system description for Vicuna. In addition, we modified the prompt to show that the answer can, and in many cases should, deviate from the identity ordering, $[1]>[2]>\ldots>[m]$. The exact input prompt to Vicuna is shown in Figure 1.

We prepend the prompt with the system description, which, in Vicuna's case, is "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite an-[^1]

USER: I will provide you with \{num\} passages, each indicated by a numerical identifier []. Rank the passages based on their relevance to the search query: \{query\}.

$[1]$ \{passage 1\}

$[2]$ passage 2\}

$[\{$ num\} $]\{$ passage $\{$ num\}\}

Search Query: \{query\}.

Rank the \{num\} passages above based on their relevance to the search query. All the passages should be included and listed using identifiers, in descending order of relevance. The output format should be [] > [], e.g., [4] > [2]. Only respond with the ranking results, do not say any word or explain.

Figure 1: User Input for both RankVicuna and our replication of RankGPT.

swers to the user's questions." We hope that aligning our model with the exact prompt setup used to train Vicuna would help generate higher-quality ranked lists for our task.

### 3.2 RankVicuna

We leveraged RankGPT 3.5 as a teacher model for Vicuna to prompt-decode high-quality ranked lists. More specifically, we trained RankVicuna on the ranked lists generated by RankGPT 3.5 for the $100 \mathrm{~K}$ training set queries provided by Sun et al. (2023). To generate this dataset, the authors randomly sampled 100K queries from the MS MARCO v1 passage ranking training set and retrieved 20 candidates using BM25 for each query using Pyserini (Lin et al., 2021a). Then, these candidates were passed into RankGPT 3.5 to generate teacher orderings, which we distill down to our student, RankVicuna. Since both RankGPT 3.5 and RankVicuna are not directly exposed to human-labeled relevant query-passage pairs, our approach can still be considered zero-shot.

To ensure higher quality and more robust trained models, we took the following additional steps:

- We did not train on malformed generations. More specifically, examples with incorrect list formatting, missing document identifiers, or repetitions were excluded from the training set. This is important as we find that about $12 \%$ of the outputs were malformed, and we desire a model that consistently generates a well-formed ordering.
- Besides including the original generations provided by the teacher, which reranks the top 20 re-
sults by BM25 (Robertson and Zaragoza, 2009), we also include a condition where the input order is shuffled. Our hope is that this exposes the model to a more complex reordering task while not incurring additional data generation costs. However, we still retain the original BM25 input ordering, as we believe it is important to model "success", given it is the closest to what the model sees during inference. All RankVicuna settings in the rest of the paper involve this data augmentation (DA) process unless specified.

We trained our 7B parameter RankVicuna for two epochs with an effective batch size of 128 and a learning rate of $2 \times 10^{-5}$ in bfloat 16 . Training took roughly 80 hours on four NVIDIA RTX A6000 GPUs. The Vicuna model that served as our initial weights can be found under lmsys/vicuna-7b-v1. 5 in the HuggingFace Hub. This model is instruction fine-tuned from Meta's LLaMA-v2 model (Touvron et al., 2023).

It is worth noting that the "out-of-the-box" Vicuna model, which was not trained on the Rank$\mathrm{GPT}_{3.5}$ data, completely fails at the reranking task, often simply returning an identity ordering or a malformed generation.

## 4 Experimental Setup

To demonstrate the effectiveness of RankVicuna, we compared it with existing representative unsupervised ranking methods (BM25 and Contriever) as well as our replications of two closed-source prompt-decoder models: LRL (Ma et al., 2023) with GPT $_{3.5}$ and RankGPT (Sun et al., 2023), with both $\mathrm{GPT}_{3.5}$ and $\mathrm{GPT}_{4}$, which we refer to as Rank$\mathrm{GPT}_{3.5}$ and RankGPT 4 , respectively. $\mathrm{GPT}_{3.5}$ refers to the model dubbed gpt-3.5-turbo in the Open$\mathrm{AI}$ suite while $\mathrm{GPT}_{4}$ refers to gpt-4. We also compared RankVicuna with our replication of PRPSliding-10 from Qin et al. (2023), albeit with Vicuna (7B parameters). For these experiments, we used Vicuna instead of FLAN-T5 or FLAN-UL2 because we wanted an apples-to-apples comparison with the same base LLM. Additionally, we note that the FLAN mixture, used to pretrain the models, includes the MS MARCO QA task, ${ }^{3}$ thereby rendering the results suspect from the perspective of zero-shot retrieval.[^2]

We evaluated our methods using test collections from the TREC 2019 and 2020 Deep Learning Tracks (Craswell et al., 2020, 2021), using query and relevance judgments from the passage retrieval tasks. These tasks use the MS MARCO v1 passage corpus (Bajaj et al., 2016), which contains 8.8 million passages. For convenience, we refer to these datasets as DL19 and DL20. We report effectiveness in terms of nDCG@10 and average precision at a rank cutoff of 100 (denoted MAP@100).

The context size is 4096 for Vicuna and GPT $_{3.5}$ and 8192 for $\mathrm{GPT}_{4}$. To reorder the top 100 candidates for each query given these context sizes, we used a sliding window similar to RankGPT and LRL. In our experiments, we have adopted the same values as RankGPT (window size 20, stride 10) to isolate the impact of window and stride size in our comparisons.

Unlike RankVicuna, we (surprisingly) observe non-deterministic outputs for $\mathrm{GPT}_{3.5}$ and $\mathrm{GPT}_{4}$, even with a temperature of zero. For these two models, we report the mean over six and three runs, respectively, with $99 \%$ confidence intervals. We limited the number of $\mathrm{GPT}_{4}$ runs to three due to our computation budget.

In all our reranking experiments, we replaced any reference of the form $[n]$ in the passages with $(n)$ to avoid confusing the models. We also leveraged ftfy's fix_text method to preprocess any input sent to the rerankers.

## 5 Results

Table 1 compares different reranking pipelines using data from DL19 and DL20. Rows (1) and (2) report baselines using two first-stage retrievers, BM25 and Contriever (Izacard et al., 2021). The remaining rows (besides the last one) report the results of using zero-shot LLM rerankers to reorder top 100 candidate documents retrieved by BM25. Rows (6) and (7) show scores of two variants of PRP-Sliding-10, FLAN-T5-XXL and FLAN-UL2, directly copied from Qin et al. (2023). The final row represents our best system, where we apply RankVicuna to rerank the top 100 candidates generated by SPLADE++ EnsembleDistil (Formal et al., 2021), a state-of-the-art neural first-stage sparse retrieval method.

As expected, all LLM rerankers outperform the baseline (first-stage) methods. The effectiveness of RankVicuna, with 7B parameters, is on par with the effectiveness of RankGPT 3.5 , with 175B pa-

|  | Source |  | DL19 |  | DL20 |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Prev. $\quad T$ | Top-k | nDCG@10 | MAP@100 | nDCG@10 | MAP@100 |
| (1) BM25 | None | $\|C\|$ | 0.5058 | 0.2476 | 0.4796 | 0.2685 |
| (2) Contriever | None | $\|C\|$ | 0.6164 | 0.3163 | 0.5986 | 0.3309 |
| (3) $\mathrm{LRL}\left(\mathrm{GPT}_{3.5}\right)$ | BM25 | 100 | $0.6451 \pm 0.003$ | $0.3035 \pm 0.004$ | $0.6099 \pm 0.004$ | $0.3496 \pm 0.004$ |
| (4) $\operatorname{RankGPT}_{3.5}$ | BM25 | 100 | $0.6855 \pm 0.006$ | $0.3335 \pm 0.002$ | $0.6202 \pm 0.005$ | $0.3525 \pm 0.002$ |
| (5) $\mathrm{RankGPT}_{4}$ | BM25 | 100 | $0.7500 \pm 0.002$ | $0.3703 \pm 0.004$ | $0.7036 \pm 0.004$ | $0.4134 \pm 0.004$ |
| (6) PRP-Sliding-10 (FLAN-T5-XXL) | BM25 | 100 | 0.6700 | - | 0.6735 | - |
| (7) PRP-Sliding-10 (FLAN-UL2) | BM25 | 100 | 0.7265 | - | 0.7046 | - |
| (8) PRP-Sliding-10 (Vicuna) | BM25 | 100 | 0.5606 | 0.2735 | 0.5367 | 0.2990 |
| (9) RankVicuna | BM25 | 100 | 0.6682 | 0.3316 | 0.6549 | 0.3789 |
| (10) RankVicuna | SPLADE++ ED | 100 | 0.7459 | 0.4416 | 0.7473 | 0.5183 |

Table 1: nDCG@10 and MAP@100 on DL19 and DL20 for different reranking pipelines, with BM25 and Contriever as baselines. Each reranker uses the top 100 retrieved results of the previous stage as input. Rows (3-4) and row (5) represent averages of six and three runs, respectively. We directly copied results in rows (6-7) from Qin et al. (2023). All other results are from our own experiments.

|  | OK | Wrong Format | Repetition | Missing | Total |
| :--- | :---: | :---: | :---: | :---: | :---: |
| RankGPT $_{3.5}$ | 838.67 | 0 | 1.16 | 33.16 | 873 |
| RankGPT $_{4}$ | 830.33 | 40.67 | 1.67 | 0.33 | 873 |
| RankVicuna | 873 | 0 | 0 | 0 | 873 |

Table 2: The number of malformed responses for each reranking method. Reported numbers for RankGPT 3.5 and RankGPT 4 are averages of three and six runs, respectively.

rameters. Specifically, compared to its teacher RankGPT $_{3.5}$, RankVicuna achieves higher scores on DL20 but slightly lower scores on DL19. Compared with another zero-shot reranking method, LRL, which uses RankGPT ${ }_{3.5}$, RankVicuna demonstrates considerably higher effectiveness on both DL19 and DL20.

We note that PRP-Sliding-10 (FLAN-T5-XXL) with 11B parameters is comparable to RankVicuna both in terms of model size and effectiveness. Other than being fully open-source, our main advantage over PRP-Sliding-10 (FLAN-T5-XXL) is the prompt cost: to bring the top 10 most relevant candidates to the top of the list, PRP-Sliding-10 (FLAN-T5-XXL) requires each passage to be included in $\sim 40$ prompts on average. In contrast, we only require two prompts for our listwise approach with a sliding window of size 20 and a stride of 10. Furthermore, training on the FLAN mixture, which includes the MS MARCO QA task, calls into question the validity of PRP-Sliding-10 (FLAN-T5-XXL) as a true zero-shot method. We suspect this to be a contributing factor to the effectiveness gap between PRP-Sliding-10 (FLAN-T5XXL) and PRP-Sliding-10 (Vicuna).

Not surprisingly, both RankGPT 4 (rumored to contain more than 1T parameters) and PRP-Sliding-
10 (FLAN-T5-UL2) with 20B parameters outperform RankVicuna. This could be because, in addition to the differences in model sizes, the effectiveness of RankVicuna is bounded by its teacher, RankGPT 3.5 .

Finally, in row (10), we used RankVicuna to rerank the top 100 candidates from SPLADE++ EnsembleDistil instead of BM25. This combination achieves effectiveness on par with RankGPT 4 with an open-source model that is more than two orders of magnitude smaller.

Table 2 shows the number of malformed responses generated by the RankGPT variants and RankVicuna, which we have grouped into the following categories:

1. Wrong Format: includes responses that do not follow the requested format. For example, when RankGPT $_{4}$ refuses to generate a sorted list, its response falls into this category.
2. Repetition: includes responses that contain repeated document ids.
3. Missing: includes responses with missing document ids.

Since RankVicuna is deterministic, we report the results of a single run. For every request in this

|  | Source |  | DL19 |  | DL20 |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Prev. | Top-k | nDCG@10 | MAP@100 | nDCG@10 | MAP@100 |
| (1a) BM25 | None | $\|C\|$ | 0.5058 | 0.2476 | 0.4796 | 0.2685 |
| (1b) RankVicuna | BM25 | 20 | 0.6164 | 0.2867 | 0.5986 | 0.3194 |
| (1c) RankVicuna | BM25 | 100 | 0.6682 | 0.3316 | 0.6549 | 0.3789 |
| (2a) BM25 + RM3 | None | $\mid \overline{\|C\|}$ | 0.5216 | 0.2807 | 0.4896 | 0.2821 |
| (2b) RankVicuna | BM25 + RM3 | 20 | 0.6053 | 0.3110 | 0.5825 | 0.3323 |
| (2c) RankVicuna | BM25 + RM3 | 100 | 0.6588 | 0.3573 | 0.6567 | 0.3991 |
| (3a) OpenAI ada2 | None | $\mid \overline{\|C\|}$ | 0.7035 | 0.4151 | 0.6759 | 0.4587 |
| (3b) RankVicuna | OpenAI ada2 | 20 | 0.7448 | 0.4398 | 0.7101 | 0.4718 |
| (3c) RankVicuna | OpenAI ada2 | 100 | 0.7374 | 0.4409 | 0.7210 | 0.4755 |
| (4a) DistillBERT KD TASB | None | $\|C\|$ | 0.7210 | 0.4050 | 0.6854 | 0.4520 |
| (4b) RankVicuna | DistillBERT KD TASB | 20 | 0.7588 | 0.4121 | 0.7404 | 0.4648 |
| (4c) RankVicuna | DistillBERT KD TASB | B $\quad 100$ | 0.7551 | 0.4170 | 0.7049 | 0.4620 |
| (5a) SPLADE++ ED | None | $\|C\|$ | 0.7308 | 0.4464 | 0.7197 | 0.4826 |
| (5b) RankVicuna | SPLADE++ ED | 20 | 0.7532 | 0.4491 | 0.7455 | 0.5150 |
| (5c) RankVicuna | SPLADE++ ED | 100 | 0.7459 | 0.4416 | 0.7473 | 0.5183 |

Table 3: nDCG@10 and MAP@100 for RankVicuna with different first-stage candidate generation methods. For each method, reranking is performed using the top 20 or 100 candidates.

run, RankVicuna returned a correctly formatted response. In contrast, for RankGPT 3.5 and Rank$\mathrm{GPT}_{4}$, we averaged the results of six and three runs, respectively. Both RankGPT methods occasionally return malformed responses. Most of the malformed responses from RankGPT 3.5 are missing documents in the ordered list; when malformed, RankGPT 4 mostly refuses to rank. Repetition is a rare problem for both RankGPT methods.

## 6 Ablation Studies

### 6.1 First-Stage Candidate Generation

To evaluate the impact of the quality and quantity of the generated candidates on the final results, we repeated our experiments with the following five first-stage retrieval methods using either top 20 or top 100 retrieved results: (1) BM25 (Robertson and Zaragoza, 2009), (2) BM25+RM3 (AbdulJaleel et al., 2004), (3) OpenAI ada2 (Neelakantan et al., 2022; Lin et al., 2023), (4) DistillBERT KD TASB (Hofstätter et al., 2021), (5) SPLADE++ EnsembleDistil (ED) (Formal et al., 2022). The first two represent strong traditional "bag-of-words" retrieval baselines; the others represent a sample of effective neural first-stage retrievers that are commonly seen in research studies today. OpenAI ada2 and DistillBERT KD TASB are dense retrieval methods, while SPLADE++ ED is a sparse one.

Our experiment shows that as the first-stage effectiveness increases, additional improvements from RankVicuna decrease (see Table 3). For example, while RankVicuna over the top 100 BM25 candidates improves effectiveness by $30 \%-45 \%$ for all metrics, the improvement for SPLADE++ ED is only $2 \%-4 \%$ for the same metrics. This is a commonly noted phenomenon across multi-stage ranking systems (Pradeep et al., 2021, 2022b,a).

Comparing top 20 vs. top 100 results shows that reranking more candidates generally results in a higher MAP@ 100. However, in cases where the first-stage effectiveness is "good enough", rows (35) for DL19 and rows (4-5) for DL20, reranking only the top 20 candidates achieves an nDCG@ 10 score on par with reranking the top 100 candidates.

### 6.2 Data Augmentation

Section 3.2 discussed the training process of RankVicuna, highlighting the use of data augmentation (DA) as a crucial step in our training pipeline. To recap, the DA process involves shuffling the input order of the documents and permuting the original generations provided by the teacher. This step exposes the model to a more complex reordering task, which hopefully enhances its robustness and effectiveness.

In this section, we study the dependence of RankVicuna on the order of generated candidates. We compared two versions of the model: (1) the default version trained using Data Augmentation (DA), and (2) a variant trained without DA. Experimental results are shown in Table 4.

Using BM25 as the first stage, our experiments show that RankVicuna without DA results in worse effectiveness than using RankVicuna with DA.

|  | Source | DL19 |  | DL20 |  |  |
| :--- | :--- | :---: | :--- | :--- | :--- | :--- |
|  | Prev. | Top-k | nDCG@10 | MAP@100 | nDCG@10 | MAP@ 100 |
| (1a) RankVicuna | BM25 | 100 | 0.6682 | 0.3316 | 0.6549 | 0.3789 |
| (1b) RankVicuna | Shuf. BM25 | 100 | $0.6702 \pm 0.009$ | $0.2977 \pm 0.006$ | $0.6537 \pm 0.006$ | $0.3553 \pm 0.006$ |
| (1c) RankVicuna | SPLADE++ ED | 100 | 0.7459 | 0.4416 | 0.7473 | 0.5183 |
| (1d) RankVicuna | Shuf. SPLADE++ ED | 100 | $0.7271 \pm 0.009$ | $0.3860 \pm 0.008$ | $0.7071 \pm 0.007$ | $0.4312 \pm 0.006$ |
| (2a) RankVicuna (w/o DA) | BM25 | 100 | 0.6612 | 0.3254 | 0.6420 | 0.3612 |
| (2b) RankVicuna (w/o DA) | Shuf. BM25 | 100 | $0.5893 \pm 0.017$ | $0.2666 \pm 0.011$ | $0.5293 \pm 0.010$ | $0.2754 \pm 0.007$ |
| (2c) RankVicuna (w/o DA) | SPLADE++ ED | 100 | 0.7653 | 0.4672 | 0.7536 | 0.5180 |
| (2d) RankVicuna (w/o DA) | Shuf. SPLADE++ ED | 100 | $0.5893 \pm 0.010$ | $0.3289 \pm 0.009$ | $0.5373 \pm 0.020$ | $0.3406 \pm 0.013$ |

Table 4: nDCG@10 and MAP@100 of two variants of RankVicuna with different first-stage candidate generation methods. For each method, reranking is performed using top 100 candidates from the previous step on six shuffled orderings. We report average metrics and with $99 \%$ confidence intervals.

![](https://cdn.mathpix.com/cropped/2024_05_26_310ecebb043844291274g-07.jpg?height=480&width=1628&top_left_y=851&top_left_x=228)

Figure 2: Comparing the effectiveness of RankVicuna vs. PRPVicuna on DL19 and DL20, varying the number of times the ranked list is progressively refined. The zeroth pass corresponds to the BM25 run.

When we replace BM25 with SPLADE++ ED, RankVicuna without DA outperforms RankVicuna with DA. While data augmentation can cause a small drop in effectiveness (depending on the first stage), it makes the model less vulnerable to poor quality candidates (whether intentional or not), as shown by Qin et al. (2023) in methods like PRPSliding-10 and RankGPT 3.5 .

To showcase this vulnerability, we provided both model variants with shuffled candidate documents (rows $b$ and $d$ ). The results show that the model without DA exhibited a significant effectiveness drop (up to $34 \%$ ) and higher variance among different runs. In contrast, the default model, which is more robust due to its exposure to a more complex reordering task, better retained its effectiveness (comparing rows $b$ vs. $a$ and $d$ vs. $c$, respectively, for each version).

### 6.3 Effect of Progressive Reranking

Finally, Figure 2 compares the effectiveness of two reranking methods, RankVicuna and a variant of PRP-Sliding from Qin et al. (2023), we call PRPVicuna, on two datasets, DL19 and DL20. The $x$-axis represents the number of sliding window passes, ranging from 0 to 10 , and the $y$-axis represents the nDCG @10 score. We plot four curves, each representing a combination of a reranking method and a dataset. The solid lines show results on DL19 and the dashed lines show results on DL20. The blue lines represent the RankVicuna method and the red lines represent the PRPVicuna method (Qin et al., 2023).

We see that, for both datasets, RankVicuna consistently outperforms PRPVicuna. The nDCG@ 10 score for RankVicuna on DL19 starts at 0.5058 and increases to 0.6837 at the second pass, remaining relatively stable thereafter. The score for RankVicuna on DL20 follows a similar pattern, starting at 0.4796 and rising to about 0.6604 at pass four, albeit at a slower pace after the first pass. On the other hand, the nDCG@10 scores for PRPVicuna on both datasets increase gradually with each pass but remain far below RankVicuna.

This plot suggests that RankVicuna is more effective than PRPVicuna and that multiple passes of the sliding window have a minimal impact as an effectiveness boost for RankVicuna. It is also
worth noting that a single pass of reranking with both methods takes about the same time, around 30 seconds per query using a batch size of one on an RTX A6000 GPU. These results show that RankVicuna is much more efficient and achieves quicker convergence to the best possible results. This is likely because PRPVicuna handles only two passages at a time, whereas RankVicuna attends to 20 passages simultaneously, resulting in more effective relevance estimation.

## 7 Conclusion

In this study, we introduce RankVicuna, a listwise zero-shot reranking approach powered by an opensource large language model, Vicuna. Experimental studies show that our model achieves effectiveness on par with much larger models. We also quantitatively demonstrated the stability of RankVicuna results compared to closed-source counterparts.

Along the way, we explored many aspects of prompt-decoder models for reranking, including the impact of first-stage retrievers on downstream effectiveness. Our work also sheds light on the importance of data augmentation for system robustness, which plays a vital role in ensuring stability in the face of document shuffling and variations in initial retrieval quality.

In summary, RankVicuna advances zero-shot reranking for information retrieval, demonstrating the potential of large language models to enhance search effectiveness, even in data-scarce settings. We are able to achieve high-quality reranking using fully open-source models, which provides a firm foundation for the rest of the research community to build on. As we further refine and expand these techniques, we anticipate exciting opportunities for integrating large language models into end-to-end information access applications.

## Acknowledgments

This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada.

## References

Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In Proceedings of the
Thirteenth Text REtrieval Conference (TREC 2004), Gaithersburg, Maryland.

Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. MS MARCO: A human generated machine reading comprehension dataset. arXiv:arXiv:1611.09268v3.

Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Unsupervised dataset generation for information retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2022), pages 2387-2392, Madrid, Spain.

Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-effective unsupervised training of efficient rankers. arXiv:2301.02998.

B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems. In Proceedings of the Third ACM International Conference on Web Search and Data Mining (WSDM 2010), pages 411420, New York, New York.

Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning track. arXiv:2102.07662.

Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC 2019 deep learning track. arXiv:2003.07820.

Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv:2209.11755.

Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE v2: Sparse lexical and expansion model for information retrieval. arXiv:2109.10086.

Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2022. From distillation to hard negative sampling: Making sparse neural ir models more effective. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2022), page 2353-2359, Madrid, Spain.

Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Rethink training of BERT rerankers in multi-stage retrieval pipeline. In Proceedings of the 43rd European Conference on Information Retrieval (ECIR 2021).

Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise zero-shot dense retrieval without relevance labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1762-1777, Toronto, Canada.

Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pages 113-122.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv:2112.09118.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online.

Jimmy Lin. 2021. A proposed conceptual framework for a representational approach to information retrieval. arXiv:2110.01529.

Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pages 2356-2362.

Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021b. Pretrained Transformers for Text Ranking: BERT and Beyond. Morgan \& Claypool Publishers.

Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian. 2023. Vector search with OpenAI embeddings: Lucene is all you need. arXiv:2308.14963,

Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with a large language model. arXiv:2305.02156.

Irina Matveeva, Chris Burges, Timo Burkard, Andy Laucius, and Leon Wong. 2006. High accuracy retrieval with multiple nested ranker. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006), pages 437-444, Seattle, Washington.

Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power,
Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. 2022. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv: Arxiv2201.10005.

Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking with BERT. arXiv:1901.04085.

Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence model. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 708-718.

Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. arXiv:1910.14424.

Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, and Bing Xiang. 2020. Beyond [CLS] through ranking by generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1722-1727, Online.

Ronak Pradeep, Kai Hui, Jai Gupta, Adam D. Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, and Vinh Q. Tran. 2023. How does generative retrieval scale to millions of passages? arXiv:2305.11841.

Ronak Pradeep, Yilin Li, Yuetong Wang, and Jimmy Lin. 2022a. Neural query synthesis and domain-specific ranking templates for multi-stage clinical trial matching. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2022), pages $2325-$ 2330, Madrid, Spain.

Ronak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, Andrew Yates, and Jimmy Lin. 2022b. Squeezing water from a stone: A bag of tricks for further improving cross-encoder effectiveness for reranking. In Proceedings of the 44th European Conference on Information Retrieval (ECIR 2022), Part I, pages 655-670, Stavanger, Norway.

Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models. arXiv:2101.05667.

Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv:2306.17563.

Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333-389.

Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions are all you need to train a dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600-616.

Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT good at search? Investigating large language models as re-ranking agent. arXiv:2304.09542.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv: 2307.09288 .

Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A cascade ranking model for efficient ranked retrieval. In Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2011), pages 105-114, Beijing, China.

Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. 2022. RankT5: Finetuning T5 for text ranking with ranking losses. arXiv:2210.10634.
