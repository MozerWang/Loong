# OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 

Fuzhao Xue $^{1 \dagger} \quad$ Zian Zheng $^{1} \quad$ Yao Fu $^{2} \quad$ Jinjie Ni ${ }^{1} \quad$ Zangwei Zheng $^{1}$

Wangchunshu Zhou ${ }^{3}$

Yang You ${ }^{1}$

${ }^{1}$ National University of Singapore

${ }^{2}$ University of Edinburgh

${ }^{3}$ ETH Zurich


#### Abstract

To help the open-source community have a better understanding of Mixture-ofExperts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from $650 \mathrm{M}$ to $34 \mathrm{~B}$ parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable costeffectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.

One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towardsthe-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs. 3


${ }^{\dagger}$ Email: f.xue@u.nus.edu

${ }^{3}$ https://github.com/XueFuzhao/OpenMoE

## 1 Introduction

Large Language Model (LLM) has exhibited remarkable performance on various NLP tasks [29. 37], and has even become a part of our daily lives through chatbot applications such as ChatGPT, Bard, and Copilot. However, LLMs are computationally expensive in both training and inference. As LLMs become increasingly prevalent, enhancing their performance without proportionally increasing computational resources is a critical challenge. In response to this challenge, Fedus et al. [15] and Riquelme et al. [38] proposed the Mixture-of-Experts (MoE) to scale up the trainable parameters of the transformer with little additional computation overhead. Recent advancements in MoE-based language models, such as GLaM [14] and ST-MoE [61] have demonstrated superior performance in various tasks. However, before the release of OpenMoE, there were few open-sourced MoE language models trained with trillion-level diverse datasets.

In this work, we set forth three primary goals: (1) To offer a first-attempt solution in detail for training a decoder-only MoE model within the existing framework of training LLMs. (2) To perform an in-depth analysis of the MoE routing mechanisms, thereby providing the research community with deeper insights into the behaviors and potential limitations of MoE-based LLMs. (3) To pave the way for future MoE LLM development. Through this early endeavor, we aim to stimulate and accelerate the growth of the open-source MoE community.

Releasing OpenMoE. First, we release OpenMoE, a series of open-sourced MoE-based LLMs, including: (1) OpenMoE-Base/16E: a small model with 0.65B parameters for debugging purposes. $16 \mathrm{E}$ means 16 experts per MoE layer; (2) OpenMoE-8B/32E: this variant features $8 \mathrm{~B}$ parameters in total, activating around 2B parameters per token in Transformer blocks, and is pre-trained on over 1 trillion tokens; (3) OpenMoE-8B/32E-Chat, a chat version of OpenMoE-8B/32E, fine-tuned with a $100 \mathrm{~K}$ subset of the WildChat [2] dataset; (4) OpenMoE-34B/32E: a larger scale model, activating 6B parameters per token in Transformer blocks and trained with 200B tokens, serving as a testament to the scalability of our approach. Detailed configuration can be found in Appendix B Our OpenMoE-8B/32E models achieved comparable performance with OpenLLaMA-3B [19] and TinyLLaMA-1.1B [56], two dense open LLMs used higher training cost. Notably, On the MTBench [58], OpenMoE-8B/32E-Chat outperformed the two dense LLMs significantly on the singleturn conversation. In addition, we release 5 intermediate checkpoints of OpenMoE-8B/32E, each trained with 200B more tokens than the previous one, to support and encourage future research. Section 2 and 3 will discuss the design, training details, and evaluation results of OpenMoE.

Exploring Advanced Training Strategies. As part of our research endeavor, we are committed to exploring more advanced Techniques in LLM training: (1) Different from the common practice of training models on in-house or text-dominated open-sourced data, we train OpenMoE with a substantial proportion of code, constituting up to $52.25 \%$ during the early stages of pre-training; (2) Moving beyond the conventional next-token prediction training objective, we investigate UL2 training objective [47], motivated by its proven effectiveness in previous work [1] and its good alignment with coding data [5]. We acknowledge that the performance of our model, while acceptable, does not significantly exceed our expectations, which may be attributed to some sub-optimal design choices. Nevertheless, we believe that this exploratory work offers substantial value to the opensource community, particularly in assessing the potential and effectiveness of these under-explored techniques.

Studying MoE Routing In-depth. While MoE is effective, there remains a lack of study on why MoE performs well. From a high level, MoE introduces more trainable parameters than its dense counterpart. To keep the FLOPs fixed when scaling the number of parameters, MoE applies a routing layer that sparsely and adaptively assigns each token to a few experts. This process of sparse expert selection is crucial to MoE's functionality. Unfortunately, despite existing pieces of literature briefly visualizing the routing decison [28, 33, 38, 42, 61], we still don't have a clear understanding of how the router works and how the routing decision impacts the results in MoE models, especially for the post-ChatGPT LLMs trained on a mixture of datasets from diverse domains. In this work, we study this problem based on various taxonomies, including domain, language, task, and token. Our key findings are as follows: (1) Context-independent Specialization: MoE tends to simply cluster tokens based on similar token-level semantics, implying that, regardless of context, a certain token is more likely to be routed to a certain expert; (2) Early Routing Learning: Token ID routing specialization is established early in pre-training and remains largely fixed, resulting in tokens being consistently processed by the same experts throughout the training; (3) Drop-towards-the-End:

Since each expert has a fixed max capacity, tokens appearing later in the sequence face a higher risk of being dropped if the expert is already at capacity. This issue is more severe in instruction-tuning datasets. These datasets often exhibit a domain gap compared to the pre-training data, meaning that the balanced token assignment strategies established and solidified during early pre-training may not be samely effective in instruction-tuning scenarios. This is concerning as instruction data plays an important role in deploying LLMs to real-world applications. Section 4 discusses the above phenomenons in detail.

Rethinking Our Mistakes and Proposing Potential Solutions. In retrospect, our project encountered several mistakes and made sub-optimal decisions (e.g., aggressive data mixture), as detailed in Section 5 As an early open-source effort, we believe that sharing these experiences and insights is crucial, perhaps even more important than solely focusing on successful strategies. Based on our empirical findings during training and subsequent visualization analysis (Section4), we have developed a set of potential solutions. We sincerely hope these insights can help the community develop better models in the future.

The structure of this paper mirrors the lifecycle of the OpenMoE project, encompassing all its phases. This includes the initial design (Section 2), training and evaluation (Section 3, in-depth analysis (Section 4), and a rethinking of the OpenMoE project (Section 5).

## 2 Designing OpenMoE

First, we introduce our initialized design of OpenMoE models regarding the pre-training data, model architecture, training objective, and supervised fine-tuning data.

### 2.1 Pre-training Dataset: More Code than Usual

Modern LLMs are usually trained by a combination of datasets from various domains, i.e., data mixture [7, 9, 21, 36, 49]. Except for the LLMs customized towards coding (e.g., StarCoder [30], CodeLLaMA [40]), most existing models' pre-training data is dominated by text data. For instance, the sampling rate of the GitHub dataset is only $4.5 \%$ for LLaMA [49]. However, we argue that the code data is highly important for two reasons. First, the code data is likely to improve the ability of complex reasoning with chain-of-thought [16]. More importantly, different from natural language, which is sometimes blurry and easy to misunderstand, code is always precise. This enables code to be a more efficient language for machines to convey information concisely without misunderstanding between different (embodied) AI agents, and as a result, code has great potential to dominate LLM communications in real-life applications. Therefore, we design a more code-dominated pre-training data mixture. As shown in Table 1, we extracted $50 \%$ of data from the RedPajama [11] and $50 \%$ of data from the duplication version of The Stack [25]. Our experimental results show that the version I data mixture might be a bit aggressive in its code proportion. We fix these issues at the later stage of pre-training, please see the following Section 3.2 for details.

Table 1: Three versions of OpenMoE pre-training data mixture.

| Model <br> Period | Version I <br> OpenMoE-Base, <br> before 780B tokens <br> OpenMoE-8B/32E <br> after 780B tokens | Version II <br> OpenMoE-34B/32E <br> from start to end |  |
| :--- | :---: | :---: | :---: |
| Dataset | $50.0 \%$ | Sampling Ratio |  |
| RedPajama | $7.50 \%$ | $83.5 \%$ | $67.5 \%$ |
| C4 | $2.25 \%$ | $15.0 \%$ | $15.0 \%$ |
| Wikipedia | $1.00 \%$ | $6.50 \%$ | $4.50 \%$ |
| Stackexchange | $1.25 \%$ | $2.50 \%$ | $1.00 \%$ |
| ArXiv | $2.25 \%$ | $4.50 \%$ | $4.50 \%$ |
| Books | $2.25 \%$ | $6.50 \%$ | $4.50 \%$ |
| GitHub | $33.5 \%$ | $5.00 \%$ | $5.00 \%$ |
| Commoncrawl | $0.00 \%$ | $6.50 \%$ | $33.0 \%$ |
| Wikipedia-en | $50.0 \%$ | $10.0 \%$ | $2.50 \%$ |
| The Stack Dedup | $50.0 \%$ | $30.0 \%$ |  |

### 2.2 Model Architecture: Decoder-only ST-MoE

Tokenizer. We applied umT5 [10] tokenizer with $256 \mathrm{~K}$ vocab size for two reasons: (1) umT5 tokenizer with a large multi-lingual vocab supports low-resource language better than the tokenizers using a small vocab (e.g., LLaMA tokenizer with $32 \mathrm{~K}$ vocab); (2) comparing to some old tokenizers, such as BERT [24] and T5 [37] tokenizer, umT5 tokenizer has byte fallback feature to support out-of-vocab tokens better.

Token-choice Routing. We generally follow ST-MoE [61] for our model architecture and routing design to ensure training stability, which is extremely important when training larger models. Given $E$ trainable experts and input representation $x \in \mathbb{R}^{D}$, the output of MoE model can be formulated as:

$$
\begin{equation*}
\operatorname{MoE}(x)=\sum_{i=1}^{E} g(x)_{i} e_{i}(x) \tag{1}
\end{equation*}
$$

where $e_{i}(\cdot)$ is a non-linear transformation $\mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ of the $i^{\text {th }}$ expert, and $g(\cdot)_{i}$ is the $i^{\text {th }}$ element of the output of the trainable router $g(\cdot)$, a non-linear mapping $\mathbb{R}^{D} \rightarrow \mathbb{R}^{E}$. Usually, both $e(\cdot)$ and $g(\cdot)$ are parameterized by neural networks. Please note each expert is an FFN layer instead of a complete Transformer model in most MoE-based Transformer models, including ours.

Top-2 Selection. According to the formulation above, when $g(\cdot)$ is a sparse vector, only part of the experts would be activated and updated by back-propagation during training. We set the gating layer as a top-K selection as:

$$
\begin{equation*}
\mathrm{g}(x)=\operatorname{TopK}(\operatorname{softmax}(f(x))) \tag{2}
\end{equation*}
$$

where $f(\cdot)$ is routing linear transformation $\mathbb{R}^{D} \rightarrow \mathbb{R}^{E}$. When $K \ll E$, most elements of $\mathrm{g}(x)$ would be zero so that sparse conditional computation is achieved. We set $K=2$ following Zoph et al. [61].

Residual MoE. Each vanilla Transformer block can be written as:

$$
\begin{align*}
x^{\prime} & =\operatorname{LayerNorm}_{i}^{\mathrm{att}}(x) \\
x & =\operatorname{MHA}\left(x^{\prime}\right)+x \\
x^{\prime \prime} & =\operatorname{LayerNorm}_{i}^{\mathrm{ffn}}(x)  \tag{3}\\
x & =\operatorname{FFN}\left(x^{\prime \prime}\right)+x
\end{align*}
$$

In OpenMoE, for each MoE-based Transformer block, we use one residual MoE layer to ensure that one fixed FFN layer is always activated for every token. That is:

$$
\begin{align*}
& x^{\prime}=\operatorname{LayerNorm}_{i}^{\text {att }}(x) \\
& x=\operatorname{MHA}\left(x^{\prime}\right)+x \\
& x^{\prime \prime}=\operatorname{LayerNorm}_{i}^{\mathrm{ffn}}(x),  \tag{4}\\
& x=\operatorname{MoE}\left(x^{\prime \prime}\right)+\operatorname{FFN}\left(x^{\prime \prime}\right)+x
\end{align*}
$$

Note we use MoE-based Transformer blocks in an interleaved manner instead of placing MoE in every Transformer block. In our setting, we use MoE every 4 layers in OpenMoE-Base/16E and OpenMoE 34B/32E and use MoE every 6 layers for OpenMoE-8B/32E. This setting is inspired by the findings in ViT-MoE [38], i.e., using MoE every layer introduces more computational overhead during routing, and then induces a worse cost-effective trade-off than interleaved MoE usage.

Load Balance Loss and Router Z-loss. ST-MoE [61] follows Shazeer et al. [42], using MoE load balance loss to ensure a balanced number of tokens assigned to different experts so that MoE models can achieve better parallelism. For each routing operation, given $E$ experts and $N$ batches with $B=N L$ tokens, the following auxiliary loss is added to the total model loss during training:

$$
\begin{equation*}
L_{b}=E \cdot \sum_{i=1}^{E} m_{i} \cdot P_{i} \tag{5}
\end{equation*}
$$

where $m$ is a vector, $P_{i}$ is softmax $(f(x)) . i$ denotes the expert ID. The $i^{\text {th }}$ element is the fraction of tokens dispatched to expert $i$ :

$$
\begin{equation*}
m_{i}=\frac{1}{B} \sum_{j=1}^{B} \mathrm{~h}\left(x_{j}\right)_{i} \tag{6}
\end{equation*}
$$

Table 2: UL2's mixture-of-denoisers configuration, $\mu$ is average span length and $r$ is the mask ratio.

| Training Objective | Percentage |
| :--- | :---: |
| PrefixLM, $r=0.5$ | $50 \%$ |
| SpanCorrupt |  |
| $\mu=3, r=0.15$ | $10 \%$ |
| $\mu=8, r=0.15$ | $10 \%$ |
| $\mu=3, r=0.5$ | $10 \%$ |
| $\mu=8, r=0.5$ | $10 \%$ |
| $\mu=64, r=0.5$ | $10 \%$ |

where $\mathrm{h}(\cdot)$ is an index vector selected by TopK in Eq. 2. $\mathrm{h}\left(x_{j}\right)_{i}$ is the $i^{\text {th }}$ element of $\mathrm{h}\left(x_{j}\right)$. It is noticeable that, different from $g(x)_{i}$ in Eq. 2. $m_{i}$ and $\mathrm{h}\left(x_{j}\right)_{i}$ are non-differentiable. However, a differentiable loss function is required to optimize $\mathrm{MoE}$ in an end-to-end fashion, so we use the routing score softmax $(f(x))$ in Eq. 2 (i.e., $P_{i}$ in Eq. 5) to make the routing decision differentiable and then learnable.

In addition to the load balance loss, Zoph et al. [61] proposed router z-loss for more stable MoE training:

$$
\begin{equation*}
L_{z}(x)=\frac{1}{B} \sum_{i=1}^{B}\left(\log \sum_{j=1}^{E} e^{x_{j}^{(i)}}\right)^{2} \tag{7}
\end{equation*}
$$

This router z-loss can penalize large logits input to the gating network and encourage the absolute magnitude of numbers to be small so that it can reduce the round-off errors in $\mathrm{MoE}$ layers. Please refer to ST-MoE paper [61] for a detailed explanation.

Taken together, our final training loss can be written as:

$$
\begin{equation*}
L=L_{C E}+L_{b}+L_{z} \tag{8}
\end{equation*}
$$

where $L_{C E}$ is the cross-entropy loss in language model pre-training.

### 2.3 Training Objective: UL2 and CasualLM

Instead of adopting vanilla casual language modeling (CasualLM) directly, we explore UL2 [46], a more diverse language model pre-training objective combining span corruption (SpanCorrupt) and prefix language modeling (PrefixLM) [37]. It is noteworthy that the SpanCorrupt in UL2 is more diverse than the vanilla SpanCorrupt because it mixes various span lengths and corruption rates. We have two reasons to explore UL2 in OpenMoE. First, UL2 has shown promising results in PaLM-2 [1]. More importantly, the aggressive token masking is very similar to the code completion task in the real world, such as Copilot. Bavarian et al. [5] also found that the similar filling-in-the-middle (FiM) objective can model the code better than the vanilla training objective. Since we used more code in our pre-training data mixture, adapting UL2 that covers FiM is a more reasonable choice intuitively.

Our detailed UL2 training objective configuration is shown in Table 2. We use only $20 \%$ low mask ratio ( $r=0.15$ ) because there are fewer output tokens during training, which may slow down the learning. We also use more PrefixLM than the default UL2 setting because we think the zero-shot and in-context learning ability enhanced by PrefixLM training is important. We faced some difficulties when training with UL2 in OpenMoE, which will be discussed in Section 3.2.

### 2.4 Supervised Fine-tuning

Although alignment is not the focus of this OpenMoE project, we still conduct supervised fine-tuning (SFT) with a subset of the open-sourced WildChat dataset [2] to enhance the instruction following ability and study the behavior of the MoE model before and after SFT. We only pick the instructionresponse pairs from GPT-4 in WildChat because of the lack of computation resources at the late stage of OpenMoE development. The subset includes $58 \mathrm{~K}$ conversations and each conversation includes 1.8 turns on average.

Table 3: Ablation study with OpenMoE-Base/16E on zero-shot TriviaQA [23].

| Method | EM | F1 |
| :--- | :---: | :---: |
| OpenMoE | 1.4 | 4.5 |
| w/o MoE | 0.1 | 0.3 |
| w/o UL2 (PrefixLM only) | 0.0 | 0.0 |
| w/o Code data | 0.7 | 1.1 |
| w/ LLaMA tokenizer | 2.2 | 5.7 |

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-06.jpg?height=564&width=1353&top_left_y=618&top_left_x=386)

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-06.jpg?height=448&width=569&top_left_y=627&top_left_x=428)

(a) Comparison of the validation loss on different pre-training datasets.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-06.jpg?height=436&width=574&top_left_y=644&top_left_x=1098)

(b) Comparison of validation accuracy on different pre-training datasets.

Figure 1: Comparison of the validation loss and accuracy on different pre-training datasets. We can observe that models are easier to achieve higher accuracy and lower loss on code data.

### 2.5 Other Designs

Following recent LLMs, we adopt RoPE [45] for position embedding and SwiGLU [41] for activation function for FFNs in both dense and MoE Transformer blocks. More detailed model configuration and training hyperparameters for OpenMoE models can be found in Appendix B. We applied data parallelism, tensor parallelism [43, 52], and expert parallelism [27] for training models at scale. We train OpenMoE models on Google Cloud TPU with 64 to 512 v3 chips depending on the availability.

## 3 Training OpenMoE

### 3.1 Ablation Study

As an initial evaluation of our design decisions, we conducted an ablation study using the OpenMoEBase/16E model. It's important to note that while these results provide early insights, we cannot be certain of their generalizability to larger models, primarily due to computational resource constraints that preclude larger-scale ablations.

Our findings indicate that several elements - the MoE approach, the UL2 training objective, and the increased emphasis on code data - all contribute positively to the base version's performance in zero-shot TriviaQA tasks. The model using LLaMA tokenizer [49] outperforms the one with umT5 tokenizer. This outcome is considered acceptable, even though a larger vocabulary size might slightly impair performance. We believe that supporting low-resource languages is crucial, as foundational models should be accessible and beneficial to a diverse global audience. After this sanctity check, we proceed to scale OpenMoE up to OpenMoE-8B/32E.

We also conduct an ablation study to compare the progress of learning the data from different domains. As shown in Figure 1, we can observe that models are easier to achieve higher accuracy and lower loss on code data. On Github, although our model is small, it can still achieve over $80 \%$ token prediction accuracy. We infer that this is because of the long-tail token distribution in code data. For instance, a large number of tokens in code are "\n" and "\t", which are relatively easier to predict.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-07.jpg?height=468&width=621&top_left_y=243&top_left_x=752)

Figure 2: Token prediction accuracy of OpenMoE models. OpenMoE-8B/32E uses UL2 before 390K steps and falls back to CasualLM after 790K steps. OpenMoE-34B/32E uses UL2 until 50B tokens.

### 3.2 Training Progress

UL2 Saturation During training, we found that, although UL2 can help the model to learn faster at the early stage of training, it is easier to saturate at the later training stage of OpenMoE-8B/32E. As shown in Figure 2 , if we zoom in, we can find that OpenMoE-8B/32E improves very slowly from $35 \mathrm{~K}$ to $39 \mathrm{~K}$ steps. We suggest that this may be because, although UL2 is more diverse, the SpanCorrupt is still relatively easy compared to CasualLM. Therefore, we fall back to CasualLM after 390K steps (780B) tokens. In addition, since code data aligns better with UL2 and our initial code data mixture is relatively aggressive, we also decreased our code data sampling ratio to $15 \%$. The Second version data mixture is reported in Table 1 .

Obviously, in Figure 2, after 780B tokens, there is a significant drop in the token prediction accuracy after $390 \mathrm{~K}$ steps for OpenMoE-8B/32E. This is caused by the more difficult CasualLM objective and less easy code data. Note that, although we encountered a saturation issue at the later stage of OpenMoE-8B/32E training, we think such an easy-to-hard curriculum may be helpful for LLM training. Therefore, we still adapted UL2 for $25 \mathrm{~K}$ steps (50B tokens) in OpenMoE-34B/32E. We used a relatively moderate code-heavy data mixture in OpenMoE-34B/32E. As shown in Table 1, we utilize $35 \%$ of code data in total. Due to the computation resource limitation, we train OpenMoE-34B/32E with only 200B tokens to verify its scalability. We leave training a large-scale OpenMoE with more tokens as future work if possible.

### 3.3 Evaluation on Benchmarks

### 3.3.1 Raw Model Evaluation

Before all, we highlight that we did not hack the benchmarks at all and the pre-training is purely on the open-sourced datasets mentioned above. Since our model is relatively small in terms of training budget, we mainly evaluate the raw model on established but not that hard benchmarks, i.e., TriviaQA [23], HumanEval [8], WMT16-En-Ro [6], BigBench-Lite (24 tasks) [4], and a subset of the lm-evaluation-harness collection [18] with 13 tasks. For popular but relatively challenging benchmarks like 5-shot MMLU [20], our OpenMoE-8B/32E achieves around $26.2 \%$ accuracy, which means the model is almost randomly guessing from the four options. We mainly compare with the open-sourced models with more training cost, i.e., TinyLLaMA-1.1B [56] and OpenLLaMA-3B [19]. On BigBench-Lite, we also compare with GPT-3 [7], Big-G [4] and Big-G-Sparse [4]. Big-G and Big-G-Sparse are two sets of Google in-house Transformer models evaluated on BigBench-Lite, and Big-G-Sparse models are MoE-based Transformers.

We first report our results on Commonsense QA (TriviaQA), Coding (HumanEval), and Low-resource Machine Translation (WMT16 En-Ro). We think these three benchmarks are meaningful for us because (1) Commonsense is to check whether OpenMoE can memorize more commonsense given its efficient parameter scaling advantage; (2) Coding is important because of its prevalent use cases in solving coding-related user prompts, LLM as agents, and embodied AI; (3) Low-resource Machine Translation is important because we want to share the benefits of foundation models to everyone on

Table 4: Results on TriviaQA (Exact Match). We also report the number of training tokens from Wikipedia because the commonsense questions in TriviaQA have a relatively close relation with Wikipedia data.

| Model | Act. Params | Total Tokens | Text Tokens | Wiki Tokens | TriviaQA |
| :--- | :---: | :---: | :---: | :---: | :---: |
| TinyLLaMA-1.1B | $0.9 \mathrm{~B}$ | $3.0 \mathrm{~T}$ | $2.1 \mathrm{~T}$ | $75 \mathrm{~B}$ | 11.2 |
| OpenLLaMA-3B | $2.9 \mathrm{~B}$ | $1.0 \mathrm{~T}$ | $991 \mathrm{~B}$ | $24 \mathrm{~B}$ | 29.7 |
| OpenMoE-8B/32E | $2.1 \mathrm{~B}$ | $1.1 \mathrm{~T}$ | $644 \mathrm{~B}$ | $58 \mathrm{~B}$ | 32.7 |
| OpenMoE-34B/32E | 6.4B | $0.2 \mathrm{~T}$ | $130 \mathrm{~B}$ | $14 \mathrm{~B}$ | 31.3 |

Table 5: Results on HumanEval (Pass @ 1). We also report the number of training tokens from the code domain (The Stack and GitHub data).

| Model | Act. Params | Total Tokens | Code Tokens | HumanEval |
| :--- | :---: | :---: | :---: | :---: |
| TinyLLaMA-1.1B | $0.9 \mathrm{~B}$ | $3.0 \mathrm{~T}$ | $900 \mathrm{~B}$ | 9.1 |
| OpenLLaMA-3B | 2.9B | $1.0 \mathrm{~T}$ | $59 \mathrm{~B}$ | 0 |
| OpenMoE-8B/32E | 2.1B | $1.1 \mathrm{~T}$ | $456 \mathrm{~B}$ | 9.8 |
| OpenMoE-34B/32E | 6.4B | $0.2 \mathrm{~T}$ | $70 \mathrm{~B}$ | 10.3 |

earth. As shown in Table 4. OpenMoE-8B/32E outperforms baselines clearly with less training cost (Activated parameter $\times$ Total Training tokens). Also, please note that TinyLLaMA-1.1B performs significantly worse than other models on TriviaQA although it has a comparable training cost with OpenLLaMA-3B. Therefore, this highlights the importance of the number of parameters to keeping knowledge in LLMs, which also indicates the significance of using MoE.

In Table 5, OpenMoE models achieve better performance than baselines. OpenMoE-34B/32E only used 70B code data, while it still performs relatively well on HumanEval, which shows the scalability of OpenMoE, although we don't have enough computation resources to train it until the end. OpenLLaMA-3B struggles on HumanEval because consecutive whitespaces are treated as one, contradicting the Python syntax[34].

Table 6 shows our results on WMT16 En-Ro translation task. Note that our model did not include much multi-lingual data intentionally. Most multi-lingual data should be from the multi-lingual version of Wikipedia in the RedPajama, which is also used in TinyLLaMA-1.1B and OpenLLaMA-3B. However, OpenMoE models still show better results than baselines, which potentially highlights the importance of umT5 tokenizer.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-08.jpg?height=551&width=1111&top_left_y=1752&top_left_x=496)

Figure 3: Results on BigBench-Lite. The relative cost is computed based on multiplying activated parameters in the Transformer and the number of training tokens. The size of the color dots denotes the number of activated parameters, and the size of the shadow denotes the number of total parameters for MoE models.

Table 6: Results on WMT16 En-Ro (BLEU score). We also report the number of explicit multi-lingual tokens in the pre-training dataset, i.e., the multi-lingual version of Wikipedia from the RedPajama dataset.

| Model | Act. Params | Total Tokens | Multi-lingual Tokens | WMT16 En-Ro |
| :--- | :---: | :---: | :---: | :---: |
| TinyLLaMA-1.1B | $0.9 \mathrm{~B}$ | $3.0 \mathrm{~T}$ | $75 \mathrm{~B}$ | 2.6 |
| OpenLLaMA-3B | $2.9 \mathrm{~B}$ | $1.0 \mathrm{~T}$ | $24 \mathrm{~B}$ | 1.9 |
| OpenMoE-8B/32E | $2.1 \mathrm{~B}$ | $1.1 \mathrm{~T}$ | $38 \mathrm{~B}$ | 3.1 |
| OpenMoE-34B/32E | $6.4 \mathrm{~B}$ | $0.2 \mathrm{~T}$ | $9 \mathrm{~B}$ | 3.4 |

Table 7: Evaluate OpenMoE-8B/32E on Im-evaluation-harness. The results of OpenLLaMA are from its homepage, which only provides two effective digits.

| Dataset | TinyLLaMA-1.1B | OpenLLaMA-3B | OpenMoE-8B/32E |
| :--- | :---: | :---: | :---: |
| ANLI-R1 | 34.2 | 33.0 | 32.7 |
| ANLI-R2 | 32.4 | 36.0 | 33.2 |
| ANLI-R3 | 35.1 | 38.0 | 33.9 |
| HellaSwag | 59.2 | 52.0 | 45.5 |
| WinoGrande | 59.1 | 63.0 | 60.3 |
| PIQA | 73.3 | 77.0 | 74.2 |
| ARC-Easy | 55.2 | 68.0 | 64.1 |
| ARC-Challenge | 30.1 | 34.0 | 30.3 |
| Boolq | 57.8 | 66.0 | 61.2 |
| TruthfulQA | 37.6 | 35.0 | 36.0 |
| OpenbookQA | 21.8 | 26.0 | 24.6 |
| RTE | 51.9 | 55.0 | 53.4 |
| WiC | 50.1 | 50.0 | 49.8 |
| Average | 45.9 | 48.7 | 46.1 |

In Figure 3, the relative cost is computed based on multiplying activated parameters (Act. Params) in Transformer blocks and the number of training tokens. The size of the color dots denotes the number of activated parameters, and the size of the shadow denotes the number of total parameters for MoE models. We can observe that OpenMoE achieved a better cost-effectiveness trade-off on BigBench-Lite, in terms of both training and inference cost.

We also evaluate OpenMoE on the 13 tasks from the LM-Evaluation-Harness collection. As shown in Table 7 , both OpenMoE and TinyLLaMA performed worse than OpenLLaMA. However, the scores achieved by OpenMOE are acceptable. We suggest that the initial high sampling rate on the code data may harm the results on these text-dominated benchmarks, which is one of the issues we will discuss in Section 5

### 3.3.2 Chat Model Evaluation

We further evaluate our model on MTBench, an established ChatBot benchmark that is able to examine models comprehensively. We report both single-turn and multi-turn results in Figure 4a and Table 8 We can observe that OpenMoE outperforms baselines by a large margin on the single-turn results, especially on coding tasks. However, OpenMoE's performance drops more on the second turn, which results in worse multi-turn results in Figure $4 \mathrm{~b}$ We found that this probably be caused by the token drop of a long sequence. Please see the following Section 4 for a detailed analysis.

## 4 Analyzing OpenMoE

We generally think MoE is an effective way to scale parameters up with a fixed computation budget. However, we have little idea about what the experts in MoE specialize in. In this section, we conduct an in-depth analysis of OpenMoE in multiple aspects to study the routing behavior.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-10.jpg?height=439&width=664&top_left_y=255&top_left_x=384)

(a) Single-turn results.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-10.jpg?height=453&width=664&top_left_y=245&top_left_x=1072)

(b) Multi-turn results.

Figure 4: Evaluate OpenMoE on MTBench.

Table 8: Average scores on MT-Bench.

| Model | MT-Bench 1st Turn | MT-Bench 2nd Turn | MT-Bench Avg |
| :--- | :---: | :---: | :---: |
| GPT-J-6B (0.4T) | 2.51 | 2.35 | 2.43 |
| TinyLLaMA-1.1B (3T) | 4.08 | 2.54 | 3.31 |
| OpenLLaMA-3B (1T) | 4.36 | $\mathbf{3 . 6 2}$ | $\mathbf{3 . 9 9}$ |
| OpenMoE-8B/32E (1.1T) | $\mathbf{4 . 6 9}$ | 3.26 | $\mathbf{3 . 9 8}$ |

### 4.1 What are the Experts Specializing in?

Does MoE specialize in domain level? We first visualize the routing decision of the tokens from different subsets in the RedPajama dataset. Note that all visualization results are from the third MoE layer by default because we did not observe significant differences across layers. We can observe that the tokens from different subsets (i.e., domains) are uniformed distributed on the plot. That is, although $E_{21}$ slightly prefers code tokens and $E_{10}$ like books a little, most experts in MoE are not specialized based on the domains.

Does MoE specialize in language level? We move forward toward finer-grain data to check whether MoE specializes in different coding languages and natural languages. In Figure 6, we compare 4 different coding languages, i.e., Assembly, Blitzmax, Java, and Python. Similar to the domain level,

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-10.jpg?height=634&width=829&top_left_y=1746&top_left_x=645)

Figure 5: Visualization of the routing decision on the RedPajama dataset. $E_{i}$ denotes the ratio of tokens routed to $i_{\text {th }}$ expert.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-11.jpg?height=613&width=783&top_left_y=249&top_left_x=668)

Figure 6: Visualization of the routing decision on TheStack dataset. $E_{i}$ denotes the ratio of tokens routed to $i_{\text {th }}$ expert.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-11.jpg?height=585&width=697&top_left_y=1025&top_left_x=714)

Figure 7: Visualization of the routing decision on TED-Parallel-Corpus including 12 languages, i.e., ar (Arabic), de (German), es (Spanish), fr (French), he (Hebrew), it (Italian), ja (Japanese), ko (Korean), nl (Dutch), ru (Russian), zh-cn (Chinese Simplified), zh-tw (Chinese, Traditional), $E_{i}$ denotes the ratio of tokens routed to the $i_{\mathrm{th}}$ expert.

even for Assembly and Blitzmax, i.e., two low-resource languages compared with Java and Python, they still did not exhibit significant expert specialization.

We further study the expert specialization on different natural languages. We adopted a multi-lingual parallel corpus, i.e., TED-Parallel-Corpus ${ }^{4}$ as the platform. In Figure 7, we found that there is a relatively clear specialization among different experts. For instance, zh-cn (Chinese, Simplified) and zh-tw (Chinese, Traditional) both have a strong preference for $E_{5}$ and $E_{16}$; ja (Japanese), and ko (Korean) both prefer $E_{14}$.

Does MoE specialize in task level? Based on the findings above, finer-grained data has clearer expert specialization observation. We then visualize the routing decision on MT-Bench conversation data in Figure 8 . We can see a similar specialization as above, especially for the math data. We suggest that the main reason is that the math tasks include more special tokens than other tasks.

Does MoE specialize in Position ID? Routers in MoE make decisions based on the token representations. The token representations are from token embeddings and position embeddings. We thus[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-12.jpg?height=599&width=767&top_left_y=251&top_left_x=668)

Figure 8: Visualization of the routing decision on MT-Bench. We adopt the conversation history when evaluating OpenMoE MT-Bench as the visualization data source. $E_{i}$ denotes the ratio of tokens routed to the $i_{\text {th }}$ expert.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-12.jpg?height=523&width=667&top_left_y=1102&top_left_x=363)

(a) Uniform sampled token IDs.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-12.jpg?height=517&width=673&top_left_y=1102&top_left_x=1062)

(b) Consecutive token IDs.

Figure 9: Visualization of the routing decision at different Position IDs. $E_{i}$ denotes the ratio of tokens routed to the $i_{\text {th }}$ expert.

visualize the routing decisions on different positions in Figure $9 \mathrm{a}$ and Figure $9 \mathrm{~b}$. We can observe:(1) there are indeed some specializations in different Position IDs; (2) consecutive positions prefer similar experts, such as the $E_{10}$ and $E_{19}$ in Figure 9 .

Does MoE specialize in Token ID? Since we are using the umT5 tokenizer, tokens from different languages usually have different token IDs. Therefore, we further study whether the router in MoE mainly makes its decisions based on the Token ID. We visualize the routing decisions of a few representative tokens in Figure 10. All these tokens show a very strong specialization on only a few experts. This is a very interesting finding because the tokens with the same Token ID have very diverse contexts in different sentences. For instance, the token "ed" can be the suffix of many different words, e.g., "preferred", and "led". The token "an" can also be part of "an apple" or "another". However, all these tokens have very strong specialization on only a few fixed experts. That means, MoE simply routes based on the Token ID instead of high-level semantics. We name this observation as Context-independent Specialization in the following sections. To verify that the Context-independent Specialization also exists for other Token IDs, we plot the routing decision standard deviation in Appendix $E$

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-13.jpg?height=568&width=624&top_left_y=234&top_left_x=748)

Figure 10: Visualization of the routing decision at different Token IDs. $E_{i}$ denotes the ratio of tokens routed to the $i_{\text {th }}$ expert.

Table 9: Top Tokens selected by each expert.

| Expert ID | Top Tokens |
| :---: | :---: |
| 0 | $\ln , `,,, s,-, \$, y, \_, 2$ |
| 1 | $\ln , 1, \ldots, 2, \mathbb{S}, \mathrm{S}, \ldots,-, \mathrm{C},\{$ |
| 21 | ,, and $,, ., \backslash \mathrm{n},=, \backslash \mathrm{t}$, the,, $\mathrm{n}$ |
| 30 | \}, ed, d, have, ing, ,, has, s, ", had |
| 31 | to , can, $\mathrm{s}$, of, ing, will, not, e , ed, would |

### 4.2 Token Specialization Study

Are experts clustering similar tokens? As we discussed above, the tokens with the same Token ID are always routed to the same expert no matter what the context is, i.e., Context-independent Specialization. We thus investigate whether the experts prefer the Token IDs corresponding to the tokens with similar low-level semantics. We list the top 10 favorite tokens for each expert in Table 9 We can observe that similar tokens are clustered in experts. For instance, "can". "will", and "would" are all in expert 31. "have". "has", and "had" are all included in expert 30. This visualization can also explain many observations above. An example is that, in most figures above, we can find most coding and math data prefer expert 21 . Here it reveals the real reason. Expert 21 has a strong preference for "=","and", and "\n", which appear more frequently in math and code.

When did the model learn the specialization? According to the Context-independent Specialization observed above, the model is not learning how to route based on high-level semantics. Therefore, we raise another question, when did the model learn and fix the routing decision for the tokens? We compare the routing decisions of different OpenMoE intermediate checkpoints in Figure 11a and Figure 11b. We can see that the expert preferences are almost totally overlapped for different checkpoints, which means that the model has started to fix its routing at the very early stage of training. Even if we change the training data mixture (from $52.25 \%$ code to $20 \%$ code) and training objective (from UL2 to CasualLM), the routing decision is still fixed. We infer that the reason is that, when the token is usually assigned to one specific expert, the loss would increase a lot if the token is sent to another unseen expert, which pushes the model to assign the token back to the original expert. Therefore, the routing probably has been learned at the warmup stage or so, and kept throughout the whole following training stage.

### 4.3 Token Drop During Routing

Drop-towards-the-End In MoE models, we usually set a pre-defined max capacity $C$ for every expert to ensure a balanced workload, which means each expert cannot process more than $C$ tokens.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-14.jpg?height=461&width=678&top_left_y=255&top_left_x=363)

(a) Token "ed" routing decision of different intermediate checkpoints.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-14.jpg?height=458&width=675&top_left_y=259&top_left_x=1061)

(b) Token "an" routing decision of different intermediate checkpoints.

Figure 11: Visualization of token IDs' routing decision of different intermediate checkpoints. $E_{i}$ denotes the ratio of tokens routed to the $i_{\text {th }}$

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-14.jpg?height=610&width=1358&top_left_y=977&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-14.jpg?height=504&width=658&top_left_y=995&top_left_x=365)

(a) Different datasets.

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-14.jpg?height=504&width=672&top_left_y=995&top_left_x=1038)

(b) Before and after supervised fine-tuning.

Figure 12: Comparing the ratio of tokens dropped at different position IDs.

This can ensure the throughput when training and deploying the MoE model with expert parallelism, i.e., distributing different experts to different GPUs. However, this will also introduce an issue, the later tokens would be dropped if the previous tokens have filled the expert. In decoder-only MoE architecture, due to the auto-regressive nature, the later tokens in a sequence may be dropped more. For instance, if one expert prefers "\n" token, and a sequence starts with many "\n"s and also has a lot of "\n's in the following output generated, the expert would be filled with "In" tokens quickly and all other tokens appeared later, which should be assigned to this expert, would be dropped. To verify this, we visualize the ratio of tokens dropped at different position IDs. As shown in Figure 12a, the general pre-training datasets, e.g., RedPajama and TheStack achieved balanced token assignment, only having a small proportion of tokens dropped, even for the Position ID after 1500. However, for multi-lingual and instruction-following datasets, a large ratio of tokens is dropped. We suggest the reason is, as we discussed above, the routing decision is fixed at the early stage of training and does not change anymore, so the load balance is also achieved based on the pre-training dataset. The instruction following data can be seen as a type of out-of-domain (OOD) data of the $\mathrm{MoE}$ router, which would induce an unbalanced token assignment so that many tokens appearing later would be dropped.

Can supervised fine-tuning with instruction-following data alleviate this Drop-towards-the-End issue? Since the Drop-towards-the-End issue is mainly caused by the OOD data, it is natural to think and study whether it is possible to convert the instruction-following data to in-domain data by tuning MoE with the instruction dataset. Therefore, we compare the models before and after

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-15.jpg?height=570&width=658&top_left_y=257&top_left_x=365)

(a) Mixtral- $8 \times 7 \mathrm{~B}$

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-15.jpg?height=570&width=656&top_left_y=257&top_left_x=1038)

(b) Deepseek-MoE-16B

Figure 13: Visualization of the routing decision at different Token IDs.

supervised fine-tuning in Figure 12b We can see the models do not have a significant difference in the Drop-towards-the-End issue. This matches well with our insight above, i.e., the routing behavior learned and fixed at the very early stage of LLM pre-training.

### 4.4 Study Other MoE Models

Background In this section, we investigate whether the issues we found above exist in other MoEbased LLMs, i.e., Mixtral and DeepSeek-MoE. Both Mixtral and Deepseek-MoE are trained by dropless token routing, which means that these models would not drop the token even if the workload of different is unbalanced. This design is fine if our model is not that large after applying some implementation tricks like Megablock [17], which can handle the imbalanced workload better if the experts are on the same GPU. However, implementation tricks like Megablock cannot work efficiently when there is only one expert on the single GPU, and unfortunately, this happens for very large MoE LLM (e.g. one GPT-style MoE with over 2 T parameters). Considering that the GPU memory size is not growing as fast as before, having a balanced workload for each expert is still extremely important for efficient large MoE model training.

Context-Independent Specialization We visualize the token ID specialization of Mixtral and Deepseek-MoE in Figure 13a and 13b We found, similar to OpenMoE, Deepseek-MoE has a clear Context-Independent Specialization, but Mixtral doesn't have that. We suggest that the reason is, according to this blod ${ }^{5}$ Mixtral is probably finetuned based on the Mistral-7B dense checkpoint, i.e. MoE upcycling [26], instead of training from scratch like deepseek-MoE and OpenMoE. Since the experts in Mixtral are very similar, it makes sense that there is a relatively weak specialization in their MoE model, and at the same time, since the model has learned high-level semantics when converting dense LLM to MoE LLM, it is less likely to develop Context-Independent Specialization. Therefore, we suggest that Context-Independent Specialization is an issue only for training MoE from scratch. However, as we discussed in our paper, MoE is more efficient during training than inference, it is still highly desirable to study how to avoid Context-Independent Specialization when training MoE from scratch or converting dense LLM to MoE at the early stage of training. One feasible solution can be that, first train a dense half-cooked LLM (maybe using $20 \%$ pretraining tokens or so), and then convert the dense LLM to MoE via MoE upcycling. We can then train the MoE with $80 \%$ of the training tokens left to ensure a better cost-effectiveness trade-off.

Early Routing Learning Since Mixtral and Deepseek-MoE have no open-sourced intermediate checkpoints, we cannot study this issue on these models.

Drop-towards-the-End As mentioned before, Mixtral and Deepseek-MoE have no token drop mechanism. However, this is not friendly to expert parallelism, especially for very large MoE-LLM[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-16.jpg?height=601&width=1353&top_left_y=239&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-16.jpg?height=496&width=658&top_left_y=256&top_left_x=365)

(a) Mixtral- $8 \times 7 \mathrm{~B}$

![](https://cdn.mathpix.com/cropped/2024_06_04_310ffc46f2c7ae32af64g-16.jpg?height=499&width=667&top_left_y=255&top_left_x=1035)

(b) DeepSeek-MoE-16B

Figure 14: Comparing the ratio of tokens dropped at different position IDs.

with trillion-level parameters, although it is okay if the model is relatively small $(<100 \mathrm{~B})$. Therefore, we still study whether there is a Drop-towards-the-End issue in Mixtral and Deepseek-MoE by manually adding a token drop mechanism when there are too many tokens routed to an expert. As shown in Figure 14a and Figure 14b, there is a clear token drop at the later tokens in the input sequences, which means the Drop-towards-the-End is an issue for all these MoE LLMs.

## 5 Rethinking OpenMoE

Working on this project is a long journey for authors. We indeed made some mistakes during design and development, but we also achieved some new insights in the analysis. We thus write down everything we found without any reservation in this paper to help future practitioners. Then, in this section, we discuss how to train a better model in the future, which are the most important takeaways of our work.

How much code shall we use? To be honest, we do not have a very precise answer. Conducting an ablation study is extremely expensive because of the cost of pre-training LLM at scale. The conclusion may also strongly depend on the model size and data quality. However, according to our observation, over $50 \%$ code looks too aggressive which may harm the abilities on text tasks, but considering the importance of writing code, we suggest using around $30 \%$ code as we used in OpenMoE-34B/32E.

Tokenizer Selection Our large tokenizer vocabulary introduces computation overhead at the last output layer after Transformer blocks. Although this overhead would become relatively small after scaling the Transformer model up, it is still valuable to make the tokenizer selection smarter. We conduct a quantitative analysis of the tokenizer with the datasets we used in Section 4 . As shown in Table 10, umT5 tokenizer is indeed much better than LLaMA tokenizer on the multi-lingual dataset, especially on the low-resource language. It is also slightly better than LLaMA on the instruction-following data. However, it did not match well with our expectation that it could save more tokens for the code data. In addition, we observe that the token usage in both tokenizers is extremely long-tail distributed, which indicates that there is a large room to improve the tokenizer and following algorithms. As we know, learning from long-tailed data is hard [57]. Since we only have a little multi-lingual data in our pre-training data mixture, the computation cost of predicting the logits of those low-resource tokens is wasted. Based on our sub-optimal choice, we also need a solid tokenizer benchmark, which would help people evaluate tokenizers systematically. And we can then pick the best tokenizer before training the model.

More Efficient MoE Architecture According to our observation, MoE routing is almost contextindependent (i.e., Context-independent Specialization), we suggest that we can (1) remove the trainable router after warmup stage; (2) adopt parallel Transformer layer [9, 50] computing FFN layer based on the input directly instead of using the output of attention layer; (3) overlapping the attention

Table 10: Compare umT5 tokenizer and LLaMA tokenizer on the subsets extracted from different datasets. Vocab used denotes the number of token IDs activated when tokenizing the whole subset. The umT5/LLaMA means, when tokenizing the same subset, the ratio of the number of tokens generated by umT5 and LLaMA.

| Dataset | Subset | LLaMA Tokenizer |  | umT5 Tokenizer |  | umT5/LLaMA |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | \#Tokens | Vocab Used | \#Tokens | Vocab Used |  |
| RedPajama | arxiv | 125,339 | 8,327 | 131,059 | 8,762 | 1.046 |
|  | book | 137,972 | 11,603 | 131,072 | 15,202 | 0.950 |
|  |  | 28,592 | 5,439 | 26,428 | 5,554 | 0.924 |
|  |  | 78,450 | 8,738 | 73,403 | 9,927 | 0.936 |
|  | github | 54,707 | 4,769 | 59,732 | 4,539 | 1.092 |
|  | stackexchange | 40,659 | 4,714 | 43,195 | 4,317 | 1.062 |
|  | wikipedia | 37,406 | 7,179 | 30,555 | 8,748 | 0.817 |
| TheStack | assembly | 49,143 | 3,066 | 50,738 | 3,130 | 1.032 |
|  | blitzmax | 78,259 | 4,200 | 80,658 | 4,209 | 1.031 |
|  | java | 64,236 | 4,229 | 69,902 | 3,905 | 1.088 |
|  | python | 66,243 | 5,095 | 70,795 | 4,799 | 1.069 |
| MTBench | writing | 6,062 | 1,700 | 5,786 | 1,535 | 0.954 |
|  | roleplay | 4,309 | 1,291 | 4,076 | 1,172 | 0.946 |
|  | reasoning | 2,369 | 478 | 2,309 | 429 | 0.975 |
|  | math | 5,163 | 290 | 5,154 | 282 | 0.998 |
|  | coding | 4,955 | 651 | 5,256 | 631 | 1.061 |
|  | extraction | 7,058 | 1,376 | 6,817 | 1,234 | 0.966 |
|  | stem | 4,783 | 1,151 | 4,527 | 1,039 | 0.946 |
|  | humanities | 6,398 | 1,451 | 5,946 | 1,320 | 0.929 |
| Multi-lingual <br> TED | $\mathrm{ar}$ | 256,952 | 187 | 88,406 | 8,037 | 0.344 |
|  | de | 103,270 | 4,880 | 80,593 | 8,470 | 0.780 |
|  | es | 101,212 | 4,745 | 78,713 | 8,519 | 0.778 |
|  | $\mathrm{fr}$ | 115,057 | 5,156 | 95,978 | 8,164 | 0.834 |
|  | he | 242,446 | 239 | 86,891 | 4,074 | 0.358 |
|  | it | 109,591 | 4,593 | 84,201 | 8,833 | 0.768 |
|  | ja | 144,825 | 931 | 63,491 | 6,860 | 0.438 |
|  | ko | 257,107 | 596 | 106,770 | 2,736 | 0.415 |
|  | $\mathrm{nl}$ | 102,703 | 4,234 | 75,084 | 7,540 | 0.731 |
|  |  | 107,144 | 2,502 | 74,445 | 9,658 | 0.695 |
|  | zh-cn | 149,581 | 1,058 | 88,107 | 3,611 | 0.589 |
|  | zh-tw | 173,415 | 1,107 | 93,693 | 3,619 | 0.540 |

layer computation and MoE layer all-to-all communication. (1) and (3) will improve the hardware utilization and (2) can enable (3) without performance drop when scaling up [9].

Mix instruction-following data during pre-training warm-up to control load balance and alleviate Drop-towards-the-End. According to our results on multi-turn MT-Bench, it is very important to alleviate the Drop-towards-the-End issue. To this end, the key is to make the MoE achieve load balance on instruction-following data. Again, since the $\mathrm{MoE}$ learns and fixes the routing behavior at the early stage of pre-training, a straightforward solution is mixing the instruction-tuning data into the pre-training corpus during warm-up. This data mixing is not to align the model to learn how to follow instructions. Instead, we hope the model achieves the balanced token routing on instruction-tuning data, which paves the way to our final usage case of LLMs.

## 6 Conclusion

In this work, we explore how to train MoE for open-sourced communities. We achieved positive results that verified the effectiveness of MoE-based LLM in the post-ChatGPT stage. We disclosed all details, and our model is fully reproducible with the open-sourced code and data. More importantly, we conducted an in-depth analysis on our MoE-based LLM and found important "Context-independent Specialization" "Early Routing Learning" and "Drop-towards-the-End". We also rethink the mistakes we made and propose possible solutions for future developers. We sincerely
hope this work can help the open-source community have a better understanding of MoE models. All the best!

## References

[1] R. Anil et al., "Palm 2 technical report," arXiv preprint arXiv:2305.10403, 2023.

[2] Anonymous, "(inthe)wildchat: 570k chatGPT interaction logs in the wild," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https: //openreview.net/forum?id=Bl8u7ZRIbM

[3] M. Artetxe et al., "Efficient large scale language modeling with mixtures of experts," arXiv preprint arXiv:2112.10684, 2021.

[4] B.-b. authors, "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," Transactions on Machine Learning Research, 2023, ISSN: 2835-8856. [Online]. Available: https://openreview.net/forum?id=uyTL5Bvosj

[5] M. Bavarian et al., "Efficient training of language models to fill in the middle," arXiv preprint arXiv:2207.14255, 2022.

[6] O. r. Bojar et al., "Findings of the 2016 conference on machine translation," in Proceedings of the First Conference on Machine Translation, Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 131-198. [Online]. Available: http : / www . aclweb . org/ anthology/W/W16/W16-2301

[7] T. B. Brown et al., "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.

[8] M. Chen et al., "Evaluating large language models trained on code," 2021. arXiv: 2107.03374 [cs.LG].

[9] A. Chowdhery et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.

[10] H. W. Chung et al., "Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining," arXiv preprint arXiv:2304.09151, 2023.

[11] T. Computer, Redpajama: An open source recipe to reproduce llama training dataset, 2023. [Online]. Available: https://github.com/togethercomputer/RedPajama-Data.

[12] D. Dai et al., "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models," arXiv preprint arXiv:2401.06066, 2024.

[13] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.

[14] N. Du et al., "Glam: Efficient scaling of language models with mixture-of-experts," in International Conference on Machine Learning, PMLR, 2022, pp. 5547-5569.

[15] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," J. Mach. Learn. Res, vol. 23, pp. 1-40, 2021.

[16] H. Fu Yao; Peng and T. Khot, "How does gpt obtain its ability? tracing emergent abilities of language models to their sources," Yao Fu's Notion, Dec. 2022. [Online]. Available: https : / / yaofu . notion . site / How - does - GPT - Obtain - its - Ability Tracing - Emergent - Abilities - of - Language - Models - to - their - Sources b9a57ac0fcf74f30a1ab9e3e36fa1dc1.

[17] T. Gale, D. Narayanan, C. Young, and M. Zaharia, "Megablocks: Efficient sparse training with mixture-of-experts," Proceedings of Machine Learning and Systems, vol. 5, 2023.

[18] L. Gao et al., A framework for few-shot language model evaluation, version v0.4.0, Dec. 2023. DOI: $10.5281 / z e n o d o .10256836$. [Online]. Available: https://zenodo.org/records/ 10256836

[19] X. Geng and H. Liu, Openllama: An open reproduction of llama, May 2023. [Online]. Available: https://github.com/openlm-research/open_llama.

[20] D. Hendrycks et al., "Measuring massive multitask language understanding," arXiv preprint arXiv:2009.03300, 2020.

[21] J. Hoffmann et al., "Training compute-optimal large language models," arXiv preprint arXiv:2203.15556, 2022.

[22] A. Q. Jiang et al., "Mixtral of experts," arXiv preprint arXiv:2401.04088, 2024.

[23] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension," in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y. Kan, Eds., Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 16011611. DOI: 10.18653/v1/P17-1147. [Online]. Available: https://aclanthology.org/ P17-1147.

[24] J. D. M.-W. C. Kenton and L. K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in Proceedings of naacL-HLT, vol. 1, 2019, p. 2.

[25] D. Kocetkov et al., "The stack: 3 tb of permissively licensed source code," Preprint, 2022.

[26] A. Komatsuzaki et al., "Sparse upcycling: Training mixture-of-experts from dense checkpoints," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=T5nUQDrM4u.

[27] D. Lepikhin et al., "Gshard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16668, 2020.

[28] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, "Base layers: Simplifying training of large, sparse models," in International Conference on Machine Learning, PMLR, 2021, pp. 6265-6274.

[29] J. Li, Z. Zhang, and H. Zhao, "Self-prompting large language models for open-domain qa," arXiv preprint arXiv:2212.08635, 2022.

[30] R. Li et al., "Starcoder: May the source be with you!" arXiv preprint arXiv:2305.06161, 2023.

[31] Y. Liu et al., "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.

[32] Y. Lou, F. Xue, Z. Zheng, and Y. You, "Cross-token modeling with conditional computation," arXiv preprint arXiv:2109.02008, 2021.

[33] B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby, "Multimodal contrastive learning with limoe: The language-image mixture of experts," Advances in Neural Information Processing Systems, vol. 35, pp. 9564-9576, 2022.

[34] E. Nijkamp et al., "Xgen-7b technical report," arXiv preprint arXiv:2309.03450, 2023.

[35] J. Puigcerver, C. Riquelme, B. Mustafa, and N. Houlsby, "From sparse to soft mixtures of experts," arXiv preprint arXiv:2308.00951, 2023.

[36] J. W. Rae et al., "Scaling language models: Methods, analysis \& insights from training gopher," arXiv preprint arXiv:2112.11446, 2021.

[37] C. Raffel et al., "Exploring the limits of transfer learning with a unified text-to-text transformer," Journal of Machine Learning Research, vol. 21, no. 140, pp. 1-67, 2020. [Online]. Available: http://jmlr.org/papers/v21/20-074.html.

[38] C. Riquelme et al., "Scaling vision with sparse mixture of experts," Advances in Neural Information Processing Systems, vol. 34, pp. 8583-8595, 2021.

[39] S. Roller, S. Sukhbaatar, J. Weston, et al., "Hash layers for large sparse models," Advances in Neural Information Processing Systems, vol. 34, pp. 17 555-17 566, 2021.

[40] B. Roziere et al., "Code llama: Open foundation models for code," arXiv preprint arXiv:2308.12950, 2023.

[41] N. Shazeer, "Glu variants improve transformer," arXiv preprint arXiv:2002.05202, 2020.

[42] N. Shazeer et al., "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017.

[43] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv preprint arXiv:1909.08053, 2019.

[44] L. Soldaini et al., "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research," arXiv preprint, 2023.

[45] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, "Roformer: Enhanced transformer with rotary position embedding," Neurocomputing, vol. 568, p. $127063,2024$.

[46] Y. Tay et al., "U12: Unifying language learning paradigms," in The Eleventh International Conference on Learning Representations, 2022.

[47] Y. Tay et al., "Unifying language learning paradigms," arXiv preprint arXiv:2205.05131, 2022.

[48] L.-M. Team, Llama-moe: Building mixture-of-experts from llama with continual pre-training, Dec. 2023. [Online]. Available: https://github.com/pjlab-sys4nlp/llama-moe.

[49] H. Touvron et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.

[50] B. Wang and A. Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, https://github.com/kingoflolz/mesh-transformer-jax. May 2021.

[51] G. Wenzek et al., "CCNet: Extracting high quality monolingual datasets from web crawl data," English, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari et al., Eds., Marseille, France: European Language Resources Association, May 2020, pp. 4003-4012, ISBN: 979-10-95546-34-4. [Online]. Available: https : //aclanthology. org/2020.1rec-1.494.

[52] Y. Xu et al., "Gspmd: General and scalable parallelization for ml computation graphs," arXiv preprint arXiv:2105.04663, 2021.

[53] F. Xue, X. He, X. Ren, Y. Lou, and Y. You, "One student knows all experts know: From sparse to dense," arXiv preprint arXiv:2201.10890, 2022.

[54] F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, "Go wider instead of deeper," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8779-8787.

[55] P. Yu et al., "Efficient language modeling with sparse all-mlp," arXiv preprint arXiv:2203.06850, 2022.

[56] P. Zhang, G. Zeng, T. Wang, and W. Lu, Tinyllama: An open-source small language model, 2024. arXiv: 2401.02385 [CS.CL].

[57] Y. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, "Deep long-tailed learning: A survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.

[58] L. Zheng et al., "Judging llm-as-a-judge with mt-bench and chatbot arena," arXiv preprint arXiv:2306.05685, 2023.

[59] Y. Zhou et al., "Brainformers: Trading simplicity for efficiency," in International Conference on Machine Learning, PMLR, 2023, pp. 42 531-42 542.

[60] Y. Zhou et al., "Mixture-of-experts with expert choice routing," Advances in Neural Information Processing Systems, vol. 35, pp. 7103-7114, 2022.

[61] B. Zoph et al., "St-moe: Designing stable and transferable sparse expert models," URL https://arxiv. org/abs/2202.08906, 2022.
