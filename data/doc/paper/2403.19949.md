# FairCLIP: Harnessing Fairness in Vision-Language Learning 

Yan Luo ${ }^{1 *}$ Min Shi ${ }^{1 *}$ Muhammad Osama Khan ${ }^{2 *}$ Muhammad Muneeb Afzal ${ }^{2}$ Hao Huang ${ }^{3}$<br>Shuaihang Yuan ${ }^{3} \quad$ Yu Tian $^{1} \quad$ Luo Song $^{1} \quad$ Ava Kouhana ${ }^{1} \quad$ Tobias Elze $^{1} \quad$ Yi Fang $^{2,3 \dagger}$ Mengyu Wang ${ }^{1 \dagger}$<br>${ }^{1}$ Harvard Ophthalmology AI Lab, Harvard University<br>${ }^{2}$ Tandon School of Engineering, New York University<br>${ }^{3}$ Multimedia and Visual Computing Lab, New York University Abu Dhabi<br>\{yluo16, mshi6\}@meei.harvard.edu, \{osama.khan, muneeb.afzal, hh1811, sy2366\}@nyu.edu,<br>\{ytian11, lsong7, akouhana, tobias_elze\}@meei.harvard.edu,<br>yfang@nyu.edu, mengyu_wang@meei.harvard.edu


#### Abstract

Fairness is a critical concern in deep learning, especially in healthcare, where these models influence diagnoses and treatment decisions. Although fairness has been investigated in the vision-only domain, the fairness of medical vision-language (VL) models remains unexplored due to the scarcity of medical VL datasets for studying fairness. To bridge this research gap, we introduce the first fair visionlanguage medical dataset (Harvard-FairVLMed) that provides detailed demographic attributes, ground-truth labels, and clinical notes to facilitate an in-depth examination of fairness within VL foundation models. Using HarvardFairVLMed, we conduct a comprehensive fairness analysis of two widely-used VL models (CLIP and BLIP2), pretrained on both natural and medical domains, across four different protected attributes. Our results highlight significant biases in all VL models, with Asian, Male, NonHispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively. In order to alleviate these biases, we propose FairCLIP, an optimal-transport-based approach that achieves a favorable trade-off between performance and fairness by reducing the Sinkhorn distance between the overall sample distribution and the distributions corresponding to each demographic group. As the first VL dataset of its kind, Harvard-FairVLMed holds the potential to catalyze advancements in the development of machine learning models that are both ethically aware and clinically effective. Our dataset and code are available at https://ophai.hms.harvard.edu/datasets/ harvard-fairvlmedlok.


[^0]Table 1. Public medical fairness datasets. In contrast to existing medical VL datasets, our Harvard-FairVLMed provides detailed demographic attributes, ground-truth labels, and clinical notes with imaging as well as non-imaging clinical information.

| Dataset | Data Modality | \# of Images | \# of Patients | Identity Attribute |  | $\mathbf{L}$ | L Info Type |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\overline{\text { Fitzpatrick 17k [16] }}$ | Skin Photos | $\overline{16,012} \quad$ | 1,373 | Skin type |  | $\bar{x}$ |  |
| HAM10000 [77] | Dermatoscopy | $9,948 \quad 2 \quad-1$ |  | Age; Gender |  |  |  |
| OL31[92] | Heart CT | 8,139 | 1,686 | Age; Gender |  |  |  |
| ODIR-2019 [1] | Color Fundus | 8,000 | 5,000 | Agc; Gender | $\checkmark$ | $x$ |  |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-01.jpg?height=33&width=127&top_left_y=1251&top_left_x=1075) | Color Fundus <br> OCT | 488 <br> 1,000 | 244 <br> 1,000 | Age; Gender <br> Agc; Gender; Racc; | 1 <br> $d$ | $x$ |  |
| COVID-CTMD[3] | Lung CT | 308 | 305 | Ethnicity <br> Agc; Gender | 1 | $x$ |  |
| AMD-OCTI14 |  | 384 |  |  | $\checkmark$ |  |  |
| ADNI 1.5T [85] | Brain MRI | 550 |  | Age; Gender | $\checkmark$ | $x$ |  |
| $\overline{\text { CheXpert [24] }}$ | Chest X-ray and Radiology Report | 222,793 | $65,240 \quad$ | Age; Gender; Race | $\checkmark$ | $\bar{V}$ |  |
| MIMIC-CXR [27] | Chest X-ray and Radiology Report | 370,955 | 65,079 | Age; Gender; Race | $\checkmark$ | $v$ | Imag |
| PadChest [12] | Chest X-ray and Radiology Report | 160,868 | 69,882 | Age; Gender | $\checkmark$ | $\checkmark$ | Description <br> Image |
| Harvard- <br> FairVLMed | (Spanish) <br> SLO Fundus and Clinical Note | Fundus: 0,$00 ;$ <br> Note: 10,000 | 10,000 | Age; Gender, Race; <br> Etthiciti; PPerefred <br> Language; Marital <br> S | $\checkmark$ | $\checkmark$ | Description <br> Image <br> Description + <br> Non-Imaging <br> Clinical Info |

## 1. Introduction

Fairness has received increasing interest in deep learning in recent years. This is vital, especially in healthcare, where these deep learning models influence diagnoses and treatment decisions. Biases in these models related to factors like race, gender, or socioeconomic status can lead to healthcare disparities and adverse patient outcomes. Hence, ensuring that these models are free from bias is not only an ethical and legal requirement but also a necessity for ensuring patient safety and healthcare equity. This makes fairness in medical computer vision a critical and immediate concern, essential for providing equitable healthcare.

Previous research has identified biases in deep learningbased medical imaging models, focusing primarily on Chest X-ray diagnosis [15, 31]. In contrast to these vision-only models, there has been a recent surge of vision-language (VL) foundation models [21, 29, 30, 46, 80, 86, 87], which have set new benchmarks across a wide spectrum of tasks. However, despite these strong performances, the fairness of these VL models remains unclear. Given the existence of biases in vision-only models (which operate on machineacquired images) and the human-written nature of clinical
notes, VL models could further aggravate fairness issues. Hence, as the field moves towards multi-modal foundation models, it becomes increasingly critical to scrutinize how the interplay of vision and text affects equity in algorithmic outcomes. The current landscape for such an investigation is, however, constrained by the scarcity of VL datasets that include comprehensive demographic information, with the existing public VL datasets primarily focused on Chest Xrays [24, 26]. Prior studies [24, 67] have highlighted the challenges of studying fairness using these datasets since their ground-truth labels are automatically extracted from radiology reports, potentially leading to inaccurate fairness conclusions due to the noisy labels. Moreover, since these datasets were not primarily designed for fairness, they provide only a handful of demographic characteristics, limiting the potential for a comprehensive fairness study across multiple dimensions. Furthermore, radiology reports, focusing mainly on direct observations of the imaging data with limited additional patient-specific information, are not representative of most clinical text, thus limiting their utility in fairness studies of medical VL models.

To bridge this research gap, we introduce the first fair vision-language medical dataset (Harvard-FairVLMed for short) that provides detailed demographic attributes, ground-truth labels, and clinical notes to facilitate an indepth examination of fairness within VL foundation models (Table 1). Harvard-FairVLMed contains records for 10,000 patients, each paired with an SLO fundus image and a clinical note for diagnosing Glaucoma, along with fine-grained protected attributes such as age, gender, race, ethnicity, preferred language, and marital status. Unlike radiology reports, the clinical notes in our dataset provide much more detailed information featuring not only image descriptions but also rich non-imaging clinical information such as medication, non-imaging test results, and family history. Hence, these clinical notes are more representative of clinical textual information, making them better suited for studying the fairness of medical VL models.

Glaucoma, affecting millions globally [17, 48, 49, 69, $71,72,74,76]$, exemplifies the need for equitable diagnostic models. Timely detection is crucial in order to avoid irreversible vision loss. However, many patients remain undiagnosed [68] due to the disease's asymptomatic nature and barriers to ophthalmic care. Moreover, this undiagnosed issue is even more pronounced among minority groups. For example, previous studies have shown that individuals from Black communities are 4.4 times more likely to have undiagnosed and untreated Glaucoma compared to their White counterparts [68], highlighting the importance of addressing healthcare disparities. Deep learning systems hold significant potential for improving healthcare. However, it is imperative to address potential fairness issues in these deep learning systems prior to their clinical implementation to ensure equitable healthcare delivery.

In this work, we conduct an extensive fairness analysis with two widely-used VL methods (i.e., CLIP [63] and BLIP2 [40]) on Harvard-FairVLMed. Our experimental findings reveal significant disparities across various groups based on race, gender, ethnicity, and language. To address these fairness issues, we introduce FairCLIP, an optimal transport-based approach. FairCLIP is designed to enhance fairness by optimizing the Sinkhorn distance, thereby aligning the overall sample distribution with the distributions specific to each demographic group.

Our main contributions can be summarized as follows:

- We introduce the first fair vision-language medical dataset (Harvard-FairVLMed) featuring detailed demographic attributes, ground-truth labels, and clinical notes for studying the fairness of VL foundation models.
- Using Harvard-FairVLMed, we conduct a comprehensive fairness analysis of two widely-used VL models (i.e., CLIP and BLIP2), pre-trained on both natural and medical domains, across four different protected attributes.
- Our results highlight significant biases in all VL models, with Asian, Male, Non-Hispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively.
- We propose an optimal transport-based approach named FairCLIP, which demonstrates a significant improvement over CLIP in terms of both performance and fairness.


## 2. Related Work

Fairness Models Fairness in machine learning is crucial for creating dependable systems that avoid discrimination. Historical instances of bias have been identified in various technologies. In the field of image processing, biases exist in facial recognition $[7,11,18,78,79]$ and pedestrian detection algorithms [9, 33, 38]. Recent efforts in deep learning aim to rectify these biases by adjusting training processes to minimize unfair predictions across different demographic groups. These deep learning methods focus on removing or neutralizing sensitive features and fostering representations that are fair and unbiased.

Particularly in the medical domain, fairness models have emerged as a critical tool to mitigate biases and promote equity, particularly within healthcare applications. This field has developed a range of methodologies, each targeting a different stage: pre-processing, in-processing, and postprocessing. Pre-processing strategies $[10,28,57,61,64$, $64,67,76,95,95,99,100]$ are designed to mitigate biases by enhancing the dataset prior to model training. In inprocessing approaches $[8,8,65,65,66,66,70,91,93,93]$ fairness is incorporated directly into the model's training stage. Post-processing methods [51, 60, 84], applied after the model has been trained, aim to adjust outputs to improve fairness.

To address the challenges of fairness in VL tasks, this work introduces FairCLIP, a method designed to promote fairness. FairCLIP aligns the distribution of all samples with those of specific demographic groups, leveraging the similarities between visual and textual features.

Vision-Language Models The integration of vision and language in machine learning, especially through models like CLIP (Contrastive Language-Image Pretraining) [63], represents a significant advancement in computer vision. CLIP, trained from a large scale of images and their related text descriptions, marks an important step in aligning visual information with text. BLIP (Bootstrapping LanguageImage Pre-training) [39] represents a novel approach in vision-language models, utilizing a visual transformer as an image encoder and a transformer for text processing. BLIP2 [40] advances the field with its scalable multimodal pretraining approach, enabling LLMs to process and comprehend images.

The success of CLIP-like models has inspired numerous adaptations in the medical field [37, 44, 80, 81, 89, 94, 98], where it confronts challenges like limited data and the need for accurate interpretations. For instance, Pathology Language-Image Pretraining (PLIP) [21] utilizes the OpenPath dataset [21], consisting of pathology images with language descriptions derived from medical Twitter posts. PMCCLIP (PubMedCentral-CLIP) [41] joins these efforts by enhancing CLIP's application in the biomedical domain.

Beyond these adaptations of CLIP, several other visionlanguage models have emerged to address various medical tasks that range from generating reports from medical images $[43,54,55,75,83,88]$ to diagnosing medical conditions [52, 53, 96, 97], and facilitating medical image-based question-answering (VQA) [6, 34].

Despite these advancements, a gap remains in understanding and addressing fairness concerns within visionlanguage models (VLMs). The complex interplay of visual and textual data in VLMs poses unique challenges for fairness, which have been largely unexplored, especially in the context of medical applications.

Vision-Language Medical Fairness Datasets Medical fairness datasets serve a critical role in the development of machine learning models [ $23,27,47,49,50]$, ensuring that they operate equitably across diverse patient populations. In this context, datasets equipped with imaging data as well as associated textual reports are particularly valuable. They not only enable the development of models but also allow for nuanced explorations of fairness by providing a richer context for each medical case.

CheXpert [23] is a prominent chest radiography dataset annotated for 14 distinct observations. CheXpert uses an automated system to extract labels from radiology reports. However, this method introduces uncertainties due to the inherent ambiguities in the reports and varying interpreta-

![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-03.jpg?height=325&width=843&top_left_y=239&top_left_x=1064)

Figure 1. Examples of non-glaucomatous and glaucomatous samples with the corresponding SLO fundus image and clinical note.

tions by radiologists. This uncertainty, such as the $16 \%$ of Atelectasis labels that are uncertain, represents a significant challenge in using datasets like this in real-world scenarios [2, 23]. MIMIC-CXR [27] contributes to this area in which each study is complemented by a semi-structured free-text radiology report. Padchest [12] further expands the resources by providing chest X-ray images along with a set of radiographic findings that have been processed using radiology reports. However, the radiological reports are only available in Spanish, which may present a barrier for the global research community. Similarly, datasets pertaining to image captioning [13, 25, 58, 73] and VQA [20, 36, 42] have been developed in the medical domain.

In contrast, fundus image datasets present a different scenario. While ODIR2019 [1] and PAPILA [35] offer valuable imaging data for ocular conditions, they do not provide the same depth of textual information as CheXpert and MIMIC-CXR. Also, the Glaucoma Detection and Progression [47], Eye Fairness [49], and Glaucoma Fairness [50] datasets lack textual information required for studying the VL problem. While datasets such as CheXpert and MIMICCXR have advanced the field of fairness research by pairing images with radiology reports, they fall short in providing extensive demographic data and are prone to noisy labels due to their automated labeling processes. Additionally, they were not initially created with a focus on fairness, which poses limitations in studying fairness.

Our Harvard-FairVLMed dataset stands as a pioneering resource tailored for fairness studies in VL modeling. It comprises ground-truth labels, ensuring high label quality, and includes a broad range of identity attributes such as age, gender, race, ethnicity, preferred language, and marital status. Hence, this dataset not only enhances the quality and reliability of fairness studies but also broadens the horizon for research in the VL domain.

## 3. Dataset Analysis

This study strictly adheres to the principles outlined in the Declaration of Helsinki and has been approved by our institute's Institutional Review Board. All data in this dataset are de-identified.

### 3.1. Data Collection and Quality Control

Harvard-FairVLMed contains subjects who received glaucoma care from Massachusetts Eye and Ear at Harvard Medical School between 2015 and 2022. There are three types of data to be released in this study: (1) scanning laser ophthalmoscopy (SLO) fundus images; (2) demographic identity group information; and (3) de-identified clinical notes written by ophthalmologists to provide a summary of the glaucoma diagnosis. SLO fundus images are a valuable marker for assessing retinal damage from diseases like glaucoma. Each SLO fundus image is associated with six demographic identity attributes, including age, gender, race, ethnicity, preferred language, and marital status. The accompanying clinical notes vary in length. These notes may detail assessments, treatment plans, and diagnostic strategies, and are considered to correspond with the visual semantics in SLO fundus images. Two examples of pairs of SLO fundus images and clinical notes are shown in Figure 1. The subjects are categorized into non-glaucoma (visual function measured by visual field (VF) test is normal: VF mean deviation $\geq-1 \mathrm{~dB}$ and normal $\mathrm{VF}$ glaucoma hemifield test and pattern standard deviation (PSD) results) and glaucoma categories (visual function measured by VF test is abnormal: VF mean deviation $<-3 \mathrm{~dB}$ and abnormal VF glaucoma hemifield test and PSD results).

### 3.2. Protected Information De-Identification

The raw clinical notes may contain protected sensitive information, such as the date of glaucoma diagnosis, patient name, phone number, email address, physical location, institution, etc. We de-identified this sensitive information in the following three steps. First, we anonymized all the clinical notes using Microsoft's Presidio ${ }^{1}$, which replaced sensitive information with respective placeholders (e.g., PERSON_NAME, PHONE_NUMBER, and LOCATION) so that the original sentence structure and coherence can be maintained. Then, we used rules to match and de-identify the protected information (e.g., physical address) that has not been fully recognized by Presidio. Lastly, the de-identified clinical notes were further verified by four medical experts. Specially, every clinical note was checked by an expert and the sensitive information was manually replaced with respective placeholders when necessary.

### 3.3. Data Characteristics

Our Harvard-FairVLMed dataset comprises 10,000 samples from 10,000 subjects. It is divided into 7,000 training, 1,000 validation, and 2,000 test samples. The dataset's collective age average is $60.9 \pm 16.2$ years. The dataset includes samples from three major groups: Asian, with 819 samples; Black, with 1,491 samples; and White, with 7,690 samples.[^1]

Gender-wise, females constitute $56.3 \%$ of the subjects, with the remainder being males. The ethnic distribution is highlighted by $90.6 \%$ Non-Hispanic, $4.0 \%$ Hispanic, and $5.4 \%$ unspecified. In terms of preferred language, $92.5 \%$ of the subjects prefer English, $1.7 \%$ prefer Spanish, $0.8 \%$ prefer other languages, and $5.0 \%$ remain unknown. From a marital status perspective, $57.4 \%$ are in a marriage or partnered, $26.4 \%$ are single, $6.6 \%$ have experienced divorce, $1.0 \%$ are legally separated, $6.1 \%$ are widowed, and $2.5 \%$ are not specified. After de-identification, clinical notes vary from 11 to 332 words, with an average word count of 147 .

## 4. Methodology

### 4.1. Background

With the labeled data $D=\left\{\left(x_{I i}, x_{T i}, y_{i}, a_{i}\right)\right\}$, where $x_{I i} \in$ $\mathcal{X}_{I}$ represents an SLO fundus image, $x_{T i} \in \mathcal{X}_{T}$ represents a clinical note, $y \in \mathcal{Y}$ denotes a glaucoma diagnosis label, and $a \in \mathcal{A}$ signifies an identity attribute associated with the patient, such as gender, race, or ethnicity. To process multimodal data, a vision-language (VL) pre-trained model (e.g., CLIP [62]) $f$ employs a vision encoder $f_{I}$ and a text encoder $f_{T}$ to generate vision features $z_{I}$ and text features $z_{T}$, respectively. Given a batch of $N$ image and text pairs, CLIP is trained to generate a similarity matrix $M \in \mathbb{R}^{N \times N}$ with each entry $M_{i j}=z_{I i}^{\top} z_{T j}$ representing the cosine similarity between the $i$-th image feature $z_{I i}$ and the $j$-th text feature within the given batch. To achieve this, CLIP adopts a contrastive learning scheme to maximize the cosine similarity of the image and text features of the $N$ positive pairs in the batch, while minimizing the cosine similarity of the image and text features of the remaining $N^{2}-N$ negative pairs. Following [62], a symmetric cross-entropy loss is calculated over $M$ to optimize $f_{I}$ and $f_{T}$. Mathematically, the optimization goal is defined as follows:

$$
\min _{f}-\sum_{i=1}^{N} \sum_{j=1}^{N} \delta(i-j) \log \left(z_{I i}^{\top} z_{T j} /\left(\left\|z_{I i}\right\| \cdot\left\|z_{T j}\right\|\right)\right)
$$

where $\delta(i-j)$ is Dirac's Delta function, $\delta(i-j)=1$ when $i=j$. In the context of fairness learning, it requires to incorporate identity information during model training, i.e., $f \in \mathcal{F}: \in \mathcal{X}_{I} \times \mathcal{X}_{T} \times \mathcal{A} \xrightarrow{\theta} \mathcal{Y}$. Consequently, fairness learning aims to minimize disparities between different identity groups while also maximizing accuracy.

### 4.2. FairCLIP

As illustrated in Figure 2, the proposed FairCLIP framework is designed to improve fairness during the pre-training phase. This is achieved by minimizing the disparity between the probability distributions of $M_{i, i}$, which represents the correlation between visual and language features, across different racial groups (or other attribute-

![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-05.jpg?height=512&width=1410&top_left_y=237&top_left_x=325)

Figure 2. Schematic view of the proposed FairCLIP. Clinical notes containing PHI (e.g., names and gender) undergo de-identification and summarization to fit text encoder limitations, such as CLIP's 77-token maximum length. FairCLIP equalizes the overall sample distribution with the distributions corresponding to each demographic group, thereby achieving a favorable trade-off between performance and fairness.

based groups). Denote $\mathcal{D}_{\left\{\left(x_{I}, x_{T}, a\right)\right\} \mid f}$ as a distribution of $M_{i, i}$ given the model $f$. If the samples are from a specific group (e.g., white), the corresponding distribution is $\mathcal{D}_{\left\{\left(x_{I}, x_{T}, a\right) \mid a=\text { white }\right\} \mid f}$. Then, the objective of enhancing fairness can be defined as:

$$
\begin{equation*}
\min _{f} \sum_{\alpha}^{\mathcal{A}} d\left(\mathcal{D}_{\left\{\left(x_{I}, x_{T}, a\right)\right\} \mid f}-\mathcal{D}_{\left\{\left(x_{I}, x_{T}, a\right) \mid a=\alpha\right\} \mid f}\right) \tag{1}
\end{equation*}
$$

where $d$ is a distance function, $\mathcal{D}_{\left\{\left(x_{I}, x_{T}, a\right)\right\} \mid f}$ and $\mathcal{D}_{\left\{\left(x_{I}, x_{T}, a\right) \mid a=\alpha\right\} \mid f}$ are the underlying distributions that are computationally intractable. Instead, we use empirical distributions in Equation (1) that are estimated based on batch $\mathcal{B}$, i.e., $\mathcal{D}_{\mathcal{B} \mid f}$ and $\mathcal{D}_{\mathcal{B}_{a} \mid f}$. $\mathcal{B}_{a}$ means the samples in the batch are from group $a$.

To optimize the objective (1), one straightforward way is to minimize Kullback-Leibler (KL) divergence between the two distributions. However, KL-divergence is not symmetric and does not satisfy the triangle inequality, and thus not a true metric of distance. Instead, we follow [59] to minimize the Sinkhorn distance between the two distributions. Sinkhorn distance is a type of probability metric and a variant of the Wasserstein distance.

Specifically, we have a measurable set $\mathcal{Z}=\left\{z_{I}^{\top} z_{T}\right\}$, denoted by $\mathcal{M}(\mathcal{Z})$ the set of measures (not necessarily probability measures) on $\mathcal{Z}$, and $\mathcal{P}(\mathcal{Z})$ the set of probability measures on $\mathcal{Z}$. Consider distributions $\mathcal{D}_{\mathcal{B}}, \mathcal{D}_{\mathcal{B}_{a}} \in \mathcal{P}(\mathcal{Z})$, and let $\mu, \nu \in \mathcal{M}(\mathcal{Z})$ be two reference measures. For regularization parameter $\epsilon \geq 0$, the Sinkhorn distance between two distributions $\mathcal{D}_{\mathcal{B}}$ and $\mathcal{D}_{\mathcal{B}_{a}}$ is defined as

$$
\begin{align*}
& \mathcal{W}_{\epsilon}\left(\mathcal{D}_{\mathcal{B}}, \mathcal{D}_{\mathcal{B}_{a}}\right)= \\
& \inf _{\gamma \in \Gamma\left(\mathcal{D}_{\mathcal{B}}, \mathcal{D}_{\mathcal{B}_{a}}\right)}\left\{\mathbb{E}_{(x, y) \sim \gamma}[c(p, q)]+\epsilon H(\gamma \mid \mu \otimes \nu)\right\} \tag{2}
\end{align*}
$$

where $\Gamma\left(\mathcal{D}_{\mathcal{B}}, \mathcal{D}_{\mathcal{B}_{a}}\right)$ denotes the set of joint distributions whose first and second marginal distributions are $\mathcal{D}_{\mathcal{B}}$ and $\mathcal{D}_{\mathcal{B}_{a}}$, respectively, $c(p, q)$ denotes the transport cost, and $H(\gamma \mid \mu \otimes \nu)$ denotes the relative entropy of $\gamma$ with respect to the product measure $\mu \otimes \nu . p$ and $q$ are the points in the distributions $\mathcal{D}_{\mathcal{B}}$ and $\mathcal{D}_{\mathcal{B}_{a}}$, respectively. The Sinkhorn loss would be added to the loss used by CLIP in the pre-training phase for optimizing each component in CLIP.

## 5. Experiment \& Analysis

### 5.1. Experimental Setup

In this section, we describe the pre-training and evaluation setups as well as the metrics used to study performance and fairness of VL models on our proposed HarvardFairVLMed dataset.

Pre-Training: We use the widely-adopted VL methods CLIP [62] and BLIP2 [40] - for our analysis. For the natural pre-trained variants, we use the official checkpoints provided by CLIP and BLIP2. For the medical pre-trained variants, we pre-train both methods on the Harvard-FairVLMed dataset after initializing from the official checkpoints. More details are in the supplementary material.

Evaluation: We utilize two types of evaluation strategies - linear probing and zero-shot transfer. For linear probing, we follow the official MAE [19] implementation and train a linear classifier on top of the visual features from CLIP and BLIP2, respectively. Similar to MAE, we use a BatchNorm layer [22] before the linear classifier and employ a LARS optimizer [90] with a base learning rate of 0.1 , weight decay of 0 , and batch size of 512 . We conduct linear probing for 1000 epochs on a single V100 GPU for all experiments. For zero-shot transfer, we use the same setup as CLIP and select the class corresponding to the text embedding, which has the highest similarity with the image embedding.

Metrics: To comprehensively understand the balance between model performance and fairness, we use multiple metrics for evaluation, including Demographic Parity Dif-

Table 2. VL fairness analysis of two widely-used VL methods, i.e., CLIP and BLIP2, pre-trained on both natural and medical domains and evaluated across four different protected attributes, with all scores presented in percentage.

| Attribute | Model | $\overline{\text { DPD } \downarrow}$ | $\overline{\text { DEOdds } \downarrow}$ | $\overline{\mathbf{A U C} \uparrow}$ | $\overline{E S-A U C \uparrow} \uparrow$ | Group-wise AUC $\uparrow$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  |  | Asian | Black | White |
| Race | CLIP | $5.30 \pm 0.63$ | $14.00 \pm 1.01$ | $77.27 \pm 0.03$ | $72.43 \pm 0.29$ | $79.74 \pm 0.31$ | $73.60 \pm 0.12$ | $77.82 \pm 0.03$ |
|  | CLIP-FT | $4.01 \pm 0.47$ | $9.57 \pm 0.83$ | $80.27 \pm 0.08$ | $74.70 \pm 0.33$ | $82.19 \pm 0.26$ | $75.67 \pm 0.21$ | $81.20 \pm 0.08$ |
|  | BLIP2 | $9.44 \pm 0.65$ | $10.62 \pm 0.22$ | $73.81 \pm 0.02$ | $68.88 \pm 0.04$ | $76.28 \pm 0.06$ | $69.55 \pm 0.09$ | $74.22 \pm 0.03$ |
|  | BLIP2-FT | $8.30 \pm 0.36$ | $10.91 \pm 0.32$ | $80.10 \pm 0.03$ | $73.81 \pm 0.10$ | $82.09 \pm 0.09$ | $74.43 \pm 0.08$ | $80.97 \pm 0.07$ |
| Gender |  |  |  |  |  | Female | Male |  |
|  | CLIP | $1.08 \pm 0.19$ | $5.19 \pm 0.54$ | $77.27 \pm 0.03$ | $72.47 \pm 0.10$ | $74.25 \pm 0.07$ | $80.88 \pm 0.03$ |  |
|  | CLIP-FT | $0.39 \pm 0.26$ | $4.55 \pm 0.33$ | $80.27 \pm 0.08$ | $75.81 \pm 0.12$ | $77.59 \pm 0.09$ | $83.47 \pm 0.04$ |  |
|  | BLIP2 | $1.07 \pm 0.22$ | $5.88 \pm 0.24$ | $73.81 \pm 0.02$ | $69.16 \pm 0.03$ | $70.76 \pm 0.02$ | $77.48 \pm 0.05$ |  |
|  | BLIP2-FT | $2.41 \pm 0.06$ | $6.40 \pm 0.26$ | $80.10 \pm 0.03$ | $75.08 \pm 0.15$ | $77.03 \pm 0.10$ | $83.72 \pm 0.12$ |  |
| Ethnicity |  |  |  |  |  | Non-Hispanic | Hispanic |  |
|  | CLIP | $15.83 \pm 0.42$ | $14.73 \pm 0.54$ | $77.27 \pm 0.03$ | $71.70 \pm 0.08$ | $77.51 \pm 0.03$ | $69.73 \pm 0.13$ |  |
|  | CLIP-FT | $14.50 \pm 0.72$ | $22.49 \pm 1.44$ | $80.27 \pm 0.08$ | $76.31 \pm 0.36$ | $80.48 \pm 0.07$ | $75.30 \pm 0.46$ |  |
|  | BLIP2 | $8.78 \pm 1.35$ | $16.56 \pm 1.89$ | $73.81 \pm 0.02$ | $68.75 \pm 0.08$ | $74.10 \pm 0.02$ | $66.74 \pm 0.14$ |  |
|  | BLIP2-FT | $16.64 \pm 1.17$ | $18.41 \pm 1.98$ | $80.10 \pm 0.03$ | $77.13 \pm 0.09$ | $80.25 \pm 0.03$ | $76.39 \pm 0.12$ |  |
| Language |  |  |  |  |  | Engli | Spani | Others |
|  | CLIP | $13.57 \pm 1.35$ | $33.11 \pm 0.53$ | $77.27 \pm 0.03$ | $70.89 \pm 0.21$ | $77.25 \pm 0.03$ | $84.00 \pm 0.16$ | $75.02 \pm 0.28$ |
|  | CLIP-FT | $16.75 \pm 1.08$ | $15.74 \pm 0.28$ | $80.27 \pm 0.08$ | $67.06 \pm 0.46$ | $80.77 \pm 0.07$ | $74.43 \pm 0.99$ | $66.91 \pm 0.17$ |
|  | BLIP2 | $22.40 \pm 0.33$ | $15.41 \pm 1.41$ | $73.81 \pm 0.02$ | $70.34 \pm 0.64$ | $73.40 \pm 0.02$ | $75.95 \pm 0.87$ | $76.19 \pm 0.12$ |
|  | BLIP2-FT | $14.08 \pm 3.56$ | $37.68 \pm 2.51$ | $80.10 \pm 0.03$ | $69.98 \pm 0.56$ | $80.62 \pm 0.04$ | $83.14 \pm 1.18$ | $69.51 \pm 0.32$ |

ference (DPD) [4, 5], Difference in Equalized Odds (DEOdds) [4], Area Under the Receiver Operating Characteristic Curve (AUC), Equity-Scaled AUC [50], and Group-wise AUC. More details can be found in the supplementary material.

### 5.2. VL Fairness Analysis

In this section, we present a comprehensive fairness analysis of two widely-used VL models, benchmarked across two different pre-training domains and four different protected attributes. Table 2 presents the linear probing results, examining various performance (AUC) and fairness (DPD, DEOdds, ES-AUC) metrics, as well as reporting the groupwise AUC scores across the individual subgroups within each of the four protected attributes. We primarily focus on the ES-AUC metric in the subsequent analysis since it captures notions of both overall performance as well as fairness - both of which are important for safety-critical medical applications (Sec. 5.1). Next, we briefly discuss the disparities in VL performance across the various protected attributes, and the impact of different VL pre-training domains (natural vs. medical) and VL pre-training methods (CLIP vs. BLIP2) on model performance and fairness.

Protected Attributes: Across the four protected attributes - race, gender, ethnicity, and language - our results indicate that VL models exhibit the best performance-fairness trade-off on ethnicity and the worst on language with average ES-AUC scores (across the four VL models) of 73.47 and 69.57 , respectively. A granular analysis reveals that in terms of racial subgroups, Asian patients consistently have the highest diagnostic performance, whereas Black patients have the lowest. Across genders, male patients are consistently better diagnosed than female patients. Moreover,
non-Hispanic is the highest-performing subgroup across ethnicities, whereas Spanish speakers have the best diagnostic performance across the language subgroups. Some of these performance disparities could be attributed to the imbalances in the pre-training datasets. For instance, nonHispanic patients make up $90.6 \%$ of our dataset, potentially leading to improved performance in this subgroup. However, this is unlikely to be the only factor responsible for these performance disparities since the Asian, Male, and Spanish subgroups exhibit superior performances despite being the minority subgroups. This indicates that the pretraining of these models could potentially play a role in the biases exhibited by these models. We delve deeper into this in the subsequent analysis.

Natural vs. Medical Pre-training: Here, we compare the VL models pre-trained on two different domains - natural vs. medical VL datasets. Comparing the two pretraining domains, we observe that pre-training on medical data (i.e., paired fundus images + clinical notes) improves the performance-fairness trade-off across all protected attributes except language. Concretely, we see the most benefit on ethnicity, with improvements of as much as 8.38 in terms of ES-AUC. A granular analysis of the individual subgroups shows that the White and Hispanic subgroups witness the most improvement from medical pre-training, with improvements of as much as 6.75 and 9.65 ES-AUC in the former and latter, respectively. This indicates that medical VL pre-training could be an effective strategy for improving the performances of under-performing subgroups, thereby effectively reducing the performance disparities across different protected attributes.

CLIP vs. BLIP2: Next, we investigate the impact of different VL pre-training methods on the performance dispar-

Table 3. Zero-shot transfer results of CLIP vs. FairCLIP, reporting the mean and standard deviation across three random seeds.

| Attribute | Model | DPD $\downarrow$ | DEOdds $\downarrow$ | AUC $\uparrow$ | ES-AUC $\uparrow$ | Group-wise AUC $\uparrow$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Race |  |  |  |  |  | Asian | Black | White |
|  | CLIP (ViT-B/16) | $15.35 \pm 6.50$ | $15.11 \pm 5.01$ | $67.84 \pm 0.90$ | $61.67 \pm 0.63$ | $73.11 \pm 2.74$ | $70.78 \pm 1.84$ | $66.02 \pm 0.60$ |
|  | FairCLIP (ViT-B/16) | $\mathbf{6 . 0 7} \pm 2.44$ | $\mathbf{1 0 . 5 0} \pm 2.73$ | $\mathbf{7 0 . 2 4} \pm 1.26$ | $65.50 \pm 2.60$ | $\mathbf{7 4 . 8 3} \pm 0.46$ | $71.39 \pm 0.66$ | $69.17 \pm 2.10$ |
|  | CLIP (ViT-L/14) | $10.10 \pm 9.44$ | $10.79 \pm 10.41$ | $67.83 \pm 2.92$ | $63.53 \pm 1.83$ | $70.65 \pm 4.58$ | $70.12 \pm 3.39$ | $66.22 \pm 2.97$ |
|  | FairCLIP (ViT-L/14) | $17.79 \pm 4.86$ | $18.30 \pm 2.07$ | $69.88 \pm 2.00$ | $66.54 \pm 1.73$ | $71.78 \pm 2.18$ | $71.79 \pm 2.13$ | $68.67 \pm 1.99$ |
| Gender |  |  |  |  |  | Female | Male |  |
|  | LIP (ViT-B/16) | $4.34 \pm 0.66$ | $9.95 \pm 0.64$ | $67.84 \pm 0.90$ | $63.21 \pm 0.83$ | $64.62 \pm 0.83$ | $71.96 \pm 1.22$ |  |
|  | FairCLIP (ViT-B/16) |  |  | 6976 - | 65.39 | $66.81 \pm 2.50$ | $\mathbf{7 3 . 5 0} \pm 2.41$ |  |
|  | CLIP (ViT-L/14) | $2.93=$ | $4.29 \pm 4.05$ | $67.83 \pm 2.92$ | $63.86 \pm 2.36$ | $65.13 \pm 2.60$ | $71.31 \pm 3.24$ |  |
|  | FairCLIP (ViT-L/14) | $5.82 \pm 1.22$ | $8.14 \pm 2.62$ | $69.74 \pm 0.95$ | $\mathbf{6 6 . 0 0} \pm 1.55$ | $67.29 \pm 1.38$ | $72.99 \pm 0.83$ |  |
| Ethnicity |  |  |  |  |  | Non-Hispanic | Hispanic |  |
|  | IP | 8 | 1 | J | 6 |  |  |  |
|  | FairCLIP (ViT-B/16) | $9.1 \quad$ | $14.30=$ | $69.30=$ | $63.33=$ | $69.62 \pm 2.02$ | $60.19 \pm 1.18$ |  |
|  | CLIP (ViT-L/14) | 77 | $11.56 \pm$ | $67.83 \pm$ | $62.20 \pm$ | $68.14 \pm 3.07$ | $59.13 \pm 0$ |  |
|  | FairCLIP (ViT-L/14) | $9.44 \pm 3.28$ | $13.03 \pm 5.56$ | $69.87 \pm 1.05$ | $64.90 \pm 2.05$ | $70.16 \pm 1.05$ | $\mathbf{6 2 . 4 3} \pm 2.99$ |  |
| Language |  |  |  |  |  | Eng | Spanish | 0 |
|  | CLIP (ViT-B/16) | $11.11 \pm 2.42$ | $16.97 \pm 2.73$ | $67.84 \pm 0.90$ | $60.19 \pm 3.47$ | $67.88 \pm 1.15$ | $61.93 \pm 4.57$ | $61.89 \pm 2.90$ |
|  | FairCLIP (ViT-B/16) | $7.34 \pm 4.63$ | $17.15 \pm 11.13$ | $\mathbf{7 0 . 0 8} \pm 1.14$ | $\mathbf{6 2 . 3 1} \pm 0.96$ | $\mathbf{7 0 . 2 2} \pm 1.27$ | $\mathbf{6 8 . 4 7} \pm 5.49$ | $\mathbf{6 3 . 4 7} \pm 2.13$ |
|  | CLIP (ViT-L/14) | $7.62 \pm 5.39$ | $\mathbf{1 0 . 8 4} \pm 9.84$ | $67.83 \pm 2.92$ | $61.29 \pm 1.28$ | $67.77 \pm 3.18$ | $63.83 \pm 3.20$ | $62.24 \pm 2.25$ |
|  | FairCLIP (ViT-L/14) | $11.63 \pm 3.53$ | $11.05 \pm 5.66$ | $68.68 \pm 2.26$ | $61.50 \pm 2.29$ | $68.71 \pm 2.31$ | $65.06 \pm 3.96$ | $61.18 \pm 1.20$ |

ities of the downstream models. In order to ensure a fair comparison, we use the same ViT-L/14 architecture for the vision encoders of both models. Unlike CLIP, BLIP2 is a parameter-efficient VL method that keeps the vision encoder frozen and leverages a Q-former to learn the most suitable image representations corresponding to the text. From the natural pre-training results in Table 2, we note that CLIP consistently outperforms BLIP2 in terms of not only overall performance ( 77.27 vs. 73.81 AUC) but also fairness, as evidenced by the superior ES-AUC scores across all four protected attributes. However, we observe that medical pre-training largely closes this gap, with BLIP2 yielding similar results as CLIP (80.10 vs. 80.27 AUC). This large boost in BLIP2 performance indicates that (1) the parameter-efficient training in BLIP2 is effective for adapting to the medical VL data, and that (2) the clinical notes are useful for guiding BLIP2 to extract meaningful visual features from the frozen vision encoder. Among the medical pre-trained CLIP and BLIP2 models, CLIP exhibits a better performance-fairness trade-off than BLIP2 on the race and gender attributes, whereas BLIP2 yields more favorable results on the ethnicity and language attributes.

In summary, we observe that all VL models exhibit biases, with Asian, Male, Non-Hispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively. Medical pre-training enhances the performance-fairness trade-off across all attributes except language. Furthermore, different VL pre-training methods exhibit varying strengths, with CLIP outperforming on race and gender, whereas BLIP2 yields superior results on ethnicity and language.

### 5.3. CLIP vs. FairCLIP

Table 3 compares the zero-shot performance of CLIP against FairCLIP across two different architectures (ViT-
B/16 and ViT-L/14) and four different protected attributes. Both CLIP and FairCLIP are fine-tuned with the pairs of images and clinical notes without supervised information (i.e., labels). Then, the resulting models are evaluated in the classification task. CLIP exhibits notable disparities in groupwise AUC for attributes such as race, gender, ethnicity, and language, indicating the presence of bias in glaucoma detection. Regarding the racial groups, FairCLIP (ViT-B/16) gains a significant improvement in fairness, reducing DPD and DEOdds to 6.07 and 10.50 , respectively, while increasing the AUC for the White group to 69.17. In terms of gender disparity, FairCLIP (ViT-B/16) addresses the bias effectively, elevating the female AUC to 66.81 from 64.62, indicating a substantial enhancement in gender. Similar enhancements by FairCLIP are observed in ethnicity and language groups.

Overall, FairCLIP demonstrates a significant improvement over CLIP in terms of both fairness metrics (DPD, DEOdds) as well as ES-AUC and AUC scores across various demographic subgroups. The supplementary material shows more end-to-end fine-tuning results, further validating the effectiveness of FairCLIP. These empirical findings suggest that optimizing the distance between the overall sample distribution and the distribution w.r.t. specific subgroups effectively improves fairness, indicating a promising direction in addressing and mitigating inherent biases.

### 5.4. Ablation Studies

Clinical Note Summarization: VL models are usually trained with images and captions, which tend to be quite short. In contrast, the clinical notes in our HarvardFairVLMed dataset are fairly lengthy, capturing a lot more nuanced information. Hence, we summarize these clinical notes in order to align our setup with the standard VL frameworks while retaining essential medical informa-
tion. We use three different types of LLMs for summarization. In addition to the SOTA GPT-4 [56] model, we also use two LLMs specialized for the medical domain PMC-LLAMA [82] and MED42 ${ }^{2}$. We use the following prompt for summarization for all three LLMs: Summarize the key details, including the presence of glaucoma, from the clinical note within 180 characters. Figure 3a compares the performance-fairness trade-off of BLIP2 models trained via the three summarized notes against the original notes. Across all protected attributes except language, we note that all three summarization methods yield a consistent improvement in ES-AUC. Particularly, MED42 yields the best performance across race, GPT-4 yields the best performance across gender, whereas PMC-LLAMA yields the best performance across both ethnicity and language. Overall, the medical pre-trained LLMs (PMC-LLAMA and MED42) yield superior results, highlighting the efficacy of domain-specific LLMs for clinical note summarization.

Vision vs. Multimodal Features: In order to decouple the benefits of image and text features, we conduct linear probing on the BLIP2 pre-trained models using either vision-only or (vision + language) features. Table 4 presents the performance-fairness trade-off in terms of ES-AUC. We note that the multimodal features consistently improve the performance-fairness trade-off across all protected attributes except language. This highlights that the VL models make effective use of the clinical textual features, with the most appreciable gains observed on the race attribute.

Natural vs. Medical Vision Encoder: To investigate the impact of different vision encoders on model fairness in BLIP2, we utilize two different pre-trained encoders - 1) CLIP trained on the natural domain, whereas 2) PMC-CLIP trained on the medical domain. The results in Figure 3b reveal that PMC-CLIP outperforms CLIP across all four protected attributes, with the most appreciable gains on the racial subgroups. We note that medical-specific LLM summarizers and vision encoders consistently improve the performance-fairness trade-off of VL models, with the most appreciable improvements across the race attribute.

Comparison with Adversary Fairness: Beutel et al. [8] introduce a fairness approach that employs adversarial loss to prevent the model from inaccurately predicting sensitive attributes. This method aims to ensure that the model predicts the label of an image without relying on its sensitive attributes, thereby reducing bias in classification. Figure $3 \mathrm{c}$ shows the performance comparison among CLIP, CLIP with adversarial loss (CLIP w/ Adv), and FairCLIP. The performance of CLIP with adversarial training (CLIP w/ Adv) does not consistently surpass that of the standard CLIP across all attributes. In contrast, FairCLIP consistently outperforms CLIP. This variation in performance can be attributed to the inherent challenges of adversarial train-[^2]

Table 4. Impact of vision-only and (vision + language) features on the performance-fairness trade-off of linear probing via BLIP2.

| Attribute | $\mathbf{V}$ | $\mathbf{L}$ | ES-AUC $\uparrow$ | Group-wise AUC $\uparrow$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | Asian | Black | White |
| Race | $\checkmark$ | $x$ | 73.76 | 82.08 | 74.35 | 81.03 |
|  | $\checkmark$ | $\checkmark$ | 76.70 | 79.79 | 77.77 | 82.37 |
| Gender |  |  |  | Female | Male |  |
|  | $\checkmark$ | $x$ | 75.22 | 77.13 | 83.66 |  |
|  | $\checkmark$ | $\checkmark$ | 79.15 | 80.21 | 83.38 |  |
| Ethnicity |  |  |  | Non-Hispanic | Hispanic |  |
|  | $\checkmark$ | $x$ | 77.18 | 80.28 | 76.46 |  |
|  | $\checkmark$ | $\checkmark$ | 75.54 | 81.95 | 73.85 |  |
| Language |  |  |  | English | Spanish | Others |
|  |  | $x$ | 69.88 | 80.64 | 83.52 | 69.36 |
|  | $\checkmark$ | $\checkmark$ | 66.35 | 82.32 | 69.89 | 71.01 |

![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-08.jpg?height=254&width=816&top_left_y=762&top_left_x=1077)

![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-08.jpg?height=176&width=274&top_left_y=779&top_left_x=1083)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-08.jpg?height=176&width=258&top_left_y=779&top_left_x=1351)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_b6ef75341d59049bde6fg-08.jpg?height=187&width=268&top_left_y=777&top_left_x=1617)

(c)
Figure 3. Extensive analyses on Harvard-FairVLMed, including (a) Effects of various LLM summarizations on the performancefairness trade-off of BLIP2, (b) Effects of using pre-trained vision encoders from natural (CLIP) and medical (PMC-CLIP) domains on the performance-fairness trade-off of BLIP2, and (c) Performance comparison among CLIP, CLIP with adversarial loss (CLIP w/ Adv), and FairCLIP.

ing in maintaining equivalent prediction accuracy for each attribute. On the other hand, FairCLIP utilizes Sinkhorn loss, which effectively encourages uniformity in the distribution of all samples relative to the distributions corresponding to each group.

## 6. Conclusion

Given the critical need for fairness in healthcare-focused settings, we introduce the first fair vision-language medical dataset (Harvard-FairVLMed) for studying the fairness of medical VL foundation models. Our comprehensive fairness analysis on Harvard-FairVLMed reveals significant biases in all VL models. To address these biases, we propose FairCLIP, an optimal transport-based approach that effectively balances both performance and fairness. To aid future research in this area, we make our dataset and code publicly available at https://ophai.hms.harvard.edu/ datasets/harvard-fairvlmed10k.

## Acknowledgement

This work was supported by NIH R00 EY028631, NIH R21 EY035298, NIH P30 EY003790, Research To Prevent Blindness International Research Collaborators Award, and Alcon Young Investigator Grant. We also acknowledge the generous funding resources provided by NYU Abu Dhabi with code AD131.

## References

[1] Peking university international competition on ocular disease intelligent recognition (odir-2019). 1, 3

[2] Mohamed Abdalla and Benjamin Fine. Hurdles to artificial intelligence deployment: Noise in schemas and "gold" labels. Radiology: Artificial Intelligence, 5(2):e220056, 2023. 3

[3] Parnian Afshar, Shahin Heidarian, Nastaran Enshaei, Farnoosh Naderkhani, Moezedin Javad Rafiee, Anastasia Oikonomou, Faranak Babaki Fard, Kaveh Samimi, Konstantinos N Plataniotis, and Arash Mohammadi. Covid-ctmd, covid-19 computed tomography scan dataset applicable in machine learning and deep learning. Scientific Data, 8(1):121, 2021. 1

[4] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pages 60-69. PMLR, 2018. 6, 2

[5] Alekh Agarwal, Miroslav Dudík, and Zhiwei Steven Wu. Fair regression: Quantitative definitions and reductionbased algorithms. In International Conference on Machine Learning, pages 120-129. PMLR, 2019. 6, 2

[6] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Vision-language model for visual question answering in medical imagery. Bioengineering, 10 (3):380, 2023. 3

[7] Sebastian Benthall and Bruce D Haynes. Racial categories in machine learning. In Proceedings of the conference on fairness, accountability, and transparency, pages 289-298, 2019. 2

[8] Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017. 2, 8

[9] Martim Brandao. Age and gender bias in pedestrian detection algorithms. arXiv preprint arXiv:1906.10490, 2019. 2

[10] Alexander Brown, Nenad Tomasev, Jan Freyberg, Yuan Liu, Alan Karthikesalingam, and Jessica Schrouff. Detecting and preventing shortcut learning for fair medical ai using shortcut testing (short). arXiv preprint arXiv:2207.10384, 2022. 2

[11] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pages 77-91. PMLR, 2018. 2

[12] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria De La Iglesia-Vaya. Padchest: A large chest x-ray image dataset with multi-label annotated reports. Medical image analysis, 66:101797, 2020. 1, 3

[13] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J McDonald. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304-310, 2016. 3
[14] Sina Farsiu, Stephanie J Chiu, Rachelle V O’Connell, Francisco A Folgar, Eric Yuan, Joseph A Izatt, Cynthia A Toth, Age-Related Eye Disease Study 2 Ancillary Spectral Domain Optical Coherence Tomography Study Group, et al. Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography. Ophthalmology, 121(1):162-172, 2014. 1

[15] Ben Glocker, Charles Jones, Mélanie Bernhardt, and Stefan Winzeck. Algorithmic encoding of protected characteristics in chest x-ray disease detection models. Ebiomedicine, 89, 2023. 1

[16] Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri. Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick $17 \mathrm{k}$ dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1820-1828, 2021. 1

[17] Priya Gupta, Di Zhao, Eliseo Guallar, Fang Ko, Michael V Boland, and David S Friedman. Prevalence of glaucoma in the united states: the 2005-2008 national health and nutrition examination survey. Investigative ophthalmology \& visual science, 57(6):2905-2913, 2016. 2

[18] Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 501-512, 2020. 2

[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000-16009, 2022. 5

[20] Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathological visual question answering. arXiv preprint arXiv:2010.12435, 2020. 3

[21] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual-language foundation model for pathology image analysis using medical twitter. Nature medicine, pages 1-10, 2023. 1, 3

[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. pmlr, 2015. 5

[23] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, pages 590-597, 2019. 3

[24] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, pages 590-597, 2019. 1, 2

[25] Baoyu Jing, Pengtao Xie, and Eric Xing. On the auto-
matic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017. 3

[26] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a deidentified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. 2

[27] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. 1, 3

[28] Neil Joshi and Phil Burlina. Ai fairness via domain adaptation. arXiv preprint arXiv:2104.01109, 2021. 2

[29] Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, and Yuexian Zou. Visiongpt-3d: A generalized multimodal agent for enhanced $3 \mathrm{~d}$ vision understanding. arXiv preprint arXiv:2403.09530, 2024. 1

[30] Chris Kelly, Luhui Hu, Bang Yang, Yu Tian, Deshun Yang, Cindy Yang, Zaoshan Huang, Zihao Li, Jiayin Hu, and Yuexian Zou. Visiongpt: Vision-language understanding agent using generalized multimodal framework. arXiv preprint arXiv:2403.09027, 2024. 1

[31] Muhammad Osama Khan, Muhammad Muneeb Afzal, Shujaat Mirza, and Yi Fang. How fair are medical imaging foundation models? In Machine Learning for Health (ML4H), pages 217-231. PMLR, 2023. 1

[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014. 2

[33] Shunsuke Kogure, Kai Watabe, Ryosuke Yamada, Yoshimitsu Aoki, Akio Nakamura, and Hirokatsu Kataoka. Age should not matter: Towards more accurate pedestrian detection via self-training. In Computer Sciences \& Mathematics Forum, page 11. MDPI, 2022. 2

[34] Shivansh Kohli, Utkarsh Verma, Vinay V Kirpalani, and Ramamoorthy Srinath. Dermatobot: an image processing enabled chatbot for diagnosis and tele-remedy of skin diseases. In 2022 3rd International Conference for Emerging Technology (INCET), pages 1-5. IEEE, 2022. 3

[35] Oleksandr Kovalyk, Juan Morales-Sánchez, Rafael VerdúMonedero, Inmaculada Sellés-Navarro, Ana PalazónCabanes, and José-Luis Sancho-Gómez. Papila: Dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment. Scientific Data, 9 (1):291, 2022. 1, 3

[36] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):1-10, 2018. 3

[37] Yiming Lei, Zilong Li, Yan Shen, Junping Zhang, and Hongming Shan. Clip-lung: Textual knowledge-guided lung nodule malignancy prediction. arXiv preprint arXiv:2304.08013, 2023. 3

[38] Gang Li, Jian Li, Shanshan Zhang, and Jian Yang. Learning hierarchical graph for occluded pedestrian detection. In
Proceedings of the 28th ACM International Conference on Multimedia, pages 1597-1605, 2020. 2

[39] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888-12900. PMLR, 2022. 3

[40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 2, 3, 5

[41] Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. arXiv preprint arXiv:2303.07240, 2023. 3

[42] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650-1654. IEEE, 2021. 3

[43] Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. Clinically accurate chest x-ray report generation. In Machine Learning for Healthcare Conference, pages 249-269. PMLR, 2019. 3

[44] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi $\mathrm{Lu}$, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21152-21164, 2023. 3

[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 2

[46] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pretrained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6):bbac409, 2022. 1

[47] Yan Luo, Min Shi, Yu Tian, Tobias Elze, and Mengyu Wang. Harvard glaucoma detection and progression: A multimodal multitask dataset and generalization-reinforced semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20471-20482, 2023. 1, 3

[48] Yan Luo, Min Shi, Yu Tian, Tobias Elze, and Mengyu Wang. Harvard glaucoma detection and progression: A multimodal multitask dataset and generalization-reinforced semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20471-20482, 2023. 2

[49] Yan Luo, Yu Tian, Min Shi, Tobias Elze, and Mengyu Wang. Eye fairness: A large-scale 3d imaging dataset for equitable eye diseases screening and fair identity scaling. arXiv preprint arXiv:2310.02492, 2023. 2, 3

[50] Yan Luo, Yu Tian, Min Shi, Louis R. Pasquale, Lucy Q. Shen, Nazlee Zebardast, Tobias Elze, and Mengyu Wang. Harvard glaucoma fairness: A retinal nerve disease dataset
for fairness learning and fair identity normalization. IEEE Transactions on Medical Imaging, pages 1-1, 2024. 3, 6, 2

[51] Ricards Marcinkevics, Ece Ozkan, and Julia E Vogt. Debiasing deep chest x-ray classifiers using intra-and postprocessing methods. In Machine Learning for Healthcare Conference, pages 504-536. PMLR, 2022. 2

[52] Masoud Monajatipoor, Mozhdeh Rouhsedaghat, Liunian Harold Li, C-C Jay Kuo, Aichi Chien, and Kai-Wei Chang. Berthop: An effective vision-and-language model for chest x-ray disease diagnosis. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 725-734. Springer, 2022. 3

[53] Usman Naseem, Matloob Khushi, and Jinman Kim. Visionlanguage transformer for interpretable pathology visual question answering. IEEE Journal of Biomedical and Health Informatics, 27(4):1681-1690, 2022. 3

[54] Hoang TN Nguyen, Dong Nie, Taivanbat Badamdorj, Yujie Liu, Lingzi Hong, Jason Truong, and Li Cheng. Eddietransformer: Enriched disease embedding transformer for x-ray report generation. In 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2022. 3

[55] Saeed Niksaz and Fahimeh Ghasemian. Improving chest xray report generation by leveraging text of similar images. Available at SSRN 4211036, 2023. 3

[56] OpenAI. Gpt-4 technical report, 2023. 8

[57] Sungho Park, Jewook Lee, Pilhyeon Lee, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. Fair contrastive learning for facial attribute classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10389-10398, 2022. 2

[58] Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa, and Christoph M Friedrich. Radiology objects in context (roco): a multimodal image dataset. In Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVIISTENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3, pages 180-189. Springer, 2018. 3

[59] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends $®$ in Machine Learning, 11(5-6):355-607, 2019. 5

[60] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and calibration. Advances in neural information processing systems, 30, 2017. 2

[61] Esther Puyol-Antón, Bram Ruijsink, Stefan K Piechnik, Stefan Neubauer, Steffen E Petersen, Reza Razavi, and Andrew P King. Fairness in cardiac mr image analysis: an investigation of bias due to data imbalance in deep learning based segmentation. In Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part III 24, pages 413-423. Springer, 2021. 2
[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 4, 5, 2

[63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 2, 3

[64] Vikram V Ramaswamy, Sunnie SY Kim, and Olga Russakovsky. Fair attribute classification through latent space de-biasing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9301-9310, 2021. 2

[65] Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. Fr-train: A mutual information-based approach to fair and robust training. In International Conference on Machine Learning, pages 8147-8157. PMLR, 2020. 2

[66] Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, and Shadi Albarqouni. Fairness by learning orthogonal disentangled representations. In Computer Vision-ECCV 2020. 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIX 16, pages 746-761. Springer, 2020. 2

[67] Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Irene Y Chen, and Marzyeh Ghassemi. Chexclusion: Fairness gaps in deep chest x-ray classifiers. In BIOCOMPUTING 2021: proceedings of the Pacific symposium, pages 232-243. World Scientific, 2020. 2

[68] Yahya Shaikh, Fei Yu, and Anne L Coleman. Burden of undetected and untreated glaucoma in the united states. American journal of ophthalmology, 158(6):1121-1129, 2014. 2

[69] Min Shi, Anagha Lokhande, Mojtaba S Fazli, Vishal Sharma, Yu Tian, Yan Luo, Louis R Pasquale, Tobias Elze, Michael V Boland, Nazlee Zebardast, et al. Artifact tolerant clustering-guided contrastive embedding learning for ophthalmic images in glaucoma. IEEE Journal of Biomedical and Health Informatics, 2023. 2

[70] Min Shi, Yan Luo, Yu Tian, Lucy Q Shen, Tobias Elze, Nazlee Zebardast, Mohammad Eslami, Saber Kazeminasab, Michael V Boland, David S Friedman, et al. Equitable artificial intelligence for glaucoma screening with fair identity normalization. medRxiv, pages 2023-12, 2023. 2

[71] Min Shi, Jessica A Sun, Anagha Lokhande, Yu Tian, Yan Luo, Tobias Elze, Lucy Q Shen, and Mengyu Wang. Artifact correction in retinal nerve fiber layer thickness maps using deep learning and its clinical utility in glaucoma. Translational Vision Science \& Technology, 12(11):12-12, 2023. 2

[72] Min Shi, Yu Tian, Yan Luo, Tobias Elze, and Mengyu Wang. Rnflt2vec: Artifact-corrected representation learning for retinal nerve fiber layer thickness maps. Medical Image Analysis, page 103110, 2024. 2

[73] Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine van Zuylen, Sravanthi Parasa, Sameer

Singh, Matt Gardner, and Hannaneh Hajishirzi. Medicat: A dataset of medical images, captions, and textual references. arXiv preprint arXiv:2010.06000, 2020. 3

[74] Yih-Chung Tham, Xiang Li, Tien Y Wong, Harry A Quigley, Tin Aung, and Ching-Yu Cheng. Global prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis. Ophthalmology, 121(11):2081-2090, 2014. 2

[75] Jiang Tian, Cong Li, Zhongchao Shi, and Feiyu Xu. A diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism. In Medical Image Computing and Computer Assisted InterventionMICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, pages 702-710. Springer, 2018. 3

[76] Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze, and Mengyu Wang. Fairseg: A large-scale medical image segmentation dataset for fairness learning with fair errorbound scaling. arXiv preprint arXiv:2311.02189, 2023. 2

[77] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):1-9, 2018. 1

[78] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware reinforcement learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9322-9331, 2020. 2

[79] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8919-8928, 2020. 2

[80] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. arXiv preprint arXiv:2210.10163, 2022. 1,3

[81] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge enhanced language-image pre-training. medRxiv, pages 2023-01, 2023. 3

[82] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023. 8

[83] Fan Wu, Haiqiong Yang, Linlin Peng, Zongkai Lian, Mingxin Li, Gang Qu, Shancheng Jiang, and Yu Han. Agnet: Automatic generation network for skin imaging reports. Computers in biology and medicine, 141:105037, 2022. 3

[84] Yawen Wu, Dewen Zeng, Xiaowei Xu, Yiyu Shi, and Jingtong Hu. Fairprune: Achieving fairness through pruning for dermatological disease diagnosis. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 743-753. Springer, 2022. 2

[85] Bradley T Wyman, Danielle J Harvey, Karen Crawford, Matt A Bernstein, Owen Carmichael, Patricia E Cole, Paul K Crane, Charles DeCarli, Nick C Fox, Jeffrey L
Gunter, et al. Standardization of analysis sets for reporting results from adni mri data. Alzheimer's \& Dementia, 9 (3):332-337, 2013. 1

[86] Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng, Attila Kiraly, Sahar Kazemzadeh, Zakkai Melamed, et al. Elixr: Towards a general purpose x-ray artificial intelligence system through alignment of large language models and radiology vision encoders. arXiv preprint arXiv:2308.01317, 2023. 1

[87] Deshun Yang, Luhui Hu, Yu Tian, Zihao Li, Chris Kelly, Bang Yang, Cindy Yang, and Yuexian Zou. Worldgpt: A sora-inspired video ai agent as rich world models from text and image inputs. arXiv preprint arXiv:2403.07944, 2024. 1

[88] Shaokang Yang, Jianwei Niu, Jiyan Wu, Yong Wang, Xuefeng Liu, and Qingfeng Li. Automatic ultrasound image report generation with adaptive multimodal attention mechanism. Neurocomputing, 427:40-49, 2021. 3

[89] Kihyun You, Jawook Gu, Jiyeon Ham, Beomhee Park, Jiho Kim, Eun K Hong, Woonhyuk Baek, and Byungseok Roh. Cxr-clip: Toward large scale chest x-ray language-image pre-training. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 101-111. Springer, 2023. 3

[90] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017. 5

[91] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In Artificial intelligence and statistics, pages 962-970. PMLR, 2017. 2

[92] Juan M Zambrano Chaves, Akshay S Chaudhari, Andrew L Wentland, Arjun D Desai, Imon Banerjee, Robert D Boutin, David J Maron, Fatima Rodriguez, Alexander T Sandhu, R Brooke Jeffrey, et al. Opportunistic assessment of ischemic heart disease risk using abdominopelvic computed tomography and medical record data: a multimodal explainable artificial intelligence approach. medRxiv, pages 2021-01, 2021. 1

[93] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335-340, 2018. 2

[94] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2023. 3

[95] Yi Zhang and Jitao Sang. Towards accuracy-fairness paradox: Adversarial example-based data augmentation for visual debiasing. In Proceedings of the 28th ACM International Conference on Multimedia, pages 4346-4354, 2020. 2

[96] Zizhao Zhang, Pingjun Chen, Mason McGough, Fuyong Xing, Chunbao Wang, Marilyn Bui, Yuanpu Xie, Manish Sapkota, Lei Cui, Jasreman Dhillon, et al. Pathologist-level interpretable whole-slide cancer diagnosis with deep learning. Nature Machine Intelligence, 1(5):236-245, 2019. 3

[97] Yue Zhao, Ming Tian, Jing Jin, Qiucheng Wang, Jian Song, and Yi Shen. An automatically thyroid nodules feature extraction and description network for ultrasound images. In 2021 IEEE International Ultrasonics Symposium (IUS), pages 1-4. IEEE, 2021. 3

[98] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. arXiv preprint arXiv:2310.18961, 2023. 3

[99] Yuyin Zhou, Shih-Cheng Huang, Jason Alan Fries, Alaa Youssef, Timothy J Amrhein, Marcello Chang, Imon Banerjee, Daniel Rubin, Lei Xing, Nigam Shah, et al. Radfusion: Benchmarking performance and fairness for multimodal pulmonary embolism detection from ct and ehr. arXiv preprint arXiv:2111.11665, 2021. 2

[100] Dominik Zietlow, Michael Lohaus, Guha Balakrishnan, Matthäus Kleindessner, Francesco Locatello, Bernhard Schölkopf, and Chris Russell. Leveling down in computer vision: Pareto inefficiencies in fair deep classifiers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10410-10421, 2022. 2

# FairCLIP: Harnessing Fairness in Vision-Language Learning 

Supplementary Material
