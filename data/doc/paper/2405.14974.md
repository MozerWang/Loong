# LOVA ${ }^{3}$ : Learning to Visual Question Answering, Asking and Assessment 

Henry Hengyuan Zhao ${ }^{1}$, Pan Zhou ${ }^{2,3 \dagger}$, Difei Gao ${ }^{1}$ Mike Zheng Shou ${ }^{1 \dagger}$<br>${ }^{1}$ Show Lab, National University of Singapore, Singapore,<br>${ }^{2}$ Singapore Management University, Singapore,<br>${ }^{3}$ Sea AI Lab, Singapore<br>https://github.com/showlab/LOVA3


#### Abstract

Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. However, current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. To this end, we introduce $\mathbf{L O V A}^{3}$, an innovative framework named "Learning tO Visual Question Answering, Asking and Assessment," designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called EvalQABench, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 validation and testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions will improve their multimodal comprehension and lead to better performance. We validate our hypothesis by training an MLLM using the $\mathbf{L O V A}^{3}$ framework and testing it on 10 multimodal benchmarks. The results demonstrate consistent performance improvements, thereby confirming the efficacy of our approach.


## 1 Introduction

To acquire knowledge, we humans often answer lots of questions and then improve ourselves by comparing our answers with the ground-truth answers. As a result, this learning mechanism empowers humans with the answering ability, which allows humans to handle well many real tasks, such as visual question answering $[29,59,33,30,46,45]$. However, as described in the following slogan,

"The art of proposing a question must be held in higher value than solving it." Georg Cantor [8]

asking a question is very valuable and even more important than answering a question. Indeed, humans also acquire knowledge from learning to ask questions since it encourages individuals to engage more deeply with information, thereby enhancing problem-solving skills [74, 75, 24, 62]. In addition to asking questions, humans also improve themselves through self-evaluation: humans try to identify the correctness of the answer and thus are involved in a deep understanding of our diverse world $[92,72]$. These three learning mechanisms not only play a vital role in the human intelligence learning process but also empower humans with the corresponding abilities, including answering,[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_16f0e7734de6a63860b3g-02.jpg?height=280&width=652&top_left_y=190&top_left_x=367)

GenQA Ability
Prompt: Analyze the image and respond with a
direct question and answer.
LLaVA1.5: What is the color of the pink frosted
donut? \Answer: Pink
LOVA3(Ours):
Question: What is the donut on the left side of
the image called?
A. apple fritter B. long john C. eclair D. cruller
Answer: D

Figure 1: Comparison of three abilities reveals that LLaVA1.5 excels in providing answers but struggles in asking accurate questions and assessing question-answer pairs.

asking, and assessing abilities. Among them, the answering ability is necessary to handle QA-like tasks; the asking ability allows AI models to interact with humans or other AI models for necessary information; and the evaluation ability assesses the solution candidates given by humans or other models with many applications, e.g., the filtering synthetic data.

Although innate to humans, apart from answering ability, two other learning mechanisms of asking and assessment remain formidable challenges for contemporary multimodal large language models (MLLMs). Even though some advanced MLLMs [45, 90, 4, 101] have even achieved remarkable proficiency in handling multimodal questions pertaining to mathematics [5], science [45], and commonsense knowledge [19]. However, the focus of most MLLMs [45, 19, 4, 90] predominantly revolves around visual question answering (VQA) tasks. As a result, as shown in Fig. 1, current MLLMs, e.g., the representative LLaVA-1.5 [45], suffer from inferior performance on asking questions and self-assess question-answer pairs (QA), which underscores their efficacy as problem-solvers and prohibits profound multimodal understanding.

To address these challenges, we introduce two essential tasks: GenQA and EvalQA, aiming at bolstering the intelligence and robustness of MLLMs. GenQA focuses on enabling the model to generate diverse question-answer (QA) pairs for images, thus equipping the MLLM with the capability to ask questions. We believe that if an MLLM can successfully generate QA pairs for challenging tasks, it indicates a higher level of problem-solving ability. Specifically, we define the GenQA task to include not only generic VQA (e.g., VQAv2 [29] and GQA [33]) but also Multi-Choice VQA (MC VQA), and Multi-Turn VQA (MT) to increase the variety of data formats. Additionally, we incorporate two challenging multimodal grounding tasks into the training process: Referring Expression Comprehension (REC) and Referring Expression Generation (REG). Learning to generate the data of these grounding tasks forces the MLLM to extract fine-grained visual cues from images, such as explicit object localization and compositional relationships. This, in turn, enhances the multimodal reasoning ability of MLLMs. During training, we gather the relevant datasets for these tasks and transform them into a generative format using our proposed instruction template. EvalQA, on the other hand, involves tasking the MLLM to predict the correctness of a given visual-questionanswer triplet. Recognizing the absence of datasets specifically designed to assess VQA quality, we have developed a new benchmark called EvalQABench for evaluating VQA data. Rather than asking humans to label such a dataset, we propose a new automatic pipeline for data construction. This benchmark comprises training, validation, and test sets, with each VQA pair accompanied by a "Yes" or "No" label indicating correctness, along with a one-sentence explanation as the feedback. For instance, "Yes, the oranges are not in a bag".

By integrating the GenQA and EvalQA tasks into the vanilla multimodal learning, we develop an effective training framework called LOVA ${ }^{3}$. In this study, we select the SOTA MLLM LLaVA-1.5 as the backbone model for evaluation. We conduct experiments on 10 widely used multimodal benchmarks such as GQA [33], VQAv2 [29], Vizwiz [30], MME [21], MMBench [48], and MMvet [98], and observe consistent improvements across these benchmarks. To summarize, our proposed LOVA $^{3}$ is a new framework that endows the MLLM with the ability to ask and assess and finally achieve profound multimodal understanding capability. Overall, our contributions are three folds:

(1) To the best of our knowledge, $\mathbf{L O V A}^{3}$ is the first effort to imbue the asking and assessment abilities in training a robust and intelligent MLLM. LOVA $^{3}$ open an avenue for imitating the human abilities towards holistic intelligence for MLLM.

(2) We build a new benchmark EvalQABench for the VQA evaluation as the first effort to advance the development of future research.

(3) The experimental results demonstrate that training with LOVA $^{3}$ consistently improves performance across several multimodal benchmarks, including MME, VizWiz, and MMBench.

## 2 Related Work

### 2.1 Multimodal Large Language Models

Large Language Models (LLMs) [18, 104, 70, 7, 87, 17, 88] such as GPT-4 [66] demonstrate their exceptional capacity to handle a wide range of complex tasks to play an important role in assisting humans in daily life. Equipped with these LLMs, a surge of multimodal modes [41, 46, 19, 4, $12,90,108,11,10,55,91,32,67,63,94,25,50,31,38,9,106,51,16,42,27,73,26,71]$ are proposed to integrate the visual information with the pre-trained LLM decoder for diverse multimodal reasoning tasks such as image captioning [14, 1, 97] and visual question answering [29, 59, 33, 30]. LLaVA [46, 45] is a pioneering approach that proposes to project the CLIP [69] visual features to the language embedding space and showcase promising performance on various multimodal benchmarks. Unlike the architecture of LLaVA, InstructBLIP [19] employs an instruction-aware feature extractor and obtains advanced performance on various tasks. Besides focusing on traditional vision-language tasks, Shikra [12], Kosmos-2 [67], PVIT [10], Ferret [96] pay attention to the image-region based multimodal tasks (i.e., Referring Expression Comprehension). By adopting a large-scale image-text corpus for instruction tuning, Qwen-VL [4], CogVLM [90], AnyMAL [63] and Chameleon [85] achieve exceptional performance on various multimodal tasks. However, these MLLMs primarily concentrate on training the model to answer questions as effectively as possible, neglecting the significance of enabling the model to act as a questioner and a competent evaluator within the training paradigm.

### 2.2 Visual Question Answering and Generation

Nine years ago, visual question answer [3] was defined and became an essential task for evaluating multimodal systems. A surge of VQA-related benchmarks [3, 29, 82, 61, 60, 30, 76, 59, 54, 33, 43] are emerged to advance the development of this research area, including generic VQA benchmarks [3, 29, 33], text-based VQA [82, 61, 60, 84], knowledge-augmented VQA [59, 76, 54, 15], and benchmark [30] aimed at assisting blind people. Simutaneously, various studies [56, 80, 52, 23, 107, $6,2,35]$ are proposed with diverse model architecture such as LSTMs[80], attention network[35, 2] or convolution network $[52,107]$.

Besides the VQA task, the Visual Question Generation (VQG) task was first formulated in [65], which contributes a VQG dataset with each image annotated with multi-questions and benchmarking on generative models and retrieval models. [103] first employs an RNN-based encoder-decoder framework alongside model-generated captions to generate questions. After that a list of works [64, $65,20,36,77,93,89$ ] are proposed for promoting this research area. Two interesting studies [44, 78] pose that treating VQG as a complementary task can enhance the robustness of visual question answering. This finding reaffirms our motivation that training a model to generate diverse questions contributes to a deeper understanding of visual information, thereby improving its problem-solving capabilities.

As for assessing the quality of VQA triplets, to the best of our knowledge, there are no efforts to do that. Therefore, we propose a new task EvalQA for the model training tailored with a new benchmark EvalQABench to advance this research area.

### 2.3 Multimodal Benchmarks

Traditional multimodal benchmarks focus on answering ability, such as visual question answering [29], image captioning $[14,68,1]$, as well as other benchmarks for specialized scenarios such as scene text understanding [82, 81], commonsense reasoning [100], outside knowledge [59, 76]. The recent development of MLLM posts a strong need for modernized multimodal benchmarks [21, 48, 40, $98,28,99,53,22,13,102,47,105,83,86]$ such as MME [21], MMBench [48], SEED-Bench [40] which involve comprehensively evaluating current MLLMs on various multimodal abilities.

Unlike existing multimodal benchmarks focusing primarily on evaluating the model's ability to answer, we introduce EvalQABench, a benchmark designed to evaluate the quality of VQA pairs, each with a binary "Yes/No" annotation. Furthermore, recognizing the lack of emphasis on providing feedback for incorrect answers in current benchmarks, we develop an LLM-based pipeline. This pipeline can automatically generate feedback, paving the way for enhanced automated data processing in the future.

Table 1: Data taxonomy of GenQA, detailing the data type, name, size, and instruction prompts of each dataset.

| Data Type | Dataset | Size | Instruction Prompts |
| :---: | :---: | :---: | :---: |
| Generic VQA | VQAv2 [29] | $100 \mathrm{~K}$ | Note: randomly choose from 58 instruction prompts |
|  | GQA [33] | $100 \mathrm{~K}$ | Example: Can you provide a clear question and its answer based on the image? |
|  | OCR-VQA [61] | $80 \mathrm{~K}$ |  |
|  | Counting $20 \mathrm{~K}^{\dagger}$ | $20 \mathrm{~K}$ |  |
|  | LLaVA- $250 \mathrm{~K}^{\dagger}[46]$ | $250 \mathrm{~K}$ |  |
| Multi-choice VQA | A-OKVQA [76] | $17 \mathrm{~K}$ | Can you provide a clear question and its answer based on the image? nThis is a Multi-choice VQA <br> task. |
| Multi-turn VQA | VQAv2 [29] | $83 \mathrm{~K}$ | Design a conversation between you and a person asking about this photo. |
|  | GQA [33] | $72 \mathrm{~K}$ | The answers should be in a tone that a visual AI assistant is seeing the image and answering the <br> question. Ask diverse questions and give corresponding answers. |
| REC | VG [37] | $30 \mathrm{~K}$ | Note: randomly choose from 58 instruction prompts with a specific task description prompt. <br> Can you review the image and articulate a concise question and its answer? \nThis is a Referring |
|  | RefCOCO [34] | $30 \mathrm{~K}$ | Expression Comprehension (REC) task. The question will express a specific region of the image. <br> Please provide the coordinates in the answer. |
| REG | VG [37] | $30 \mathrm{~K}$ | Note: randomly choose from 58 instruction prompts with a specific task description prompt. <br> Can you review the image and articulate a concise question and its answer? \nThis is a Referring |
|  | RefCOCO [34] | $30 \mathrm{~K}$ | Expression Generation (REG) task. The purpose of REG is to generate a unique description for a <br> specified location. |
| Total | - | $842 \mathrm{~K}$ |  |

## 3 Methodology

In this section, we introduce $\mathrm{LOVA}^{3}$, a new framework designed to imitate two essential abilities asking and assessment - within multimodal learning. We delve into the specifics of addressing this challenge through GenQA data collection, EvalQA data creation, model architecture, and training.

### 3.1 Data Collection for GenQA

If one MLLM is able to successfully generate high-quality question-answer pairs based on visual input, it indicates a stronger problem-solving ability and deep visual understanding. To enable the MLLM to ask questions, it is natural for us to gather existing annotated datasets as the training corpus and then train the model to predict both questions and answers. We carefully define five main multimodal data types as listed in Tab. 1. For each data type, we gather widely used humanannotated datasets or high-quality instruction tuning datasets generated by GPT-4. We select Generic VQA tasks to generate fundamental questions, e.g., object count and object action. We incorporate Multi-choice VQA (MC VQA) and Multi-turn VQA (MT) to increase the diversity of data formats. Additionally, we include two multimodal grounding tasks: Referring Expression Comprehension (REC) and Referring Expression Generation (REG). Generating REC and REG data requires a deeper understanding of image content, enabling the model to fully comprehend visual cues. Both tasks increase the difficulty of GenQA, which helps MLLM acquire a higher level of multimodal understanding. In total, we gather $842 \mathrm{~K}$ data for training questioning ability.

### 3.2 Data Creation for EvalQA

Completing the VQA assessment often requires fine-grained and deep visual understanding. As emphasized in Sec. 1, the ability to assess is often overlooked yet crucial in MLLM training. To address this gap, we introduce a new benchmark, EvalQABench, to address the problem of assessing visual question-answering data. Moreover, instead of merely labeling each VQA pair with "Yes/No", we advocate for integrating feedback into each instance, an important aspect rarely seen in prior multimodal benchmarks. We consider training the model not only to assess the correctness of the answer but also to provide reasonable feedback that would increase the capability for multimodal understanding. EvalQABench comprises three datasets: training, validation, and test sets. As illustrated in Tab. 2, we present examples of the training set from EvalQABench across various question types.

MLLM-based Negative Answer Generation. The main challenge of EvalQABench lies in constructing negative answers. When dealing with large-scale ground-truth VQA pairs, how can we automatically produce the negative answer? One viable solution is to leverage a multimodal model for this purpose. Recognizing that Fuyu-8B [5] is an open-source free MLLM that stands out with the exceptional ability to process high-resolution images and perform robust well on many complex tasks, rivaling the capabilities of GPT-4V. We utilize it to generate negative answers with the following prompt:

Table 2: Selected examples from EvalQABench training set, including the ground truth answer, negative answer, and feedback.

| Question Types | Object | Yes/No | Counting |
| :---: | :---: | :---: | :---: |
| Image |  |  |  |
| Question | What kind of flowers are on <br> the picture to the left? | Is the sun shining? | How many vases are there? |
| Ground-truth Answer | roses | no | 6 |
| Negative Answer | pansy | yes | 5 |
| Feedback | No, the left of the picture <br> shows roses. | No, the sun is not shining. | No, there are 6 vases in the <br> picture. |
| Question Types | Color | Attribute | Number |
| Image |  |  |  |
| Question | What color is the truck? | What type of tree is on the <br> right? | What number is written on <br> the sheep? |
| Ground-truth Answer | silver | cherry | 3 |
| Negative Answer | white | palm | 5 |
| Feedback | No, the truck is silver. | No, the tree on the right is a <br> cherry tree. | The number written on the <br> sheep is 3 . |
| Question Types | Relation | Action | Other |
| Image |  |  |  |
| Question | What does the woman have <br> on her back? | What are the people doing? | What does the second sign <br> say? |
| Ground-truth Answer | backpack | motorcycling | all-war |
| Negative Answer | jacket | riding bikes | stop |
| Feedback | No, the woman has a <br> backpack on her back. | No, the people in the picture <br> are motorcycling. | No, the second sign says <br> "all-war" |

<Img> This is the question: <Q>. Please give me the wrong answer to this question. The answer should be a single word or phrase. $\backslash \mathrm{n}$

Here, <Img> and <Q> are two placeholders for the image and question from ground truth VQA pair. The output of Fuyu-8B provides a negative answer, such as "pansy", as illustrated in Fig. 2.

Manual Filtering and Error Correction. Acknowledging that the Fuyu-8B model is not flawless and recognizing that no multimodal model, including GPT-4V, is perfect, we have implemented both manual filtering and error corrections, as illustrated in Fig. 3 for the post-data processing. Through empirical analysis, we identified 4 primary types of errors. For instance, an answer generated by Fuyu-8B may be present in the question but lacks semantic relevance or is identical to the correct answer. Additionally, some incorrect answers may result from misunderstanding the question's category, as exemplified by the example in Fig. 3. Beyond filtering, we propose error corrections for

![](https://cdn.mathpix.com/cropped/2024_06_04_16f0e7734de6a63860b3g-06.jpg?height=301&width=1393&top_left_y=191&top_left_x=363)

Figure 2: Illustration of the proposed pipeline for generating negative answers and feedback.

two types of questions: "Yes/No" and "Counting". For "Yes/No" questions, we directly substitute an incorrect answer with "Yes". For "Counting" questions, we first verify if the English numeral matches the correct answer; if not, we replace it with a random number. After applying the above filtering and correction processes, we found that most of the incorrect samples had been removed.

LLM-based Feedback Generation. With the candidate's negative answer, we then focus on generating error feedback. We consider the feedback describing the reason for incorrectness will help the MLLM obtain a deeper understanding. We thus utilize the SOTA LLM Llama 2 [88] to generate the feedback by reasoning the ground truth question-answer pairs with the following prompt:

Please rephrase the question and answer: $<\mathrm{Q}>\backslash \mathrm{n}<\mathrm{A}>$ into one short description.

After processing by Llama 2, we can get feedback like "No, the left of the picture shows roses.". Moreover, we use similar manual filtering strategies that are used in the negative answer generation step to remove the noisy samples with wrong formats or empty output.

In summary, we start by randomly selecting 100,000 samples from 443,758 annotated VQA pairs in the VQAv2 training set [29] to generate negative answers. After manual filtering, this number is reduced to 61,094 samples. We then generate feedback for each sample and further filter out those with incorrect formats, resulting in a final set of 41,592 samples. For the training set of EvalQABench, we create a one-positive-one-negative format by randomly selecting 32,000 negative samples from the 41,592 filtered samples, yielding a total of 64,000 training data points. For the validation and test subsets, we follow a similar sampling procedure. We randomly select 100,000 samples from the VQAv2 validation set, resulting in 41,907 negative samples. From these, we randomly select 2,500 negative samples each for the validation set and the test set.

### 3.3 Model Architecture

In this subsection, we introduce the model architecture of $\mathrm{LOVA}^{3}$. This model is built upon the prevalent MLLM LLaVA-1.5 [45] with three key components: Vision Encoder, MLP Adapter, and Large Language Model. For the vision encoder, we follow LLaVA-1.5 [45] and implement it with a pre-trained CLIP-Large vision encoder [69] with resolution $336 \times 336$. For the large language model, we adopt the widely used instruction fine-tuned model, Vicuna-7B [17]. Following [45], the MLP adapter is a simple two-layer MLP since such a simple design is better for reserving the visual information while achieving running efficiency. In this study, we leverage LLaVA-1.5 to build upon because of its exceptional performance and highly reproducible training and validating codes. Other outstanding MLLMs, such as CogVLM [90] and Qwen-VL [4], are pre-trained on billions-scale datasets or in-house datasets. This scale of data makes the training process difficult to replicate and poses challenges in incorporating our proposed training tasks, GenQA and EvalQA.

### 3.4 Training

For brevity, we denote the $\mathrm{LOVA}^{3}$ model as $F_{M}$. Given an image $X_{I}$, our target is to enforce $F_{M}$ to generate the response $X_{R}$ :

$$
\begin{equation*}
X_{R}=F_{M}\left(X_{T}, X_{I}\right) \tag{1}
\end{equation*}
$$

where $X_{T}$ represents the input text. $X_{T}$ can be an example of the three types: 1) VQA data, e.g., "What color is the pot?"; 2) GenQA data like "Can you provide a concise question and answer based on the image?"; 3) EvalQA data, such as "What kind of flowers are on the picture to the left? $\backslash n$ Answer: pansy. $\backslash$ nPlease examine the correctness of this question and answer according to
![](https://cdn.mathpix.com/cropped/2024_06_04_16f0e7734de6a63860b3g-07.jpg?height=312&width=644&top_left_y=191&top_left_x=360)

Different answer type "question": "Is this a little boy's or little girl's room?",
"answer": "boy"

"fuyu_answer": "no"

Number representation issue "question": "How many sinks?", "answer": "2",

![](https://cdn.mathpix.com/cropped/2024_06_04_16f0e7734de6a63860b3g-07.jpg?height=300&width=166&top_left_y=197&top_left_x=1294)

Manual Correction

Convert the text to yes/no "question": "Are these buildings less than 10 years old?", "answer": "no",

"fuyu_answer": "new" -> "yes"

Convert text to number "question": "What number is on the girls shirt?",

"fuyu answer": "two " -> "2"

Figure 3: Examples from the manual filtering and error correction process. Red text indicates error answers, while Green text represents manually corrected answers.

the image content. Output Yes or No with the feedback". Accordingly, the input instruction template can be unified into the following ones:

<s>USER: $X_{I} X_{T} \backslash \mathrm{n}$ ASSISTANT: $X_{R}</ \mathrm{s}>$

We follow previous MLLMs[45, 19], and design the training objective in an autoregressive manner:

$$
\begin{equation*}
\max \sum_{i=i}^{L} \log p\left(X_{R} \mid X_{T}, X_{I}\right)=\prod_{i}^{L} p_{\theta}\left(x_{i} \mid X_{T}, X_{I}, X_{R,<i}\right) \tag{2}
\end{equation*}
$$

where $x_{i}$ is the current prediction token and $L$ denotes the response sequence length. $\theta$ denotes the trainable parameters.

## 4 Experiments

### 4.1 Datasets and Settings

Training Datasets. For the fair comparison, we utilize the $665 \mathrm{~K}$ instruction-following dataset introduced in LLaVA1.5, combined with the $842 \mathrm{~K}$ GenQA data as outlined in Tab. 1, and an additional $64 \mathrm{~K}$ data comprising one-positive-one-negative pairs as described in Section 3.2, totaling our training datasets. It is important to note that the datasets and annotations used in both VQA and GenQA are the same. There are no additional datasets involved, thus avoiding unfair comparisons caused by the introduction of new instruction data. For EvalQA, we adopt VQAv2 to build the training set, which is already included in the original $665 \mathrm{~K}$ instruction dataset.

Validation Datasets. We assess LOVA 3 on 10 widely used multimodal datasets and benchmarks. (1) VQAv2 [29] and GQA [33] are two large-scale annotated VQA datasets comprising 430K and 943K instances. (2) VizWiz [30] is a challenging dataset comprising 8000 instances of test-dev set. Most of the images in this dataset are blurred, making it difficult to respond. (3) ScienceQA [54] is a benchmark comprising $21 \mathrm{k}$ multimodal multiple-choice questions with diverse science topics. (4) POPE [43] is a benchmark for evaluating the object hallucination in the MLLM. (5) MME [21], SEED-Bench [40], MMBench [48], LLaVA-Bench [46], MM-Vet [98] are five prominent multimodal benchmarks designed to evaluate various capabilities of MLLMs, including object existence, color recognition, counting, OCR, etc.

Competitors. We compare LOVA ${ }^{3}$ with other SOTA models inlcuding MiniGPT-4 [108], BLIP2 [41], InstructBLIP [19], mPLUG-owl [95], LLaMA-AdapterV2 [25] and LLaVA-1.5 [45]. We report the results from their paper or the benchmark leaderboard.

Implementation Details. To ensure a fair comparison, we train LOVA $^{3}$ model without tuning any hyperparameters of LLaVA-1.5 [45] from its original supervised finetuing stage. The model is trained for one epoch across three tasks: VQA, GenQA, and EvalQA. Specifically, we employ the AdamW [49] optimizer with a learning rate of $2 \times 10^{-5}$ and a total batch size of 128. The training process takes 24.5 hours on an 8 Nvidia A100 (40G) GPU setup.

### 4.2 Main Results

Generic tasks. As shown in Tab. 3, LOVA 3 outperforms LLaVA1.5 across all five datasets and obtains $3.6 \%$ improvement on VizWiz dataset, $1.3 \%$ improvement on GQA, $1.8 \%$ improvement on VQAv2 (1,932 samples are correctly predicted), and $1.2 \%$ improvement on ScienceQA. As for the

Table 3: Results on five generic tasks including VQAv2 [29], GQA [33], VizWiz [30], ScienceQA [54], and POPE [43]. The first two columns represent the results on held-in datasets marked as *, and the last three columns represent the held-out datasets. The best result on each subtask is bolded.

| Method | Train Paradigm | LLM | VQAv2 <br> test-dev | GQA <br> test | VizWiz <br> test-dev | ScienceQA <br> img | POPE <br> avg |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| BLIP-2 [41] | $\overline{V Q A}$ | Vicuna-13B | 41.0 | 41.3 | 19.6 | 61.0 | 85.3 |
| InstructBLIP [19] | $V Q A, V Q G$ | Vicuna-7B | - | 49.2 | 34.5 | 60.5 | - |
| InstructBLIP [19] | $V Q A, V Q G$ | Vicuna-13B | - | 49.5 | 33.4 | 63.1 | 78.9 |
| IDEFICS-9B [39] | $V Q A$ | LlamA-7B | 50.9 | 38.4 | 35.5 | 44.2 | _ |
| Qwen-VL [4] | $V Q A$ | Qwen-7B | $78.8^{*}$ | $59.3^{*}$ | 35.2 | 67.1 | - |
| LLaVA | $V Q A$ | Vicuna-7B | $78.5^{*}$ | $62.0^{*}$ | 50.0 | 66.8 | 85.9 |
| $\mathrm{LOVA}^{3}$ (ours) | $V Q A$, GenQA, EvalQA | Vicuna-7B | $\mathbf{8 0 . 3} \mathbf{3}_{+1.8}^{*}$ | $\mathbf{6 3 . 3} \mathbf{3}_{+1.3}^{*}$ | $\mathbf{5 3 . 6}_{+3.6}$ | $\mathbf{6 8 . 0 _ { + 1 . 2 }}$ | $\mathbf{8 7 . 4} 4_{+1.5}$ |

Table 4: Results on multimodal benchmarks, including MME [21] and SEED-Bench [40], MMBench [48] and LLava-Bench [46]

| Method | Train Paradigm | LLM | $\mathrm{MME}$ | SEED-Bench <br> Image | MMBench |  | LLaVA-Bench <br> All |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | En | $\mathrm{Cn}$ |  |
| BLIP-2 [41] | $V Q A$ | Vicuna-13B | 1293.8 | 49.7 | - | - | 38.1 |
| InstructBLIP [19] | $V Q A, V Q G$ | Vicuna-7B | _ | 58.8 | 36.0 | 23.7 | 60.9 |
| InstructBLIP [19] | $V Q A, V Q G$ | Vicuna-13B | 1212.8 | - | _ | - | 58.2 |
| mPLUG-owl [95] | $V Q A$ | Llama-7B | 967.3 | 37.9 | _ | _ | - |
| LLaMA-AdapterV2 [25] | $V Q A$ | Llama-7B | 972.7 | 35.2 | 41.0 | - | - |
| LLaVA-1.5 [45] | $V Q A$ | Vicuna-7B | 1510.7 | 66.2 | 64.3 | 58.3 | 64.0 |
| $\mathrm{LOVA}^{3}$ (ours) | $V Q A$, GenQA, EvalQA | Vicuna-7B | $1552.7_{+42.0}$ | $\mathbf{6 7 . 1}+0.9$ | $66.8_{+2.5}$ | $\mathbf{6 0 . 5}+2.2$ | $\mathbf{6 8 . 3} 3_{+4.3}$ |

object hallucination benchmarks, our model attains $87.4 \%$ accuracy at an average of its three subsets. Remarkably, these enhancements in VQAv2 and GQA performance are achieved without any extra datasets, underscoring the significant impact of integrating GenQA and EvalQA into our training to promote performance improvements on these generic VQA tasks.

MME, SEED-Bench, MMBench, LLaVA-Bench. In Tab. 4, we evaluate four prevalent multimodal benchmarks, where our LOVA ${ }^{3}$ surpass LLaVA1.5 with $42.0 \%$ on MME benchmark, $0.9 \%$ increase in accuracy on SEED-Bench, $2.5 \%$ on MMBench (En), $2.2 \%$ MMBench (Cn) and $4.3 \%$ on LLaVA-Bench. Such results showcase enhanced multimodal reasoning capabilities for complex tasks compared to vanilla LLaVA1.5, which is solely trained with VQA tasks.

MM-Vet. In Tab. 5, we compare LOVA ${ }^{3}$ with other approaches on MM-Vet, which is a challenging benchmark including numerous complex VQA samples that demand integration of several multimodal capabilities for answering. As illustrated in Tab. 5, the results show that our LOVA $^{3}$ outperforms LLaVA-1.5 by $4.0 \%$ at an average. Such improvement demonstrates the effectiveness of LOVA $^{3}$ in solving these challenging multimodal questions.

### 4.3 Ablation Study

We split the data used in the GenQA task into two groups: GenQA-General and GenQA-Grounding. The findings, presented in Tab. 6, are instrumental in investigating the contributions of GenQA and EvalQA to model efficacy. (1) Comparing the first four rows, one can find that both GenQA-General and EvalQA data are more effective in improving performance than GenQA-Grounding. (2) By comparing rows 4 and 7, it demonstrates the effectiveness of EvalQA across five datasets, especially on MME. (3) When comparing rows 6 and 7, by removing GenQA-General from the finetuning corpus, the performance drops significantly on MME and VizWiz. (4) Compare the rows 0 and 3, one can observe that even adding $64 \mathrm{~K}$ data into the training, there are obvious improvements in GQA, ScienceQA, and MME. By analyzing the data size, we did not introduce any new datasets for training the GenQA task. For EvalQA, we only added $32 \mathrm{~K}$ new negative answer annotations while retaining the original questions used for training VQA capabilities. The details of the data size are provided in the right column in Tab. 6.

### 4.4 Benchmark of EvalQABench

We report the evaluation results on our EvalQABench test set in Tab. 7 to validate the EvalQA ability of current SOTA models and LOVA 3 . We select BLIP2 [41], InstructBLIP [19], CogVLM [90], QwenVL-Chat [4], InternLM-XC [101], and LLaVA1.5 [45] for the comparison. We ask these models to answer "Yes" or "No" strictly and record the results for calculating the Accuracy, Precision, F1 score, and No (\%) metrics. Here, No (\%) indicates the percentage of results classified as "No," which ideally

Table 5: Multimodal reasoning ability on MM-Vet [98]. Rec denotes Recognition; Know denotes knowledge; Gen denotes Language generation; and Spat denotes Spatial awareness.

| Method | Train Paradigm | LLM | $\operatorname{Rec}$ | OCR | Know | Gen | Spat | Total |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| MiniGPT-4 [108] | $V Q A$ | Vicuna-7B | 27.4 | 15.0 | 12.8 | 13.9 | 20.3 | 22.1 |
| BLIP-2 [41] | $V Q A$ | Vicuna-13B | 27.5 | 11.1 | 11.8 | 7.0 | 16.2 | 22.1 |
| InstructBLIP [19] | $V Q A, V Q G$ | Vicuna-7B | 32.4 | 14.6 | 16.5 | 18.2 | 18.6 | 26.2 |
| InstructBLIP [19] | $V Q A, V Q G$ | Vicuna-13B | 30.8 | 16.0 | 9.8 | 9.0 | 21.1 | 25.6 |
| LLaVA-1.5 [45] | $V Q A$ | Vicuna-7B | 37.0 | 21.0 | 17.6 | 20.4 | 24.9 | 31.2 |
| LOVA $^{3}$ (ours) | $V Q A$, GenQA, EvalQA | Vicuna-7B | $\mathbf{4 1 . 5}_{+4.5}$ | 23.6 $6_{+2.6}$ | $\mathbf{2 3 . 9}_{+6.3}$ | $\mathbf{2 4 . 6} \mathbf{6}_{+4.2}$ | $\mathbf{3 0 . 3}_{+5.4}$ | $\mathbf{3 5 . 2}+4.0 \quad$ |

Table 6: Abaltion studies on different finetuning datasets.

| Row | Fin <br> GenQA-Generic | etuning Corpus <br> GenQA-Grounding | EvalQA | GQA | VizWiz | ScienceQA | POPE | MME | Size |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 0 | $\mathrm{LLaV}$ | JA-1.5 (Baseline) |  | 62.0 | 50.0 | 66.8 | 85.9 | 1510.7 | $665 \mathrm{~K}$ |
| 1 | $\checkmark$ |  |  | 63.1 | 53.1 | 67.4 | 86.9 | 1550.7 | $722 \mathrm{~K}$ |
| 2 |  | $\sqrt{ }$ |  | 62.8 | 50.9 | 66.4 | 86.6 | 1495.8 | $120 \mathrm{~K}$ |
| 3 |  |  | $\checkmark$ | 62.8 | 49.1 | 67.8 | 87.0 | 1535.6 | $64 \mathrm{~K}$ |
| 4 | $\sqrt{ }$ | $\checkmark$ |  | 63.3 | 53.2 | 67.4 | 86.7 | 1523.6 | $842 \mathrm{~K}$ |
| 5 | $\checkmark$ |  | $\sqrt{ }$ | 63.7 | 54.4 | 67.0 | 86.9 | 1520.8 | $786 \mathrm{~K}$ |
| 6 |  | $\checkmark$ | $\checkmark$ | 63.1 | 51.1 | 67.5 | 86.8 | 1478.7 | $184 K$ |
| 1 | $\checkmark$ | $\checkmark$ | $\checkmark$ | 63.3 | 53.6 | 68.0 | 87.4 | 1552.7 | 906 |

Table 7: Results of multimodal large language models on the test set of EvalQABench (ours).

| Method | LLM | Test Set |  |  |  |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: |
|  |  | Accuracy | Precision | F1 Score | No (\%) |  |
| Vision Language Pretraining Model |  |  |  |  |  |  |
| BLIP2 [41] | Flan-T5-XXL-11B | 58.00 | 82.79 | 32.47 | 87.80 |  |
| Multimodal Large Language Models |  |  |  |  |  |  |
| InstructBLIP [19] | Vicuna-7B | 38.04 | 41.49 | 48.47 | 29.76 |  |
| InstructBLIP [19] | Vicuna-13B | 61.42 | 57.60 | 69.18 | 24.82 |  |
| CogVLM [90] | Vicuna-7B | 60.64 | 56.59 | 69.88 | 19.32 |  |
| Qwen-VL-Chat [4] | Qwen-7B | 63.66 | 63.48 | 63.90 | 49.34 |  |
| InternLM-XC [101] | InternLM-7B | 69.58 | 70.66 | 68.76 | 52.62 |  |
| LLaVA-1.5 [45] | Vicuna-7B | 64.92 | 61.28 | 69.80 | 33.84 |  |
| LOVA 3 (ours) | Vicuna-7B | $\mathbf{7 9 . 5 8}_{+14.66}$ | $\mathbf{7 9 . 1 5}_{+17.87}$ | $\mathbf{7 9 . 7 2}_{+9.92}$ | 49.26 |  |

should approximate $50 \%$ due to the one-positive-one-negative setting utilized in our test set. As indicated by the data presented in the table, BLIP2 predominantly yields "No" responses across most test instances. Among the state-of-the-art MLLMs, InternLM-XC stands out by delivering superior performance on these four metrics. Trained with EvalQA data, LOVA ${ }^{3}$ shows several improvements over our baseline LLaVA1.5 by margins of $14.66 \%, 17.87 \%$, and $9.92 \%$ in Accuracy, Precision, and F1 Score, respectively.

## 5 Conclusion and Limitations

In this work, we propose a novel multimodal framework, $\mathbf{L O V A}^{3}$, which is capable of mimicking the human visual question answering, asking, and assessment to achieve deeper multimodal understanding. We introduce two additional training tasks, GenQA and EvalQA, to help MLLM acquire these abilities. We establish EvalQABench, a novel benchmark to assess the VQA samples between multiple MLLMs. Experimental results show that LOVA $^{3}$ achieves superior performance across various benchmarks, including MM-Vet, SEED, and VizWiz, demonstrating the effectiveness of the two additional abilities.

Limitations. (1) Due to computational constraints, we do not test larger LLMs, such as the 13B or 34B variants. However, we believe that our LOVA $^{3}$ could be beneficial for larger LLMs, as other MLLMs have shown performance improvements with increased LLM scale. (2) GenQA and EvalQA as two additional tasks increase training costs, but it is inevitable for an MLLM to acquire new capabilities. (3) Due to the limited scope of instruction tuning datasets, $\mathrm{LOVA}^{3}$ cannot address domain-specific multimodal tasks well, such as text-centric VQA or mathematic-relevant VQA.

## References

[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, 2019. 3

[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077-6086, 2018. 3

[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015. 3

[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 2, 3, 6, 8, 9

[5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. Introducing our multimodal models, 2023. 2, 4

[6] Hedi Ben-Younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome. Mutan: Multimodal tucker fusion for visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2612-2620, 2017. 3

[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 3

[8] Georg Cantor. Über unendliche, lineare Punktmannigfaltigkeiten: Arbeiten zur Mengenlehre aus den Jahren 1872-1884, volume 2. Springer-Verlag, 2013. 1

[9] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Localityenhanced projector for multimodal llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3

[10] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Position-enhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. 3

[11] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. 3

[12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv:2306.15195, 2023. 3, 18

[13] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024. 3

[14] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv:1504.00325, 2015. 3

[15] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and MingWei Chang. Can pre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713, 2023. 3

[16] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024. 3

[17] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. 3, 6

[18] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 3

[19] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. 2023. 2, 3, 7, 8, 9

[20] Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, and Xuan-Jing Huang. A reinforcement learning framework for natural question generation using bi-discriminators. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1763-1774, 2018. 3

[21] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. 2, 3, 7, 8

[22] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 3

[23] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint arXiv:1606.01847, 2016. 3

[24] Richard Gale. Asking questions that matter... asking questions of value. International Journal for the Scholarship of teaching and learning, 3(2):3, 2009. 1

[25] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv:2304.15010, 2023. 3, 7, 8

[26] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, and Yu Qiao. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models, 2024. 3

[27] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, and Rongrong Ji. Cantor: Inspiring multimodal chain-of-thought of mllm, 2024. 3

[28] Brian Gordon, Yonatan Bitton, Yonatan Shafir, Roopal Garg, Xi Chen, Dani Lischinski, Daniel Cohen-Or, and Idan Szpektor. Mismatch quest: Visual and textual feedback for image-text misalignment. arXiv preprint arXiv:2312.03766, 2023. 3

[29] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 1, 2, 3, 4, 6, 7, 8, 17, 18, 19

[30] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 1, 2, 3, 7, 8

[31] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530, 2024. 3

[32] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv:2302.14045, 2023. 3

[33] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 1, 2, 3, 4, 7, 8, 17, 18

[34] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 4, 18

[35] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. Advances in neural information processing systems, 31, 2018. 3

[36] Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Information maximizing visual question generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2008-2018, 2019. 3

[37] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 4, 18

[38] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. 2024. 3

[39] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 8

[40] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023. 3, 7, 8, 18

[41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023. $3,7,8,9$

[42] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 3

[43] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv:2305.10355, 2023. 3, 7, 8

[44] Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou. Visual question generation as dual task of visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6116-6124, 2018. 3

[45] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 1, 2, 3, 6, 7, 8, 9, 17

[46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 3, 4, 7, 8, 18

[47] Mengchen Liu, Chongyan Chen, and Danna Gurari. An evaluation of gpt-4v and gemini in online vqa, 2024. 3

[48] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 2, 3, 7, 8, 18

[49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 7

[50] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 3

[51] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. 2024. 3

[52] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image coattention for visual question answering. Advances in neural information processing systems, 29, 2016. 3

[53] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 3

[54] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS, 2022. 3, 7, 8, 18

[55] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. 2023. 3

[56] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A neural-based approach to answering questions about images. In Proceedings of the IEEE international conference on computer vision, pages 1-9, 2015. 3

[57] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russakovsky. Point and ask: Incorporating pointing into visual question answering. arXiv preprint arXiv:2011.13681, 2020. 17

[58] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 18

[59] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In CVPR, 2019. 1, 3, 18

[60] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In WACV, 2021. 3

[61] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 3, 4, 17, 18

[62] Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens Van Der Maaten. Learning by asking questions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11-20, 2018. 1

[63] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and scalable any-modality augmented language model. arXiv preprint arXiv:2309.16058, 2023. 3

[64] Issey Masuda Mora, Santiago Pascual de la Puente, and X Giro-i Nieto. Towards automatic generation of question answer pairs from images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1-2, 2016. 3

[65] Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende. Generating natural questions about an image. arXiv preprint arXiv:1603.06059, 2016. 3

[66] OpenAI. Gpt-4 technical report, 2023. 3

[67] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824, 2023. 3

[68] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, J. Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. International Journal of Computer Vision, 123:74-93, 2015. 3

[69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 6

[70] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. 3

[71] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. 3

[72] Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, and Balaji Lakshminarayanan. Self-evaluation improves selective generation in large language models. arXiv preprint arXiv:2312.09300, 2023. 1

[73] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large language model for long video understanding. In CVPR, 2024. 3

[74] Azzurra Ruggeri and Tania Lombrozo. Learning by asking: How children ask questions to achieve efficient search. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 36, 2014. 1

[75] Claude Sammut and Ranan B Banerji. Learning concepts by asking questions. Machine learning: An artificial intelligence approach, 2:167-192, 1986. 1

[76] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022. 3, 4, 18

[77] Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano, and Patrick Gallinari. What bert sees: Cross-modal transfer for visual question generation. arXiv preprint arXiv:2002.10832, 2020. 3

[78] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-consistency for robust visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6649-6658, 2019. 3

[79] ShareGPT. https://sharegpt.com/, 2023. 18

[80] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4613-4621, 2016. 3

[81] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In ECCV, 2020. 3, 18

[82] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 3, 20

[83] Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context, 2024. 3

[84] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, and Can Huang. Textsquare: Scaling up text-centric visual instruction tuning, 2024. 3

[85] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. 3

[86] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. 3

[87] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. 3

[88] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3, 6

[89] Nihir Vedd, Zixu Wang, Marek Rei, Yishu Miao, and Lucia Specia. Guiding visual question generation. arXiv preprint arXiv:2110.08226, 2021. 3

[90] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. 2023. 2, 3, 6, 8, 9

[91] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175, 2023. 3

[92] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2024. 1

[93] Xing Xu, Tan Wang, Yang Yang, Alan Hanjalic, and Heng Tao Shen. Radial graph convolutional network for visual question generation. IEEE transactions on neural networks and learning systems, 32(4):1654-1667, 2020. 3

[94] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. $\operatorname{arXiv}: 2304.14178,2023$. 3

[95] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. 7, 8

[96] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In The Twelfth International Conference on Learning Representations, 2024. 3

[97] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. $A C L, 2014.3$

[98] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 2, 3, 7, 9, 20

[99] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 3, 18

[100] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720-6731, 2019. 3

[101] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 2, 8, 9

[102] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multimodal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 3

[103] Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, and Jiawan Zhang. Automatic generation of grounded visual questions. arXiv preprint arXiv:1612.06530, 2016. 3

[104] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv:2205.01068, 2022. 3

[105] Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, and Navdeep Jaitly. How far are we from intelligent visual deductive reasoning?, 2024. 3

[106] Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou. Genixer: Empowering multimodal large language models as a powerful data generator. arXiv preprint arXiv:2312.06731, 2023. 3

[107] Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Simple baseline for visual question answering. arXiv preprint arXiv:1512.02167, 2015. 3

[108] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv:2304.10592, 2023. $3,7,9$
