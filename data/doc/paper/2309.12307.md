# LONGLoRA: EfFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS 

Yukang Chen ${ }^{1} \quad$ Shengju Qian $^{1} \quad$ Haotian Tang $^{2} \quad$ Xin Lai $^{1}$<br>Zhijian Liu $^{2} \quad$ Song Han ${ }^{2,3} \quad$ Jiaya Jia $^{1}$<br>${ }^{1}$ CUHK $\quad{ }^{2}$ MIT $\quad{ }^{3}$ NVIDIA


#### Abstract

We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs $16 \times$ computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention ( $\mathrm{S}^{2}-\mathrm{Attn}$ ) effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with $\mathrm{S}^{2}$-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from $4 \mathrm{k}$ context to $100 \mathrm{k}$, or Llama2 70B to $32 \mathrm{k}$ on a single $8 \times \mathrm{A} 100$ machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset. All our code, models, dataset, and demo are available at github.com/dvlab-research/LongLoRA.


![](https://cdn.mathpix.com/cropped/2024_06_04_aef63bd28bd1ec2e644bg-01.jpg?height=320&width=922&top_left_y=1732&top_left_x=366)

$\rightarrow-$ Full FT $\rightarrow$ LoRA $\rightarrow$ LongLoRA

![](https://cdn.mathpix.com/cropped/2024_06_04_aef63bd28bd1ec2e644bg-01.jpg?height=304&width=460&top_left_y=1740&top_left_x=1296)

Figure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning, while still maintaining up to $1.8 \times$ lower memory cost than full fine-tuning. Furthermore, LongLoRA improves the training speed of LoRA by up to $1.8 \times$ with $S^{2}$-Attn. Llama2-7B are fine-tuned to various context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al. 2020) stage 2 and evaluated on the proof-pile (Azerbayev et al. 2022) test set in perplexity.

## 1 INTRODUCTION

Large language models (LLMs) are typically trained with a pre-defined context size, such as 2048 tokens for LLaMA (Touvron et al., 2023a) and 4096 tokens for Llama2 (Touvron et al., 2023b).
![](https://cdn.mathpix.com/cropped/2024_06_04_aef63bd28bd1ec2e644bg-02.jpg?height=432&width=1390&top_left_y=278&top_left_x=366)

Figure 2: Overview of LongLoRA. We introduce Shifted Sparse Attention ( $\mathrm{S}^{2}$-Attn) during finetuning. The trained model retains original standard self-attention at inference time. In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable. This extension is pivotal for context extension, and only introduces a minimal number of additional trainable parameters.

However, the pre-defined size limits LLMs in many applications, like summarizing long documents or answering long questions. To resolve this limitation, some recent works (Chen et al., 2023; Tworkowski et al., 2023, Mohtashami \& Jaggi, 2023) train or fine-tune LLMs to longer context. However, training an LLM from scratch with long sequences poses computational challenges, and fine-tuning an existing pre-trained LLM is also considerably expensive. For instance, Position Interpolation (Chen et al., 2023) spent 32 A100 GPUs to extend LLaMA models from $2 \mathrm{k}$ to $8 \mathrm{k}$ context, and 128 A100 GPUs for longer context fine-tuning. FOT (Tworkowski et al. 2023) used 32 TPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources are typically unaffordable for common researchers, which naturally leads us to question: can we extend the context window of LLMs efficiently?

One straightforward approach is to fine-tune a pre-trained LLM via low-rank adaptation (LoRA) $\sqrt{\mathrm{Hu}}$ et al. 2022). LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank matrices, which are generally efficient and reduce the number of trainable parameters. However, our empirical findings indicate that training long context models in this manner is neither sufficiently effective nor efficient. In terms of effectiveness, plain low-rank adaptation results in a high perplexity in long context extension, as in Table 2 . Increasing the rank to a higher value, e.g., rank $=256$, does not alleviate this issue. In terms of efficiency, regardless of whether LoRA is employed or not, computational cost increases dramatically as the context size expands, primarily due to the standard self-attention mechanism (Vaswani et al. 2017). As shown in Figure 1, even with LoRA, the training hours for the standard Llama2 model increase substantially when the context window expands.

In this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context windows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses low-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is also able to approximate long context during training. We present shifted sparse attention ( $\mathrm{S}^{2}-\mathrm{Attn}$ ) as an efficient substitute for standard self-attention. As shown in Figure 2, we split context length into several groups and conduct attention in each group individually. In half attention heads, we shift the tokens by half group size, which ensures the information flow between neighboring groups. For example, we use $S^{2}$-Attn with group size 2048 to approximate the total 8192 context length training. This shares a high-level spirit with Swin Transformer (Liu et al., 2021).

Models fine-tuned via $S^{2}$-Attn retain the original attention architecture during inference. This facilitates most existing optimization and infrastructure. Techniques for common LLMs can also be applied to ours. For example, Flash-Attention2 (Dao et al. 2022, Dao, 2023) is compatible with our method in both training and inference time. The reason behind this is that short attention resembles the attention scheme in the pre-training stage of LLMs. Other efficient attentions, e.g., dilated or sparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6

We empirically show that learnable embedding and normalization layers are the key to unlocking long context LoRA fine-tuning, in Table 2. Embedding and normalization layers take up a small

![](https://cdn.mathpix.com/cropped/2024_06_04_aef63bd28bd1ec2e644bg-03.jpg?height=330&width=1393&top_left_y=274&top_left_x=363)

Figure 3: Illustration of $S^{2}$-Attn. It involves three steps. First, it splits features along the head dimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size. Third, we split tokens into groups and reshape them into batch dimensions. Attention only computes in each group in ours while the information flows between groups via shifting. Potential information leakage might be introduced by shifting, while this is easy to prevent via a small modification on the attention mask. We ablate this in the variant 2 in Section B.3 in the appendix.

proportion of parameters in the entire LLM. For example, embedding has $(<2 \%)$ parameters, and normalization has $(\leq 0.004 \%)$ parameters in Llama2 7B. This ratio decreases for even larger LLMs.

In experiments, we show that LongLoRA is effective and efficient. We present experimental results of extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings of Position Interpolation (Chen et al. 2023), we fine-tune models with proper position embeddings. The trained models achieve comparable performance to the full-attention and fully fine-tuned results, while the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B up to $100 \mathrm{k}$ context, or a $70 \mathrm{~B}$ model up to $32 \mathrm{k}$, on a single $8 \times$ A100 machine.

In addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long instruction-following dataset, LongAlpaca. Our LongLoRA models are further fine-tuned with long questions and the corresponding answers. We design various types of questions for technical papers, science fiction, and other books. SFT is important for improving the chat ability of LLMs. We introduce our SFT settings in Section B.6 in the appendix.

## 2 RELATED WORK

Long-context Transformers. A large body of research has been developed to increase the context length of transformers. Some of these approaches are retrieval-based (Karpukhin et al., 2020, Izacard et al. 2022; Guu et al. 2020), which augment language models via fetching related documents and including the retrieved results into contexts. Our work is complementary to these works, as our attention mechanism is unmodified during inference. Many works modify multi-head attention to be approximated ones (Wang et al. 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Bulatov et al. 2022; Ding et al. 2023; Qiu et al. 2020). They alleviate the quadratic complexity of the self-attention computation. For example, Longformer (Beltagy et al. 2020) and BigBird (Zaheer et al. 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov et al. 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens. One limitation of these works is that these compressions have a large gap to full attention, making it infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of attention mechanism, it has a similar shape and a small gap to standard attention. This enables fine-tuning pre-trained LLMs on $\mathrm{S}^{2}$-Attn and maintain full attention during inference.

Long-context LLMs. LLMs are typically pre-trained with a pre-defined context length, such as 2048 for LLaMA (Touvron et al., 2023a) and 4096 for Llama2 (Touvron et al. 2023b). Training LLMs with long context from scratch is prohibitively expensive for most researchers. Recently, several works have tried to extend the context length of LLMs via fine-tuning. Position Interpolation (Chen et al. 2023) modifies rotary position encoding (Su et al. 2021) and extends the context length of LLaMA to 32768. Focused Transformer (Tworkowski et al., 2023) utilizes contrastive learning to train LongLLaMA. Both of them rely on full fine-tuning, which is computationally expensive (128 A100 GPUs / 128 TPUv3 for training). Landmark attention (Mohtashami \& Jaggi, 2023) is an

Table 1: Effectiveness of $\mathbf{S}^{2}$-Attn under different context lengths. 'Short' means $1 / 4$ of the target context length, while 'Long' equals to the target context length. Models are fully fine-tuned upon a Llama2 (Touvron et al., 2023b) model with 7B parameters on the RedPajama (Computer, 2023) dataset. Results are tested in perplexity on PG19 (Rae et al., 2020) validation split.

| Setting | Position Embedding | Training |  | Target Context Length |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Attention | Shift | 8192 | 16384 | 32768 |
|  |  | $\overline{\text { Long }}$ | - | 8.02 | 8.05 | 8.04  |
| Sho | PI (Chen et al., 2023) | Short | $x$ | 8.29 | 8.83 | 9.47 |
| $\mathrm{S}^{2}$-Attn |  | Short | $\checkmark$ | 8.04 | 8.03 | 8.08 |

efficient approach, but somewhat lossy. It compresses long context inputs into retrieved tokens. Our method saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours maintain full access to the entire input via unmodified attention during inference.

Some literature focuses on the position embedding modification of LLMs for long context extension, including Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023), positional Skipping (Zhu et al. 2023), and methods based on out-of-distribution analysis (Han et al. 2023). Our method focuses on efficient fine-tuning and retaining the original architecture during inference, which is orthogonal to these position embedding methods.

Efficient Fine-tuning. This work is based on LoRA (Hu et al., 2022), a classical efficient fine-tuning approach. In addition to LoRA (Hu et al. 2022), there are many other parameter-efficient fine-tuning methods, including prompt tuning (Lester et al., 2021), prefix tuning (Li \& Liang, 2021), hidden state tuning (Liu et al. 2022), bias tuning (Zaken et al., 2022), and masked weight learning (Sung et al., 2021). Input-tuning (An et al. 2022) introduces an adapter to tune input embedding. Although the input embedding layers are also trainable in ours, this is not enough for long context extension. We make a comprehensive analysis on layer types in experiments, in Table 2 Existing work (Chen et al. 2022) shows sparse masks can effectively save training costs and avoid performance drops.

## 3 LONGLoRA

### 3.1 BACKGROUND

Transformer. LLMs are typically built with transformers. Taking Llama2 (Touvron et al., 2023b) for example, as shown in Figure 2, an LLM model consists of an embedding input layer and a number of decoder layers. Each decoder layer comprises a self-attention module. It maps input features into a set of queries, keys, and values $\{q, k, v\}$, via linear projection layers with weight matrices $\left\{W_{q}, W_{k}, W_{v}\right\}$. Given $\{q, k, v\}$, it computes the outputs $o$ as

$$
\begin{equation*}
o=\operatorname{softmax}\left(q k^{T}\right) v \tag{1}
\end{equation*}
$$

The outputs are then projected by a linear layer with a weight matrix $W_{o}$. And MLP layers are followed. Before and after self-attention modules, layer normalization (Ba et al. 2016) is applied. A final normalization is conducted after all decoder layers.

For long sequences, self-attention struggles with computation cost, which is quadratic to the sequence length. This dramatically slows down the training procedure and increases GPU memory costs.

Low-rank Adaptation. LoRA (Hu et al. 2022) hypothesizes that the weight updates in pre-trained models have a low intrinsic rank during adaptation. For a pre-trained weight matrix $W \in \mathbb{R}^{d \times k}$, it is updated with a low-rank decomposition $W+\Delta W=W+B A$, where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$. The rank $r \ll \min (d, k)$. During training, $W$ is frozen with no gradient updates, while $\mathrm{A}$ and $\mathrm{B}$ are trainable. This is the reason why LoRA training is much more efficient than full fine-tuning.

In the Transformer structure, LoRA only adapts the attention weights ( $W_{q}, W_{k}, W_{v}, W_{o}$ ) and freezes all other layers, including MLP and normalization layers. This manner is simple and parameterefficient. However, we empirically show that only low-rank adaptation in attention weights does not work for long context extension.

```
Algorithm 1: Pseudocode of $S^{2}$-Attn in PyTorch-like style.
\# B: batch size; S: sequence length or number of tokens; G: group size;
\# H: number of attention heads; D: dimension of each attention head
\# qkv in shape (B, N, 3, H, D), projected queries, keys, and values
\# Key line 1: split qkv on $\mathrm{H}$ into 2 chunks, and shift $\mathrm{G} / 2$ on $\mathrm{N}$
$q k v=\operatorname{cat}((q k v \cdot \operatorname{chunk}(2,3)[0], q k v \cdot \operatorname{chunk}(2,3)[1] \cdot \operatorname{roll}(-G / 2,1)), 3) \cdot v i e w(B * N / G, G, 3, H, D)$
\# standard self-attention function
out $=$ self_attn $(q k v)$
\# out in shape (B, N, H, D)
\# Key line 2: split out on $\mathrm{H}$ into 2 chunks, and then roll back G/2 on N
out $=\operatorname{cat}($ (out.chunk $(2,2)[0], \operatorname{out.chunk}(2,2)[1] . \operatorname{roll}(\mathrm{G} / 2,1)), 2)$
```

cat: concatenation; chunk: split into the specified number of chunks; roll: roll the tensor along the given dimension.

### 3.2 SHIFTED SPARSE ATTENTION

Standard self-attention costs $O\left(n^{2}\right)$ computations, making LLMs on long sequences high memory cost and slow. To avoid this issue during training, we propose Shifted Sparse Attention ( $\mathrm{S}^{2}$-Attn), as shown in Figure 2. In the following, we make a pilot study and explain our design step by step.

Pilot Study. In Table 1, we build up a standard baseline that is trained and tested with full attention and fine-tuning, which presents consistently good quality in various context lengths. The first trial is to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high cost mainly comes from self-attention modules. Thus, in this trial, since the input is long, we split into several groups in self-attention. For example, the model takes 8192 tokens as input in both the training and testing stages, but self-attention is conducted in each group with a 2048 size. The group number is 4, as ablated in Section B. 2 in the appendix. This pattern is efficient but still does not work in a very long context, as shown in Table 1. The perplexity becomes larger as the context length increases. The reason behind this is that there is no information exchange between different groups.

To introduce communication between groups, we include a shifted pattern, as shown in Figure 2. We shift the group partition by half group size in half attention heads. Taking the overall 8192 context length for example, in pattern 1, the first group conducts self-attention from $1^{\text {st }}$ to $2048^{\text {th }}$ tokens. In Pattern 2, the group partition is shifted by 1024. The first attention group begins from $1025^{\text {th }}$ and ends at $3072^{\text {th }}$ tokens, while the first and the last 1024 tokens belong to the same group. We use patterns 1 and 2 in each half self-attention heads respectively. This manner does not increase additional computation costs but enables the information flow between different groups. We show that it gets close to the standard attention baseline in Table 1 .

Consistency to Full Attention. Existing efficient attention designs can also improve the efficiency of long-context LLMs. However, most of them are not suitable for long-context fine-tuning. Because, these transformers (Qiu et al. 2020, Child et al. 2019), designed for training from scratch, have gaps to the standard full attention, which is used in pre-training. In Table 6, we show that $S^{2}$-Attn not only enables efficient fine-tuning but also supports full attention testing. Although other attentions can also be used in long context fine-tuning, models must be tested with the attention used during fine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.

Easy Implementation. $\quad \mathrm{S}^{2}$-Attn is easy to implement. It involves only two steps: (1) shifting tokens in half attention heads, and (2) transposing features from token dimension to batch dimension. Two lines of code are enough. We provide a PyTorch-style code in Algorithm 1 .

### 3.3 IMPROVED LORA FOR LONG CONTEXT

LoRA (Hu et al. 2022) is an efficient and popular manner for adapting LLMs to other datasets. It saves much trainable parameters and memory cost, compared to full fine-tuning. However, adapting LLMs from short context length to long is not easy. We empirically observe an obvious gap between LoRA and full fine-tuning. As shown in Table 2, the gap between LoRA and full fine-tuning grows as the target context length becomes larger. And LoRA with larger ranks cannot reduce the gap.

Table 2: Finetuning normalization and embedding layers is crucial for low-rank long-context adaptation. Llama2 7B (Touvron et al. 2023b) models with the proposed $\mathrm{S}^{2}$-Attn are trained on the RedPajama (Computer, 2023) dataset. The target context length is 32768. '+ Normal / Embed' means normalization or embedding layers are trainable. Perplexity results are evaluated on PG19 (Rae et al. 2020) validation set. For long context adaptation, there is a large performance gap between standard LoRA (Hu et al. 2022) and full fine-tuning. Without trainable normalization or embeddings, larger ranks in LoRA can not close this gap.

| Method | Full FT | LoRA (rank) |  |  |  |  |  | LoRA (rank $=8$ ) |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | 8 | 16 | 32 | 64 | 128 | 256 | + Norm | + Embed | + Norm \& Embed |
| $\overline{P P L}$ | 8.08 | 11.44 | 11.82 | 11.92 | 11.96 | 11.97 | 11.98 | 10.49 | 8.29 | 8.12 |

Table 3: Perplexity evaluation on proof-pile (Rae et al. 2020) test split. S²-Attn: Shifted Sparse Attention. LoRA ${ }^{+}$: improved LoRA. We fine-tune Llama2 (Touvron et al. 2023b) in 7B and 13B model sizes on the RedPajama (Computer 2023) dataset under $8 \mathrm{k}-32 \mathrm{k}$ context lengths. We show that our method achieves comparable performance to the full attention or full FT baselines, with better efficiency. We use the same training setting as the model evaluated on PG19 (Rae et al., 2020) introduced in Section B. 1 in the appendix.

| Size | Training <br> Context Length | LongLoRA |  | Evaluation Context Length |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathrm{S}^{2}$-Attn | $\mathrm{LoRA}^{+}$ | 2048 | 4096 | 8192 | 16384 | 32768 |
| 7B | 8192 |  |  | 3.14 | 2.85 | 2.66 | - | - |
|  |  | $\checkmark$ |  | 3.15 | 2.86 | 2.68 | - | - |
|  |  | $\checkmark$ | $\checkmark$ | 3.20 | 2.91 | 2.72 | - | - |
|  | 16384 | $\checkmark$ |  | 3.17 | 2.87 | 2.68 | 2.55 | - |
|  |  | $\checkmark$ | $\checkmark$ | 3.17 | 2.87 | 2.66 | 2.51 | - |
|  | 32768 | $\checkmark$ |  | 3.20 | 2.90 | 2.69 | 2.54 | 2.49 |
|  |  | $\checkmark$ | $\checkmark$ | 3.35 | 3.01 | 2.78 | 2.61 | 2.50 |
| $13 B$ | 8192 |  |  | 2.96 | 2.69 | 2.53 | - | - |
|  |  | $\checkmark$ |  | 3.01 | 2.74 | 2.57 | - | - |
|  |  | $\checkmark$ | $\checkmark$ | 3.04 | 2.77 | 2.60 | - | - |
|  | 16384 | $\checkmark$ |  | 2.99 | 2.72 | 2.53 | 2.40 | - |
|  |  | $\checkmark$ | $\checkmark$ | 3.03 | 2.74 | 2.55 | 2.41 | - |
|  | 32768 | $\checkmark$ |  | 3.04 | 2.75 | 2.56 | 2.42 | 2.33 |
|  |  | $\checkmark$ | $\checkmark$ | 3.05 | 2.76 | 2.57 | 2.42 | 2.32 |

To bridge this gap, we open embedding and normalization layers for training. As shown in Table 2 . they occupy limited parameters but make effects for long context adaptation. Especially for normalization layers, the parameters are only $0.004 \%$ in the whole Llama2 7B. We denote this improved version of LoRA as LoRA ${ }^{+}$in experiments.

## 4 EXPERIMENT

### 4.1 EXPERIMENTAL SETTINGS

Models We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al. 2023b) models. The maximum extended context window sizes are up to $100 \mathrm{k}$ for 7B models, 65536 for 13B models, and 32768 for $70 \mathrm{~B}$ models. The position indices for these models are re-scaled with Position Interpolation (Chen et al. 2023).

Training Procedure We follow most training hyper-parameters in Position Interpolation (Chen et al. 2023), except that our batch size is smaller as we use a single $8 \times$ A100 GPUs machine in some cases. All models are fine-tuned via the next token prediction objective. We use AdamW (Loshchilov $\&$ Hutter 2019) with $\beta_{1}=0.9$ and $\beta_{2}=0.95$. The learning rate is set to $2 \times 10^{-5}$ for $7 \mathrm{~B}$ and $13 \mathrm{~B}$ models, and $10^{-5}$ for $70 \mathrm{~B}$ models. We also use a linear learning rate warmup. The weight decay is

Table 4: Maximum context length that we can fine-tune for various model sizes on a single $8 \times$ A100 machine. We use the same training and evaluation settings as in Table 3. We use FlashAttention2 (Dao, 2023) and DeepSpeed (Rasley et al. 2020) in stage 3 during fine-tuning. With LongLoRA, the maximum context length for $7 \mathrm{~B}, 13 \mathrm{~B}$, and $70 \mathrm{~B}$ models are $100 \mathrm{k}, 64 \mathrm{k}$, and $32 \mathrm{k}$ respectively. Evaluation on PG19 (Rae et al. 2020) is in Section B.1 in the appendix.

| Size | Training | Evaluation Context Length |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Context Length | 2048 | 4096 | 8192 | 16384 | 32768 | 65536 | 100,000 |
| 7B | 100,000 | 3.36 | 3.01 | 2.78 | 2.60 | 2.58 | 2.57 | 2.52 |
| 13B | 65536 | 3.20 | 2.88 | 2.66 | 2.50 | 2.39 | 2.38 | - |
| 70B | 32768 | 2.84 | 2.57 | 2.39 | 2.26 | 2.17 | - | - |

Table 5: Topic retrieval evaluation with LongChat (Li et al., 2023). We compare our model to other open-source long-context LLMs. This task involves retrieving target topics from a very long conversation with around $3 \mathrm{k}, 6 \mathrm{k}, 10 \mathrm{k}, 13 \mathrm{k}$, and $16 \mathrm{k}$ context lengths. As some questions in the evaluation set are longer than $16 \mathrm{k}$, our model is fine-tuned upon Llama2 13B. It achieves comparable performance to the state-of-the-art LongChat-13B (Li et al. 2023) with a lower fine-tuning cost.

| Evaluation Context | $3 \mathrm{k}$ | $6 \mathrm{k}$ | $10 \mathrm{k}$ | $13 \mathrm{k}$ | $16 \mathrm{k}$ |
| :--- | :---: | :---: | :---: | :---: | :---: |
| ChatGLM2-6B (Du et al., 2022) | 0.88 | 0.46 | 0.02 | 0.02 | 0.02 |
| MPT-30B-chat (Team, 2023a) | 0.96 | $\mathbf{1 . 0}$ | 0.76 | - | - |
| MPT-7B-storywriter (Team, 2023b) | 0.46 | 0.46 | 0.28 | 0.34 | 0.36 |
| LongChat-13B (Li et al. 2023) | $\mathbf{1 . 0}$ | $\mathbf{1 . 0}$ | $\mathbf{1 . 0}$ | $\mathbf{0 . 9 8}$ | 0.9 |
| Ours-13B | $\mathbf{1 . 0}$ | 0.98 | 0.98 | $\mathbf{0 . 9 8}$ | $\mathbf{0 . 9 4}$ |

zero. We set the per-device batch size as 1 and gradient accumulation steps as 8 , which means that the global batch size equals 64 , using 8 GPUs. We train our models for 1000 steps.

Datasets We use the Redpajama (Computer, 2023) dataset for training. We evaluate the longsequence language modeling performance of our fine-tuned models on the book corpus dataset PG19 (Rae et al. 2020) and the cleaned Arxiv Math proof-pile dataset (Azerbayev et al. 2022). We use the test split of PG19 (Rae et al. 2020), consisting of 100 documents. For the proof-pile dataset, we also use the test split of it for evaluation. We follow Position Interpolation (Chen et al. 2023) for proof-pile data processing. We evaluate perplexity by using a sliding window approach with $S=256$, following (Press et al. 2022).

### 4.2 MAIN RESULTS

Long-sequence Language Modeling. In Table 3, we report the perplexity for our models and baseline on proof-pile (Azerbayev et al. 2022) and PG19 datasets. Under certain training context lengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness of our efficient fine-tuning method. In Table 3 , for the same training and evaluation context length cases, the perplexity decreases as the context size increases. By increasing the context window size from 8192 to 32768 , for the Llama2 7B model, we observe that the perplexity gets better from 2.72 to 2.50 by -0.22 . For Llama2 13B model, we observe that the perplexity reduces by -0.28 .

In Table 4 , we further examine the maximum context length that we can fine-tune on a single $8 \times$ A100 machine. We extend Llama2 7B, 13B, and 70B to $100 \mathrm{k}, 65536$, and 32768 context length respectively. LongLoRA achieves promising results on these extremely large settings. In addition, we find some perplexity degradation on small context sizes for the extended models. This is a known limitation of Position Interpolation (Chen et al., 2023).

Retrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we compare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al. 2023). This task is to retrieve the target topic from a very long conversation, with lengths varying from $3 \mathrm{k}, 6 \mathrm{k}, 10 \mathrm{k}, 13 \mathrm{k}$, to $16 \mathrm{k}$. As some questions in LongChat (Li et al. 2023) are longer than $16 \mathrm{k}$, we fine-tuned Llama2 13B with a context length of $18 \mathrm{k}$. The training cost is similar to that for $16 \mathrm{k}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_aef63bd28bd1ec2e644bg-08.jpg?height=353&width=1391&top_left_y=276&top_left_x=367)

Figure 4: Accuracy comparison on passkey retrieval between Llama2 7B and our 7B model fine-tuned on 32768 context length. Our model presents no retrieval accuracy degradation until $33 \mathrm{k}$ or $34 \mathrm{k}$, which exceeds the context length. It can further enhance its capability of long sequence modeling through a straightforward extension of position embeddings, without additional fine-tuning.

![](https://cdn.mathpix.com/cropped/2024_06_04_aef63bd28bd1ec2e644bg-08.jpg?height=331&width=1387&top_left_y=859&top_left_x=369)

Figure 5: Ablation on fine-tuning steps in both full fine-tuning and LoRA ${ }^{+}$. We fine-tune Llama2 (Touvron et al., 2023b) 7B with the proposed $S^{2}$-Attn. The target context length is 8192 . We use RedPajama (Computer 2023) for training and PG19 (Rae et al. 2020) validation set for perplexity testing. Full fine-tuning converges faster than LoRA ${ }^{+}$at the beginning, but the final performance gap is small.

Our model achieves comparable performance to LongChat-13B (Li et al. 2023), the state-of-the-art model in this task. Unlike LongChat-13B (Li et al. 2023), which is fully fine-tuned on self-collected long context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via next-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.

In Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Attention (Mohtashami \& Jaggi, 2023). This task has also been adopted by other literature (Chen et al. 2023; Tworkowski et al. | 2023). In this task, the models need to find a random passkey hidden in a long document. We show the document format is in Section A.2 in the appendix. We study Llama2 7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768 context length. We test the passkey retrieval accuracy from $1 \mathrm{k}$ to $34 \mathrm{k}$, with an interval of roughly $1 \mathrm{k}$ (as the sentence length can not be precisely controlled). For each document length, we test the model 10 times with different random passkey values. Our model achieves reasonable passkey retrieval accuracy until $33 \mathrm{k}$ or $34 \mathrm{k}$. Without further fine-tuning, We modify the max position embeddings to 48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this model can handle longer documents by simply extending the position interpolation. As the dashed orange line in Figure 4, the model, fine-tuned on $32 \mathrm{k}$ context length, presents moderate retrieval ability ( $60 \%-90 \%$ accuracy) in the range of $33 \mathrm{k}$ to $45 \mathrm{k}$. Even with the position interpolation extended, Llama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the $4 \mathrm{k}$ context length.

### 4.3 ABLATION STUDY

In this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns. Other experimental results including ablations on group sizes, attention variants, and efficiency analysis are Section B in the appendix.

Ablation on Fine-tuning Steps. We report the relationship between perplexity and fine-tuning steps for a Llama2 7B model extending to the 8192 context length on the PG19 validation set, in

Table 6: Comparisons among $\mathrm{S}^{2}$-Attn and alternative attention patterns during fine-tuning. We adapt a Llama2 7B model to 32768 context length with different attention patterns and improved LoRA at training time. We include four typical efficient attention designs, e.g., shift, dilate (Ding et al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al. 2019) for comparison. ' cro. heads / layers' means to swap different attention settings across attention heads or sequential layers. Taking $\mathrm{S}^{2}$-Attn as an example, 'cro. layers' is to swap between $\mathrm{w} /$ and w/o shift in sequential self-attention layers. 'only P1/P2' means all attention heads use pattern 1 (all no shift) or Pattern 2 (all shift) in Figure 2. We visualize the patterns of different attention in Figure 7 in the appendix. For each attention pattern, we evaluate its performance under two protocols. In the first row, we use sparse attention in both training and testing. In the second row, we use full attention for testing.

| Test w/ <br> Full-Attn | cro. heads | cro. layers | only P1. | only P2. | Dilate <br> cro. heads | Block sparse <br> cro. heads | Stride sparse <br> cro. heads |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\boldsymbol{X}$ | 8.64 | 8.63 | 9.17 | 9.64 | 8.75 | 11.49 | 32.81 |
| $\boldsymbol{J}$ | 8.12 | 9.70 | 8.39 | 9.81 | 11.78 | 8.30 | 24.03 |

Figure 5. We see that without fine-tuning, at step 0 , the model has a limited long context capability, e.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster than low-rank training. They come closer after 200 steps, without a large gap at the end.

Attention Patterns. In Table 6, we show the effects of different attention patterns during finetuning. We fine-tune a Llama2 7B (Touvron et al., 2023b) model to 32768 context length on Redpajama (Computer, 2023) datasets and evaluate the perplexity on PG19 (Rae et al., 2020) validation set. We first examine the manner of swapping among various settings. For the shift operation we used in LongLoRA, there are three choices: disabling it, shifting between sequential layers, and shifting among attention heads. We show that shifting between layers is acceptable but not the best. In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition, we empirically find that shifting left or right has little difference in performance.

We then test other types of efficient attention designs, including dilated attention (Ding et al. 2023), block sparse attention (Qiu et al., 2020), and stride sparse attention (Child et al. 2019). For dilated attention (Ding et al., 2023), we vary the dilate rate from 1 to 2 evenly among attention heads. For block sparse attention (Qiu et al., 2020), we use $n=4$ block-wise masking matrices in attention heads and move the block left to make it causal. Stride sparse attention (Child et al., 2019) contains both local and stride patterns. These settings share similar computational costs. We visualize these patterns in Figure 7 in the appendix. These attention patterns are invented in training-fromscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained LLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in full fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is harmful. They have a large gap to full attention, which is applied in the pre-training stage.

## 5 CONCLUSION

In this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be significantly larger. LongLoRA has less GPU memory cost and training time than standard full fine-tuning, with minimal accuracy compromise. At the architecture level, we propose $S^{2}$-Attn to approximate the standard self-attention pattern during training. $S^{2}$-Attn is easy to implement, requiring only two lines of code. Moreover, models trained via $S^{2}$-Attn retain the original standard attention architecture during inference, making most pre-existing infrastructure and optimization reusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable normalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B model to $32 \mathrm{k}$ context length, on a single $8 \times$ A100 machine. We also present a long instructionfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA. We believe that LongLoRA is a general method that could be compatible with more types of LLMs and position encodings. We plan to investigate these in future work.

Acknowledgement We would like to thank Xiuyu $\mathrm{Li}$ and Bohao Peng for the helpful discussions.

## REFERENCES

Ntk-aware scaled rope, 2023. URL https://www.reddit.com/r/LocalLLaMA/ comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_ have/

Byeongjoo Ahn, Michael DeZeeuw, Ioannis Gkioulekas, and Aswin C. Sankaranarayanan. Neural kaleidoscopic space sculpting. In CVPR, pp. 4349-4358, 2023.

Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023.

Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR, abs/2203.03131, 2022.

Zhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https: //github.com/zhangir-azerbayev/proof-pile.

Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020.

Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. In NeurIPS, 2022.

Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Ré. Pixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR, 2022.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. CoRR, abs/2306.15595, 2023.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019.

Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URLhttps://github.com/togethercomputer/RedPajama-Data.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, $\mathrm{abs} / 2307.08691,2023$.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR, abs/2307.02486, 2023.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In ACL, pp. 320-335, 2022.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrievalaugmented language model pre-training. CoRR, abs/2002.08909, 2020.

Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.

Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299, 2022.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP, pp. 6769-6781, 2020.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR, 2020.

Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), EMNLP, pp. 3045-3059, 2021.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. URL https://lmsys.org/blog/2023-06-29-longchat.

Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582-4597, 2021.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS, 2022.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp. $9992-10002,2021$.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.

Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: Stateof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/ peft, 2022.

Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. CoRR, abs/2305.16300, 2023.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In NeurIPS, pp. 8024-8035, 2019.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023.

Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In ICLR, 2022.

Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for RGBD semantic segmentation. In ICCV, pp. 5209-5218, 2017.

Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise selfattention for long document understanding. In EMNLP, volume EMNLP 2020 of Findings of ACL, pp. 2555-2565, 2020.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In ICLR, 2020.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD, pp. 3505-3506. ACM, 2020.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021.

Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In NeurIPS, pp. 24193-24205, 2021.

MosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models, 2023a. URLWww.mosaicml.com/blog/mpt-30b.

MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023b. URL www.mosaicml.com/blog/mpt-7b.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, $\mathrm{abs} / 2307.09288,2023 \mathrm{~b}$.

Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. Focused transformer: Contrastive training for context scaling. CoRR, abs/2307.03170, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998-6008, 2017.

Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In ICLR, 2022.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In NeurIPS, 2020.

Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), ACL, pp. 1-9, 2022.

Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu. Safeconv: Explaining and correcting conversational unsafe behavior. In ACL, pp. 22-35, 2023.

Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training, 2023.
