# Improved Generation of Adversarial Examples Against Safety-aligned LLMs 

Qizhang Li ${ }^{1,2}$, Yiwen Guo ${ }^{3 *}$, Wangmeng $\mathbf{Z u o}^{1}$, Hao Chen ${ }^{4}$<br>${ }^{1}$ Harbin Institute of Technology, ${ }^{2}$ Tencent Security Big Data Lab, ${ }^{3}$ Independent Researcher, ${ }^{4}$ UC Davis<br>\{liqizhang95, guoyiwen89\}@gmail.com wmzuo@hit.edu.cn chen@ucdavis.edu


#### Abstract

The remarkable performance of large language models (LLMs) has raised both interest and concern regarding their safety and trustworthiness. Despite numerous efforts to ensure they adhere to safety standards and produce harmless content, some successes have been achieved in bypassing these restrictions, known as jailbreak attacks against LLMs. Adversarial prompts (or say, adversarial examples) generated using gradient-based methods exhibit outstanding performance in performing jailbreak attacks automatically. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the white-box setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking black-box image classification models. For the first time, we appropriate the ideologies of effective methods among these transferbased attacks, i.e., Skip Gradient Method [48] and Intermediate Level Attack [16], for improving the effectiveness of automatically generated adversarial examples against white-box LLMs. With appropriate adaptations, we inject these ideologies into gradient-based adversarial prompt generation processes and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that $87 \%$ of the query-specific adversarial suffixes generated by the developed combination can induce Llama-2-7B-Chat to produce the output that exactly matches the target string on AdvBench. This match rate is $33 \%$ higher than that of a very strong baseline known as GCG, demonstrating advanced discrete optimization for adversarial prompt generation against LLMs. In addition, without introducing obvious cost, the combination achieves $>30 \%$ absolute increase in attack success rates compared with GCG when generating both query-specific $(38 \% \rightarrow 68 \%)$ and universal adversarial prompts ( $26.68 \% \rightarrow 60.32 \%$ ) for attacking the Llama-2-7B-Chat model on AdvBench. Code at: hhttps://github.com/qizhangli/Gradient-based-JailbreakAttacks.


## 1 Introduction

Large language models (LLMs) have demonstrated a formidable capacity for language comprehension and the generation of human-like text. Safty-aligned LLMs, refined through specific fine-tuning mechanisms [33, 1, 19, 10], are anticipated to yield responses that are not only helpful but also devoid of harm in response to user instructions. However, certain studies [37, 44, 52, 35, 4, 25] reveal that[^0]these models have not yet achieved perfect alignment. It has been demonstrated that these models can be carefully prompted to produce harmful content through the introduction of meticulously crafted prompts, a phenomenon known as "jailbreak" [44]. The manually designed jailbreak prompts are crafted by carefully constructing scenarios that mislead the models, necessitating a significant amount of work. In contrast, adversarial examples are automatically generated with the intent deceiving models to generate harmful responses, presenting a more insidious challenge to model robustness.

One of the main difficulties in generating adversarial examples for NLP models lies in the fact that text is discrete by nature, making it challenging to use gradient-based optimization methods to devise adversarial attacks. There has been some work [13, 47, 38, 18, 52] attempted to overcome this issue. For instance, recently, a method called Greedy Coordinate Gradient (GCG) attack [52] has shown significant jailbreaking improvements, by calculating gradients of cross-entropy loss w.r.t. one-hot representations of chosen tokens in a prompt and utilizing them in a greedy manner. However, due to the fact that the gradients w.r.t. one-hot vectors do not provide precise indication of the loss change that results from a token replacement, the GCG attack shows limited white-box attack success rates against some safty-aligned LLMs, e.g., Llama-2-Chat models [41].

In this paper, we carefully examine the discrepancy between the gradient of the adversarial loss w.r.t. one-hot vectors and the real effect of the change in loss that results from token replacement. We present a new perspective that this gap resembles the gap between input gradients calculated using a substitute model and the real effect of perturbing inputs on the prediction of a black-box victim model, which has been widely studied in transfer-based attacks against black-box image classification models [40, 34, 26, 16, 48, 14, 24]. Based on this new perspective, for the first time, we attempt to appropriating the ideologies of two effective methods among these transfer-based methods, i.e., Skip Gradient Method (SGM) [48] and Intermediate Level Attack (ILA) [16], to improve the gradient-based attacks against LLMs. With appropriate adaptations, we successfully inject these ideologies into the gradient-based adversarial prompt generation without additional computational cost. By discussing the mechanisms behind the advanced performance, we provide new insights about improving discrete optimizations on LLMs. Moreover, we provide an appropriate combination of these methods. The experimental results demonstrate that $87 \%$ of the query-specific adversarial suffixes generated by the combination for attacking Llama-2-7B-Chat on AdvBench can induce the model output exact target string, which outperforms a strong baseline named GCG attack ( $54 \%$ ), indicating an advanced discrete optimization for adversarial prompt generation against LLMs. In addition, the combination achieves attack success rates of $68 \%$ for query-specific and $60.32 \%$ for universal adversarial prompt generation when attacking Llama-2-7B-Chat on AdvBench, which are higher than those of the baseline GCG ( $38 \%$ and $26.68 \%$ ).

## 2 Related Work

Jailbreak attacks. Recent work highlights that the safety-aligned models are still not perfectly aligned [2, 37, 44], the safety-aligned LLMs can be induced to produce harmful content by some carefully designed prompts, known as jailbreak attacks [44]. This has raised security concerns and attracted great attention. In addition to some manually designed prompt methods [44, 37], numerous automatic jailbreak attack methods have been proposed. Some methods directly optimize the text input through gradient-based optimization [42, 13, ,38, 47, 18, 52]. Another line of work involves using LLMs as optimizers to jailbreak the victim LLM [35, 4, 28]. There are also methods that focus on designing special jailbreaking templates or pipelines [25, 36, 3, 5, 50, 45]. According to the knowledge of the victim model, these methods can also be divided into white-box attacks and black-box attacks. In the context of white-box attacks, the attackers have full access to the architecture and parameters of the victim LLM, making them can leverage the gradient with respect to the inputs. As demonstrated by recent benchmark [27], represented by a current method as known as GCG attack [52], gradient-based automatic jailbreak attacks have shown the most powerful performance in compromising LLMs in the setting of white-box attack. However, due to the discrete nature of text input, dealing with the discrete optimization problem is rather challenging, which limits the success rates of attacks, especially those against Llama-2-Chat models [41]. In our work, we primarily focus on solving the discrete optimization problem in gradient-based automatic jailbreak attacks to improve the success rate in the white-box setting.

Transfer-based attacks. Transfer-based attacks attempt to craft adversarial examples on a white-box substitute model to attack the black-box victim model, by leveraging the transferability of adversarial
examples [40], which is a phenomenon that adversarial examples crafted on a white-box substitute model can also mislead the unknown victim models with a decent success rate. The transferbased attacks have been thoroughly investigated in the setting of attacking image classification models [20, 49, 6, 48, 16, 11, 14, 15, 23, 24]. Some recent methods also utilize the transferability of adversarial prompts to perform black-box attacks against LLMs [52, 25, 39]. While in this work, we mainly focus on improving the white-box attack success rate. In our work, we reveal a closely relationship between the optimization in transfer-based attacks and discrete optimization of the gradient-based jailbreak attacks against LLMs. We then appropriate the ideologies of two effective transfer-based attack methods developed in the setting of attacking image classification model, i.e., SGM [48] and ILA [16]. Moreover, by adapting these strategies and analyzing the mechanism behind them, we provide some new insights about potential solutions for addressing problems involving discrete optimization in NLP models with transformer architecture, e.g., prompt tuning.

## 3 Gap Between Input Gradients and the Effects of Token Replacements

In this section, we first discuss the main obstacle in achieving effective gradient-based attacks on safety-aligned LLMs, which is considered as the gap between input gradients and the effects of token replacements, and then we show how transfer-based strategies can be adapted to overcome the issue.

### 3.1 Rethinking Gradient-based Attacks Against LLMs

Previous endeavors utilize the gradient w.r.t. the token embeddings [21, 47] or w.r.t. the one-hot representations of tokens [8, 38, 52, 18], in order to solve the discrete optimization problem efficiently during attacking LLMs. A recent method, named Greedy Coordinate Gradient (GCG) [52], shows a significant improvement over other optimizers (e.g., AutoPrompt [38] and PEZ [47]) on performing gradient-based attacks against LLMs in the white-box setting. Due to its effectiveness, we take GCG as a strong baseline and as a representative example to analyze previous gradient-based attacks against LLMs in this section.

A typical LLM $f: \mathcal{X} \rightarrow \mathbb{R}^{|V|}$ with a vocabulary $V$ is trained to map a sequence of tokens $x_{1: n}=\left[x_{1}, \ldots, x_{n}\right] \in \mathcal{X}, x_{i} \in V$ to a probability distribution over the next token, denoted as $p_{f}\left(x_{n+1} \mid x_{1: n}\right)$. To evoke a jailbreak to induce the a safety-aligned LLM generate harmful content according the user query, GCG attempts to add an adversarial suffix to the original user query and iteratively modifies the adversarial suffix to encourages the model output an affirmative target phrase, e.g., "Sure, here's ...". Consider an adversarial prompt (which is the concatenation of a user query and an adversarial suffix) as $x_{1: n}$ and a further concatenation with a target phrase as $x_{1: n^{*}}$, GCG aims to minimize the adversarial loss, which corresponds to the negative log probability of the target phrase. It can be written as

$$
\begin{equation*}
\min _{x_{\mathcal{A}} \in\{1, \ldots, V\}|\mathcal{A}|} \frac{1}{n^{*}-n} \sum_{i=1}^{n^{*}-n}-\log p_{f}\left(x_{n+i} \mid x_{1: n+i-1}\right) \tag{1}
\end{equation*}
$$

where $x_{\mathcal{A}}$ denotes the tokens of adversarial suffix in $x_{1: n}$ and $\mathcal{A}$ denotes the set of indices of the adversarial suffix tokens, and we use $L(x)$ to denote the adversarial loss. At each iteration, it computes the gradient of the cross-entropy loss w.r.t. the one-hot vector of a single token and uses each value of gradient to estimate the effect of replacing the current token with the corresponding one on loss. Since the discrete nature of input, there is a gap between input gradients and the effects of token replacements, thus the estimate is not accurate enough. Previous efforts attempt to solve this problem by collecting a candidate set of token replacements according to the Top- $k$ values in the gradient and evaluating the candidates to pick the best token replacement with minimal loss [52, 38]. We target this problem in another fashion by refining the input gradient to narrow the gap.

We attempt to solve this problem from a new perspective that treats the estimation as using a "surrogate model" which takes continuous inputs to attack the "real model" which takes discrete text inputs. Let us provide explanations as follows. The strategy of utilizing the gradient w.r.t. the one-hot vector is actually treating the one-hot vector as though it was a continuous variable. Thus, we consider that the gradient-based attacks essentially build a surrogate model, $f^{S}: \mathbb{R}^{n \times|V|} \rightarrow \mathbb{R}^{|V|}$, which has the same architecture and parameters as $f$, except it maps input from a continuous space to the probability distribution. This is different from the real model $f$, which maps its input from a discrete space to the probability distribution. Then, the gradient-based attacks use the input gradient computed on the

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-04.jpg?height=284&width=284&top_left_y=281&top_left_x=362)

Figure 1: An example of the residual block.

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-04.jpg?height=347&width=526&top_left_y=222&top_left_x=691)

(a) Loss

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-04.jpg?height=333&width=504&top_left_y=232&top_left_x=1233)

(b) Match rate

Figure 2: How (a) the loss and (b) the match rate changes with attack iterations. The attacks are performed against Llama-2-7B-Chat model to generate queryspecific adversarial suffixes on AdvBench. Best viewed in color.

"surrogate model" to estimate the effect of the loss change resulting from token replacements on the "real model". Therefore, the problem of inaccurate estimates is similar to the challenges encountered in transfer-based attacks on image classification models. Specifically, in the context of transfer-based attacks on image classification models, the goal is to compromise an unknown victim model. To achieve this, the attackers employ a substitute model that performs the same task as the victim model, and use this substitute model to calculate the gradient of the classification loss w.r.t. the input. Such input gradient estimates how changes in the input affect the changes in the prediction on the victim model. The attackers then modify the adversarial example to increase the classification loss on the victim model. Many methods [7, 49, 16, 48, 14, 23] have been developed to refine the gradient computed on the substitute model, in order to narrow the gap between the input gradients and the real effects of loss change results from changes in the inputs on the victim model. This new perspective allows one to introduce a series of innovations developed within the realm of transfer-based attacks on image classification models to refine the gradient computation in the context of gradient-based adversarial prompt generation against safety-aligned LLMs.

From the results of a recent benchmark of these transfer-based attacks [24], we can observe that several strategies, including SGM [48], SE [31], PNA [46], and a series of intermediate level attacks (ILA [16], ILA++ [15], FIA [43], and NAA [51]), are obviously effective when generating adversarial examples on models with a transformer architecture. Among them, PNA ignores the backpropagation of the attention map and SE requires considerable ability to directly predict the probability from the intermediate representations, thus their strategies are difficult to be adapted to the context of LLM attacks. Therefore, we consider drawing inspiration from SGM and ILA (which is the representative of all intermediate level attacks).

### 3.2 Reducing the Gradients from Residual Modules

Modern deep neural networks typically comprise a number of residual blocks, each consisting of a residual module and a skip connection branch, as depicted in Figure 1. SGM [48] experimentally found that reducing the gradients from residual modules during backpropagation can improve the transfer-based attacks against image classification models, indicating that it can reduce the gap between the input gradients and the effects resulting from perturbing inputs on a unknown victim model. In this section, we investigate whether the strategy of reducing gradients from residual modules can also enhance gradient-based attacks against LLMs in a white-box setting. Additionally, we discuss the mechanisms behind this strategy to provide new insights.

An $l$-layers LLM can be decomposed into $2 l$ residual building blocks, with each block consisting of a residual module (which should be an MLP or an attention module) and a parallel skip connection branch as illustrated in Figure 1 The $m$-th block maps an intermediate representation $z_{m}$ to $z_{m+1}$, i.e., $z_{m+1}=I\left(z_{m}\right)+R_{m}\left(z_{m}\right)$, where $I$ is an identity function representing the skip connection branch and $R_{m}$ denotes the residual module of the $m$-th block. By adopting a decay factor $\gamma \in[0,1]$ for the residual modules, SGM calculates the derivative of the $m$-th block as $\nabla_{z_{m}} z_{m+1}=1+\gamma \nabla_{z_{m}} R_{m}\left(z_{m}\right)$. We incorporate this strategy into gradient-based automatic adversarial prompt generation, denoted as Language SGM (LSGM). We evaluate the performance of integrating this strategy into GCG and AutoPrompt attacks by setting $\gamma=0.5$, and show the results in Figure 2. The experiment is conducted to generate query-specific adversarial suffixes against Llama-2-7B-Chat [41] on the first 100 harmful queries in AdvBench [52]. To provide a more comprehensive comparison, we report

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-05.jpg?height=382&width=482&top_left_y=199&top_left_x=426)

Figure 3: The cosine similarities between the gradients from residual modules and the gradients from skip connections in different residual blocks.

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-05.jpg?height=387&width=718&top_left_y=197&top_left_x=1034)

Figure 4: Comparing the average effects of residual modules and the average effects of skip connections on the change in adversarial loss varies with different residual blocks. Best viewed in color.

not only the adversarial loss but also the fraction of adversarial prompts that result in outputs exactly matching the target string, dubbed match rate, which is also used as an evaluation metric in the paper of GCG [52]. It can be seen from the figure that reducing gradients from residual modules can indeed improve the performance of gradient-based adversarial prompt generation. The GCG-LSGM achieves a match rate of $72 \%$, while the baseline (i.e., GCG) only obtains $54 \%$ match rate. We also evaluate the attack success rate (ASR) by using the evaluator proposed by HarmBench [27]. It shows an ASR of $62 \%$ when using GCG-LSGM, while the GCG only achieves $38 \%$. The results confirm that reducing gradients from residual modules helps the gradients w.r.t. one-hot representations to be more effective to indicate the real effects of token replacements.

Having seen the effectiveness of GCG-LSGM, let us further delve deep into the mechanism behind the method. We begin by analyzing the computational graph and gradient flow of a building block. Following the chain rule, the gradient of adversarial loss w.r.t. $z_{m}$ can be written as a summation of two terms: $\nabla_{z_{m}} L(x)=\nabla_{z_{m+1}} L(x)+\nabla_{z_{m}} R\left(z_{m}\right) \nabla_{z_{m+1}} L(x)$. The first term represents the gradient from the skip connection, and the second term represents the gradient from the residual module. They represent the impact on the loss caused by changes in $z_{m}$ through skip connection branch and residual module branch, respectively. In Figure 3, we visualize the average cosine similarity between these two terms at the 100-th iteration of the GCG attack against Llama2-7B-Chat across 100 examples. The same observations can be obtained at other iterations during the GCG attack. Somewhat surprisingly, it can be seen that these two terms have negative cosine similarity in most blocks during the iterative optimization. Obviously, the summation of two directionally conflicting gradients may mitigate the effect of each other's branch on reducing the loss. Hence, reducing the gradients from residual modules achieves lower loss values by sacrificing in the effects of the residual modules to trade more effects of the skip connection on loss. The success of GCG-LSGM somehow implies the gradient flowing from skip connections better reflect the effect of token replacements on adversarial loss.

To confirm the conjecture, we attempt to evaluate the effect of each branch towards reducing the loss. Inspired by the causal tracing technique [29], we perform the following steps to compute the effect of each branch's hidden state. First, we give an adversarial prompt to the model to obtain the loss, denoted by $L(x)$. Second, we randomly alter a token from the adversarial prompt and record the hidden states in the two branches, i.e., $I\left(\tilde{z}_{m}\right)$ and $R_{m}\left(\tilde{z}_{m}\right)$, given the altered adversarial prompt. Finally, we feed the original adversarial prompt into the model and modify the forward computation by replacing $I\left(z_{m}\right)$ with $I\left(\tilde{z}_{m}\right)$ (or $R_{m}\left(z_{m}\right)$ with $R_{m}\left(\tilde{z}_{m}\right)$ ), to obtain the modified loss $\tilde{L}(x)$. The effect of a branch is represented by the difference between the loss of obtained on the third step and the first step, i.e., $\tilde{L}_{m}(x)-L(x)$. A high $\tilde{L}_{m}(x)-L(x)$ indicates the branch to replaced hidden state is important on affect the adversarial loss. By averaging the values $\tilde{L}_{m}(x)-L(x)$ over a collection of adversarial prompts, we can get the average effects of residual module and skip connection in the $m$-th block. In Figure 4, we show the average effects of residual modules and skip connections. The adversarial promp are obtained by performing GCG attack against Llama-2-7B-Chat model on AdvBench. It can be seen that the skip connections show much greater effects than residual modules on the adversarial loss, which confirms the conjecture. The observation further implies that the effects of adversarial information primarily propagate through the skip connection branch within each residual block. This observation, alongside the superior gradient-based attack performance of LSGM, further suggests that certain discrete optimization challenges within LLMs, e.g., prompt tuning, might be more effectively addressed by understanding the information flow throughout the forward pass.

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-06.jpg?height=496&width=1361&top_left_y=251&top_left_x=382)

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-06.jpg?height=426&width=610&top_left_y=264&top_left_x=389)

(a) $h_{r}$ and $h_{r, n}$

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-06.jpg?height=428&width=723&top_left_y=263&top_left_x=1010)

(b) $h_{r, o}$

Figure 5: Negative average PCCs (with higher values indicating stronger negative correlations) computed on (a) the entire intermediate representation $h_{r}$ and the last adversarial prompt token representation $h_{r, n}$, and (b) the token representations $h_{r, o}$, at the $r$-th layer. Best viewed in color.

### 3.3 Adapting Intermediate Level Attack for Gradient-base Attacks Against LLMs

Intermediate level attacks [16, 12, 43, 51,22] opt to maximizing the scalar projection of the intermediate level representation onto a "directional guide" for generating non-target adversarial examples to attack image classification models. As a representative method, ILA [16] defines the directional guide as the intermediate level representation discrepancy between the adversarial example obtained by a preliminary attack, e.g., I-FGSM [20], and corresponding benign example. In this section, we first examine whether the strategy of ILA can be directly applied to the gradient-based attacks on LLMs. Then, by seeing the failure of direct application, we investigate the proper way to adapt this strategy to the gradient-based attacks on LLMs.

Let $h_{r}$ be the intermediate representation at the $r$-th layer, and the directional guide $v_{r}=\hat{h}_{r}^{t}-\hat{h}_{r}^{0}$ is obtained by a $t$-iterations preliminary attack (e.g., GCG). To directly apply ILA, we maximize the scalar projection of the intermediate representation $h_{r}$ onto the directional guide, i.e., maximize $L_{\text {ILA }}(x)=v_{r}^{T} h_{r}$. We obtain the directional guide from the GCG attack and then evaluate the direct application of ILA using the GCG attack as the baseline. We find that it exhibits deteriorated performance compared with GCG. Specifically, it achieves a match rate of $44 \%$, while the original GCG obtains $54 \%$ match rate. This result demonstrates that the ILA fails to be directly applied to gradient-based adversarial prompt generation.

Now, let us discuss the reason why directly introducing ILA fails to improve the performance. Recall that the intermediate level attacks against image classification models essentially assume that the scalar projection of intermediate representation onto the directional guide has a positive correlation with the transferability of adversarial examples, which means that the larger the scalar projection obtained, the more transferability (higher classification loss on victim models) the adversarial example achieved [22]. Back to our setting, direct application of ILA requires that there is a negative correlation between the scalar projection of the intermediate representation onto the directional guide and the adversarial loss (since we aim to minimize the adversarial loss in contrast to the setting of non-target attack against image classification model). We conduct experiments to verify whether this assumption holds in attacking LLMs. We use Pearson's correlation coefficient (PCC) to show the correlation between them. With a range in $[-1,1]$, PCC close to 1 indicates a positive correlation, and PCC close to -1 means a negative correlation. The experiment is performed with following steps. Firstly, we perform GCG attack to obtain an adversarial example and a directional guide $v_{r}$. Next, we randomly alter adversarial tokens several times and pass these new prompts into the model to collect a set of scalar projections paired with their corresponding adversarial losses, denoted as $\left\{\left(v_{r}^{T} \tilde{h}_{r}^{c^{*}}, L\left(\tilde{x}^{c^{*}}\right)\right\}\right.$. Then we can calculate a PCC between the scalar projections and loss values on this set. By doing this experiment on different examples, we can obtain the average PCC. In Figure 5(a), we show the negative average PCCs (with higher values indicating stronger negative correlations) between the scalar projection and the adversarial loss at different layers. The negative average PCCs are only between 0.4 and 0.5 , showing low correlation between them. It confirms that the failure to apply ILA directly to gradient-based attacks against LLMs is due to the low correlation between the scalar projection of the intermediate representation onto the directional guide and the adversarial loss.

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-07.jpg?height=425&width=1395&top_left_y=232&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-07.jpg?height=403&width=390&top_left_y=243&top_left_x=369)

Figure 6: An example of the internal computation of an LLM. The $\ell_{o}$ represents cross-entropy loss.

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-07.jpg?height=322&width=485&top_left_y=259&top_left_x=793)

(a) Loss

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-07.jpg?height=323&width=469&top_left_y=256&top_left_x=1278)

(b) Match rate

Figure 7: How (a) the loss and (b) the match rate changes with attack iterations. The attacks are performed against Llama-2-7B-Chat model to generate query-specific adversarial suffixes on AdvBench. Best viewed in color.

This phenomenon is distinct from the one observed in the setting of attacking image classification models [22].

Nevertheless, by unfolding the internal computation of an LLM as a grid of token representations $\left\{h_{r, o}\right\}$, where $h_{r, o}$ denotes the $o$-th token representation at $r$-th layer, as depicted in Figure 6 some previous work [29, 30, 9, 32] have shown that different token representations at a middle layer show distinct effects on the model output. It inspires us to investigate whether the negative correlation exists between the scalar projection of a single token representation $h_{r, o}$ onto the corresponding directional guide $v_{r, o}$ and the adversarial loss. Note that during the optimization, the input is a concatenation of the adversarial prompt tokens $x_{1: n}$ and the target tokens $x_{n+1: n^{*}}$, and the adversarial loss is the average over $n^{*}-n$ cross-entropy losses. Consider $L(x)=1 /\left(n^{*}-n\right) \sum_{o=n}^{n^{*}-1} \ell_{o}$, where $\ell_{o}=-\log p_{f}\left(x_{o+1} \mid x_{1: o}\right)$, for a token representation of $h_{r, o^{*}}, o^{*} \in\left\{n, \ldots, n^{*}-1\right\}$, it only affects the loss values of $\left\{\ell_{o^{*}}, \ldots, \ell_{n^{*}-1}\right\}$, hence we only average these loss values as the corresponding adversarial loss to evaluate the correlation. Since the numbers of target tokens are different across different adversarial prompts, we select the first 10 tokens from the target tokens to compute the PCCs. In Figure 5(b), we show the negative average PCCs corresponding to single token representations $\left\{h_{r, n-|\mathcal{A}|+1}, \ldots, h_{r, n^{*}-1}\right\}$ at different middle layers. Interestingly, we can see a high negative correlation between the scalar projection of the last adversarial prompt token representation onto the directional guide, i.e., $v_{r, n}^{T} h_{r, n}$, and the adversarial loss, i.e., $L(x)$, at some middle layers. To clearly demonstrate the difference with the scalar projection of the entire representation onto the directional guide and the adversarial loss, we compare them in Figure 5(a) It can be seen that from the 8-th layer onward, the negative average PCCs of $h_{r, n}$ become larger than those of $h_{r}$ and then exceed 0.7 starting from the 16 -th layer, showing a high negative correlation. We conjecture that this phenomenon is attributed to two factors. Firstly, the last adversarial prompt token representation contains information from the entire adversarial prompt, making it more representative than the preceding token representations. Secondly, since the last adversarial prompt token representation corresponds to the generation of the first target token, which is crucial for the generation of subsequent target tokens [2], the changes in the losses of subsequent target tokens are primarily affected by the changes in the last adversarial prompt token representation.

Based on the observation, we adapt the strategy of ILA for use in gradient-based attacks adversarial prompt generation by maximizing the scalar projection of the last adversarial prompt token representation onto the directional guide. That is, we opt to maximize $L_{\mathrm{ILA}^{\dagger}}(x)=v_{r, n}^{T} h_{r, n}$. Another issue of the original ILA here is that it requires a preliminary attack, which is time-consuming to obtain in the setting of attacking LLMs. We make a compromise by utilizing the current adversarial prompt to obtain the directional guide, which leads to no increase in computational complexity. Note that we only modify the gradient computation step in each iteration. Therefore, the step of evaluating candidate adversarial suffixes can help reduce the adversarial loss, providing useful directional guidance in the initial iterations. We denote this adaptation of ILA as Language ILA (LILA) and evaluate its performance using GCG and AutoPrompt as baseline attacks against the Llama-2-7B-Chat model on AdvBench. As shown in Figure 7, LILA demonstrates a lower adversarial loss and a higher match rate compared to the baseline attacks. The experimental results suggest that our adaptation of ILA indeed improves discrete optimization in gradient-based adversarial prompt generation and offers insights to more effectively address similar discrete optimization problems within LLMs.

### 3.4 Combinations and Balance of Gradient Magnitude in Combinations

In the previous sections, we have confirmed the success of LSGM and LILA. To further improve the performance of discrete optimization using previous gradient-based attacks on LLMs, we then discuss possible approach to combining these strategies.

We first try to combine the LILA loss and the cross-entropy loss by introducing a scaling factor $\beta \geq 0$ which is used to balances the magnitude of the two loss terms, i.e., we minimize a combination loss $L_{\mathrm{CE}}-\beta L_{\mathrm{ILA}^{\dagger}}$. With this combination loss, the gradient w.r.t. $h_{r}$ can be written as $\left[\nabla_{h_{r, 1}} L_{\mathrm{CE}}(x), \ldots, \nabla_{h_{r, n}} L_{\mathrm{CE}}(x)-\beta v_{r}, \ldots, \nabla_{h_{r, n^{*}-1}} L_{\mathrm{CE}}(x)\right]$. It may result in excessively larger or smaller gradient flow through the last adversarial prompt token representation $h_{r, n}$ than other token representations, leading to overestimation or underestimation of the impact on adversarial loss resulting from the changes of the last adversarial prompt token representation. We alleviate the issue by re-normalizing the gradient w.r.t. the last adversarial prompt token representation, i.e., propagating $\alpha\left(\nabla_{h_{r, n}} L_{\mathrm{CE}}(x)-\beta v_{r}\right)$ back to lower layers instead of using the original gradient, in which $\alpha=\left\|\nabla_{h_{r, n}} L_{\mathrm{CE}}(x)\right\|_{2} /\left\|\nabla_{h_{r, n}} L_{\mathrm{CE}}(x)-\beta v_{r}\right\|_{2}$. When we set $\beta \rightarrow \infty$, it can be approximately regarded as replacing the gradient w.r.t. $h_{r, n}$ with $-\alpha v$, and when we set $\beta=0$, it is equivalent to only using the cross-entropy loss.

We denote such a combination of the LILA loss and the cross-entropy loss as LILA $^{\dagger}$. With LILA ${ }^{\dagger}$, it is natural to further combine it with LSGM and obtain LSGM-LILA ${ }^{\dagger}$. We evaluate using GCG and AutoPrompt as baseline attacks and report attack performance against Llama-2-7B-Chat on AdvBench, in the task of query-specific adversarial suffix generation. For simplicity, we can set $\beta \rightarrow \infty$ and adopt the aforementioned approximation for $\mathrm{LILA}^{\dagger}$. The results are shown in Table 8. It can be seen that by combining with the cross-entropy loss, LILA $^{\dagger}$ outperforms LILA, while LSGM-LILA ${ }^{\dagger}$ achieves even higher match rates, demonstrating that the combination leads to obvious further improvement.

![](https://cdn.mathpix.com/cropped/2024_06_04_270d290a5cd232cd2055g-08.jpg?height=394&width=612&top_left_y=930&top_left_x=1125)

(a) GCG

(b) AutoPrompt (AP)

Figure 8: Evaluating our combination method using (a) GCG and (b) AutoPrompt (AP) as baseline attacks. Best viewed in color.

## 4 Experiments

We introduce the evaluation metrics in Section 4.1, and then present the experimental results in Sectior 4.2. Some detailed experimental settings and ablation studies are presented in the Appendix.

### 4.1 Metrics

We use two metrics for the evaluations: match rate (MR) and attack success rate (ASR). The match rate counts the fraction of adversarial examples that make the output exactly match the target string, and was used to evaluate different optimization methods in the paper of GCG [52]. The attack success rate is evaluated using the evaluator proposed by HarmBench [27], which achieves over $93 \%$ human agreement rate as reported in their paper. We set the models to generate 512 tokens with greedy decoding during the evaluation phase. Note that for the universal adversarial suffix generation, we observed that the ASRs of the adversarial suffixes obtained by multiple runs for the same method differ significantly. Hence, we run each method ten times and report not only the average ASR (AASR) but also the best ASR (BASR) and the worst ASR (WASR).

### 4.2 Experimental Results

Following the evaluations by GCG [52], we perform evaluations in the settings of query-specific adversarial suffix generation and universal adversarial suffix generation. For query-specific adversarial suffix generation, following [52], we use first 100 harmful behaviors in AdvBench. For universal adversarial suffix generation, we use the first 10 harmful queries in AdvBench to generate a universal adversarial suffix and test it on the rest 510 harmful queries in AdvBench. We use a Top- $k$ selection of 4 and a candidate set size of 20 for all methods at each iteration. Since Llama-2-Chat [41] models

Table 1: Match rates, attack success rates, and time costs for generating query-specific adversarial suffixes on AdvBench are shown. The symbol * indicates the use of the default setting for GCG, i.e., using a Top- $k$ of 256 and a candidate set size of 512. Time cost is derived by generating a single adversarial adversarial suffix on a single NVIDIA V100 32GB GPU.

| Method | Llama-2-7B-Chat |  | Llama-2-13B-Chat |  | Mistral-7B-Instruct |  | Time $\cos 1$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{M R}$ | ASR | $\overline{M R}$ | ASR | $\overline{M R}$ | ASR |  |
| $\mathrm{GCG}^{*}$ | $60 \%$ | $44 \%$ | $58 \%$ | $40 \%$ | $\mathbf{9 5 \%}$ | $92 \%$ | $85 \mathrm{~m}$ |
| GCG | $54 \%$ | $38 \%$ | $37 \%$ | $33 \%$ | $73 \%$ | $74 \%$ | $3 \mathrm{~m}$ |
| GCG-LSGM | $72 \%$ | $62 \%$ | $52 \%$ | $43 \%$ | $93 \%$ | $88 \%$ | $3 \mathrm{~m}$ |
| GCG-LILA | $62 \%$ | $53 \%$ | $38 \%$ | $36 \%$ | $75 \%$ | $78 \%$ | $3 \mathrm{~m}$ |
| GCG-LILA $^{\dagger}$ | $70 \%$ | $59 \%$ | $52 \%$ | $48 \%$ | $83 \%$ | $80 \%$ | $3 \mathrm{~m}$ |
| GCG-LSGM-LILA ${ }^{\dagger}$ | $87 \%$ | $\mathbf{6 8 \%}$ | $62 \%$ | $\mathbf{5 2 \%}$ | $94 \%$ | $\mathbf{9 3 \%}$ | $3 \mathrm{~m}$ |

show great robust performance to gradient-based adversarial prompt generation [27], we mainly evaluate the methods on the model of Llama-2-7B-Chat [41] and the model of Llama-2-13B-Chat [41]. In addition, the model of Mistral-7B-Instruct-v0.2 [17] is also considered.

The comparison results in the setting of query-specific adversarial suffix generation are shown in Table 1. Experimental results demonstrate that both LSGM and LILA outperform the GCG attack on three safety-aligned LLMs. Our combination method further boosts both the match rates and the attack success rates, achieving the best performance. Specifically, GCG-LSGM-LILA ${ }^{\dagger}$ achieves gains of $+30 \%,+19 \%$, and $+19 \%$ when attacking the Llama-2-7B-Chat and Llama-2-13B-Chat, and Mistral-7B-Instruct, respectively. Moreover, since these methods reduce the gap between the input gradients and the effects of loss change results from token replacements, the size of candidate set at each iteration can be reduced for saving running time. We show the results of GCG with the default setting described in their paper, which evaluates 512 candidate adversarial suffixes at each iteration. It can be seen that our combination still shows outstanding performance, by only using $4 \%$ time cost compared with the GCG with their initial setting (denoted as $\mathrm{GCG}^{*}$ in the table).

Table 2: Attack success rates for generating universal adversarial suffixes on AdvBench. The average ASR (AASR), the worst ASR (WASR), and the best ASR (BASR) are obtained by performing each attack ten times.

| Model | GCG |  |  |  | GCG-LSGM-ILA $^{\dagger}$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | AASR | WASR | BASR |  | AASR | WASR | BASR |
| Llama-2-7B-Chat | $26.68 \%$ | $0.40 \%$ | $55.80 \%$ |  | $\mathbf{6 0 . 3 2 \%}$ | $\mathbf{3 5 . 2 2 \%}$ | $\mathbf{8 6 . 8 6 \%}$ |
| Llama-2-13B-Chat | $20.98 \%$ | $0.00 \%$ | $37.06 \%$ |  | $\mathbf{4 5 . 2 7 \%}$ | $\mathbf{7 . 4 5 \%}$ | $\mathbf{6 7 . 2 5 \%}$ |
| Mistral-7B-Instruct | $56.53 \%$ | $34.51 \%$ | $92.16 \%$ |  | $\mathbf{7 3 . 4 8 \%}$ | $\mathbf{5 0 . 8 0 \%}$ | $\mathbf{9 2 . 2 5 \%}$ |

The results of the attacks in the setting of universal adversarial suffix generation are shown in Table 2 It can be observed that our combination not only achieves a remarkable improvement in the average ASR but also enhances both the worst and best ASRs obtained over 10 runs. Specifically, when attacking Llama-2-7B-Chat model, the GCG-LSGM-LILA ${ }^{\dagger}$ achieves an average ASR of $60.32 \%$, which gains a $+33.64 \%$ improvement compared with the GCG attack. Moreover, for the worst and best ASR, GCG-LSGM-LILA ${ }^{\dagger}$ achieves gains of $+34.82 \%$ and $+31.06 \%$, respectively.

## 5 Conclusions

In this paper, we present a new perspective on the discrete optimization problem in gradient-based adversarial prompt generation. That is, using gradient w.r.t. the input to reflect the change in loss that results from token replacement resembles using input gradient calculated on the substitute model to indicate the real effect of perturbing inputs on the prediction of a black-box victim model, which has been studied in transfer-based attacks against image classification models. By making some appropriate adaptations, we have appropriated the ideologies of two transfer-based methods, namely, SGM and ILA, into the gradient-based white-box attack against LLMs. Our analysis of the mechanisms behind their effective performance has provided new insights into solving discrete optimization problem within LLMs. Furthermore, we have developed an appropriate combination method to further enhance the discrete optimization in gradient-based adversarial prompt generation. Experimental results demonstrate that our combination method significantly improves the attack success rate.

## References

[1] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[2] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems, 36, 2024.

[3] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442, 2023.

[4] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.

[5] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023.

[6] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In CVPR, 2019.

[7] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In CVPR, 2018.

[8] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751, 2017.

[9] Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscope: A unifying framework for inspecting hidden representations of language models. arXiv preprint arXiv:2401.06102, 2024.

[10] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.

[11] Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon, and Koushik Sen. Lgv: Boosting adversarial example transferability from large geometric vicinity. arXiv preprint arXiv:2207.13129, 2022.

[12] Chuan Guo, Jacob R Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q Weinberger. Simple black-box adversarial attacks. arXiv preprint arXiv:1905.07121, 2019.

[13] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. arXiv preprint arXiv:2104.13733, 2021.

[14] Yiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves transferability of adversarial examples. In NeurIPS, 2020.

[15] Yiwen Guo, Qizhang Li, Wangmeng Zuo, and Hao Chen. An intermediate-level attack framework on the basis of linear regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[16] Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. Enhancing adversarial example transferability with an intermediate level attack. In ICCV, 2019.

[17] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023

[18] Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. In International Conference on Machine Learning, pages 15307-15329. PMLR, 2023.

[19] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506-17533. PMLR, 2023.

[20] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In ICLR, 2017.

[21] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.

[22] Qizhang Li, Yiwen Guo, Wangmeng Zuo, and Hao Chen. Improving adversarial transferability via intermediate-level perturbation decay. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[23] Qizhang Li, Yiwen Guo, Wangmeng Zuo, and Hao Chen. Making substitute models more bayesian can enhance transferability of adversarial examples. In International Conference on Learning Representations, 2023.

[24] Qizhang Li, Yiwen Guo, Wangmeng Zuo, and Hao Chen. Towards evaluating transfer-based attacks systematically, practically, and fairly. arXiv preprint arXiv:2311.01323, 2023.

[25] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023.

[26] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In ICLR, 2017.

[27] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024.

[28] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023.

[29] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.

[30] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. arXiv preprint arXiv:2210.07229, 2022.

[31] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, and Fatih Porikli. On improving adversarial transferability of vision transformers. arXiv preprint arXiv:2106.04169, 2021.

[32] nostalgebraist. interpreting gpt: the logit lens. LessWrong, 2020.

[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[34] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.

[35] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.

[36] Rusheb Shah, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando, et al. Scalable and transferable black-box jailbreaks for language models via persona modulation. arXiv preprint arXiv:2311.03348, 2023.

[37] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023.

[38] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.

[39] Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre Araujo. Pal: Proxy-guided black-box attack on large language models. arXiv preprint arXiv:2402.09674, 2024.

[40] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.

[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[42] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125, 2019.

[43] Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan Qin, and Kui Ren. Feature importanceaware transferable adversarial attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7639-7648, 2021.

[44] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does 1lm safety training fail? Advances in Neural Information Processing Systems, 36, 2024.

[45] Zeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context demonstrations. arXiv preprint arXiv:2310.06387, 2023.

[46] Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, and Yu-Gang Jiang. Towards transferable adversarial attacks on vision transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.

[47] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36, 2024.

[48] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Rethinking the security of skip connections in resnet-like neural networks. In ICLR, 2020.

[49] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. Improving transferability of adversarial examples with input diversity. In CVPR, 2019.

[50] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.

[51] Jianping Zhang, Weibin Wu, Jen-tse Huang, Yizhan Huang, Wenxuan Wang, Yuxin Su, and Michael R Lyu. Improving adversarial transferability via neuron attribution-based attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14993-15002, 2022.

[52] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
