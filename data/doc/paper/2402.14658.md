# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement 

Tianyu Zheng ${ }^{1 *}$, Ge Zhang ${ }^{1,2 *}$, Tianhao Shen ${ }^{1 *}$, Xueling Liu $^{1}$,<br>Bill Yuchen Lin ${ }^{3}$, Jie Fu ${ }^{1,4}$, Wenhu Chen ${ }^{1,2}$, Xiang Yue ${ }^{1,5 \dagger}$<br>${ }^{1}$ Multimodal Art Projection Research Community, ${ }^{2}$ University of Waterloo,<br>${ }^{3}$ Allen Institute for Artificial Intelligence, ${ }^{4} \mathrm{HKUST},{ }^{5} \mathrm{IN}$.AI Research<br>\{zhengtianyu0428, xiangyue.work\}@gmail.com, ge.zhang@uwaterloo.ca<br>https://opencodeinterpreter.github. io


#### Abstract

The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by CodeFeedback, a dataset featuring $68 \mathrm{~K}$ multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.


## 1 Introduction

Code generation has been a pivotal challenge within computer science for several decades. Recently, the landscape of code generation has been revolutionized by the advent of large language models (LLMs) pre-trained on extensive code corpora (Nijkamp et al., 2022; Christopoulou et al., 2022; Zheng et al., 2023; Li et al., 2023a; Wang et al., 2023c; Roziere et al., 2023; Guo et al., 2024). These models have showcased remarkable capabilities in generating code that accurately aligns with[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_33d6a508028b66d8423fg-01.jpg?height=1413&width=780&top_left_y=824&top_left_x=1049)

Figure 1: Overview of the OpenCodeInterpreter and its pass@1 accuracy on the HumanEval. With appropriate feedback, OpenCodeInterpreter-33B achieves performance comparable to that of the GPT-4 Code Interpreter.

user intents, thus providing substantial support for software development (GitHub, 2023).

To unleash the capabilities of pre-trained code models, instruction-tuning methods have been de-
veloped. For instance, CodeAlpaca (Chaudhary, 2023) comprises $20 \mathrm{~K}$ code instructions automatically generated by applying self-instruct (Wang et al., 2023b) to ChatGPT, utilizing 21 seed tasks as the foundation. To further refine the coding proficiency of LLMs, Luo et al. (2023) introduces Code Evol-Instruct, a method that applies a variety of heuristics to enrich the complexity of initial code instructions, building upon the dataset provided by CodeAlpaca. Meanwhile, MagicCoder (Wei et al., 2023) employs a robust LLM to generate novel coding challenges, sourcing inspiration from a diverse range of open-source code snippets. Additionally, WaveCoder (Yu et al., 2023) implements an LLM generator-discriminator framework for creating code instruction data, offering customization and control over the data generation process.

Despite these advancements, current code models are constrained by their capacity to utilize feedback for refinement. Essentially, feedback can have two forms: (1) execution feedback, which includes execution outputs and diagnostics, and (2) human feedback, comprising follow-up guidance or instructions from users. Execution feedback plays a vital role in enabling models to rectify syntactic and logical errors, and human feedback aids models in better understanding user instructions, facilitating the generation of solutions that more closely align with user expectations.

To address these challenges, we propose OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. OpenCodeInterpreter is trained on our constructed Code-Feedback dataset, which features $68 \mathrm{~K}$ multi-turn interactions between users, code models, and compilers. OpenCodeInterpreter uniquely integrates both execution and human feedback, employing compiler diagnostics to rectify errors and human insights to refine code generation. This approach allows OpenCodeInterpreter to produce solutions that are both technically sound and closely matched to user requirements, significantly boosting its overall performance.

Our thorough evaluation of OpenCodeInterpreter on widely recognized benchmarks, such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and their augmented counterparts from EvalPlus (Liu et al., 2023), highlights its superior ability to generate and iteratively refine code, achieving exemplary standards of quality and functionality. Remarkably, OpenCodeInterpreter-33B secures an impressive accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, showcasing performance on par with GPT4's 84.2 (76.2). Furthermore, when augmented with synthesized human feedback from GPT-4, OpenCodeInterpreter's performance notably increases to 91.6 (84.6). OpenCodeInterpreter thereby establishes a new benchmark in code generation, effectively narrowing the performance gap between open-source models and sophisticated proprietary systems like the GPT-4 Code Interpreter.

## 2 Code-Feedback

In this section, we detail the creation of our code instruction tuning dataset, Code-Feedback (Figure 2), designed to train OpenCodeInterpreter. CodeFeedback is crafted to meet specific criteria: 1) Diverse and challenging real-world queries: The dataset should encompass a wide range of queries derived from real-world coding tasks, presenting both diversity and complexity. 2) Multi-turn dialogue structure: Code-Feedback is structured as multi-turn dialogues, incorporating two types of feedback: execution feedback, which includes outputs and diagnostics from compilers, and human feedback, consisting of additional guidance or instructions from users. 3) Interleaved text and code responses: Each response is expected to provide responses that blend natural language explanations with code snippets, offering a holistic approach to solving coding queries.

To assemble a dataset that fulfills these desiderata, we have employed five distinct methods. Examples of these five categories can be found in Appendix E. The sources of our queries fall into two main categories: a variety of open-source datasets and coding challenges from LeetCode. In the next subsections, we will discuss how we develop data construction methods to meet the three aforementioned criteria from the two data sources.

### 2.1 Coding Queries from Open-source Data

We have aggregated $287 \mathrm{k}$ queries from four distinguished open-source code instruction tuning datasets: Magicoder-OSS-Instruct ${ }^{1}$, Python code subset of ShareGPT ${ }^{2}$, Magicoder-Evol-Instruct ${ }^{3}$, and Evol-Instruct-Code ${ }^{4}$. To refine this extensive[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_33d6a508028b66d8423fg-03.jpg?height=537&width=648&top_left_y=237&top_left_x=241)

| Dataset | \#Sample | \#Turn | M.T | E.F | H.F |
| :--- | :---: | :---: | :---: | :---: | :---: |
| CodeAlpaca $^{0}$ | $20 \mathrm{k}$ | $20 \mathrm{~K}$ | $x$ | $x$ | $x$ |
| Magicoder-OSS-Instruct $^{1}$ | $75 \mathrm{~K}$ | $75 \mathrm{~K}$ | $x$ | $x$ | $x$ |
| Python-Code-ShareGPT $^{2}$ | $23 \mathrm{~K}$ | $23 \mathrm{~K}$ | $x$ | $x$ | $x$ |
| Magicoder-Evol-Instruct $^{3}$ | $111 \mathrm{~K}$ | $111 \mathrm{~K}$ | $x$ | $x$ | $x$ |
| EvolInstruct-Code $^{4}$ | $80 \mathrm{k}$ | $80 \mathrm{~K}$ | $x$ | $x$ | $x$ |
| Code-Feedback (Ours) | $68 \mathrm{~K}$ | $192 \mathrm{~K}$ | $\checkmark$ | $\checkmark$ | $v$ |
| Single-turn Packing | $16 \mathrm{~K}$ | $33.5 \mathrm{~K}$ |  | $x$ | $\checkmark$ |
| Interaction Simulation | $51 \mathrm{~K}$ | $155.5 \mathrm{~K}$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Code Correction | $0.5 \mathrm{~K}$ | $1.2 \mathrm{~K}$ | $\checkmark$ | $\checkmark$ | $x$ |
| LeetCode Similar Problem | $0.3 \mathrm{~K}$ | $0.65 \mathrm{~K}$ | $\checkmark$ | $x$ | $\checkmark$ |
| LeetCode Follow-Up | $0.2 \mathrm{~K}$ | $0.76 \mathrm{~K}$ | $\checkmark$ | $x$ | $\checkmark$ |

Figure 2: Summary of our proposed dataset Code-Feedback construction and comparison with existing code instruction tuning datasets. M.T: Multi Turn, E.F: Execute Feedback, H.F: Human Feedback.

collection and isolate the most intricate and informative instructions, we employ a very capable open-source chat model, Qwen-72B-Chat (Bai et al., 2023), for a selective filtering process. This involves the LLM assessing each code query and its corresponding response within the compiled datasets on a complexity score from 1 to 5 . Only the most challenging queries, with ratings of 4 or 5 , were retained for our seed set, ensuring a focus on the most difficult instructions. To guarantee the robustness of our selection, this filtering operation is repeated with two distinct prompts (detailed in Appendix A), thereby solidifying the complexity of our final query selection. This meticulous process resulted in $156 \mathrm{k}$ high-quality single-turn code instructions as the challenging query pool. Detailed statistics of this data compilation are provided in Appendix A.

Subsequently, we describe three methods employed to transform this curated single-turn data into multi-turn dialogues enriched with both execution and human feedback.

Singe-turn Packing. A direct approach to crafting multi-turn data is to group single-turn queryresponse pairs into multi-turn formats. Inspired by in-context pre-training techniques (Shi et al., 2023), which consolidate similar sequences to foster model learning of dependencies among related documents, we merge similar single-turn queryresponse pairs to form multi-turn dialogues.

Utilizing the BERT-base embedding (Devlin et al., 2019), we convert queries into vectorized representations. For each query, the $k$-nearest neighbors algorithm is employed to identify its four closest counterparts. From these, we randomly select two or three to assemble multi-turn sequences. To maintain data uniqueness, once a query is chosen as a neighbor, it is exempt from future selections as a neighboring query, ensuring no single instruction is repeated across the dataset. Should a query's potential neighbors have been previously utilized, that query is bypassed. This method results in the creation of $16.6 \mathrm{~K}$ multi-turn instances derived from $105 \mathrm{~K}$ single-turn instances.

Interaction Simulation. Gathering authentic human interaction data poses significant challenges. To replicate a realistic code interpreter usage scenario, we developed a simulator using GPT-3.5 and GPT-4. For each selected query, GPT-3.5 first generates a preliminary response from which we extract the code snippet and execute it. The outcome of this execution, along with any compiler diagnostics, is then fed into GPT-4 to elicit a follow-up response. This cycle is repeated until GPT-4 delivers what it deems a correct solution or until a maximum of three iterations is reached.

Subsequently, we introduce simulated human feedback into the interaction. We predefine ten common feedback categories, including issues related to syntax and formatting, efficiency, functionality, clarity, bugs, security, compatibility, resource use, scalability, and best practices, with each category detailed in Appendix B. GPT-4 is then prompted to select the most relevant feedback for the scenario and generate appropriate responses within that feedback category. By incorporating this simulated feedback into the dialogue history, GPT-4 is encouraged to refine its solutions further, mimicking intricate user-model exchanges and demonstrating self-correction in response to human input. Through this simulation approach, we have constructed $51 \mathrm{~K}$ examples, effectively cap-
turing the nuanced dynamics of user interactions and feedback-driven solution refinement.

Code Correction. To boost the model's errorhandling capabilities, we include a focused stage in our data compilation that generates 500 specific error correction interactions. We initiate this by prompting GPT-4 to intentionally produce incorrect code snippets, as outlined in Appendix B. The model then uses the error messages from executing these snippets as cues for corrections. This approach mirrors the real-life coding cycle, where developers continuously debug and refine their code, thus enriching our dataset with a broad spectrum of error correction examples. Following this, we replace the initial prompts that resulted in incorrect code with the ones that encourage the generation of correct code outputs. This method ensures the model learns from both successful code generation and error identification and correction, significantly enhancing its problem-solving skills and understanding of the debugging process.

### 2.2 Coding Challenges from LeetCode

LeetCode Similar Problem. Drawing inspiration from the practice among programmers of honing their skills through LeetCode challenges, we gather similar LeetCode questions and their solutions from the TACO dataset (Li et al., 2023b). LeetCode ${ }^{5}$ categorizes related questions through tags, facilitating the extraction of connected problems. TACO ensures the LeetCode dataset is cleansed to prevent any unintended impact on other task datasets, such as HumanEval and MBPP. By amalgamating associated LeetCode questions, we compile 303 multi-turn instances, enriching the dataset with varied coding challenges.

LeetCode Follow-up Question. We further delve into the LeetCode dataset to isolate solutions to identical questions that differ in time or space complexity or are implemented in various programming languages. This process of aggregating diverse solutions to the same LeetCode questions yields 200 multi-round instances, showcasing alternative problem-solving approaches.

Given the original LeetCode solutions often lack comprehensive natural language explanations, we engage GPT-4 to enhance these solutions with integrated text explanations and code snippets, standardizing all instances into a consistent format. The specific prompts used to guide GPT-4 in this enrich-[^2]

ment process are detailed in Appendix C, ensuring clarity and educational value in the responses.

## 3 Experimental Setup

Training Setup. We select two capable base models CodeLlama (Roziere et al., 2023) and DeepSeekCoder (Guo et al., 2024) varying capacities to illustrate the dataset's universal applicability and benefits across different scales (7B, 13B, 34B, 70B). We maintain uniform hyperparameter configurations across all models. We fine-tune the base models for 3 epochs. The learning rate is set as $2 \mathrm{e}-5$ with a 0.05 warm-up ratio and a cosine scheduler. We impose a token cutoff length of 4096 to maintain consistency in the input size.

To optimize the fine-tuning process, we strategically combine high-quality single-turn data from the WizardCoder 110k dataset with our CodeFeedback at a ratio of $2: 1$. Blending with singleturn high-quality data may further boost the coding ability. This blend is carefully selected and more details are discussed in Table 2.

Evaluation Setup. Our evaluation framework primarily leverages HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), two benchmarks renowned for their rigorous testing of code generation capabilities. Acknowledging the limitations of their original test suites in covering all edge cases (Liu et al., 2023), we further incorporate their extended versions, HumanEval+ and MBPP+, utilizing the EvalPlus framework (Liu et al., 2023) for a more comprehensive assessment.

In line with best practices outlined in recent studies (Liu et al., 2023; Chen et al., 2023), OpenCodeInterpreter's solutions are generated via greedy decoding. For comparisons involving GPT3.5 Turbo (OpenAI, 2022) and GPT-4 Turbo (OpenAI, 2023), we maintain a temperature setting of 0 . EvalPlus's unified sanitizer tool post-processes these solutions, which are then evaluated across the four benchmarks using EvalPlus's toolset.

For single-turn code generation, we craft a simple instruction to encapsulate the original prompt, forming a new input for the model. The exact prompts are detailed in Appendix D, and we assess the model's performance using the pass @ 1 metric, as per EvalPlus's guidelines.

Our analysis extends to multi-turn pass rates to explore OpenCodeInterpreter's proficiency in refining code through iterative feedback. This aspect of the evaluation draws on execution results

![](https://cdn.mathpix.com/cropped/2024_06_04_33d6a508028b66d8423fg-05.jpg?height=2302&width=1614&top_left_y=231&top_left_x=238)

Table 1: Pass@ 1 accuracy of different code models on HumanEval (+), MBPP (+) and their average (+). 'CL': based on CodeLlama; 'DS': based on DeepseekCoder. Baseline results are copied from the EvalPlus Leaderboard or replicated by running the official checkpoints. We highlight strong baselines and our methods for each scale.
and synthetic human feedback, generated by GPT4 (OpenAI, 2023), to simulate real-world coding scenarios and interactions. Specifically, the multiturn evaluation encompasses three scenarios, offering a holistic view of OpenCodeInterpreter's capabilities in dynamic code refinement:

- Execution Feedback: Here, OpenCodeInterpreter independently leverages execution outcomes and compiler diagnostics to pinpoint and correct errors, mirroring a developer's process of refining code based on direct execution feedback.
- Synthetic Human Feedback: In this scenario, GPT-4 generates feedback that mimics human input by considering the task description, initial model response, and any execution feedback. This tests OpenCodeInterpreter's adaptability to nuanced, human-like feedback, reflecting realworld developer or user interactions.
- Synthetic Human Feedback (Oracle): Building on the previous scenario, GPT-4 also accesses the ground-truth solution, offering insight into OpenCodeInterpreter's optimal performance in code refinement when guided by precise feedback.

For each task, the code generation and evaluation process concludes either when the model's solution successfully passes the evaluation or when it reaches the set maximum of two rounds. If a code sample fails the evaluation, both the solution and the test results are reincorporated into the prompt for refinement. The evaluation identifies three principal scenarios for non-passing outcomes: 1) Exception Handling: Captures and relays any exceptions or errors encountered during execution as error messages, providing direct feedback for correction. 2) Not-Expected: In instances where outputs deviate from expected results, the model receives feedback including test inputs, expected outputs, and actual outputs, highlighting the discrepancy. 3) Timeout Handling: Implements a timeout threshold to prevent evaluation delays from solutions with excessive or infinite runtimes. Exceeding this threshold triggers an "Execution timed out" notification.

## 4 Main Results

This section reports OpenCodeInterpreter and baselines in single-turn and multi-turn code generation settings. The results are in Table 1.

### 4.1 Results of Single-turn Code Generation

We compare OpenCodeInterpreter's single-turn code generation performance against premier models such as GPT-3.5/4-Turbo (OpenAI, 2022, 2023), CodeLlama-Python (Roziere et al., 2023), WizardCoder (Luo et al., 2023), Deepseek-Coder (Guo et al., 2024), CodeT5+ (Wang et al., 2023c) across different scales. Leveraging data from the EvalPlus leaderboard as of February 10th, 2024, we examine OpenCodeInterpreter's achievements on the HumanEval and MBPP benchmarks, as well as their advanced versions, HumanEval+ and MBPP+. For straightforward comparisons, we consolidate results across different model scales into one table, facilitating direct performance comparisons between each model scale and the respective variants of OpenCodeInterpreter.

Our experimental analysis reveals OpenCodeInterpreter's strong performance, with several configurations matching or surpassing leading benchmarks. The OpenCodeInterpreter-DS 33B variant achieves the highest scores among open-source models. This accomplishment is remarkable, especially considering the significant presence of lowquality or incorrect data in the initial training set.

### 4.2 Results of Multi-turn Code Generation

This section evaluates the proficiency of OpenCodeInterpreter in multi-turn interactions through iterative refinement, leveraging interpreter diagnostics and human insights.

Our experimental evaluation imposes a tworound limit on iterations to maintain fairness and consistency across tasks. While some issues may benefit from multiple refinements, others require fewer. This limitation offers clear insights into the model's iterative capabilities. In the execution feedback scenario, our models across all scales exhibited superiority over state-of-the-art (SOTA) benchmarks, with the OpenCodeInterpreter 33B model achieving parity with GPT-4 Turbo's single-round score, thus establishing a new SOTA benchmark among the evaluated code models.

Due to budget constraints, our Human Feedback and Human Feedback (Oracle) assessments concentrate on the OpenCodeInterpreter 6.7B and OpenCodeInterpreter 33B models. The outcomes reveal that with Human Feedback, the OpenCodeInterpreter $6.7 \mathrm{~B}$ model significantly outperformed GPT-4 Turbo's single-round score, while in the Human Feedback (Oracle) scenario, the OpenCodeIn-

| $\overline{\text { Ratio }}$ | $\overline{\text { E.F }}$ | HumanEval (+) | $\operatorname{MBPP}(+)$ | Average (+) |
| :---: | :---: | :---: | :---: | :---: |
| $2: 1$ | $x$ <br> $v$ | 76.2 (72.0) <br> $\mathbf{8 1 . 1}$ (78.7) | $73.9(63.7)$ <br> $\mathbf{8 2 . 7}(72.4)$ | 75.1 (67.9) <br> $\mathbf{8 1 . 9}(\mathbf{7 5 . 6 )}$ |
| $1: 1$ | $x$ | $\mathbf{7 7 . 3}(\mathbf{7 2 . 6})$ <br> $78.0(72.6)$ | $74.6(62.6)$ <br> 78.4 (65.9) | $\mathbf{7 6 . 0}(67.6)$ <br> $78.2(69.3)$ |
| $1: 2$ | $x$ | $75.7(71.9)$ <br> $78.7(75.6)$ | $7 \overline{7} . \overline{9}$ <br> 77.9 <br> $(62.9)$ <br> $(65.9)$ | $74.3(67.4)$ <br> $78.3(70.8)$ |
| $1: 3$ | $-\bar{x}$ <br> $v$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_33d6a508028b66d8423fg-07.jpg?height=97&width=213&top_left_y=546&top_left_x=429) | $-\overline{75.4}-\overline{\mathbf{4}}-\overline{65.4)}$ <br> $79.2(69.9)$ | $75.8(\overline{68.7})$ <br> $78.6(72.5)$ |
| $1: 5$ | $\bar{x}$ | $70.7(67.0)$ <br> $75.6(70.7)$ | $73.4(63.1)$ <br> $79.2(67.9)$ | $72.1(65.1)$ <br> $77.4(69.3)$ |
| $0: 1$ | $-\bar{x}$ | $\overline{7} \overline{-}-\overline{-} \overline{(68.9)}$ <br> $76.2(71.3)$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_33d6a508028b66d8423fg-07.jpg?height=89&width=163&top_left_y=719&top_left_x=663) | $73.9-(\overline{65} . \overline{9})$ <br> $71.5(74.0)$ |

Table 2: Performance of OpenCodeInterpreter with data mixed ratios of single-turn data and Code-Feedback. "E.F" indicates the use of execution feedback.

terpreter 33B model's average score notably exceeded the 90 benchmark in the HumanEval/MBPP benchmarks. These results highlight the significant role of iterative feedback and refinement in advancing code generation models, establishing OpenCodeInterpreter as a leader in software development tools. Through this refined approach, OpenCodeInterpreter not only demonstrates its remarkable adaptability and code refinement based on diverse feedback but also sets a new benchmark for future code generation technologies.

### 4.3 Ablations of Data Sources

This section systematically explores the impact of various data sources on the performance of OpenCodeInterpreter. We conduct a series of ablation studies to evaluate the influence of high-quality single-turn data and diverse multi-turn feedback mechanisms on the model's code generation, debugging, and refinement capabilities.

Impact of High-Quality Single-Turn Data. To evaluate the effect of high-quality single-turn data on OpenCodeInterpreter's efficacy, we incorporate the WizardCoder $110 \mathrm{~K}^{3}$ dataset, renowned for its syntactic accuracy and logical coherence, into our extensive multi-turn dataset. This integration seeks to identify the optimal mix of precise, single-turn code generation and the advanced, iterative refinement enabled by multi-turn interactions.

Our experiments employ a soft-target fine-tuning strategy across six configurations, varying the proportion of WizardCoder $110 \mathrm{~K}$ data in our multiturn dataset. These configurations span from full incorporation to total exclusion of the WizardCoder dataset, assessing the performance of the model in two versions: DeepSeekCoder-Base-6.7B and

| Datasets | E.F | Average (+) |
| :---: | :---: | :---: |
| Single-turn Packing | $x$ | $75.0(66.9)$ <br> $77.5(69.5)$ |
| Interaction Simulation | $\bar{x}$ | $75.1-(-\overline{66.9)}$ <br> $78.5(69.6)$ |
| Single-turn Packing <br> + Interaction Simulation | $\bar{x}$ <br> $v$ | $74.7(66.5)$ <br> $78.2(70.1)$ |
| Single-turn Packing + Interaction <br> Simulation + Code Correction | $\bar{x}$ <br> $v$ | $75.2(65.4)$ <br> $79.1(71.3)$ |
| --------- <br> Code-Feedback (Full) | $\bar{x}$ <br> $v$ | $75.1-(\overline{67.9})$ <br> $\mathbf{8 1 . 9}(\mathbf{7 5 . 6})$ |

Table 3: Performance comparison of the model across different settings with incremental data source integration. "E.F" indicates the use of execution feedback.

DeepSeekCoder-Base-33B.

Our findings are illustrated in Table 2. It shows that incorporating high-quality single-turn data (e.g., WizardCoder dataset) significantly improves our model's multi-turn performance. This strategic incorporation ensures that the model benefits from the syntactic accuracy and logical coherence inherent in single-turn tasks, thereby enriching its capacity for nuanced, iterative refinement in subsequent turns. It reveals the critical role of high-quality single-turn inputs in setting the stage for more effective multi-turn code generation and refinement.

## Benefits of Diverse Multi-Turn Data Sources.

 Following the enhanced baseline established by fully integrating the WizardCoder dataset, this subsection investigates the advantages of different data sources on the model's refinement and debugging efficacy. We add diverse data sources to our training regimen, including Single-turn Packing, Interaction Simulation, and Code Correction Data, both individually and in combination.The use of these multi-turn data sources, including Single-turn Packing, Interaction Simulation, and Code Correction Data, individually and in combination, demonstrably enhances OpenCodeInterpreter 's debugging and refinement functions. Notably, the inclusion of Code Correction Data significantly elevates the model's efficiency in correcting errors. This underscores the profound impact of a varied and targeted training approach on advancing the capabilities of sophisticated code generation models. Such an approach enables these models to more effectively address complex coding challenges, correct errors, and refine outputs via extensive feedback mechanisms.

### 4.4 Case Study: Coding Queries in the Wild

This section delves into three distinct case studies to demonstrate OpenCodeInterpreter's operational dynamics when faced with "wild" user queries. The motivation behind these case studies is to showcase the practical applications of OpenCodeInterpreter.

In a notable success story (Figure A8), we tasked OpenCodeInterpreter with developing a function to calculate all prime numbers within the 1-100 range, later extending the solution to any arbitrary range $\mathrm{x}-\mathrm{y}$. Another commendable instance (Figure A9) involved OpenCodeInterpreter implementing a Python function to validate IPv6 addresses using regular expressions. Demonstrating its capability to iteratively refine its approach, OpenCodeInterpreter not only identified and corrected errors but also enhanced the solution based on human feedback. These two cases exemplify OpenCodeInterpreter's strength in understanding mathematical logic and dynamically adjusting algorithms to meet specified criteria.

A challenging case (Figure A10) arose when OpenCodeInterpreter was asked to design a function identifying the intersection of two input lists, returning tuples of distinct elements present in both lists alongside their occurrence frequencies. Despite OpenCodeInterpreter's attempts at correction, it addressed errors incrementally, ultimately exceeding the maximum number of attempts (three). This case sheds light on OpenCodeInterpreter's limitations in simultaneously tackling multiple challenging errors.

Through these case studies, we gain invaluable insights into OpenCodeInterpreter's capabilities and limitations. These insights are crucial for guiding future enhancements to OpenCodeInterpreter.

## 5 Related Work

LLMs for Code. It becomes a common practice to include code data for pre-training LLMs. For example, $5 \%$ of PaLM's (Chowdhery et al., 2023) pre-training data is code, and this ratio for LaMDA (Thoppilan et al., 2022), Galactica (Taylor et al., 2022), LLaMA (Touvron et al., 2023), Gopher (Rae et al., 2021), GPT-NeoX (Black et al., 2022 ) is $13 \%, 7 \%, 5 \%, 3 \%$, and $8 \%$, respectively.

Additionally, specialized LLMs have been pretrained for generating code, e.g., CodeGen (Nijkamp et al., 2022), PanGu-Coder (Christopoulou et al., 2022), CodeGeeX (Zheng et al., 2023), CodeFuse (Di et al., 2023), CodeT5+ (Wang et al., 2023d), AlphaCode (Li et al., 2022), InCoder (Fried et al., 2022), StarCoder (Li et al., 2023a), DeepSeek-Coder (Guo et al., 2024). On the other hand, code LLMs can be fine-tuned from general-purpose LLMs, e.g., CodeLlama (Roziere et al., 2023), WizardCoder (Luo et al., 2023), which is the approach we take here. Compared to specialized LLMs, the fine-tuning paradigm enables us to explore ways to improve code generation capabilities by leveraging pre-trained general-purpose LLMs, especially because these LLMs have already been trained on an extensive amount of code data. Iterative Code Generation and Refinement. For many sequence generation tasks, iterative approaches are often taken to improve the generation quality, e.g., script generation (Tandon et al., 2021), summarization (Scheurer et al., 2022), and other tasks as shown in (Madaan et al., 2022; Saunders et al., 2022). Notably, in Self-Refine (Madaan et al., 2023), an LLM generates feedback after generating initial outputs, and the LLM iteratively updates the outputs with the feedback. Whereas it focuses on a general-purpose LLM setting, we focus on code generation tasks. As for code generation with LLMs, DebugBench (Tian et al., 2024) observes that incorporating runtime feedback improves code LLMs' debugging performance. A most recent and relevant work is StepCoder (Dou et al., 2024), where, following the paradigm of relying on reinforcement learning with compiler feedback (Le et al., 2022; Shojaee et al., 2023), the authors further divide the original exploration problems into a sequence of easier sub-tasks. However, our approach does not rely on reinforcement learning and has access to the intermediate generation, which makes the training easier and more stable.

## 6 Conclusion

In conclusion, OpenCodeInterpreter represents a significant leap forward in the field of code generation, bridging the previously identified gap between open-source models and the advanced capabilities of proprietary systems like the GPT-4 Code Interpreter. By integrating compiler diagnostics and human feedback into an iterative refinement process, OpenCodeInterpreter not only surpasses traditional one-off generation approaches but also introduces a level of adaptability and precision previously unseen in open-source models. The introduction of Code-Feedback, with its extensive multi-turn interactions, further empowers OpenCodeInterpreter
to dynamically refine code in response to evolving user intents and complex coding tasks.

## Ethics Statement

The development and deployment of OpenCodeInterpreter, alongside the use of Code-Feedback, take ethical considerations to ensure responsible usage. We have made efforts to ensure that the dataset represents a diverse range of coding styles, problem domains, and user scenarios to prevent the propagation of biased or unfair outcomes. Given that OpenCodeInterpreter can generate and refine code based on user inputs, we strictly check out the dataset to ensure that it does not expose sensitive information or create security vulnerabilities. OpenCodeInterpreter has the potential to democratize coding by lowering the barrier to entry for non-experts and developers. We open-source all our code, models, and datasets to maximize accessibility.

## Limitations

While OpenCodeInterpreter introduces significant advancements in automated code generation, it is important to acknowledge the limitations inherent in the system and the Code-Feedback that supports it. Although OpenCodeInterpreter is designed to support multi-language code generation and understand a wide range of programming contexts, its performance may vary across different languages and specific domains. While OpenCodeInterpreter excels at interpreting and responding to a variety of coding tasks, it may struggle with extremely complex or ambiguous user intents. The ability to accurately capture and address such intents is limited by the model's current understanding and the specificity of the data in Code-Feedback.

## References

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. ArXiv preprint, abs/2108.07732.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. ArXiv preprint, abs/2309.16609.

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and
Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode \#5 - Workshop on Challenges \& Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.

Sahil Chaudhary. 2023. Code Alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/ codealpaca. Accessed: 2024-02-13.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. ArXiv preprint, abs/2304.05128.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113.

Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling. ArXiv preprint, abs/2207.11280.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, et al. 2023. Codefuse-13b: A pretrained multi-lingual code large language model. ArXiv preprint, abs/2310.06266.

Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, et al. 2024. Stepcoder: Improve code generation with reinforcement learning from compiler feedback. ArXiv preprint, $\mathrm{abs} / 2402.01391$.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. ArXiv preprint, abs/2204.05999.

GitHub. 2023. Github copilot. https://github. com/features/copilot. Accessed: 2024-0214 .

Gemini Google, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. ArXiv preprint, abs/2312.11805.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. ArXiv preprint, abs/2401.14196.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. ArXiv preprint, abs/2310.06825.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314-21328.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023a. Starcoder: may the source be with you! ArXiv preprint, abs/2305.06161.

Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023b. Taco: Topics in algorithmic code generation dataset. ArXiv preprint, abs/2312.14852.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):1092-1097.

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. ArXiv preprint, abs/2306.08568.

Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve GPT-3 after deployment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2833-2861, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. ArXiv preprint, abs/2303.17651.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. ArXiv preprint, abs/2203.13474.

OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/ blog / chatgpt / . Accessed on 14 Feb. 2024.

OpenAI. 2023. Gpt-4 technical report.

Phind. 2023. Phind/phind-codellama-34b-v2.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \& insights from training gopher. ArXiv preprint, abs/2112.11446.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950.

William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. ArXiv preprint, abs/2206.05802.

Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with natural language feedback. ArXiv preprint, abs/2204.14146.

Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. 2023. Incontext pretraining: Language modeling beyond document boundaries. ArXiv preprint, abs/2310.10638.

Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using deep reinforcement learning. ArXiv preprint, abs/2301.13816.

speechless. 2023. speechless-codellama-34b-v2.0.

Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: A dataset for interactive learning of scripts through error feedback. ArXiv preprint, abs/2112.07867.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. ArXiv preprint, abs/2211.09085.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. ArXiv preprint, abs/2201.08239.

Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Debugbench: Evaluating debugging capability of large language models. ArXiv preprint, abs/2401.04621.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971.

Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023a. Openchat: Advancing open-source language models with mixed-quality data. ArXiv preprint, abs/2309.11235.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 13484-13508

Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023c. Codet5+: Open code large language models for code understanding and generation. ArXiv preprint, abs/2305.07922.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023d Codet5+: Open code large language models for code understanding and generation. ArXiv preprint, $\mathrm{abs} / 2305.07922$.

Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. ArXiv preprint, abs/2312.02120.

Xwin-LM. 2023. Xwin-lm.

Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2023. Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. ArXiv preprint, abs/2312.14187.

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. ArXiv preprint, abs/2303.17568.
