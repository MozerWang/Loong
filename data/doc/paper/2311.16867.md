# The Falcon Series of Open Language Models 

The Falcon LLM Team<br>Ebtesam Almazrouei Hamza Alobeidli Abdulaziz Alshamsi Alessandro Cappelli<br>Ruxandra Cojocaru MÃ©rouane Debbah Etienne Goffinet Daniel Hesslow Julien Launay<br>Quentin Malartic Daniele Mazzotta Badreddine Noune Baptiste Pannier Guilherme Penedo<br>Technology Innovation Institute, Abu Dhabi

https://huggingface.co/tiiuae/


#### Abstract

We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoderonly models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text-the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.


![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-01.jpg?height=504&width=697&top_left_y=1686&top_left_x=714)

Figure 1: The Falcon series of models achieves competitive performance, with Falcon-180B nearly matching the 1 -shot performance of PaLM-2 Large. 1-shot performance of PaLM (Chowdhery et al. 2022), PaLM-2 (Anil et al., 2023), and Falcon-180B, on a set of tasks from Brown et al. (2020). These evaluation results are only a small snapshot of our evaluations, see Section 6 for details and comparisons with GPT-3.5/4, LLaMA-1/2, and Inflection-1.[^0]

## 1 Introduction

The on-going Cambrian explosion of language models has been primarily fueled by the unique scalability of popular Transformer-based recipes. This scalability manifests over multiple axes:

- Performance scalability (and predictability). Increase in pretraining compute budgets systematically yield improvements in language modeling capabilities, in a consistent and predictable way (Kaplan et al. 2020). Falcon-180B is the first publicly documented GPT-3sized model to follow the updated scaling law recommendations of Hoffmann et al. (2022), with a total pretraining length of 3,500 billion tokens, without any upsampling.
- Data scalability. To scale-up pretraining efficiently, and to decouple pretraining and inference compute, increasingly large models should be trained for longer, on larger corpora. To sustain the Falcon series, we developed RefinedWeb (Penedo et al., 2023), a 5 trillion tokens high-quality filtered and deduplicated web dataset-the largest publicly documented.
- Hardware scalability. Transformer models (Vaswani et al. 2017) are naturally suited for modern GEMM optimized hardware, allowing their training and inference to be efficiently distributed over a large number of accelerators (Narayanan et al., 2021b, Pope et al., 2023). With Falcon-180B, we demonstrate scaling-up pretraining to 4,096 A100 40GB with only $50 \mathrm{Gbps}$ interconnect per accelerator on cost-efficient AWS cloud infrastructure.

Building upon these fundamentals, increasingly large language models give rise to so-called emergent capabilities (Wei et al. 2022a). These capabilities can be further tailored to human preference, to build instruction-following or chatty models (Ouyang et al., 2022). All together these methods have lead to the widespread deployment of large language models in customer-facing applications, such as ChatGPT (GPT-3.5/4, OpenAI (2023a)), Claude, Bard (PaLM-2, Anil et al. (2023)), or Pi (Inflection-1, Inflection (2023)). In this paper, we primarily report on the pretraining alone of the Falcon series of models, and leave further downstream finetuning and alignment to future works.

The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al. 2023)-a massive filtered and deduplicated web dataset. The architecture of the models is based on PaLM (Chowdhery et al. 2022), although we independently validated each decision, ultimately resulting in some minor tweaks-see Section 4 for details. The Falcon series leverages extensive custom tooling (i.e., pretraining codebase, data pipeline), of which development started in August 2022, with training of the models kicked-off in December 2022. In-depth evaluation shows that the Falcon series is competitive across scale, and that Falcon-180B nears the performance of PaLM-2 Large, positioning it as the best open model and in the top-3 of the best language models.

Contributions. With this paper and the Falcon series, we make the following contributions:

- Public documentation of the pretraining of a large-scale model. Recent state-of-the-art models have been scarcely documented, hindering further research and progress in the field. At variance with these works, we extensively document the pretraining of the Falcon series.
- Open data and models. To accelerate research, and to enable community-driven improvements of large language models, we openly release Falcon-7/40/180B, and a 600 billion tokens extract of the RefinedWeb dataset: https://huggingface.co/tiiuae/.

Table 1: The Falcon series of models covers a wide range of capabilities and inference requirements, enabled by large-scale web data. Falcon-7B can efficiently run on consumer hardware (e.g., Apple M2), while Falcon-180B typically requires dedicated inference infrastructure (e.g., $8 \times$ A100 80GB ). We report steady zero-shot performance gains across the entire Falcon series.

|  | Falcon-7B | Falcon-40B | Falcon-180B |
| :--- | :---: | :---: | :---: |
| Pretraining [tokens] | $1,500 \mathrm{~B}$ | $1,000 \mathrm{~B}$ | $3,500 \mathrm{~B}$ |
| Compute [PF-days] | 730 | 2,800 | 43,500 |
| Training [A100s] | 384 | 384 | 4,096 |
| Availability | Apache 2.0 | Apache 2.0 | Responsible use license |
| Agg. performance (Section6.5) | 60.8 | 67.1 | 70.3 |
| Closest model | $<$ GPT-3 | Chinchilla | PaLM-2 Large |

## Contents

1 Introduction ..... 2
2 State-of-the-art: from language modeling to frontier models ..... 5
3 Design philosophy ..... 6
4 Experiments and motivations for data, architecture, and hyperparameters ..... 7
4.1 Setup for small-scale experiments ..... 7
4.2 Data: web vs curated, code and multilinguality impact on English performance ..... 8
4.2.1 Web data alone can outperform curated corpora ..... 8
4.2.2 $\quad$ Against a strong web baseline, curated data can even be detrimental ..... 10
4.2.3 Limited code and multilingual data do not strongly degrade English performance ..... 11
4.3 Architecture and pretraining: validating popular recipes, and inference optimizations ..... 12
4.3.1 Extending multiquery into multigroup for tensor parallel training and inference ..... 12
4.3.2 Rotary positionnal embeddings may only offer a limited edge over ALiBi . ..... 14
4.3.3 The extra memory cost of GLU may not be worth it for cost-efficient training ..... 14
4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers ..... 15
4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search ..... 15
4.4 Further experimentation required: ideas that did not make the cut ..... 17
4.5 Wrapping-it up: validating overall dataset and architecture recipes ..... 18
5 Implementation ..... 19
5.1 The Falcon dataset: predominantly web, with added curated and conversational data ..... 19
5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset ..... 20
5.1.2 The Microdata curated corpora and conversational masking. ..... 21
5.2 The Falcon architecture and recipe for efficient inference and (stable) training . ..... 22
5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up ..... 22
5.2.2 Large language model alchemy: hyperparameters for pretraining ..... 24
5.3 Large-scale distributed training on cloud infrastructure with Gigatron ..... 24
5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability ..... 25
5.3.2 State-of-the-art throughput with dedicated Triton kernels ..... 28
5.3.3 Efficient memory use via selective recomputation implemented as a monolayer ..... 28
5.3.4 Numerical precision: all you need is bfloat16? ..... 29
5.3.5 Quality-of-life features for improved flexibility and reliability ..... 29
5.4 Run management: keeping large-scale infrastructure running ..... 29
6 Results ..... 30
6.1 To prompt or not to prompt: comparing evaluations across codebases. ..... 31
6.2 Comparisons with PaLM on a natural language tasks aggregate ..... 33
6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks ..... 34
6.4 State-of-the-art comparisons on common sense, question answering, and code tasks ..... 34
6.5 Comparison with other models using the EleutherAI Evaluation Harness ..... 36
$7 \quad$ Limitations ..... 37
7.1 Limitations of our findings and ablations ..... 37
7.2 Limitations of the Falcon models ..... 38
8 Conclusion ..... 39
A Contributions ..... 51
B Acknowledgements ..... 51
C Model card ..... 52
D Datasheet ..... 53
E Comparisons with undocumented models ..... 53
F Pseudocode samples ..... 53
F. 1 Measurement plan to measure all to all bandwidths/latencies efficiently ..... 53
F. 2 Converting tree token depth into an attention mask: ..... 53
F. 3 Zero-1 pseudo-code ..... 53
G Prompts ..... 54

## 2 State-of-the-art: from language modeling to frontier models

We provide in this section an overview of general trends and works adjacent to the Falcon series. For an in-depth literature review of individual technical components, see the relevant sections.

Language modeling. Beyond corpus/task-specific approaches, the first large-scale vector-based word embeddings methods (Mikolov et al. 2013, Pennington et al. 2014) pioneered unsupervised learning from massive unstructured text corpora. The integration of deep recurrent neural architectures enabled models to deal with polysemy and to integrate contextual information (Peters et al. 2018); up to the emergence of the transfer learning paradigm, leveraging universal models specialized to downstream tasks through finetuning (Howard and Ruder, 2018). Despite the existence of many of the first principles currently used, early scaling attempts (Jozefowicz et al. 2016) only had mixed success, partly due to the fastidiousness and poor scalability on common hardware of recurrent approaches.

Transfomer models. The introduction of the attention-based Transformer architecture Vaswani et al. (2017) sparked an explosion in the number of recipes to produce efficient, generalist models: from embedding and classification focused encoder-only BERTs (Kenton and Toutanova, 2019), to causal decoder-only GPTs (Radford et al., 2018). Specifically, GPT-2 (Radford et al. 2019) was the first series of models to popularize emergent few-shot generalization abilities, allowing a model to understand and perform arbitrary tasks simply from in-context instructions and demonstrations.

Large language models. The aforementioned works laid out the key components to current models; the last ingredient, scaling, was demonstrated by GPT-3 (Brown et al., 2020), and consecrated by the outlining of scaling laws (Kaplan et al. 2020). As increasingly large amounts of compute are spent to pretrain models, commensurate gains are made in language modeling performance. The tantalizing prospect of a systematic and direct path to more capable language models lead to a "scaling frenzy": first with reproductions of GPT-3 with Jurassic-1 (Lieber et al., 2021) or PanGu-Alpha (Zeng et al. 2021), and with open efforts such as GPT-J (Wang and Komatsuzaki |2021), OPT (Zhang et al., 2022), or BLOOM (Scao et al. 2022a); second, with works pushing further the limits of scaling with Gopher (Rae et al. 2021), MT-NLG (Smith et al., 2022), or PaLM (Chowdhery et al., 2022). Increased development and adoption of large language models also lead to improvements of pretraining methods. Notably, Hoffmann et al. (2022) demonstrated with Chinchilla that optimal scaling should actually jointly increase model size and pretraining dataset. For deployment in the real-world, it may even be desirable to train far past so-called optimality, to decouple training and inference compute and reduce serving costs. This is illustrated with the LLaMA models (Touvron et al. $2023 \mathrm{a} \mathrm{b}$ ), with $7 \mathrm{~B} / 13 \mathrm{~B} / 30 \mathrm{~B} / 70 \mathrm{~B}$ parameters models trained on up to 2 trillion tokens.

Frontier models. Concurrently to this work, so-called frontier models have emerged, under the shifting definition of "large-scale machine-learning models that exceed the capabilities currently present in the most advanced existing models, and can perform a wide variety of tasks". Although a moving goal, we attribute recent works on GPT-4 (OpenAI 2023a) and PaLM-2 (Anil et al., 2023) as early contributions to this category. These stand out by their significantly increased compute budget, and improved capabilities. See Appendix Efor details on our approach to undocumented models.

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-05.jpg?height=505&width=1233&top_left_y=1840&top_left_x=446)

Figure 2: Although open models lag behind closed models in pretraining compute (by $\sim 18$ months), the gap is not widening. After a 1 year lag, GPT-3 sparked a "scaling frenzy", with the emergence of numerous LLMs. Models in the $>10,000$ PF-days range remain rare for open-source.

## 3 Design philosophy

Inspired by the bitter lesson (Sutton, 2019), we believe that scalable methods that best leverage compute are ultimately the most effective. Accordingly, in designing the Falcon series of models, we focused primarily on scalability, across three axes: performance, data, and hardware.

Performance scalability. The sustained increase in the scale of large language models (Fig. 2) has been primarily motivated by so-called scaling laws: with increased pretraining compute comes commensurate improvements in language modeling capabilities (Hestness et al. 2017, Kaplan et al. 2020). This systematic path to model improvement has proved far more reliable than waiting for the occasional research epiphany to manifest a paradigm-shifting method. But upstream and downstream performance are not simply motivators of scaling, they are also a powerful driver. Indeed, quantifying the impact of modeling interventions (e.g., architecture tweaks, data sourcing, hyperparameters selection) is critical to sustaining the feedback loop provided by small-scale ablations. However, the question of what to quantify is not trivial: upstream performance alone can be at odds with downstream tasks (Tay et al. 2021), and even downstream metrics may not be aligned with human preferences (Stiennon et al. 2020). This is made even more challenging by the fundamental contrast between pretraining objective (i.e., predict the next word on a predominantly web-based corpora) and common downstream use (i.e., follow users' instructions in a helpful, harmless, and honest way) (Bai et al. 2022a). As we focus on the pretraining stage of the Falcon models, we choose to center our evaluations on measuring zero/few-shot generalization on large aggregates of natural language tasks with the EleutherAI Harness (Gao et al., 2021)-similar to the setup of Le Scao et al. (2022); Wang et al. (2022a). We build aggregates enabling comparisons with other state-of-the-art models, but note the difficulty in executing principled comparisons: practices diverge widely across papers in terms of task selection, prompting, and even mode of evaluation-standardized benchmarks remain only scarcely adopted. We discuss in Section 7 limitations of our evaluation setup.

Data scalability. An increase in pretraining compute budget can be either spent towards a larger model, or towards longer training. While Kaplan et al. (2020) had first found optimal scaling to be mostly model size driven, Hoffmann et al. (2022) revised this finding and found that joint scaling is preferable-see Fig. 33 to see impact. Furthermore, growing model size also increases inference burden; where as increasing the pretraining dataset size is decoupled from inference costs. Recent open models have been trained on datasets of up to two trillions tokens (Touvron et al., 2023b); because naive repetition risks degrading the model (Hernandez et al., 2022, Muennighoff et al., 2023), this has led to concerns about the sustainability of data scaling (Villalobos et al., 2022). These concerns are exacerbated by the widely belief that curated corpora are necessary to build state-of-the-art models, requiring manual addition of individual sources such as arXiv papers, books, and more (Brown et al. 2020; Gao et al. 2020). For the Falcon series of models, we choose to focus on scaling high-quality web data through stringent filtering and deduplication, enabling us to collect an English web dataset of 5,000 billion tokens and to not repeat any data during training. We extensively report on this work in Penedo et al. (2023), but also provide some key elements in this paper.

Hardware scalability. Large-scale training requires thousands of hardware accelerators to work efficiently in unison; making the best use of these accelerators requires in turn principled distributed training methods (Shoeybi et al. 2019). Methods that are able to best run efficiently and leverage large-scale compute are often the ones that gain the most traction in the community (Hooker, 2021), as best evidenced by the Transformer architecture itself (Vaswani et al. 2017). Furthermore, it is difficult to find architectural improvements that significantly improve the task performance of models, compared to the impact of data for instance (Le Scao et al., 2022). Accordingly, we focus architectural decisions not on improving task performance, but on improving hardware scalability and throughput. We are also concerned with inference scalability (Pope et al., 2023), leading us to the adoption of tweaks such as a (revised) multiquery attention scheme (Shazeer, 2019).

Finally, beyond scalability, we are also concerced with cost-efficiency and relying on proven approaches. We reimplement a 3D parallelism strategy (Narayanan et al., 2021b) combined with optimizer sharding (Rajbhandari et al., 2020), enabling us to run on a more cost-efficient cloud AWS infrastructure with limited interconnect. We also put a strong focus on memory-saving methods, enabling us to run on cheaper 40GB A100. Reimplementing our data preprocessing and pretraining pipelines from scratch allow us to extensively verify them, rather than relying on an unknown external codebase. We also do not explore radical departures from the classic designs of large language models, such as state-space models (Fu et al. 2022), as these have usually not been proven at scale.

## 4 Experiments and motivations for data, architecture, and hyperparameters

We first focus on a series of small-scale experiments with models in the 1B-3B parameters range to validate recommended practices, as well as to identify interesting tweaks. We also conducted extensive experiments to validate our web data pipeline, reported in Penedo et al. (2023). With each subject of interest, we also outline common practices and our motivations for exploring this direction.

### 4.1 Setup for small-scale experiments

Small models. For these ablations, we seek to be able to rapidly iterate with limited compute costs. This leads us to train $1 / 3$ billion parameters models, for $30 / 60$ billion parameters respectively. We choose these short training lengths to be illustrative of the optimality regime from Hoffmann et al. (2022) under which our larger models are likely to be trained. We base our reference architecture and hyperparameters on the one described for GPT-3 (Brown et al. 2020), with the caveat of using ALiBi positionnal embeddings as our baseline (Press et al., 2022). With reasonable resources (32-64 A100), these ablation models can be trained overnight or in a few days, enabling rapid iterations. We note that a caveat of using smaller models is that they may not be illustrative of some of the behaviours of larger models: for instance, Dettmers et al. (2022) found that outlier features emerge at the 6B scale, impacting quantization; concerns around data duplication and memorization have also shown to disprotionately affect larger models (Carlini et al., 2022; Hernandez et al., 2022)

Dedicated aggregates. Although small models enable rapid iterations, they also have limited zeroshot capabilities; naive bulk evaluation would result in most tasks being close to the random baseline, and in significant noise in the evaluations. Using models trained on subsets of The Pile, we identified tasks that showed both reasonable performance at small-scale, and limited variability across runs. We independently quantified variance and average performance on over 50 tasks across 5 runs with a different seed (we use this $\sigma$ for architecture experiments) and across 10 runs with different data subsets and shuffling (for data experiments). Based on this analysis, we source 11 tasks from the evaluation setup of Brown et al. (2020); Le Scao et al. (2022); Srivastava et al. (2023) for our ablations (zs-main/data/web). Note that differences between these three subsets are mostly due to differing practices across time and between teams. zs-comp is a subset of main for comparisons with other models, based on commonly reported tasks. We also report perplexities on The Pile (Gao et al. 2020) (ppl-pile) for architectures (for data experiments, we found perplexities on The Pile to mostly illustrate differences in formatting rather than content), and on a restricted subset of 3 NLP tasks with low variance (zs-small). For our small-scale evaluations, we use both the EleutherAI harness (Gao et al. 2021) and BigBench (Srivastava et al. 2023); note that our inference and evaluation codebase differ significantly between this setup and the final results we report in Section6, so results are not directly comparable. We present an outline of the aggregates in Table 2

Table 2: To evaluate small models used in ablations (1/3B models trained on $30 / 60 \mathrm{~B}$ tokens), we build four aggregates across 11 tasks on which to measure zero-shot performance and perplexity. We trained 15 reference models on subsets of The Pile and with random seeds to identify tasks with performance better than random and low variablity at this scale. All evaluations leverage the EAI Harness (Gao et al., 2021) except date which is taken from BigBench (Srivastava et al., 2023). main was built for architecture and hyperparameters ablations; data for data mixtures experiments; web for small-scale ablations on web data; and core for its low variance on a reduced number of tasks. This setup only covers natural language abilities. For main, data, and web, differences are mostly due to individual preferences at the time of the experiments.

| Tasks | Type | Random | main | comp | data | web | core | pile |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LAMBADA $\sqrt{\text { Paperno et al. }} 2016$ | Reading Comprehension | 0.0 | $\checkmark$ |  |  |  | $\checkmark$ |  |
| RACE (Lai et al. 2017 ) | Reading Comprehension | 25.0 | $\checkmark$ |  |  |  |  |  |
| HellaSwag Zellers et al. 2019) | Common Sense | 25.0 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |  |
| Winogrande (Sakaguch1 et al. 2019 . | Common Sense | 50.0 |  | $\checkmark$ | $\checkmark$ |  |  |  |
| PIQA Bisk et al. 2020 | Common Sense | 50.0 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |  |
| BoolQ Clark et al. 2019 | Common Sense | 50.0 | $\checkmark$ |  | $\checkmark$ |  |  |  |
| COPA Gordon et al. 2012 | Common Sense | 50.0 | $\checkmark$ |  | $\checkmark$ |  |  |  |
| Date (Srivastava et al. 2023 ) | Common Sense | 25.0 | $\checkmark$ |  |  |  |  |  |
| ARC Clark et al. 2018 | Question Answering | 25.0 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |  |  |
| OpenBookQA (Mihaylov et al. 2018 | Question Answering | 25.0 |  | $\checkmark$ |  |  |  |  |
| SciQ Johannes WeIbI 2017) | Question Answering | 25.0 | $\checkmark$ |  | $\checkmark$ | $\checkmark$ |  |  |
| The P1le Gao et al. 2020 | Language Modeling |  |  |  |  |  |  | $\checkmark$ |

### 4.2 Data: web vs curated, code and multilinguality impact on English performance

### 4.2.1 Web data alone can outperform curated corpora

- Our motivations for predominantly training on web data, details of the processing pipeline, and extensive evaluations are detailed in our dedicated RefinedWeb paper (Penedo et al., 2023). In this section, we only highlight key ablations guiding our decision to focus our efforts on web data.

Background. Since its origins with simpler and shallower statistical language models (Shannon. 1951; Mikolov et al. 2013), natural language processing has long leveraged unstructured massive text corpora. If these corpora were at first built "sentence-wise" (Chelba et al., 2013), the emergence of more advanced architectures enabled models to best use long context information present in unified documents (Devlin et al., 2018; Radford et al., 2018). Starting with single-domain sources, such as Wikipedia or BookCorpus (Zhu et al., 2015), datasets scaled along with models, and massive web-scrapes gained prevalence (Ortiz SuÃ¡rez et al., 2019, Raffel et al., 2019). However, it is widely believed that web data alone is insufficient to build performant models (Brown et al., 2020, Gao et al. 2020). Accordingly, large language models train on mixed corpora, combining both large-scale web data, and curated so-called "high-quality" individual sources (e.g., books, technical papers, social media conversations)- see Table 3 for an overview of common pretraining mixes.

However, as we outlined in the data scalability discussion in Section 3, sourcing the trillions of tokens required for pretraining a modern language model may be challenging. This leads us to challenge the idea that curated data is fundamentally better than web data. Notably, building upon the work of Lee et al. (2022), we study how stringent deduplication and extensive filtering inspired by Rae et al. (2021) may enable web data alone to train performant models.

IQuestion. Can web data alone (with filtering and deduplication) be used to train models outperforming models trained on curated data, as measured by natural language zero-shot performance?

Methods. We train 1/3B parameters models on 27/60B tokens, on a number of datasets of interest and on intermediary artefacts of our data pipeline. For state-of-the-art web datasets, we consider two versions of OSCAR (Ortiz SuÃ¡rez et al. 2019, Abadji et al., 2022) and C4 (Raffel et al., 2019). For curated datasets, we consider The Pile (Gao et al., 2020), the most popular pre-aggregated dataset-many models have also elected to base their pretraining data on specific components of The Pile (Smith et al. 2022, Zhang et al., 2022). RW-Raw corresponds to the output of our pipeline with the least amount of filtering, immediately after text extraction-but still with English language identification applied as well as URL blocklist for known adult content; RW-Filtered applies a first round of heuristics, similar to the ones used by Rae et al. (2021); finally, REFINEDWEB corresponds to our final web dataset, with deduplication applied in two stages. We evaluate all models on the zs-web aggregate-see Table 2 for details of its composition.

Table 3: Following the recommendations of Hoffmann et al. 2022, pretraining datasets have increased in size, causing an increase in the prevalence of web data. Web data sources are massive web scrapes such as $\mathrm{C} 4$ (Raffel et al. 2019), sourced from CommonCrawl. Curated web data undergoes a targeted domain filtering: this includes CC-News (Hamborg et al., 2017) for instance. We consider sources such as arXiv, Wikipedia, or PubMed as technical curated data, and sources such as Reddit, HackerNews, or StackOverflow as conversational. We report not the size of the overall dataset, but the amount of tokens used for pretraining. For LaMDA (Thoppilan et al., 2022), numbers are roughly estimated as only rough counts are provided in the paper.

|  | Web | Curated web | Curated |  |  |  |  | Total |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | Books | Tech. | Code | Conv. | Epochs |  |
| GPT-3 | $60 \%$ | $22 \%$ | $18 \%$ | $16 \%$ | $2 \%$ | $0 \%$ | $0 \%$ | 2.4 | $300 \mathrm{~B}$ |
| The Pile | $18 \%$ | $10 \%$ | $72 \%$ | $15 \%$ | $40 \%$ | $7 \%$ | $10 \%$ | 1.8 | $340 \mathrm{~B}$ |
| MT-NLG | $38 \%$ | $29 \%$ | $33 \%$ | $16 \%$ | $9 \%$ | $2 \%$ | $6 \%$ | 2 | $270 \mathrm{~B}$ |
| Gopher | $58 \%$ | $10 \%$ | $32 \%$ | $27 \%$ | $2 \%$ | $3 \%$ | $0 \%$ | $\sim 1$ | $300 \mathrm{~B}$ |
| LaMDA | $25 \%$ | $0 \%$ | $75 \%$ | $0 \%$ | $25 \%$ | $0 \%$ | $50 \%$ |  | $340 \mathrm{~B}$ |
| PaLM | $27 \%$ | $1 \%$ | $72 \%$ | $13 \%$ | $4 \%$ | $5 \%$ | $50 \%$ |  | $780 \mathrm{~B}$ |
| Chinchilla | $55 \%$ | $10 \%$ | $35 \%$ | $30 \%$ | $1 \%$ | $4 \%$ | $0 \%$ | 1.1 | $1400 \mathrm{~B}$ |
| LLaMA | $82 \%$ | $0 \%$ | $18 \%$ | $5 \%$ | $7 \%$ | $4 \%$ | $2 \%$ | 1.7 | $1400 \mathrm{~B}$ |
| Falcon | $84 \%$ | $0 \%$ | $16 \%$ | $6 \%$ | $2 \%$ | $3 \%$ | $5 \%$ | 1 | $3500 \mathrm{~B}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-09.jpg?height=550&width=1390&top_left_y=251&top_left_x=365)

Figure 3: Two eras of pretraining practices, from predominant model scaling $\square$ to joint model and data scaling $\bullet$. Before Hoffmann et al. (2022), models ( $\square$ ) predominantly scaled their parameter count at a fixed dataset size (around 300 billion tokens), in line with the recommendations of Kaplan et al. (2020). Afterward ( $\bullet$ ), models started scaling both model size and dataset size jointly, sharply increasing the need for scalable data pipelines. Estimate of clean data available in CommonCrawl from our work in Penedo et al. (2023), considering English only-would double if allowing multilinguality.

Results. Results for this round of experiments are presented in Table 4 In line with expectations, we find that raw web data (RW-Raw) performs poorly; similarly OSCAR-22.01 offers the worst performance of all datasets. This is likely because its creators have opted to distribute it by default without any deduplication applied. Conversely, OSCAR-21.09 and C4 are both strong baselines. We notably find that The Pile very likely does not deliver better performance than web data.

Subsequent stages in our pipeline significantly uplift dataset quality and the performance of models trained on it. Filtering alone enables us to close the gap with The Pile, while the addition of stringent deduplication allows for RefinedWEB to be the best dataset among the ones we benchmarked.

We note two limitations of these experiments. First, the models are small, and trained on a limited amount of data. However, it is likely that gains from deduplication actually increase with model scale, as larger models are more sensitive to duplicates (Hernandez et al. 2022) and more likely to memorize individual samples (Carlini et al. 2022). Second, our evaluation focuses on natural language tasks. It is unlikely that models trained on web data alone would compare favorably to models trained on The Pile on code tasks for instance, as The Pile explicitely include code, while massive web scrapes are likely to be mostly devoid of it except for some incidental occurrences.

Finding. Challenging beliefs on data quality, filtered and deduplicated web data alone allows models to match the natural language tasks performance of models trained on curated data.

Table 4: Curation is not a silver bullet for zero-shot generalization: small-scale models trained on ReFinedWeb outperform models trained on web data (C4, OSCAR), and on curated corpora (The Pile). Zero-shot accuracy on zs-web aggregate ( $\sigma=0.69$, likely $\pm 0.69 \%$, very likely $\pm 1.38 \%$ differences in scores). All models trained with identical architectures and hyperparameters, for the same amount of tokens. We find that OSCAR-22.01 underperforms other datasets signficantly, perhaps because deduplication is only optional. C4 is a strong baseline, with OSCAR-21.09 lagging slightly behind, but we find that RefinedWeb outperforms both web datasets and the curated dataset, The Pile-performance gap with $\mathrm{C} 4$ is insufficient to be conclusive, but $\mathrm{C} 4$ would be too small for our models. Both filtering and deduplication contribute significantly to improving zero-shot performance.

|  | MASSIve WEB dATASETS |  | Curated | OURS |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | OSCAR-21.09 | OSCAR-22.01 | C4 | The Pile | RW-Raw | RW-Filtered | RefinedWEB |
| 1B@27GT | $55.0 \%$ | $52.7 \%$ | $55.7 \%$ | $53.4 \%$ | $52.7 \%$ | $54.3 \%$ | $\mathbf{5 6 . 2 \%}$ |
| 3B@60GT | $59.1 \%$ | $55.9 \%$ | $59.6 \%$ | $57.9 \%$ | $57.4 \%$ | $58.2 \%$ | $\mathbf{5 9 . 8 \%}$ |

Table 5: We split curated data in three broad categories: conversations, books, and technical. Individual components inspired by Gao et al. (2020), but and processed through our own data pipeline.

| Conversations | Reddit (Baumgartner et al. 2020), HackerNews, OpenSubtitles (Tiede- |
| :--- | :--- |
|  | mann, 2016), Ubuntu IRC, Youtube Subtitles, StackOverflow |
| Books | Project Gutenberg Rae et al. (2019) |
| Technical | ArXiv, PubMed Central, PubMed Abstracts, USPTO Patents |

### 4.2.2 Against a strong web baseline, curated data can even be detrimental

Background. In Section 4.2.1 and Table 3, we have noted that large language models use pretraining datasets combining both massive web crawl data and individual curated sources. Such sources were first employed to build domain-specific models (Beltagy et al. 2019); they have also been proposed to broaden the expressiveness of models, for instance for conversational modalities (Adiwardana et al. 2020; Thoppilan et al., 2022). Some of these sources can also exist at the intersection of strongly curated data and crawls: LaurenÃ§on et al. (2022) has for instance proposed to seed the first links in a crawl using human-selected URLs. However, these tailored sources raise challenges for practioners. First, individual corpora are much less scalable than massive web crawls, as they require scattered work instead of a centralized pipeline. Second, the providers of some of these sources have begun taking steps to forbid LLMs from being trained on their data Paresh (2023), and adequate licensing may be costly to obtain. Based on our findings from the previous section, we may wonder what happens when we combine curated data with a strong web baseline like RefinedWeb.

- Question. When added in substitution of a strong web baseline, is curated data from individual corpora still beneficial to the natural language zero-shot performance of a model?

Methods. We train small 1B models on 30B tokens, with the pretraining data split between web data and a specific curated category. We sample training on $1,10,25,50,75$, and $100 \%$ of the targeted category. We only consider a one-dimensional approach, and mix web data with a single category of curated data. We split our categories in books, conversations, and technical data as outlined in Table 5 For the individual corpora making these categories, we draw inspiration from The Pile (Gao et al., 2020) which we enhance with data from Reddit (Baumgartner et al. 2020) for the conversational category. Our web data is taken from RefinedWeb (Penedo et al., 2023) and we process curated sources through a similar pipeline, applying filtering and deduplication to make for a fair comparison. We evaluate performance on the zs-data aggregate-see Table 2 for detailed make-up.

Results. We plot the zero-shot accuracy as we increase the fraction of curated data in Fig. 4 , When combined with a strong web baseline, we find that the addition of curated data never significantly uplifts performance. In fact, excess of curated data even worsens performance: for books and technical, past $50 \%$, we start observing meaningful degradation of accuracy. We believe this is likely caused by "mode collapse" compared to the high diversity of web data.

Interestingly, conversations perform decently throughout, with the smallest degradation at $100 \%$. We posit this could either be due to our conversation category being the most diverse of the three, or to conversations being closer to the distribution of tasks. Indeed, this category likely includes people interacting, answering questions, and giving instructions to one another-a style which is less prevalent in books or technical-driven content like papers and patents.

Once again, this ablation is limited by the scope of the tasks considered. It is likely that highly technical tasks may benefit from domain specific data, and that web data would be a rather poor baseline for code tasks. Furthermore, our zero-shot evaluations are all done under a short context length: books could for instance be beneficial in helping the model learn long-range correlations.

Finding. When added in substitution of a strong web baseline, curated categories of data do not systematically result in an improvement in natural language zero-shot performance.

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-11.jpg?height=561&width=761&top_left_y=310&top_left_x=671)

Figure 4: When compared to a strong web data baseline, high-quality curated data does not improve zero-shot performance. Over-reliance on a single type of curated data is even detrimental to performance. The make-up of the conversations, books, and tech data corpora is outlined in Table 5 We report the zero-shot performance using different data-mixtures and compare with a baseline model trained on RefinedWeb only, our deduplicated and filtered web-only dataset Penedo et al. (2023). The shaded green-area indicates the $\pm 3 \sigma$ confidence interval based on 10 experiments across data splits.

### 4.2.3 Limited code and multilingual data do not strongly degrade English performance

Question. Can limited amounts (5-10\%) of code and multilingual data be substituted in pretraining data added without compromising the English performance of the model?

Multilinguality. Some of the first large-scale deployment of RNNs and Transformers were for machine translation (Wu et al., 2016); with the attention mechanism originally introduced for such use cases (Bahdanau et al., 2014). Accordingly, it's no surprise that multilingual language models rapidly flourished along their monolingual counterparts (Xue et al., 2021). However, multilingual generative large language models have remained more elusive. Indeed, both Lin et al. (2021) and Scao et al. (2022b) have reported that massive multilinguality comes at the expense of English performance, resulting in multilingual models that underperform their monolingual counterparts (Scao et al. 2022a). This so-called curse of multilinguality (Conneau et al. 2020) has lead practitioners to be weary of naively adding other languages in bulk - even PaLM (Chowdhery et al., 2022), which explicitly targets multilingual capabilities, only trains on $20 \%$ non-English data. Furthermore, multilingual data is scarcely available (Costa-jussÃ  et al. 2022): nearly $60 \%$ of the documents in CommonCrawl are English, and the distribution of top languages is skewed towards European ones. Notably, Mandarin Chinese is only the 6th top language in CommonCrawl, despite being 2nd worldwide, and Hindi does not show-up even in the top-20 despite being 3rd worldwide (Eberhard et al., 2023).

We choose to experiment with a restricted multilingual setup: we consider languages with a Latin alphabet, and focus on those for which we can collect a non-trivial amount out of CommonCrawl (over 10 billion tokens) using our data pipeline. Our splits for experiments are in Table 6. We train models with a fixed $10 \%$ multilingual data (weighing individual languages according to their prevalence in CommonCrawl), and evaluate on English tasks. Each data split uses a dedicated tokenizer.

Table 6: We split languages in three categories: English-only, Restricted (adding the 3 most spoken languages in Europe), and European. We only consider languages with a Latin alphabet, and sufficient presence in CommonCrawl to collect at least 10 billion tokens.

| Set | Languages |
| :--- | :--- |
| English | English |
| Restricted | English, German, Spanish, French |
| European | English, German, Spanish, French, Italian, Dutch, Polish, Portuguese, Czech, |
|  | Swedish, Romanian, Danish, Norwegian, Catalan, Slovene, Bulgarian |

Table 7: Including in the pretraining data a small fraction of code or multilingual data broadly does not degrade performance significantly, except on specific tasks. Overall, the degradation observed is barely measurable on our larger aggregate, and mostly driven by HellaSwag on the $\mathrm{zs}-\mathrm{small}$ one. Underlined values have crossed the likely 1- $\sigma$ degradation threshold.

| Pretraining data | Zero-shot accuracy <br> zs-main $\uparrow$ |  |
| :--- | :---: | :---: |
| zs-small $\uparrow$ |  |  |
| Likely threshold $(1-\sigma)$ | $\pm 1.0$ | $\pm 0.5$ |
| English-only | $\mathbf{5 3 . 7}$ | $\mathbf{4 9 . 2}$ |
| $10 \%$ Restricted | 53.4 | $\underline{48.3}$ |
| $10 \%$ European | 53.6 | $\underline{48.2}$ |
| $5 \%$ Code | 53.6 | $\underline{48.5}$ |

We present results in Table 7 . We find that the performance degradation from $10 \%$ multilinguality is very limited, and that the addition of other European languages over German, Spanish, and French do not drive additional degradation. We saw most of the reduction in performance on HellaSwag, while other tasks are not strongly impacted. We note that these experiments apply in a very restricted setting, and may not be illustrative of other multilingual setups (e.g., languages without a latin alphabet, using other languages to compensate for running out of English tokens, etc.)

Code. Large language models have demonstrated strong coding abilities, either through finetuning after general pretraining (Chen et al. 2021; Chowdhery et al. 2022, RoziÃ¨re et al. 2023), or through dedicated pretraining recipes (Li et al., 2023a). Furthermore, at variance with multilingual data, code data is plentiful (Kocetkov et al., 2022), with trillions of tokens available from crawling public repositories. Code tasks are a predominant application of LLM-powered assistants (Luo et al., 2023), and so allowing for such use cases with Falcon is important. However, we do not want to risk compromising language capabilities: we thus validate here the common practice of adding limited code data to pretraining, in line with other models (see Table 3 for common fractions in pretraining).

We select the top-30 programming languages from GitHub, and substitute $5 \%$ of our pretraining data for code. Note that we apply deduplication to the code dataset, but adjust our heuristics to avoid removing too much of the data. We consider only performance on English tasks. Results are outlined in Table 7 . We find the addition of code data to have a similar effect to multilinguality (if only weaker, perhaps because of the smaller proportion), with only little to no task degradation.

Finding. Small fractions of code and multilingual data (5-10\%), in line with common recipes for large language models, do not broadly impact zero-shot performance on English tasks.

We note that the small scale of our ablations is here a stronger limitation, likely leading us to a more conservative choice on multilinguality and code. It has been argued that larger models, thanks to their increased capacity, can better deal with multilinguality (Shaham et al., 2022). Code data has also been shown for larger models to boost commonsense abilities (Madaan et al. 2022). More broadly, a similar effect has been observed for multilingual models (Aghajanyan et al. 2023).

### 4.3 Architecture and pretraining: validating popular recipes, and inference optimizations

### 4.3.1 Extending multiquery into multigroup for tensor parallel training and inference

Background. Unanimously, large language models first adopted the multihead attention scheme described in Vaswani et al. (2017). Each token produces $n_{\text {head }}$ triplets of (query, keys, and values), and the result of each head is then summed to produce the final output of the attention module. However, this scheme can be altered. Shazeer (2019) found that one can share the same keys and values between all attention heads with only a small degradation in performance. In this so-called multiquery attention, the number of heads for the queries remains $n_{q}=n_{\text {head }}$ but there is only one head for the keys and values, $n_{k v}=1$. This significantly reduces inference memory consumption: during autoregressive generation, the keys and values are cached to accelerate generation-with multiquery, the $\mathrm{K}, \mathrm{V}$-cache size is divided by $n_{\text {head }}$ compared to vanilla attention, resulting in a 10 -100x reduction in memory consumption for common models-see Table 8 Multiquery improves the scalability of inference for large models (Pope et al. 2023). Chowdhery et al. (2022) has recently popularized this architectural modification, which has notably been adopted by LLaMA-2 (Touvron et al., 2023b).

Table 8: Multiquery/group schemes significantly reduces the size of the $\mathbf{K}$, V-cache for inference. Assuming $\mathrm{TP}=1$ for Falcon-7B and $\mathrm{TP}=8$ for Falcon-40/180B, sequence length 2,048.

| Attention scheme | $n_{q}$ | $n_{k v}$ | K,V-cache for a 2,048 sequence |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | $7 \mathrm{~B}$ | $40 \mathrm{~B}$ | $180 \mathrm{~B}$ |
| Vanilla | $n_{\text {head }}$ | $n_{\text {head }}$ | $O(\sqrt{N} \log (n))$ | $1 \mathrm{~GB}$ | $4 \mathrm{~GB}$ | $10 \mathrm{~GB}$ |
| Multiquery (Shazeer, 2019) | $n_{\text {head }}$ | 1 | $O(\log (N))$ | $20 \mathrm{MB}$ | $30 \mathrm{MB}$ | $40 \mathrm{MB}$ |
| Multigroup ( Ainslie et al. 2023$)$ | $n_{\text {head }}$ | $\mathrm{TP}$ | $O(\log (N))$ | N/A | $250 \mathrm{MB}$ | $335 \mathrm{MB}$ |

Scaling. Interestingly, multiquery is disproportionately effective for larger models. With $N$ the total number of parameters, $d_{\text {model }}$ the model size, $n_{\text {layer }}$ the number of layers, and assuming a fixed $d_{\text {head }}=d_{\text {model }} / n_{\text {head }}-$ which is typically the case when using FlashAttention (Dao et al., 2022). For efficient scaling, it is recommended that $n_{\text {layer }} \sim O(\log (N))$ (Levine et al., 2020); since we can approximate $N \simeq n_{\text {layer }}\left(d_{\text {model }}\right)^{2} \sqrt{\text { Kaplan et al. } 2020}$, it follows that the size of the K,V-cache with multihead attention scales in $O(\sqrt{N} \log (n))$. Conversely, for multiquery the K,V-cache only stores a fixed $2 d_{\text {head }}$ per layer, which does not increase with width; this results in overall more efficient scaling, in $O(\log (N))$ instead.

Multigroup. One caveat of multiquery attention is that it is difficult to efficiently parallelize when relying on tensor parallelism, as is common with GPU-based infrastructure (Shoeybi et al. 2019). Either each GPU keeps a copy of the shared key/value, recomputing them individually and then sharing gradients to keep them in sync, or they are computed on a single GPU and then communicated as necessary. We propose to introduce separate key/value pairs for each tensor parallel rank, simplifying the required communications. As in Shazeer (2019), we keep $n_{q}=n_{\text {head }}$, but now have $n_{k v}=\mathrm{TP}$. This scheme doesn't change the scaling of the $\mathrm{K}, \mathrm{V}$-cache, as it only applies a fixed TP factor. Concurrently to the development of the Falcon series, Ainslie et al. (2023) also proposed this modification; we refer to this variant of attention as grouped query attention or multigroup. We note that the communication reduction applies not just during inference, but also during training.

Results. We train 1B/3B models on 30/60B tokens from The Pile (Gao et al., 2020), with multiquery and varying degrees of multigroup attention. Importantly, we do not control for the reduction in parameters caused by the loss of additional keys and values-some degradation is thus expected. Results are presented in Table 9 . Both multiquery and multigroup do not result in any large reduction in zero-shot performance, even without compensating for the reduction in trainable parameters.

Recipe decision. To improve performance scalability for the largest models, the Falcon series implement multigroup with $\mathrm{KV}=\mathrm{TP}$ for all models (respectively 1/8/8 for Falcon-7/40/180B).

Table 9: Even without controlling for the reduction in parameters, multiquery only comes at a limited zero-shot performance cost. Impact on perplexity is more directly measurable, whereas impact on zero-shot performance is less consistent. Multigroup with $\mathrm{KV}=8$ consistently performs close to the vanilla baseline. Underlined values have crossed the very likely 2- $\sigma$ degradation threshold.

| Model size | KV | Performance <br> zs-main $\uparrow$ <br> $\pm 2.2$ | zs-small $\uparrow$ <br> $\pm 0.8$ | ppl-pile $\downarrow$ <br> $\pm 0.005$ |
| :---: | :---: | :---: | :---: | :---: |
| Very likely threshold $(2-\sigma)$ | 1 | 48.5 | 42.6 | $\underline{0.908}$ |
| 1B | 2 | 48.2 | $\underline{42.1}$ | 0.899 |
|  | 4 | 48.9 | 42.4 | $\underline{0.908}$ |
|  | 8 | 48.6 | 42.8 | $\underline{0.903}$ |
| 3B | 1 | $\mathbf{4 9 . 2}$ | $\mathbf{4 3 . 1}$ | $\underline{\mathbf{0 . 8 9 5}}$ |
|  | 8 | $\underline{\mathbf{5 4 . 6}}$ | $\underline{\mathbf{5 0 . 1}}$ | $\underline{\underline{0.825}}$ |
|  | Vanilla | 54.4 | 49.8 | $\mathbf{0 . 8 0 7}$ |

Table 10: Although at small-scale URPE and RoPE may likely be better than ALiBi, that advantage isn't as clear at increased size. We find ALiBi to be likely worst than rotary on two of our three aggregates for $1 \mathrm{~B}$ models, but to be closer to the performance of rotary at the $3 \mathrm{~B}$ scale. Underlined values have crossed the likely 1- $\sigma$ degradation threshold over RoPE.

| Model size | Pos. Emb. | Performance <br> zs-main $\uparrow$ <br> Likely threshold $(1-\sigma)$ | zs-small $\uparrow$ <br> $\pm 0.4$ | ppl-pile $\downarrow$ <br> $\pm 0.002$ |
| :---: | :---: | :---: | :---: | :---: |
| 1B | ALiBi | 49.2 | $\underline{43.1}$ | $\underline{0.895}$ |
|  | URPE | 49.6 | $\underline{43.1}$ | 0.885 |
|  | RoPE | $\mathbf{5 0 . 0}$ | $\underline{\mathbf{4 4 . 2}}$ | $\mathbf{0 . 8 8 3}$ |
| 3B | ALiBi | $\mathbf{5 4 . 4}$ | $\underline{49.8}$ | $\underline{0.807}$ |
|  | RoPE | $\mathbf{5 4 . 4}$ | $\underline{\mathbf{0 0 . 5}}$ | $\mathbf{0 . 7 9 9}$ |

### 4.3.2 Rotary positionnal embeddings may only offer a limited edge over ALiBi

Background. By default, attention does not provide positional information to the model: it only sees the sequence as a bag-of-word. Accordingly, the original Transformer architecture adopted absolute sinusoidal embeddings to encode positional information Vaswani et al. (2017). However, absolute embeddings have since declined in popularity, the community shifting to relative embeddings instead. While this shift is well motivated empirically (Shaw et al., 2018, Scao et al., 2022b), practionners have yet to crystallize on a single relative positional embedding: BLOOM and MPT (Scao et al., 2022a; MosaicML, 2023) use ALiBi (Press et al., 2022), while GPT-J, PaLM, and LLaMA (Wang and Komatsuzaki, 2021; Chowdhery et al., 2022; Touvron et al., 2023a bb use Rotary Positional Embeddings (RoPE) (Su et al. 2021). RoPE are often cited as delivering better upstream performance in the works above, while ALiBi benefits from built-in extrapolation abilities. Another recently introduced alternative are Universal Relative Positonal Embeddings, URPE (Luo et al., 2022), which address shortcomings in the expressivity of typical relative positional embeddings. As a curiosity, we note that the autoregressive mask of a causal model also provides some positionnal information to the model (Scao et al. 2022b, Haviv et al., 2022), enabling training without any positional embeddings to be comparable to absolute sinusoidal ones for zero-shot performance.

Concurrently to this work, recipes have emerged to enable zero-shot or finetuned length extrapolation with RoPE (Chen et al. 2023), briding the gap with ALiBi for extrapolation.

Results. We train 1/3B models on 30/60B tokens on The Pile (Gao et al., 2020). We report results in Table 10 We find no evidence for URPE outperforming RoPE-since it would require significant modifications to the fused-attention kernels to deliver acceptable performance, we do not pursue it further. At the 1B scale, we find a likely advantage to using RoPE over ALiBi; however, that advantage diminishes at the 3B scale, and is insufficient to conclude clearly. One remaining advantage of ALiBi is its compute overhead: it is significantly cheaper to compute than RoPE; however, with custom Triton kernels (Section 5.3.2) we are able to mitigate that overhead.

Recipe decision. In-line with other popular large-scale models, we adopt rotary positionnal embeddings, and use custom kernels to mitigate the overhead.

### 4.3.3 The extra memory cost of GLU may not be worth it for cost-efficient training

Background. Activations based on gated linear units (Shazeer 2020) are widely believed to outperform traditional activation functions such as GeLU (Scao et al. 2022b). They have seen adoption in models such as PaLM and LLaMA (Chowdhery et al., 2022; Touvron et al., 2023a b).

Scaling. Scaling-wise, GLU activations have been preferred as well, as they increase the size of the MLP (doubling its first layer), shifting more compute towards simple matrix multiplications. However, this does come at a cost: the memory required to store the intermediary activations in the MLP is higher. Remember that, typically, the inputs to the activation function are saved for the backward (as recomputing the function itself is negligible). For gated units, this input is now twice large. Overall, SwiGLU doubles intermediary activations, and increases by $50 \%$ the number of parameters in the MLP: the activation memory per parameter for the MLP is thus increased by $33 \%$.

Table 11: Small architectural tweaks like GLU, z-loss, and removing biases are unlikely to improve zero-shot performance. However, in some scenarios, these have been proposed to improve scalability and/or stability, which may warrant their adoption. Underlined values have crossed above the unlikely $0.4-\sigma$ improvement threshold over our baseline.

| Model size | Pos. Emb. | Performance <br> zs-main $\uparrow$ <br> zs-small $\uparrow$ | ppl-pile $\downarrow$ |  |
| :---: | :---: | :---: | :---: | :---: |
| Unlikely threshold $(0.4-\sigma)$ | $\pm 0.4$ | $\pm 0.2$ | $\pm 0.001$ |  |
| 1B |  | $\mathbf{4 9 . 2}$ | 43.1 | 0.895 |
|  | SwiGLU | $\mathbf{4 9 . 2}$ | 43.1 | $\underline{\mathbf{0 . 8 9 1}}$ |
|  | z-loss | 49.0 | $\underline{\mathbf{4 3 . 6}}$ | 0.895 |
| 3B |  | $\mathbf{5 4 . 5}$ | $\mathbf{4 9 . 8}$ | $\mathbf{0 . 8 0 7}$ |
|  | No biases | 54.4 | $\mathbf{4 9 . 8}$ | $\mathbf{0 . 8 0 7}$ |

Results. In Table 11, training 1B models on 30B tokens of The Pile, we find no clear benefits from adopting SwiGLU for zero-shot performance-and the improvement on perplexity is just at the threshold of being unlikely to characterize an actual improvement due to variance in the evaluation setup. We note that this could be due to the fact SwiGLU may require dedicated hyperparameters tuning-as for all architecture ablations, we simply adopted the ones of GPT-3 (Brown et al., 2020). Furthermore, we saw little additional throughput gains from SwiGLU (as we already use parallel attention/MLP layers). Since we train the Falcon series on 40GB A100 to optimize costs, we were concerned early on about memory consumption-accordingly, SwiGLU, with its increased memory intensity, and similar performance to GeLU, would likely be a net negative for us.

Recipe decision. Out of concern for the memory footprint of our trainings on A100-40GB, and because of no clear uplift in zero-shot, we choose not to adopt SwiGLU.

### 4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers

Parallel attention and MLP blocks. Wang and Komatsuzaki(2021) first introduced parallel attention and MLP layers while training GPT-J. This augmentation is important to reduce the communication costs associated with tensor parallelism: this simple modification cuts the number of all_reduce necessary from two to one per layer. We found no measurable degradation in zero-shot performance or perplexity, in line with Chowdhery et al. (2022), and adopt this practice. See Fig. 5 for an illustration.

No biases. Chowdhery et al. (2022) found that removing the biases in the linear layers and layer norms improves stability. We validate that removing the biases in the linear layer does not result in worse performance (see Table 11): neither in terms of language modeling loss nor in terms of the final zero-shot performance. Accordingly, we remove biases from the linear layers in the Falcon series.

Recipe decision. We adopt parallel attention and MLP, and remove biases from linear layers.

### 4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search

Z-loss. First introduced in the mesh-tensorflow codebase 2 Shazeer et al. (2018), z-loss aims at increasing the stability of training, by encouraging the logits to stay close to zero. It can be implemented as an auxiliary loss: z_loss $=10^{-4} \log ^{2}\left(\Sigma_{i} e^{z_{i}}\right)$, where $z_{i}$ is the output logits of the model. Note that z-loss does not have a significant impact on task performance at small scale (Table 11).

Recipe decision. We adopt z-loss, as it is claimed to improve large-scale training stability and does not impact zero-shot performance in our ablations.[^1]![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-16.jpg?height=560&width=696&top_left_y=237&top_left_x=712)

Figure 5: Parallelizing the attention and MLP blocks allows us to remove one sync point during tensor parallel training. This was first proposed by Wang and Komatsuzaki (2021) for GPT-J.

Weight decay. We attempted to reproduce the weight decay schedule from Chowdhery et al. (2022), but failed to obtain an improvement-we suspect this is due to differences in initialization. We found that weight decay has a disproportionate effect on datasets which have not been deduplicated/are of lower quality (Table 12). We use AdamW for weight decay (Loshchilov and Hutter, 2018).

Recipe decision. We use a fixed weight decay of 0.1 with AdamW for all Falcon models.

Optimal learning rate. Practices for setting the learning rate of a run differs, from naive grid search to more principled approaches (Yang et al. 2022; Dinan et al. 2023). We further discuss in Section 4.4 our failures to implement and reproduce some of the later; in this short section, we focus on the naive approach and our validation of it. Broadly speaking, setting too high of a learning rate risks to cause divergence of the run and instabilities during training; too low, on the opposite, will leave some upstream and downstream performance on the table, leading to inefficient training.

We propose to search through possible learning rate in the following away: (1) we select 4-6 roughly logarithmically-spaced candidate learning rates, anchoring around the ones used in GPT-3 (Brown et al. 2020), and favoring higher learning rates; (2) we run through a long 500 million tokens warmup for all candidate learning rates; (3) we pick the learning rate which has achieved the lowest loss at this point, and discard any learning rate which has already caused spikes.

We test this method at small scale, picking 7 learning rates for a 1B model, and then comparing the ranking obtained after warmup against the actual final ranking achieved. Results are presented in Table 13 We find that this naive method succeeds at finding the best learning rate from the rankings at the end of warm-up. More broadly, ranks at the end of warm-up and at the end of training are relatively stable, with only the 2nd and 3rd best switching places.

Recipe decision. From candidates LRs, we pick the one with the lowest loss after warm-up.

Table 12: Weight decay is likely to improve performance, especially so for datasets such as The Pile, which may not have been adequately deduplicated. Surprisingly, the effect of weight decay is disproportionately strong depending on the underlying dataset.

| Dataset | Weight decay | Performance |  |  |
| :---: | :---: | :---: | :---: | :---: |
| zs-main $\uparrow$ | zs-small $\uparrow$ | ppl-pile $\downarrow$ |  |  |
| Likely threshold $(1-\sigma)$ | $\pm 1.1$ | $\pm 0.4$ | $\pm 0.002$ |  |
| RefinedWeb | 0. | $\mathbf{5 2 . 1}$ | 47.9 | 1.07 |
|  | 1. | 52.0 | $\underline{\mathbf{4 8 . 4}}$ | $\underline{\mathbf{1 . 0 6}}$ |
| The Pile | 0. | 50.3 | 43.7 | 0.877 |
|  | 1. | $\underline{\mathbf{5 1 . 7}}$ | $\underline{\mathbf{4 5 . 0}}$ | $\underline{\mathbf{0 . 8 6 8}}$ |

Table 13: Loss rankings at the end of learning rate warm-up broadly reflects rankings at the end of training, enabling us to search for optimal learning rates efficiently This simple heuristic is easy to use, and consume only a fraction of resources for a larger run.

| Learning rate | End LR warm-up [0.5GT] |  | End of run [27GT] |  | Run stability |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | LR | Improv. $\downarrow$ | Rank | Improv. $\downarrow$ | Rank |  |
| x1 | $2 \times 10^{-4}$ | $\pm 0.0 \%$ | $(4)$ | $\pm 0.0 \%$ | $(4)$ | No spikes |
| x2 | $4 \times 10^{-4}$ | $-\frac{-2.0 \%}{-2.6 \%}$ | $(2)$ | $-1.7 \%$ | $(3)$ | No spikes |
| x5 | $1 \times 10^{-3}$ | $-1)$ | $-\mathbf{2 . 5 \%}$ | $(1)$ | No spikes |  |
| x10 | $2 \times 10^{-3}$ | $-1.4 \%$ | $(3)$ | $\underline{-1.9 \%}$ | $(2)$ | One small spike |
| x20 | $4 \times 10^{-3}$ | $+1.3 \%$ | $(5)$ | $+1.9 \%$ | $(5)$ | Multiple spikes |
| x50 | $1 \times 10^{-2}$ | $+7.6 \%$ | $(6)$ | $+6.8 \%$ | $(6)$ | Multiple large spikes |
| x100 | $2 \times 10^{-2}$ | Diverging after |  |  |  |  |

### 4.4 Further experimentation required: ideas that did not make the cut

In this section, we briefly mention some practices and ideas we experimented with, but for which we were unable to reproduce results or obtain a meaningful outcome. We note that this is not be viewed as an indictment of these practices: many have been adopted in popular models successfully.

Alternative training objectives. The largest of language models have been typically trained with a causal decoder-only architecture and objective (Brown et al. 2020, Rae et al. 2021, Chowdhery et al., 2022). Wang et al. (2022a) found that such models exhibit better zero-shot abilities than masked encoder-decoders such as T5 (Raffel et al., 2019); however, they also found that after multitask finetuning (Sanh et al. 2021), masked encoder-decoder performed better, highlighting that different regimes and use cases may favor different architectures and objective. Throughout, a non-causal decoder-only (so-called prefix language model) performed competitively as a close second. With UL2, Tay et al. (2022a) found that these paradigms could be unified, by training on a mixture of objectives instead. We experimented with UL2, but were unable to obtain an uplift in zero-shot performance, even after adapting tasks to the various paradigms when relevant. Due to time and ressource constraints, we did not end-up pushing our experiments further, as Tay et al. (2022b) showed that a posteriori adaptation to the UL2 objective was not only possible but efficient.

For code models, so-called fill-in-the-middle (FIM) training (Bavarian et al. 2022) has been popular, as it addresses a common use cases for such models. FIM is claimed to come at little to no expense of autoregressive modeling capabilities; we were broadly able to confirm these results, showcasing likely degradation only for a handful of tasks for intermediary infilling rates (0.25-0.5). Low and high infilling rates had the lowest effect on zero-shot performance. Nevertheless, due to lack of wide adoption at the time, we choose to skip FIM, and to instead consider it as an adaptation step for a Falcon-Coder model-this was concurrently demonstrated for Code-LLaMA (RoziÃ¨re et al., 2023).

Principled hyperparameters. We experimented with $\mu$-parametrization (Yang et al., 2022) as a way to scale our hyperparameters in a principled way from smaller to larger runs. Unfortunately, we were unable to demonstrate an improvement over our naive strategy of hyperparameters estimation.

Alternative optimizers. Encouraged by its strong reported results on BERT and T5, we experimented with Amos (Tian and Parikh, 2022), an alternative to Adam with adaptive learning rate and weight decay. We were unable to obtain an improvement for causal decoder-only models, even at small scale.

Conversation mining. In developing RefinedWeb, we experimented with the idea of mining specific types of data from the web. Training dedicated classifiers based on BERT models often resulted in over-fitting, so we instead used simple heuristics (e.g., identifying arguments by the density of transition words, conversations by finding turns of users). We were able to obtain significant zero-shot performance uplift, however this data was very scarce. Since training of the models was started before we had processed all of CommonCrawl, we could not effectively surface all of that data and use it in priority over standard web data, leading us to leave that idea for future models.

### 4.5 Wrapping-it up: validating overall dataset and architecture recipes

For convenience, we refer readers to Section? for a full overview of the Falcon recipe. We will now validate the performance of the recipe at larger scale, with a longer training run and comparisons with models from the state-of-the-art. We independently verify: (1) the pretraining dataset, in particular its web component; (2) the architecture, by training a model adopting it on The Pile.

Dataset validation. We train 1B and 7B parameters models for 27B tokens and 350B tokens, to reproduce common practices from previous models. We use our baseline architecture, which is based on GPT-3 (Brown et al., 2020) with ALiBi (Press et al. 2022). We train on The Pile (Gao et al. 2020), RefinedWeb (our web dataset, Penedo et al. (2023)), and the Falcon data mixture which combines RefinedWeb and curated sources without any upsampling. This last mixture is designed for Falcon-180B, and targets a total of 3,500B tokens. See Section 5.1 for details.

In Table 14, we find our data mixture significantly uplifts performance. The large majority of these gains are also achieved by RefinedWeb alone, without the curated data. This highlights our previous finding that web data alone, when adequately filtered and deduplicated, can train performant models. We find our 1/7B model compares favorably with other models from the state-of-the-art, however we note our setup put it at a small advantage by training for slightly longer.

Architecture validation. We follow the set-up of our dataset validation, and train the architecture validation 1B models on The Pile for 27B and 350B tokens. Note that we do not include in this experiment hyperparameters tweaks: all models use weight decay, and the same learning rate from Brown et al. (2020-this experiment concerns only the architecture of the models itself.

We find our architecture comes with a small performance degradation (Table 14, which we mostly attribute to the reduction in parameters caused by multiquery. We suspect that growing up the multiquery model would likely close that gap. Nevertheless, it's interesting to note that while data improvements have very significant effects, architecture improvements are mostly focused on improving training and inference scalability; they do not result in an uplift in task performance.

Table 14: Our data recipe, predominantly based on our work on RefinedWeb Penedo et all. 2023), significantly improves upon The Pile and other models from the state-of-the-art. Because multiquery makes models smaller (and we do not control for that effect), our architecture comes with a small zero-shot performance degradation. We suspect controlling for parameter count would bring our architecture to be on-par or better than the baseline. Nevertheless, improvements to the architecture mostly deliver improvements in hardware scalability for inference and training, while improvements to the data recipe significantly uplift the downstream performance of models. Underline for relevant changes, bold for improvement over baseline, italics for degradation over baseline. ${ }^{\dagger}$ flags independent evaluations with the EleutherAI Harness (Gao et al., 2021), and ${ }^{\text {a }}$ indicates our architecture run is better, while ${ }^{\mathrm{d}}$ shows our data run is better.

| Scale | Dataset | Architecture | Performance <br> zs-main $\uparrow$ | zs-comp $\uparrow$ | zs-small $\uparrow$ | ppl-pile $\downarrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 1B@27GT | The Pile | Baseline | 51.7 | 40.3 | 45.0 | 0.868 |
|  | Falcon | Baseline | 53.5 | 42.3 | 48.8 |  |
|  | $\underline{\text { RefinedWeb }}$ | Baseline | 53.2 | 43.4 | 48.4 |  |
|  | The Pile | Falcon | 51.1 | 40.0 | 45.1 | 0.870 |
| 1B@350GT | The Pile | Baseline | 57.8 | 47.1 | 54.0 | 0.763 |
|  | RefinedWeb | Baseline | 59.8 | $\mathbf{5 0 . 1}$ | 55.7 |  |
|  | The Pile | $\underline{\text { Falcon }}$ | 56.6 | 46.1 | 52.7 | 0.775 |
| 1B@300GT | $\overline{\text { OpenAI }}$ | babbage $^{\dagger}$ |  | $47.8^{\mathrm{d}}$ |  |  |
| 1B@380GT | The Pile | GPT-Neo $^{\dagger}$ |  | $44.3^{\mathrm{a}, \mathrm{d}}$ |  |  |
| 1B@300GT | The Pile | $\mathrm{BS}-\mathrm{A} \& \mathrm{~S}^{\dagger}$ |  | $46.1^{\mathrm{d}}$ |  |  |
| 1B@300GT | The Pile | Pythia $^{\dagger}$ |  | $45.2^{\mathrm{a}, \mathrm{d}}$ |  |  |
| 7B@350GT | RefinedWeb | Baseline |  | 55.3 |  |  |
| 6B@300GT | OpenAI | curie $^{\dagger}$ |  | $53.7^{\mathrm{d}}$ |  |  |
| 6B@400GT | The Pile | GPT-J $\mathrm{J}^{\dagger}$ |  | $53.5^{\mathrm{d}}$ |  |  |

## 5 Implementation

Based on our findings from the previous ablations Section 4, and further tests and best practices from the literature, we now describe the codebases and methods used to train the Falcon series of models.

### 5.1 The Falcon dataset: predominantly web, with added curated and conversational data

Based on estimates of our compute budget of 30,000-50,000 PF-days, we target a pretraining dataset size in the range of $\mathbf{3 , 0 0 0 - 5 , 0 0 0}$ billion tokens-we use Hoffmann et al. (2022) as an upper boundary for model size and lower boundary for pretraining length. This is more than $2 \mathrm{x}$ the size of the dataset for Chinchilla, and 10x the one for GPT-3; although this range of size is recently becoming more common in concurrent models like LLaMA-2 or OLMo (Touvron et al. 2023b; Soldaini et al., 2023). Out of concerns for memorization and degradation caused by repeating data (Carlini et al., 2022, Hernandez et al. 2022) we choose to not upsample any sources.

High-level overview. In Section 4.2.1 we have shown that sufficiently filtered and deduplicated web data can deliver performant models: this leads to focus on scaling-up web data to achieve the scale necessary. Because improvements to data quality translate to significant improvements to downstream performance, and because data processing is tremendously cheaper than model training, we do not concern ourselves too much with optimizing for costs with data processing-it is likely that a $90 \%$ cheaper recipe with less than a $10 \%$ relative performance degradation could be found.

We still include a small amount of curated data, inspired by The Pile (Gao et al., 2020) with the addition of conversations from Reddit (Baumgartner et al. 2020), as it is unlikely to degrade performance if adequately processed (Section 4.2.2) and as it may broaden the downstream applicability of the model. However, these sources are bound to remain a minority, given that we do not allow any upsampling-they end-up accounting for $13 \%$ of our final dataset. Regarding code and multilinguality, we take a conservative approach: based on our results in Section 4.2.3 we include $8 \%$ multilingual data and 3\% code. These lower fractions than the ones experimented with are due to stock constraint; specifically for code, further improvements to our pipeline enabled us to significantly scale availability, but this was after the models had started training, so we did not revise the mix.

The final Falcon mixture is presented in Table 15 We designed the mixture based on a 3,500B tokens pretraining dataset, not allowing any upsampling of the curated sources. Despite differing training lengths, the same mixture (in \%) is used for Falcon-7B, 40B, and 180B.

Table 15: The final Falcon mixture is predominantly web-based (nearly $85 \%$ ), but includes other curated corpora (without any upsampling) to broaden the expressiveness of the model. Individual curated corpora are inspired from The Pile (Gao et al., 2020), but rebuilt from scratch to ensure high-quality and compatibility with our data pipeline. Code stock is a rough estimate based on an updated pipeline; code data used in Falcon was sourced from permissively licensed GitHub repositories. Mixture was designed to avoid upsampling; note that total stocks for RefinedWeb were not known at the beginning of training, as processing was still in-progress. Quantities in tokens.

| Corpora <br> Name | Source | Stock | Pretraining |  |
| :---: | :---: | :---: | :---: | :---: |
|  |  |  | Fraction | Used |
| RefinedWeb-English | Filtered and deduplicated Common- <br> Crawl, see Penedo et al. (2023) | $\sim 5,000 \mathrm{~B}$ | $76 \%$ | $2,700 \mathrm{~B}$ |
| RefinedWeb-Euro | Filtered and deduplicated multi- <br> lingual (Europe-focused) Common- <br> Crawl, see Penedo et al. (2023) | $\sim 2,000 \mathrm{~B}$ | $8 \%$ | 400B |
| Books | Project Gutenberg | 215B | $6 \%$ | 214B |
| Conversations | Reddit, StackOverflow, Hack- <br> erNews, IRC, YouTube Subtitles | $170 \mathrm{~B}$ | $5 \%$ | 168B |
| Code | GitHub | $\sim 1,000 \mathrm{~B}$ | $3 \%$ | 115B |
| Technical | arXiv, PubMed, USPTO, Wikipedia | 60B | $2 \%$ | 57B |

### 5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset

Our web data processing pipeline is extensively described in the dedicated paper RefinedWeb paper (Penedo et al. 2023). In this section, we only highlight key components and decisions.

To scale-up pretraining data, two approaches are possible:

- Repeat data. This is the easiest option, and was the norm in computer vision originally. However, most large language models have been far more conservative, usually only upsampling specific corpora for 2-6 times (see Table 3). This is largely due to concerns with memorization (Carlini et al., 2022) and with deduplicates disproportionately degrading the quality of models (Lee et al., 2022, Hernandez et al., 2022). Recently, Muennighoff et al. (2023) has argued that while up to 4 epochs on the same data may be acceptable, further repetition will cause degradation-this leads us to eschew this strategy.
- Scale-up web data processing. While scaling curated sources is cumbersome and requires extensive manual work, web data is a massive, plentiful source. Improvements to web data have high leverage, as they impact a large amount of tokens at once: public crawls such as CommonCrawl may contain in excess of 50-100 trillion tokens, such that even a $90 \%$ rejection rate would result in a trillion-scale dataset. However, raw web data is also of extremely poor quality (Trinh and Le, 2018, Kreutzer et al., 2022), containing large amounts of undesirable adult content and machine generated spam. We choose to focus our work on improving the quality of web data, through large-scale filtering and deduplication.

These approaches are orthogonal, but not antagonist; scaling to frontier models, with pretraining datastes of 10-100 trillion tokens, will likely require repeating massive web datasets for a few epochs.

Philosophy. RefinedWeb differentiates itself from previous web datasets in the following ways:

- Extreme-scale. Our Macrodata Refinement pipeline focuses on scalability: we used up to 20,000 CPU cores to produce RefinedWeb. With nearly five trillion deduplicated tokens, the RefinedWeb dataset is the largest documented pretraining dataset, supporting the training of larger models than previously though possible without relying on multiple epochs.
- Stringent deduplication and filtering. Inspired by Lee et al. (2022), RefinedWeb is fully deduplicated. Fuzzy deduplication with MinHash is used to first massively shrink the dataset, and then extract substring deduplication is applied. Filtering heuristics are also first used to reduce text extraction artefacts, and to remove machine-generated content. We find in Section 4.2.1 that this allows web data to match curated corpora.
- Neutral filtering. With the exception of language identification, the Macrodata Refinement pipeline does not rely on ML-based filtering strategies. Indeed, such filters can easily introduce or amplify biases into the data Dodge et al. (2021); Welbl et al. (2021).

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-20.jpg?height=548&width=986&top_left_y=1748&top_left_x=575)

Figure 6: Subsequent stages of Macrodata Refinement remove nearly $90 \%$ of the documents originally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the data available: around $50 \%$ of documents are discarded for not being English, $24 \%$ of remaining for being of insufficient quality, and $12 \%$ for being duplicates. We report removal rate (grey) with respect to each previous stage, and kept rate (shade) overall. Figure from Penedo et al. (2023).

Overview. The Macrodata Refinement pipeline is split into three subsequent stages (see also Fig. 6 for detailed removal rates): (1) document preparation; (2) filtering; (3) deduplication.

For the document preparation, before undertaking any compute-heavy processing, we first filter documents based on the URL alone, using a blocklist of adult sites and scoring URLs based on their name. We found that the preprocessed.WET files offered by CommonCrawl still contain undesirable content (e.g., navigation menus) so we instead process raw .WARC files (HTML response) with trafilatura to extract natural text. Finally, we use the fastText classifier from CCNet (Wenzek et al. 2020) to identify the top language of documents. For English, about $48 \%$ of documents remain.

In the filtering stage, we apply a number of heuristics to remove repeated text (which may be an artefact of crawling/text extraction), and documents which are outliers in terms of length, symbolto-word ratio, etc. These heuristics are inspired by Rae et al. (2021). We also introduce so-called line-wise corrections, which remove lingering artefacts such as likes counter or navigation buttons. The size of the suitable data is against halved, resulting in about $23 \%$ of CommonCrawl being kept.

Finally, we apply large-scale deduplication in two steps: first, we remove approximate duplicates at the document-level with MinHash (Broder, 1997), before removing exact substring matches with a suffix array (Manber and Myers, 1993). With the deduplication settings of Lee et al. (2022), this results in a final halving of the usable data, down to only about $12 \%$ of the total data in CommonCrawl.

### 5.1.2 The Microdata curated corpora and conversational masking

In Section 4.2.2 we found that adding curated data from conversations, books, or technical sources did not further improve performance on top of a strong web baseline like RefinedWeb. However, we believe this sort of data, along with code, can broaden the expressiveness of the model, and its applicability to different kinds of downstream tasks not captured by our evaluation setup. Accordingly, we also add a small fraction of curated data, with individual sources inspired from (Gao et al., 2020). We also add data from Reddit (Baumgartner et al. 2020), and introduce a new attention masking strategy for formatting tree-like conversations efficiently. On top of the Microdata curated corpora, we apply the Macrodata Refinement pipeline with adjusted filter settings (e.g., tuning document length thresholds for books) and deduplicate individual corpora.

Components from The Pile. We reuse individual components of The Pile (Gao et al., 2020), except where there are significant quality or licensing concerns. In all cases, we reimplement them from scratch, to ensure the formats match our data pipeline. We introduced a number of special tokens to reproduce structured information in curated corpora and better control generation: Â»TITLEÂ«, Â»ABSTRACTÂ«, Â»INTRODUCTIONÂ«, and Â»COMMENTÂ«. When ITEX files are available, we convert them to markdown, similar to Lewkowycz et al. (2022). Heuristics from Macrodata Refinement where hand-tuned for each corpora, manually analysing rejected and accepted samples; for books, we also introduce new rules to remove irrelevant content such as indexes, disclaimers, or tables of content.

Conversational data. Increasingly, large language models are deployed in "chatty" use cases, with back and forth interactions between users and models (Adiwardana et al., 2020, Zheng et al. 2023). Altough models are adapted downstream to this use case, we put a focus on enhancing pretraining with conversational data; we notably add data from Reddit (Baumgartner et al., 2020).

Conversation trees and attention masking. One issue with using data from online forums such as Reddit or HackerNews is that this data is formatted in trees, with diverging turns of conversation between users. Past models have sampled trajectories from these trees (Thoppilan et al., 2022; Chowdhery et al. 2022), but this either means that data has to be repeated, or some trajectories left out. Instead, we find that we can use the attention mask to encode a tree-like structure, allowing later comments to only attend to comments from their trajectory, ignoring "side" comments. Conversations are serialized into sequences in depth-first order, and then comments/turns are masked if they are not relevant to the current one (i.e., in a different branch or deeper). For positionnal embeddings, we use the depth of the tree, which naturally serializes the conversation. We illustrate with pseudocode for converting depth first positions to an attention mask in Appendix F.2. We also employ this strategy to mask documents from one another, instead of relying on the $<$ EOD $>$ token alone. Note that cursory experiments did not find a benefit for zero-shot performance.

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-22.jpg?height=193&width=894&top_left_y=234&top_left_x=453)

| Do <br> 0 | you <br> 1 | like <br> 2 | Paris? <br> 3 | Not <br> 4 | really, <br> 5 | it's <br> 6 | too |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 7 | dirty! <br> 8 | Oh, <br> 9 | I <br> 10 | agree <br> 11 | Really? | it's | so |
| 10 | 11 | nice! | Yes! | It's | lovely! |  |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-22.jpg?height=304&width=306&top_left_y=585&top_left_x=888)

Figure 7: Tree-like attention masking enables us to realize all conversation trajectories without sampling or repeating data. The tree is serialized depth-first, where the position of the token is the depth in the tree. This allows us to train on all conversations in the tree at the same time, without repeating the early turns of the conversation. For instance, the passage Yes! It's lovely, in pink, has visibility on Do you like Paris? in green and causal attention on itself, but cannot see other passages.

### 5.2 The Falcon architecture and recipe for efficient inference and (stable) training

Our goals with the Falcon architecture are to maximize training and inference efficiency, while minimizing impact to downstream performance and risks for the models. In Section 4.3 we outlined a number of decisions we made based on the ablations:

- Architecture. We use multigroup attention (Section 4.3.1) to improve the scalability of inference, an extension of multiquery (Shazeer 2019); we use rotary embeddings (Su et al. 2021); we do not use GLU (Shazeer 2020) because of the increased memory footprint, and use vanilla GeLU instead; we use parallel attention and MLP blocks (Wang and Komatsuzaki 2021) and remove biases from linear layers (Chowdhery et al., 2022).
- Hyperparameters. We use z-loss to help with stability (Shazeer et al., 2018); we use a fixed 0.1 weight decay; we perform a learning rate logarithmic grid search during warm-up and pick the learning rate with the lowest loss at the end of warm-up.


### 5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up

Layer norms. When using parallel attention, it is possible to either have separate layer norms for the MLP and the attention block (closer to the vanilla Transformer architecture), or to use a unified layer norm for both. Since the gradient computation with regards to the input of the layer norm is linear, it is possible to maintain the favourable communication volume of parallel attention and MLP while using two layer norms. Furthermore, after training, it is possible to merge the two layer norms back into one, by multiplying in the weights and biases of the layer norm into the subsequent linear layer. This leads us to stay close to the vanilla architecture, and to use two separate layer norms; however, we note this introduces unnecessary additional complexity for downstream conversion to other popular formats. For Falcon-7B (later trained), we switched to a single layer norm.

Tied embeddings. Tying embeddings is a ubiquitous practice for Transformer models: the embedding weights converting the tokens $x$ into $z^{0}=x W$ are the same that converts the embedding back into the predicted logits $p=z^{n} W^{T}$. Although it is still used by most recent LLMs (GPT-3 Brown et al. (2020), PaLM Chowdhery et al. (2022), LLaMA Touvron et al. (2023a)), the original motivations (Press and Wolf, 2016; Inan et al., 2016) may not entirely be relevant. Notably, weight sharing was at the time used to reduce the size of models; but for models with over 100 billion parameters, embeddings are not a significant fraction of the parameters. Furthermore, weight sharing poses challenges in distributed training, as it requires additionnal communications. The semantic argument remains, but cursory experiments showed no strong impact at the 1B parameters scale. Still, in the interest of not adding additional risks to the training of the Falcon series, we keep our embeddings tied.

Table 16: Summary of the shape, hyperparameters, and distribution strategy of the Falcon models. Falcon-7B was trained after Falcon-40/180B, with an experimental increased batch size.

|  | Falcon-7B | Falcon-40B | Falcon-180B |
| :--- | :---: | :---: | :---: |
| Data | $1,500 \mathrm{~B}$ | $1,000 \mathrm{~B}$ | $3,500 \mathrm{~B}$ |
| Shape |  |  |  |
| $n_{\text {layer }}$ | 32 | 60 | 80 |
| $d_{\text {model }}$ | 4,544 | 8,192 | 14,848 |
| $d_{\text {head }}$ |  | 64 |  |
| $n_{\mathrm{q}}$ | 71 | 128 | 232 |
| $n_{\text {kv }}$ | 1 | 8 | 8 |
| $d_{\text {vocab }}$ |  | 65,024 |  |
| $n_{\text {tokens }}$ | 2,048 |  |  |
| Pretraining |  |  |  |
| Learning rate | $6 \times 10^{-4}$ | $1.85 \times 10^{-4}$ | $1.25 \times 10^{-4}$ |
| Decay |  | Cosine, divides by 10 |  |
| $\quad$ Ramp-up | $4 \mathrm{~B}$ | $4 \mathrm{~B}$ |  |
| Batch-size | 2,304 | 1,152 | 2,048 |
| $\quad$ Warm-up | $30 \mathrm{~B}$ | $100 \mathrm{~B}$ | $100 \mathrm{~B}$ |
| Weight decay |  | 0.1 |  |
| Gradient clipping | 1. | 0.6 | 0.4 |
| Z-loss |  | $1 \times 10^{-4}$ |  |
| Parallelism |  |  |  |
| TP | 1 | 8 | 8 |
| PP | 2 | 4 | 6 |
| DP | 192 |  |  |

Vocabulary size. The size of the vocabulary in language models can differ widely from character level models with 256 entries (Xue et al. 2022) to massively multilingual models with millions (Liang et al. 2023). For generative models, practices have collapsed around two modes: vocabularies in the 30-60k range for monolingual models (Brown et al., 2020, Rae et al., 2021, Touvron et al. 2023a), or in the $+100 \mathrm{k}$ range for models more inclined towards multilinguality (Scao et al. 2022a; Chowdhery et al., 2022). Although larger vocabularies may have better fertility, and hence yield faster inference per byte of text, they also come with some caveats: from a scalability perspective, they can lead to unbalanced pipeline stages, and may require more storage space; also, it is unclear whether models optimally use them (Lieber et al. 2021). We train our tokenizer on a vocabulary size of 65,024 , and store vocabulary information in a 16bit unsigned integer-this leaves about 500 extra values to use for downstream adaptations (e.g., paradigm tokens for UL2 Tay et al. (2022b)).

Model scaling. We outline the shape and hyperparameters of the Falcon models in Table 16. When increasing compute budget, resources can be either spent towards a larger model (increased parameter count) or towards longer training (increased token count). Hoffmann et al. (2022) recommends a joint (i.e., equal) increase for optimal scaling. However, this finding should be nuanced in two ways: (1) increasing parameter count also increases downstream inference costs, which can be significant if a model is widely deployed; (2) on the other hand, if data-constrained, increasing the size may be a way to trade compute for increased downstream performance. For Falcon, we choose to use Hoffmann et al. (2022) as a lower bound for pretraining length and as an upper bound for model size.

Beyond model size, there is also the question of how to shape these parameters: e.g., should they be allocated to make a deeper or shallower model, to widen attention heads or to increase their count. Scao et al. (2022b) conducted a short review, highlighting that model depth is typically only scaled logarithmically with total parameter count (Levine et al., 2020). Practices around attention head size however varied. We broadly follow the logarithmic depth scaling recommendation, and uses a fixed attention head size of 64 to optimize for performance with FlashAttention (Dao et al., 2022). We also fix our number of queries in multigroup to be equal to the number of tensor parallel degrees used during training, and train for a fixed sequence length of 2,048 . We note that this context length can be efficiently increased with an a posteriori adaptation (Chen et al. 2023).

### 5.2.2 Large language model alchemy: hyperparameters for pretraining

We report hyperparameters used during pretraining in Table 16, and highlight some decisions below.

Learning rate search. The procedure we describe in Section 4.3 .5 results in a learning rate of $6 \times 10^{-4}$, $1.85 \times 10^{-4}$, and $1.15 \times 10^{-4}$ for Falcon-7, 40, and 180B respectively. This is significantly higher than learning rates reported by previous models, but we found our training runs to be (mostly) stable. In hindsight we believe our search procedure may result in higher learning rates than optimal-an acceptable tradeoff, since recovering from spikes is relatively easy (Section 5.4).

Learning rate ramp-up. We perform a long ramp-up over 4 billion tokens for all models.

Batch size warm-up. Practices around batch-size warm-up have surprisingly diverged widely depending on the underlying hardware: models trained on GPUs, such as GPT-3 (Brown et al., 2020), BLOOM (Scao et al. 2022a), or MT-NLG (Smith et al., 2022), have typically performed a fined-grained warm-up for 10-20 billion tokens; meanwhile, models trained on TPU, such as Gopher (Rae et al. 2021), Chinchilla (Hoffmann et al. 2022), or PaLM (Chowdhery et al. 2022) often double the batch size mid-training or take larger batch size steps at $25 / 50 \%$ through training. Some recent models have also elected to skip batch size warm-up entirely (Zhang et al., 2022; Touvron et al. $2023 \mathrm{a} \mathrm{b}$ ). At small scale, we found longer warm-ups to never hurt downstream performance, and in fact to generally deliver better models; this leads us to adopt a long warm-up strategy, over 100 billion tokens-note that we are able to scale the data parallelism degree and overall size of the cluster during training, which means this comes at limited throughput cost. For Falcon-7B, wall-clock time constraints lead us to opt for an accelerated schedule over only 30 billion tokens.

Gradient clipping. We set the treshold for gradient clipping to 0.6 for Falcon-40B. For Falcon-180B, we initially started with 0.6 but later reduced it to 0.4 to improve training stability. In both cases, we tuned the gradient clipping threshold to only affects outlier events and not the vast majority of the training steps. For Falcon-7B, we set it at 1 . We find that training at this scale is anyway not really prone to instabilities, even with the large batch size we adopted.

Optimizer. We use AdamW (Loshchilov and Hutter, 2017): it is the most commonly used optimizer, and has been proven time and time again to perform well, both in general and for large language model training in particular. To increase performance during training we use the fused optimizer kernel from Megatron Shoeybi et al. (2019). However, we note that fused kernels for optimizers are less important in large scale training, especially when when optimizer sharding is used.

No dropout. As large language models are typically trained for a single epoch of relatively unique data, they typically do not use dropout (Srivastava et al., 2014). Shortage of data and recent papers suggesting a few epochs may be tolerable (Xue et al.. 2023) will surely challenge this practice as future model require tens of trillions of tokens, but for the Falcon series we did not use dropout.

### 5.3 Large-scale distributed training on cloud infrastructure with Gigatron

Rather than training on expensive dedicated HPC resources, we elected to train the Falcon series on cloud infrastructure to improve cost-efficiency. The Falcon series was trained on clusters of p4d on AWS-with up to 4,096 A100s for Falcon-180B. Key metrics for our training infrastructure include:

- Nodes with $8 \times$ A100 40GB. We found that configurations with 4x A100 40/80GB, popular in some datacenters with power or granularity constraints, resulted in lower throughputs because of the reduction in available degrees of tensor parallelism (4 instead of 8 ). However, we found the $40 \mathrm{~GB}$ version of the A100 to offer increased availability and cost-efficiency.
- 50Gbps interconnect per GPU. State-of-the-art infrastructure will come with 200Gbps interconnect per A100, powered by low-latency InfiniBand; however, these configurations can be prohibitively expensive. We found that for models up to size of Falcon-180B, bandwidth had only a limited impact on overall throughput (mostly linked to the size of the all_reduce across data parallel degrees). Conversely, the higher latency of EFA on p4d was the main bottleneck for pipeline communications, especially for small scale models.
- No distributed filesystem. We stream data directly from S3, instead of relying on a dedicated filesystem. Distributed filesystems are expensive and difficult to maintain, and the small data I/O volumes incurred by large language model training do not justify their use.

Our infrastructure accordingly exists as an in-between between a true HPC system and a flexible cloud environment-notably, we found that the GPUs still had to share a single spine in the datacenter, as multispine configurations were unreliable. Altough it is more cost-efficient, it also requires us to be mindful of its limitations. We found that the popular (and simple) recipe of training with fully sharded data parallelism (Rajbhandari et al. 2020) did not scale well to this infrastructure. Instead, we required the finer control of 3D parallelism (Narayanan et al., 2021b) to achieve optimal performance.

Because of limitations in open-source frameworks at the time, we elected to build our own proprietary distributed training framework. Gigatron is based on pytorch, and at its core implements a 3D distributed parallelism strategy (Shoeybi et al. 2019, Narayanan et al., 2021b) combined with ZeRO optimizer sharding (Rajbhandari et al. 2020) to reduce memory consumption and improve scalability.

### 5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability

Data Parallelism (DP). Data parallelism (Fig. 88) is by far the most commonly used form of parallelism. All machine learning frameworks now provide ways to easily parallelize model training across data samples: pytorch with Distributed Data Parallel (DDP), jax with pmap, and tensorflow with MirroredStrategy (Paszke et al., 2017; Bradbury et al., 2021, Abadi et al., 2015). Data parallelism is appealing thanks to its simplicity: the distributed machinery can be hidden away inside the framework easily, without any interactions with the user-defined architecture. In its simplest implementation, data parallelism only requires two modifications to the training procedure: (1) each device needs to operate on unique data samples; (2) the gradients needs to be reduced across devices to keep the weights in sync. We note that this all_reduce will grow with the batch size, eventually making data parallelism bandwidth-bound. Furthermore, data parallelism is however no cure-all: since the model is not sharded, memory footprint per device is constant, even as we add more devices. Accordingly, data parallelism alone is constrained to models which fit on a single device.

Tensor Parallelism (TP). To share the model across devices, we need to turn to model parallelism. Introduced by Shoeybi et al. (2019) in its most popular form, tensor parallelism splits linear layers in the attention block and MLP in a principled way to reduce communication volume. This can be viewed as an instance of width-wise model parallelism. Specifically, for the simple case of a two-layer MLP, by making the first layer column parallel and the second row parallel, only a single all reduce is necessary to produce the final result-no communication of the intermediary result is required. To propagate the gradient backward, the matrices are transposed, and hence the row parallel layer turn into a column parallel layer and vice-versa: this maintains the the efficient communication pattern. We illustrate column and row parallelism in Fig. 9. A similar approach can be taken to split attention blocks, splitting the heads across GPUs-wherein the $\mathrm{K}, \mathrm{Q}, \mathrm{V}$ calculations are column parallel and the final projection row parallel, with all computations in between independent of one another between GPUs. Accordingly, entire blocks in Transformers can be efficiently parallelized this way. Tensor parallel, however, requires both high-bandwith and low-latency interconnect to be effective: accordingly, on current GPU infrastructure, it is constrained within a single node, and cannot efficiently be used across nodes. Models will thus typically be trained with a tensor parallel degree up to 8 ; this may still be insufficient to create small enough shards of the model.

## Data Parallel

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-25.jpg?height=399&width=616&top_left_y=1969&top_left_x=749)

Figure 8: Data parallelism creates model replicas on each device and process different samples in parallel. Model replicas are placed on different devices and compute gradients on different data samples in parallel. The gradients are then reduced before the optimization step is carried out.

## Column Parallel

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-26.jpg?height=170&width=264&top_left_y=302&top_left_x=602)

$\square$ GPU 1

GPU 2

GPU 3

All GPUs

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-26.jpg?height=296&width=263&top_left_y=302&top_left_x=866)

## Row Parallel

Input

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-26.jpg?height=87&width=253&top_left_y=778&top_left_x=600)

Weight
![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-26.jpg?height=254&width=510&top_left_y=781&top_left_x=604)

Result
![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-26.jpg?height=688&width=354&top_left_y=434&top_left_x=1168)

Figure 9: By alternating column and row parallelism, no communications are required between two subsequent matrix multiplications, enabling tensor parallelism to efficiently split attention and MLP blocks across GPUs. For column parallel matrix multiplication, the input is replicated (all_reduce in the backward) across all GPUs, which can then independently perform their operation. The result is already split across GPUs to execute the next matrix multiplication in a row parallel fashion. Finally, the output of each GPU is summed through an all_reduce. Intermediary results between the two matrix multiplications are never communicated.

Pipeline parallelism. Model parallelism can also be performed depth-wise, by grouping layers into subsequent stages to be executed on different accelerators. However, naive pipeline parallelism would either be inefficient or result in an immediate roadblock: indeed, stages have to split batches to concurrently process multiple forwards and backwards. We adopt the PipeDream-Flush schedule of Narayanan et al. (2021a), now commonly referred to as 1F1B. We found more involved schedules, such as Interleaved-1F1B (Narayanan et al. 2021b), to be beneficial when the number of microbatches per model replica is small, typically below 64. After the batch size rampup is completed, we train with a larger number of microbatches; as such we do not observe any speedup from interleaving. In order to reduce the communication volumes, we consistently make use of so called scatter-gather optimizations, where the activations sent to the next stage are first sharded over the tensor parallel degree and then gathered again once received on the next stage (see Fig. 10 for an illustration). This optimizes for underlying interconnect topology (intranode communications over NVLink where the gatheris executed are significantly faster), even more so if internode links have been rail optimized. This optimization reduces communication volume for pipeline parallelism by a factor of the tensor parallel world size, which in our case is 8 .

Optimizer sharding. Model weights and gradients are materialized in a contiguous bfloat16 memory buffer. Similar to (Rae et al. 2021), the optimizer maintains a $\mathrm{fp} 32$ version of those weights/gradients with which to calculate updates. When using the Adam optimizer in this setting, as we also need to keep track of exponential averages, we require 20 bytes per model parameters:

$$
\begin{equation*}
\underbrace{2_{\text {model param }}+2_{\text {model grad }}}_{\text {bfloat } 16}+\underbrace{4_{\text {opt param }}+4_{\text {opt grad }}+4_{\text {exp avgs }}+4_{\text {exp avg sqs }}}_{\text {float } 32}=20 \text { bytes } / \text { param } \tag{1}
\end{equation*}
$$

A full state of Falcon-7/40/180B will occupy 140GB, 800GB, and 3,600GB of memory per replica, requiring at least 4,20 , or 90 A100 $40 \mathrm{~GB}$ per replica just in weights+gradients+states of memory.

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-27.jpg?height=588&width=816&top_left_y=229&top_left_x=641)

Figure 10: Instead of each rank redundantly sending the full tensor, scatter/gather optimization has each rank sends a shard of the tensor, and then leverages the fast intranode interconnect to gather it back. This patterns is more efficient as internode communications are typically much slower than intranode ones; note that the scatter is effectively a no-op, as each rank already has the full tensor. Internode rail optimization typically enables GPUs of the same rank in different nodes to communicate peer-to-peer with minimal intermediaries, further accelerating this pattern.

The optimizer state alone accounts for $80 \%$ of the memory footprint, and eventually bottlenecks our memory use, preventing us from, for instance, increasing the batch size to better saturate GPUs instead. Accordingly, we choose to shard the optimizer state across data parallel degrees-a practice commonly referred to as ZeRO-1 in the literature (Rajbhandari et al. 2020), and illustrated in Fig. 11 Rather than storing a full redundant copy of the optimizer on each data parallel degree, the optimizer is independently sharded DP times. With optimizer state sharding the number of bytes per parameter is further reduced with increased data parallelism, improving scalability:

$$
\begin{equation*}
\text { Bytes per parameter }=4+\frac{16}{\mathrm{DP}} \tag{2}
\end{equation*}
$$

For Falcon-7/40/180B, we now only require 30, 215, and $765 \mathrm{~GB}$ of memory, or $1,6,20$ A100 40GB to hold a model replica. Note that these do not account for activation memory. Now that the optimizer is split, we however have to face increased communication burden during the backward. First, a reduce_scatter is applied on the bfloat16 model gradients: for each optimizer shard, this derives the relevant gradient shard reduced across all DP degrees. The resulting synchronized gradient chunk is then copied to the $\mathrm{fp} 32$ optimizer gradient buffer for use by the optimizer. The optimizer step is performed, resulting in an updated copy of the relevant $\mathrm{fp} 32$ sharded optimizer weights. Finally, an all_gather is used to consolidate the bfloat16 model weights for all workers from the aforementioned $\mathrm{fp} 32$ sharded optimizer weights. Pseudo-code for this ZeRO-1 workflow is provided in Appendix F.3 Interestingly, in terms of total communication volume, this workflow is equivalent to the all_reduce usually used in the data parallel setting Rajbhandari et al. (2020).

We found that across all scales (from 1 billion parameters to 180 billion parameters), optimizer sharding had no performance overhead compared to traditional data parallelism. In fact, since optimizer sharding frees-up memory, it enables us to use a larger microbatch size, hence resulting in better ressource saturation and higher throughput systematically.

No sequence parallelism. Li et al. (2021) proposed a novel strategy to reduce the activation memory during training: it's possible to shard the activations on the residual stream across the tensor parallel workers where these activations are otherwise replicated. This sharding is relatively cheap: instead of using an all_reduce after the MLP one can replace it with a reduce_scatter followed by an all_gather right before the next decoder block. However, after implementing the measures to save memory discussed in 5.3.3 we observe that sequence parallelism is not necessary. We also saw a slight decrease in throughput which made us decide against employing it during the final training.
$\mathbf{G P U}_{i}$

## parameters $\mathrm{b} f 16$ gradients b $f 16$ <br> parameters $\mathrm{fp} 32$

Full Optim State

Full Optim State

gradients $\mathrm{fp} 32$

exp avg gradients $\mathrm{fp} 32$

exp avg gradients ${ }^{2} \mathrm{fp} 32$

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-28.jpg?height=322&width=1263&top_left_y=598&top_left_x=434)

Figure 11: Optimizer sharding splits the large optimizer state across data parallel degrees, reducing memory footprint and improving scalability. The freed memory can be traded for an increased microbatch size, improving throughput. Figure inspired from Rajbhandari et al. (2020).

### 5.3.2 State-of-the-art throughput with dedicated Triton kernels

Flash Attention. Memory efficient attention alternatives have long garnered the attention of the community-however, these came with significant changes to the attention scheme, often resulting in degraded downstream performance, especially when scaling-up. It's only (relatively) recently that exact algorithms have been developed to compute attention without requiring materialization of the full attention matrix in memory (Rabe and Staats, 2021). Dao et al. (2022) subsequently showed that unified kernels leveraging the same techniques can significantly speed-up model training. We use a custom implementation in Triton (Tillet et al. 2019), which incorporates some of the improvements concurrently developed for Dao (2023). We note that the use of FlashAttention is the main driver of improved throughput during training. Notably, the memory savings allow us to not rely on activation checkpointing during training, instead focusing our FLOPS on contributing to model training directly. Dedicated FlashAttention kernels are also faster in absolute terms than their naive counterparts. Interestingly, we note that FlashAttention is significantly more numerically accurate than traditional attention when both are performed in bfloat 16 and compared to $\mathrm{fp} 32$ as the ground truth.

Other Triton kernels. We also employ specialised kernels for the rotary positional embeddings as well as the layer norms in Triton. For rotary embeddings we see a particularly large improvement compared to their unfused PyTorch counterpart. We note that since the transformation is the same for all heads at the same position it's possible to reuse the expensive trigonometric functions, additionally simplifying its use as nothing needs to be cached in RAM, as is common in other implementations.

### 5.3.3 Efficient memory use via selective recomputation implemented as a monolayer

As we target A100 40GB for cost-efficiency, memory footprint is a significant concern for us. Both ZeRO and FlashAttention already significantly improve memory availability for us, but we free up additionnal memory via selective recomputation. Li et al. (2021), on top of sequence parallelism, proposed to recompute some activations rather than storing their output in memory for backpropagation-as activations are typically cheap to evaluate. We take this idea a step further, and recompute not only all the activation functions but also the layer norms. We save to memory the statistics of the layer norm only, which makes recomputation trivial. Overall, this implementation of selective recomputation reduces the memory consumption of the decoder block by a factor $2 \mathrm{x}$, while causing no degradation in throughput from the additional computations.

Due to limitations in the autograd engine of PyTorch, it's rather difficult to only recompute e.g. the layer norms without also recomputing the subsequent linear layer. In order to achieve this, we create a single custom autograd function for the entire decoder block-we dub this idea the monolayer.

### 5.3.4 Numerical precision: all you need is bfloat16?

Brown et al. (2020); Zhang et al. (2022); Scao et al. (2022a) all reported stability issues training models in the hundred billion parameters range when relying on $\mathrm{fp} 16$; we instead adopt bfloat16, which results in stable training more or less out-of-the-box. Rae et al. (2021) reported significant improvements from using stochastic rounding when quantizing the $\mathrm{float} 32$ parameters to $\mathrm{bfloat} 16$ after the optimization step. In our case however, where the entire optimizer state is stored in float32, we do not observe a similar improvement. Instead, we observe that stochastic rounding helps during the initial steps, before the optimizer state has equilibriated to the training dynamics, but that training without stochastic rounding approaches the same training trajectory shortly thereafter.

### 5.3.5 Quality-of-life features for improved flexibility and reliability

Topology-agnostic checkpoints. Surprisingly, when we developed the Falcon series, most distributed training libraries strongly tied their checkpoint format and distribution topology: a change to the topology would require manually converting the checkpoint to a new format. Since we were planning to grow our clusters during training, we ensure that the model and optimizer checkpoints are readable/writable between any topology configurations-similar to $\mathrm{t} 5 \mathrm{x}$ (Roberts et al., 2022).

Low-discrepancy data loading. When aggregating different sources into a single mixed dataset it is common to randomly sample each source according to a target probability/weighing per dataset. However, this only guarantees sampling from the sources with the target probability in expectation. Instead, it is desirable that the effective weight during each subset of the training is constant. We use a predefined sampling pattern between different data-sources with a relative short length of 10,000 sequences which is guaranteed to contain the exact weights for each data source.

Topology discovery and optimization. In order to optimize throughput, it's important to place ranks with high communication volume on instances close to one another. At variance with traditional HPC platforms, AWS does not expose topology information for the instances allocated to a training. Accordingly, to optimize placement, we first discover the topology by measuring the bandwidth between all pairs of nodes, and we subsequently optimize rank placement against the measured topology. Regarding the topology discovery phase, a naive solution would be for the first node to first check the bandwidth to all other nodes, then the second node and so on. That would require $O\left(n^{2}\right)$ sequential measurements. Instead, it is possible to achieve this in only $O(n)$ steps, by doing many comparisons in parallel-see pseudocode provided in F.1. Once the topology has been (efficiently) discovered, we leverage Gromov-Wasserstein optimal transport (MÃ©moli, 2011) for placement. We measure the source distance matrix based on the measured bandwidths, and define the target distance matrix from the distances in the pipeline/data parallel grid. We then use the Python Optimal Transport Toolbox (Flamary et al. 2021) to solve the resulting problem efficiently.

### 5.4 Run management: keeping large-scale infrastructure running

Hardware failures. When scaling-up to hundreds of nodes, hardware failures become increasingly common-for Falcon-180B, every day on 4,096 A100 is equivalent to over 11 years of cumulative use! Furthermore, in our cloud setup, whenever the run is restarted, we sample a new anonymised selection of nodes-we are unable to keep a history of which nodes from previous allocations were suspect. Accordingly, it is critical to be able to rapidly identify and exclude faulty nodes. We found a large majority of hardware failures to be linked to faulty A100s, specifically to corrupted memory rows. These failures do not always manifest with an Xid code, and require manual tests to catch them-they typically result in computations returning NaNs. On start-up, we run a series of large matrix multiplications to catch these failures; during training, we also keep track of NaNs in communications to rapidly identify culprit nodes. Additionally, we run some simple communication tests on start-up, to ensure that the communication primitives work as expected.

Monitoring. We find that many web-based monitoring tools sample metrics, even when no smoothing is applied: this can hide critical events like spikes. Accordingly, we deploy our own local viewers.

Spikes. We encountered few spikes during training. Similar to Chowdhery et al. (2022), when a spike occurred, we resumed from the latest pre-spike checkpoint and skip 1 billion tokens. Nine spikes were encountered during the training of both the 40B model and the 180B model.

## 6 Results

Background. The natural language processing community has developed a plethora of benchmarks to assess the capabilities of models: from tasks inspired by reading comprehension tests (e.g., RACE Lai et al. (2017)), to world-knowledge evaluations (e.g., OpenBookQA Mihaylov et al. (2018)), or socalled commonsense tasks (e.g., HellaSwag (Zellers et al., 2019)). Historical benchmarks like GLUE and SuperGLUE also aggregate many linguistically-motivated tasks, measuring abilities such as word disambiguation or recognizing entailment (Wang et al., 2018, 2019). However, as large language models have gained in capabilities and broadened in their applications, the landscape of evaluations has shifted away from this last genre of tasks; instead, recent benchmarks such as MMLU (Hendrycks et al. 2020) or BigBench (Srivastava et al. 2023) attempt to capture generic knowledge and abilities, rather than linguistic behavior-perhaps in an attempt to stay closer to common use cases of large language models. Code evaluations have also grown increasingly common (Chen et al. 2021), along with mathematics tasks (Cobbe et al., 2021). Recently, technical reports on latest models have also included numerous academic and professionnal exams (OpenAI, 2023a; Anil et al., 2023).

Along with this shift in subjects, practices around how models are evaluated have also changed. For zero/few-shot evaluations, the canonical setup was popularized by Brown et al. (2020): as most classic NLP tasks have a limited number of choices, these can be evaluated by calculating the log-probabilities of the choices given the question, instead of generating freeform answers. This puts strict guardrails on the model, simplifying implementation and reproduction of evaluations as they do not depend on autoregressive inference with sampling in this framing. However, this inflexibility is not always desired: for instance, chain-of-thought prompting (Wei et al. 2022b) let the model write down intermediate steps of its reasoning before giving a final answer. This setup may also not best illustrate how models are used downstream; common freeform tasks, such as summarization, require autoregressive generation. However, for these tasks, a separate challenge of evaluating the model's answer rapidly arise. For instance, Stiennon et al. (2020) found that ROUGE scores for model-generated summaries are not necessarily aligned with human preferences.

Typical large language models will be deployed as a chatbot/virtual assistant. This is an extremely challenging use case to quantify: it requires appropriate style and chattiness, wide world knowledge, and often reasoning/code abilities. Such deployments should also ideally be harmless and honest on top of being helpful (Bai et al. 2022a), further complexifying evaluation and tradeoffs. Notably, classical NLP recipes translate poorly into better chatbots: we illustrate this in Fig. 12, where we evaluate variants of Falcon-40B on the popular SuperGLUE benchmark (Wang et al. 2019), and on a set of 250 prompts with completions rated by human annotators. The popular FLAN recipe (Longpre et al. 2023) results in the model with the best zero-shot performance on SuperGLUE; however, such a model is also the one least preferred by human annotators. Instead, models trained on so-called self-instruct (Wang et al., 2022b, Taori et al., 2023) datasets improve SuperGLUE performance in a smaller way, but results in significant improvements to annotator preference-unfortunately, human ratings are more expensive to collect than evaluating on SuperGLUE!

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-30.jpg?height=543&width=699&top_left_y=1821&top_left_x=710)

Figure 12: Human ratings can be at odds with NLP task performance. Variants of Falcon-40B finetuned on different datasets, horizontal lines for baselines from the OpenAI API and from human annotators. Star ratings collected blind, from a pool of 15 annotators across 250 unique prompts.

As a middle ground between expensive human preference data and cheap NLP evaluations, it has been recently proposed to use a strong external model (e.g., GPT-4) as a judge (Chiang and Lee, 2023, Chiang et al. 2023, Zheng et al. 2023)-broadly inspired by RLAIF, which seeks to substitute human annotators in RLHF with models themselves (Bai et al. 2022b, Dubois et al. 2023). However, it is unclear yet how reliable these practices are (Wang et al. 2023)-they are also predominantly focused on evaluating models finetuned for downstream usecases, not pretrained models.

Finally, fair comparisons across models are difficult. First, task selections diverge widely: the PaLM papers (Chowdhery et al., 2022; Anil et al. 2023) typically reproduce the setup of Brown et al. (2020); recent technical reports (OpenAI 2023a; Inflection 2023) report arbitrary selection of tasks in varying settings; and state-of-the-art models like Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al. 2022), or LLaMA (Touvron et al. 2023a b) have all elected to report varying selections of NLP tasks. Notably, there is a lack of standardization across these setups; while standardized benchmarks exist, such as the Eleuther AI Evaluation Harness (Gao et al. 2021), or HELM (Liang et al., 2022), they have only seen limited adoption among open-access models-and most papers do not report details of their evaluation setup, including prompts used. Differing practices on data formatting or tokenization may result in widely different task scores (Fourrier et al., 2023); in a way, one-size-fits-all evaluation does not quite exist yet. We further discuss this issue in Section 6.1 with concrete examples.

Our evaluation setup. Since this paper focuses on the pretrained models in the Falcon series, we choose to center our evaluations on the more classic logprobs-based setup of Brown et al. (2020). This simplifies comparisons, although fairness concerns remain across models. To address these, we split our evaluations across four comparisons: (1) in 1-shot against the PaLM models (Chowdhery et al. 2022; Anil et al., 2023), with the tasks of (Brown et al., 2020); (2) on a small set of few-shot tasks reported by the GPT-4 paper (OpenAI, 2023a); (3) against state-of-the-art models across common sense, question answering, and code tasks; (4) against models which also report results from the EAI Harness (Gao et al., 2021), for which we are able to compare with identical prompts and metrics. We note that this evaluation setup is only intended to provide a broad, first look at the performance of the model; adequate in-depth domain specific evaluations after dedicated specialization of the model would be required to inform about the use of the Falcon series in downstream use cases.

### 6.1 To prompt or not to prompt: comparing evaluations across codebases

When evaluating models in few-shot, two main discrepancies across setups can arise: (1) variations in the style and writing of the prompt; (2) differences in the metrics used to measure accuracy.

Prompting. Specifically regarding (1), not only can small wording tweaks drive changes in performance (use of plurals, yes/no instead of true/false, typos, etc.), but tasks can also be framed in different ways. For instance Rae et al. (2021) found that for multiple-choice question answering tasks, writing down the options in the prompt was beneficial for larger models, but detrimental for smaller models on RACE (Lai et al. 2017)-regardless of whether the model was asked to predict the answer itself or a letter key for it. In the case of RACE, this can account for an absolute change of $10-20 \%$. As most commonly used tasks were created before zero/few-shot evaluations became popular, there is rarely such a thing as a canonical prompt, or even framing, for a task. Furthermore, the vast majority of papers do not report the prompts they used, which hinders reproducibility.

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-31.jpg?height=355&width=1141&top_left_y=1934&top_left_x=381)

Figure 13: In all our evaluations, we rank possible answers by their logprobabilities according to the model given a prompt. In grey in the prompt, we highlight the alternative possibility of outlining each candidate explicitely in the prompt (CH)-in line with Rae et al. (2021), we find this beneficial for larger and more capable models, but detrimental for smaller ones.

Table 17: At different model sizes, different prompt formulations and metrics calculations deliver the best zero-shot performance. Notably, we finding that outlining multiple-choice questions options in the prompt explicitly $(\mathrm{CH})$ nearly always improve performance for larger models, but degrades it for smaller, less capable models. Unconditional normalization (UN) has a smaller effect, with improvements on some tasks but not others. Using both tricks together (BT) never improves performance over using the best one individually. Degradation, improvement over baseline: Gao et al. (2021) default prompt and length-normalized logprob calculation for $\mathrm{CH}$ and $\mathrm{UN}$, best of $\mathrm{CH} / \mathrm{UN}$ for BT.

|  | ARC-Easy | ARC-Challenge |  |  |  | OpenBookQA |  |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  |  | UN | CH | BT |  | UN | CH | BT |  | UN | CH | BT |
| Falcon-7B | $\mathbf{7 3 , 7}$ | 68,1 | 33,3 | 24,7 | 44,5 | $\mathbf{4 9 , 5}$ | 27,4 | 24,8 | $\mathbf{4 4 , 6}$ | 39,2 | 24,4 | 24,0 |
| Falcon-40B | $\mathbf{8 1 , 2}$ | 76,6 | 80,5 | 78,8 | 56,7 | 59,0 | $\mathbf{6 2 , 1}$ | 61,3 | 48,0 | 43,2 | $\mathbf{6 1 , 2}$ | 56,4 |
| Falcon-180B | 84,7 | 79,5 | $\mathbf{9 4 , 4}$ | 84,6 | 63,7 | 63,9 | $\mathbf{8 3 , 5}$ | 81,6 | 47,6 | 48,8 | $\mathbf{7 6 , 4}$ | 71,8 |

Metrics. On most tasks, accuracy is typically reported; however, practices on the calculation of the logprobabilities used to evaluate candidate answers differ. For tasks with multiple choices, $P$ (candidate|context) is evaluated with each choice used as a candidate successively, and the choice with the higehst probability is taken as the model's answer. For tasks with long answer (i.e., which don't fit in a single token) the logprobabilities of each token in the candidate are summed to estimate the logprobabilities of the overall completion. When comparing choices/completions with widely differing lengths, this can be unfair to the longer ones: they end-up being more unlikely. To compensate for this, the logprobabilities are sometimes normalized by the length of the candidate answer-a first potential divergence. Brown et al. (2020) also found that some answers were heavily skewed by being more likely to follow the final sequence "Answer:" in the prompt. Accordingly, for ARC, OpenBookQA, and RACE they choose to normalize the probabilities by the so-called unconditional probability of each completion: $P$ (candidate|context)/ $P$ (candidate|"Answer:"). Touvron et al. (2023a) also adopts this practice for OpenBookQA and BoolQ, but it's often unclear whether other papers have adopted it as well, as details on the calculations of logprobabilities are rarely provided.

Experiments. We explore the impact of outlining multiple-choice options in the prompt (see Fig. 13 for an illustration), and of normalizing by the unconditional probability, in zero-shot on ARC and OpenBookQA, for Falcon-7B/40B/180B in Table 17 We find widely different effects for the two practices across both model size and tasks. On ARC-Easy, unconditional normalization (UN) always degrade performance, while it improves it for all models on ARC-Challenge, and improves performance only for Falcon-180B on OpenBookQA. Outlining the choices in the prompt (CH) always degrade performance for Falcon-7B, but always improves it for Falcon-180B; on Falcon-40B, it slightly degrades performance on ARC-Easy, and improves it significantly on ARC-Challenge and OpenBookQA. These findings are in line with the observations of Rae et al. (2021) on RACE, and we find that the effect is indeed very significant $(+10-20 \%$ absolute gains on Falcon-180B). Combining both practices never improves performance over using one individually.

For fairness, we seek to match the evaluation setting of other papers and models when comparing to their results-so that using one of the trick above doesn't put us at an unfair advantage for example.

- Comparisons with PaLM. Like PaLM (Chowdhery et al., 2022, Anil et al., 2023) we reproduce the evaluation setup of Brown et al. (2020): we do not outline candidates in the prompt for multiple-choice questions (e.g., ARC, RACE), and use unconditionnal normalization for OpenBookQA. When candidates are longer than one token, we normalize logprobabilities by length. We do not significantly tweak prompts, staying close to the ones detailed by Brown et al. (2020). These results are presented in detail in Section 6.2
- Comparisons with GPT-3.5/GPT-4. For MMLU, we use the format proposed by the authors originally (Hendrycks et al. 2020); for ARC, we explicitly outline choices in the prompt, as reported in the GPT-4 paper (OpenAI 2023a). For HellaSwag and Winogrande, we do not change the prompt used by Gao et al. (2021). Results are reported in Section 6.3
- State-of-the-art comparisons. Since these are the widest comparisons, they are likely the ones with the largest variation in practices. We use unconditional normalization on OpenBookQA, and outline answers explicitely on ARC, OpenBookQA, and RACE. We lightly tweak the wording of prompts, but stay close to the ones of Brown et al. (2020), as implemented in Gao et al. (2021). These results are reported in Section6.4
- Comparisons with the EleutherAI Evaluation Harness. We strictly use Gao et al. (2021), and do not make any change to the prompt formulation or logprobability calculations for these. We only compare against other papers that adopt the same practice. This is our fairest set-up, for which direct one-to-one comparisons are possible and for which we didn't had to guess the set-up used by other models. These results are reported in Section 6.5

Finally, for all set-ups, we provide the prompts used in Appendix G for reproducibility.

### 6.2 Comparisons with PaLM on a natural language tasks aggregate

Set-up. We compare with PaLM (Chowdhery et al. 2022) and PaLM-2 (Anil et al., 2023) in the 1 -shot NLP task benchmark reported in the PaLM-2 paper. This benchmark broadly reproduces the set of tasks used for GPT-3 (Brown et al., 2020). We note that we are a missing a few of these tasks (e.g., StoryCloze) as we did not implement and validate them in our evaluation framework.

Results. We report detailed results in Table 18. We find that when averaging performance across tasks, Falcon-180B recovers $99.5 \%$ of the performance of PaLM-2 Large; significantly above the $94.8 \%$ and $94.4 \%$ of PaLM-2 Medium and PaLM. Notably, Falcon-180B even outperforms PaLM-2 on some classic benchmarks such as HellaSwag, Winogrande, and PIQA. However, Falcon-180B lags behind on two particular benchmarks: RACE and ANLI. We found RACE to be very sensitive to the formatting of the prompt and samples, which may explain some of the difference; furthermore, we observed that ANLI typically exhibits staggered progression throughout training, rather than a smooth increase (e.g., like HellaSwag)-it's possible Falcon-180B is just under the next phase transition that would allow it to catch-up to the range of performance of PaLM-2 Large.

Table 18: Falcon-180B matches the performance of PaLM-2 Large on most tasks, with the notable exception of ANLI and RACE for which it performs closer to PaLM-2 Medium. We found these tasks to be very sensitive to the prompt format used, in particular RACE, and suspect that further improvements could be unlocked with additional tweaking-however, for fairness, we stay close to the prompts proposed by Brown et al. (2020). Overall, we find that Falcon-180B recovers $99.5 \%$ of the performance of PaLM-2 Large, significantly above the $94.8 \%$ of PaLM-2 Medium.

| Task | Subtask | PaLM | PaLM-2 |  |  |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: |
| Sald |  |  |  |  |  |  |
| WebQuestions (EM) |  | 22,6 | 21,8 | 26,9 | 28,2 | $\mathbf{3 1 , 9}$ |
| HellaSwag |  | 83,6 | 82,0 | 84,0 | 86,8 | $\mathbf{8 7 , 5}$ |
| LAMBADA |  | 81,8 | 80,7 | 83,7 | $\mathbf{8 6 , 9}$ | 84,4 |
| WSC |  | 86,3 | 84,6 | $\mathbf{8 8 , 1}$ | 86,9 | 87,5 |
| Winogrande |  | 83,7 | 77,9 | 79,2 | 83,0 | $\mathbf{8 5 , 1}$ |
| RACE | Hard | 52,1 | 53,3 | 57,2 | $\mathbf{6 2 , 3}$ | 56,7 |
| PIQA |  | 83,9 | 82,2 | 83,2 | 85,0 | $\mathbf{8 6 , 1}$ |
| ARC | Challenge | 60,1 | 59,6 | 64,9 | $\mathbf{6 9 , 2}$ | 67,8 |
|  | Easy | 85,0 | 85,6 | 88,0 | $\mathbf{8 9 , 7}$ | 88,8 |
| OpenBookQA |  | 72,6 | 72,6 | 76,5 | $\mathbf{7 9 , 5}$ | 78,3 |
| BoolQ |  | 53,6 | 57,4 | 56,2 | 58,5 | $\mathbf{6 4 , 2}$ |
| CB |  | 88,7 | 88,1 | 88,6 | $\mathbf{9 0 , 9}$ | 89,0 |
| COPA | 83,9 | 82,1 | 80,4 | 87,5 | $\mathbf{8 9 , 3}$ |  |
| RTE |  | 91,0 | 89,0 | 90,0 | $\mathbf{9 6 , 0}$ | $\mathbf{9 6 , 0}$ |
| WiC |  | 78,7 | 78,7 | $\mathbf{8 1 , 8}$ | 79,3 | 80,1 |
| ReCORD |  | 93,2 | 50,6 | 52,0 | $\mathbf{6 6 , 8}$ | 66,1 |
| ANLI |  | 92,8 | 92,1 | 92,4 | $\mathbf{9 3 , 8}$ | 93,2 |
| Task average |  | 52,6 | 53,1 | 58,1 | $\mathbf{7 3 , 1}$ | 60,5 |
| Fraction of PaLM-2 L |  | 48,7 | 48,8 | 49,5 | $\mathbf{6 3 , 4}$ | 55,5 |

Table 19: Falcon-180B delivers downstream performance between GPT-3.5 and GPT-4. Falcon180B performs well on commonsense tasks (HellaSwag and Winogrande), where it is well ahead of GPT-3.5. For multiple choice question answering (ARC and MMLU), Falcon-180B performs above GPT-3.5 but not as significantly so.* we report 2-shot, not 25-shot performance on ARC Challenge.

|  | GPT-3.5 | GPT-4 | Falcon-180B |
| :--- | :---: | :---: | :---: |
| HellaSwag (10-shot) | 85.5 | $\mathbf{9 5 . 3}$ | 89.0 |
| Winogrande (5-shot) | 81.6 | $\mathbf{8 7 . 5}$ | 87.1 |
| ARC Challenge (25-shot) | 85.2 | $\mathbf{9 6 . 3}$ | $87.8^{*}$ |
| MMLU (5-shot) | 70.0 | $\mathbf{8 6 . 5}$ | 70.6 |

Overall, these are strong scores, exhibiting the robustness of the Falcon recipe. Beside dataset composition and architectural tweaks, we note two differences between PaLM-2 Large and Falcon180B: (1) PaLM-2 L was supposedly trained with a larger compute budget, with twice the parameters and a similar amount of tokens as Falcon-180B (see Appendix E); (2) PaLM-2 models used a mixture of objectives (Tay et al. 2022a), which has been reported to enhance downstream task performance. Further a posteriori adaptation of Falcon-180B (e.g., with the PaLM-U recipe (Tay et al., 2022b)) could help recover more of the performance of PaLM-2 L with Falcon-180B as a base model.

### 6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks

Set-up. We compare with GPT-3.5 and the GPT-4 as reported in GPT-4 paper (OpenAI, 2023a), focusing on natural language tasks: MMLU, HellaSwag, Winogrande, and ARC. We use the same number of shots as proposed, except for ARC, for which we report 2-shot instead of 25 -shot accuracy.

Results. Results are in Table 19. We find that Falcon-180B systematically performs above GPT-3.5, but below GPT-4 on all tasks. Notably, Falcon-180B achieves strong performance on commonsense tasks: on HellaSwag, it is close to midway between GPT-3.5 and GPT-4, while on Winogrande, it nearly matches the performance of GPT-4. We find the performance of Falcon-180B on multiple choice question answering tasks to be closer to GPT-3.5, albeit always higher. We note that GPT-4 was allegedly trained with 4-5x more pretraining compute than Falcon-180B (see Appendix E), which likely contributes to most of the difference in performance between the two models.

### 6.4 State-of-the-art comparisons on common sense, question answering, and code tasks

Set-up. In this section, we compare the Falcon series with other models on commonsense, question answering, and code tasks. We compare with models that have successively defined the state-ofthe-art "before" PaLM-2 Large and GPT-4: GPT-3 (Brown et al., 2020), Gopher (Rae et al. 2021), Chinchilla (Hoffmann et al., 2022), MT-NLG (Smith et al., 2022), PaLM (Chowdhery et al. 2022), LLaMA-2 (Touvron et al.|,2023b), and Inflection-1 (Inflection| 2023). For commonsense tasks, we include PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2019), BoolQ (Clark et al. 2019), and LAMBADA (Paperno et al. 2016)-only for BoolQ does our prompt differ slightly from the default one of the EleutherAI Evaluation Harness (Gao et al., 2021). For question answering datasets, we include ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and MMLU (Hendrycks et al., 2020). To enable fair comparisons, we evaluate ARC without outlining choices (see Section6.1). For OpenBookQA, we outline choices, as we find performance to be flat otherwise; finally, for MMLU, we use the canonical setup from Hendrycks et al. (2020). For all these tasks we take results for other models from the papers outlined above. For code, we reproduce the set-up of the BigCode Models Leaderboard for HumanEval in Python, and include further code-specialized models such as Codex (Chen et al. 2021), StarCoder (Li et al., 2023a), and Code LLaMA (RoziÃ¨re et al. 2023). As we focus on comparisons with the Falcon models immediately after pretraining, we do not consider instruct variants of the models above.

Commonsense. We report results in Table 20. With the exception of BoolQ, Falcon-180B improves significantly over state-of-the-art models across all tasks. Broadly speaking, we find that Falcon-40B is slightly under LLaMA-2 34B, which we attribute to smaller pretraining compute: 2,800PF-days for Falcon-40B against 4,700PF-days for LLaMA-2 34B (nearly 70\% more). Falcon-7B also slightly underperforms LLaMA-2 7B; this time the difference in compute is smaller (730PF-days against

Table 20: Outside of PaLM-2 Large and GPT-4, Falcon-180B significantly improves other stateof-the-art models such as LLaMA-2 or Inflection-1 on commonsense tasks. Falcon-40B performs slightly under LLaMA-2 34B, because of a significantly smaller compute budget (2,800PF-days against 4,700PF-days, $70 \%$ more for LLaMA-2). We note the exceptional performance of Inflection-1 on BoolQ; conversely, we note that, despite our best prompt engineering efforts, we were unable to reproduce the performance reported by the LLaMA-2 paper on BoolQ for the Falcon series: the results we report can likely be improved. Bold for best, underline for second-best.

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-35.jpg?height=656&width=1357&top_left_y=556&top_left_x=384)

970PF-days, $30 \%$ more), but we suspect that multiquery with a single head of dimension 64 is a very aggressive configuration for Falcon-7B. We find that LLaMA-2 and Inflection-1 achieve relatively similar performance, with Inflection-1 perhaps ahead thanks to exceptional performance on BoolQ.

Question answering. In Table 21, we find again that Falcon-180B strongly outperforms other models from the state-of-art on question answering tasks. We do note that the Falcon series seems to underperform slightly (comparatively to other tasks) on MMLU: we believe this may be attributed to the large prevalence of web data in our pretraining dataset, compared to more technical sources which may be more immediately relevant to the style and content of questions found in MMLU.

Code. We report results on HumanEval in Table 22 We find that Falcon-180B performs best amongst models focusing on natural language, with performance only matched by Inflection-1. In fact, despite being trained on only 3\% code, Falcon-180B nearly matches the performance of both PaLM-Coder and PaLM-2 $\mathrm{S}^{*}$, two models which have undergone dedicated code specialization following their pretraining. This is an encouraging result for the development of a Falcon-Coder specialization.

Table 21: Falcon-180B improves significantly over GPT-3, PaLM, and LLaMA-2 on question answering datasets, while Falcon-40B performs in-line with LLaMA-2 34B. *: for Falcon-7B on OpenBookQA, we report accuracy without outlining candidates, as performance is otherwise close to random (see Table 17 for details). Bold for best, underline for second-best.

|  |  | ARC-Challenge | ARC-Easy | OpenBookQA | MMLU |
| :--- | :--- | :---: | :---: | :---: | :---: |
| GPT-3 |  | 51,4 | 68,8 | 57,6 |  |
| PaLM | 53,0 | 76,6 | 53,4 | 69,3 |  |
| LLaMA-2 | 7B | 45,9 | 75,2 | 58,6 | 45,3 |
|  | 13B | 49,4 | 77,3 | 57,0 | 54,8 |
|  | 34B | 54,5 | 79,4 | 58,2 | 62,6 |
|  | 70B | $\underline{57,4}$ | 80,2 | 60,2 | $\underline{68,9}$ |
| Falcon | 7B | 44,5 | 73,6 | $44,6^{*}$ | 28,0 |
|  | 40B | 56,7 | $\underline{81,2}$ | $\underline{61,2}$ | 57,0 |
|  | 180B | $\mathbf{6 3 , 7}$ | $\mathbf{8 4 , 7}$ | $\mathbf{7 6 , 4}$ | $\mathbf{7 0 , 6}$ |

Table 22: Falcon-180B outperforms all other predominantly English language models on HumanEval. Morever, despite being trained on only $3 \%$ code, it nearly matches the performance of PaLM-Coder and PaLM-2 S*, both having undergone code specialization. ${ }^{\dagger}$ : the GPT-3.5 and GPT-4 models are somewhat unique in being pretrained on a large fraction of code on top of natural language (OpenAI, 2023b). Bold for best, underline for second-best (per specialization), pass @ 1.

|  |  | Specialized for code? | HumanEval |
| :--- | :--- | :---: | :---: |
| PaLM |  |  | 26,2 |
| LLaMA-2 | 7B |  | 12,2 |
|  | 13B |  | 20,1 |
|  | $34 \mathrm{~B}$ |  | 22,6 |
|  | 70B |  | 30,5 |
| Inflection-1 |  |  | $\mathbf{3 5 , 4}$ |
| Falcon | 180B | $\checkmark$ | $\mathbf{3 5 , 4}$ |
| Codex | cushman-001 | $\checkmark$ | 33,5 |
|  | davinci-002 | $\checkmark$ | 45,9 |
| StarCoder |  | $\checkmark$ | 30,4 |
| PaLM-Coder |  | $\checkmark$ | 35,9 |
| PaLM-2 S* |  | $\checkmark$ | 37,6 |
| GPT-3.5 |  | $\checkmark$ | 48,1 |
| GPT-4 |  | $\checkmark$ | $\mathbf{6 7 . 0}$ |
| Code LLaMA | 7B |  | 33,5 |
|  | 13B |  | 36,0 |
|  | 34B |  | 48,8 |

### 6.5 Comparison with other models using the EleutherAI Evaluation Harness

Set-up. For this final comparison, we only consider models which have had reports of evaluations performed with the EleutherAI Evaluation Harness (Gao et al., 2021). These results are the most directly comparable, as the preprocessing of samples, the prompt, and the calculations of the metrics are identical. We report results for GPT-3 evaluated through the API by the BigScience group, FairSeq (Artetxe et al., 2021), GPT-Neo-1.3B (Black et al., 2021), GPT-J (Wang and Komatsuzaki, 2021), GPT-NeoX-20B (Black et al., 2022), OPT (Zhang et al., 2022), Pythia (Biderman et al. 2023), CerebrasGPT (Dey et al.| 2023), Aleph Alpha (Aleph Alpha, 2023), and BLOOM (Scao et al. 2022a). We average performance on HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al. 2016), Winogrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and OpenBookQA (Mihaylov et al., 2018)-this was the widest set of tasks we could assemble based on different reporting practices across papers. We note that these models mostly span a smaller compute range (up to a few thousands PF-days at most), and with performance starkly below other state-of-the-art models on with which we have compared before. We report results both for the Falcon series presented in this paper, and for the smaller-scale Falcon-RefinedWeb models trained in Penedo et al. (2023), which used only the RefinedWeb dataset for performance validation.

Results. We present results in Fig. 14. We find that, across scales, the Falcon series significantly improves against other models in this set of comparisons. Notably, Falcon-40B outperforms GPT-3 175B, despite being trained with a smaller compute budget. In fact, even Falcon-7B approaches the performance of GPT-3 175B: we believe that with longer training and a less aggressive multiquery setup, it should be possible to match the performance of the original GPT-3 model with 7B parameters (or less). Finally, we also note that the smaller validation models trained on RefinedWeb alone also compare favorably, reproducing the performance of the GPT-3 models of the same size.

Taking a step back, it's interesting to note some broader trends in this plot. Most older series of models achieve very similar performance, with the GPT-3 series as a roofline. The OPT models, despite underperforming at the smaller sizes, eventually match the performance of GPT-3 175B. Two outsiders stand out: GPT-Neo-1.3B, the first open-source large language model, likely because of issues in its pretraining, and the BLOOM series, likely because of its heavily multingual pretraining data and conservative use of an additionnal layer norm after the embeddings (Scao et al. 2022b).

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-37.jpg?height=624&width=1377&top_left_y=249&top_left_x=366)

Figure 14: The Falcon series strongly improves at all scales against other models which have reported results with the EleutherAI Evaluation Harness. Models trained on RefinedWeb alone also reproduce the performance of the GPT-3 series, despite not using curated data. Aggregated zero-shot accuracy on HellaSwag, LAMBADA, Winogrande, PIQA, ARC, and OpenBookQA.

## 7 Limitations

We highlight in this section limitations of our findings and of the Falcon series of models itself, both to nuance potential applications of our models, and to highlight further research directions. Broadly speaking, these limitations stem from two factors: (1) most of the research on the Falcon series was conducted prior to December 2022, when model training started-in the meanwhile significant developments have occurred in the world of large language models; (2) constraints in compute resources preventing a more exhaustive exploration of some directions.

### 7.1 Limitations of our findings and ablations

Scale. Our ablations are conducted with 1B and 3B models trained to optimality; although we validated the final architecture and datasets with 1B and 7B models trained for 350B tokens, this remains order of magnitude below the compute budget of the actual models in the Falcon series. On the model side, as scale increases, other works have noted the emergence of outlier features, which may drive important dynamics which are not captured by our set-up (Dettmers et al., 2022). On the data side, increasingly large models are known to become increasingly sensitive to memorization (Carlini et al. 2022), potentially leading to catastrophic quality degradation (Hernandez et al. 2022).

Benchmarks. In our ablations, we focused on measuring zero-shot performance on a limited set of tasks, using only logprobs-based evaluations. Not only can downstream task performance be at odds with actual human preference (see Fig. 12 for our own experience of that issue), but our set of tasks does not capture code or multilingual performance. Evaluating general purpose language models is difficult, and single aggregated metrics often poorly translate the nuances brought forth by modeling or data interventions. Broadly speaking, on our set of tasks, we never experienced a consistent trend with one intervention systematically improving a specific metrics but not the others (e.g., adding technical data did not consistently uplift performance on SciQ or PubMedQA); however, this is likely to be a limitation of our small-scale setup, which end-up predominantly measuring general gains. It is likely the performance on popular benchmarks such as MMLU (Hendrycks et al., 2020) or AGIEval (Zhong et al. 2023) could be improved with dedicated in-domain data. We notably believe in the value of larger-scale ablations on optimal pretraining data composition, and on better understanding quantitative versus qualitative aspects of deduplication (i.e., deduplication on web data may be unreasonably effective because it happens to filter out some of the worst samples).

Overall, we view Section 4 as a set of cursory experiments that helped us ground and broadly validate some decisions for the Falcon series. Future practitioners will likely want to revisit these experiments and results at increased scale, enabling broader benchmarking to better capture performance.

### 7.2 Limitations of the Falcon models

Decoupling training and inference compute. In the past year, there has been a dramatic explosion in the adoption of large language models, leading to significantly increased downstream use. Under that paradigm, inference costs can become predominant, shadowing pretraining costs; furthermore, smaller open-source models are easier for the community to build upon, enabling local deployments and even finetuning. When scaling-up pretraining, additional compute may be either spent towards a larger model, or towards longer training. While increasing parameter count caries over to inference, increasing costs over the entire lifecycle of the model, this is not the case for longer pretraining, offering an axis for decoupling training and inference compute. Recent open-source models, such as the LLaMA series (Touvron et al. 2023ab), have adopted this practice by eventually training their 7-70B parameters models for a fixed 2,000B tokens. While our 7B follows a similar idea, our 40B and 180B models are trained closer to the pretraining optimality suggested by Hoffmann et al. (2022). This makes deployment of models in the Falcon series more challenging. At the time, this decision was partially motivated by constraints over data availability: we elected early on to strictly stick to a single epoch of training, and the total stock of RefinedWeb tokens was unknown to us when we started training (data processing jobs only concluded in early January). Recent works have however suggested that up to 4 epochs may lead to minimal degradation in performance, at least for models in the 10B parameters range (Xue et al., 2023, Muennighoff et al., 2023). Based on the final size of our available data (around 5,000B tokens of English and 1,000B tokens of code) we would recommend training future models on at least $4,000-5,000 \mathrm{~B}$ tokens, and skewing towards $10,000-15,000 \mathrm{~B}$ tokens for the larger ones. Note that this does pose some challenges during pretraining, as larger models are easier to distribute efficiently. We also see promise in enabling training and inference compute decoupling through architecture interventions such as layerwise mixture of experts (MoE), also known as routed language models (Clark et al. 2022, Fedus et al. 2022). In MoE models, parameters are only sparsely activated, allowing for another 8-16 reduction in inference costs.

Code performance and pretraining data. As we did not allow upsampling for pretraining subsets, we predominantly trained Falcon on web data from RefinedWeb. We also were conservative with the fraction of code in the pretraining, whereas available data could have allowed us to reach $10-30 \%$ of code data in pretraining. For future models, we would recommend making code data significantly more prevalent, and potentially upsampling sources illustrative of common usecases or data domains of interest for downstream use cases. Code data is particularly promising, because it is widely available from a single source (i.e., GitHub) and can undergo large-scale processing similar to web data-deduplication is also extremely effective for code data (Kocetkov et al., 2022, Allal et al., 2023, Li et al., 2023a). Furthermore, the GPT-3.5 and GPT-4 models have been seemingly trained on a mix of both natural language and code data (OpenAI, 2023b a). Such hybrid language/code models are also likely to be easier to adapt for downstream use cases as chatbots which may make extensive use of code to answer user questions, or to interface with a variety of tools. We also see potential promise in the use of synthetic data (Gunasekar et al., 2023, Li et al., 2023b); however, many of these methods are currently closer to distillation than true pretraining, and it is somewhat unclear how they may be used to bootstrap a new class of larger models beyond downstream adaptation.

Longer sequence lengths. All Falcon models are pretrained with a 2,048 sequence length, which can be limiting for multi-turn chat or code use cases. Thanks to the use of rotary embeddings ( $\mathrm{Su}$ et al., 2021), a posteriori adaptation to longer sequences is possible: kaiokenmdenv (2023); Chen et al. (2023) concurrently found that extending the context length with rotary positionnal embeddings was possible by interpolation and lightweight finetuning. In our own experiments on long context finetuning, we found long context data to be plentiful in RefinedWeb. Upward of and more than $13 \%$ of the tokens come from documents of over $8 \mathrm{k}$ tokens, with $1.5 \%$ of the tokens in documents of over 32k tokens. This approach was further refined for LLaMA-2 Long (Xiong et al. 2023), resulting in a significant performance boost on tasks with many shots (e.g., MMLU and ARC in Section 6.3).

Pretrained only. Although we make lightly tuned versions of the Falcon models available to demonstrate possibilities as instruct or chat models, our work predominantly concerns the pretrained versions. Prior to deployment of the Falcon models, we strongly recommend for adequate guardrails to be deployed and for bias/harm evaluations specific to the target use case to be systematically undertaken. In particular, we note that reinforcement learning from human feedback may be relevant for not only making the models more helpful, but also more robust to adversarial queries (Ganguli et al. 2022). We also see promise in code adaptation of the Falcon models, similar to PaLM-Coder (Chowdhery et al. 2022) and PaLM-2 S* (Anil et al. 2023) or Code LLaMA (RoziÃ¨re et al., 2023).

## 8 Conclusion

In this paper, we have introduced and extensively described the Falcon series of pretrained models, with Falcon-7/40/180B, trained on up to 3,500B tokens.

First, we described some of the ablations and experiments we performed to prepare the training dataset and architecture of the Falcon models. We found adequately filtered and deduplicated web data to be a surprisingly strong baseline; concerns around memorization for larger models (Carlini et al. 2022; Hernandez et al. 2022) led us to elect not to upsample any sources, and to predominantly train on this web data. On the architecture-side, we found most interventions to have a limited effect, and adopted multigroup attention (an extension of multiquery (Shazeer, 2019) as a way to improve inference scalability by significantly reducing the size of the $\mathrm{K}, \mathrm{V}$-cache. For future generations of the Falcon series, we see promise in significantly increasing the fraction of code in the pretraining data, and in training with longer sequence lengths in a staged process (e.g., half of the training up to $8 \mathrm{k}$, and second half up to $16 \mathrm{k}$, with downstream adaptation to $32-256 \mathrm{k}$ ).

Then, we described our implementation of our final strategy for the pretraining of the Falcon series. We report extensively on our data pipeline in Penedo et al. (2023). We described our approach to conversational masking, and our distributed training strategy for running on cost-efficient cloud infrastructure, relying notably on 3D parallelism combined with ZeRO. We also discussed some of our interventions for fast memory efficient training, such as dedicated FlashAttention (Dao et al. 2022) kernels in Triton (Tillet et al. 2019) and our monolayer strategy. We also discussed some of the details around setting hyper-parameters and managing runs over multiple thousand GPUs.

Finally, we outlined some of the results obtained by the Falcon series on key benchmarks. We found Falcon-180B to near the performance of PaLM-2 Large (Anil et al., 2023), and to end-up in-between GPT-3.5 and GPT-4 (OpenAI, 2023a). Falcon-180B is the best open-source model currently available, and likely one of the best models overall. We note our evaluation predominantly focuses on classic natural language tasks, and that further work will be required for evaluating human preferences on downstream versions of Falcon having undergone dedicated finetuning or reinforcement learning.

To foster open research on large language models, and accelerate technological development in this space, we make the following artefacts public available under permissive open-source licenses:

- Falcon-7/40/180B. We make all models in the Falcon series available, with Falcon-7/40B under an Apache 2.0 license and Falcon-180B under a dedicated responsible AI use license. At time of release, Falcon-180B is the most powerful open large language model available.
- A 600B tokens extract of RefinedWeb. We make a 600B tokens extract of our web dataset available, for use by researchers to study large-scale filtered and deduplicated web data, and for other practioners to adopt as a standard for high-quality web data. We also open-source 1/7B models trained on 350B tokens from this extract.
- Detailed research. With this paper and the RefinedWeb paper (Penedo et al., 2023), we have detailed numerous of our decisions and experiments concerning the Falcon series.

We believe large language models to be a foundational technology for the future of our civilization, and in turn we believe they should be shared responsibly. Widespread exchange of ideas is a staple of accelerated technological and economical progress in our history; in turn, this acceleration uplifts all. By open-sourcing artificial intelligence research and models, we can foster a broader and more diverse community, and benefit from vibrant collaborative efforts to improve the safety and reliability of large language models. We hope the Falcon series can be a small step towards this vision.

## References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., ManÃ©, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., ViÃ©gas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org.

Abadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B. (2022). Towards a Cleaner Document-Oriented Multilingual Crawled Corpus. arXiv e-prints, page arXiv:2201.06642.

Adiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et al. (2020). Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.

Aghajanyan, A., Yu, L., Conneau, A., Hsu, W.-N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal, N., Levy, O., and Zettlemoyer, L. (2023). Scaling laws for generative mixed-modal language models. arXiv preprint arXiv:2301.03728.

Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., LebrÃ³n, F., and Sanghai, S. (2023). Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245.

Aleph Alpha (2023). Luminous: performance benchmarks. arXiv preprint arXiv:1810.12885.

Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., et al. (2023). Santacoder: don't reach for the stars! In Deep Learning for Code (DL4C) Workshop.

Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403.

Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al. (2021). Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684.

Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022b). Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.

Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., and Blackburn, J. (2020). The pushshift reddit dataset.

Bavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. (2022). Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255.

Beltagy, I., Lo, K., and Cohan, A. (2019). Scibert: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages $3615-3620$.

Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., OâBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. (2023). Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR.

Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. (2020). Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence.

Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., et al. (2022). Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode\# 5-Workshop on Challenges \& Perspectives in Creating Large Language Models, pages 95-136.

Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. (2021). GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., et al. (2021). Jax: Autograd and xla. Astrophysics Source Code Library, pages ascl-2111.

Broder, A. Z. (1997). On the resemblance and containment of documents. In Proceedings. Compression and Complexity of Sequences 1997, pages 21-29. IEEE.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.

Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. (2022). Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.

Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. (2013). One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_fae1fd0fa75ae040bd4cg-41.jpg?height=38&width=260&top_left_y=1564&top_left_x=407)

Chen, S., Wong, S., Chen, L., and Tian, Y. (2023). Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.

Chiang, C.-H. and Lee, H.-y. (2023). Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937.

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Clark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud, S., et al. (2022). Unified scaling laws for routed language models. In International Conference on Machine Learning, pages 4057-4086. PMLR.

Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. (2019). Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL.

Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, $\mathrm{abs} / 1803.05457$.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., GuzmÃ¡n, F., Grave, Ã., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages $8440-8451$.

Costa-jussÃ , M. R., Cross, J., Ãelebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., et al. (2022). No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.

Dao, T. (2023). Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691.

Dao, T., Fu, D., Ermon, S., Rudra, A., and RÃ©, C. (2022). Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359$.

Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. (2022). Llm.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:3031830332 .

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Dey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness, J., et al. (2023). Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208.

Dinan, E., Yaida, S., and Zhang, S. (2023). Effective theory of transformers at initialization. arXiv preprint arXiv:2304.02034.

Dodge, J., Sap, M., MarasoviÄ, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286-1305.

Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.

Eberhard, D. M., Simons, G. F., and Fennig, C. D. (2023). Ethnologue: Languages of the World. SIL International, Dallas, TX, USA, twenty-sixth edition.

Fedus, W., Zoph, B., and Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):52325270 .

Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. (2021). Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1-8.

Fourrier, C., Habib, N., Launay, J., and Wolf, T. (2023). What's going on with the open $11 \mathrm{~m}$ leaderboard? "https://huggingface.co/blog/evaluating-mmlu-leaderboard".

Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. (2022). Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations.

Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858.

Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. (2020). The Pile: an 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.

Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for few-shot language model evaluation.

Gordon, A., Kozareva, Z., and Roemmele, M. (2012). SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 394-398, MontrÃ©al, Canada. Association for Computational Linguistics.

Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. (2023). Textbooks are all you need. arXiv preprint arXiv:2306.11644.

Hamborg, F., Meuschke, N., Breitinger, C., and Gipp, B. (2017). news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pages 218-223.

Haviv, A., Ram, O., Press, O., Izsak, P., and Levy, O. (2022). Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382-1390.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2020). Measuring massive multitask language understanding. In International Conference on Learning Representations.

Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., HatfieldDodds, Z., Henighan, T., Hume, T., et al. (2022). Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487.

Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. (2017). Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409.

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.

Hooker, S. (2021). The hardware lottery. Communications of the ACM, 64(12):58-65.

Howard, J. and Ruder, S. (2018). Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328-339.

Inan, H., Khosravi, K., and Socher, R. (2016). Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462.

Inflection (2023). Inflection 1.

Johannes Welbl, Nelson F. Liu, M. G. (2017). Crowdsourcing multiple choice science questions.

Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. (2016). Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410.

kaiokenmdenv (2023). Extending context is hard. . . but not impossible. Accessed: 2023-10-02.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186.

Kocetkov, D., Li, R., Jia, L., Mou, C., Jernite, Y., Mitchell, M., Ferrandis, C. M., Hughes, S., Wolf, T., Bahdanau, D., et al. (2022). The stack: 3 tb of permissively licensed source code. Transactions on Machine Learning Research.

Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A. A., Subramani, N., Sokolov, A., Sikasote, C., et al. (2022). Quality at a glance: An audit of webcrawled multilingual datasets. Transactions of the Association for Computational Linguistics, $10: 50-72$.

Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. (2017). Race: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785-794.

LaurenÃ§on, H., Saulnier, L., Wang, T., Akiki, C., del Moral, A. V., Scao, T. L., Werra, L. V., Mou, C., Ponferrada, E. G., Nguyen, H., Frohberg, J., Å aÅ¡ko, M., Lhoest, Q., McMillan-Major, A., Dupont, G., Biderman, S., Rogers, A., allal, L. B., Toni, F. D., Pistilli, G., Nguyen, O., Nikpoor, S., Masoud, M., Colombo, P., de la Rosa, J., Villegas, P., Thrush, T., Longpre, S., Nagel, S., Weber, L., MuÃ±oz, M. R., Zhu, J., Strien, D. V., Alyafeai, Z., Almubarak, K., Chien, V. M., Gonzalez-Dios, I., Soroa, A., Lo, K., Dey, M., Suarez, P. O., Gokaslan, A., Bose, S., Adelani, D. I., Phan, L., Tran, H., Yu, I., Pai, S., Chim, J., Lepercq, V., Ilic, S., Mitchell, M., Luccioni, S., and Jernite, Y. (2022). The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.

Le Scao, T., Wang, T., Hesslow, D., Bekman, S., Bari, M. S., Biderman, S., Elsahar, H., Muennighoff, N., Phang, J., Press, O., et al. (2022). What language model to train if you have one million gpu hours? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages $765-782$.

Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2022). Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84248445 .

Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A. (2020). Limits to depth efficiencies of self-attention. Advances in Neural Information Processing Systems, 33:22640-22651.

Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. (2022). Solving quantitative reasoning problems with language models.

Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. (2023a). Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161.

Li, S., Xue, F., Li, Y., and You, Y. (2021). Sequence parallelism: Making 4d parallelism possible. arXiv preprint arXiv:2105.13120.

Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. (2023b). Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463.

Liang, D., Gonen, H., Mao, Y., Hou, R., Goyal, N., Ghazvininejad, M., Zettlemoyer, L., and Khabsa, M. (2023). Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models. arXiv preprint arXiv:2301.10472.

Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. (2022). Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.

Lieber, O., Sharir, O., Lenz, B., and Shoham, Y. (2021). Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs.

Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O'Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M., Stoyanov, V., and Li, X. (2021). Few-shot learning with multilingual language models. ArXiv, abs/2112.10668.

Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. (2023). The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688.

Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.

Loshchilov, I. and Hutter, F. (2018). Decoupled weight decay regularization. In International Conference on Learning Representations.

Luo, S., Li, S., Zheng, S., Liu, T.-Y., Wang, L., and He, D. (2022). Your transformer may not be as powerful as you expect. arXiv preprint arXiv:2205.13401.

Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. (2023). Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568.

Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. (2022). Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384-1403.

Manber, U. and Myers, G. (1993). Suffix arrays: a new method for on-line string searches. Journal on Computing, 22(5):935-948.

MÃ©moli, F. (2011). Gromov-wasserstein distances and the metric approach to object matching. Foundations of computational mathematics, 11:417-487.

Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. (2018). Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.

Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.

Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D., and Gebru, T. (2019). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220-229.

MosaicML (2023). Introducing mpt-30b: Raising the bar for open-source foundation models Accessed: 2023-06-22.

Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. (2023). Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.

Narayanan, D., Phanishayee, A., Shi, K., Chen, X., and Zaharia, M. (2021a). Memory-efficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR.

Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al. (2021b). Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15.

OpenAI (2023a). Gpt-4 technical report. arXiv, pages 2303-08774.

OpenAI (2023b). Model index for researchers. Accessed: 2023-09-26.

Ortiz SuÃ¡rez, P. J., Sagot, B., and Romary, L. (2019). Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 - 16, Mannheim. Leibniz-Institut fÃ¼r Deutsche Sprache.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and FernÃ¡ndez, R. (2016). The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534, Berlin, Germany. Association for Computational Linguistics.

Paresh, D. (2023). Stack overflow will charge ai giants for training data.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in pytorch. In NIPS-W.

Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023). The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.

Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. (2018). Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.

Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J. (2023). Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5 .

Press, O., Smith, N., and Lewis, M. (2022). Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations.

Press, O. and Wolf, L. (2016). Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859.

Rabe, M. N. and Staats, C. (2021). Self-attention does not need $o\left(n^{2}\right)$ memory. arXiv preprint arXiv:2112.05682.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI Blog.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. (2021). Scaling language models: Methods, analysis \& insights from training gopher. arXiv preprint arXiv:2112.11446.

Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. (2019). Compressive transformers for long-range sequence modelling.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, $\mathrm{abs} / 1910.10683$.

Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. (2020). Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16. IEEE.

Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J., Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Saeta, B., Sepassi, R., Spiridonov, A., Newlan, J., and Gesmundo, A. (2022). Scaling up models and data with $\mathrm{t} 5 \mathrm{x}$ and seqio. arXiv preprint arXiv:2203.17189.

RoziÃ¨re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. (2023). Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950.

Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. (2019). Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641.

Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L. A., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., BARI, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., FÃ©vry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T. G. O., Wolf, T., and Rush, A. M. (2021). Multitask prompted training enables zero-shot task generalization. ArXiv, $\mathrm{abs} / 2110.08207$.

Scao, T. L., Fan, A., Akiki, C., Pavlick, E., IliÄ, S., Hesslow, D., CastagnÃ©, R., Luccioni, A. S., Yvon, F., GallÃ©, M., et al. (2022a). Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.

Scao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M. S., Bideman, S., Elsahar, H., Muennighoff, N., Phang, J., et al. (2022b). What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424.

Shaham, U., Elbayad, M., Goswami, V., Levy, O., and Bhosale, S. (2022). Causes and cures for interference in multilingual translation. arXiv preprint arXiv:2212.07530.

Shannon, C. E. (1951). Prediction and entropy of printed english. Bell system technical journal, $30(1): 50-64$.

Shaw, P., Uszkoreit, J., and Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint arXiv:1803.02155.

Shazeer, N. (2019). Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150.

Shazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202.

Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. (2018). Mesh-TensorFlow: Deep learning for supercomputers. In Neural Information Processing Systems.

Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatronlm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.

Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., et al. (2022). Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.

Soldaini, L., Lo, K., Kinney, R., Naik, A., Ravichander, A., Bhagia, A., Groeneveld, D., Schwenk, D., Magnusson, I., and Chandu, K. (2023). Dolma.

Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, $15(56): 1929-1958$.

Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021.

Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. (2021). Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.

Sutton, R. (2019). The bitter lesson. Incomplete Ideas (blog), 13(1).

Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model. https://github.com/ tatsu-lab/stanford_alpaca

Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., and Metzler, D. (2021). Scale efficiently: Insights from pre-training and fine-tuning transformers. ArXiv, abs/2109.10686.

Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Bahri, D., Schuster, T., Zheng, S., et al. (2022a). Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations.

Tay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao, J., Chowdhery, A., et al. (2022b). Transcending scaling laws with $0.1 \%$ extra compute. arXiv preprint arXiv:2210.11399.

Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. (2022). Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.

Tian, R. and Parikh, A. P. (2022). Amos: An adam-style optimizer with adaptive weight decay towards model-oriented scale. arXiv preprint arXiv:2210.11693.

Tiedemann, J. (2016). Finding alternative translations in a large corpus of movie subtitle. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3518-3522, PortoroÅ¾, Slovenia. European Language Resources Association (ELRA).

Tillet, P., Kung, H.-T., and Cox, D. (2019). Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10-19.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023a). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Trinh, T. H. and Le, Q. V. (2018). A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., and Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.

Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A. (2022). Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325.

Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2019). Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32 .

Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.

Wang, B. and Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax

Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. (2023). Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.

Wang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C. (2022a). What language model architecture and pretraining objective work best for zero-shot generalization? arXiv preprint arXiv:2204.05832.

Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2022b). Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.

Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. (2022a). Emergent abilities of large language models. Transactions on Machine Learning Research.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022b). Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.

Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447-2469.

Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., GuzmÃ¡n, F., Joulin, A., and Grave, Ã. (2020). Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4003-4012.

Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. (2023). Effective long-context scaling of foundation models.

Xue, F., Fu, Y., Zhou, W., Zheng, Z., and You, Y. (2023). To repeat or not to repeat: Insights from scaling llm under token-crisis. arXiv preprint arXiv:2305.13230.

Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. (2022). Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291-306.

Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2021). mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498.

Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J. (2022). Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.

Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang, X., et al. (2021). Pangu- $\alpha$ : Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv preprint arXiv:2104.12369.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. (2022). Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.

Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. (2023). Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364.

Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages $19-27$.
