# TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models 

Chen Zhang ${ }^{1}$ Chengguang Tang ${ }^{2}$ Dading Chong ${ }^{3}$<br>Ke Shi $^{2} \quad$ Guohua Tang $^{2} \quad$ Feng Jiang $^{4}$ Haizhou $\mathbf{L i}^{1,4}$<br>${ }^{1}$ National University of Singapore, Singapore<br>${ }^{2}$ Tencent AI Lab, China ${ }^{3}$ Peking University, China<br>${ }^{4}$ The Chinese University of Hong Kong, Shenzhen, China<br>chen_zhang@u.nus.edu; jeffreyjiang@cuhk.edu.cn


#### Abstract

Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the "TS-Align" framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a smallscale student model. The policy fine-tuning process can be iteratively repeated using onpolicy generations within our proposed teacherstudent collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of $69.7 \%$ across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.


## 1 Introduction

General-purpose conversational AI assistants, such as GPT-4 (Achiam et al., 2023) and Gemini (Google et al., 2023), are empowered by aligning large pretrained language models with humanpreferred behaviors (Stiennon et al., 2020a; Ouyang et al., 2022; Bai et al., 2022a). These aligned LLMs showcase exceptional capabilities in instruction following (Touvron et al., 2023; Tunstall et al., 2023), natural conversation (Thoppilan et al., 2022; Ding et al., 2023), safety (Ganguli et al., 2022; Dai et al., 2023), reasoning (Wei et al., 2022b; Kojima et al., 2022), among others. Commonly-used LLM alignment techniques include instruction tuning (Wei et al., 2022a; Chung et al., 2022), reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019), and direct preference optimization (DPO) (Rafailov et al., 2023).

While recent research has focused significantly on the development of more sophisticated alignment techniques (Song et al., 2023; Yuan et al., 2023; Liu et al., 2023; Xu et al., 2023b; Ethayarajh et al., 2024; Meng et al., 2024), it is worth noting that LLM alignment is not a one-time process and the model requires continuous refinement to adapt to evolving user needs and changing linguistic patterns. The standard practice for iterative alignment of the LLMs is to gather new human preference data for every subsequent update to the model. For instance, Touvron et al. (2023) performs five iterations of RLHF finetuning on the base SFT LLaMA2 model. For each iteration, they update the reward model with newly collected human preference data. This process poses challenges regarding scalability and resource requirements.

To alleviate the issue, existing research adopts self-evolution (Li et al., 2023a; Yuan et al., 2024; Chen et al., 2024) or external model supervision (Xu et al., 2023b; Singh et al., 2023; Guo et al., 2024). The effectiveness of self-evolution is highly dependent on the quality of the base model as it operates without the introduction of external supervision or knowledge during refinement. For instance, in their study, Yuan et al. (2024) utilize a sophisticated 70B LLaMA-2 model to demonstrate the potential of their iterative self-rewarding procedure. When employing external model supervision, it is crucial to utilize a robust model that can effectively generalize to new data. Typically, these models are substantially large to avoid reward overoptimization (Gao et al., 2023). Despite being reliable, labeling abundant data with a large-scale model is still very costly and time-consuming.

In this paper, we aim to balance reliability and efficiency in the data labeling process during the it-
erative fine-tuning of the policy model. To achieve this, we propose TS-Align, a teacher-student collaborative framework that leverages the reliability of the large-scale teacher model without requiring it to process all the candidates. Specifically, TS-Align uses a base supervised fine-tuned policy model to generate response candidates for a diverse set of instruction prompts sampled from public instruction-tuning datasets. A small-scale student reward model (RM) provides coarse-grained annotations, allowing for the quick processing of abundant unlabeled data and the selection of preference pairs from the candidates. Next, the strong teacher helps re-rank the selected pairs reliably. The policy model is then fine-tuned on the re-ranked preference data using DPO. This process is repeated in several iterations. Given that the student RM, with its smaller parameter size, is not as robust as the teacher model, we iteratively update the student using an adapter-based multi-task training setup (Pfeiffer et al., 2021). This training uses the same model-labeled preference data to enhance the student's reliability, which can be perceived as distilling new knowledge from the large teacher model to the small student RM.

Our contributions are three-fold: (1) We introduce "TS-Align", an efficient and reliable pipeline for the iterative alignment of large language models. This approach circumvents the need for costly human annotations by employing a teacher-student model collaboration to automatically extract preference data from the policy model's own outputs. (2) We demonstrate that the teacher-student collaborative mechanism produces a strong aligned policy model with an average win rate of $69.7 \%$ over the base policy on 7 conversational or instructionfollowing datasets, while also being efficient. (3) Through our pipeline, the response ranking capability of the teacher model is progressively distilled into the student model. We demonstrate that the enhanced capability of the final student model can be transferred to align other policy models.

## 2 Preliminaries

This section presents the background information. We also provide the notation list in Table 1 for better illustration.

Supervised Finetuning The base policy model should possess basic instruction-following and natural conversational capabilities. Hence, the initial step involves supervised finetuning of a pretrained

| Symbol | Definition |
| :--- | :--- |
| $\pi$ | A general notation for the policy model. |
| $\pi_{0}$ | The supervised fine-tuned base policy model. |
| $\pi_{t}$ | The policy model to be aligned at the t-th iteration |
| $r$ | A general notation for reward model. |
| $\mathcal{S}_{0}$ | The base student reward model. |
| $\mathcal{S}_{t}$ | The student reward model to be updated at the t-th iteration. |
| $\mathcal{M}$ | The teacher reward model. |
| $\mathcal{X}$ | The source of prompt instructions. |
| $\mathcal{D}_{S F T}$ | The supervised fine-tuning dataset. |
| $\mathcal{D}_{\text {pref }}$ | The offline human preference dataset. |
| $x$ | A single instruction prompt. |
| $\mathbf{y}$ | A set of completion candidates of $x$. |
| $y$ | The completion of $x$. |
| $y^{+}$ | The favored completion of $x$. |
| $y^{-}$ | The unfavored completion of $x$. |
| $\mathcal{D}_{\text {ins }}^{t}$ | The batch of instruction prompts at the t-th iteration. |
| $\mathcal{D}_{\text {auto }}^{t}$ | The model-annotated preference dataset derived from $\mathcal{D}_{\text {ins }}^{t}$ |

Table 1: The list of notations.

language model:

$$
\mathcal{L}_{\mathrm{SFT}}\left(\pi_{0}, \mathcal{D}_{\mathrm{SFT}}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{SFT}}}\left[\log P_{\pi}(y \mid x)\right]
$$

Direct Preference Optimization DPO is derived from the Bradley-Terry model of human preferences (Bradley and Terry, 1952), which defines the human preference distribution as:

$$
\begin{equation*}
P^{*}\left(y^{+}>y^{-} \mid x\right)=\frac{\exp \left(r^{*}\left(x, y^{+}\right)\right)}{\exp \left(r^{*}\left(x, y^{+}\right)\right)+\exp \left(r^{*}\left(x, y^{-}\right)\right)} \tag{1}
\end{equation*}
$$

where $r^{*}$ represents a latent reward model that captures the true preferences and it is parameterized by $r_{\phi}$, which is trained via the following binary classification objective on $\mathcal{D}_{\text {pref }}$ :

$$
\begin{aligned}
\mathcal{L}_{\mathrm{RM}}\left(r_{\phi}, \mathcal{D}_{\text {pref }}\right)= & -\mathbb{E}_{\left(x_{j}, y_{j}^{+}, y_{j}^{-}\right) \sim \mathcal{D}_{\text {pref }}}\left[\operatorname { l o g } \sigma \left(r_{\phi}\left(x_{j}, y_{j}^{+}\right)\right.\right. \\
& \left.\left.-r_{\phi}\left(x_{j}, y_{j}^{-}\right)\right)\right]
\end{aligned}
$$

Instead of modeling $r_{\phi}$, DPO utilizes a reparameterization trick on $r^{*}(x, y)$, effectively converting the objective 1 to rely solely on the optimal policy $\left(\pi^{*}\right)$ and reference policy $\left(\pi_{\text {ref }}\right.$ ) models:

$$
P^{*}\left(y^{+}>y^{-} \mid x\right)=\frac{1}{1+\exp \left(\beta \log \frac{\pi^{*}\left(y^{-} \mid x\right)}{\pi_{\text {ref }}\left(y^{-} \mid x\right)}-\beta \log \frac{\pi^{*}\left(y^{+} \mid x\right)}{\pi_{\text {ref }}\left(y^{+} \mid x\right)}\right)}
$$

where $\beta$ is a hyperparameter. $\pi^{*}$ is estimated with a parameterized policy $\pi_{\theta}$, which is learned with the maximum likelihood objective:

$$
\begin{aligned}
\mathcal{L}_{\mathrm{DPO}}\left(\pi_{\theta} ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x_{j}, y_{j}^{+}, y_{j}^{-}\right) \sim \mathcal{D}_{\text {pref }}} & {\left[\operatorname { l o g } \sigma \left(\beta \log \frac{\pi_{\theta}\left(y_{j}^{+} \mid x_{j}\right)}{\pi_{\text {ref }}\left(y_{j}^{+} \mid x_{j}\right)}\right.\right.} \\
& \left.\left.-\beta \log \frac{\pi_{\theta}\left(y_{j}^{-} \mid x_{j}\right)}{\pi_{\text {ref }}\left(y_{j}^{-} \mid x_{j}\right)}\right)\right]
\end{aligned}
$$

Both $\pi_{r e f}$ and $\pi_{\theta}$ are initialized as $\pi_{0}$. During training, $\pi_{r e f}$ is frozen while $\pi_{\theta}$ is optimized.

## 3 The TS-Align Pipeline

The overall workflow of TS-Align is summarized in Algorithm 1. The key idea is to align the policy model through multiple alignment iterations and the procedure for each iteration is depicted in Figure 1. Sections $\S 3.1$ to $\S 3.3$ discuss the essential details of this pipeline.

```
Algorithm 1 TS-Align
Require: $\pi_{0}, \mathcal{S}_{0}, \mathcal{M}, \mathcal{X}$
    for $t \leftarrow 0$ to $T$ do
        Sample prompts from $\mathcal{X}$ to form $\mathcal{D}_{\text {ins }}^{t}$.
        Initialize empty set $\mathcal{D}_{\text {auto }}^{t}$.
        for $x$ in $\mathcal{D}_{\text {ins }}^{t}$ do
            $\mathbf{y} \leftarrow \operatorname{Generate}\left(\pi_{t}, x\right)$.
            $\left\{x, y_{j}^{+}, y_{j}^{-}\right\} \leftarrow \operatorname{Score}\left(\mathcal{S}_{t}, x, \mathbf{y}\right)$.
            Rerank $\left(x, y_{i}^{+}, y_{i}^{-}\right)$with $\mathcal{M}$.
            Add re-ranked $\left(x, y_{i}^{+}, y_{i}^{-}\right)$to $\mathcal{D}_{\text {auto }}^{t}$
        end for
        $\mathcal{S}_{t+1} \leftarrow \operatorname{Update}\left(\mathcal{S}_{t}, \mathcal{D}_{\text {auto }}^{t}\right)$
        $\pi_{t+1} \leftarrow \operatorname{DPO}\left(\pi_{t}, \mathcal{D}_{\text {auto }}^{t}\right)$
    end for
```


### 3.1 Automatic Preference Pair Construction

We construct a prompt source $\mathcal{X}$ that contains instruction prompts from diverse public instructiontuning datasets (described in §4.1). For each alignment iteration $t$, we sample an abundant amount of instructions from $\mathcal{X}$ to form $\mathcal{D}_{\text {ins }}^{t}$ for preference pair construction. For each $x \in \mathcal{D}_{\text {ins }}^{t}, K$ response candidates, $\mathbf{y}=\left\{y_{1}, y_{2}, \ldots, y_{k}\right\}$, is generated from $\pi_{t}$. $\mathcal{S}_{t}$ is applied to score the candidates. A preference pair, $\left(y^{+}, y^{-}\right)$, is formed using the candidates with the highest $\left(y^{\text {best }}\right.$ ) and lowest scores $\left(y^{\text {worst }}\right.$ ) respectively.

Given the potential unreliability of annotations from $\mathcal{S}_{t}$, we utilize a strong teacher model, $\mathcal{M}$, to refine the ranking of $\left(y^{+}, y^{-}\right)$. Subsequently, we add the pair to the model-annotated preference dataset $\mathcal{D}_{\text {auto }}^{t}$. The benefits of this teacher-student collaborative mechanism are the efficiency in data annotation and the continuous improvement of the student reward model through knowledge distillation in each alignment iteration.

### 3.2 The Student Reward Model

Initial Base Version $\mathcal{S}_{0}$ is initially pre-trained on a predefined human-labeled preference dataset, $\mathcal{D}_{\text {pref }}=\left\{y_{j}^{+}>y_{j}^{-} \mid x_{j}\right\}_{j=1}^{\left|\mathcal{D}_{\text {pref }}\right|}$. We implement $\mathcal{S}_{0}$ as a RoBERTa-based scoring model, which is first trained on concatenated text sequences $\left(x_{j}, y_{j}\right)$ for faster convergence and domain adaptation, utilizing the masked language modeling (MLM) objective. Next, $\mathcal{S}_{0}$ learns to predict a higher score for $y_{j}^{+}$ than $y_{j}^{-}$by minimizing the following margin ranking loss:

$\mathcal{L}_{\mathrm{RM}}\left(\mathcal{S}, \mathcal{D}_{\text {pref }}\right)=\frac{1}{\left|\mathcal{D}_{\text {pref }}\right|} \sum_{j=1}^{\left|\mathcal{D}_{\text {pref }}\right|} \max \left(0, s_{j}^{-}-s_{j}^{+}+0.1\right)$

where $s_{j}^{-}$and $s_{j}^{+}$represent the output scores for $y_{j}^{-}$ and $y_{j}^{+}$respectively.

Subsequent Update After constructing the modelannotated preference dataset $\mathcal{D}_{\text {auto }}^{t}$ using the procedure outlined in $\S 3.1$, we adapt the student reward model to the new data using adapter-based multitask learning (Pfeiffer et al., 2021). Specifically, the student is re-trained with preference data batches from previous iterations, along with those from the current iteration, $\left\{\mathcal{D}_{\text {pref }}, \mathcal{D}_{\text {auto }}^{0}, \ldots, \mathcal{D}_{\text {auto }}^{t}\right\}$. Each adapter is fine-tuned with one data batch using the above-mentioned margin ranking loss function, while the shared RoBERTa encoder is fine-tuned on all the data. This approach not only facilitates the distillation of the new knowledge from the teacher into the student but also mitigates the forgetting of previously learned knowledge. Motivated by previous research on model weight averaging (Wortsman et al., 2022; Rame et al., 2022), we average the weights of all the injected adapters from different alignment iterations for faster inference.

### 3.3 Aligning Policy Model

We adopt DPO to align the base policy model $\pi_{0}$. To stabilize the training process, we add the supervised finetuning loss term to the DPO objective:

$$
\mathcal{L}_{\text {final }}\left(\pi_{\theta}\right)=\alpha \mathcal{L}_{\mathrm{SFT}}+\mathcal{L}_{\mathrm{DPO}}
$$

where alpha is a hyperparameter set to 0.05 . The SFT objective is optimized with the positive responses $\left\{x_{j}, y_{j}^{+}\right\}$in $\mathcal{D}_{\text {auto }}^{t}$.

## 4 Experiment Setup

### 4.1 Datasets

Prompt Source We sample new instruction prompts from a diverse array of open-source instruction-tuning datasets, which are summarized in table 8. For each alignment iteration, we sample

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-04.jpg?height=457&width=1587&top_left_y=223&top_left_x=226)

Figure 1: The figure depicts one alignment iteration of TS-Algin. The process can be repeated multiple times on the updated policy model and student reward model.

| Test Datasets | Size | Avg. \#Prompt <br> Words | Avg. \#Turns | Purpose |
| :--- | :---: | :---: | :---: | :---: |
| HH-RLHF | 8,550 | 93.05 | 2.38 | $\mathrm{P}, \mathrm{R}$ |
| PKU-BeaverTails | 2,985 | 13.17 | 1.00 | $\mathrm{P}, \mathrm{R}$ |
| Alpaca-Eval | 805 | 28.56 | 1.00 | $\mathrm{P}$ |
| IFEval | 541 | 37.07 | 1.00 | $\mathrm{P}$ |
| SHP | 18,409 | 148.79 | 1.00 | $\mathrm{R}$ |
| Alpaca-Farm | 17,701 | 28.57 | 1.00 | $\mathrm{R}$ |

Table 2: Statistics of the test data. In the purpose column, "P" stands for policy model evaluation, and "R" denotes reward model evaluation.

$5 \mathrm{~K}$ prompts from each dataset. In total, we use $30 \mathrm{~K}$ prompts per alignment iteration.

Test Datasets We evaluate the policy models on four conversational or instruction-following test datasets: (1) Anthropic HH-RLHF Test ${ }^{1}$ (Bai et al., 2022a), (2) PKU-BeaverTails Test (Ji et al., 2023), (3) Alpaca-Eval (Li et al., 2023b), and (4) IFEval (Zhou et al., 2023). All the datasets measure the model's ability to follow instructions and provide helpful responses. HH-RLHF and PKUBeaverTails also examine the models' abilities to handle harmful user input.

The reward models are assessed on four offline human preference test datasets: (1) Anthropic HHRLHF Test, (2) PKU-BeaverTails Test, (3) the Standford Human Preference (SHP) Test (Ethayarajh et al., 2022), and (4) Alpaca-Farm (Dubois et al., 2023). The statistics of test datasets are presented in table 2 .

### 4.2 Implementation Details

Policy Models We use the LLaMA Factory library (Zheng et al., 2024) for all the finetuning[^0]

experiments. Low-rank adaptation (LoRA) (Hu et al., 2022) is used with rank set to 8 and alpha set to 16. The target modules are the query and key projection matrices. Each finetuning experiment is performed on a single 40GB NVIDIA A100 GPU card. We use a batch size of 8 and 2 gradient accumulation steps and employ a cosine learning rate schedule. The off-the-shelf Alpaca-7B (Taori et al., 2023) is adopted as $\pi_{0}$ in algorithm 1 and the number of responses sampled from the policy model in the "Generate" step is set to 16. In total, two alignment iterations are performed.

Reward Model We implement the student RM using the adapter-transformers library (Pfeiffer et al., 2020). The base student RM is a RoBERTa-Large encoder followed by a linear layer with output dimension 1 and a sigmoid activation function. It is fine-tuned on $40 \mathrm{~K}$ human preference data, employing a learning rate of $5 e^{-6}$ and a batch size of 8 . The human preference data consist of instances sampled equally from the training splits of Anthropic HH-RLHF, Stanford SHP, PKUBeaverTails, and UltraFeedback (Cui et al., 2023). In the subsequent alignment iterations, the adapters are finetuned with a learning rate of $1 e^{-5}$ and a batch size of 8 . The adapters are configured to the variant introduced in Houlsby et al. (2019).

For the teacher model, we experiment with both the UltraRM-13B model (Cui et al., 2023), which is initialized from LLaMA2-13B and fine-tuned on a mixture of UltraFeedback and an equal-size sample from three open-source datasets including Anthropic HH-RLHF, Standford SHP, and OpenAI Summarization (Stiennon et al., 2020b).

|  | Harmless Base | Helpful Base | Helpful Online | Helpful Rejection | Beavertails | Alpaca-Eval | IFEval | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Direct DPO | $57.66(0.91)$ | $67.74(0.87)$ | $64.09(1.30)$ | $67.97(0.81)$ | $57.73(0.74)$ | $54.89(1.54)$ | $52.74(1.74)$ | 60.40 |
| BoN | $55.41(0.93)$ | $61.60(0.92)$ | $60.54(1.33)$ | $63.13(0.85)$ | $54.48(0.76)$ | 47.04 (1.58) | $43.71(1.78)$ | 55.13 |
| OAIF (iter1) | $53.58(0.92)$ | $69.71(0.86)$ | $64.12(1.29)$ | $70.44(0.80)$ | $59.27(0.73)$ | $56.22(1.54)$ | $51.41(1.77)$ | 60.68 |
| OAIF (iter2) | $56.60(0.93)$ | $70.61(0.85)$ | $66.88(1.27)$ | $71.12(0.79)$ | $60.03(0.73)$ | $56.45(1.55)$ | $53.31(1.75)$ | 62.14 |
| Student RM on | $62.50(0.91)$ | $73.91(0.83)$ | 69.87 (1.24) | $74.47(0.76)$ | $65.01(0.70)$ | 57.26 (1.57) | $52.32(1.76)$ | 65.05 |
| Student RM only (iter2) | $64.47(0.86)$ | $77.57(0.78)$ | $71.66(1.21)$ | $76.52(0.73)$ | $63.48(0.69)$ | $59.63(1.52)$ | $54.90(1.79)$ | 66.89 |
| Teacher RM only (iter1) | $61.96(0.92)$ | $77.26(0.79)$ | $73.04(1.19)$ | $77.14(0.72)$ | $63.00(0.72)$ | 62.54 | $57.92(1.73)$ | 67.55 |
| Teacher RM only (iter2) | $64.57(0.89)$ | $82.92(0.70)$ | 78.04 (1.10) | $82.68(0.64)$ | $70.08(0.66)$ | $\overline{67.65}(1.44)$ | $\underline{58.67}(1.74)$ | 72.09 |
| TS-Align (iter1) | $\overline{60.70}(0.91)$ | $75.66(0.80)$ | $69.68(1.24)$ | $76.03(0.74)$ | $62.54(0.71)$ | $60.06(1.53)$ | $\overline{55.20}(1.77)$ | 65.70 |
| TS-Align (iter2) | $64.82(0.89)$ | $79.22(0.75)$ | 73.70 (1.18) | $79.46(0.69)$ | $69.45(0.66)$ | $62.11(1.50)$ | $\mathbf{5 9 . 1 2 ( 1 . 7 7 )}$ | 69.70 |

Table 3: Win rate (\%) of the aligned policy models against the base Alpaca-7B model as judged by GPT-4-Turbo. The standard errors are displayed in the bracket. All the methods went through two alignment iterations except "Direct DPO" and "BoN". Iter1 and Iter2 represent the first and the second alignment iterations respectively. The best score is highlighted in bold while the second best is underlined.

| Annotator | Speed | Cost | \#Parameters |
| :--- | :---: | :---: | :---: |
| RoBERTa RM | $23.19 \mathrm{it} / \mathrm{s}$ | - | $\sim 370 \mathrm{M}$ |
| UltraRM | $14.60 \mathrm{it} / \mathrm{s}$ | - | $\sim 13 \mathrm{~B}$ |
| GPT-3.5-turbo | $0.55 \mathrm{it} / \mathrm{s}$ | $4.6 \mathrm{e}-4 \$ \mathrm{it}$ | - |
| Human | $0.027 \mathrm{it} / \mathrm{s}$ | $0.3 \$ / \mathrm{it}$ | - |

Table 4: Cost analysis of different annotators used in our experiments. "it/s" denotes the average number of instances per second and "\$/it" denotes the average USD per instance. The human annotation information is obtained from (Li et al., 2023b).

### 4.3 Evaluation \& Baselines

Metrics We use the accuracy metric to evaluate the reward model. To evaluate the policy model, we employ both automatic and human evaluation. For automatic evaluation, we utilize the pairwise comparison framework from AlpacaEval (Li et al., 2023b), setting the base policy model as the reference and "weighted_alpaca_eval_gpt4_turbo" as the LLM annotator, which is reported to have the highest agreement with the human evaluation. The models are compared based on their win rate against the reference model.

For human evaluation, we apply an identical pairwise comparison method, focusing on a subset of 200 data instances from Alpaca-Eval and IFEval respectively. The setup details of human evaluation are presented in Appendix C.

Baselines We benchmark our final aligned policy model against the following baselines: (1) Iterative DPO alignment with the fixed student model, (2) Best-of-N (BoN) sampling (Touvron et al., 2023) using the teacher model annotations, (3) Iterative DPO alignment with the fixed teacher model, (4) Iterative DPO alignment using online AI Feedback ${ }^{2}$ (Guo et al., 2024) (OAIF), and (5) direct[^1]

DPO alignment using the $40 \mathrm{~K}$ human preference data, which is also used to train the base student RM. Additional descriptions of the baselines are presented in Appendix D. We excluded the Iterative RLHF (Touvron et al., 2023) baseline due to the unstable training associated with LoRA-based proximal policy optimization, and the insufficient computational resources for full model training.

## 5 Results \& Analysis

### 5.1 Alignment Performance

In this section, we discuss the results of various iterative alignment strategies. Table 3 presents the win rate of the final aligned policy model compared to the base Alpaca-7B SFT model, as evaluated by GPT-4-Turbo. Firstly, we observe that even after the initial alignment iteration, the average win rates of on-policy iterative alignment methods, which use preference data derived from policy model outputs, exceed the direct DPO method which utilizes human-labeled preference data. This observation aligns with recent research on using on-policy data for preference fine-tuning (Tajwar et al., 2024; Yuan et al., 2024) and supports the feasibility of using the model-in-the-loop data annotation procedure as an efficient alternative to the human preference data collection method. Additionally, as shown in Table 4, human annotation is much more expensive than using models.

Secondly, we also observe that SFT with bestof-N sampling is less effective compared to direct DPO and "UltraRM-13B only (iter1)." Notably, "UltraRM-13B only (iter1)", which utilizes the same annotated preference data as BoN, outperforms BoN by an average win rate of $\sim 11 \%$. These results highlight the advantage of DPO, which provides both positive and negative responses for the

|  | Harmless Base | Helpful Base | Helpful Online | Helpful Rejection | Beavertails | Alpaca-Eval | IFEval | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| SPIN (iter2) | $61.51(0.91)$ | $67.90(0.88)$ | $66.26(1.25)$ | $68.90(0.80)$ | $62.39(0.70)$ | $73.50(1.37)$ | $\mathbf{6 9 . 2 2}(1.75)$ | 67.10 |
| Zephyr-7B-Beta | $63.73(0.91)$ | $75.11(0.81)$ | $72.83(1.17)$ | $75.33(0.75)$ | $\mathbf{6 8 . 6 6}(0.67)$ | $70.97(1.45)$ | $67.64(1.75)$ | $\mathbf{7 0 . 6 1}$ |
| Initial Student RM | $\mathbf{6 5 . 8 7}(0.83)$ | $78.76(0.72)$ | $72.15(1.16)$ | $77.00(0.68)$ | $63.87(0.85)$ | $72.82(1.39)$ | $56.95(1.82)$ | 69.63 |
| Final Student RM | $60.42(0.90)$ | $\mathbf{7 9 . 9 0}(0.74)$ | $\mathbf{7 3 . 6 1}(1.15)$ | $\mathbf{8 0 . 0 4}(0.67)$ | $61.23(0.89)$ | $\mathbf{7 6 . 2 1}(1.34)$ | $61.26(1.84)$ | 70.38 |

Table 5: Win rate (\%) of the final aligned models vs the base "Mistral-7B-SFT-Beta" as judged by GPT-4-Turbo.

policy model to learn from, supporting our decision to use DPO for iterative alignment.

Furthermore, the iterative OAIF approach does not perform as well as the iterative DPO, which utilizes either the fixed RoBERTa student RM or the fixed UltraRM-13B teacher RM. A key reason is that OAIF samples only two responses per instruction prompt and relies on external API to rank them, whereas using an RM allows for the simultaneous scoring of multiple responses and the identification of preference pairs with a large score margin, which are beneficial for DPO finetuning (Tajwar et al., 2024). Although API-based prompting could also rank or score multiple responses, this process is considerably slower than using an RM, as demonstrated by the annotation speed comparison in Table 4 between GPT-3.5-Turbo and the RMs.

Additionally, the win rate of our proposed student-teacher collaboration approach falls between the results achieved using solely the student RM and those using only the teacher RM across both iterations. These results are in line with our goal of achieving a good balance between efficiency and alignment performance, especially when the number of instruction prompts and the size of response candidates are large. The collaborative mechanism effectively distills the teacher's ranking capabilities into the student RM, as evidenced in subsequent sections, where we demonstrate that the refined student RM facilitates strong alignment with other base SFT models (\$5.2) and shows improvement in preference annotation on offline human preference test data (\$5.3).

Finally, the policy models demonstrate improved performance after two alignment iterations compared to just a single iteration. For example, our proposed pipeline leads to a $4 \%$ win rate improvement on average. This highlights the effectiveness of leveraging on-policy model generations for continuous updates of the policy model.

### 5.2 Transfer to Another Base Policy

In this section, we try to answer the question: Does the final student RM $\left(\mathcal{S}_{T}\right)$ help with the alignment of other base SFT models? Specifically, we experi- ment with a "Mistral-7B-SFT-Beta" (Tunstall et al., 2023) base policy model and compare the aligned model after one alignment iteration to Zephyr-7BBeta, SPIN ${ }^{3}$ (Chen et al., 2024), and a DPO baseline using the initial student $\mathrm{RM}\left(\mathcal{S}_{0}\right)$. All are based on the same Mistral (Jiang et al., 2023) backbone. Table 5 presents the win rate (\%) of various aligned policy models against the base "Mistral-7B-SFTBeta" model. Our method surpasses SPIN (two alignment iterations) by an average win rate of $3.28 \%$. SPIN is a strong self-evolution alignment method at the 7B scale, utilizing iterative supervised fine-tuning. The results demonstrate the effectiveness of DPO alignment with our student RM.

Additionally, our approach matches the performance of Zephyr-7B-Beta, a strong DPO-aligned model using $64 \mathrm{k}$ high-quality GPT-4 annotated preference data. Although our student RM is significantly smaller than GPT-4, it effectively leverages the distilled knowledge from the teacher model, enabling policy models to achieve comparable results. The performance of Zephyr-7B-Beta and our model complement each other, as each model excels on different datasets. This suggests a promising future exploration of combining offline with online preference data for policy model alignment.

Furthermore, we observe that the updated student RM outperforms the base student RM, indicating that the teacher's ranking capabilities have been effectively distilled into the student RM through our teacher-student collaborative mechanism. However, we also observe that DPO alignment with the initial student RM outperforms that with the final student RM on Harmless Base and Beavertails. This is because the initial student RM is trained on human data that includes both helpfulness and harmlessness preferences, while the teacher RM is optimized solely for helpfulness. Throughout the alignment iterations, the teacher's strengths in identifying helpful responses and its weaknesses in recognizing safe responses are gradually transferred to the students. Since helpfulness and harm-[^2]lessness are conflicting objectives, balancing them is outside the scope of this paper (Dai et al., 2023; Touvron et al., 2023). Future research may focus on better controlling the type of knowledge transferred from the teacher to the student. Nonetheless, the costs of maintaining the student RM in sync with the policy model are relatively low in TS-Align pipeline, and this efficient setup allows for scalable and continuous refinement of the policy models.

### 5.3 Performance of the Student

Table 6 shows the performance of various reward models on human preference test datasets. It is evident that the student RM's performance increasingly aligns with the teacher RM's after the iterative alignments, i.e., the performance of the student RM on the helpfulness preference datasets is increasingly better while that on harmless base is becoming worse. OpenAssistant's OASST Pythia and OASST DeBERTa reward models are fine-tuned using a diverse mixture of human-annotated preference data, including samples from the HH-RLHF training split, SHP training split, OpenAI's WebGPT (Nakano et al., 2021), and summarization comparisons (Stiennon et al., 2020b). The close average accuracy of our final student RM to both models indicates the effectiveness of our automatic preference data annotation pipeline.

Agreement with the Teacher To further validate the increasing agreement between the student RM and the teacher throughout our TS-Align pipeline, we compute the scores of $\mathcal{S}_{0}, \mathcal{S}_{1}, \mathcal{S}_{2}$, and $\mathcal{M}$ on three batches of on-policy data derived from $\pi_{0}, \pi_{1}$, and $\pi_{2}$ respectively. Here, $\pi_{0}$ represents the base policy "Mistral-7B-SFT-Beta" or "Alpaca-7B", $\pi_{1}$ is the policy model (iter1) with the teacher as the RM, and $\pi_{2}$ is the policy model (iter2) with the teacher as the RM. Each batch of on-policy preference data consists of approximately $30 \mathrm{~K}$ instruction prompts and a total of around $480 \mathrm{~K}$ candidate responses. The agreement between the students and the teacher is quantified using the Pearson correlation of their respective scores. We observe a clear increasing trend in the Pearson correlation coefficients for the base student $\left(\mathcal{S}_{0}\right)$, student iteration $1\left(\mathcal{S}_{1}\right)$, and student iteration $2\left(\mathcal{S}_{2}\right)$ with the teacher $(\mathcal{M})$, across different batches of on-policy data (generation from the base policy, policy iteration 1, and policy iteration 2), for both Mistral-7B-SFTBeta and Alpaca-7B as the base policy, suggesting the effectiveness of the student model in mimicking the teacher through the iterative alignment process.

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-07.jpg?height=736&width=748&top_left_y=326&top_left_x=1065)

Figure 2: Agreements between the teacher and students on various batches of on-policy data generated by policy models across different alignment iterations.

### 5.4 Additional Analysis

Human Evaluation Table 7 presents the pairwise human judgments on a randomly sampled subset of Alpaca-Eval and IFEval. The results show an increase in the win rate of policy models after the first and second alignment iterations using our TS-Align pipeline, which agrees with the GPT-4 judgments shown in Table 3 and validates the effectiveness of TS-Align. Additional analysis of the human evaluation is included in Appendix C.

Number of Sampled Responses We assess the alignment performance of the policy model with varying values of $K=\{2,4,8,16\}$ and conduct a single alignment iteration using the UltraRM-13B teacher as the reward model and Alpaca-7B as the base policy. The win rates of the aligned policy model compared to the base Alpaca-7B model on Alpaca-Eval, IFEval, Helpful Base, and Helpful Online are shown in Figure 3. Results for Helpful Rejection, Beavertails, and Harmless Base are detailed in Appendix E.1.

Generally, alignment performance improves with increasing $K$. A notable improvement is observed when $K$ increases from 8 to 16 across most datasets, supporting our chosen value of $K$ in prior experiments. Ideally, we should sample a highly diverse set of candidate responses, potentially setting $K>100$. Our teacher-student alignment pipeline will work much more efficiently than solely us-

|  | Harmless Base | Helpful Base | Helpful Online | Helpful Rejection | Beavertails | SHP | Alpaca-Farm | Average |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| OASST Pythia | 60.03 | 65.76 | 56.04 | 61.84 | 60.57 | 68.62 | 56.32 | 61.31 |
| OASST DeBERTa | 64.14 | 68.39 | 57.80 | 61.99 | 61.01 | 53.83 | 54.68 | 60.26 |
| UItraRM-13B (Teacher) | 39.40 | 71.79 | 62.20 | 67.08 | 64.05 | 71.57 | 61.65 | 62.53 |
| RoBERTa RM (Student Base) | 57.10 | 56.63 | 50.48 | 56.71 | 64.32 | 50.70 | 59.40 | 56.48 |
| RoBERTa RM (Student Iter1) | 54.89 | 61.43 | 53.57 | 61.73 | 65.56 | 55.87 | 61.48 | 59.97 |
| RoBERTa RM (Student Iter2) | 48.62 | 64.57 | 57.89 | 63.44 | 65.83 | 57.19 | 62.29 | 59.98 |

Table 6: Accuracy scores (\%) of different reward models on seven human preference test datasets.

|  | Alpaca-Eval |  |  |  | IFEval |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Pairwise (\%) | Win | Tie | Loss |  | Win | Tie | Loss |
| Iter1 vs SFT | 61.50 | 3.50 | 35.00 |  | 56.50 | 2.00 | 41.50 |
| Iter2 vs SFT | 70.00 | 3.00 | 27.00 |  | 63.00 | 1.00 | 36.00 |

Table 7: Human evaluation of pairwise comparisons of TS-Algined policy models vs the base Alpaca-7B SFT model on subsets of Alpaca-Eval and IFEval.
![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-08.jpg?height=468&width=744&top_left_y=1028&top_left_x=244)

Figure 3: Win rates (\%) of different numbers of $\mathrm{K}$.

ing the teacher in such a scenario. However, due to limited computational resources, we defer this exploration to future work.

Size of On-Policy Data We assess the impact of the on-policy data size by conducting a single alignment iteration using the UltraRM-13B teacher as the reward model and Alpaca-7B as the base policy. We compute the win rates of the aligned model versus the base policy on Alpaca-Eval, Helpful Base, Helpful Online, and Beavertails. As shown in Table 4, performance generally improves with increasing size of on-policy preference data. The differences from $18 \mathrm{~K}$ to $30 \mathrm{~K}$ are not significant on most datasets, suggesting that further increasing the size of instruction data may not bring performance gain. Hence, our choice of $30 \mathrm{~K}$ instruction data is reasonable.

## 6 Related Work

Iterative LLM Alignment can be broadly divided into two main approaches: The first focuses on selfevolution without relying on an external reward

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-08.jpg?height=486&width=771&top_left_y=622&top_left_x=1059)

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-08.jpg?height=226&width=346&top_left_y=641&top_left_x=1072)

Helpful Online

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-08.jpg?height=186&width=340&top_left_y=901&top_left_x=1075)

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-08.jpg?height=232&width=356&top_left_y=635&top_left_x=1455)

Beavertails

![](https://cdn.mathpix.com/cropped/2024_06_04_97113f0bb7a37d05a72ag-08.jpg?height=200&width=353&top_left_y=894&top_left_x=1434)

Figure 4: Win rates (\%) of different on-policy data size.

model (Li et al., 2023a; Yuan et al., 2024; Chen et al., 2024). For example, Yuan et al. (2024) proposes self-rewarding language models, where the process begins by bootstrapping instructions from the policy model, which then creates candidate responses based on these instructions. The model employs "LLM-as-a-Judge" prompting (Zheng et al., 2023) to evaluate and reward its own outputs. This approach allows the model to align itself through directed preference optimization using the selfcurated data. Li et al. (2023a) introduces instruction back-translation. This involves using the policy model to generate new instructions from text spans within the Clueweb corpus. The model then produces responses given the newly generated instructions. The resulting instruction-response pairs serve as a basis for further fine-tuning the policy model, enhancing its alignment through continuous refinement. However, these approaches heavily rely on the scale of the LLMs as the "LLM-as-aJudge" may not work well on smaller language models. Additionally, the self-rewarding mechanism tends to bias towards their generations.

The second approach, in contrast, relies on an external reward model to guide the alignment process (Touvron et al., 2023; Xu et al., 2023b; Singh et al., 2023; Guo et al., 2024; Dong et al., 2024). Touvron et al. (2023) uses human annotations of policy generations during each alignment iteration and employs rejection sampling to guide
the policy model to produce human-favored outputs. The rest adopt a similar pipeline to ours, using an external reward model to annotate policy model generations and derive pseudo-labeled preference data for alignment.

The key difference between TS-Align and other approaches is the teacher-student collaboration mechanism, which enables reliable and efficient annotation of large-scale preference data for policy model alignment. Our approach is also more practically feasible under conditions of limited budget and resources.

Synthetic Preference Data Several recent approaches propose to curate preference data through AI feedback (Bai et al., 2022b; Lee et al., 2023; Pace et al., 2024; Guo et al., 2024), which is an efficient way to obtain large-scale preference data than using human annotators. Bai et al. (2022b); Lee et al. (2023); Guo et al. (2024) propose to annotate model generations by prompting large language models while Pace et al. (2024) relies on a semi-supervised self-training setup (Scudder, 1965). Kim et al. (2023) employs a series of heuristic rules to generate preference data for reinforcement learning. For example, one of their assumptions is that models with larger sizes typically yield better responses than their smaller counterparts. Yang et al. (2023) leverages contrasting positive and negative prompts to create high- and low-quality response pairs. Our method aligns with the approach of using on-policy model generations for preference data collection and employs an efficient and reliable teacher-student collaborative framework for annotations. Additionally, we focus on enhancing a small-scale student reward model by distilling the ranking capabilities of a strong teacher model into the student through iterative alignment.

## 7 Conclusion

In this paper, we introduce TS-Align, a teacherstudent collaborative framework designed to balance reliability and efficiency in the data labeling process for iterative fine-tuning of policy models. By leveraging the strengths of a large-scale teacher model without requiring it to process all candidates, TS-Align combines the efficiency of a smaller student reward model with the reliability of a robust teacher model. This iterative alignment process results in a highly aligned policy model with an impressive average win rate of $69.7 \%$ over the base policy, as judged by GPT-4. Human evaluations also confirm the effectiveness of TS-Align. Additionally, we demonstrate that the teacher's knowledge is effectively distilled into the student, and the final student reward model, after iterative alignment, can be transferred to align other base policy models.

## Limitation

The effectiveness of TS-Align relies on the quality and robustness of the teacher model. If the teacher model is not sufficiently strong, the knowledge distilled into the student model may be suboptimal, affecting the overall performance of the alignment process. Additionally, while our approach is efficient for the current scale of models used, its scalability to even larger models or more complex tasks remains to be validated. Lastly, the applicability and effectiveness of TS-Align across a wide range of domains and tasks also need further exploration. The current results are promising, but additional testing is required to ensure that the approach generalizes well to various types of data and instructions.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.

Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345 .

Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv: 2401.01335.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.

Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback.

Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo $\mathrm{Xu}$, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. Safe RLHF: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773.

Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233.

Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. 2024. RLHF workflow: From reward modeling to online rlhf. arXiv preprint arXiv: 2405.07863.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. AlpacaFarm: A simulation framework for methods that learn from human feedback. In Thirty-seventh Conference on Neural Information Processing Systems.

Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with $\mathcal{V}$-usable information. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5988-6008. PMLR.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. KTO: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306.

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858.

Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10835-10866. PMLR.
Gemini Team Google, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.

Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. 2024. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. BeaverTails: Towards improved safety alignment of llm via a humanpreference dataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv: 2310.06825 .

Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Yoo, and Minjoon Seo. 2023. Aligning large language models through synthetic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13677-13700, Singapore. Association for Computational Linguistics.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213 .

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Minh Nguyen, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Alexandrovich Glushkov, Arnav Varma Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu

Nguyen, and Alexander Julian Mattick. 2023. Openassistant conversations - democratizing large language model alignment. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. 2023. RLAIF: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv: 2309.00267.

Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023a. Self-alignment with instruction backtranslation. arXiv preprint arXiv: 2308.06259.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.

Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv: 2302.02676 .

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 22631-22648. PMLR.

Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. SimPO: Simple preference optimization with a reference-free reward. arXiv preprint arXiv:2405.14734.

Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, et al. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.

Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2024. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv: 2401.12086.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487-503, Online. Association for Computational Linguistics.

Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020. AdapterHub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 46-54, Online. Association for Computational Linguistics.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems.

Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, patrick gallinari, and Matthieu Cord. 2022. Diverse weight averaging for out-of-distribution generalization. In Advances in Neural Information Processing Systems.

Henry Scudder. 1965. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11(3):363-371.

Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. 2023. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585.

Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2023. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020a. Learning to summarize with human feedback. In $A d$ vances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020b. Learning to summarize with human feedback. In $A d$ vances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.

Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. 2024. Prefer-
ence fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An instruction-following LLaMa model. https:// github.com/tatsu-lab/stanford_alpaca.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of $1 \mathrm{~m}$ alignment. arXiv preprint arXiv:2310.16944.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.

Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. Model soups: averaging weights of multiple finetuned models improves accuracy without increasing inference time. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 23965-23998. PMLR.

Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023a. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 62686278, Singapore. Association for Computational Linguistics.

Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. 2023b. Some things are more CRINGE than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv: 2312.16682.

Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2023. RLCD: Reinforcement learning from contrast distillation for language model alignment. arXiv preprint arXiv: 2307.12950.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. arXiv preprint arXiv: 2401.10020 .

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. RRHF: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv: 2304.05302 .

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, et al. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.

Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. 2024. LlamaFactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372.

Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv: Arxiv-1909.08593.
